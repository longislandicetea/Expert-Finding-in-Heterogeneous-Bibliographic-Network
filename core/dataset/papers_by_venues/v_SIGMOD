#index 210118
#* Proceedings of the 1996 ACM SIGMOD international conference on Management of data
#@ Jennifer Widom
#t 1996
#c 5

#index 210160
#* Mining quantitative association rules in large relational tables
#@ Ramakrishnan Srikant;Rakesh Agrawal
#t 1996
#c 5
#% 36672
#% 86950
#% 152934
#% 201894
#% 412588
#% 463883
#% 481290
#% 481588
#% 481754
#% 481758
#! We introduce the problem of mining association rules in large relational tables containing both quantitative and categorical attributes. An example of such an association might be "10% of married people between age 50 and 60 have at least 2 cars". We deal with quantitative attributes by fine-partitioning the values of the attribute and then combining adjacent partitions as necessary. We introduce measures of partial completeness which quantify the information lost due to partitioning. A direct application of this technique can generate too many similar rules. We tackle this problem by using a "greater-than-expected-value" interest measure to identify the interesting rules in the output. We give an algorithm for mining such quantitative association rules. Finally, we describe the results of using this approach on a real-life dataset.

#index 210162
#* Data mining using two-dimensional optimized association rules: scheme, algorithms, and visualization
#@ Takeshi Fukuda;Yasukiko Morimoto;Shinichi Morishita;Takeshi Tokuyama
#t 1996
#c 5
#% 83104
#% 136350
#% 152934
#% 201894
#% 210160
#% 213977
#% 281856
#% 412588
#% 449588
#% 452821
#% 459008
#% 463742
#% 480940
#% 480964
#% 481100
#% 481281
#% 481290
#% 548431
#! We discuss data mining based on association rules for two numeric attributes and one Boolean attribute. For example, in a database of bank customers, "Age" and "Balance" are two numeric attributes, and "CardLoan" is a Boolean attribute. Taking the pair (Age, Balance) as a point in two-dimensional space, we consider an association rule of the form((Age, Balance) ∈ P) ⇒ (CardLoan = Yes),which implies that bank customers whose ages and balances fall in a planar region P tend to use card loan with a high probability. We consider two classes of regions, rectangles and admissible (i.e. connected and x-monotone) regions. For each class, we propose efficient algorithms for computing the regions that give optimal association rules for gain, support, and confidence, respectively. We have implemented the algorithms for admissible regions, and constructed a system for visualizing the rules.

#index 210164
#* IDEA: interactive data exploration and analysis
#@ Peter G. Selfridge;Divesh Srivastava;Lynn O. Wilson
#t 1996
#c 5
#% 412588
#% 452821
#! The analysis of business data is often an ill-defined task characterized by large amounts of noisy data. Because of this, business data analysis must combine two kinds of intertwined tasks: exploration and analysis. Exploration is the process of finding the appropriate subset of data to analyze, and analysis is the process of measuring the data to provide the business answer. While there are many tools available both for exploration and for analysis, a single tool or set of tools may not provide full support for these intertwined tasks. We report here on a project that set out to understand a specific business data analysis problem and build an environment to support it. The results of this understanding are, first of all, a detailed list of requirements of this task; second, a set of capabilities that meet these requirements; and third, an implemented client-server solution that addresses many of these requirements and identifies others for future work. Our solution incorporates several novel perspectives on data analysis and combines a history mechanism with a graphical, re-usable representation of the analysis and exploration process. Our approach emphasizes using the database itself to represent as many of these functions as possible.

#index 210166
#* Rapid bushy join-order optimization with Cartesian products
#@ Bennet Vance;David Maier
#t 1996
#c 5
#% 554
#% 83154
#% 102765
#% 152940
#% 411554
#% 464700
#% 481429
#% 565457
#! Query optimizers often limit the search space for join orderings, for example by excluding Cartesian products in subplans or by restricting plan trees to left-deep vines. Such exclusions are widely assumed to reduce optimization effort while minimally affecting plan quality. However, we show that searching the complete space of plans is more affordable than has been previously recognized, and that the common exclusions may be of little benefit.We start by presenting a Cartesian product optimizer that requires at most a few seconds of workstation time to search the space of bushy plans for products of up to 15 relations. Building on this result, we present a join-order optimizer that achieves a similar level of performance, and retains the ability to include Cartesian products in subplans wherever appropriate. The main contribution of the paper is in fully separating join-order enumeration from predicate analysis, and in showing that the former problem in particular can be solved swiftly by novel implementation techniques. A secondary contribution is to initiate a systematic approach to the benchmarking of join-order optimization, which we apply to the evaluation of our method.

#index 210167
#* SQL query optimization: reordering for a general class of queries
#@ Piyush Goel;Bala Iyer
#t 1996
#c 5
#% 32878
#% 86947
#% 116043
#% 149845
#% 172933
#% 201927
#% 411554
#% 463276
#% 464078
#% 564426
#% 581485
#% 581486
#! The strength of commercial query optimizers like DB2 comes from their ability to select an optimal order by generating all equivalent reorderings of binary operators. However, there are no known methods to generate all equivalent reorderings for a SQL query containing joins, outer joins, and groupby aggregations. Consequently, some of the reorderings with significantly lower cost may be missed. Using hypergraph model and a set of novel identities, we propose a method to reorder a SQL query containing joins, outer joins, and groupby aggregations. While these operators are sufficient to capture the SQL semantics, it is during their reordering that we identify a powerful primitive needed for a dbms. We report our findings of a simple, yet fundamental operator, generalized selection, and demonstrate its power to solve the problem of reordering of SQL queries containing joins, outer joins, and groupby aggregations.

#index 210169
#* Fundamental techniques for order optimization
#@ David Simmen;Eugene Shekita;Timothy Malkemus
#t 1996
#c 5
#% 32889
#% 43162
#% 58377
#% 83154
#% 116043
#% 136740
#% 172930
#% 287222
#% 287295
#% 318049
#% 411554
#% 427195
#% 463735
#% 463761
#% 481101
#! Decision support applications are growing in popularity as more business data is kept on-line. Such applications typically include complex SQL queries that can test a query optimizer's ability to produce an efficient access plan. Many access plan strategies exploit the physical ordering of data provided by indexes or sorting. Sorting is an expensive operation, however. Therefore, it is imperative that sorting is optimized in some way or avoided all together. Toward that goal, this paper describes novel optimization techniques for pushing down sorts in joins, minimizing the number of sorting columns, and detecting when sorting can be avoided because of predicates, keys, or indexes. A set of fundamental operations is described that provide the foundation for implementing such techniques. The operations exploit data properties that arise from predicate application, uniqueness, and functional dependencies. These operations and techniques have been implemented in IBM's DB2/CS.

#index 210170
#* A Teradata content-based multimedia object manager for massively parallel architectures
#@ W. O'Connell;I. T. Ieong;D. Schrader;C. Watson;G. Au;A. Biliris;S. Choo;P. Colin;G. Linderman;E. Panagos;J. Wang;T. Walter
#t 1996
#c 5
#% 5201
#% 83235
#% 111351
#% 159110
#% 172961
#% 172962
#% 172963
#% 173004
#% 191405
#% 201953
#% 403195
#% 458552
#% 463884
#% 481425
#% 481428
#% 565455
#! The Teradata Multimedia Object Manager is a general-purpose content analysis multimedia server designed for symmetric multiprocessing and massively parallel processing environments. The Multimedia Object Manager defines and manipulates user-defined functions (UDFs), which are invoked in parallel to analyze or manipulate the contents of multimedia objects. Several computationally intensive applications of this technology, which use large persistent datasets, include fingerprint matching, signature verification, face recognition, and speech recognition/translation.

#index 210171
#* Fault-tolerant architectures for continuous media servers
#@ Banu Özden;Rajeev Rastogi;Prashant Shenoy;Avi Silberschatz
#t 1996
#c 5
#% 43172
#% 83130
#% 128184
#% 149275
#% 151340
#% 159275
#% 201932
#% 395431
#% 632238
#% 661673
#! Continuous media servers that provide support for the storage and retrieval of continuous media data (e.g., video, audio) at guaranteed rates are becoming increasingly important. Such servers, typically, rely on several disks to service a large number of clients, and are thus highly susceptible to disk failures. We have developed two fault-tolerant approaches that rely on admission control in order to meet rate guarantees for continuous media requests. The schemes enable data to be retrieved from disks at the required rate even if a certain disk were to fail. For both approaches, we present data placement strategies and admission control algorithms. We also present design techniques for maximizing the number of clients that can be supported by a continuous media server. Finally, through extensive simulations, we demonstrate the effectiveness of our schemes.

#index 210172
#* Optimizing queries over multimedia repositories
#@ Surajit Chaudhuri;Luis Gravano
#t 1996
#c 5
#% 554
#% 67565
#% 71573
#% 77648
#% 77659
#% 86950
#% 108508
#% 152940
#% 164360
#% 172931
#% 201876
#% 213981
#% 287349
#% 411554
#% 411751
#% 427199
#% 479938
#% 480093
#% 480944
#% 481748
#% 614579
#! Repositories of multimedia objects having multiple types of attributes (e.g., image, text) are becoming increasingly common. A selection on these attributes will typically produce not just a set of objects, as in the traditional relational query model (filtering), but also a grade of match associated with each object, indicating how well the object matches the selection condition (ranking). Also, multimedia repositories may allow access to the attributes of each object only through indexes. We investigate how to optimize the processing of queries over multimedia repositories. A key issue is the choice of the indexes used to search the repository. We define an execution space that is search-minimal, i.e., the set of indexes searched is minimal. Although the general problem of picking an optimal plan in the search-minimal execution space is NP-hard, we solve the problem efficiently when the predicates in the query are independent. We also show that the problem of optimizing queries that ask for a few top-ranked objects can be viewed, in many cases, as that of evaluating selection conditions. Thus, both problems can be viewed together as an extended filtering problem.

#index 210173
#* BIRCH: an efficient data clustering method for very large databases
#@ Tian Zhang;Raghu Ramakrishnan;Miron Livny
#t 1996
#c 5
#% 114667
#% 451051
#% 451052
#% 481281
#% 527022
#% 674087
#! Finding useful patterns in large datasets has attracted considerable interest recently, and one of the most widely studied problems in this area is the identification of clusters, or densely populated regions, in a multi-dimensional dataset. Prior work does not adequately address the problem of large datasets and minimization of I/O costs.This paper presents a data clustering method named BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies), and demonstrates that it is especially suitable for very large databases. BIRCH incrementally and dynamically clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resources (i.e., available memory and time constraints). BIRCH can typically find a good clustering with a single scan of the data, and improve the quality further with a few additional scans. BIRCH is also the first clustering algorithm proposed in the database area to handle "noise" (data points that are not part of the underlying pattern) effectively.We evaluate BIRCH's time/space efficiency, data input order sensitivity, and clustering quality through several experiments. We also present a performance comparisons of BIRCH versus CLARANS, a clustering method proposed recently for large datasets, and show that BIRCH is consistently superior.

#index 210174
#* On-line reorganization of sparsely-populated B+-trees
#@ Chendong Zou;Betty Salzberg
#t 1996
#c 5
#% 40633
#% 83183
#% 116085
#% 116086
#% 154310
#% 403195
#% 458528
#% 462174
#% 463264
#% 480939
#% 481624
#! In this paper, we present an efficient method to do online reorganization of sparsely-populated B+-trees. It reorganizes the leaves first, compacting in short operations groups of leaves with the same parent. After compacting, optionally, the new leaves may swap locations or be moved into empty pages so that they are in key order on the disk. After the leaves are reorganized, the method shrinks the tree by making a copy of the upper part of the tree while leaving the leaves in place. A new concurrency method is introduced so that only a minimum number of pages are locked during reorganization. During leaf reorganization, Forward Recovery is used to save all work already done while maintaining consistency after system crashes. A heuristic algorithm is developed to reduce the number of swaps needed during leaf reorganization, so that better concurrency and easier recovery can be achieved. A detailed description of switching from the old B+-tree to the new B+-tree is described for the first time.

#index 210175
#* Two techniques for on-line index modification in shared nothing parallel databases
#@ Kiran J. Achyutuni;Edward Omiecinski;Shamkant B. Navathe
#t 1996
#c 5
#% 5381
#% 63288
#% 83183
#% 88054
#% 102807
#% 115661
#% 116086
#% 158047
#% 317949
#% 442860
#% 480939
#% 702311
#! Whenever data is moved across nodes in the parallel database system, the indexes need to be modified too. Index modification overhead can be quite severe because there can be a large number of indexes on a relation. In this paper, we study two alternatives to index modification, namely OAT (One-At-a-Time page movement) and BULK (bulk page movement). OAT and BULK are two extremes on the spectrum of the granularity of data movement. OAT and BULK differ in two respects: first, OAT uses very little additional disk space (at most one extra page), whereas BULK uses a large amount of disk space. Second, BULK uses sequential prefetch I/O to optimize on the number of I/Os during index modification, while OAT does not. Using an experimental testbed, we show that BULK is an order of magnitude faster than OAT. In terms of the impact on transaction performance during reorganization, BULK and OAT perform differently: when the number of indexes to be modified is either one or two, OAT has a lesser impact on the transaction performance degradation. However, when the number of indexes is greater than two, both techniques have the same impact on transaction performance.

#index 210176
#* Query caching and optimization in distributed mediator systems
#@ S. Adali;K. S. Candan;Y. Papakonstantinou;V. S. Subrahmanian
#t 1996
#c 5
#% 8850
#% 24408
#% 32914
#% 59350
#% 77944
#% 83526
#% 85086
#% 86946
#% 102780
#% 116303
#% 116987
#% 130189
#% 152928
#% 158909
#% 171049
#% 172902
#% 201930
#% 216621
#% 368248
#% 411554
#% 427218
#% 442888
#% 443075
#% 463736
#% 463906
#% 464056
#% 464203
#% 480623
#% 480788
#% 481101
#% 481125
#% 481425
#! Query processing and optimization in mediator systems that access distributed non-proprietary sources pose many novel problems. Cost-based query optimization is hard because the mediator does not have access to source statistics information and furthermore it may not be easy to model the source's performance. At the same time, querying remote sources may be very expensive because of high connection overhead, long computation time, financial charges, and temporary unavailability. We propose a cost-based optimization technique that caches statistics of actual calls to the sources and consequently estimates the cost of the possible execution plans based on the statistics cache. We investigate issues pertaining to the design of the statistics cache and experimentally analyze various tradeoffs. We also present a query result caching mechanism that allows us to effectively use results of prior queries when the source is not readily available. We employ the novel invariants mechanism, which shows how semantic information about data sources may be used to discover cached query results of interest.

#index 210177
#* Performance tradeoffs for client-server query processing
#@ Michael J. Franklin;Björn Thór Jónsson;Donald Kossmann
#t 1996
#c 5
#% 2853
#% 3771
#% 13020
#% 68206
#% 77656
#% 83126
#% 86949
#% 116040
#% 136740
#% 164748
#% 172939
#% 172958
#% 201058
#% 339717
#% 395735
#% 411554
#% 463728
#% 479937
#% 481104
#! The construction of high-performance database systems that combine the best aspects of the relational and object-oriented approaches requires the design of client-server architectures that can fully exploit client and server resources in a flexible manner. The two predominant paradigms for client-server query execution are data-shipping and query-shipping We first define these policies in terms of the restrictions they place on operator site selection during query optimization. We then investigate the performance tradeoffs between them for bulk query processing. While each strategy has advantages, neither one on its own is efficient across a wide range of circumstances. We describe and evaluate a more flexible policy called hybrid-shipping, which can execute queries at clients, servers, or any combination of the two. Hybrid-shipping is shown to at least match the best of the two "pure" policies, and in some situations, to perform better than both. The implementation of hybrid-shipping raises a number of difficult problems for query optimization. We describe an initial investigation into the use of a 2-step query optimization strategy as a way of addressing these issues.

#index 210178
#* Data access for the masses through OLE DB
#@ José A. Blakeley
#t 1996
#c 5
#% 47623
#% 111372
#% 137650
#% 172964
#% 184573
#% 207260
#% 403195
#% 437029
#! This paper presents an overview of OLE DB, a set of interfaces being developed at Microsoft whose goal is to enable applications to have uniform access to data stored in DBMS and non-DBMS information containers. Applications will be able to take advantage of the benefits of database technology without having to transfer data from its place of origin to a DBMS. Our approach consists of defining an open, extensible Collection of interfaces that factor and encapsulate orthogonal, reusable portions of DBMS functionality. These interfaces define the boundaries of DBMS components such as record containers, query processors, and transaction coordinators that enable uniform, transactional access to data among such components. The proposed interfaces extend Microsoft's OLE/COM object services framework with database functionality, hence these interfaces are collectively referred to as OLE DB. The OLE DB functional areas include data access and updates (rowsets), query processing, schema information, notifications, transactions, security, and access to remote data. In a sense, OLE DB represents an effort to bring database technology to the masses. This paper presents an overview of the OLE DB approach and its areas of componentization.

#index 210179
#* The dangers of replication and a solution
#@ Jim Gray;Pat Helland;Patrick O'Neil;Dennis Shasha
#t 1996
#c 5
#% 2027
#% 9241
#% 32884
#% 201869
#% 403195
#% 602675
#% 778539
#! Update anywhere-anytime-anyway transactional replication has unstable behavior as the workload scales up: a ten-fold increase in nodes and traffic gives a thousand fold increase in deadlocks or reconciliations. Master copy replication (primary copy) schemes reduce this problem. A simple analytic model demonstrates these results. A new two-tier replication algorithm is proposed that allows mobile (disconnected) applications to propose tentative update transactions that are later applied to a master copy. Commutative update transactions avoid the instability of other replication schemes.

#index 210180
#* Hot mirroring: a method of hiding parity update penalty and degradation during rebuilds for RAID5
#@ Kazuhiko Mogi;Masaru Kitsuregawa
#t 1996
#c 5
#% 43172
#% 83130
#% 107692
#% 125400
#% 128184
#% 146597
#% 148175
#% 148176
#% 152944
#% 152945
#% 202141
#% 340592
#% 340629
#% 460667
#% 462779
#% 481761
#! This paper proposes a storage management scheme for disk arrays, named hot mirroring. In this scheme, storage space is partitioned into two regions. One is the mirrored region, which is characterized by high performance and low storage efficiency. The other is the RAID5 region, which is characterized by low performance and high storage efficiency. Hot data blocks are stored in the former area, while cold blocks are stored in the latter. In addition, mirrored pairs and RAID5 stripes are orthogonally laid out, through which the performance degradation during rebuilding is minimized. Hot block clustering in hot mirroring achieves higher performance than conventional RAID5 arrays. The potential of hot mirroring is examined through extensive simulation.

#index 210181
#* Random I/O scheduling in online tertiary storage systems
#@ Bruce K. Hillyer;Avi Silberschatz
#t 1996
#c 5
#% 6069
#% 86796
#% 102744
#% 111378
#% 152965
#% 164766
#% 191225
#% 201707
#% 209896
#% 317962
#% 481613
#% 503542
#% 612099
#% 674934
#! New database applications that require the storage and retrieval of many terabytes of data are reaching the limits for disk-based storage systems, in terms of both cost and scalability. These limits provide a strong incentive for the development of databases that augment disk storage with technologies better suited to large volumes of data. In particular, the seamless incorporation of tape storage into database systems would be of great value. Tape storage is two orders of magnitude more efficient than disk in terms of cost per terabyte and physical volume per terabyte; however, a key problem is that the random access latency of tape is three to four orders of magnitude slower than disk. Thus, to incorporate a tape bulk store in an online storage system, the problem of tape access latency must be solved. One approach to reducing the latency is careful I/O scheduling. The focus of this paper is on efficient random I/O scheduling for tape drives that use a serpentine track layout, such as the Quantum DLT and the IBM 3480 and 3590. For serpentine tape, I/O scheduling is problematic because of the complex relationships between logical block numbers, their physical positions on tape, and the time required for tape positioning between these physical positions. The results in this paper show that our scheduling schemes provide a significant improvement in the latency of random access to serpentine tape.

#index 210182
#* Implementing data cubes efficiently
#@ Venky Harinarayan;Anand Rajaraman;Jeffrey D. Ullman
#t 1996
#c 5
#% 136740
#% 186417
#% 191154
#% 210182
#% 214219
#% 462204
#% 481288
#% 481604
#% 481749
#! Decision support applications involve complex queries on very large databases. Since response times should be small, query optimization is critical. Users typically view the data as multidimensional data cubes. Each cell of the data cube is a view consisting of an aggregation of interest, like total sales. The values of many of these cells are dependent on the values of other cells in the data cube. A common and powerful query optimization technique is to materialize some or all of these cells rather than compute them from raw data each time. Commercial systems differ mainly in their approach to materializing the data cube. In this paper, we investigate the issue of which cells (views) to materialize when it is too expensive to materialize all views. A lattice framework is used to express dependencies among views. We present greedy algorithms that work off this lattice and determine a good set of views to materialize. The greedy algorithm performs within a small constant factor of optimal under a variety of models. We then consider the most common case of the hypercube lattice and examine the choice of materialized views for hypercubes in detail, giving some good tradeoffs between the space used and the average time to answer a query.

#index 210183
#* Providing better support for a class of decision support queries
#@ Sudhir G. Rao;Antonio Badia;Dirk van Gucht
#t 1996
#c 5
#% 2247
#% 32878
#% 63293
#% 66208
#% 116043
#% 149629
#% 191154
#% 235914
#% 285932
#% 286253
#% 415987
#% 427195
#% 462499
#% 463871
#% 463894
#% 480091
#% 481288
#! Relational database systems do not effectively support complex queries containing quantifiers (quantified queries) that are increasingly becoming important in decision support applications. Generalized quantifiers provide an effective way of expressing such queries naturally. In this paper, we consider the problem of processing quantified queries within the generalized quantifier framework. We demonstrate that current relational systems are ill-equipped, both at the language and at the query processing level, to deal with such queries. We also provide insights into the intrinsic difficulties associated with processing such queries. We then describe the implementation of a quantified query processor, Q2P, that is based on multidimensional and boolean matrix structures. We provide results of performance experiments run on Q2P that demonstrate superior performance on quantified queries. Our results indicate that it is feasible to augment relational systems with query subsystems like Q2P for significant performance benefits for quantified queries in decision support applications.

#index 210184
#* A query language for multidimensional arrays: design, implementation, and optimization techniques
#@ Leonid Libkin;Rona Machlin;Limsoon Wong
#t 1996
#c 5
#% 60748
#% 67851
#% 102763
#% 137862
#% 137866
#% 148242
#% 159506
#% 163444
#% 164376
#% 172950
#% 174683
#% 189868
#% 191577
#% 201873
#% 370758
#% 464058
#% 481614
#% 503542
#% 511184
#% 562129
#% 562130
#% 569211
#% 569212
#% 700905
#% 837654
#! While much recent research has focussed on extending databases beyond the traditional relational model, relatively little has been done to develop database tools for querying data organized in (multidimensional) arrays. The scientific computing community has made little use of available database technology. Instead, multidimensional scientific data is typically stored in local files conforming to various data exchange formats and queried via specialized access libraries tied in to general purpose programming languages.To allow such data to be queried using known database techniques, we design and implement a query language for multidimensional arrays. Our main design decision is to treat arrays as functions from index sets to values rather than as collection types. This leads to clean syntax and semantics as well as simple but powerful optimization rules.We present a calculus for arrays that extends standard calculi for complex objects. We derive a higher-level comprehension style query language based on this calculus and describe its implementation, including a data driver for the NetCDF data exchange format. Next, we explore some optimization rules obtained from the equational laws of our core calculus. Finally, we study the expressiveness of our calculus and prove that it essentially corresponds to adding ranking to a query language for complex objects.

#index 210185
#* A super scalar sort algorithm for RISC processors
#@ Ramesh C. Agarwal
#t 1996
#c 5
#% 14204
#% 63505
#% 80728
#% 172911
#% 173037
#% 174816
#% 188711
#% 340670
#% 356652
#% 365700
#% 435159
#! The compare and branch sequences required in a traditional sort algorithm can not efficiently exploit multiple execution units present in currently available high performance RISC processors. This is because of the long latency of the compare instructions and the sequential algorithm used in sorting. With the increased level of integration on a chip, this trend is expected to continue. We have developed new sort algorithms which eliminate almost all the compares, provide functional parallelism which can be exploited by multiple execution units, significantly reduce the number of passes through keys, and improve data locality. These new algorithms outperform traditional sort algorithms by a large factor.For the Datamation disk to disk sort benchmark (one million 100-byte records), at SIGMOD'94, Chris Nyberg et al presented several new performance records using DEC alpha processor based systems.We have implemented the Datamation sort benchmark using our new sort algorithm on a desktop IBM RS/6000 model 39H (66.6 MHz) with 8 IBM SSA 7133 disk drives (total cost $73K). The total elapsed time for the 100 MB sort was 5.1 seconds (vs the old uni-processor record of 9.1 seconds). We have also established a new price performance record (0.2¢ vs the old record of 0.9¢, as the cost of the sort). The entire sort processing was overlapped with I/O. During the read phase, we achieved a sustained BW of 47 MB/sec and during the write phase, we achieved a sustained BW of 39 MB/sec. Key extraction and sorting of one million 10-byte keys took only 0.6 second of CPU time. The rest of the CPU time was used in moving records, servicing I/O, and other overheads.Algorithmic details leading to this level of performance are described in this paper. A detailed analysis of the CPU time spent during various phases of the sort algorithm and I/O is also provided.

#index 210186
#* Spatial hash-joins
#@ Ming-Ling Lo;Chinya V. Ravishankar
#t 1996
#c 5
#% 3771
#% 25924
#% 32913
#% 58369
#% 77963
#% 86950
#% 86952
#% 114577
#% 152937
#% 172909
#% 210187
#% 427195
#% 427199
#% 462956
#% 462957
#% 463116
#% 463595
#% 480093
#% 480272
#% 481428
#% 526847
#% 527012
#! We examine how to apply the hash-join paradigm to spatial joins, and define a new framework for spatial hash-joins. Our spatial partition functions have two components: a set of bucket extents and an assignment function, which may map a data item into multiple buckets. Furthermore, the partition functions for the two input datasets may be different.We have designed and tested a spatial hash-join method based on this framework. The partition function for the inner dataset is initialized by sampling the dataset, and evolves as data are inserted. The partition function for the outer dataset is immutable, but may replicate a data item from the outer dataset into multiple buckets. The method mirrors relational hash-joins in other aspects. Our method needs no pre-computed indices. It is therefore applicable to a wide range of spatial joins.Our experiments show that our method outperforms current spatial join algorithms based on tree matching by a wide margin. Further, its performance is superior even when the tree-based methods have pre-computed indices. This makes the spatial hash-join method highly competitive both when the input datasets are dynamically generated and when the datasets have pre-computed indices.

#index 210187
#* Partition based spatial-merge join
#@ Jignesh M. Patel;David J. DeWitt
#t 1996
#c 5
#% 2115
#% 12181
#% 13041
#% 18614
#% 25924
#% 32913
#% 58369
#% 83118
#% 83132
#% 86950
#% 86952
#% 152902
#% 152937
#% 172908
#% 172909
#% 172939
#% 172962
#% 210186
#% 210187
#% 285932
#% 321455
#% 408343
#% 415957
#% 427199
#% 445701
#% 462331
#% 463436
#% 463595
#% 480966
#% 481428
#% 481589
#% 527012
#% 563595
#! This paper describes PBSM (Partition Based Spatial-Merge), a new algorithm for performing spatial join operation. This algorithm is especially effective when neither of the inputs to the join have an index on the joining attribute. Such a situation could arise if both inputs to the join are intermediate results in a complex query, or in a parallel environment where the inputs must be dynamically redistributed. The PBSM algorithm partitions the inputs into manageable chunks, and joins them using a computational geometry based plane-sweeping technique. This paper also presents a performance study comparing the the traditional indexed nested loops join algorithm, a spatial join algorithm based on joining spatial indices, and the PBSM algorithm. These comparisons are based on complete implementations of these algorithms in Paradise, a database system for handling GIS applications. Using real data sets, the performance study examines the behavior of these spatial join algorithms in a variety of situations, including the cases when both, one, or none of the inputs to the join have an suitable index. The study also examines the effect of clustering the join inputs on the performance of these join algorithms. The performance comparisons demonstrates the feasibility, and applicability of the PBSM join algorithm.

#index 210188
#* Bifocal sampling for skew-resistant join size estimation
#@ Sumit Ganguly;Phillip B. Gibbons;Yossi Matias;Avi Silberschatz
#t 1996
#c 5
#% 58348
#% 77940
#% 82346
#% 102786
#% 116084
#% 126699
#% 137885
#% 145196
#% 164361
#% 190330
#% 268747
#% 277347
#% 481749
#! This paper introduces bifocal sampling, a new technique for estimating the size of an equi-join of two relations. Bifocal sampling classifies tuples in each relation into two groups, sparse and dense, based on the number of tuples with the same join value. Distinct estimation procedures are employed that focus on various combinations for joining tuples (e.g., for estimating the number of joining tuples that are dense in both relations). This combination of estimation procedures overcomes some well-known problems in previous schemes, enabling good estimates with no a priori knowledge about the data distribution. The estimate obtained by the bifocal sampling algorithm is proven to lie with high probability within a small constant factor of the actual join size, regardless of the skew, as long as the join size is Ω(n lg n), for relations consisting of n tuples. The algorithm requires a sample of size at most O(√n lg n). By contrast, previous algorithms using a sample of similar size may require the join size to be Ω(n√n) to guarantee an accurate estimate. Experimental results support the theoretical claims and show that bifocal sampling is practical and effective.

#index 210189
#* Estimating alphanumeric selectivity in the presence of wildcards
#@ P. Krishnan;Jeffrey Scott Vitter;Bala Iyer
#t 1996
#c 5
#% 2008
#% 25208
#% 55253
#% 69273
#% 82346
#% 116084
#% 152939
#% 172892
#% 190330
#% 201889
#% 201921
#% 201927
#% 201935
#% 216139
#% 252608
#% 288578
#% 289010
#% 411554
#% 464062
#! Success of commercial query optimizers and database management systems (object-oriented or relational) depend on accurate cost estimation of various query reordering [BGI]. Estimating predicate selectivity, or the fraction of rows in a database that satisfy a selection predicate, is key to determining the optimal join order. Previous work has concentrated on estimating selectivity for numeric fields [ASW, HaSa, IoP, LNS, SAC, WVT]. With the popularity of textual data being stored in databases, it has become important to estimate selectivity accurately for alphanumeric fields. A particularly problematic predicate used against alphanumeric fields is the SQL like predicate [Dat]. Techniques used for estimating numeric selectivity are not suited for estimating alphanumeric selectivity.In this paper, we study for the first time the problem of estimating alphanumeric selectivity in the presence of wildcards. Based on the intuition that the model built by a data compressor on an input text encapsulates information about common substrings in the text, we develop a technique based on the suffix tree data structure to estimate alphanumeric selectivity. In a statistics generation pass over the database, we construct a compact suffix tree-based structure from the columns of the database. We then look at three families of methods that utilize this structure to estimate selectivity during query plan costing, when a query with predicates on alphanumeric attributes contains wildcards in the predicate.We evaluate our methods empirically in the context of the TPC-D benchmark. We study our methods experimentally against a variety of query patterns and identify five techniques that hold promise.

#index 210190
#* Improved histograms for selectivity estimation of range predicates
#@ Viswanath Poosala;Peter J. Haas;Yannis E. Ioannidis;Eugene J. Shekita
#t 1996
#c 5
#% 1331
#% 2152
#% 30191
#% 43161
#% 43163
#% 54047
#% 82346
#% 86949
#% 102784
#% 152585
#% 152917
#% 172902
#% 201921
#% 411554
#% 427219
#% 464062
#% 481266
#% 481749
#% 689389
#! Many commercial database systems maintain histograms to summarize the contents of relations and permit efficient estimation of query result sizes and access plan costs. Although several types of histograms have been proposed in the past, there has never been a systematic study of all histogram aspects, the available choices for each aspect, and the impact of such choices on histogram effectiveness. In this paper, we provide a taxonomy of histograms that captures all previously proposed histogram types and indicates many new possibilities. We introduce novel choices for several of the taxonomy dimensions, and derive new histogram types by combining choices in effective ways. We also show how sampling techniques can be used to reduce the cost of histogram construction. Finally, we present results from an empirical study of the proposed histogram types used in selectivity estimation of range predicates and identify the histogram types that have the best overall performance.

#index 210192
#* Structures for manipulating proposed updates in object-oriented databases
#@ Michael Doherty;Richard Hull;Mohammed Rupawalla
#t 1996
#c 5
#% 120697
#% 209729
#% 384978
#% 395735
#% 480616
#% 481130
#% 535976
#% 562143
#! Support for virtual states and deltas between them is useful for a variety of database applications, including hypothetical database access, version management, simulation, and active databases. The Heraclitus paradigm elevates delta values to be "first-class citizens" in database programming languages, so that they can be explicitly created, accessed and manipulated.A fundamental issue concerns the trade-off between the "accuracy" or "robustness" of a form of delta representation, and the ease of access and manipulation of that form. At one end of the spectrum, code-blocks could be used to represent delta values, resulting in a more accurate capture of the intended meaning of a proposed update, at the cost of more expensive access and manipulation. In the context of object-oriented databases, another point on the spectrum is "attribute-granularity" deltas which store the net changes to each modified attribute value of modified objects.This paper introduces a comprehensive framework for specifying a broad array of forms for representing deltas for complex value types (tuple, set, bag, list, o-set and dictionary). In general, the granularity of such deltas can be arbitrarily deep within the complex value structure. Applications of this framework in connection with hypothetical access to, and "merging" of, proposed updates are discussed.

#index 210194
#* Safe and efficient sharing of persistent objects in Thor
#@ B. Liskov;A. Adya;M. Castro;S. Ghemawat;R. Gruber;U. Maheshwari;A. C. Myers;M. Day;L. Shrira
#t 1996
#c 5
#% 57954
#% 107720
#% 111351
#% 111361
#% 116057
#% 121873
#% 151533
#% 151540
#% 152904
#% 156715
#% 158845
#% 172206
#% 172939
#% 172943
#% 196444
#% 200004
#% 201871
#% 202979
#% 320108
#% 320388
#% 340620
#% 442703
#% 480795
#% 481431
#% 701689
#% 702279
#! Thor is an object-oriented database system designed for use in a heterogeneous distributed environment. It provides highly-reliable and highly-available persistent storage for objects, and supports safe sharing of these objects by applications written in different programming languages.Safe heterogeneous sharing of long-lived objects requires encapsulation: the system must guarantee that applications interact with objects only by invoking methods. Although safety concerns are important, most object-oriented databases forgo safety to avoid paying the associated performance costs.This paper gives an overview of Thor's design and implementation. We focus on two areas that set Thor apart from other object-oriented databases. First, we discuss safe sharing and techniques for ensuring it; we also discuss ways of improving application performance without sacrificing safety. Second, we describe our approach to cache management at client machines, including a novel adaptive prefetching strategy.The paper presents performance results for Thor, on several OO7 benchmark traversals. The results show that adaptive prefetching is very effective, improving both the elapsed time of traversals and the amount of space used in the client cache. The results also show that the cost of safe sharing can be negligible; thus it is possible to have both safety and high performance.

#index 210196
#* An open abstract-object storage system
#@ Stephen Blott;Lukas Relly;Hans-Jörg Schek
#t 1996
#c 5
#% 13044
#% 86950
#% 122917
#% 146202
#% 172939
#% 442699
#% 445701
#% 464007
#% 481125
#% 481258
#% 481425
#% 481586
#% 526873
#% 527015
#% 554877
#% 567866
#% 614579
#! Database systems must become more open to retain their relevance as a technology of choice and necessity. Openness implies not only databases exporting their data, but also exporting their services. This is as true in classical application areas as in non-classical (GIS, multimedia, design, etc).This paper addresses the problem of exporting storage-management services of indexing, replication and basic query processing. We describe an abstract-object storage model which provides the basic mechanism, 'likeness', through which these services are applied uniformly to internally-stored, internally-defined data, and to externally-stored, externally-defined data. Managing external data requires the coupling of external operations to the database system. We discuss the interfaces and protocols required of these to achieve correct resource management and admit efficient realisation. Throughout, we demonstrate our solutions in the area of semi-structured file management; in our case, geospatial metadata files.

#index 210197
#* Static detection of security flaws in object-oriented databases
#@ Keishi Tajima
#t 1996
#c 5
#% 18521
#% 32907
#% 43189
#% 51451
#% 69537
#% 91075
#% 102749
#% 102780
#% 151148
#% 164390
#% 287297
#% 287298
#% 287462
#% 287711
#% 287795
#% 320319
#% 320629
#% 408140
#% 442782
#% 458525
#% 462199
#% 462348
#% 462624
#% 463111
#% 480958
#% 481124
#% 664495
#% 664503
#! Access control in function granularity is one of the features of many object-oriented databases. In those systems, the users are granted rights to invoke composed functions instead of rights to invoke primitive operations. Although primitive operations are invoked inside composed functions, the users can invoke them only through the granted functions. This achieves access control in abstract operation level. Access control utilizing encapsulated functions, however, easily causes many "security flaws" through which malicious users can bypass the encapsulation and can abuse the primitive operations inside the functions. In this paper, we develop a technique to statically detect such security flaws. First, we design a framework to describe security requirements that should be satisfied. Then, we develop an algorithm that syntactically analyzes program code of the functions and determines whether given security requirements are satisfied or not. This algorithm is sound, that is, whenever there is a security flaw, it detects it.

#index 210198
#* Goal-oriented buffer management revisited
#@ Kurt P. Brown;Michael J. Carey;Miron Livny
#t 1996
#c 5
#% 43171
#% 102805
#% 152943
#% 218376
#% 319473
#% 435145
#% 442131
#% 442706
#% 476489
#% 481092
#% 481127
#% 481450
#% 481459
#% 482070
#! In this paper we revisit the problem of achieving multi-class workload response time goals by automatically adjusting the buffer memory allocations of each workload class. We discuss the virtues and limitations of previous work with respect to a set of criteria we lay out for judging the success of any goal-oriented resource allocation algorithm. We then introduce the concept of hit rate concavity and develop a new goal-oriented buffer allocation algorithm, called Class Fencing, that is based on this concept. Exploiting the notion of hit rate concavity results in an algorithm that not only is as accurate and stable as our previous work, but also more responsive, more robust, and simpler to implement.

#index 210199
#* Multi-dimensional resource scheduling for parallel queries
#@ Minos N. Garofalakis;Yannis E. Ioannidis
#t 1996
#c 5
#% 26729
#% 66135
#% 115661
#% 115962
#% 116040
#% 116041
#% 126779
#% 152915
#% 156014
#% 172907
#% 191608
#% 201885
#% 411554
#% 442065
#% 442700
#% 444296
#% 463108
#% 480943
#% 480954
#% 481131
#% 481289
#% 481617
#% 481753
#% 604671
#% 697172
#! Scheduling query execution plans is an important component of query optimization in parallel database systems. The problem is particularly complex in a shared-nothing execution environment, where each system node represents a collection of time-shareable resources (e.g., CPU(s), disk(s), etc.) and communicates with other nodes only by message-passing. Significant research effort has concentrated on only a subset of the various forms of intra-query parallelism so that scheduling and synchronization is simplified. In addition, most previous work has focused its attention on one-dimensional models of parallel query scheduling, effectively ignoring the potential benefits of resource sharing. In this paper, we develop an approach that is more general in both directions, capturing all forms of intra-query parallelism and exploiting sharing of multi-dimensional resource nodes among concurrent plan operators. This allows scheduling a set of independent query tasks (i.e., operator pipelines) to be seen as an instance of the multi-dimensional bin-design problem. Using a novel quantification of coarse grain parallelism, we present a list scheduling heuristic algorithm that is provably near-optimal in the class of coarse grain parallel executions (with a worst-case performance ratio that depends on the number of resources per node and the granularity parameter). We then extend this algorithm to handle the operator precedence constraints in a bushy query plan by splitting the execution of the plan into synchronized phases. Preliminary performance results confirm the effectiveness of our scheduling algorithm compared both to previous approaches and the optimal solution. Finally, we present a technique that allows us to relax the coarse granularity restriction and obtain a list scheduling method that is provably near-optimal in the space of all possible parallel schedules.

#index 210201
#* Semi-automatic, self-adaptive control of garbage collection rates in object databases
#@ Jonathan E. Cook;Artur W. Klauser;Alexander L. Wolf;Benjamin G. Zorn
#t 1996
#c 5
#% 32912
#% 58343
#% 152904
#% 152930
#% 172938
#% 172948
#% 322978
#% 340620
#% 395735
#% 463741
#% 473366
#% 481756
#% 753876
#! A fundamental problem in automating object database storage reclamation is determining how often to perform garbage collection. We show that the choice of collection rate can have a significant impact on application performance and that the "best" rate depends on the dynamic behavior of the application, tempered by the particular performance goals of the user. We describe two semi-automatic, self-adaptive policies for controlling collection rate that we have developed to address the problem. Using trace-driven simulations, we evaluate the performance of the policies on a test database application that demonstrates two distinct reclustering behaviors. Our results show that the policies are effective at achieving user-specified levels of I/O operations and database garbage percentage. We also investigate the sensitivity of the policies over a range of object connectivities. The evaluation demonstrates that semi-automatic, self-adaptive policies are a practical means for flexibly controlling garbage collection rate.

#index 210202
#* Towards effective and efficient free space management
#@ Mark L. McAuliffe;Michael J. Carey;Marvin H. Solomon
#t 1996
#c 5
#% 82858
#% 154310
#% 169822
#% 172939
#% 174220
#% 234905
#% 364329
#% 442706
#% 466944
#! An important problem faced by many database management systems is the "online object placement problem"--the problem of choosing a disk page to hold a newly allocated object. In the absence of clustering criteria, the goal is to maximize storage utilization. For main-memory based systems, simple heuristics exist that provide reasonable space utilization in the worst case and excellent utilization in typical cases. However, the storage management problem for databases includes significant additional challenges, such as minimizing I/O traffic, coping with crash recovery, and gracefully integrating space management with locking and logging.We survey several object placement algorithms, including techniques that can be found in commercial and research database systems. We then present a new object placement algorithm that we have designed for use in Shore, an object-oriented database system under development at the University of Wisconsin--Madison. Finally, we present results from a series of experiments involving actual Shore implementations of some of these algorithms. Our results show that while current object placement algorithms have serious performance deficiencies, including excessive CPU or main memory overhead, I/O traffic, or poor disk utilization, our new algorithm consistently excellent performance in all of these areas.

#index 210203
#* Rule languages and internal algebras for rule-based optimizers
#@ Mitch Cherniack;Stanley B. Zdonik
#t 1996
#c 5
#% 497
#% 2899
#% 3888
#% 5379
#% 32878
#% 43162
#% 58377
#% 77931
#% 82858
#% 90641
#% 99436
#% 102763
#% 120690
#% 135438
#% 162885
#% 174398
#% 201874
#% 287005
#% 288366
#% 320200
#% 395735
#% 411557
#% 462643
#% 464540
#% 480091
#% 481121
#% 562122
#% 562135
#% 564426
#! Rule-based optimizers and optimizer generators use rules to specify query transformations. Rules act directly on query representations, which typically are based on query algebras. But most algebras complicate rule formulation, and rules over these algebras must often resort to calling to externally defined bodies of code. Code makes rules difficult to formulate, prove correct and reason about, and therefore compromises the effectiveness of rule-based systems.In this paper we present KOLA: a combinator-based algebra designed to simplify rule formulation. KOLA is not a user language, and KOLA's variable-free queries are difficult for humans to read. But KOLA is an effective internal algebra because its combinator-style makes queries manipulable and structurally revealing. As a result, rules over KOLA queries are easily expressed without the need for supplemental code. We illustrate this point, first by showing some transformations that despite their simplicity, require head and body routines when expressed over algebras that include variables. We show that these transformations are expressible without supplemental routines in KOLA. We then show complex transformations of a class of nested queries expressed over KOLA. Nested query optimization, while having been studied before, have seriously challenged the rule-based paradigm.

#index 210205
#* Evaluating queries with generalized path expressions
#@ Vassilis Christophides;Sophie Cluet;Guido Moerkotte
#t 1996
#c 5
#% 32889
#% 36188
#% 43162
#% 58375
#% 58377
#% 77656
#% 86954
#% 116091
#% 152942
#% 172927
#% 459260
#% 481121
#! In the past few years, query languages featuring generalized path expressions have been proposed. These languages allow the interrogation of both data and structure. They are powerful and essential for a number of applications. However, until now, their evaluation has relied on a rather naive and inefficient algorithm.In this paper, we extend an object algebra with two new operators and present some interesting rewriting techniques for queries featuring generalized path expressions. We also show how a query optimizer can integrate the new techniques.

#index 210206
#* Query execution techniques for caching expensive methods
#@ Joseph M. Hellerstein;Jeffrey F. Naughton
#t 1996
#c 5
#% 36117
#% 77944
#% 86943
#% 136740
#% 152940
#% 172930
#% 172931
#% 210207
#% 277347
#% 286189
#% 411554
#% 427195
#% 461897
#% 479753
#% 480272
#% 480286
#% 481621
#% 481749
#% 510676
#% 674940
#! Object-Relational and Object-Oriented DBMSs allow users to invoke time-consuming ("expensive") methods in their queries. When queries containing these expensive methods are run on data with duplicate values, time is wasted redundantly computing methods on the same value. This problem has been studied in the context of programming languages, where "memoization" is the standard solution. In the database literature, sorting has been proposed to deal with this problem. We compare these approaches along with a third solution, a variant of unary hybrid hashing which we call Hybrid Cache. We demonstrate that Hybrid Cache always dominates memoization, and significantly outperforms sorting in many instances. This provides new insights into the tradeoff between hashing and sorting for unary operations. Additionally, our Hybrid Cache algorithm includes some new optimization for unary hybrid hashing, which can be used for other applications such as grouping and duplicate elimination. We conclude with a discussion of techniques for caching multiple expensive methods in a single query, and raise some new optimization problems in choosing caching techniques.

#index 210207
#* Cost-based optimization for magic: algebra and implementation
#@ Praveen Seshadri;Joseph M. Hellerstein;Hamid Pirahesh;T. Y. Cliff Leung;Raghu Ramakrishnan;Divesh Srivastava;Peter J. Stuckey;S. Sudarshan
#t 1996
#c 5
#% 554
#% 1932
#% 11797
#% 17084
#% 86943
#% 101623
#% 109357
#% 116040
#% 140389
#% 145196
#% 164365
#% 172889
#% 215948
#% 286916
#% 289370
#% 320113
#% 322884
#% 411554
#% 416029
#% 435130
#% 442706
#% 462320
#% 479938
#% 480955
#% 565457
#! Magic sets rewriting is a well-known optimization heuristic for complex decision-support queries. There can be many variants of this rewriting even for a single query, which differ greatly in execution performance. We propose cost-based techniques for selecting an efficient variant from the many choices.Our first contribution is a practical scheme that models magic sets rewriting as a special join method that can be added to any cost-based query optimizer. We derive cost formulas that allow an optimizer to choose the best variant of the rewriting and to decide whether it is beneficial. The order of complexity of the optimization process is preserved by limiting the search space in a reasonable manner. We have implemented this technique in IBM's DB2 C/S V2 database system. Our performance measurements demonstrate that the cost-based magic optimization technique performs well, and that without it, several poor decisions could be made.Our second contribution is a formal algebraic model of magic sets rewriting, based on an extension of the multiset relational algebra, which cleanly defines the search space and can be used in a rule-based optimizer. We introduce the multiset θ-semijoin operator, and derive equivalence rules involving this operator. We demonstrate that magic sets rewriting for non-recursive SQL queries can be modeled as a sequential composition of these equivalence rules.

#index 210208
#* Materialized view maintenance and integrity constraint checking: trading space for time
#@ Kenneth A. Ross;Divesh Srivastava;S. Sudarshan
#t 1996
#c 5
#% 13016
#% 36117
#% 101623
#% 116044
#% 123589
#% 147843
#% 152928
#% 198465
#% 201898
#% 201929
#% 442663
#% 442767
#% 462645
#% 463117
#% 463735
#% 464056
#% 480623
#% 481132
#% 481604
#% 565457
#! We investigate the problem of incremental maintenance of an SQL view in the face of database updates, and show that it is possible to reduce the total time cost of view maintenance by materializing (and maintaining) additional views. We formulate the problem of determining the optimal set of additional views to materialize as an optimization problem over the space of possible view sets (which includes the empty set). The optimization problem is harder than query optimization since it has to deal with multiple view sets, updates of multiple relations, and multiple ways of maintaining each view set for each updated relation.We develop a memoing solution for the problem; the solution can be implemented using the expression DAG representation used in rule-based optimizers such as Volcano. We demonstrate that global optimization cannot, in general, be achieved by locally optimizing each materialized subview, because common subexpressions between different materialized subviews can allow nonoptimal local plans to be combined into an optimal global plan. We identify conditions on materialized subviews in the expression DAG when local optimization is possible. Finally, we suggest heuristics that can be used to efficiently determine a useful set of additional views to materialize.Our results are particularly important for the efficient checking of assertions (complex integrity constraints) in the SQL-92 standard, since the incremental checking of such integrity constraints is known to be essentially equivalent to the view maintenance problem.

#index 210209
#* Maintaining database consistency in presence of value dependencies in multidatabase systems
#@ Claire Morpain;Michéle Cart;Jean Ferrié;Jean-François Pons
#t 1996
#c 5
#% 9241
#% 77982
#% 340666
#% 435104
#% 463101
#% 679387
#! The emergence of new criteria specifically adapted to multidatabase systems, in response to constraints imposed by global serializability, leads to restrictive hypotheses in order to ensure correctness of executions. This is the case with the two level serializability presented in [6], that ensures strongly correct executions if transaction programs are Local Database Preserving (LDP). The main drawback of the LDP hypothesis is that it relies on rigorous programming. The principal objective of this paper has been to suppress this drawback while conserving the strong correctness of 2LSR executions We propose defining precisely the notion of value dependencies, and managing them so as not to impose the LDP property.

#index 210210
#* Algorithms for deferred view maintenance
#@ Latha S. Colby;Timothy Griffin;Leonid Libkin;Inderpal Singh Mumick;Howard Trickey
#t 1996
#c 5
#% 6798
#% 13015
#% 13016
#% 32914
#% 137866
#% 152928
#% 201922
#% 201928
#% 201929
#% 209729
#% 277332
#% 287672
#% 370758
#% 427218
#% 442663
#% 442767
#% 458556
#% 480623
#% 562129
#% 993493
#! Materialized views and view maintenance are important for data warehouses, retailing, banking, and billing applications. We consider two related view maintenance problems: 1) how to maintain views after the base tables have already been modified, and 2) how to minimize the time for which the view is inaccessible during maintenance.Typically, a view is maintained immediately, as a part of the transaction that updates the base tables. Immediate maintenance imposes a significant overhead on update transactions that cannot be tolerated in many applications. In contrast, deferred maintenance allows a view to become inconsistent with its definition. A refresh operation is used to reestablish consistency. We present new algorithms to incrementally refresh a view during deferred maintenance. Our algorithms avoid a state bug that has artificially limited techniques previously used for deferred maintenance.Incremental deferred view maintenance requires auxiliary tables that contain information recorded since the last view refresh. We present three scenarios for the use of auxiliary tables and show how these impact per-transaction overhead and view refresh time. Each scenario is described by an invariant that is required to hold in all database states. We then show that, with the proper choice of auxiliary tables, it is possible to lower both per-transaction overhead and view refresh time.

#index 210211
#* A framework for supporting data integration using the materialized and virtual approaches
#@ Richard Hull;Gang Zhou
#t 1996
#c 5
#% 13016
#% 85088
#% 85089
#% 111913
#% 116303
#% 152928
#% 174043
#% 201928
#% 201929
#% 209729
#% 210192
#% 210210
#% 213442
#% 458556
#% 480616
#% 480623
#% 702028
#% 703487
#! This paper presents a framework for data integration currently under development in the Squirrel project. The framework is based on a special class of mediators, called Squirrel integration mediators. These mediators can support the traditional virtual and materialized approaches, and also hybrids of them.In the Squirrel mediators, a relation in the integrated view can be supported as (a) fully materialized, (b) fully virtual, or (c) partially materialized (i.e., with some attributes materialized and other attributes virtual). In general, (partially) materialized relations of the integrated view are maintained by incremental updates from the source databases. Squirrel mediators provide two approaches for doing this: (1) materialize all needed auxiliary data, so that data sources do not have to be queried when processing the incremental updates; or (2) leave some or all of the auxiliary data virtual, and query selected source databases when processing incremental updates.The paper presents formal notions of consistency and "freshness" for integrated views defined over multiple autonomous source databases. It is shown that Squirrel mediators satisfy these properties.

#index 210212
#* Change detection in hierarchically structured information
#@ Sudarshan S. Chawathe;Anand Rajaraman;Hector Garcia-Molina;Jennifer Widom
#t 1996
#c 5
#% 66654
#% 84549
#% 135476
#% 201928
#% 201934
#% 394417
#% 463919
#% 481130
#! Detecting and representing changes to data is important for active databases, data warehousing, view maintenance, and version and configuration management. Most previous work in change management has dealt with flat-file and relational data; we focus on hierarchically structured data. Since in many cases changes must be computed from old and new versions of the data, we define the hierarchical change detection problem as the problem of finding a "minimum-cost edit script" that transforms one data tree to another, and we present efficient algorithms for computing such an edit script. Our algorithms make use of some key domain characteristics to achieve substantially better performance than previous, general-purpose algorithms. We study the performance of our algorithms both analytically and empirically, and we describe the application of our techniques to hierarchically structured documents.

#index 210214
#* A query language and optimization techniques for unstructured data
#@ Peter Buneman;Susan Davidson;Gerd Hillebrand;Dan Suciu
#t 1996
#c 5
#% 55349
#% 58354
#% 98958
#% 116091
#% 163444
#% 268797
#% 463919
#% 464540
#% 481614
#% 598211
#% 700905
#! A new kind of data model has recently emerged in which the database is not constrained by a conventional schema. Systems like ACeDB, which has become very popular with biologists, and the recent Tsimmis proposal for data integration organize data in tree-like structures whose components can be used equally well to represent sets and tuples. Such structures allow great flexibility y in data representation.What query language is appropriate for such structures? Here we propose a simple language UnQL for querying data organized as a rooted, edge-labeled graph. In this model, relational data may be represented as fixed-depth trees, and on such trees UnQL is equivalent to the relational algebra. The novelty of UnQL consists in its programming constructs for arbitrarily deep data and for cyclic structures. While strictly more powerful than query languages with path expressions like XSQL, UnQL can still be efficiently evaluated. We describe new optimization techniques for the deep or "vertical" dimension of UnQL queries. Furthermore, we show that known optimization techniques for operators on flat relations apply to the "horizontal" dimension of UnQL.

#index 210215
#* Is GUI programming a database research problem?
#@ Nita Goyal;Charles Hoch;Ravi Krishnamurthy;Brian Meckler;Michael Suckow
#t 1996
#c 5
#% 36683
#% 55408
#% 102748
#% 152929
#% 173481
#% 194610
#% 464061
#% 480285
#! Programming nontrivial GUI applications is currently an arduous task. Just as the use of a declarative language simplified the programming of database applications, we ask whether we can do the same for GUI programming? Can we then import a large body of knowledge from database research? We answer these questions by describing our experience in building nontrivial GUI applications initially using C++ programming and subsequently using Logic++, a higher order Horn clause logic language on complex objects with object-oriented features. We abstract a GUI application as a set of event handlers. Each event handler can be conceptualized as a transition from the old screen/program state to a new screen/program state. We use a data centric view of the screen/program state (i.e., every entity on the screen corresponds to proxy datum in the program) and express each event handler as a query dependent update, albeit a complicated one. To express such complicated updates we use Logic++. The proxy data are expressed as derived views that are materialized on the screen. Therefore, the system must be active in maintaining these materialized views. Consequently, each event handler is conceptually an update followed by a fixpoint computation of the proxy data. Based on our experience in building the GUI system, we observe that many database techniques such as view maintenance, active DB, concurrency control, recovery, optimization as well as language concepts such as higher order logic are useful in the context of GUI programming.

#index 210217
#* Accessing relational databases from the World Wide Web
#@ Tam Nguyen;V. Srinivasan
#t 1996
#c 5
#% 88613
#% 90659
#% 90684
#% 189966
#% 358440
#% 361523
#! With the growing popularity of the internet and the World Wide Web (Web), there is a fast growing demand for access to database management systems (DBMS) from the Web. We describe here techniques that we invented to bridge the gap between HTML, the standard markup language of the Web, and SQL, the standard query language used to access relational DBMS. We propose a flexible general purpose variable substitution mechanism that provides cross-language variable substitution between HTML input and SQL query strings as well as between SQL result rows and HTML output thus enabling the application developer to use the full capabilities of HTML for creation of query forms and reports, and SQL for queries and updates. The cross-language variable substitution mechanism has been used in the design and implementation of a system called DB2 WWW Connection that enables quick and easy construction of applications that access relational DBMS data from the Web. An end user of these DB2 WWW applications sees only the forms for his or her requests and resulting reports. A user fills out the forms, points and clicks to navigate the forms and to access the database as determined by the application.

#index 211565
#* Domains, relations and religious wars
#@ R. Camps
#t 1996
#c 5
#% 4279
#% 42401
#% 68198
#% 68199
#% 172032
#% 182903
#% 201960
#% 205246
#% 282431
#% 287333
#% 319216
#% 322880
#% 380546
#% 750960

#index 211566
#* Guidelines for presentation and comparison of indexing techniques
#@ Justin Zobel;Alistair Moffat;Kotagiri Ramamohanarao
#t 1996
#c 5
#% 321250
#! Descriptions of new indexing techniques are a common outcome of database research, but these descriptions are sometimes marred by poor methodology and a lack of comparison to other schemes. In this paper we describe a framework for presentation and comparison of indexing schemes that we believe sets a minimum standard for development and dissemination of research results in this area.

#index 211567
#* Much ado about shared-nothing
#@ Michael G. Norman;Thomas Zurek;Peter Thanisch
#t 1996
#c 5
#% 115661
#% 158047
#% 188719
#% 340668
#% 462175
#% 463427
#% 463428
#% 481099
#! In a 'shared-nothing' parallel computer, each processor has its own memory and disks and processors communicate by passing messages through an interconnect. Many academic researchers, and some vendors, assert that shared-nothingness is the 'consensus' architecture for parallel DBMSs. This alleged consensus is used as a justification for simulation models, algorithms, research prototypes and even marketing campaigns.We argue that shared-nothingness is no longer the consensus hardware architecture and that hardware resource sharing is a poor basis for categorising parallel DBMS software architectures if one wishes to compare the performance characteristics of parallel DBMS products.

#index 211568
#* On the cost of monitoring and reorganization of object bases for clustering
#@ Carsten A. Gerlhof;Alfons Kemper;Guido Moerkotte
#t 1996
#c 5
#% 54
#% 18555
#% 47621
#% 59348
#% 102746
#% 114578
#% 116057
#% 156979
#% 288865
#% 404765
#% 460666
#% 481619
#% 481622
#! Clustering is one of the most effective means to enhance the performance of object base applications. Consequently, many proposals exist for algorithms computing good object placements depending on the application profile. However, in an effective object base reorganization tool the clustering algorithm is only one constituent. In this paper, we report on our object base reorganization tool that covers all stages of reorganizing the objects: the application profile is determined by a monitoring tool, the object placement is computed from the monitored access statistics utilizing a variety of clustering algorithms and, finally, the reorganization tool restructures the object base accordingly. The costs as well as the effectiveness of these tools is quantitatively evaluated on the basis of the OO1-benchmark.

#index 211569
#* Open issues in parallel query optimization
#@ Waqar Hasan;Daniela Florescu;Patrick Valduriez
#t 1996
#c 5
#% 58375
#% 58377
#% 83933
#% 86929
#% 115661
#% 116040
#% 130419
#% 158047
#% 159237
#% 188692
#% 188719
#% 191608
#% 197202
#% 198068
#% 217052
#% 340597
#% 340663
#% 340664
#% 442698
#% 442700
#% 480761
#% 480960
#% 480966
#% 481289
#% 481597
#% 481784
#% 499803
#% 569230
#! We provide an overview of query processing in parallel database systems and discuss several open issues in the optimization of queries for parallel machines.

#index 211570
#* Control strategies for complex relational query processing in shared nothing systems
#@ Lionel Brunie;Harald Kosch
#t 1996
#c 5
#% 102784
#% 114577
#% 116041
#% 158047
#% 159237
#% 166189
#% 177450
#% 318049
#% 340664
#% 463095
#% 480595
#% 480966
#% 481289
#% 481617
#% 481753
#% 530879
#! In this paper, we present an original and complete methodology for supervising relational query processing in shared nothing systems. A new control mechanism is introduced which allows the detection and the correction of optimizer estimation errors and load imbalance. We especially focus on the management of intraprocessor communication and on the overlapping of communication and computation. Performance evaluations on an hypercube and a grid interconnection machine show the efficiency and the robustness of the proposed methods.

#index 211571
#* The active database management system manifesto: a rulebase of ADBMS features
#@ Corporate Act-Net Consortium
#t 1996
#c 5
#% 1486
#% 53706
#% 116045
#% 394417
#% 480620
#% 480771
#% 481773
#% 681395
#! Active database systems have been a hot research topic for quite some years now. However, while “active functionality” has been claimed for many systems, and notions such as “active objects” or “events” are used in many research areas (even beyond database technology), it is not yet clear which functionality a database management system must support in order to be legitimately considered as an active system. In this paper, we attempt to clarify the notion of “active database management system” as well as the functionality it has to support. We thereby distinguish mandatory features that are needed to qualify as an active database system, and desired features which are nice to have. Finally, we perform a classification of applications of active database systems and identify the requirements for an active database management system in order to be applicable in these application areas.

#index 211572
#* Report on first international workshop on real-time database systems
#@ Azer Bestavros;Kwei-Jay Lin;Sang Son
#t 1996
#c 5

#index 211573
#* BeSS object storage manager: architecture overview
#@ Alexandros Biliris;Euthimios Panagos
#t 1996
#c 5
#% 29590
#% 111351
#% 114582
#% 116073
#% 172939
#% 172943
#% 201895
#% 210170
#% 480617
#% 481091
#% 565455
#! BeSS is a high performance, memory-mapped object storage manager offering distributed transaction management facilities and extensible support for persistence. In this paper, we present an overview of the peer-to-peer architecture of BeSS, and we discuss issues related to space management, inter-object references, database corruption, operation modes, cache replacement, and transaction management.

#index 211574
#* Middle East Technical University Software Research and Development Center
#@ Asuman Dogac
#t 1996
#c 5
#% 149617
#% 191170
#% 208383
#% 214683
#% 252369
#% 563732
#% 563746
#% 614600
#! Middle East Technical University (METU) is the leading technical university in Turkey. The Software Research and Development Center was established by the Scientific and Technical Research Council of Turkey (TUBITAK) at the Department of Computer Engineering of METU in October 1991. The aim of this center is twofold: to lead large scale software research and development projects, and to foster international cooperation. SRDC is involved in a number of research and development projects supported by the government, industrial companies and international organizations. Although SRDC projects also cover other fields of computer science, the main emphasis is on database systems.SRDC is organized around the ongoing projects and several engineers and graduate students work in these projects: M. Altinel, B. Arpinar, I. Cingil, Y. Ceken, C. Dengi, E. Gokkoca, C. Evrendilek, P. Karagoz, E. Kilic, P. Koksal, S. Mancuhan, S. Nural, F. Ozcan, G. Ozhan, V. Sadjadi, N. Tatbul. Its infrastructure includes a LAN with Sun Workstations and PCs and commercial software like Oracle7, Sybase, Informix, Adabas D, Ingres and DEC's ObjectBroker. SRDC is a beta test site for several products including SunSoft's Joe and Concerto and Orbix's Object Transaction Service. The remainder of this report describes the main projects.

#index 211575
#* OLAP, relational, and multidimensional database systems
#@ George Colliat
#t 1996
#c 5
#% 134518
#% 156787
#% 164631

#index 211576
#* UniSQL's next-generation object-relational database management system
#@ Albert D'Andrea;Phil Janus
#t 1996
#c 5
#! Object-Relational DBMSs have been receiving a great deal of attention from industry analysts and press as the next generation of database management systems. The motivation for a next generation DBMS is driven by the reality of shortened business cycles. This dynamic environment demands fast, cost-effective, time-to-market of new or modified business processes, services, and products. To support this important business need, the next generation DBMS must: 1. leverage the large investments made in existing relational technology, both in data and skill set; 2. Take advantage of the flexibility, productivity, and performance benefits of OO modeling; and 3. Integrate robust DBMS services for production quality systems. The objective of this article is to provide a brief overview of UniSQL's commercial object-relational database management system.

#index 211577
#* Scientists called upon to take actions
#@ Xiaolei Qian
#t 1996
#c 5
#! Once again, scientists were called upon to take greater role in the political process. And this time, they did! We report on the continuous debate on the nation's R&D policy. We also cover funding opportunities from DoD and NSF.

#index 213944
#* Proceedings of the fifteenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems
#@ Richard Hull
#t 1996
#c 5

#index 213949
#* Accessing information from globally distributed knowledge repositories (extended abstract)
#@ Alfred V. Aho
#t 1996
#c 5
#% 101925
#% 374001

#index 213950
#* Relational expressive power of constraint query languages
#@ Michael Benedikt;Guozhu Dong;Leonid Libkin;Limsoon Wong
#t 1996
#c 5
#% 99443
#% 137864
#% 159506
#% 164406
#% 164418
#% 173860
#% 190332
#% 191584
#% 213952
#% 384978
#% 399235
#% 477218
#% 534163
#% 564945
#% 587330
#% 587366

#index 213952
#* Linear vs. order constraint queries over rational databases (extended abstract)
#@ Alexei P. Stolboushkin;Michael A. Taitslin
#t 1996
#c 5
#% 164418
#% 190332
#% 191590
#% 191614
#% 213950
#% 268788
#% 476994
#% 587330
#% 587366
#% 650185

#index 213955
#* Towards practical constraint databases (extended abstract)
#@ Stéphane Grumbach;Jianwen Su
#t 1996
#c 5
#% 771
#% 6249
#% 42430
#% 46022
#% 46033
#% 117448
#% 126329
#% 132159
#% 132779
#% 137893
#% 164360
#% 164406
#% 164418
#% 191590
#% 191592
#% 213950
#% 243299
#% 268788
#% 294169
#% 476994
#% 477218
#% 505561
#% 534163
#% 552988
#% 587330

#index 213956
#* Variable independence and aggregation closure
#@ J. Chomicki;D. Q. Goldin;G. M. Kuper
#t 1996
#c 5
#% 3506
#% 27056
#% 73520
#% 83144
#% 101956
#% 123068
#% 123070
#% 152595
#% 164406
#% 169238
#% 190332
#% 191584
#% 191586
#% 191592
#% 201872
#% 268787
#% 288654
#% 289370
#% 467630
#% 476994
#% 481113
#% 526843
#% 527014
#% 534310

#index 213957
#* Temporal versus first-order logic to query temporal databases
#@ Serge Abiteboul;Laurent Herr;Jan Van den Bussche
#t 1996
#c 5
#% 83108
#% 101955
#% 168262
#% 384978
#% 458992
#% 467630
#% 480786
#% 527795
#% 567884
#% 600560

#index 213959
#* Point vs. interval-based query languages for temporal databases (extended abstract)
#@ David Toman
#t 1996
#c 5
#% 18615
#% 36683
#% 91617
#% 108512
#% 135384
#% 213956
#% 268788
#% 319244
#% 361445
#% 384978
#% 458554
#% 458992
#% 467630

#index 213963
#* Testing complex temporal relationships involving multiple granularities and its application to data mining (extended abstract)
#@ Claudio Bettini;X. Sean Wang;Sushil Jajodia
#t 1996
#c 5
#% 107137
#% 162493
#% 225003
#% 408396
#% 408638
#% 420063
#% 452821
#% 463738
#% 542940

#index 213966
#* Topological queries in spatial databases
#@ C. H. Papadimitriou;D. Suciu;V. Vianu
#t 1996
#c 5
#% 24110
#% 95173
#% 100163
#% 164406
#% 164418
#% 172893
#% 189871
#% 191590
#% 213950
#% 268788
#% 464552
#% 476994
#% 527008
#% 587330
#% 604690
#% 1275343

#index 213969
#* Tables as a paradigm for querying and restructuring (extended abstract)
#@ Marc Gyssens;Laks V. S. Lakshmanan;Iyer N. Subramanian
#t 1996
#c 5
#% 58356
#% 207565
#% 268799
#% 384978
#% 395221

#index 213970
#* On genericity and parametricity (extended abstract)
#@ Catriel Beeri;Tova Milo;Paula Ta-Shma
#t 1996
#c 5
#% 319
#% 87562
#% 101947
#% 120671
#% 137912
#% 164382
#% 164406
#% 399066
#% 399235
#% 435157
#% 529575

#index 213971
#* Verifiable properties of database transactions
#@ Michael Benedikt;Timothy Griffin;Leonid Libkin
#t 1996
#c 5
#% 28120
#% 32908
#% 51400
#% 59349
#% 84343
#% 98462
#% 98958
#% 100591
#% 101646
#% 102547
#% 109838
#% 145167
#% 164376
#% 188350
#% 196696
#% 213950
#% 286249
#% 291885
#% 321426
#% 384051
#% 384978
#% 462040
#% 562129
#% 616281

#index 213972
#* Chasing constrained tuple-generating dependencies
#@ Michael J. Maher;Divesh Srivastava
#t 1996
#c 5
#% 583
#% 35562
#% 53390
#% 53400
#% 64420
#% 101623
#% 101945
#% 146277
#% 190332
#% 268764
#% 287793
#% 384978
#% 464701
#% 481128
#% 534177

#index 213973
#* Recovery for transaction failures in object-based databases
#@ Man Hon Wong
#t 1996
#c 5
#% 117
#% 459
#% 9241
#% 55415
#% 101644
#% 101645
#% 102755
#% 123096
#% 137927
#% 137939
#% 137940
#% 166215
#% 189874
#% 277339
#% 289399
#% 320902
#% 531907

#index 213974
#* Are window queries representative for arbitrary range queries?
#@ Bernd-Uwe Pagel;Hans-Werner Six
#t 1996
#c 5
#% 77928
#% 83319
#% 86950
#% 137887
#% 152902
#% 153260
#% 191595
#% 285932
#% 427199
#% 480093
#% 481620
#% 526845
#% 526864

#index 213975
#* A model for the prediction of R-tree performance
#@ Yannis Theodoridis;Timos Sellis
#t 1996
#c 5
#% 327
#% 1331
#% 32898
#% 32913
#% 77928
#% 86950
#% 137887
#% 153260
#% 164360
#% 191595
#% 201876
#% 201880
#% 243299
#% 252608
#% 285932
#% 286237
#% 317933
#% 427199
#% 435137
#% 467248
#% 480093
#% 481455
#% 632208

#index 213976
#* Efficient and accurate cost models for parallel query optimization (extended abstract)
#@ Sumit Ganguly;Akshay Goel;Avi Silberschatz
#t 1996
#c 5
#% 115661
#% 115962
#% 116040
#% 118673
#% 126779
#% 149860
#% 156014
#% 339717
#% 339727
#% 340663
#% 411554
#% 442700
#% 480615
#% 481104
#% 481110
#% 697172

#index 213977
#* Mining optimized association rules for numeric attributes
#@ Takeshi Fukuda;Yasuhido Morimoto;Shinichi Morishita;Takeshi Tokuyama
#t 1996
#c 5
#% 136350
#% 152934
#% 190611
#% 201894
#% 210160
#% 210162
#% 338728
#% 452821
#% 459008
#% 480940
#% 480964
#% 481100
#% 481281
#% 481290

#index 213978
#* Managing conflicts between rules (extended abstract)
#@ H. V. Jagadish;Alberto O. Mendelzon;Inderpal Singh Mumick
#t 1996
#c 5
#% 37972
#% 43209
#% 45257
#% 58345
#% 58360
#% 62021
#% 86944
#% 116043
#% 277330
#% 480620
#% 480621
#% 480765
#% 480942

#index 213979
#* Static analysis of intensional databases in U-Datalog (extended abstract)
#@ Elisa Bertino;Barbara Catania
#t 1996
#c 5
#% 23898
#% 23901
#% 35562
#% 36683
#% 77108
#% 116000
#% 122398
#% 137871
#% 140410
#% 152927
#% 169697
#% 173306
#% 175108
#% 442922
#% 463580
#% 490306

#index 213980
#* Future directions and research problems in the World Wide Web
#@ Udi Manber
#t 1996
#c 5

#index 213981
#* Combining fuzzy information from multiple systems (extended abstract)
#@ Ronald Fagin
#t 1996
#c 5
#% 238757
#% 384209
#% 614579

#index 213982
#* Answering queries using limited external query processors (extended abstract)
#@ Alon Y. Levy;Anand Rajaraman;Jeffrey D. Ullman
#t 1996
#c 5
#% 36181
#% 122398
#% 123118
#% 159113
#% 164364
#% 172874
#% 172875
#% 188853
#% 198465
#% 198466
#% 201898
#% 459241
#% 464056
#% 481128
#% 481444
#% 564419
#% 599549
#% 1499471

#index 213983
#* Integrating information by outerjoins and full disjunctions (extended abstract)
#@ Anand Rajaraman;Jeffrey D. Ullman
#t 1996
#c 5
#% 36683
#% 172933
#% 213983
#% 285926
#% 286995
#% 289350
#% 289425
#% 463919
#% 481923

#index 218598
#* Geographic database systems: issues and research needs
#@ Max J. Egenhofer
#t 1996
#c 5

#index 218599
#* In memoriam: Paris C. Kanellakis
#@ Richard Hull
#t 1996
#c 5

#index 221373
#* A response to R. Camps' article “Domains, relations and religious wars”
#@ C. J. Date
#t 1996
#c 5
#! Since it quotes extensively from writings of my own, I feel obliged to respond to the article “Domains, Relations and Religious Wars,” by R. Camps (SIGMOD Record 25, No. 3, September 1996). In that article, Camps is clearly suggesting (among other things) that my definition of the term “domain” has changed over the years. I agree, it has! But Camps goes on to say: “… considering that [Date's book An Introduction to Database Systems] was the bible [Camps' italics] where most university graduates all over the world learnt, I believe that Date can be held partly responsible for the lack of implementation of domains [in today's SQL DBMSs].”

#index 221374
#* In reply to Domains, relations and religious wars
#@ Hugh Darwen
#t 1996
#c 5

#index 221375
#* The aggregate data problem: a system for their definition and management
#@ M. Rafanelli;A. Bezenchek;L. Tininini
#t 1996
#c 5
#% 2247
#% 27056
#% 77312
#% 100610
#% 152588
#% 286236
#% 452799
#% 461861
#% 503695
#% 852959
#! In this paper we describe the fundamental components of a database management system for the definition, storage, manipulation and query of aggregate data, i.e. data which are obtained by applying statistical aggregations and statistical analysis functions over raw data. In particular, the attention has been focused on: (1) a data structure for the efficient storage and manipulation of aggregate data, called ADaS; (2) the graphical structures of the aggregate data model ADAMO for a more user-friendly definition and query of aggregate data; (3) a graphical user interface which enables a straightforward specification of the ADAMO structures; (4) a textual declarative query language to retrieve data from the aggregate database, called ADQUEL.

#index 221376
#* Information visualization
#@ Tiziana Catarci;Isabel F. Cruz
#t 1996
#c 5
#% 28144
#% 36309
#% 96288
#% 169946
#% 173311
#% 176429
#% 201991
#% 222232
#% 441058
#% 481750
#% 725439
#% 834997
#! Information visualization, an increasingly important subdiscipline within the field of Human Computer Interaction (HCI) [13], focuses on visual mechanisms designed to communicate clearly to the user the structure of information and improve on the cost of access to large data repositories. In printed form, information visualization has included the display of numerical data (e.g., bar charts, plot charts, pie charts), combinatorial relations (e.g., drawings of graphs), and geographic data (e.g., encoded maps) [1, 9, 16]. In addition to these “static” displays, computer-based systems, such as the Information Visualizer [2] and Dynamic Queries [15] have coupled powerful visualization techniques (e.g., constraints, 3D, animation) with near real-time interactivity, i.e., the ability of the system to respond quickly to the user's direct manipulation commands. Another important aspect of computer-based information systems concerns the dual communication with user and machine, which motivates the concept of visual formalism that has been introduced by Harel: “The intricate nature of a variety of computer-related systems and situations can, and in our opinion should, be represented via visual formalisms; visual because they are to be generated, comprehended, and communicated by humans; and formal, because they are to be manipulated, maintained, and analyzed by computers” [12].While historically HCI and database research were kept separate, the interests of both research communities have been converging, mainly in what concerns the topic of information visualization. In the database community, the focus on information visualization started with research in visual query languages, where the visualization of schema and/or database instances is common (for a survey, see [4]). Recently, a new generation of database systems is emerging, which tightly combine querying capabilities with visualization techniques and are information visualization systems in their own right [3, 8, 11]. Database applications that access large data repositories, such as data mining and data warehousing, and the enormous quantity of information sources on the WWW available to users with diverse capabilities also provide HCI researchers with new opportunities for information visualization (see, for instance, the ACM report on Strategic Directions in HCI [13], the reports of the “FADIVA” Working Group [10], and the work presented in [14]).The objective of this special issue is two-fold: to compile some of the most recent research on information visualization from both communities, and to make it available to a large readership whose main interests lie in the management of data. The eight papers in this issue cover fundamental topics in information visualization, including tailorable multi-visualizations (i.e., the ability of the system to provide the user with alternative visualizations, depending on their suitability to different data, tasks, and users' preferences); near real-time interactivity when dealing with very large data sizes; effective display and color usage, and multidimensionality. A short overview of each paper follows.

#index 221377
#* Dynamic information visualization
#@ Yannis E. Ioannidis
#t 1996
#c 5
#% 172811
#% 173315
#% 221378
#% 285932
#% 441058
#! Dynamic queries constitute a very powerful mechanism for information visualization; some universe of data is visualized, and this visualization is modified on-the-fly as users modify the range of interest within the domains of the various attributes of the visualized information. In this paper, we analyze dynamic queries and offer some natural generalizations of the original concept by establishing a connection to SQL. We also discuss some implementation ideas that should make these generalizations efficient as well.

#index 221378
#* Incremental data structures and algorithms for dynamic query interfaces
#@ Egemen Tanin;Richard Beigel;Ben Shneiderman
#t 1996
#c 5
#% 118773
#% 172811
#% 441058
#! Dynamic query interfaces (DQIs) form a recently developed method of database access that provides continuous realtime feedback to the user during the query formulation process. Previous work shows that DQIs are elegant and powerful interfaces to small databases. Unfortunately, when applied to large databases, previous DQI algorithms slow to a crawl. We present a new approach to DQI algorithms that works well with large databases.

#index 221380
#* Spotfire: an information exploration environment
#@ Christopher Ahlberg
#t 1996
#c 5
#% 1260
#% 39097
#% 96288
#% 118773
#% 127860
#% 172811
#% 172825
#% 238753
#% 336429
#% 364386
#% 641057
#% 725460
#! In this paper we examine the issues involved in developing information visualisation systems and present a framework for their construction. The framework addresses the components which must be considered in providing effective visualisations. The framework is specified using a declarative object oriented language; the resulting object model may be mapped to a variety of graphical user interface development platforms. This provides general support to developers of visualisation systems. A prototype system exists which allows the investigation of alternative visualisations for a range of data sources.

#index 221382
#* A framework for information visualisation
#@ Jessie B. Kennedy;Kenneth J. Mitchell;Peter J. Barclay
#t 1996
#c 5
#% 36309
#% 68659
#% 86300
#% 137473
#% 137610
#% 169783
#% 214805
#% 436116
#% 489214
#% 619859
#% 641057
#% 725440
#% 725449
#% 725468
#% 726086
#% 1783101
#! In this paper we examine the issues involved in developing information visualisation systems and present a framework for their construction. The framework addresses the components which must be considered in providing effective visualisations. The framework is specified using a declarative object oriented language; the resulting object model may be mapped to a variety of graphical user interface development platforms. This provides general support to developers of visualisation systems. A prototype system exists which allows the investigation of alternative visualisations for a range of data sources.

#index 221383
#* Pixel-oriented database visualizations
#@ Daniel A. Keim
#t 1996
#c 5
#% 28144
#% 68659
#% 102726
#% 201975
#% 252360
#% 436116
#% 443086
#% 463742
#% 488932
#% 619523
#% 641057
#% 726032
#% 727085
#! In this paper, we provide an overview of several pixel-oriented visualization techniques which have been developed over the last years to support an effective querying and exploration of large databases. Pixel-oriented techniques use each pixel of the display to visualize one data value and therefore allow the visualization of the largest amount of data possible. The techniques may be divided into query-independent techniques which directly visualize the data (or a certain portion of it) and query-dependent techniques which visualize the relevance of the data with respect to a specific query. An example for the class of query-independent techniques is the recursive pattern technique which is based on a generic recursive scheme generalizing a wide range of pixel-oriented arrangements for visualizing large databases. Examples for the class of query-dependent techniques are the generalized spiral and circle-segments techniques, which visualize the distance with respect to a database query and arrange the most relevant data items in the center of the display.

#index 221385
#* To table or not to table: a hypertabular answer
#@ Giuseppe Santucci;Laura Tarantino
#t 1996
#c 5
#% 109496
#% 127640
#% 172811
#% 172812
#% 287631
#% 619831
#% 1112941
#! Suitable data set organizers are necessary to help users assimilating information retrieved from a database. In this paper we present (1) a general hypertextual framework for the interaction with tables, and (2) a specialization of the framework in order to present in hypertextual format the results of queries expressed in terms of a visual semantic query language.

#index 221387
#* Applying database visualization to the World Wide Web
#@ Masum Z. Hasan;Alberto O. Mendelzon;Dimitra Vista
#t 1996
#c 5
#% 56638
#% 164415
#% 176429
#% 182976
#% 186341
#% 202027
#% 202092
#% 268797
#% 340295
#% 481602
#% 614598
#% 641064
#% 641065
#% 703960
#% 725441
#! In this paper, we present visualizations of parts of the network of documents comprising the World Wide Web. We describe how we are using the Hy+ visualization system to visualize the portion of the World Wide Web explored during a browsing session. As the user browses, the web browser communicates the URL and title of each document fetched as well as all the anchors contained in the document. Hy+ displays graphically the history of the navigation and multiple views of the structure of that portion of the web.

#index 221388
#* 3D geographic network displays
#@ Kenneth C. Cox;Stephen G. Eick;Taosong He
#t 1996
#c 5
#% 68659
#% 122797
#% 137610
#% 139260
#% 173425
#% 178980
#% 211590
#% 434411
#% 536598
#% 641062
#% 641090
#% 726265
#! Many types of information may be represented as graphs or networks with the nodes corresponding to entities and the links to relationships between entities. Often there is geographical information associated with the network. The traditional way to visualize geographical networks employs node and link displays on a two-dimensional map. These displays are easily overwhelmed, and for large networks become visually cluttered and confusing. To overcome these problems we have invented five novel network views that generalize the traditional displays. Two of the views show the complete network, while the other three concentrate on a portion of a larger network defined by connectivity to a given node. Our new visual metaphors retain many of the well-known advantages of the traditional network maps, while exploiting three-dimensional graphics to address some of the fundamental problems limiting the scalability of two-dimensional displays.

#index 221390
#* Report from the NSF workshop on workflow and process automation in information systems
#@ Amit Sheth;Dimitrios Georgakopoulos;Stef M. M. Joosten;Marek Rusinkiewicz;Walt Scacchi;Jack Wileden;Alexander L. Wolf
#t 1996
#c 5
#% 380645
#! An interdisciplinary research community needs to address challenging issues raised by applying workflow management technology in information systems. This conclusion results from the NSF workshop on Workflow and Process Automation in Information Systems which was held at the State Botanical Garden of Georgia during May 8-10, 1996. The workshop brought together active researchers and practitioners from several communities, with significant representation from database and distributed systems, software process and software engineering, and computer supported cooperative work. The presentations given at the workshop are available in the form of an electronic proceedings of this workshop at http://lsdis.cs.uga.edu/activities/). This report is the joint work of selected representatives from the workshop and it documents the results of significant group discussions and exchange of ideas.

#index 221392
#* An orthogonally persistent Java
#@ M. P. Atkinson;L. Daynès;M. J. Jordan;T. Printezis;S. Spence
#t 1996
#c 5
#% 360401
#% 380441
#% 390039
#% 435151
#% 568966
#% 717827
#% 979138
#% 979140
#% 979161
#! The language Java is enjoying a rapid rise in popularity as an application programming language. For many applications an effective provision of database facilities is required. Here we report on a particular approach to providing such facilities, called “orthogonal persistence”. Persistence allows data to have lifetimes that vary from transient to (the best approximation we can achieve to) indefinite. It is orthogonal persistence if the available lifetimes are the same for all kinds of data. We aim to show that the programmer productivity gains and possible performance gains make orthogonal persistence a valuable augmentation of Java.

#index 221393
#* The Mariposa distributed database management system
#@ Jeff Sidell
#t 1996
#c 5
#% 461902
#% 463728
#% 476656
#% 571217

#index 221394
#* New standard for stored procedures in SQL
#@ Andrew Eisenberg
#t 1996
#c 5
#% 191176

#index 221395
#* TPC-D—the challenges, issues and results
#@ Ramesh Bhashyam
#t 1996
#c 5
#! This paper covers what we at NCR have learned about the TPC-D benchmark as we executed and published our first set of volume points for the Teradata Database. Areas where customers should read the Full Disclosure Report carefully are pointed out as well as the weaknesses in the benchmark relative to real customer applications. The key execution and optimization elements of the Teradata Database and the 5100 WorldMark platform that contribute to our published results are discussed.

#index 221396
#* New programs at DARPA and NSF
#@ Xiolai Qian
#t 1996
#c 5
#! We will share with readers some good news on NSF and Defense budget, and report on several interesting new programs at DARPA and NSF.

#index 223769
#* Environmental information systems
#@ Oliver Günther
#t 1997
#c 5

#index 223770
#* Integrating modelling systems for environmental management information systems
#@ David J. Abel;Kerry Taylor;Dean Kuo
#t 1997
#c 5
#% 85086
#% 155010
#% 567866
#! Special purpose modelling packages can become more accessible and more effective for decision support when integrated into a spatial information system. Integration is made difficult by differences in the models due to scope, underlying data models, and command languages. This paper extends a federated information systems design methodology and architecture by identifying parallels of the model integration problem with the database integration problem in federated database design. A schema architecture is proposed together with associated schema translation functions. The role of a problem statement, analogous to a federated database query, is defined. Our design approach is demonstrated in HYDRA, a decision support system for water quality management.

#index 223771
#* Improving access to environmental data using context information
#@ Anthony Tomasic;Eric Simon
#t 1997
#c 5
#% 631868
#! A very large number of data sources on environment, energy, and natural resources are available worldwide. Unfortunately, users usually face several problems when they want to search and use environmental information. In this paper, we analyze these problems. We describe a conceptual analysis of the four major tasks in the production of environmental data, from the technology point of view, and describe the organization of the data that results from these tasks. We then discuss the notion of metainformation and outline an architecture for environmental data systems that formally models metadata and addresses some of the major problems faced by users.

#index 223772
#* WWW-UDK: a web-based environmental meta-information system
#@ Ralf Kramer;Ralf Nicholai;Arne Koschel;Claudia Rolker;Peter Lockemann;Andree Keitel;Rudolf Legat;Konrad Tirm
#t 1997
#c 5
#% 85086
#% 116304
#% 207622
#% 224186
#% 591550
#! The environmental data catalogue Umweltdatenkatalog UDK is a standard meta-information system for environmental data for use by state authorities and the public. Technically, the UDK consists of a database together with a front-end tailored to the needs of environmental specialists. FZI's contribution has been to develop a front-end that makes the UDK database available using the tools and techniques of the World-Wide Web. Among the features of WWW-UDK are several query modes for the UDK objects and addresses, an environmental thesaurus, on-line access to some of the underlying data (e.g., databases and environmental reports), multilingual query and result forms, and an on-line help system. Currently, several installations of WWW-UDK are used in Austria and in Germany on the Internet and on Intranets. WWW-UDK can be easily integrated into a federation architecture which is based on CORBA, WWW, and Java.

#index 223773
#* UIS-management of data and services in the environmental information systems of Baden-Württemberg
#@ Wolf-Fritz Riekert;Roland Mayer-Föll;Gerlinde Wiest
#t 1997
#c 5
#! In the Environmental Information System (UIS) of Baden-Württemberg, an extensive collection of information and knowledge is accumulated in the form of data, methods, and multimedia documents on a variety of computer platforms. A service-oriented approach based on meta-information and WWW technology has been chosen to bring this treasure of information and knowledge to the workplace of every employee in the environmental administration. The guiding idea was to identify data, functionalities, and multimedia information that can be used in multiple ways and to supply them as self-contained network services. This helps to make the use of the available hardware and software resources more economical and, in addition, allows for a relatively unrestricted and easy access to the information sources offered by the UIS.

#index 223774
#* Data management for earth system science
#@ James Frew;Jeff Dozier
#t 1997
#c 5
#% 171741
#% 481586
#% 673665
#! Earth system science is a relatively recent scientific discipline that seeks a global-scale understanding of the components, interactions, and evolution of the entire Earth system. The data being collected in support of Earth system science are rapidly approaching petabytes per year. The intrinsic problems of archiving, searching, and distributing such a huge dataset are compounded by both the heterogeneity of the data, and the heterogeneous nature of Earth system science inquiry, which synthesizes models, observations, and knowledge bases from a several traditional scientific disciplines.A successful data management environment for Earth system science must provide seamless access to arbitrary subsets and combinations of both local and remote data, and must be compatible with the rich data analysis environments already deployed. We describe a prototype of such an environment, built at UCSB using database technology pioneered by the Sequoia 2000 Project. We specifically address its application to a problem that requires combining point observations with gridded satellite imagery.

#index 223775
#* Open GIS and on-line environmental libraries
#@ Kenn Gardels
#t 1997
#c 5
#! An essential component of an Environmental Information System is geographic or geospatial data coupled with geoprocessing functions. Traditional Geographic Information Systems (GIS) do not address the requirements of complex digital environmental libraries, but are now incorporating strategies for geodatabase federation, catalogs, and data mining. These strategies, however, depend on increased interoperability among diverse data stores, formats, and models. Open GIS™ is an abstration of geodata and a specification for methods on geographic features and coverages that enables compliant applications to exchange information and processing services. For EIS, Open GIS provides an architecture for selecting geodata at its most atomic level, fusing those data into structured information frameworks, analysing information using spatial operators, and viewing the results in informative, decision-supporting ways.

#index 223776
#* Mediator languages—a proposal for a standard: report of an I3/POB working group held at the University of Maryland, April 12 and 13, 1996
#@ Peter Buneman;Louiqa Raschid;Jeffrey Ullman
#t 1997
#c 5
#% 380441
#! The DARPA Intelligent Integration of Information (I3) effort is based on the assumption that systems can easily exchange data. However, as a consequence of the rapid development of research, and prototype implementations, in this area, the initial outcome of this program appears to have been to produce a new set of systems. While they can perform certain advanced information integration tasks, they cannot easily communicate with each other.With a view to understanding and solving this problem, there was a group discussion at the DARPA Intelligent Integration of Information/Persistent Object Bases (I3/POB) meeting in San Diego, in January, 1996; and a further workshop was held on this topic at the University of Maryland in April, 1996. The list of participants is in Appendix A. The idea emerging from these meeting a was not to force all systems to communicate according to specified standards, but to agree on the following:• A minimal core language, or Level 1 option, which would be a restriction of the object-oriented query language OQL, such that it will accept queries for relational databases. We recommend that all system components be able, at a minimum, to accept queries in this syntax, provided they address concepts (e.g., relations or classes, attributes or instance variables) known to that component. There must be a simple protocol to determine the schema of a system (its set of supported concepts).• A simple format for representing answers. This could also be a fragment of OQL and will be included in the core language specification.• A set of extensions, one of which could be full OQL, and would handle complex structures and abstract types (with methods). Other extensions will be needed to support rules (e.g., definitions of terms that can be shared among components), semistructured data (for self-describing objects), and shared code. A system component could support one or more of these extensions, independently, and there should be some simple protocol to determine the particular extensions that are supported.

#index 223777
#* A consumer viewpoint on "Mediator languages—a proposal for a standard"
#@ Arnon Rosenthal;Erich Hughes;Scott Renner;Len Seligman
#t 1997
#c 5

#index 223778
#* Min-max compression methods for medical image databases
#@ Kosmas Karadimitriou;John M. Tyler
#t 1997
#c 5
#% 403887
#% 656044
#% 703574
#! The volume of medical imaging data produced per year is rapidly increasing, overtaxing the capabilities of Picture Archival and Communication (PACS) systems. Image compression methods can lessen the problem by encoding digital images into more space-efficient forms. Image compression is achieved by reducing redundancy in the imaging data. Existing methods reduce redundancy in individual images. However, these methods ignore an additional source of redundancy, which is based on the common information stored in more than one image in a set of similar images. We use the term "set redundancy" to describe this type of redundancy. Medical image databases contain large sets of similar images, therefore they also contain significant amounts of set redundancy.This paper presents two methods that extract set redundancy from medical imaging data: the Min-Max Differential (MMD), and the Min-Max Predictive (MMP) methods. These methods can improve compression of standard image compression techniques for sets of medical images. Our tests compressing CT brain scans have shown an average of as much as 129% improvement for Huffman encoding, 93% for Arithmetic Coding, and 37% for Lempel-Ziv compression when they are combined with Min-Max methods. Both MMD and MMP are based on reversible operations, hence they provide lossless compression.

#index 223779
#* Converting relational to object-oriented databases
#@ Joseph Fong
#t 1997
#c 5
#% 11284
#% 53706
#% 58355
#% 116218
#% 117900
#% 191160
#% 289350
#% 496436
#% 535015
#% 535027
#% 535822
#% 564246
#% 835735
#! As object-oriented model becomes the trend of database technology, there is a need to convert relational to object-oriented database system to improve productivity and flexibility. The changeover includes schema translation, data conversion and program conversion. This paper describes a methodology for integrating schema translation and data conversion. Schema translation involves semantic reconstruction and the mapping of relational schema into object-oriented schema. Data conversion involves unloading tuples of relations into sequential files and reloading them into object-oriented classes files. The methodology preserves the constraints of the relational database by mapping the equivalent data dependencies.

#index 223780
#* Extraction of object-oriented structures from existing relational databases
#@ Shekar Ramanathan;Julia Hodges
#t 1997
#c 5
#% 116185
#% 169324
#% 186583
#% 380441
#% 412588
#% 459247
#% 534841
#! Due to the wide use of object-oriented technology in software development and the existence of many relational databases, reverse engineering of relational schemas to object-oriented schemas is gaining in interest. One of the major problems with existing approaches for this schema mapping is that they fail to take into consideration many modern relational database design alternatives (e.g., use of binary data to store multiple-valued attributes). This paper presents a schema mapping procedure that can be applied on existing relational databases without changing their schema. The procedure maps a relational schema that is at least in 2NF into an object-oriented schema by taking into consideration various types of relational database design optimizations.

#index 223781
#* An overview of data warehousing and OLAP technology
#@ Surajit Chaudhuri;Umeshwar Dayal
#t 1997
#c 5
#% 32878
#% 59350
#% 115661
#% 171746
#% 172889
#% 191154
#% 191175
#% 198465
#% 199537
#% 201928
#% 210182
#% 227861
#% 287005
#% 366617
#% 420053
#% 458550
#% 461897
#% 464056
#% 480091
#% 481288
#% 481604
#% 481608
#% 482082
#% 564419
#% 564426
#! Data warehousing and on-line analytical processing (OLAP) are essential elements of decision support, which has increasingly become a focus of the database industry. Many commercial products and services are now available, and all of the principal database management system vendors now have offerings in these areas. Decision support places some rather different requirements on database technology compared to traditional on-line transaction processing applications. This paper provides an overview of data warehousing and OLAP technologies, with an emphasis on their new requirements. We describe back end tools for extracting, cleaning and loading data into a data warehouse; multidimensional data models typical of OLAP; front end client tools for querying and data analysis; server extensions for efficient query processing; and tools for metadata management and for managing the warehouse. In addition to surveying the state of the art, this paper also identifies some promising research issues, some of which are related to problems that the database research community has worked on for years, but others are only just beginning to be addressed. This overview is based on a tutorial that the authors presented at the VLDB Conference, 1996.

#index 223782
#* Query previews for networked information systems: a case study with NASA environmental data
#@ Khoa Doan;Catherine Plaisant;Ben Shneiderman;Tom Bruns
#t 1997
#c 5
#! Formulating queries on networked information systems is laden with problems: data diversity, data complexity, network growth, varied user base, and slow network access. This paper proposes a new approach to a network query user interface which consists of two phases: query preview and query refinement. This new approach is based on dynamic queries and tight coupling, guiding users to rapidly and dynamically eliminate undesired items, reduce the data volume to a manageable size, and refine queries locally before submission over a network. A two-phase dynamic query system for NASA's Earth Observing Systems--Data Information Systems (EOSDIS) is presented. The prototype was well received by the team of scientists who evaluated the interface.

#index 223783
#* Opportunities in information management and assurance
#@ Xiaolei Qian
#t 1997
#c 5

#index 227854
#* Proceedings of the 1997 ACM SIGMOD international conference on Management of data
#@ Joan M. Peckman;Sudha Ram;Michael Franklin
#t 1997
#c 5

#index 227856
#* Fast parallel similarity search in multimedia databases
#@ Stefan Berchtold;Christian Böhm;Bernhard Braunmüller;Daniel A. Keim;Hans-Peter Kriegel
#t 1997
#c 5
#% 2115
#% 3704
#% 43179
#% 86950
#% 102772
#% 121989
#% 131061
#% 169940
#% 201876
#% 217292
#% 237187
#% 238764
#% 286962
#% 317313
#% 435141
#% 463414
#% 481956
#% 527026
#% 668747
#! Most similarity search techniques map the data objects into some high-dimensional feature space. The similarity search then corresponds to a nearest-neighbor search in the feature space which is computationally very intensive. In this paper, we present a new parallel method for fast nearest-neighbor search in high-dimensional feature spaces. The core problem of designing a parallel nearest-neighbor algorithm is to find an adequate distribution of the data onto the disks. Unfortunately, the known declustering methods to not perform well for high-dimensional nearest-neighbor search. In contrast, our method has been optimized based on the special properties of high-dimensional spaces and therefore provides a near-optimal distribution of the data items among the disks. The basic idea of our data declustering technique is to assign the buckets corresponding to different quadrants of the data space to different disks. We show that our technique - in contrast to other declustering methods - guarantees that all buckets corresponding to neighboring quadrants are assigned to different disks. We evaluate our method using large amounts of real data (up to 40 MBytes) and compare it with the best known data declustering method, the Hilbert curve. Our experiments show that our method provides an almost linear speed-up and a constant scale-up. Additionally, it outperforms the Hilbert approach by a factor of up to 5.

#index 227857
#* Similarity-based queries for time series data
#@ Davood Rafiei;Alberto Mendelzon
#t 1997
#c 5
#% 67552
#% 86950
#% 102772
#% 172949
#% 191581
#% 201876
#% 427199
#% 460862
#% 480952
#% 481609
#% 481611
#% 534183
#! We study a set of linear transformations on the Fourier series representation of a sequence that can be used as the basis for similarity queries on time-series data. We show that our set of transformations is rich enough to formulate operations such as moving average and time warping. We present a query processing algorithm that uses the underlying R-tree index of a multidimensional data set to answer similarity queries efficiently. Our experiments show that the performance of this algorithm is competitive to that of processing ordinary (exact match) queries using the index, and much faster than sequential scanning. We relate our transformations to the general framework for similarity queries of Jagadish et al.

#index 227859
#* Meaningful change detection in structured data
#@ Sudarshan S. Chawathe;Hector Garcia-Molina
#t 1997
#c 5
#% 25998
#% 66654
#% 84549
#% 84755
#% 210212
#% 288885
#% 481931
#% 599921
#! Detecting changes by comparing data snapshots is an important requirement for difference queries, active databases, and version and configuration management. In this paper we focus on detecting meaningful changes in hierarchically structured data, such as nested-object data. This problem is much more challenging than the corresponding one for relational or flat-file data. In order to describe changes better, we base our work not just on the traditional “atomic” insert, delete, update operations, but also on operations that move an entire sub-tree of nodes, and that copy an entire sub-tree. These operations allows us to describe changes in a semantically more meaningful way. Since this change detection problem is NP-hard, in this paper we present a heuristic change detection algorithm that yields close to “minimal” descriptions of the changes, and that has fewer restrictions than previous algorithms. Our algorithm is based on transforming the change detection problem to a problem of computing a minimum-cost edge cover of a bipartite graph. We study the quality of the solution produced by our algorithm, as well as the running time, both analytically and experimentally.

#index 227861
#* Improved query performance with variant indexes
#@ Patrick O'Neil;Dallan Quass
#t 1997
#c 5
#% 32910
#% 159227
#% 191154
#% 201951
#% 210182
#% 252458
#% 317933
#% 464215
#% 466953
#! The read-mostly environment of data warehousing makes it possible to use more complex indexes to speed up queries than in situations where concurrent updates are present. The current paper presents a short review of current indexing technology, including row-set representation by Bitmaps, and then introduces two approaches we call Bit-Sliced indexing and Projection indexing. A Projection index materializes all values of a column in RID order, and a Bit-Sliced index essentially takes an orthogonal bit-by-bit view of the same data. While some of these concepts started with the MODEL 204 product, and both Bit-Sliced and Projection indexing are now fully realized in Sybase IQ, this is the first rigorous examination of such indexing capabilities in the literature. We compare algorithms that become feasible with these variant index types against algorithms using more conventional indexes. The analysis demonstrates important performance advantages for variant indexes in some types of SQL aggregation, predicate evaluation, and grouping. The paper concludes by introducing a new method whereby multi-dimensional group-by queries, reminiscent of OLAP/Datacube queries but with more flexibility, can be very efficiently performed.

#index 227862
#* Highly concurrent cache consistency for indices in client-server database systems
#@ Markos Zaharioudakis;Michael J. Carey
#t 1997
#c 5
#% 6716
#% 29590
#% 36118
#% 83127
#% 83183
#% 102802
#% 102803
#% 111351
#% 172939
#% 250475
#% 286929
#% 464066
#% 480604
#! In this paper, we present four approaches to providing highly concurrent B+-tree indices in the context of a data-shipping, client-server OODBMS architecture. The first performs all index operations at the server, while the other approaches support varying degrees of client caching and usage of index pages. We have implemented the four approaches, as well as the 2PL approach, in the context of the SHORE OODB system at Wisconsin, and we present experimental results from a performance study based on running SHORE on an IBM SP2 multicomputer. Our results emphasize the need for non-2PL approaches and demonstrate the tradeoffs between 2PL, no-caching, and the three caching alternatives.

#index 227864
#* Concurrency and recovery in generalized search trees
#@ Marcel Kornacker;C. Mohan;Joseph M. Hellerstein
#t 1997
#c 5
#% 6716
#% 36118
#% 57955
#% 83183
#% 83188
#% 88056
#% 102808
#% 114582
#% 116085
#% 116087
#% 135557
#% 194942
#% 286929
#% 287797
#% 320902
#% 403195
#% 427199
#% 435141
#% 480093
#% 481256
#% 481599
#% 481759
#% 531907
#% 565447
#! This paper presents general algorithms for concurrency control in tree-based access methods as well as a recovery protocol and a mechanism for ensuring repeatable read. The algorithms are developed in the context of the Generalized Search Tree (GiST) data structure, an index structure supporting an extensible set of queries and data types. Although developed in a GiST context, the algorithms are generally applicable to many tree-based access methods. The concurrency control protocol is based on an extension of the link technique originally developed for B-trees, and completely avoids holding node locks during I/Os. Repeatable read isolation is achieved with a novel combination of predicate locks and two-phase locking of data records. To our knowledge, this is the first time that isolation issues have been addressed outside the context of B-trees. A discussion of the fundamental structural differences between B-trees and more general tree structures like GiSTs explains why the algorithms developed here deviate from their B-tree counterparts. An implementation of GiSTs emulating B-trees in DB2/Common Server is underway.

#index 227866
#* Range queries in OLAP data cubes
#@ Ching-Tien Ho;Rakesh Agrawal;Nimrod Megiddo;Ramakrishnan Srikant
#t 1997
#c 5
#% 672
#% 1731
#% 17858
#% 36672
#% 64532
#% 68091
#% 69474
#% 86950
#% 113841
#% 210182
#% 211575
#% 237202
#% 317933
#% 317950
#% 319601
#% 442685
#% 442695
#% 461921
#% 462204
#% 464215
#% 481288
#% 481604
#% 481608
#% 481945
#% 481948
#% 481951
#! A range query applies an aggregation operation over all selected cells of an OLAP data cube where the selection is specified by providing ranges of values for numeric dimensions. We present fast algorithms for range queries for two types of aggregation operations: SUM and MAX. These two operations cover techniques required for most popular aggregation operations, such as those supported by SQL.For range-sum queries, the essential idea is to precompute some auxiliary information (prefix sums) that is used to answer ad hoc queries at run-time. By maintaining auxiliary information which is of the same size as the data cube, all range queries for a given cube can be answered in constant time, irrespective of the size of the sub-cube circumscribed by a query. Alternatively, one can keep auxiliary information which is 1/bd of the size of the d-dimensional data cube. Response to a range query may now require access to some cells of the data cube in addition to the access to the auxiliary information, but the overall time complexity is typically reduced significantly. We also discuss how the precomputed information is incrementally updated by batching updates to the data cube. Finally, we present algorithms for choosing the subset of the data cube dimensions for which the auxiliary information is computed and the blocking factor to use for each such subset.Our approach to answering range-max queries is based on precomputed max over balanced hierarchical tree structures. We use a branch-and-bound-like procedure to speed up the finding of max in a region. We also show that with a branch-and-bound procedure, the average-case complexity is much smaller than the worst-case complexity.

#index 227868
#* Cubetree: organization of and bulk incremental updates on the data cube
#@ Nick Roussopoulos;Yannis Kotidis;Mema Roussopoulos
#t 1997
#c 5
#% 172911
#% 172913
#% 210182
#% 286237
#% 286991
#% 365700
#% 427199
#% 462204
#% 464215
#% 481948
#% 481951
#! The data cube is an aggregate operator which has been shown to be very powerful for On Line Analytical Processing (OLAP) in the context of data warehousing. It is, however, very expensive to compute, access, and maintain. In this paper we define the “cubetree” as a storage abstraction of the cube and realize in using packed R-trees for most efficient cube queries. We then reduce the problem of creation and maintenance of the cube to sorting and bulk incremental merge-packing of cubetrees. This merge-pack has been implemented to use separate storage for writing the updated cubetrees, therefore allowing cube queries to continue even during maintenance. Finally, we characterize the size of the delta increment for achieving good bulk update schedules for the cube. The paper includes experiments with various data sets measuring query and bulk update performance.

#index 227869
#* Maintenance of data cubes and summary tables in a warehouse
#@ Inderpal Singh Mumick;Dallan Quass;Barinderpal Singh Mumick
#t 1997
#c 5
#% 6798
#% 13016
#% 32914
#% 123589
#% 152928
#% 198467
#% 201868
#% 201928
#% 201929
#% 201930
#% 210182
#% 210210
#% 210211
#% 258960
#% 287324
#% 340301
#% 427218
#% 442663
#% 442767
#% 454612
#% 454613
#% 458556
#% 464215
#% 480623
#% 481288
#% 481608
#% 481951
#! Data warehouses contain large amounts of information, often collected from a variety of independent sources. Decision-support functions in a warehouse, such as on-line analytical processing (OLAP), involve hundreds of complex aggregate queries over large volumes of data. It is not feasible to compute these queries by scanning the data sets each time. Warehouse applications therefore build a large number of summary tables, or materialized aggregate views, to help them increase the system performance.As changes, most notably new transactional data, are collected at the data sources, all summary tables at the warehouse that depend upon this data need to be updated. Usually, source changes are loaded into the warehouse at regular intervals, usually once a day, in a batch window, and the warehouse is made unavailable for querying while it is updated. Since the number of summary tables that need to be maintained is often large, a critical issue for data warehousing is how to maintain the summary tables efficiently.In this paper we propose a method of maintaining aggregate views (the summary-delta table method), and use it to solve two problems in maintaining summary tables in a warehouse: (1) how to efficiently maintain a summary table while minimizing the batch window needed for maintenance, and (2) how to maintain a large set of summary tables defined over the same base tables.While several papers have addressed the issues relating to choosing and materializing a set of summary tables, this is the first paper to address maintaining summary tables efficiently.

#index 227871
#* Database buffer size investigation for OLTP workloads
#@ Thin-Fong Tsuei;Allan N. Packer;Keng-Tai Ko
#t 1997
#c 5
#% 735
#% 1252
#% 86750
#% 114582
#% 152905
#% 319473
#% 365700
#% 463431
#! It is generally accepted that On-Line Transaction Processing (OLTP) systems benefit from large database memory buffers. As enterprise database systems become larger and more complex, hardware vendors are building increasingly large systems capable of supporting huge memory configurations. Database vendors in turn are developing buffer schemes to exploit this physical memory.How much will these developments benefit OLTP workloads? Through empirical studies on databases sized comparably to those seen in the real-world, this paper presents the characteristics of an industry-standard OLTP benchmark as memory buffer size changes. We design the experiments to investigate how the database size, the buffer size and the number of CPUs impact performance, in particular the throughput and the buffer hit rate on Symmetric Multiprocessor Systems. The relationships of these major database attributes are plotted and key observations are summarized. We discuss how these relationships change as the number of CPUs changes. We further quantify the relationships: 1) between database buffer data hit rate, buffer size and database size, 2) between throughput, buffer data hit rate and database size and 3) between throughput and number of CPUs. Algorithms, rules-of-thumb and examples are presented for predicting performance, sizing memory and making trade-offs between adding more memory and increasing the number of CPUs.

#index 227872
#* Database performance in the real world: TPC-D and SAP R/3
#@ Joachen Doppelhammer;Thomas Höppler;Alfons Kemper;Donald Kossmann
#t 1997
#c 5
#% 211575
#% 252358
#% 365700
#% 381472
#% 464215
#% 482070
#! Traditionally, database systems have been evaluated in isolation on the basis of standardized benchmarks (e.g., Wisconsin, TPC-C, TPC-D). We argue that very often such a performance analysis does not reflect the actual use of the DBMSs in the “real world.” End users typically don't access a stand-alone database system; rather they use a comprehensive application system, in which the database system constitutes an integrated component. In order to derive performance evaluations of practical relevance to the end users, the application system including the database system has to be benchmarked. In this paper, we present TPC-D benchmark results carried out using the SAP R/3 system, an integrated business administration system. Like many other application systems SAP R/3 is based on a commercial relational database system. We compare the SAP R/3 benchmark results with TPC-D results of an isolated database system, the database product that served as SAP R/3's back-end.

#index 227875
#* The BUCKY object-relational benchmark
#@ Michael J. Carey;David J. DeWitt;Jeffrey F. Naughton;Mohammad Asgarian;Paul Brown;Johannes E. Gehrke;Dhaval N. Shah
#t 1997
#c 5
#% 114578
#% 152904
#% 172220
#% 481433
#! According to various trade journals and corporate marketing machines, we are now on the verge of a revolution—the object-relational database revolution. Since we believe that no one should face a revolution without appropriate armaments, this paper presents BUCKY, a new benchmark for object-relational database systems. BUCKY is a query-oriented benchmark that tests many of the key features offered by object-relational systems, including row types and inheritance, references and path expressions, sets of atomic values and of references, methods and late binding, and user-defined abstract data types and their methods. To test the maturity of object-relational technology relative to relational technology, we provide both an object-relational version of BUCKY and a relational equivalent thereof (i.e., a relational BUCKY simulation). Finally, we briefly discuss the initial performance results and lessons that resulted from applying BUCKY to one of the early object-relational database system products.

#index 227879
#* The STRIP rule system for efficiently maintaining derived data
#@ Brad Adelberg;Hector Garcia-Molina;Jennifer Widom
#t 1997
#c 5
#% 83335
#% 86939
#% 158051
#% 242451
#% 286991
#% 339366
#% 442766
#% 442832
#% 458544
#% 480623
#% 480938
#% 481422
#% 481448
#% 481457
#% 692767
#! Derived data is maintained in a database system to correlate and summarize base data which records real world facts. As base data changes, derived data needs to be recomputed. This is often implemented by writing active rules that are triggered by changes to base data. In a system with rapidly changing base data, a database with a standard rule system may consume most of its resources running rules to recompute data. This paper presents the rule system implemented as part of the STandard Real-time Information Processor (STRIP). The STRIP rule system is an extension of SQL3-type rules that allows groups of rule actions to be batched together to reduce the total recomputation load on the system. In this paper we describe the syntax and semantics of the STRIP rule system, present an example set of rules to maintain stock index and theoretical option prices in a program trading application, and report the results of experiments performed on the running system. The experiments verify that STRIP's rules allow much more efficient derived data maintenance than conventional rules without batching.

#index 227880
#* An array-based algorithm for simultaneous multidimensional aggregates
#@ Yihong Zhao;Prasad M. Deshpande;Jeffrey F. Naughton
#t 1997
#c 5
#% 211575
#% 427195
#% 463760
#% 481951
#! Computing multiple related group-bys and aggregates is one of the core operations of On-Line Analytical Processing (OLAP) applications. Recently, Gray et al. [GBLP95] proposed the “Cube” operator, which computes group-by aggregations over all possible subsets of the specified dimensions. The rapid acceptance of the importance of this operator has led to a variant of the Cube being proposed for the SQL standard. Several efficient algorithms for Relational OLAP (ROLAP) have been developed to compute the Cube. However, to our knowledge there is nothing in the literature on how to compute the Cube for Multidimensional OLAP (MOLAP) systems, which store their data in sparse arrays rather than in tables. In this paper, we present a MOLAP algorithm to compute the Cube, and compare it to a leading ROLAP algorithm. The comparison between the two is interesting, since although they are computing the same function, one is value-based (the ROLAP algorithm) whereas the other is position-based (the MOLAP algorithm). Our tests show that, given appropriate compression techniques, the MOLAP algorithm is significantly faster than the ROLAP algorithm. In fact, the difference is so pronounced that this MOLAP algorithm may be useful for ROLAP systems as well as MOLAP systems, since in many cases, instead of cubing a table directly, it is faster to first convert the table to an array, cube the array, then convert the result back to a table.

#index 227883
#* Online aggregation
#@ Joseph M. Hellerstein;Peter J. Haas;Helen J. Wang
#t 1997
#c 5
#% 58348
#% 102786
#% 120422
#% 145196
#% 210206
#% 210207
#% 210353
#% 214602
#% 277347
#% 284910
#% 340635
#% 411554
#% 427195
#% 452838
#% 458550
#% 461897
#% 464215
#% 464224
#% 479753
#% 481604
#% 481608
#% 481951
#% 571294
#! Aggregation in traditional database systems is performed in batch mode: a query is submitted, the system processes a large volume of data over a long period of time, and, eventually, the final answer is returned. This archaic approach is frustrating to users and has been abandoned in most other areas of computing. In this paper we propose a new online aggregation interface that permits users to both observe the progress of their aggregation queries and control execution on the fly. After outlining usability and performance requirements for a system supporting online aggregation, we present a suite of techniques that extend a database system to meet these requirements. These include methods for returning the output in random order, for providing control over the relative rate at which different aggregates are computed, and for computing running confidence intervals. Finally, we report on an initial implementation of online aggregation in POSTGRES.

#index 227885
#* Balancing push and pull for data broadcast
#@ Swarup Acharya;Michael Franklin;Stanley Zdonik
#t 1997
#c 5
#% 32884
#% 66172
#% 124011
#% 151529
#% 161681
#% 172876
#% 175253
#% 201897
#% 242695
#% 243299
#% 285813
#% 464065
#% 481777
#% 978507
#! The increasing ability to interconnect computers through internet-working, wireless networks, high-bandwidth satellite, and cable networks has spawned a new class of information-centered applications based on data dissemination. These applications employ broadcast to deliver data to very large client populations. We have proposed the Broadcast Disks paradigm [Zdon94, Acha95b] for organizing the contents of a data broadcast program and for managing client resources in response to such a program. Our previous work on Broadcast Disks focused exclusively on the “push-based” approach, where data is sent out on the broadcast channel according to a periodic schedule, in anticipation of client requests. In this paper, we study how to augment the push-only model with a “pull-based” approach of using a backchannel to allow clients to send explicit requests for data to the server. We analyze the scalability and performance of a broadcast-based system that integrates push and pull and study the impact of this integration on both the steady state and warm-up performance of clients. Our results show that a client backchannel can provide significant performance improvement in the broadcast environment, but that unconstrained use of the backchannel can result in scalability problems due to server saturation. We propose and investigate a set of three techniques that can delay the onset of saturation and thus, enhance the performance and scalability of the system.

#index 227886
#* InfoSleuth: agent-based semantic integration of information in open and dynamic environments
#@ R. J. Bayardo, Jr.;W. Bohrer;R. Brice;A. Cichocki;J. Fowler;A. Helal;V. Kashyap;T. Ksiezyk;G. Martin;M. Nodine;M. Rashid;M. Rusinkiewicz;R. Shea;C. Unnikrishnan;A. Unruh;D. Woelk
#t 1997
#c 5
#% 127860
#% 152934
#% 156337
#% 159110
#% 159113
#% 172394
#% 188853
#% 199556
#% 213437
#% 232106
#% 232166
#% 232170
#% 481923
#% 591543
#% 676653
#% 703817
#% 1290115
#! The goal of the InfoSleuth project at MCC is to exploit and synthesize new technologies into a unified system that retrieves and processes information in an ever-changing network of information sources. InfoSleuth has its roots in the Carnot project at MCC, which specialized in integrating heterogeneous information bases. However, recent emerging technologies such as internetworking and the World Wide Web have significantly expanded the types, availability, and volume of data available to an information management system. Furthermore, in these new environments, there is no formal control over the registration of new information sources, and applications tend to be developed without complete knowledge of the resources that will be available when they are run. Federated database projects such as Carnot that do static data integration do not scale up and do not cope well with this ever-changing environment. On the other hand, recent Web technologies, based on keyword search engines, are scalable but, unlike federated databases, are incapable of accessing information based on concepts. In this experience paper, we describe the architecture, design, and implementation of a working version of InfoSleuth. We show how InfoSleuth integrates new technological developments such as agent technology, domain ontologies, brokerage, and internet computing, in support of mediated interoperation of data and services in a dynamic and open environment. We demonstrate the use of information brokering and domain ontologies as key elements for scalability.

#index 227891
#* STARTS: Stanford proposal for Internet meta-searching
#@ Luis Gravano;Chen-Chuan K. Chang;Héctor García-Molina;Andreas Paepcke
#t 1997
#c 5
#% 67565
#% 172898
#% 194246
#% 360914
#% 443052
#% 481748
#% 648866
#% 672354
#% 672628
#! Document sources are available everywhere, both within the internal networks of organizations and on the Internet. Even individual organizations use search engines from different vendors to index their internal document collections. These search engines are typically incompatible in that they support different query models and interfaces, they do not return enough information with the query results for adequate merging of the results, and finally, in that they do not export metadata about the collections that they index (e.g., to assist in resource discovery). This paper describes STARTS, an emerging protocol for Internet retrieval and search that facilitates the task of querying multiple document sources. STARTS has been developed in a unique way. It is not a standard, but a group effort coordinated by Stanford's Digital Library project, and involving over 11 companies and organizations. The objective of this paper is not only to give an overview of the STARTS protocol proposal, but also to discuss the process that led to its definition.

#index 227894
#* On saying “Enough already!” in SQL
#@ Michael J. Carey;Donald Kossmann
#t 1997
#c 5
#% 32889
#% 43162
#% 116040
#% 123589
#% 191175
#% 210169
#% 210172
#% 213981
#% 214602
#% 252608
#% 340635
#% 380546
#% 411554
#% 481288
#% 614579
#! In this paper, we study a simple SQL extension that enables query writers to explicitly limit the cardinality of a query result. We examine its impact on the query optimization and run-time execution components of a relational DBMS, presenting two approaches—a Conservative approach and an Aggressive approach—to exploiting cardinality limits in relational query plans. Results obtained from an empirical study conducted using DB2 demonstrate the benefits of the SQL extension and illustrate the tradeoffs between our two approaches to implementing it.

#index 227896
#* A framework for implementing hypothetical queries
#@ Timothy Griffin;Richard Hull
#t 1997
#c 5
#% 6304
#% 32889
#% 33376
#% 36683
#% 84343
#% 95618
#% 109838
#% 127959
#% 136740
#% 157217
#% 209729
#% 210192
#% 213971
#% 321426
#% 370758
#% 384051
#% 479585
#% 481130
#% 587347
#! Previous approaches to supporting hypothetical queries have been “eager”: some representation of the hypothetical state (or the corresponding delta) is materialized, and query evaluation is filtered through that representation. This paper develops a framework for evaluating hypothetical queries using a “lazy” approach, or using a hybrid of eager and lazy approaches.We focus on queries having the form “Q when {{U}}” where Q is a relational algebra query and U is an update expression. The value assigned to this query in state DB is the value that Q would return in the state resulting from executing U on DB. Nesting of the keyword when is permitted, and U may involve a sequence of several atomic updates.We present an equational theory for queries involving when that can be used as a basis for optimization. This theory is very different from traditional rules for the relational algebra, because the semantics of when is unlike the semantics of the algebra operators. Our theory is based on the observation that hypothetical states can be represented as substitutions, similar to those arising in functional and logic programming. Furthermore, hypothetical queries of the form Q when {{U}} can be thought of as representing the suspended application of a substitution. Using the equational theory we develop an approach to optimizing the evaluation of hypothetical queries that uses deltas in the sense of Heraclitus, and permits a range of evaluation strategies from lazy to eager.

#index 227914
#* High-performance sorting on networks of workstations
#@ Andrea C. Arpaci-Dusseau;Remzi H. Arpaci-Dusseau;David E. Culler;Joseph M. Hellerstein;David A. Patterson
#t 1997
#c 5
#% 14204
#% 43201
#% 80728
#% 86928
#% 100554
#% 112212
#% 123821
#% 125391
#% 144773
#% 152600
#% 152688
#% 172911
#% 173044
#% 201706
#% 201956
#% 201957
#% 202137
#% 210185
#% 217313
#% 319473
#% 340670
#% 439900
#% 439903
#% 439979
#% 442698
#% 442700
#% 471775
#% 581647
#% 648163
#% 979345
#! We report the performance of NOW-Sort, a collection of sorting implementations on a Network of Workstations (NOW). We find that parallel sorting on a NOW is competitive to sorting on the large-scale SMPs that have traditionally held the performance records. On a 64-node cluster, we sort 6.0 GB in just under one minute, while a 32-node cluster finishes the Datamation benchmark in 2.41 seconds.Our implementations can be applied to a variety of disk, memory, and processor configurations; we highlight salient issues for tuning each component of the system. We evaluate the use of commodity operating systems and hardware for parallel sorting. We find existing OS primitives for memory management and file access adequate. Due to aggregate communication and disk bandwidth requirements, the bottleneck of our system is the workstation I/O bus.

#index 227917
#* Dynamic itemset counting and implication rules for market basket data
#@ Sergey Brin;Rajeev Motwani;Jeffrey D. Ullman;Shalom Tsur
#t 1997
#c 5
#% 152934
#% 452821
#% 463903
#% 481290
#% 481609
#% 481779
#! We consider the problem of analyzing market-basket data and present several important contributions. First, we present a new algorithm for finding large itemsets which uses fewer passes over the data than classic algorithms, and yet uses fewer candidate itemsets than methods based on sampling. We investigate the idea of item reordering, which can improve the low-level efficiency of the algorithm. Second, we present a new way of generating “implication rules,” which are normalized based on both the antecedent and the consequent and are truly implications (not simply a measure of co-occurrence), and we show how they produce more intuitive results than other methods. Finally, we show how different characteristics of real data, as opposed by synthetic data, can dramatically affect the performance of the system and the form of the results.

#index 227919
#* Beyond market baskets: generalizing association rules to correlations
#@ Sergey Brin;Rajeev Motwani;Craig Silverstein
#t 1997
#c 5
#% 662
#% 152934
#% 172386
#% 199537
#% 201894
#% 213977
#% 232102
#% 232136
#% 412588
#% 452821
#% 463883
#% 464714
#% 481290
#% 481588
#% 481754
#% 481758
#% 481779
#! One of the most well-studied problems in data mining is mining for association rules in market basket data. Association rules, whose significance is measured via support and confidence, are intended to identify rules of the type, “A customer purchasing item A often also purchases item B.” Motivated by the goal of generalizing beyond market baskets and the association rules used with them, we develop the notion of mining rules that identify correlations (generalizing associations), and we consider both the absence and presence of items as a basis for generating rules. We propose measuring significance of associations via the chi-squared test for correlation from classical statistics. This leads to a measure that is upward closed in the itemset lattice, enabling us to reduce the mining problem to the search for a border between correlated and uncorrelated itemsets in the lattice. We develop pruning strategies and devise an efficient algorithm for the resulting problem. We demonstrate its effectiveness by testing it on census data and finding term dependence in a corpus of text documents, as well as on synthetic data.

#index 227922
#* Scalable parallel data mining for association rules
#@ Eui-Hong Han;George Karypis;Vipin Kumar
#t 1997
#c 5
#% 25998
#% 140385
#% 152934
#% 443091
#% 459006
#% 463883
#% 481100
#% 481290
#% 481588
#% 481754
#% 481758
#! One of the important problems in data mining is discovering association rules from databases of transactions where each transaction consists of a set of items. The most time consuming operation in this discovery process is the computation of the frequency of the occurrences of interesting subset of items (called candidates) in the database of transactions. To prune the exponentially large space of candidates, most existing algorithms, consider only those candidates that have a user defined minimum support. Even with the pruning, the task of finding all association rules requires a lot of computation power and time. Parallel computers offer a potential solution to the computation requirement of this task, provided efficient and scalable parallel algorithms can be designed. In this paper, we present two new parallel algorithms for mining association rules. The Intelligent Data Distribution algorithm efficiently uses aggregate memory of the parallel computer by employing intelligent candidate partitioning scheme and uses efficient communication mechanism to move data among the processors. The Hybrid Distribution algorithm further improves upon the Intelligent Data Distribution algorithm by dynamically partitioning the candidate set to maintain good load balance. The experimental results on a Cray T3D parallel computer show that the Hybrid Distribution algorithm scales linearly and exploits the aggregate memory better and can generate more association rules with a single scan of database per pass.

#index 227924
#* Efficiently supporting ad hoc queries in large datasets of time sequences
#@ Flip Korn;H. V. Jagadish;Christos Faloutsos
#t 1997
#c 5
#% 36684
#% 115478
#% 137711
#% 210173
#% 287672
#% 319273
#% 359751
#% 375388
#% 460862
#% 480075
#% 481281
#% 527022
#% 670380
#! Ad hoc querying is difficult on very large datasets, since it is usually not possible to have the entire dataset on disk. While compression can be used to decrease the size of the dataset, compressed data is notoriously difficult to index or access.In this paper we consider a very large dataset comprising multiple distinct time sequences. Each point in the sequence is a numerical value. We show how to compress such a dataset into a format that supports ad hoc querying, provided that a small error can be tolerated when the data is uncompressed. Experiments on large, real world datasets (AT&T customer calling patterns) show that the proposed method achieves an average of less than 5% error in any data value after compressing to a mere 2.5% of the original space (i.e., a 40:1 compression ratio), with these numbers not very sensitive to dataset size. Experiments on aggregate queries achieved a 0.5% reconstruction error with a space requirement under 2%.

#index 227927
#* DEVise: integrated querying and visual exploration of large datasets
#@ M. Livny;R. Ramakrishnan;K. Beyer;G. Chen;D. Donjerkovic;S. Lawande;J. Myllymaki;K. Wenger
#t 1997
#c 5
#% 157742
#% 164242
#% 176867
#% 210173
#% 435787
#% 440808
#% 440809
#% 464224
#% 481102
#% 641572
#% 1796202
#! DEVise is a data exploration system that allows users to easily develop, browse, and share visual presentation of large tabular datasets (possibly containing or referencing multimedia objects) from several sources. The DEVise framework is being implemented in a tool that has been already successfully applied to a variety of real applications by a number of user groups.Our emphasis is on developing an intuitive yet powerful set of querying and visualization primitives that can be easily combined to develop a rich set of visual presentations that integrate data from a wide range of application domains. While DEVise is a powerful visualization tool, its greatest strengths are the ability to interactively explore a visual presentation of the data at any level of detail (including retrieving individual data records), and the ability to seamlessly query and combine data from a variety of local and remote sources. In this paper, we present the DEVise framework, describe the current tool, and report on our experience in applying it to several real applications.

#index 227930
#* Partitioned garbage collection of a large object store
#@ Umesh Maheshwari;Barbara Liskov
#t 1997
#c 5
#% 2903
#% 107720
#% 127979
#% 136859
#% 151537
#% 152930
#% 172938
#% 202979
#% 210194
#% 210201
#% 320108
#% 340620
#% 463741
#% 473360
#% 473370
#% 481756
#% 604239
#% 631845
#% 640512
#% 668688
#% 669470
#% 702279
#! We present new techniques for efficient garbage collection in a large persistent object store. The store is divided into partitions that are collected independently using information about inter-partition references. This information is maintained on disk so that it can be recovered after a crash. We use new techniques to organize and update this information while avoiding disk accesses. We also present a new global marking scheme to collect cyclic garbage across partitions. Global marking is piggybacked on partitioned collection; the result is an efficient scheme that preserves the localized nature of partitioned collection, yet is able to collect all garbage.We have implemented the part of garbage collection responsible for maintaining information about inter-partition references. We present a performance study to evaluate this work; the results show that our techniques result in substantial savings in the usage of disk and memory.

#index 227932
#* Size separation spatial join
#@ Nick Koudas;Kenneth C. Sevcik
#t 1997
#c 5
#% 13041
#% 18614
#% 152937
#% 172908
#% 191154
#% 210186
#% 210187
#% 285932
#% 380546
#% 427199
#% 462957
#% 480093
#% 481920
#% 527012
#! We introduce a new algorithm to compute the spatial join of two or more spatial data sets, when indexes are not available on them. Size Separation Spatial Join (S3J) imposes a hierarchical decomposition of the data space and, in contrast with previous approaches, requires no replication of entities from the input data sets. Thus its execution time depends only on the sizes of the joined data sets.We describe S3J and present an analytical evaluation of its I/O and processor requirements comparing them with those of previously proposed algorithms for the same problem. We show that S3J has relatively simple cost estimation formulas that can be exploited by a query optimizer. S3J can be efficiently implemented using software already present in many relational systems. In addition, we introduce Dynamic Spatial Bitmaps (DSB), a new technique that enables S3J to dynamically or statically exploit bitmap query processing techniques.Finally, we present experimental results for a prototype implementation of S3J involving real and synthetic data sets for a variety of data distributions. Our experimental results are consistent with our analytical observations and demonstrate the performance benefits of S3J over alternative approaches that have been proposed recently.

#index 227934
#* Building a scaleable geo-spatial DBMS: technology, implementation, and evaluation
#@ Jignesh Patel;JieBing Yu;Navin Kabra;Kristin Tufte;Biswadeep Nag;Josef Burger;Nancy Hall;Karthikeyan Ramasamy;Roger Lueder;Curt Ellmann;Jim Kupsch;Shelly Guo;Johan Larson;David De Witt;Jeffrey Naughton
#t 1997
#c 5
#% 77963
#% 86929
#% 86950
#% 114582
#% 115661
#% 152902
#% 172939
#% 201876
#% 210187
#% 427199
#% 442700
#% 458552
#% 463760
#% 480608
#% 480761
#% 480966
#% 481428
#% 565258
#! This paper presents a number of new techniques for parallelizing geo-spatial database systems and discusses their implementation in the Paradise object-relational database system. The effectiveness of these techniques is demonstrated using a variety of complex geo-spatial queries over a 120 GB global geo-spatial data set.

#index 227936
#* A toolkit for negotiation support interfaces to multi-dimensional data
#@ Michael Gebhardt;Matthias Jarke;Stephan Jacobs
#t 1997
#c 5
#% 3640
#% 93504
#% 157234
#% 176525
#% 183913
#% 210182
#% 230397
#% 481933
#% 595801
#! CoDecide is an experimental user interface toolkit that offers an extension to spreadsheet concepts specifically geared towards support for cooperative analysis of the kinds of multi-dimensional data encountered in data warehousing. It is distinguished from previous proposals by direct support for drill-down/roll-up analysis without redesign of an interface; more importantly, CoDecide can link multiple views on a data cube for synchronous or asynchronoous cooperation by multiple analysts, through a conceptual model visualizing the problem dimensions on so-called tapes. Tapes generalize the ideas of ranging and pivoting in current data warehouses for the multi-perspective and multi-user case. CoDecide allows the rapid composition of multi-matrix interfaces and their linkage to underlying data sources. A LAN version of CoDecide has been used in a number of design decision support applications. A WWW version representing externally materialized views on databases is currently under development.

#index 227937
#* Distance-based indexing for high-dimensional metric spaces
#@ Tolga Bozkaya;Meral Ozsoyoglu
#t 1997
#c 5
#% 68091
#% 84654
#% 86950
#% 169940
#% 172949
#% 201876
#% 281750
#% 322309
#% 427199
#% 460862
#% 480093
#% 481279
#% 481460
#! In many database applications, one of the common queries is to find approximate matches to a given query item from a collection of data items. For example, given an image database, one may want to retrieve all images that are similar to a given query image. Distance based index structures are proposed for applications where the data domain is high dimensional, or the distance function used to compute distances between data objects is non-Euclidean. In this paper, we introduce a distance based index structure called multi-vantage point (mvp) tree for similarity queries on high-dimensional metric spaces. The mvp-tree uses more than one vantage point to partition the space into spherical cuts at each level. It also utilizes the pre-computed (at construction time) distances between the data points and the vantage points. We have done experiments to compare mvp-trees with vp-trees which have a similar partitioning strategy, but use only one vantage point at each level, and do not make use of the pre-computed distances. Empirical studies show that mvp-tree outperforms the vp-tree 20% to 80% for varying query ranges and different distance distributions.

#index 227939
#* The SR-tree: an index structure for high-dimensional nearest neighbor queries
#@ Norio Katayama;Shin'ichi Satoh
#t 1997
#c 5
#% 86950
#% 172908
#% 201876
#% 321455
#% 411694
#% 427199
#% 435141
#% 437405
#% 437509
#% 462503
#% 464195
#% 480093
#% 481956
#% 565447
#! Recently, similarity queries on feature vectors have been widely used to perform content-based retrieval of images. To apply this technique to large databases, it is required to develop multidimensional index structures supporting nearest neighbor queries efficiently. The SS-tree had been proposed for this purpose and is known to outperform other index structures such as the R*-tree and the K-D-B-tree. One of its most important features is that it employs bounding spheres rather than bounding rectangles for the shape of regions. However, we demonstrate in this paper that bounding spheres occupy much larger volume than bounding rectangles with high-dimensional data and that this reduces search efficiency. To overcome this drawback, we propose a new index structure called the SR-tree (Sphere/Rectangle-tree) which integrates bounding spheres and bounding rectangles. A region of the SR-tree is specified by the intersection of a bounding sphere and a bounding rectangle. Incorporating bounding rectangles permits neighborhoods to be partitioned into smaller regions than the SS-tree and improves the disjointness among regions. This enhances the performance on nearest neighbor queries especially for high-dimensional and non-uniform data which can be practical in actual image/video similarity indexing. We include the performance test results the verify this advantage of the SR-tree and show that the SR-tree outperforms both the SS-tree and the R*-tree.

#index 227941
#* Wave-indices: indexing evolving databases
#@ Narayanan Shivakumar;Héctor García-Molina
#t 1997
#c 5
#% 51746
#% 86532
#% 102759
#% 172922
#% 182902
#% 204673
#% 452782
#% 462804
#% 480948
#% 481439
#% 978507
#! In many applications, new data is being generated every day. Often an index of the data of a past window of days is required to answer queries efficiently. For example, in a warehouse one may need an index on the sales records of the last week for efficient data mining, or in a Web service one may provide an index of Netnews articles of the past month. In this paper, we propose a variety of wave indices where the data of a new day can be efficiently added, and old data can be quickly expired, to maintain the required window. We compare these schemes based on several system performance measures, such as storage, query response time, and maintenance work, as well as on their simplicity and ease of coding.

#index 227944
#* On-line warehouse view maintenance
#@ Dallan Quass;Jennifer Widom
#t 1997
#c 5
#% 8194
#% 9241
#% 58379
#% 86946
#% 116055
#% 123589
#% 201869
#% 201929
#% 210182
#% 287366
#% 341233
#% 411701
#% 411741
#% 442663
#% 463261
#% 463429
#% 480941
#! Data warehouses store materialized views over base data from external sources. Clients typically perform complex read-only queries on the views. The views are refreshed periodically by maintenance transactions, which propagate large batch updates from the base tables. In current warehousing systems, maintenance transactions usually are isolated from client read activity, limiting availability and/or size of the warehouse. We describe an algorithm called 2VNL that allows warehouse maintenance transactions to run concurrently with readers. By logically maintaining two versions of the database, no locking is required and serializability is guaranteed. We present our algorithm, explain its relationship to other multi-version concurrency control algorithms, and describe how it can be implemented on top of a conventional relational DBMS using a query rewrite approach.

#index 227945
#* Supporting multiple view maintenance policies
#@ Latha S. Colby;Akira Kawaguchi;Daniel F. Lieuwen;Inderpal Singh Mumick;Kenneth A. Ross
#t 1997
#c 5
#% 6798
#% 13015
#% 13016
#% 32914
#% 58345
#% 98469
#% 152928
#% 201922
#% 201928
#% 201929
#% 201930
#% 210210
#% 210211
#% 277332
#% 287324
#% 427218
#% 442663
#% 442767
#% 458544
#% 462789
#% 464705
#% 480623
#! Materialized views and view maintenance are becoming increasingly important in practice. In order to satisfy different data currency and performance requirements, a number of view maintenance policies have been proposed. Immediate maintenance involves a potential refresh of the view after every update to the deriving tables. When staleness of views can be tolerated, a view may be refreshed periodically or (on-demand) when it is queried. The maintenance policies that are chosen for views have implications on the validity of the results of queries and affect the performance of queries and updates. In this paper, we investigate a number of issues related to supporting multiple views with different maintenance policies.We develop formal notions of consistency for views with different maintenance policies. We then introduce a model based on view groupings for view maintenance policy assignment, and provide algorithms, based on the viewgroup model, that allow consistency of views to be guaranteed. Next, we conduct a detailed study of the performance aspects of view maintenance policies based on an actual implementation of our model. The performance study investigates the trade-offs between different maintenance policy assignments. Our analysis of both the consistency and performance aspects of various view maintenance policies are important in making correct maintenance policy assignments.

#index 227947
#* Efficient view maintenance at data warehouses
#@ D. Agrawal;A. El Abbadi;A. Singh;T. Yurek
#t 1997
#c 5
#% 13016
#% 152928
#% 201928
#% 201929
#% 209729
#% 210210
#% 210211
#% 340300
#% 340301
#% 427218
#% 442767
#% 458556
#% 480616
#% 480623
#! We present incremental view maintenance algorithms for a data warehouse derived from multiple distributed autonomous data sources. We begin with a detailed framework for analyzing view maintenance algorithms for multiple data sources with concurrent updates. Earlier approaches for view maintenance in the presence of concurrent updates typically require two types of messages: one to compute the view change due to the initial update and the other to compensate the view change due to interfering concurrent updates. The algorithms developed in this paper instead perform the compensation locally by using the information that is already available at the data warehouse. The first algorithm, termed SWEEP, ensures complete consistency of the view at the data warehouse in the presence of concurrent updates. Previous algorithms for incremental view maintenance either required a quiescent state at the data warehouse or required an exponential number of messages in terms of the data sources. In contrast, this algorithm does not require that the data warehouse be in a quiescent state for incorporating the new views and also the message complexity is linear in the number of data sources. The second algorithm, termed Nested SWEEP, attempts to compute a composite view change for multiple updates that occur concurrently while maintaining strong consistency.

#index 227949
#* Eliminating costly redundant computations from SQL trigger executions
#@ François Llirbat;Françoise Fabret;Eric Simon
#t 1997
#c 5
#% 59350
#% 116044
#% 147843
#% 197480
#% 201898
#% 210182
#% 210208
#% 394417
#% 452763
#% 481132
#% 481952
#% 565457
#! Active database systems are now in widespread use. The use of triggers in these systems, however, is difficult because of the complex interaction between triggers, transactions, and application programs. Repeated calculations of rules may incur costly redundant computations in rule conditions and actions. In this paper, we focus on active relational database systems supporting SQL triggers. In this context, we provide a powerful and complete solution to eliminate redundant computations of SQL triggers when they are costly. We define a model to describe programs, rules and their interactions. We provide algorithms to extract invariant subqueries from trigger's condition and action. We define heuristics to memorize the most “profitable” invariants. Finally, we develop a rewriting technique that enables to generate and execute the optimized code of SQL triggers.

#index 227951
#* Temporal aggregation in active database rules
#@ Iakovos Motakis;Carlo Zaniolo
#t 1997
#c 5
#% 46269
#% 116049
#% 135384
#% 172950
#% 201924
#% 201982
#% 210183
#% 242279
#% 394417
#% 452818
#% 459259
#% 459263
#% 463287
#% 463738
#% 463894
#% 480938
#% 481255
#% 481448
#% 527800
#% 567850
#! An important feature of many advanced active database prototypes is support for rules triggered by complex patterns of events. Their composite event languages provide powerful primitives for event-based temporal reasoning. In fact, with one important exception, their expressive power matches and surpasses that of sophisticated languages offered by Time Series Management Systems (TSMS), which have been extensively used for temporal data analysis and knowledge discovery. This exception pertains to temporal aggregation, for which, current active database systems offer only minimal support, if any.In this paper, we introduce the language TREPL, which addresses this problem. The TREPL prototype, under development at UCLA, offers primitives for temporal aggregation that exceed the capabilities of state-of-the-art composite event languages, and are comparable to those of TSMS languages. TREPL also demonstrates a rigorous and general approach to the definition of composite event language semantics. The meaning of a TREPL rule is formally defined by mapping it into a set of Datalog1S rules, whose logic-based semantics characterizes the behavior of the original rule. This approach handles naturally temporal aggregates, including user-defined ones, and is also applicable to other composite event languages, such as ODE, Snoop and SAMOS.

#index 227953
#* Association rules over interval data
#@ R. J. Miller;Y. Yang
#t 1997
#c 5
#% 36672
#% 152934
#% 201894
#% 210160
#% 210173
#% 412588
#% 443085
#% 452747
#% 463883
#% 481281
#% 481290
#% 481588
#% 481754
#% 481758
#% 481779
#! We consider the problem of mining association rules over interval data (that is, ordered data for which the separation between data points has meaning). We show that the measures of what rules are most important (also called rule interest) that are used for mining nominal and ordinal data do not capture the semantics of interval data. In the presence of interval data, support and confidence are no longer intuitive measures of the interest of a rule. We propose a new definition of interest for association rules that takes into account the semantics of interval data. We developed an algorithm for mining association rules under the new definition and overview our experience using the algorithm on large real-life datasets.

#index 227954
#* Secure transaction processing in firm real-time database systems
#@ Binto George;Jayant Haritsa
#t 1997
#c 5
#% 27057
#% 116053
#% 117903
#% 124815
#% 149627
#% 205092
#% 238731
#% 238734
#% 320902
#% 322637
#% 339364
#% 664535
#% 690419
#! Many real-time database applications arise in safety-critical installations and military systems where enforcing security is crucial to the success of the enterprise. A secure real-time database system has to simultaneously satisfy who requirements guarantee data security and minimize the number of missed transaction deadlines. We investigate here the performance implications, in terms of missed deadlines, of guaranteeing security in a real-time database system. In particular, we focus on the concurrency control aspects of this issue.Our main contributions are the following: First, we identify which among the previously proposed real-time concurrency control protocols are capable of providing protection against both direct and indirect (covert channels) means of unauthorized access to data. Second, using a detailed simulation model of a firm-deadline real-time database system, we profile the real-time performance of a representative set of these secure concurrency control protocols. Our experiments show that a prioritized optimistic concurrency control protocol. OPT-WAIT, provides the best overall performance. Third, we propose and evaluate a novel dual approach to secure transaction concurrency control that allows the real-time database system to simultaneously use different concurrency control mechanisms for guaranteeing security and for improving real-time performance. By appropriately choosing these different mechanisms, we have been able to design hybrid concurrency control algorithms that provide even better performance than OPT-WAIT.

#index 227956
#* A unified framework for enforcing multiple access control policies
#@ Sushil Jajodia;Pierangela Samarati;V. S. Subrahmanian;Eliza Bertino
#t 1997
#c 5
#% 53385
#% 53388
#% 64407
#% 91075
#% 128261
#% 151510
#% 238713
#% 287493
#% 287670
#% 443012
#% 443103
#% 481432
#% 488009
#% 664550
#! Although several access control policies can be devised for controlling access to information, all existing authorization models, and the corresponding enforcement mechanisms, are based on a specific policy (usually the closed policy). As a consequence, although different policy choices are possible in theory, in practice only a specific policy can be actually applied within a given system. However, protection requirements within a system can vary dramatically, and no single policy may simultaneously satisfy them all.In this paper we present a flexible authorization manager (FAM) that can enforce multiple access control policies within a single, unified system. FAM is based on a language through which users can specify authorizations and access control policies to be applied in controlling execution of specific actions on given objects. We formally define the language and properties required to hold on the security specifications and prove that this language can express all security specifications. Furthermore, we show that all programs expressed in this language (called FAM/CAM-programs) are also guaranteed to be consistent (i.e., no conflicting access decisions occur) and CAM-programs are complete (i.e., every access is either authorized or denied). We then illustrate how several well-known protection policies proposed in the literature can be expressed in the FAM/CAM language and how users can customize the access control by specifying their own policies. The result is an access control mechanism which is flexible, since different access control policies can all coexist in the same data system, and extensible, since it can be augmented with any new policy a specific application or user may require.

#index 227958
#* Revisiting commit processing in distributed database systems
#@ Ramesh Gupta;Jayant Haritsa;Krithi Ramamritham
#t 1997
#c 5
#% 4619
#% 9241
#% 25378
#% 27057
#% 58353
#% 83933
#% 102308
#% 159735
#% 197715
#% 268750
#% 318418
#% 320902
#% 411707
#% 480256
#% 481129
#% 531907
#% 615537
#! A significant body of literature is available on distributed transaction commit protocols. Surprisingly, however, the relative merits of these protocols have not been studied with respect to their quantitative impact on transaction processing performance. In this paper, using a detailed simulation model of a distributed database system, we profile the transaction throughput performance of a representative set of commit protocols. A new commit protocol, OPT, that allows transactions to “optimistically” borrow uncommitted data in a controlled manner is also proposed and evaluated. The new protocol is easy to implement and incorporate in current systems, and can coexist with most other optimizations proposed earlier. For example, OPT can be combined with current industry standard protocols such as Presumed Commit and Presumed Abort.The experimental results show that distributed commit processing can have considerably more influence than distributed data processing on the throughput performance and that the choice of commit protocol clearly affects the magnitude of this influence. Among the protocols evaluated, the new optimistic commit protocol provides the best transaction throughput performance for a variety of workloads and system configurations. In fact, OPT's peak throughput is often close to the upper bound on achievable performance. Even more interestingly, a three-phase (i.e., non-blocking) version of OPT provides better peak throughput performance than all of the standard two-phase (i.e., blocking protocols evaluated in our study.

#index 227960
#* Lessons from Wall Street: case studies in configuration, tuning, and distribution
#@ Dennis Shasha
#t 1997
#c 5
#% 210179
#! Consider a setting in whichDatabase speed and reliability can make the difference between prosperity and ruin.Money for information systems is no object.Data must be accessible from many points on the globe with subsecond response.The financial industry is exactly such an environment.This tutorial presents case studies in configuration, tuning, and distribution drawn from financial applications. The cases suggest both research and product issues and so should be of interest to the entire Sigmod community.

#index 227962
#* Object-relational database systems (tutorial): principles, products and challenges
#@ Michael J. Carey;Nelson M. Mattos;Anil K. Nori
#t 1997
#c 5
#! Object-relational database systems, a.k.a. “universal servers,” are emerging as the next major generation of commercial database system technology. Products from relational DBMS vendors including IBM, Informix, Oracle, UniSQL, and others, include object-relational features today, and all of the major vendors appear to be on course to delivering full object-relational support in their products over the next few years. In addition, the SQL3 standard is rapidly solidifying in this area. The goal of this tutorial is to explain what the key features are of object-relational database systems, review what today's products provide, and then look ahead to where these systems are heading. The presentation will be aimed at general SIGMOD audience, and should therefore be appropriate for users, practitioners, and/or researchers who want to learn about object-relational database systems.

#index 227964
#* Databases on the Web: technologies for federation architectures and case studies
#@ Ralf Kramer
#t 1997
#c 5
#% 85086
#% 223772
#% 224186
#% 360606
#% 393784
#% 591550

#index 227965
#* Data warehousing and OLAP for decision support
#@ Surajit Chaudhuri;Umeshwar Dayal
#t 1997
#c 5
#% 223781
#! On-Line Analytical Processing (OLAP) and Data Warehousing are decision support technologies. Their goal is to enable enterprises to gain competitive advantage by exploiting the ever-growing amount of data that is collected and stored in corporate databases and files for better and faster decision making. Over the past few years, these technologies have experienced explosive growth, both in the number of products and services offered, and in the extent of coverage in the trade press. Vendors, including all database companies, are paying increasing attention to all aspects of decision support.

#index 227966
#* Query optimization at the crossroads
#@ Surajit Chaudhuri
#t 1997
#c 5

#index 227968
#* Delaunay: a database visualization system
#@ Isabel F. Cruz;M. Averbuch;Wendy T. Lucas;Melissa Radzyminski;Kirby Zhang
#t 1997
#c 5
#% 116048
#% 125595
#% 189739
#% 538889
#! Visual query systems have traditionally supported a set of pre-defined visual displays. We describe the Delaunay system, which supports visualizations of object-oriented databases specified by the user with a visual constraint-based query language. The highlights of our approach are the expressiveness of the visual query language, the efficiency of the query engine, and the overall flexibility and extensibility of the framework. The user interface is implemented using Java and is available on the WWW.

#index 227969
#* Picture programming project
#@ Nita Goyal;Charles Hoch;Ravi Krishnamurthy;Brian Meckler;Michael Suchow;Moshe Zloof
#t 1997
#c 5
#% 210215
#% 464061

#index 227971
#* DEVise (demo abstract): integrated querying and visual exploration of large datasets
#@ M. Livny;R. Ramakrishnan;K. Beyer;G. Chen;D. Donjerkovic;S. Lawande;J. Myllymaki;K. Wenger
#t 1997
#c 5
#% 210173
#% 210178
#! DEVise is a data exploration system that allows users to easily develop, browse, and share visual presentations of large tabular datasets (possibly containing or referencing multimedia objects) from several sources. The DEVise framework, implemented in a tool that has been already successfully applied to a variety of real applications by a number of user groups, makes several contributions. In particular, it combines support for extended relational queries with powerful data visualization features. Datasets much larger than available main memory can be handled—DEVise is currently being used to visualize datasets well in excess of 100MB—and data can be interactively examined at several levels of detail: all the way from meta-data summarizing the entire dataset, to large subsets of the actual data, to individual data records. Combining querying (in general, data processing) with visualizations gives us a very versatile tool, and presents several novel challenges.Our emphasis is on developing an intuitive yet powerful set of querying and visualization primitives that can be easily combined to develop a rich set of visual presentations that integrate data from a wide range of application domains. In this demo, we will present a number of examples of the use of the DEVise tool for visualizing and interactively exploring very large datasets, and report on our experience in applying it to several real applications.

#index 227976
#* SEMCOG: an object-based image retrieval system and its visual query interface
#@ Wen-Syan Li;K. Selçuk Candan;Kyoji Hirata;Yoshinori Hara
#t 1997
#c 5
#% 211510
#% 296938

#index 227981
#* The Context Interchange mediator prototype
#@ S. Bressan;C. H. Goh;K. Fynn;M. Jakobisiak;K. Hussein;H. Kon;T. Lee;S. Madnick;T. Pena;J. Qu;A. Shum;M. Siegel
#t 1997
#c 5
#% 58354
#% 85086
#% 116303
#% 158908
#% 172378
#! The Context Interchange strategy presents a novel approach for mediated data access in which semantic conflicts among heterogeneous systems are not identified a priori, but are detected and reconciled by a context mediator through comparison of contexts. This paper reports on the implementation of a Context Interchange Prototype which provides a concrete demonstration of the features and benefits of this integration strategy.

#index 227985
#* MDM: a multiple-data model tool for the management of heterogeneous database schemes
#@ Paolo Atzeni;Riccardo Torlone
#t 1997
#c 5
#% 11284
#% 38696
#% 77652
#% 106916
#% 158896
#% 287631
#% 289350
#% 416016
#% 458995
#% 480969
#% 535966
#! MDM is a tool that enables the users to define schemes of different data models and to perform translations of schemes from one model to another. These functionalities can be at the basis of a customizable and integrated CASE environment supporting the analysis and design of information systems. MDM has two main components: the Model Manager and the Schema Manager. The Model Manager supports a specialized user, the model engineer, in the definition of a variety of models, on the basis of a limited set of metaconstructs covering almost all known conceptual models. The Schema Manager allows designers to create and modify schemes over the defined models, and to generate at each time a translation of a scheme into any of the data models currently available. Translations between models are automatically derived, at definition time, by combining a predefined set of elementary transformations, which implement the standard translations between simple combinations of constructs.

#index 227987
#* Template-based wrappers in the TSIMMIS system
#@ Joachim Hammer;Héctor García-Molina;Svetlozar Nestorov;Ramana Yerneni;Marcus Breunig;Vasilis Vassalos
#t 1997
#c 5
#% 459241
#% 463919
#% 481935
#! In order to access information from a variety of heterogeneous information sources, one has to be able to translate queries and data from one data model into another. This functionality is provided by so-called (source) wrappers [4,8] which convert queries into one or more commands/queries understandable by the underlying source and transform the native results into a format understood by the application. As part of the TSIMMIS project [1, 6] we have developed hard-coded wrappers for a variety of sources (e.g., Sybase DBMS, WWW pages, etc.) including legacy systems (Folio). However, anyone who has built a wrapper before can attest that a lot of effort goes into developing and writing such a wrapper. In situations where it is important or desirable to gain access to new sources quickly, this is a major drawback. Furthermore, we have also observed that only a relatively small part of the code deals with the specific access details of the source. The rest of the code is either common among wrappers or implements query and data transformation that could be expressed in a high level, declarative fashion.Based on these observations, we have developed a wrapper implementation toolkit [7] for quickly building wrappers. The toolkit contains a library for commonly used functions, such as for receiving queries from the application and packaging results. It also contains a facility for translating queries into source-specific commands, and for translating results into a model useful to the application. The philosophy behind our “template-based” translation methodology is as follows. The wrapper implementor specifies a set of templates (rules) written in a high level declarative language that describe the queries accepted by the wrapper as well as the objects that it returns. If an application query matches a template, an implementor-provided action associated with the template is executed to provide the native query for the underlying source1. When the source returns the result of the query, the wrapper transforms the answer which is represented in the data model of the source into a representation that is used by the application. Using this toolkit one can quickly design a simple wrapper with a few templates that cover some of the desired functionality, probably the one that is most urgently needed. However, templates can be added gradually as more functionality is required later on.Another important use of wrappers is in extending the query capabilities of a source. For instance, some sources may not be capable of answering queries that have multiple predicates. In such cases, it is necessary to pose a native query to such a source using only predicates that the source is capable of handling. The rest of the predicates are automatically separated from the user query and form a filter query. When the wrapper receives the results, a post-processing engine applies the filter query. This engine supports a set of built-in predicates based on the comparison operators =,≠,, etc. In addition, the engine supports more complex predicates that can be specified as part of the filter query. The postprocessing engine is common to wrappers of all sources and is part of the wrapper toolkit. Note that because of postprocessing, the wrapper can handle a much larger class of queries than those that exactly match the templates it has been given. Figure 1 shows an overview of the wrapper architecture as it is currently implemented in our TSIMMIS testbed. Shaded components are provided by the toolkit, the white component is source-specific and must be generated by the implementor. The driver component controls the translation process and invokes the following services: the parser which parses the templates, the native schema, as well as the incoming queries into internal data structures, the matcher which matches a query against the set of templates and creates a filter query for postprocessing if necessary, the native component which submits the generated action string to the source, and extracts the data from the native result using the information given in the source schema, and the engine, which transforms and packages the result and applies a postprocessing filter if one has been created by the matcher. We now describe the sequence of events that occur at the wrapper during the translation of a query and its result using an example from our prototype system. The queries are formulated using a rule-based language called MSL that has been developed as a template specification and query language for the TSIMMIS project. Data is represented using our Object Exchange Model (OEM). We will briefly describe MSL and OEM in the next section. Details on MSL can be found in [5], a full introduction to OEM is given in [1].

#index 227989
#* Languages for multi-database interoperability
#@ Frédéric Gingras;Laks V. S. Lakshmanan;Iyer N. Subramanian;Despina Papoulis;Nematollaah Shiri
#t 1997
#c 5
#% 464215
#% 481944

#index 227992
#* Infomaster: an information integration system
#@ Michael R. Genesereth;Arthur M. Keller;Oliver M. Duschka
#t 1997
#c 5
#% 223201
#% 296931
#! Infomaster is an information integration system that provides integrated access to multiple distributed heterogeneous information sources on the Internet, thus giving the illusion of a centralized, homogeneous information system. We say that Infomaster creates a virtual data warehouse. The core of Infomaster is a facilitator that dynamically determines an efficient way to answer the user's query using as few sources as necessary and harmonizes the heterogeneities among these sources. Infomaster handles both structural and content translation to resolve differences between multiple data sources and the multiple applications for the collected data. Infomaster connects to a variety of databases using wrappers, such as for Z39.50, SQL databases through ODBC, EDI transactions, and other World Wide Web (WWW) sources. There are several WWW user interfaces to Infomaster, including forms based and textual. Infomaster also includes a programmatic interface and it can download results in structured form onto a client computer. Infomaster has been in production use for integrating rental housing advertisements from several newspapers (since fall 1995), and for meeting room scheduling (since winter 1996). Infomaster is also being used to integrate heterogeneous electronic product catalogs.

#index 227993
#* The InfoSleuth Project
#@ R. J. Bayardo, Jr.;W. Bohrer;R. Brice;A. Cichocki;J. Fowler;A. Halal;V. Kashyap;T. Ksiezyk;G. Martin;M. Nodine;M. Rashid;M. Rusinkiewicz;R. Shea;C. Unnikrishnan;A. Unruh;D. Woelk
#t 1997
#c 5
#% 159110
#% 172394
#% 227886

#index 227994
#* The distributed information search component (Disco) and the World Wide Web
#@ Anthony Tomasic;Rémy Amouroux;Philippe Bonnet;Olga Kapitskaia;Hubert Naacke;Louiqa Raschid
#t 1997
#c 5
#% 152980
#% 252366
#% 252369
#% 252374
#% 340305
#% 631868
#! The Distributed Information Search COmponent (DISCO) is a prototype heterogeneous distributed database that accesses underlying data sources. The DISCO prototype currently focuses on three central research problems in the context of these systems. First, since the capabilities of each data source is different, transforming queries into subqueries on data source is difficult. We call this problem the weak data source problem. Second, since each data source performs operations in a generally unique way, the cost for performing an operation may vary radically from one wrapper to another. We call this problem the radical cost problem. Finally, existing systems behave rudely when attempting to access an unavailable data source. We call this problem the ungraceful failure problem.DISCO copes with these problems. For the weak data source problem, the database implementor defines precisely the capabilities of each data source. For the radical cost problem, the database implementor (optionally) defines cost information for some of the operations of a data source. The mediator uses this cost information to improve its cost model. To deal with ungraceful failures, queries return partial answers. A partial answer contains the part of the final answer to the query that was produced by the available data sources. The current working prototype of DISCO contains implementations of these solutions and operations over a collection of wrappers that access information both in files and on the World Wide Web.

#index 227995
#* STRUDEL: a Web site management system
#@ Mary Fernandez;Daniela Florescu;Jaewoo Kang;Alon Levy;Dan Suciu
#t 1997
#c 5
#% 210176
#% 210205
#% 210214
#% 213437
#% 340295
#% 459241
#% 464716
#% 464717
#% 464720
#% 464724
#% 481923
#% 481935
#% 1499470

#index 227996
#* GeoMiner: a system prototype for spatial data mining
#@ Jaiwei Han;Krzysztof Koperski;Nebojsa Stefanovic
#t 1997
#c 5
#% 210173
#% 232102
#% 435137
#% 442847
#% 443082
#% 452747
#% 481281
#% 481290
#% 481588
#% 527021
#! Spatial data mining is to mine high-level spatial information and knowledge from large spatial databases. A spatial data mining system prototype, GeoMiner, has been designed and developed based on our years of experience in the research and development of relational data mining system, DBMiner, and our research into spatial data mining. The data mining power of GeoMiner includes mining three kinds of rules: characteristic rules, comparison rules, and association rules, in geo-spatial databases, with a planned extension to include mining classification rules and clustering rules. The SAND (Spatial And Nonspatial Data) architecture is applied in the modeling of spatial databases, whereas GeoMiner includes the spatial data cube construction module, spatial on-line analytical processing (OLAP) module, and spatial data mining modules. A spatial data mining language, GMQL (Geo-Mining Query Language), is designed and implemented as an extension to Spatial SQL [3], for spatial data mining. Moreover, an interactive, user-friendly data mining interface is constructed and tools are implemented for visualization of discovered spatial knowledge.

#index 227997
#* The WHIPS prototype for data warehouse creation and maintenance
#@ Wilburt J. Labio;Yue Zhuge;Janet L. Wiener;Himanshu Gupta;Héctor García-Molina;Jennifer Widom
#t 1997
#c 5
#% 83933
#% 340300
#% 481931
#! A data warehouse is a repository of integrated information from distributed, autonomous, and possibly heterogeneous, sources. In effect, the warehouse stores one or more materialized views of the source data. The data is then readily available to user applications for querying and analysis. Figure 1 shows the basic architecture of a warehouse: data is collected from each source, integrated with data from other sources, and stored at the warehouse. Users then access the data directly from the warehouse.As suggested by Figure 1, there are two major components in a warehouse system: the integration component, responsible for collecting and maintaining the materialized views, and the query and analysis component, responsible for fulfilling the information needs of specific end users. Note that the two components are not independent. For example, which views the integration component materializes depends on the expected needs of end users.Most current commercial warehousing systems (e.g., Redbrick, Sybase, Arbor) focus on the query and analysis component, providing specialized index structures at the warehouse and extensive querying facilities for the end user. In the WHIPS (WareHousing Information Project at Stanford) project, on the other hand, we focus on the integration component. In particular, we have developed an architecture and implemented a prototype for identifying data changes at heterogeneous sources, transforming them and summarizing them in accordance to warehouse specifications, and incrementally integrating them into the warehouse. We propose to demonstrate our prototype at SIGMOD, illustrating the main features of our architecture. Our architecture is modular and we designed it specifically to fulfill several important and interrelated goals: data sources and warehouse views can be added and removed dynamically; it is scalable by adding more internal modules; changes at the sources are detected automatically; the warehouse may be updated continuously as the sources change, without requiring “down time;” and the warehouse is always kept consistent with the source data by the integration algorithms. More details on these goals and how we achieve them are provided in [WGL+96].

#index 227998
#* Structural matching and discovery in document databases
#@ Jason Tsong-Li Wang;Dennis Shasha;George J. S. Chang;Liam Relihan;Kaizhong Zhang;Girish Patel
#t 1997
#c 5
#% 2011
#% 66654
#% 84549
#% 172927
#% 201980
#% 210212
#% 210217
#% 252364
#% 442886
#% 979358
#! Structural matching and discovery in documents such as SGML and HTML is important for data warehousing [6], version management [7, 11], hypertext authoring, digital libraries [4] and Internet databases. As an example, a user of the World Wide Web may be interested in knowing changes in an HTML document [2, 5, 10]. Such changes can be detected by comparing the old and new version of the document (referred to as structural matching of documents). As another example, in hypertext authoring, a user may wish to find the common portions in the history list of a document or in a database of documents (referred to as structural discovery of documents). In SIGMOD 95 demo sessions, we exhibited a software package, called TreeDiff [13], for comparing two latex documents and showing their differences. Given two documents, the tool represents the documents as ordered labeled trees and finds an optimal sequence of edit operations to transform one document (tree) to the other. An edit operation could be an insert, delete, or change of a node in the trees. The tool is so named because documents are represented and compared using approximate tree matching techniques [9, 12, 14].

#index 227999
#* S3: similarity search in CAD database systems
#@ Stefan Berchtold;Hans-Peter Kriegel
#t 1997
#c 5
#% 86950
#% 142639
#% 227856
#% 463414
#% 481956
#% 571100
#! S3 is the prototype of a database system supporting the management and similarity retrieval of industrial CAD parts. The major goal of the system is to reduce the cost for developing and producing new parts by maximizing the reuse of existing parts. S3 supports the following three types of similarity queries: query by example (of an existing part in the database), query by sketch and thematic similarity query. S3 is an object-oriented system offering an adequate graphical user interface. On top of providing various state-of-the-art algorithms and index structures for geometry-based similarity retrieval, it is an excellent testbed for developing and testing new similarity algorithms and index structures.

#index 228000
#* PREDATOR: an OR-DBMS with enhanced data types
#@ Praveen Seshadri;Mark Paskin
#t 1997
#c 5
#% 116043
#% 152902
#% 172939
#% 411554
#% 442705
#% 464007
#% 481428
#% 482088
#% 603136
#% 648890

#index 228001
#* Sentinel: an object-oriented DBMS with event-based rules
#@ S. Chakravarthy
#t 1997
#c 5
#% 152921
#% 177755
#% 464049
#% 481448

#index 228003
#* The MENTOR workbench for enterprise-wide workflow management
#@ Dirk Wodtke;Jeanine Weissenfels;Gerhard Weikum;Angelika Kotz Dittrich;Peter Muth
#t 1997
#c 5
#% 101955
#% 185412
#% 252361
#% 365338
#% 461900
#% 565496
#! MENTOR (“Middleware for Enterprise-Wide Workflow Management”) is a joint project of the University of the Saarland, the Union Bank of Switzerland, and ETH Zurich [1, 2, 3]. The focus of the project is on enterprise-wide workflow management. Workflows in this category may span multiple organizational units each unit having its own workflow server, involve a variety of heterogeneous information systems, and require many thousands of clients to interact with the workflow management system (WFMS). The project aims to develop a scalable and highly available environment for the execution and monitoring of workflows, seamlessly integrated with a specification and verification environment.For the specification of workflows, MENTOR utilizes the formalism of state and activity charts. The mathematical rigor of the specification method establishes a basis for both correctness reasoning and for partitioning of a large workflow into a number of subworkflows according to the organizational responsibilities of the enterprise. For the distributed execution of the partitioned workflow specification, MENTOR relies mostly on standard middleware components and adds own components only where the standard components fall short of functionality or scalability. In particular, the run-time environment is based on a TP monitor and a CORBA implementation.

#index 228007
#* Zoo: a desktop experiment management environment
#@ Yannis E. Ioannidis;Miron Livny;Anastassia Ailamaki;Anand Narayanan;Andrew Therber
#t 1997
#c 5
#% 108505
#% 169944
#% 479593
#% 481750
#% 481950
#% 503710
#% 562125
#% 725461

#index 236409
#* A query language for a Web-site management system
#@ Mary Fernandez;Daniela Florescu;Alon Levy;Dan Suciu
#t 1997
#c 5

#index 236410
#* Quasi-cubes: exploiting approximations in multidimensional databases
#@ Daniel Barbará;Mark Sullivan
#t 1997
#c 5
#! A data cube is a popular organization for summary data. A cube is simply a multidimensional structure that contains at each point an aggregate value, i.e., the result of applying an aggregate function to an underlying relation. In practical situations, cubes can require a large amount of storage. The typical approach to reducing storage cost is to materialize parts of the cube on demand. Unfortunately, this lazy evaluation can be a time-consuming operation.In this paper, we describe an approximation technique that reduces the storage cost of the cube without incurring the run time cost of lazy evaluation. The idea is to provide an incomplete description of the cube and a method of estimating the missing entries with a certain level of accuracy. The description, of course, should take a fraction of the space of the full cube and the estimation procedure should be faster than computing the data from the underlying relations. Since cubes are used to support data analysis and analysts are rarely interested in the precise values of the aggregates (but rather in trends), providing approximate answers is, in most cases, a satisfactory compromise.Alternatively, the technique can be used to implement a multiresolution system in which a tradeoff is established between the execution time of queries and the errors the user is willing to tolerate. By only going to the disk when it is necessary (to reduce the errors), the query can be executed faster. This idea can be extended to produce a system that incrementally increases the accuracy of the answer while the user is looking at it, supporting on-line aggregation.

#index 236411
#* OGDI: toward interoperability among geospatial databases
#@ Gilles Clément;Christian Larouche;Denis Gouin;Paul Morin;Henry Kucera
#t 1997
#c 5
#! The growth of the geomatics industry is stunted by the difficulty of obtaining and transforming suitable spatial data. This paper describes a remedy: the Open Geospatial Datastore Interface (OGDI), which permits application software to access a variety of spatial data products. The discussion compares the OGDI approach to other standards efforts and describes the characteristics and use of OGDI, which is in the public domain.

#index 236412
#* An extended entity-relationship model for geographic applications
#@ Thanasis Hadzilacos;Nectaria Tryfona
#t 1997
#c 5
#! A special-purpose extension of the Entity-Relationship model for the needs of conceptual modeling of geographic applications, called the Geo-ER Model, is presented. Handling properties associated to objects not because of the objects' nature but because of the objects' position, calls for dealing -at the semantic modeling level-with space, location and dimensionality of objects, spatial relationships, space-depending attributes, and scale and generalization of representations. In order to accomplish this in the framework of ER and its derivatives, we introduce special entity sets, relationships, and add new constructs. The rationale as well as examples of usage of the Geo-ER model from actual projects are presented.

#index 236413
#* Asserting beliefs in MLS relational models
#@ Nenad A. Jukic;Susan V. Vrbsky
#t 1997
#c 5
#! Multilevel relations, based on the current multilevel secure (MLS) relational data models, can present a user with information that is difficult to interpret and may display an inconsistent outlook about the views of other users. Such ambiguity is due to the lack of a comprehensive method for asserting and interpreting beliefs about lower level information. In this paper we identify different beliefs that can be held by higher level users about lower level information, and we introduce the new concept of a mirage tuple. We present a mechanism for asserting beliefs about all accessible tuples, including lower level tuples. This mechanism provides every user of an MLS database with an unambiguous interpretation of all viewable information and presents a consistent account of the views at all levels below the user's level.

#index 236414
#* Database systems—breaking out of the box
#@ Avi Silberschatz;Stan Zdonik
#t 1997
#c 5

#index 236415
#* Report on DART '96: databases: active and real-time (concepts meet practice)
#@ Krithi Ramamritham;Nandit Soparkar
#t 1997
#c 5
#! DART '96 was held in conjunction with the Conference of Information and Knowledge Management (CIKM) on Nov 15th in Baltimore. Its goal was to provide a forum for researchers and practitioners involved in integrating concepts and technologies from active and real-time databases to discuss the state of the art and chart a course of action. To this end, nine speakers from academia, industry, and research laboratories were invited to provide a perspective on the theory and practice underlying active real-time databases. In addition, some selected papers were presented briefly to complement the invited speakers' talks. The second half of the workshop was devoted to discussions aimed at identifying the problems that still need to be addressed in the contexts of the diverse target applications.

#index 236416
#* Lore: a database management system for semistructured data
#@ Jason McHugh;Serge Abiteboul;Roy Goldman;Dallas Quass;Jennifer Widom
#t 1997
#c 5
#! Lore (for Lightweight Object Repository) is a DBMS designed specifically for managing semistructured information. Implementing Lore has required rethinking all aspects of a DBMS, including storage management, indexing, query processing and optimization, and user interfaces. This paper provides an overview of these aspects of the Lore system, as well as other novel features such as dynamic structural summaries and seamless access to data from external sources.

#index 236417
#* Research in databases and data-intensive applications: Computer Science Dept. and FIZ, University of Karlsruhe
#@ Brigitta König-Ries;Peter C. Lockermann
#t 1997
#c 5
#% 212556
#% 223772
#% 227964
#% 477491
#% 482119
#% 511730
#% 511741
#% 543201
#% 591550
#% 592801
#% 631832
#! The future world of computing will be governed by large networks of communicating and interacting persons and machines, geographic mobility, temporary attachment to networks, the disintegration of formerly monolithic organizations and systems into autonomously acting units, the substitution of cooperation regimes for centralized control, and an ever-increasing spectrum of ever more ambitious applications. In such a world the methods, techniques and tools of database technology will play new and more diversified roles, not so much in combination as parts of all-inclusive database systems but rather individually as indispensable ingredients of or desirable enhancements to novel communication, control and application systems. The two information systems groups whose work is presented in this report aim at meeting these new challenges. Our contributions are in the large field of what we call “distributed data-intensive applications”.

#index 237177
#* Proceedings of the sixteenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems
#@ Alberto Mendelzon;Z. Meral Özsoyoglu
#t 1997
#c 5

#index 237179
#* Multimedia support for databases
#@ Banu Özden;Rajeev Rastogi;Avi Silberschatz
#t 1997
#c 5
#% 43172
#% 67485
#% 83981
#% 86950
#% 107702
#% 124017
#% 149275
#% 151340
#% 159080
#% 159084
#% 159275
#% 172881
#% 194009
#% 201893
#% 201932
#% 204542
#% 210171
#% 285932
#% 427199
#% 437405
#% 437407
#% 437509
#% 461898
#% 462075
#% 464195
#% 481438
#% 632238
#% 632261
#% 661673
#% 661707

#index 237180
#* On the complexity of database queries (extended abstract)
#@ Christos H. Papadimitriou;Mihalis Yannakakis
#t 1997
#c 5
#% 36181
#% 36683
#% 101956
#% 136230
#% 189741
#% 190254
#% 191584
#% 191611
#% 220977
#% 285967
#% 384978
#% 593756
#% 598376
#% 599549

#index 237181
#* Deciding containment for queries with complex objects (extended abstract)
#@ Alon Y. Levy;Dan Suciu
#t 1997
#c 5
#% 27056
#% 34242
#% 36181
#% 36683
#% 43885
#% 53400
#% 93129
#% 101951
#% 109995
#% 114580
#% 122396
#% 123085
#% 123118
#% 137864
#% 137865
#% 137867
#% 140410
#% 146882
#% 164364
#% 189868
#% 198465
#% 210214
#% 235914
#% 289266
#% 384978
#% 435157
#% 464056
#% 477212
#% 480091
#% 481128
#% 481288
#% 481293
#% 562132
#% 565259
#% 599549

#index 237182
#* On the containment and equivalence of database queries with linear constraints (extended abstract)
#@ Oscar H. Ibarra;Jianwen Su
#t 1997
#c 5
#% 36181
#% 36683
#% 123118
#% 126329
#% 137867
#% 145167
#% 145194
#% 164376
#% 164418
#% 185883
#% 190332
#% 191590
#% 205851
#% 213950
#% 213952
#% 213982
#% 224744
#% 287336
#% 287631
#% 289114
#% 289378
#% 322880
#% 384978
#% 476994
#% 587330
#% 587366
#% 599549
#% 600507

#index 237183
#* Conjunctive query equivalence of keyed relational schemas (extended abstract)
#@ Joseph Albert;Yanis Ioannidis;Raghu Ramakrishnan
#t 1997
#c 5
#% 11284
#% 22948
#% 85086
#% 286852
#% 287008
#% 322880
#% 415949
#% 416016
#% 480969
#% 534555

#index 237184
#* Managing semantic heterogeneity in databases: a theoretical prospective
#@ Richard Hull
#t 1997
#c 5
#% 319
#% 838
#% 2035
#% 3673
#% 11284
#% 13016
#% 22948
#% 36683
#% 50073
#% 55882
#% 58347
#% 58356
#% 83228
#% 83526
#% 85086
#% 85088
#% 85089
#% 86930
#% 86938
#% 101649
#% 102547
#% 102780
#% 103739
#% 111920
#% 116091
#% 116303
#% 117900
#% 121628
#% 123121
#% 125557
#% 128329
#% 135476
#% 137863
#% 142231
#% 152928
#% 153012
#% 154314
#% 156337
#% 158896
#% 159110
#% 166203
#% 168676
#% 168683
#% 168712
#% 168741
#% 168746
#% 169052
#% 169054
#% 185412
#% 198082
#% 198465
#% 198479
#% 201928
#% 201929
#% 201930
#% 209729
#% 210192
#% 210208
#% 210210
#% 210211
#% 213442
#% 213445
#% 213969
#% 213981
#% 213982
#% 227987
#% 235914
#% 237183
#% 237191
#% 277344
#% 282431
#% 286901
#% 286916
#% 289369
#% 289384
#% 339754
#% 369870
#% 384978
#% 435100
#% 458556
#% 461900
#% 462802
#% 464035
#% 464687
#% 464717
#% 464720
#% 464724
#% 480259
#% 480623
#% 480771
#% 480961
#% 480969
#% 481106
#% 481614
#% 481923
#% 482083
#% 535810
#% 565496
#% 571217
#% 614579

#index 237185
#* Complete geometrical query languages (extended abstract)
#@ Marc Gyssens;Jan Van den Bussche;Dirk Van Gucht
#t 1997
#c 5
#% 102547
#% 164406
#% 190332
#% 213966

#index 237186
#* On the decidability of semi-linearity for semi-algebraic sets and its implications for spatial databases (extended abstract)
#@ Freddy Dumortier;Marc Gyssens;Luc Vandeurzen;Dirk Van Gucht
#t 1997
#c 5
#% 3506
#% 24070
#% 26385
#% 55901
#% 99443
#% 163438
#% 164406
#% 190332
#% 213950
#% 476994
#% 477211
#% 477218
#% 485127
#% 527014
#% 542470

#index 237187
#* A cost model for nearest neighbor search in high-dimensional data space
#@ Stefan Berchtold;Christian Böhm;Daniel A. Keim;Hans-Peter Kriegel
#t 1997
#c 5
#% 2115
#% 4430
#% 17857
#% 64431
#% 86950
#% 102772
#% 121989
#% 131061
#% 169940
#% 198573
#% 201876
#% 217292
#% 238764
#% 317313
#% 317380
#% 463414
#% 464859
#% 481956
#% 527026
#% 668747

#index 237188
#* Languages for relational databases over interpreted structures
#@ Michael Benedikt;Leonid Libkin
#t 1997
#c 5
#% 6249
#% 36683
#% 73579
#% 84275
#% 101646
#% 101650
#% 123079
#% 137871
#% 137916
#% 145232
#% 173860
#% 190332
#% 191611
#% 213950
#% 213952
#% 219805
#% 231873
#% 245655
#% 277325
#% 384978
#% 534163
#% 544431
#% 552970
#% 587330
#% 587366
#% 835733

#index 237189
#* Rewriting queries using views in description logics
#@ Catriel Beeri;Alon Y. Levy;Marie-Christine Rousset
#t 1997
#c 5
#% 108697
#% 145400
#% 198465
#% 201898
#% 205398
#% 213437
#% 213982
#% 237190
#% 340619
#% 442977
#% 461605
#% 464056
#% 464203
#% 464717
#% 481444
#% 481923
#% 556363
#% 564419
#% 1499551

#index 237190
#* Answering recursive queries using views
#@ Oliver M. Duschka;Michael R. Genesereth
#t 1997
#c 5
#% 23898
#% 36683
#% 64424
#% 198465
#% 198466
#% 213982
#% 296931
#% 464056
#% 464203
#% 464717
#% 481923
#% 564419
#% 599549
#% 672627
#% 1499471

#index 237191
#* Semistructured data
#@ Peter Buneman
#t 1997
#c 5
#% 64902
#% 116090
#% 116091
#% 189868
#% 210214
#% 227995
#% 235914
#% 237193
#% 255197
#% 340295
#% 459260
#% 462062
#% 463919
#% 464719
#% 464720
#% 464724
#% 481125
#% 481614
#% 481935
#% 562141
#% 614598

#index 237192
#* Regular path queries with constraints
#@ Serge Abiteboul;Victor Vianu
#t 1997
#c 5
#% 11797
#% 23896
#% 23908
#% 66097
#% 101945
#% 116091
#% 197751
#% 210214
#% 268797
#% 288434
#% 340295
#% 384978
#% 404772
#% 408396
#% 463919
#% 464719
#% 464720
#% 481602
#% 481935
#% 482083
#% 503749
#% 587466

#index 237193
#* Formal models of Web queries
#@ Alberto O. Mendelzon;Tova Milo
#t 1997
#c 5
#% 57480
#% 64902
#% 91364
#% 115436
#% 116588
#% 172927
#% 210214
#% 340295
#% 460856
#% 464719
#% 481125
#% 481586
#% 481602
#% 614598

#index 237194
#* Cut and paste
#@ Paolo Atzeni;Giansalvatore Mecca
#t 1997
#c 5
#% 101925
#% 115515
#% 161722
#% 163445
#% 164378
#% 164419
#% 172927
#% 179696
#% 191577
#% 201934
#% 210214
#% 210388
#% 237193
#% 321327
#% 340295
#% 362115
#% 404772
#% 464551
#% 464719
#% 464720
#% 480084
#% 481125
#% 481602
#% 614598

#index 237195
#* Stochastic service guarantees for continuous data on multi-zone disks
#@ Guido Nerjes;Peter Muth;Gerhard Weikum
#t 1997
#c 5
#% 70050
#% 79312
#% 83981
#% 149275
#% 151340
#% 159079
#% 172881
#% 173595
#% 184516
#% 187079
#% 201680
#% 201824
#% 360808
#% 374002
#% 434670
#% 437335
#% 443080
#% 614607
#% 632238
#% 661666
#% 661705
#% 1797481

#index 237196
#* Epidemic algorithms in replicated databases (extended abstract)
#@ D. Agrawal;A. El Abbadi;R. C. Steinke
#t 1997
#c 5
#% 7563
#% 9241
#% 35764
#% 36180
#% 64147
#% 68207
#% 116717
#% 202146
#% 210179
#% 213978
#% 287303
#% 318258
#% 320187
#% 411707
#% 416023
#% 442722
#% 452805
#% 458546
#% 602675
#% 602800
#% 602842
#% 778539

#index 237197
#* Replication and consistency: being lazy helps sometimes
#@ Yuri Breitbart;Henry F. Korth
#t 1997
#c 5
#% 9241
#% 83125
#% 102804
#% 210179
#% 318399
#% 380821
#% 403195
#% 415993
#% 435104
#% 461887
#% 461902
#% 531918
#% 602640

#index 237198
#* OLAP and statistical databases: similarities and differences
#@ Arie Shoshani
#t 1997
#c 5
#% 77312
#% 184254
#% 210182
#% 214766
#% 227868
#% 227880
#% 286236
#% 287359
#% 461921
#% 463760
#% 464215
#% 482049
#% 503701
#% 503731
#% 852995

#index 237199
#* Correctness and parallelism in composite systems
#@ Gustavo Alonso;Stephen Blott;Armin Fessler;Hans-Jörg Schek
#t 1997
#c 5
#% 9241
#% 13017
#% 54033
#% 56455
#% 77982
#% 78665
#% 91076
#% 111924
#% 112319
#% 114582
#% 116063
#% 122904
#% 122917
#% 137940
#% 166215
#% 210196
#% 317988
#% 340308
#% 435120
#% 458568
#% 462019
#% 463101
#% 463755
#% 464688
#% 480791
#% 481955
#% 614572
#% 676592

#index 237200
#* Data mining, hypergraph transversals, and machine learning (extended abstract)
#@ Dimitrios Gunopulos;Heikki Mannila;Roni Khardon;Hannu Toivonen
#t 1997
#c 5
#% 6710
#% 125557
#% 152934
#% 169370
#% 175383
#% 191680
#% 197754
#% 210345
#% 221328
#% 232106
#% 232136
#% 450951
#% 451056
#% 464714
#% 1272175

#index 237201
#* Referential actions as logical rules
#@ Bertram Ludäscher;Wolfgang May;Georg Lausen
#t 1997
#c 5
#% 101646
#% 166202
#% 198469
#% 417546
#% 463267
#% 481952
#% 542450
#% 553827

#index 237202
#* Partial-sum queries in OLAP data cubes using covering codes
#@ Ching-Tien Ho;Jehoshua Bruck;Rakesh Agrawal
#t 1997
#c 5
#% 113841
#% 210182
#% 227866
#% 442685
#% 442695
#% 462204
#% 464215
#% 481288
#% 481604
#% 481608
#% 481948
#% 481951

#index 237203
#* On the complexity of generating optimal plans with cross products (extended abstract)
#@ Wolfgang Scheufele;Guido Moerkotte
#t 1997
#c 5
#% 554
#% 70370
#% 83154
#% 93534
#% 102765
#% 210166
#% 408396
#% 411554
#% 464700
#% 479938
#% 481110

#index 237204
#* On the analysis of indexing schemes
#@ Joseph M. Hellerstein;Elias Koutsoupias;Christos H. Papadimitriou
#t 1997
#c 5
#% 672
#% 1451
#% 69802
#% 84726
#% 86951
#% 88056
#% 137887
#% 137893
#% 164360
#% 164362
#% 214112
#% 217825
#% 252608
#% 281731
#% 285932
#% 289316
#% 317933
#% 359751
#% 407822
#% 411694
#% 427199
#% 480093
#% 481599
#% 481620
#% 481782
#% 481941

#index 237205
#* Processing queries by linear constraints
#@ Jonathan Goldstein;Raghu Ramakrishnan;Uri Shaft;Jie-Bing Yu
#t 1997
#c 5
#% 2115
#% 41922
#% 78359
#% 83319
#% 86950
#% 137425
#% 411694
#% 427199
#% 480093
#% 481428

#index 244102
#* Management of semistructured data
#@ Dan Suciu
#t 1997
#c 5
#! A huge amount of data is available today on the Internet, or on the private Intranets of many companies. This data is structured in a multitude of ways. At an extreme we find data coming from traditional relational or object-oriented databases, with a completely known structure. At another extreme we have data which is fully unstructured, such as images, sounds, and raw text. But most of the data falls somewhere in between these two extremes, for a variety of reasons: the data may be structured, but the structure is not know to the user; the user may know the structure, but chooses to ignore it, for browsing purposes; the structure may be implicit, such as in formatted text, and is not as rigid and regular as in traditional databases; the data may be in non-traditional formats, such as the ASN.1 exchange format; the schema of the data is huge and changes often, so that we may prefer to ignore it. Several researchers have worked recently on problems related to data fitting this description, and have coined the term semistructured data for it. Two recent tutorials [Abi97, Bun97] contain an excellent introduction to semistructured data and a comprehensive bibliography on this new research topic.

#index 244103
#* Wrapper generation for semi-structured Internet sources
#@ Naveen Ashish;Craig A. Knoblock
#t 1997
#c 5
#! With the current explosion of information on the World Wide Web (WWW) a wealth of information on many different subjects has become available on-line. Numerous sources contain information that can be classified as semi-structured. At present, however, the only way to access the information is by browsing individual pages. We cannot query web documents in a database-like fashion based on their underlying structure. However, we can provide database-like querying for semi-structured WWW sources by building wrappers around these sources. We present an approach for semi-automatically generating such wrappers. The key idea is to exploit the formatting information in pages from the source to hypothesize the underlying structure of a page. From this structure the system generates a wrapper that facilitates querying of a source and possibly integrating it with other sources. We demonstrate the ease with which we are able to build wrappers for a number of internet sources in different domains using our implemented wrapper generation toolkit.

#index 244105
#* Semistructured and structured data in the Web: going back and forth
#@ Paolo Atzeni;Giansalvatore Mecca;Paolo Merialdo
#t 1997
#c 5

#index 244107
#* Integrating dynamically-fetched external information into a DBMS for semistructured data
#@ Jason McHugh;Jennifer Widom
#t 1997
#c 5
#! We describe the external data manager component of the Lore database system for semistructured data. Lore's external data manager enables dynamic retrieval and integration of data from arbitrary, heterogeneous external sources during query processing. The distinction between Lore-resident and external data is invisible to the user. We introduce a flexible notion of arguments that limits the amount of data fetched from an external source, and we have incorporated optimizations to reduce the number of calls to an external source.

#index 244108
#* Extracting entity profiles from semistructured information spaces
#@ Robert A. Nado;Scott B. Huffman
#t 1997
#c 5
#! A semistructured information space consists of multiple collections of textual documents containing fielded or tagged sections. The space can be highly heterogeneous, because each collection has its own schema, and there are no enforced keys or formats for data items across collections. Thus, structured methods like SQL cannot be easily employed, and users often must make do with only full-text search. In this paper, we describe an approach that provides structured querying for particular types of entities, such as companies and people. Entity-based retrieval is enabled by normalizing entity references in a heuristic, type-dependent manner. The approach can be used to retrieve documents and can also be used to construct entity profiles — summaries of commonly sought information about an entity based on the documents' content. The approach requires only a modest amount of meta-information about the source collections, much of which is derived automatically.

#index 244109
#* Inferring structure in semistructured data
#@ Svetlozer Nestorov;Serge Abiteboul;Rajeev Motwani
#t 1997
#c 5
#! When dealing with semistructured data such as that available on the Web, it becomes important to infer the inherent structure, both for the user (e.g., to facilitate querying) and for the system (e.g., to optimize access). In this paper, we consider the problem of identifying some underlying structure in large collections of semistructured data. Since we expect the data to be fairly irregular, this structure consists of an approximate classification of objects into a hierarchical collection of types. We propose a notion of a type hierarchy for such data, and outline a method for deriving the type hierarchy, and rules for assigning types to data elements.

#index 244112
#* Workshop on workflow management in scientific and engineering applications—report
#@ R. McClatchey;G. Vossen
#t 1997
#c 5

#index 244114
#* Research issues in federated database systems: report of EFDBS '97 workshop
#@ S. Conrad;B. Eaglestone;W. Hasselbring;M. Roantree;M. Schöhoff;M. Strässler;M. Vermeer;F. Saltor
#t 1997
#c 5
#! In June 1997, an international workshop on engineering of federated database systems has been held in Barcelona in conjunction with the 9th Conference on Advanced Information Systems Engineering (CAiSE'97). This paper reports on the results of this workshop and summarises the identified open issues for future research in this area.

#index 244118
#* Virtual database technology
#@ Ashish Gupta;Venky Harinarayan;Anand Rajaraman
#t 1997
#c 5

#index 244119
#* The five-minute rule ten years later, and other computer storage rules of thumb
#@ Jim Gray;Goetz Graefe
#t 1997
#c 5
#! Simple economic and performance arguments suggest appropriate lifetimes for main memory pages and suggest optimal page sizes. The fundamental tradeoffs are the prices and bandwidths of RAMs and disks. The analysis indicates that with today's technology, five minutes is a good lifetime for randomly accessed pages, one minute is a good lifetime for two-pass sequentially accessed pages, and 16 KB is a good size for index pages. These rules-of-thumb change in predictable ways as technology ratios change. They also motivate the importance of the new Kaps, Maps, Scans, and $/Kaps, $/Maps, $/TBscan metrics.

#index 244120
#* Information systems research at George Mason University
#@ Sushil Jajodia;Daniel Barbará;Alex Brodsky;Larry Kerschberg;Ami Motro;Edgar Sibley;X. Sean Wang
#t 1997
#c 5
#% 186970
#% 191586
#% 201872
#% 201923
#% 211090
#% 211987
#% 213963
#% 214667
#% 225003
#% 225005
#% 225006
#% 227956
#% 257087
#% 257855
#% 359127
#% 383646
#% 419913
#% 442877
#% 481113
#% 489367
#% 503693
#% 503700
#% 503724
#% 543194
#% 632116
#% 664551
#% 664665
#% 664674
#! George Mason University began as an independent state university in 1972. Its development has been marked by rapid growth and innovative planning, resulting in an enrollment of more than 24,000 students in 1997. It is located in Fairfax, Virginia—about fifteen miles southwest of Washington, DC—near many governmental agencies and industrial firms specializing in information-intensive products and services.Information and Software Systems Engineering (ISSE) is one of six departments in GMU's School of Information Technology and Engineering (SITE). Established in 1985, SITE has approximately 90 faculty and ISSE has 13 full time faculty. ISSE is a rapidly growing department with wide-ranging teaching and research interests. The department offers no undergraduate degree programs and Master of Science degrees in Information Systems (MSIS) and Software Engineering (SWSE). MSIS has about 800 students and the SWSE has approximately 400 students enrolled. The MSIS program graduates about 120 students and the SWSE program awards 40 degrees per year. ISSE faculty participate in the SITE doctoral program in Information Technology. ISSE Faculty chair the committees of more than one third of the doctoral students in the SITE program, which currently graduates about 30 PhDs per year.Two research centers are associated with the department: The Center for Secure Information Systems (Sushil Jajodia, Director) and the Center for Information Systems Integration and Evolution (Larry Kerschberg, Director).Departmental research in information systems is supported by grants and contracts from several sources. The following awards have been received so far for the academic year 1997-1998 and beyond: Knowledge Rovers: A Family of Intelligent Software Agents for Logistics for the Warrior. Defense Advanced Research Projects Agency (co-PIs: Kerschberg, Gomaa, Jajodia, Motro)Electronic Commerce for Logistics, Teaming Agreement with American Management Systems for DARPA BAA 95-25 Logistics Research and Development (co-PIs: Kerschberg, Gomaa, Jajodia, Motro)Linear Constraint Databases, NSF Research Initiation Award (PI: Brodsky)Linear Constraint Programming, ONR (co-PI: Brodsky with late Kannelakis (PI), Van Hentenryck, and Lassez)Towards Expressive and Efficient Queries on Sequenced Data, NSF Research Initiation Award (PI: Wang)Supporting Multiple time granularities in Query Evaluation and Data Mining, NSF (co-PIs: Jajodia, Wang)Fine Granularity Access Controls in World Wide Web, NSA (PI: Jajodia)Information Flow Control in Object-Oriented Systems NSA (PI: Jajodia)Exploring Steganography: Seeing the Unseen, NSA (PI: Jajodia)Trusted Recovery from Information Attacks, Rome Laboratory (co-PIs: Jajodia, Ammann)A Unified Framework for Supporting Multiple Access Control Policies, DARPA (PI: Jajodia)The remainder of this article provides a brief overview of our research followed by a selected list of publications. More detailed information is available at www.isse.gmu.edu.

#index 244121
#* The database and information system research group at the University of ULM
#@ Peter Dadam;Wolfgang Klas
#t 1997
#c 5
#! The University of Ulm was founded in 1967 with focus on medicine and natural sciences. In 1989 the University established two new faculties: Engineering Sciences and Computer Science. This enlargement took place within the framework of the so-called Science City Ulm. In a joint effort, the State of Baden-Württemberg, industrial companies, the University, and the City of Ulm successfully established a research and development infrastructure at or nearby the university campus consisting of the university's research labs, university-related research institutes like the Research Institute for Applied Knowledge Processing (FAW), and industrial research and development labs, especially a large research center of Daimler-Benz AG.Today, the Faculty of Computer Science consists of seven divisions (called 'departments'), each of which equipped with two professor positions:Theoretical Computer ScienceArtificial IntelligenceDistributed SystemsDatabases and Information SystemsSoftware Technology and Compiler ConstructionComputer StructuresNeural Information Processing. The Dept. of Databases and Information Systems (DBIS) became operational at the beginning of 1990 when Peter Dadam joined the faculty. He came from the IBM Heidelberg Science Center (HDSC) where he managed the research department for Advanced Information Management (AIM). At the HDSC he was working on advanced database technology and applications and contributed to the development of the AIM-P system (see [1]). The second professor position was first occupied by Marc Scholl, who belonged to the DBIS department from 1992 to 1994. In 1996 Wolfgang Klas joined the DBIS department as second professor. He came from the GMD Institute for Integrated Publication and Information Systems (IPSI) where he managed the research division Distributed Multimedia Information Systems and was working on advanced object-oriented database systems technology, interoperable database systems, and multimedia information systems.At present, the DBIS team consists of the teaching and research assistants Thomas Bauer, Susanne Boll, Christian Heinlein, Clemens Hensinger, Erich Müller, Manfred Reichert, Birgit Schulthei&bgr;, the system engineer Rudi Seifert, the secretary Christiane Köppl, and the doctoral students Thomas Beuter and Anita Krämer.In the following, we concentrate on the research and development work performed previously and presently in the research groups of Peter Dadam and of Wolfgang Klas. For references to Marc Scholl's work please visit http://www.informatik.uni-konstanz.de/dbis.

#index 245992
#* Workshop report on experiences using object data management in the real-world
#@ Akmal B. Chaudhri
#t 1998
#c 5
#% 234243
#! The OOPSLA '97 Workshop on Experiences Using Object Data Management in the Real-World was held at the Cobb Galleria Centre in Atlanta, Georgia on Monday 6 October 1997. This report summarises some of the commercial case-study presentations made by workshop participants.

#index 245994
#* Report on the second IEEE metadata conference (Metadata '97)
#@ Ron Musick;Chris Miller
#t 1998
#c 5
#! On September 15th and 16th, 1997 the Second IEEE Metadata Conference was held at the National Oceanic and Atmospheric Administration (NOAA) complex in Silver Spring, Maryland. The main objectives of this conference series are to provide a forum to address metadata issues faced by various communities, promote the interchange of ideas on common technologies and standards related to metadata, and facilitate the development and usage of metadata. Metadata'97 met these objectives, drawing about 280 registered attendees from ten different countries and over one hundred different institutions. The audience included scientists, information technology specialists, and librarians from communities as widespread as finance, climatology, and mass storage. The technical program included two keynote addresses, two panel presentations, as well as twenty-three papers and thirteen posters selected from over one hundred abstracts. We provide highlights of the conference below. For more details, the proceedings are available electronically from the conference homepage at: http://www.llnl.gov/liv_comp/metadata/md97.html.The keynote addresses were "An Architecture for Metadata: The Dublin Core, and why you don't have to like it" by Stuart Weibel, OCLC, and "The Microsoft Repository" by Philip Bernstein, Microsoft.Weibel's talk described the Dublin core and the Warwick framework - a series of workshops whose output has been a core set of metadata elements common to data from most domains, along with a "container" based mechanism for plugging in larger domain-specific sets of metadata, like the FGDC's standard for geospatial metadata. These efforts represent two of the defining works in this community. Weibel touched on the history of this effort and described his belief that standards such as RDF (resource description framework) for the WWW coming from organizations like the World Wide Web Consortium (W3C) will have a major influence on the metadata community in the near future.Bernstein's talk covered the Microsoft Object Repository, which also has the potential for a large impact on what metadata gets stored and how they are managed. Bernstein describes the repository as "a place to persist COM objects" (component object model), and as more than just a object-oriented database. The features of a true repository are 1) objects and properties, 2) rich relational semantics, 3) extensibility, and 4) versioning. Repositories are used to help tools interoperate by storing predefined "information models". The information models are the metadata used to describe the underlying COM objects in a standard way such that the objects can be shared across tool boundaries. The main consumers of this type of technology are tool vendors.

#index 245996
#* PREDATOR: a resource for database research
#@ Praveen Seshadri
#t 1998
#c 5
#! This paper describes PREDATOR, a freely available object-relational database system that has been developed at Cornell University. A major motivation in developing PREDATOR was to create a modern code base that could act as a research vehicle for the database community. Pursuing this goal, this paper briefly describes several features of the system that should make it attractive for database research and education.

#index 245998
#* Materialized views and data warehouses
#@ Nick Roussopoulos
#t 1998
#c 5
#! A data warehouse is a redundant collection of data replicated from several possibly distributed and loosely coupled source databases, organized to answer OLAP queries. Relational views are used both as a specification technique and as an execution plan for the derivation of the warehouse data. In this position paper, we summarize the versatility of relational views and their potential.

#index 245999
#* Applications of JAVA programming language to database management
#@ Bradley F. Burton;Victor W. Marek
#t 1998
#c 5

#index 246000
#* Unbundling active functionality
#@ Stella Gatziu;Arne Koschel;Günter von Bültzingsloewen;Hans Fritschi
#t 1998
#c 5
#! New application areas or new technical innovations expect from database management systems more and more new functionality. However, adding functions to the DBMS as an integral part of them, tends to create monoliths that are difficult to design, implement, validate, maintain and adapt. Such monoliths can be avoided if one configures DBMS according to the actually needed functionality. In order to identify the basic functional components for the configuration the current monoliths should be broken up into smaller units, or in other words they could be "unbundled". In this paper we apply unbundling to active database systems. This results in a new form of active mechanisms where active functionality is no longer an integral part of the DBMS functionality. This allows the use of active capabilities with any arbitrary DBMS and in broader contexts. Furthermore, it allows the adaption of the active functionality to the application profile. Such aspects are crucial for a wide use of active functionality in real (database or not) applications.

#index 246002
#* Mining fuzzy association rules in databases
#@ Chan Man Kuok;Ada Fu;Man Hon Wong
#t 1998
#c 5
#! Data mining is the discovery of previously unknown, potentially useful and hidden knowledge in databases. In this paper, we concentrate on the discovery of association rules. Many algorithms have been proposed to find association rules in databases with binary attributes. We introduce the fuzzy association rules of the form, 'If X is A then Y is B', to deal with quantitative attributes. X, Y are set of attributes and A, B are fuzzy sets which describe X and Y respectively. Using the fuzzy set concept, the discovered rules are more understandable to human. Moreover, fuzzy sets handle numerical values better than existing methods because fuzzy sets soften the effect of sharp boundaries.

#index 246003
#* An extensible notation for spatiotemporal index queries
#@ Vassilis J. Tsotras;Christian S. Jensen;Richard Thomas Snodgrass
#t 1998
#c 5
#! Temporal, spatial and spatiotemporal queries are inherently multidimensional, combining predicates on explicit attributes with predicates on time dimension(s) and spatial dimension(s). Much confusion has prevailed in the literature on access methods because no consistent notation exists for referring to such queries. As a contribution towards eliminating this problem, we propose a new and simple notation for spatiotemporal queries. The notation aims to address the selection-based spatiotemporal queries commonly studied in the literature of access methods. The notation is extensible and can be applied to more general multidimensional, selection-based queries.

#index 246008
#* T2: a customizable parallel database for multi-dimensional data
#@ Chialin Chang;Anurag Acharya;Alan Sussman;Joel Saltz
#t 1998
#c 5
#! As computational power and storage capacity increase, processingand analyzing large volumes of data play an increasingly importantpart in many domains of scientific research. Typical examples oflarge scientific datasets include long running simulations oftime-dependent phenomena that periodically generate snapshots oftheir state (e.g. hydrodynamics and chemical transport simulationfor estimating pollution impact on water bodies [4, 6, 20],magnetohydrodynamics simulation of planetary magnetospheres [32],simulation of a flame sweeping through a volume [28], airplane wakesimulations [21]), archives of raw and processed remote sensingdata (e.g. AVHRR [25], Thematic Mapper [17], MODIS [22]), andarchives of medical images (e.g. confocal light microscopy, CTimaging, MRI, sonography).These datasets are usually multi-dimensional. The datadimensions can be spatial coordinates, time, or experimentalconditions such as temperature, velocity or magnetic field. Theimportance of such datasets has been recognized by several databaseresearch groups and vendors, and several systems have beendeveloped for managing and/or visualizing them [2, 7, 14, 19, 26,27, 29, 31].These systems, however, focus on lineage management, retrievaland visualization of multi-dimensional datasets. They providelittle or no support for analyzing or processing these datasets --the assumption is that this is too application-specific to warrantcommon support. As a result, applications that process thesedatasets are usually decoupled from data storage and management,resulting in inefficiency due to copying and loss of locality.Furthermore, every application developer has to implement complexsupport for managing and scheduling the processing.Over the past three years, we have been working with severalscientific research groups to understand the processingrequirements for such applications [1, 5, 6, 10, 18, 23, 24, 28].Our study of a large set of applications indicates that theprocessing for such datasets is often highly stylized and sharesseveral important characteristics. Usually, both the input datasetas well as the result being computed have underlyingmulti-dimensional grids, and queries into the dataset are in theform of ranges within each dimension of the grid. The basicprocessing step usually consists of transforming individual inputitems, mapping the transformed items to the output grid andcomputing output items by aggregating, in some way, all thetransformed input items mapped to the corresponding grid point. Forexample, remote-sensing earth images are often generated byperforming atmospheric correction on several days worth of rawtelemetry data, mapping all the data to a latitude-longitude gridand selecting those measurements that provide the clearestview.In this paper, we present T2, a customizable paralleldatabase that integrates storage, retrieval and processing ofmulti-dimensional datasets. T2 provides support for many operationsincluding index generation, data retrieval, memory management,scheduling of processing across a parallel machine and userinteraction. It achieves its primary advantage from the ability toseamlessly integrate data retrieval and processing for a widevariety of applications and from the ability to maintain andprocess multiple datasets with different underlying grids. Mostother systems for multi-dimensional data have focused on uniformlydistributed datasets, such as images, maps, and densemulti-dimensional arrays. Many real datasets, however, arenon-uniform or unstructured. For example, satellite data is a twodimensional strip that is embedded in a three dimensional space;water contamination studies use unstructured meshes to selectivelysimulate regions and so on. T2 can handle both uniform andnon-uniform datasets.T2 has been developed as a set of modular services. Since itsstructure mirrors that of a wide variety of applications, T2 iseasy to customize for different types of processing. To build aversion of T2 customized for a particular application, a user hasto provide functions to pre-process the input data, map input datato elements in the output data, and aggregate multiple input dataitems that map to the same output element.T2 presents a uniform interface to the end users (the clients ofthe database system). Users specify the dataset(s) of interest, aregion of interest within the dataset(s), and the desired formatand resolution of the output. In addition, they select the mappingand aggregation functions to be used. T2 analyzes the user request,builds a suitable plan to retrieve and process the datasets,executes the plan and presents the results in the desiredformat.In Section 2 we first present several motivating applicationsand illustrate their common structure. Section 3 then presents anoverview of T2, including its distinguishing features and a runningexample. Section 4 describes each database service in some detail.An example of how to customize several of the database services fora particular application is given in Section 5. T2 is a system inevolution. We conclude in Section 6 with a description of thecurrent status of both the T2 design and the implementation ofvarious applications with T2.

#index 246009
#* Workflow history management
#@ Pinar Koksal;Sena Nural Arpinar;Asuman Dogac
#t 1998
#c 5
#! A workflow history manager maintains the information essential for workflow monitoring and data mining as well as for recovery and authorization purposes.Certain characteristics of workflow systems like the necessity to run these systems on heterogeneous, autonomous and distributed environments and the nature of data, prevent history management in workflows to be handled by the classical data management techniques like distributed DBMSs. We further demonstrate that multi-database query processing techniques are also not appropriate for the problem at hand.In this paper, we describe history management, i.e., the structure of the history and querying of the history, in a fully distributed workflow architecture realized in conformance with Object Management Architecture (OMA) of OMG. By fully distributed architecture we mean that the scheduler of the workflow system is distributed and in accordance with this, the history objects related with activities are stored on data repositories (like DBMSs, files) available at the sites involved. We describe the structure of the history objects determined according to the nature of the data and the processing needs, and the possible query processing strategies on these objects using the Object Query Service of OMG. We then present the comparison of these strategies according to a cost model developed.

#index 246011
#* Towards a richer Web object model
#@ Frank Manola
#t 1998
#c 5
#! The World Wide Web is becoming an increasingly important factor in planning for enterprise distributed computing environments, both to support external access to enterprise systems and information (e.g., by customers, suppliers, and partners), and to support internal enterprise operations. Organizations perceive a number of advantages in using the Web in enterprise computing, a particular advantage being that it provides an information representation which• supports interlinking of all kinds of content• is easy for end-users to access• supports easy content creation using widely-available toolsHowever, as organizations have attempted to employ the Web in increasingly sophisticated applications, these applications have begun to overlap in complexity the sorts of distributed applications for which distributed object architectures such as OMG's CORBA, and its surrounding Object Management Architecture (OMA) [Sol95] were originally developed. Since the Web was not originally designed to support such applications, Web application development efforts increasingly run into limitations of the basic Web infrastructure.If the Web is to be used as the basis of complex enterprise applications, it must provide generic capabilities similar to those provided by the OMA (although these may need to be adapted to the more open, flexible nature of the Web, and specific requirements of Web applications). This involves such things as providing database-like services (such as enhanced query and transaction support) and their composition in the Web. However, the basic data structuring capabilities provided by the Web (its "object model") must also be addressed, since the ability to define and apply powerful generic services in the Web, and the ability to generally use the Web to support complex applications, depends crucially on the ability of the Web's underlying data structuring facilities to support these complex applications and services.

#index 246013
#* Where will object technology drive data administration?
#@ Arnon Rosenthal
#t 1998
#c 5
#! Several unifications that the application development process has long needed are now occurring, due to developments in object technologies and standards. These will (gradually) change the way data intensive applications are developed, reduce databases' prominence in this process, and change data administration's goals and participants. At the same time, the database community needs to ensure that its experiences are leveraged and its concerns are met within the new methodologies and toolsets. We discuss these issues, and illustrate how they apply to a portion of the Department of Defense (DOD). We also examine things that object technology won't accomplish, and identify research problems whose solution would enable further progress.

#index 246014
#* Repositories and object oriented databases
#@ Philip A. Bernstein
#t 1998
#c 5
#! A repository is a shared database of information about engineered artifacts. An object-oriented repository has many of the same features as an object-oriented database: properties, relationships, and versioning. However, the two technologies are different for two reasons. First, a repository system has built-in information models, which are database schemas or object models that cover both generic and tool-specific kinds of information. Second, the features of a repository are often more functional than similar features supported by object-oriented databases. This paper is primarily a survey of the latter features, drawing attention to capabilities that distinguish repositories from object-oriented databases.

#index 246016
#* Towards on-line analytical mining in large databases
#@ Jiawei Han
#t 1998
#c 5
#! Great efforts have been paid in the Intelligent Database Systems Research Lab for the research and development of efficient data mining methods and construction of on-line analytical data mining systems.Our work has been focused on the integration of data mining and OLAP technologies and the development of scalable, integrated, and multiple data mining functions. A data mining system, DBMiner, has been developed for interactive mining of multiple-level knowledge in large relational databases and data warehouses. The system implements a wide spectrum of data mining functions, including characterization, comparison, association, classification, prediction, and clustering. It also builds up a user-friendly, interactive data mining environment and a set of knowledge visualization tools. In-depth research has been performed on the efficiency and scalability of data mining methods. Moreover, the research has been extended to spatial data mining, multimedia data mining, text mining, and Web mining with several new data mining system prototypes constructed or under construction, including GeoMiner, MultiMediaMiner, and WebLogMiner.This article summarizes our research and development activities in the last several years and shares our experiences and lessons with the readers.

#index 247086
#* Intelligent access to heterogeneous information sources: report on the 4th workshop on knowledge representation meets databases
#@ Franz Baader;Manfred A. Jeusfeld;Werner Nutt
#t 1997
#c 5

#index 248009
#* Proceedings of the seventeenth ACM SIGACT-SIGMOD-SIGART symposium on Principles of database systems
#@ Alberto Mendelson;Jan Paredaens
#t 1998
#c 5

#index 248010
#* Fuzzy queries in multimedia database systems
#@ Ronald Fagin
#t 1998
#c 5
#% 52008
#% 55882
#% 68091
#% 86950
#% 169940
#% 210172
#% 229349
#% 238757
#% 238917
#% 278831
#% 285932
#% 287409
#% 384209
#% 443889
#% 464726
#% 592027
#% 607454
#% 614579

#index 248011
#* Expressiveness of structured document query languages based on attribute grammars
#@ Frank Neven;Jan Van den Bussche
#t 1998
#c 5
#% 43879
#% 46269
#% 59792
#% 64414
#% 90847
#% 150435
#% 201934
#% 234905
#% 241166
#% 459281
#% 480084
#% 481125
#% 835733

#index 248012
#* A new framework for itemset generation
#@ Charu C. Aggarwal;Philip S. Yu
#t 1998
#c 5
#% 152934
#% 172386
#% 210160
#% 227917
#% 227919
#% 443082
#% 443164
#% 461909
#% 462238
#% 481290
#% 481758

#index 248013
#* Logic based modeling and analysis of workflows
#@ Hasan Davulcu;Michael Kifer;C. R. Ramakrishnan;I. V. Ramakrishnan
#t 1998
#c 5
#% 2991
#% 55349
#% 83248
#% 86939
#% 101955
#% 122904
#% 122911
#% 169054
#% 169697
#% 185412
#% 408396
#% 461899
#% 463878
#% 464235
#% 481261
#% 535517
#% 562120
#% 562155
#% 565496

#index 248014
#* An overview of query optimization in relational systems
#@ Surajit Chaudhuri
#t 1998
#c 5
#% 11797
#% 13018
#% 32878
#% 32889
#% 43162
#% 58375
#% 58377
#% 77944
#% 83154
#% 83933
#% 86943
#% 86947
#% 116043
#% 123589
#% 136740
#% 152940
#% 172889
#% 210169
#% 210172
#% 210190
#% 210207
#% 213981
#% 223781
#% 248821
#% 271934
#% 271948
#% 286916
#% 287005
#% 340663
#% 382211
#% 411750
#% 427219
#% 458550
#% 464056
#% 464215
#% 480091
#% 480955
#% 481266
#% 481288
#% 481293
#% 481604
#% 481608
#% 481749
#% 481915
#% 482081
#% 482092
#% 482123
#% 564419
#% 564426
#% 565457
#% 697172

#index 248015
#* A lower bound theorem for indexing schemes and its application to multidimensional range queries
#@ Vasilis Samoladas;Daniel P. Miranker
#t 1998
#c 5
#% 101936
#% 137889
#% 137893
#% 164360
#% 164362
#% 201891
#% 214112
#% 227880
#% 237204
#% 248016
#% 281731
#% 463760
#% 481599
#% 593756
#% 656697
#% 679207
#% 680153

#index 248016
#* Tight bounds for 2-dimensional indexing schemes
#@ Elias Koutsoupias;D. S. Taylor
#t 1998
#c 5
#% 2465
#% 88056
#% 137893
#% 150120
#% 164360
#% 164362
#% 214112
#% 237204
#% 248015
#% 252608
#% 281731
#% 411694
#% 427199
#% 480093
#% 481599
#% 481620

#index 248017
#* A cost model for similarity queries in metric spaces
#@ Paolo Ciaccia;Marco Patella;Pavel Zezula
#t 1998
#c 5
#% 86950
#% 153260
#% 164360
#% 213975
#% 227937
#% 237187
#% 240182
#% 427199
#% 443698
#% 458742
#% 464859
#% 479462
#% 481279
#% 481460
#% 481599
#% 481620
#% 627242

#index 248018
#* Data base design principles for striping and placement of delay-sensitive data on disks
#@ Stavros Christodoulakis;Fenia A. Zioga
#t 1998
#c 5
#% 114572
#% 151340
#% 172881
#% 204397
#% 204542
#% 249264
#% 291640
#% 360808
#% 442375
#% 464031
#% 479479
#% 513572
#% 632238
#% 681746

#index 248019
#* Throughput-competitive admission control for continuous media databases
#@ Minos N. Garofalakis;Yannis E. Ioannidis;Banu Özden;Avi Silberschatz
#t 1998
#c 5
#% 1156
#% 107702
#% 149275
#% 150200
#% 175426
#% 175430
#% 183436
#% 187060
#% 190611
#% 201693
#% 201844
#% 201933
#% 203273
#% 204338
#% 224931
#% 282413
#% 282417
#% 547464
#% 547473
#% 566127
#% 632259
#% 661673
#% 670338
#% 702453
#% 1852691
#% 1852692

#index 248020
#* Querying spatial databases via topological invariants
#@ Luc Segoufin;Victor Vianu
#t 1998
#c 5
#% 6787
#% 101646
#% 152902
#% 164406
#% 183682
#% 187081
#% 188350
#% 213950
#% 213966
#% 224744
#% 245656
#% 268788
#% 384978
#% 464552
#% 464860
#% 527008
#% 544229
#% 552988
#% 587330
#% 587435
#% 598376

#index 248021
#* Safe constraint queries
#@ Michael Benedikt;Leonid Libkin
#t 1998
#c 5
#% 23907
#% 119492
#% 137916
#% 164406
#% 166244
#% 173860
#% 190332
#% 191614
#% 213952
#% 222207
#% 237182
#% 237186
#% 237188
#% 245656
#% 246560
#% 248022
#% 259488
#% 259489
#% 277325
#% 289266
#% 384978
#% 552988
#% 587330

#index 248022
#* An expressive language for linear spatial database queries
#@ Luc Vandeurzen;Marc Gyssens;Dirk Van Gucht
#t 1998
#c 5
#% 163438
#% 164406
#% 190332
#% 213950
#% 213956
#% 237186
#% 237188
#% 248021
#% 476994
#% 477218
#% 527014
#% 552988
#% 562288
#% 562306
#% 564945

#index 248023
#* External memory algorithms
#@ Jeffrey Scott Vitter
#t 1998
#c 5
#% 8199
#% 10130
#% 41684
#% 53976
#% 56081
#% 68089
#% 68091
#% 82258
#% 86950
#% 99338
#% 112170
#% 137893
#% 148278
#% 153260
#% 158793
#% 159079
#% 159275
#% 164362
#% 189744
#% 203281
#% 210187
#% 212468
#% 214112
#% 218154
#% 230000
#% 232620
#% 232758
#% 237204
#% 240384
#% 248028
#% 249263
#% 249359
#% 250208
#% 252608
#% 268709
#% 268788
#% 281655
#% 282025
#% 282569
#% 282591
#% 282908
#% 340670
#% 427199
#% 443130
#% 458741
#% 462503
#% 479463
#% 479473
#% 480093
#% 481271
#% 481304
#% 481309
#% 481326
#% 481455
#% 494184
#% 496918
#% 547451
#% 560666
#% 560837
#% 560840
#% 566144
#% 571093
#% 571296
#% 598676
#% 617131
#% 656697
#% 656712

#index 248024
#* Path constraints on semistructured and structured data
#@ Peter Buneman;Wenfei Fan;Scott Weinstein
#t 1998
#c 5
#% 111351
#% 125595
#% 210214
#% 237192
#% 464720

#index 248025
#* Query containment for conjunctive queries with regular expressions
#@ Daniela Florescu;Alon Levy;Dan Suciu
#t 1998
#c 5
#% 36181
#% 53400
#% 54225
#% 94461
#% 122396
#% 123118
#% 140410
#% 156703
#% 164364
#% 198465
#% 210214
#% 227995
#% 236409
#% 237181
#% 237191
#% 248026
#% 248819
#% 277355
#% 289266
#% 384978
#% 464056
#% 464554
#% 464717
#% 464720
#% 481128
#% 481923
#% 481935
#% 599549
#% 600179

#index 248026
#* On the decidability of query containment under constraints
#@ Diego Calvanese;Giuseppe De Giacomo;Maurizio Lenzerini
#t 1998
#c 5
#% 16
#% 6246
#% 36181
#% 101953
#% 122396
#% 123085
#% 154067
#% 164364
#% 188853
#% 190638
#% 205851
#% 237181
#% 237184
#% 264704
#% 287336
#% 289266
#% 384978
#% 459240
#% 464717
#% 464720
#% 464727
#% 599549
#% 1275330

#index 248027
#* Latent semantic indexing: a probabilistic analysis
#@ Christos H. Papadimitriou;Hisao Tamaki;Prabhakar Raghavan;Santosh Vempala
#t 1998
#c 5
#% 41374
#% 49501
#% 51749
#% 66649
#% 120104
#% 120110
#% 200694
#% 201073
#% 213981
#% 282102
#% 282905
#% 375017
#% 406493
#% 678676

#index 248028
#* Efficient searching with linear constraints
#@ Pankaj K. Agarwal;Lars Arge;Jeff Erickson;Paolo G. Franciosa;Jeffry Scott Vitter
#t 1998
#c 5
#% 1679
#% 6788
#% 9248
#% 10625
#% 11274
#% 68089
#% 68091
#% 72237
#% 86950
#% 88056
#% 116065
#% 141953
#% 144126
#% 144870
#% 159759
#% 164362
#% 178068
#% 190611
#% 210355
#% 214112
#% 237186
#% 237187
#% 237205
#% 281731
#% 285932
#% 317933
#% 411694
#% 427199
#% 435137
#% 461898
#% 480093
#% 481455
#% 481956
#% 500475
#% 560840
#% 571081
#% 593816
#% 656697

#index 248029
#* Relational transducers for electronic commerce
#@ Serge Abiteboul;Victor Vianu;Brad Fordham;Yelena Yesha
#t 1998
#c 5
#% 13043
#% 64421
#% 101955
#% 101958
#% 210761
#% 275314
#% 384978
#% 394417
#% 415981
#% 528124
#% 637757

#index 248030
#* Generalizing data to provide anonymity when disclosing information (abstract)
#@ Pierangela Samarati;Latanya Sweeney
#t 1998
#c 5

#index 248031
#* Typed query languages for databases containing queries
#@ Frank Neven;Dirk Van Gucht;Jan Van den Bussche;Gottfried Vossen
#t 1998
#c 5
#% 22947
#% 27056
#% 36683
#% 114580
#% 123121
#% 140407
#% 152952
#% 164406
#% 175524
#% 210349
#% 210352
#% 228660
#% 289370
#% 384978
#% 427214
#% 435157

#index 248032
#* On the complexity of the containment problem for conjunctive queries with built-in predicates
#@ Phokion G. Kolaitis;David L. Martin;Madhukar N. Thakur
#t 1998
#c 5
#% 391
#% 36181
#% 36683
#% 111826
#% 198465
#% 198466
#% 230142
#% 249228
#% 408396
#% 464203
#% 464717
#% 464727
#% 598389
#% 599549

#index 248033
#* Conjunctive-query containment and constraint satisfaction
#@ Phokion G. Kolaitis;Moshe Y. Vardi
#t 1998
#c 5
#% 31484
#% 36683
#% 39265
#% 69087
#% 70508
#% 101930
#% 111826
#% 125386
#% 126392
#% 145232
#% 150115
#% 150197
#% 159244
#% 190340
#% 191611
#% 198465
#% 198466
#% 251197
#% 384978
#% 408396
#% 464203
#% 464717
#% 464727
#% 511829
#% 568136
#% 587357
#% 599549
#% 600496

#index 248034
#* Deciding equivalences among aggregate queries
#@ Werner Nutt;Yehoshus Sagiv;Sara Shurin
#t 1998
#c 5
#% 123118
#% 129572
#% 137867
#% 137871
#% 190638
#% 198473
#% 247577
#% 287336
#% 289266
#% 464056
#% 481604
#% 482081
#% 599549

#index 248035
#* Data bases in digital libraries: where computer science and information management meet
#@ Judith Klavans
#t 1998
#c 5

#index 248036
#* Dynamic tree isomorphism via first-order updates to a relational database
#@ Kousha Etessami
#t 1998
#c 5
#% 13016
#% 59792
#% 80186
#% 84275
#% 115525
#% 152928
#% 175440
#% 188352
#% 201929
#% 245652
#% 275310
#% 281882
#% 282007
#% 417543
#% 442767
#% 562127
#% 562302
#% 587329
#% 616281

#index 248037
#* Complexity of nonrecursive logic programs with complex values
#@ Sergei Vorobyov;Andrie Voronkov
#t 1998
#c 5
#% 6787
#% 28120
#% 30096
#% 33376
#% 53385
#% 53386
#% 53387
#% 53388
#% 57383
#% 64440
#% 66290
#% 69086
#% 101621
#% 101650
#% 101922
#% 101949
#% 124755
#% 145169
#% 146277
#% 190332
#% 191611
#% 237180
#% 237188
#% 289287
#% 384978
#% 435157
#% 472987
#% 542627
#% 565149
#% 587426
#% 590310
#% 598376
#% 600179

#index 248038
#* Complexity of answering queries using materialized views
#@ Serge Abiteboul;Oliver M. Duschka
#t 1998
#c 5
#% 663
#% 6711
#% 23898
#% 36181
#% 36683
#% 94459
#% 122396
#% 140410
#% 145195
#% 198465
#% 198466
#% 205851
#% 213982
#% 227997
#% 230142
#% 237181
#% 237189
#% 237190
#% 258647
#% 287336
#% 289266
#% 296931
#% 366807
#% 384978
#% 464056
#% 464717
#% 481923
#% 564419
#% 598376
#% 599549
#% 601159
#% 1478768

#index 248039
#* Decidability and undecidability results for the termination problem of active database rules
#@ James Bailey;Guozhu Dong;Kotagiri Ramamohanarao
#t 1998
#c 5
#% 60876
#% 77680
#% 102547
#% 116045
#% 137871
#% 153004
#% 154067
#% 156703
#% 187081
#% 198469
#% 384978
#% 464858
#% 481456
#% 501951

#index 248040
#* Dynamic assembly of views in data cubes
#@ John R. Smith;Vittorio Castelli;Anant Jhingran;Chung-Sheng Li
#t 1998
#c 5
#% 181378
#% 210182
#% 227866
#% 227880
#% 461921
#% 462204
#% 464215
#% 479450
#% 481951
#% 626847

#index 248783
#* Proceedings of the 1998 ACM SIGMOD international conference on Management of data
#@ Laura Haas;Pamela Drew;Ashutosh Tiwary;Michael Franklin
#t 1998
#c 5

#index 248784
#* Query flocks: a generalization of association-rule mining
#@ Dick Tsur;Jeffrey D. Ullman;Serge Abiteboul;Chris Clifton;Rajeev Motwani;Svetlozar Nestorov;Arnon Rosenthal
#t 1998
#c 5
#% 36683
#% 152934
#% 234797
#% 289370
#% 384978
#% 411554
#% 463883
#% 464712
#% 481128
#% 481290
#% 599549
#! Association-rule mining has proved a highly successful technique for extracting useful information from very large databases. This success is attributed not only to the appropriateness of the objectives, but to the fact that a number of new query-optimization ideas, such as the “a-priori” trick, make association-rule mining run much faster than might be expected. In this paper we see that the same tricks can be extended to a much more general context, allowing efficient mining of very large databases for many different kinds of patterns. The general idea, called “query flocks,” is a generate-and-test model for data-mining problems. We show how the idea can be used either in a general-purpose mining system or in a next generation of conventional query optimizers.

#index 248785
#* Exploratory mining and pruning optimizations of constrained associations rules
#@ Raymond T. Ng;Laks V. S. Lakshmanan;Jiawei Han;Alex Pang
#t 1998
#c 5
#% 152934
#% 172386
#% 201894
#% 210160
#% 210162
#% 216508
#% 227883
#% 227919
#% 227922
#% 227953
#% 236414
#% 236696
#% 443091
#% 461909
#% 464204
#% 481290
#% 481588
#% 481754
#% 481758
#% 481779
#% 481954
#! From the standpoint of supporting human-centered discovery of knowledge, the present-day model of mining association rules suffers from the following serious shortcomings: (i) lack of user exploration and control, (ii) lack of focus, and (iii) rigid notion of relationships. In effect, this model functions as a black-box, admitting little user interaction in between. We propose, in this paper, an architecture that opens up the black-box, and supports constraint-based, human-centered exploratory mining of associations. The foundation of this architecture is a rich set of constraint constructs, including domain, class, and SQL-style aggregate constraints, which enable users to clearly specify what associations are to be mined. We propose constrained association queries as a means of specifying the constraints to be satisfied by the antecedent and consequent of a mined association.In this paper, we mainly focus on the technical challenges in guaranteeing a level of performance that is commensurate with the selectivities of the constraints in an association query. To this end, we introduce and analyze two properties of constraints that are critical to pruning: anti-monotonicity and succinctness. We then develop characterizations of various constraints into four categories, according to these properties. Finally, we describe a mining algorithm called CAP, which achieves a maximized degree of pruning for all categories of constraints. Experimental results indicate that CAP can run much faster, in some cases as much as 80 times, than several basic algorithms. This demonstrates how important the succinctness and anti-monotonicity properties are, in delivering the performance guarantee.

#index 248786
#* Parallel mining algorithms for generalized association rules with classification hierarchy
#@ Takahiko Shintani;Masaru Kitsuregawa
#t 1998
#c 5
#% 199538
#% 227922
#% 340290
#% 340291
#% 443085
#% 443091
#% 459006
#% 481290
#% 481758
#% 501216
#! Association rule mining recently attracted strong attention. Usually, the classification hierarchy over the data items is available. Users are interested in generalized association rules that span different levels of the hierarchy, since sometimes more interesting rules can be derived by taking the hierarchy into account.In this paper, we propose the new parallel algorithms for mining association rules with classification hierarchy on a shared-nothing parallel machine to improve its performance. Our algorithms partition the candidate itemsets over the processors, which exploits the aggregate memory of the system effectively. If the candidate itemsets are partitioned without considering classification hierarchy, both the items and its all the ancestor items have to be transmitted, that causes prohibitively large amount of communications. Our method minimizes interprocessor communication by considering the hierarchy. Moreover, in our algorithm, the available memory space is fully utilized by identifying the frequently occurring candidate itemsets and copying them over all the processors, through which frequent itemsets can be processed locally without any communication. Thus it can effectively reduce the load skew among the processors. Several experiments are done by changing the granule of copying itemsets, from the whole tree, to the small group of the frequent itemsets along the hierarchy. The coarser the grain, the easier the control but it is rather difficult to achieve the sufficient load balance. The finer the grain, the more complicated the control is required but it can balance the load quite well.We implemented proposed algorithms on IBM SP-2. Performance evaluations show that our algorithms are effective for handling skew and attain sufficient speedup ratio.

#index 248787
#* Reusing invariants: a new strategy for correlated queries
#@ Jun Rao;Kenneth A. Ross
#t 1998
#c 5
#% 32878
#% 36117
#% 83154
#% 116043
#% 172930
#% 210169
#% 210206
#% 214602
#% 227861
#% 227883
#% 258426
#% 287005
#% 411554
#% 442714
#% 442918
#% 461897
#% 479460
#% 480091
#% 481608
#! Correlated queries are very common and important in decision support systems. Traditional nested iteration evaluation methods for such queries can be very time consuming. When they apply, query rewriting techniques have been shown to be much more efficient. But query rewriting is not always possible. When query rewriting does not apply, can we do something better than the traditional nested iteration methods? In this paper, we propose a new invariant technique to evaluate correlated queries efficiently. The basic idea is to recognize the part of the subquery that is not related to the outer references and cache the result of that part after its first execution. Later, we can reuse the result and combine it with the result of the rest of the subquery that is changing for each iteration. Our technique applies to arbitrary correlated subqueries.This paper introduces algorithms to recognize the invariant part of a data flow tree, and to restructure the evaluation plan to reuse the stored intermediate result. We also propose an efficient method to teach an existing join optimizer to understand the invariant feature and thus allow it to be able to generate better join plans in the new context. Some other related optimization techniques are also discussed. The proposed techniques were implemented within three months on an existing real commercial database system.We also experimentally evaluate our proposed technique. Our evaluation indicates that, when query rewriting is not possible, the invariant technique is significantly better than the traditional nested iteration method. Even when query rewriting applies, the invariant technique is sometimes better than the query rewriting technique. Our conclusion is that the invariant technique should be considered as one of the alternatives in evaluating correlated queries since it fills the gap left by rewriting techniques.

#index 248788
#* Query unnesting in object-oriented databases
#@ Leonidas Fegaras
#t 1998
#c 5
#% 32878
#% 80488
#% 120674
#% 137864
#% 152942
#% 163444
#% 169846
#% 172939
#% 201873
#% 214693
#% 248788
#% 248789
#% 287005
#% 395735
#% 482103
#% 562153
#% 564426
#% 700905
#% 836012
#! There is already a sizable body of proposals on OODB query optimization. One of the most challenging problems in this area is query unnesting, where the embedded query can take any form, including aggregation and universal quantification. Although there is already a number of proposed techniques for query unnesting, most of these techniques are applicable to only few cases. We believe that the lack of a general and simple solution to the query unnesting problem is due to the lack of a uniform algebra that treats all operations (including aggregation and quantification) in the same way.This paper presents a new query unnesting algorithm that generalizes many unnesting techniques proposed recently in the literature. Our system is capable of removing any form of query nesting using a very simple and efficient algorithm. The simplicity of the system is due to the use of the monoid comprehension calculus as an intermediate form for OODB queries. The monoid comphrehension calculus treats operations over multiple collection types, aggregates, and quantifiers in a similar way, resulting in a uniform way of unnesting queries, regardless of their type of nesting.

#index 248789
#* Changing the rules: transformations for rule-based optimizers
#@ Mitch Cherniack;Stan Zdonik
#t 1998
#c 5
#% 82858
#% 86943
#% 114709
#% 116043
#% 135438
#% 170061
#% 210203
#% 210207
#% 238413
#% 287005
#% 462643
#% 564428
#% 565453
#% 565457
#% 709312
#! Rule-based optimizers are extensible because they consist of modifiable sets of rules. For modification to be straightforward, rules must be easily reasoned about (i.e., understood and verified). At the same time, rules must be expressive and efficient (to fire) for rule-based optimizers to be practical. Production-style rules (as in [15]) are expressed with code and are hard to reason about. Pure rewrite rules (as in [1]) lack code, but cannot atomically express complex transformations (e.g., normalizations). Some systems allow rules to be grouped, but sacrifice efficiency by providing limited control over their firing. Therefore, none of these approaches succeeds in making rules expressive, efficient and understandable.We propose a language (COKO) for expressing an alternative form of input to a rule-based optimizer. A COKO transformation consists of a set of declarative (KOLA) rewrite rules and a (firing) algorithm that specifies their firing. It is straightforward to reason about COKO transformations because all query modification is expressed with declarative rewrite rules. Firing is specified algorithmically with an expressive language that provides direct control over how query representations are traversed, and under what conditions rules are fired. Therefore, COKO achieves a delicate balance of understandability, efficiency and expressivity.

#index 248790
#* CURE: an efficient clustering algorithm for large databases
#@ Sudipto Guha;Rajeev Rastogi;Kyuseok Shim
#t 1998
#c 5
#% 1331
#% 36672
#% 68091
#% 70370
#% 86950
#% 190611
#% 210173
#% 480093
#% 481281
#% 674087
#! Clustering, in data mining, is useful for discovering groups and identifying interesting distributions in the underlying data. Traditional clustering algorithms either favor clusters with spherical shapes and similar sizes, or are very fragile in the presence of outliers. We propose a new clustering algorithm called CURE that is more robust to outliers, and identifies clusters having non-spherical shapes and wide variances in size. CURE achieves this by representing each cluster by a certain fixed number of points that are generated by selecting well scattered points from the cluster and then shrinking them toward the center of the cluster by a specified fraction. Having more than one representative point per cluster allows CURE to adjust well to the geometry of non-spherical shapes and the shrinking helps to dampen the effects of outliers. To handle large databases, CURE employs a combination of random sampling and partitioning. A random sample drawn from the data set is first partitioned and each partition is partially clustered. The partial clusters are then clustered in a second pass to yield the desired clusters. Our experimental results confirm that the quality of clusters produced by CURE is much better than those found by existing algorithms. Furthermore, they demonstrate that random sampling and partitioning enable CURE to not only outperform existing algorithms but also to scale well for large databases without sacrificing clustering quality.

#index 248791
#* Efficiently mining long patterns from databases
#@ Roberto J. Bayardo, Jr.
#t 1998
#c 5
#% 152934
#% 201894
#% 227917
#% 232136
#% 442814
#% 459006
#% 459020
#% 463903
#% 464714
#% 481754
#! We present a pattern-mining algorithm that scales roughly linearly in the number of maximal patterns embedded in a database irrespective of the length of the longest pattern. In comparison, previous algorithms based on Apriori scale exponentially with longest pattern length. Experiments on real data show that when the patterns are long, our algorithm is more efficient by an order of magnitude or more.

#index 248792
#* Automatic subspace clustering of high dimensional data for data mining applications
#@ Rakesh Agrawal;Johannes Gehrke;Dimitrios Gunopulos;Prabhakar Raghavan
#t 1998
#c 5
#% 7511
#% 35909
#% 36672
#% 56411
#% 80995
#% 150126
#% 210160
#% 210173
#% 214219
#% 227866
#% 227917
#% 227953
#% 232102
#% 232117
#% 232136
#% 237187
#% 237200
#% 248791
#% 252400
#% 369349
#% 408638
#% 459008
#% 459020
#% 481281
#% 481779
#% 481945
#! Data mining applications place special requirements on clustering algorithms including: the ability to find clusters embedded in subspaces of high dimensional data, scalability, end-user comprehensibility of the results, non-presumption of any canonical data distribution, and insensitivity to the order of input records. We present CLIQUE, a clustering algorithm that satisfies each of these requirements. CLIQUE identifies dense clusters in subspaces of maximum dimensionality. It generates cluster descriptions in the form of DNF expressions that are minimized for ease of comprehension. It produces identical results irrespective of the order in which input records are presented and does not presume any specific mathematical form for data distribution. Through experiments, we show that CLIQUE efficiently finds accurate cluster in large high dimensional datasets.

#index 248793
#* Efficient mid-query re-optimization of sub-optimal query execution plans
#@ Navin Kabra;David J. DeWitt
#t 1998
#c 5
#% 1331
#% 2833
#% 58375
#% 102784
#% 152996
#% 172900
#% 201921
#% 210190
#% 227934
#% 287667
#% 340305
#% 411554
#% 435107
#% 463444
#% 464233
#% 480430
#% 480943
#% 480955
#% 481131
#% 571062
#% 710522
#! For a number of reasons, even the best query optimizers can very often produce sub-optimal query execution plans, leading to a significant degradation of performance. This is especially true in databases used for complex decision support queries and/or object-relational databases. In this paper, we describe an algorithm that detects sub-optimality of a query execution plan during query execution and attempts to correct the problem. The basic idea is to collect statistics at key points during the execution of a complex query. These statistics are then used to optimize the execution of the query, either by improving the resource allocation for that query, or by changing the execution plan for the remainder of the query. To ensure that this does not significantly slow down the normal execution of a query, the Query Optimizer carefully chooses what statistics to collect, when to collect them, and the circumstances under which to re-optimize the query. We describe an implementation of this algorithm in the Paradise Database System, and we report on performance studies, which indicate that this can result in significant improvements in the performance of complex queries.

#index 248794
#* Interaction of query evaluation and buffer management for information retrieval
#@ Björn T. Jónsson;Michael J. Franklin;Divesh Srivastava
#t 1998
#c 5
#% 735
#% 1921
#% 46803
#% 77005
#% 102805
#% 115469
#% 115470
#% 152925
#% 152943
#% 157880
#% 169779
#% 169817
#% 194247
#% 198335
#% 210177
#% 212665
#% 227883
#% 227894
#% 319473
#% 339621
#% 463737
#% 480926
#% 481092
#% 481431
#% 481450
#% 481916
#% 482086
#! The proliferation of the World Wide Web has brought information retrieval (IR) techniques to the forefront of search technology. To the average computer user, “searching” now means using IR-based systems for finding information on the WWW or in other document collections. IR query evaluation methods and workloads differ significantly from those found in database systems. In this paper, we focus on three such differences. First, due to the inherent fuzziness of the natural language used in IR queries and documents, an additional degree of flexibility is permitted in evaluating queries. Second, IR query evaluation algorithms tend to have access patterns that cause problems for traditional buffer replacement policies. Third, IR search is often an iterative process, in which a query is repeatedly refined and resubmitted by the user. Based on these differences, we develop two complementary techniques to improve the efficiency of IR queries: 1) Buffer-aware query evaluation, which alters the query evaluation process based on the current contents of buffers; and 2) Ranking-aware buffer replacement, which incorporates knowledge of the query processing strategy into replacement decisions. In a detailed performance study we show that using either of these techniques yields significant performance benefits and that in many cases, combining them produces even further improvements.

#index 248795
#* Cost-based query scrambling for initial delays
#@ Tolga Urhan;Michael J. Franklin;Laurent Amsaleg
#t 1998
#c 5
#% 3771
#% 13018
#% 32877
#% 86949
#% 116040
#% 136740
#% 168676
#% 169061
#% 172900
#% 201926
#% 252374
#% 264691
#% 340295
#% 340305
#% 411554
#% 435149
#% 463444
#% 479467
#% 480955
#% 481919
#% 631868
#! Remote data access from disparate sources across a wide-area network such as the Internet is problematic due to the unpredictable nature of the communications medium and the lack of knowledge about the load and potential delays at remote sites. Traditional, static, query processing approaches break down in this environment because they are unable to adapt in response to unexpected delays. Query scrambling has been proposed to address this problem. Scrambling modifies query execution plans on-the-fly when delays are encountered during runtime. In its original formulation, scrambling was based on simple heuristics, which although providing good performance in many cases, were also shown to be susceptible to problems resulting from bad scrambling decisions. In this paper we address these shortcomings by investigating ways to exploit query optimization technology to aid in making intelligent scrambling choices. We propose three different approaches to using query optimization for scrambling. These approaches vary, for example, in whether they optimize for total work or response-time, and whether they construct partial or complete alternative plans. Using a two-phase randomized query optimizer, a distributed query processing simulator, and a workload derived from queries of the TPCD benchmark, we evaluate these different approaches and compare their ability to cope with initial delays in accessing remote sources. The results show that cost-based scrambling can effectively hide initial delays, but that in the absence of good predictions of expected delay durations, there are fundamental tradeoffs between risk aversion and effectiveness.

#index 248796
#* The pyramid-technique: towards breaking the curse of dimensionality
#@ Stefan Berchtold;Christian Böhm;Hans-Peter Kriegal
#t 1998
#c 5
#% 86950
#% 102772
#% 169940
#% 227856
#% 227939
#% 227965
#% 237187
#% 317313
#% 317933
#% 339622
#% 411694
#% 435141
#% 458741
#% 462239
#% 463414
#% 464195
#% 481956
#% 482109
#% 527026
#! In this paper, we propose the Pyramid-Technique, a new indexing method for high-dimensional data spaces. The Pyramid-Technique is highly adapted to range query processing using the maximum metric Lmax. In contrast to all other index structures, the performance of the Pyramid-Technique does not deteriorate when processing range queries on data of higher dimensionality. The Pyramid-Technique is based on a special partitioning strategy which is optimized for high-dimensional data. The basic idea is to divide the data space first into 2d pyramids sharing the center point of the space as a top. In a second step, the single pyramids are cut into slices parallel to the basis of the pyramid. These slices from the data pages. Furthermore, we show that this partition provides a mapping from the given d-dimensional space to a 1-dimensional space. Therefore, we are able to use a B+-tree to manage the transformed data. As an analytical evaluation of our technique for hypercube range queries and uniform data distribution shows, the Pyramid-Technique clearly outperforms index structures using other partitioning strategies. To demonstrate the practical relevance of our technique, we experimentally compared the Pyramid-Technique with the X-tree, the Hilbert R-tree, and the Linear Scan. The results of our experiments using both, synthetic and real data, demonstrate that the Pyramid-Technique outperforms the X-tree and the Hilbert R-tree by a factor of up to 14 (number of page accesses) and up to 2500 (total elapsed time) for range queries.

#index 248797
#* Optimal multi-step k-nearest neighbor search
#@ Thomas Seidl;Hans-Peter Kriegel
#t 1998
#c 5
#% 2115
#% 102772
#% 142639
#% 158905
#% 169940
#% 172949
#% 198573
#% 201876
#% 201893
#% 227856
#% 227999
#% 237187
#% 252304
#% 317313
#% 421052
#% 443889
#% 445701
#% 460862
#% 462239
#% 464195
#% 464859
#% 479462
#% 481609
#% 481947
#% 481956
#% 482109
#% 527004
#% 527026
#% 527158
#! For an increasing number of modern database applications, efficient support of similarity search becomes an important task. Along with the complexity of the objects such as images, molecules and mechanical parts, also the complexity of the similarity models increases more and more. Whereas algorithms that are directly based on indexes work well for simple medium-dimensional similarity distance functions, they do not meet the efficiency requirements of complex high-dimensional and adaptable distance functions. The use of a multi-step query processing strategy is recommended in these cases, and our investigations substantiate that the number of candidates which are produced in the filter step and exactly evaluated in the refinement step is a fundamental efficiency parameter. After revealing the strong performance shortcomings of the state-of-the-art algorithm for k-nearest neighbor search [Korn et al. 1996], we present a novel multi-step algorithm which is guaranteed to produce the minimum number of candidates. Experimental evaluations demonstrate the significant performance gain over the previous solution, and we observed average improvement factors of up to 120 for the number of candidates and up to 48 for the total runtime.

#index 248798
#* Dimensionality reduction for similarity searching in dynamic databases
#@ K. V. Ravi Kanth;Divyakant Agrawal;Ambuj Singh
#t 1998
#c 5
#% 32898
#% 55490
#% 86950
#% 88056
#% 169805
#% 172949
#% 195278
#% 201878
#% 214595
#% 227939
#% 251654
#% 317313
#% 321455
#% 427199
#% 435141
#% 464195
#% 481956
#% 682388
#% 1757433
#! Databases are increasingly being used to store multi-media objects such as maps, images, audio and video. Storage and retrieval of these objects is accomplished using multi-dimensional index structures such as R*-trees and SS-trees. As dimensionality increases, query performance in these index structures degrades. This phenomenon, generally referred to as the dimensionality curse, can be circumvented by reducing the dimensionality of the data. Such a reduction is however accompanied by a loss of precision of query results. Current techniques such as QBIC use SVD transform-based dimensionality reduction to ensure high query precision. The drawback of this approach is that SVD is expensive to compute, and therefore not readily applicable to dynamic databases. In this paper, we propose novel techniques for performing SVD-based dimensionality reduction in dynamic databases. When the data distribution changes considerably so as to degrade query precision, we recompute the SVD transform and incorporate it in the existing index structure. For recomputing the SVD-transform, we propose a novel technique that uses aggregate data from the existing index rather than the entire data. This technique reduces the SVD-computation time without compromising query precision. We then explore efficient ways to incorporate the recomputed SVD-transform in the existing index structure without degrading subsequent query response times. These techniques reduce the computation time by a factor of 20 in experiments on color and texture image vectors. The error due to approximate computation of SVD is less than 10%.

#index 248799
#* Your mediators need data conversion!
#@ Sophie Cluet;Claude Delobel;Jérǒme Siméon;Katarzyna Smaga
#t 1998
#c 5
#% 50073
#% 85089
#% 116091
#% 201873
#% 210176
#% 210184
#% 210214
#% 215389
#% 236409
#% 462051
#% 463919
#% 464222
#% 464716
#% 464720
#% 479459
#% 479465
#% 481935
#% 562141
#% 614579
#% 631868
#! Due to the development of the World Wide Web, the integration of heterogeneous data sources has become a major concern of the database community. Appropriate architectures and query languages have been proposed. Yet, the problem of data conversion which is essential for the development of mediators/wrappers architectures has remained largely unexplored.In this paper, we present the YAT system for data conversion. This system provides tools for the specification and the implementation of data conversions among heterogeneous data sources. It relies on a middleware model, a declarative language, a customization mechanism and a graphical interface.The model is based on named trees with ordered and labeled nodes. Like semistructured data models, it is simple enough to facilitate the representation of any data. Its main originality is that it allows to reason at various levels of representation. The YAT conversion language (called YATL) is declarative, rule-based and features enhanced pattern matching facilities and powerful restructuring primitives. It allows to preserve or reconstruct the order of collections. The customization mechanism relies on program instantiations: an existing program may be instantiated into a more specific one, and then easily modified. We also present the architecture, implementation and practical use of the YAT prototype, currently under evaluation within the OPAL* project.

#index 248800
#* Using schematically heterogeneous structures
#@ Reée J. Miller
#t 1998
#c 5
#% 6797
#% 11284
#% 22948
#% 83606
#% 102748
#% 111912
#% 111913
#% 116091
#% 123121
#% 126335
#% 137863
#% 137867
#% 188077
#% 198465
#% 237184
#% 411554
#% 434818
#% 463919
#% 464056
#% 464215
#% 480969
#% 481923
#% 481944
#% 482081
#% 508431
#% 565454
#% 571169
#% 571297
#% 614579
#! Schematic heterogeneity arises when information that is represented as data under one schema, is represented within the schema (as metadata) in another. Schematic heterogeneity is an important class of heterogeneity that arises frequently in integrating legacy data in federated or data warehousing applications. Traditional query languages and view mechanisms are insufficient for reconciling and translating data between schematically heterogeneous schemas. Higher order query languages, that permit quantification over schema labels, have been proposed to permit querying and restructuring of data between schematically disparate schemas. We extend this work by considering how these languages can be used in practice. Specifically, we consider a restricted class of higher order views and show the power of these views in integrating legacy structures. Our results provide insights into the properties of restructuring transformations required to resolve schematic discrepancies. In addition, we show how the use of these views permits schema browsing and new forms of data independence that are important for global information systems. Furthermore, these views provide a framework for integrating semi-structured and unstructured queries, such as keyword searches, into a structured querying environment. We show how these views can be used with minimal extensions to existing query engines. We give conditions under which a higher order view is usable for answering a query and provide query translation algorithms.

#index 248801
#* Integration of heterogeneous databases without common domains using queries based on textual similarity
#@ William W. Cohen
#t 1998
#c 5
#% 25470
#% 55490
#% 127850
#% 137995
#% 144072
#% 169774
#% 194290
#% 198335
#% 201889
#% 201936
#% 219053
#% 227886
#% 227994
#% 234905
#% 237190
#% 237192
#% 237193
#% 252834
#% 272510
#% 296931
#% 442830
#% 449508
#% 481602
#% 481923
#% 482083
#% 1499471
#! Most databases contain “name constants” like course numbers, personal names, and place names that correspond to entities in the real world. Previous work in integration of heterogeneous databases has assumed that local name constants can be mapped into an appropriate global domain by normalization. However, in many cases, this assumption does not hold; determining if two name constants should be considered identical can require detailed knowledge of the world, the purpose of the user's query, or both. In this paper, we reject the assumption that global domains can be easily constructed, and assume instead that the names are given in natural language text. We then propose a logic called WHIRL which reasons explicitly about the similarity of local names, as measured using the vector-space model commonly adopted in statistical information retrieval. We describe an efficient implementation of WHIRL and evaluate it experimentally on data extracted from the World Wide Web. We show that WHIRL is much faster than naive inference methods, even for short queries. We also show that inferences made by WHIRL are surprisingly accurate, equaling the accuracy of hand-coded normalization routines on one benchmark problem, and outperforming exact matching with a plausible global domain on a second.

#index 248802
#* The DEDALE system for complex spatial queries
#@ Stéphane Grumbach;Philippe Rigaux;Luc Segoufin
#t 1998
#c 5
#% 2115
#% 13742
#% 32890
#% 36683
#% 68091
#% 77927
#% 78365
#% 83541
#% 86950
#% 125595
#% 136740
#% 152902
#% 164406
#% 172962
#% 224744
#% 227934
#% 237186
#% 268788
#% 285932
#% 322880
#% 427199
#% 435137
#% 435148
#% 445701
#% 445703
#% 480093
#% 527005
#% 527008
#% 527164
#% 552988
#% 562306
#% 646454
#! This paper presents DEDALE, a spatial database system intended to overcome some limitations of current systems by providing an abstract and non-specialized data model and query language for the representation and manipulation of spatial objects. DEDALE relies on a logical model based on linear constraints, which generalizes the constraint database model of [KKR90]. While in the classical constraint model, spatial data is always decomposed into its convex components, in DEDALE holes are allowed to fit the need of practical applications. The logical representation of spatial data although slightly more costly in memory, has the advantage of simplifying the algorithms. DEDALE relies on nested relations, in which all sorts of data (thematic, spatial, etc.) are stored in a uniform fashion. This new data model supports declarative query languages, which allow an intuitive and efficient manipulation of spatial objects. Their formal foundation constitutes a basis for practical query optimization. We describe several evaluation rules tailored for geometric data and give the specification of an optimizer module for spatial queries. Except for the latter module, the system has been fully implemented upon the O2 DBMS, thus proving the effectiveness of a constraint-based approach for the design of spatial database systems.

#index 248803
#* Similarity query processing using disk arrays
#@ Apostolos N. Papadopoulos;Yannis Manolopoulos
#t 1998
#c 5
#% 43172
#% 86950
#% 102810
#% 116064
#% 130906
#% 137887
#% 152902
#% 159079
#% 159275
#% 164360
#% 172949
#% 201876
#% 213975
#% 227856
#% 227939
#% 237187
#% 317313
#% 427199
#% 435141
#% 463598
#% 464195
#% 464859
#% 480093
#% 481455
#% 481620
#% 481956
#! Similarity queries are fundamental operations that are used extensively in many modern applications, whereas disk arrays are powerful storage media of increasing importance. The basic trade-off in similarity query processing in such a system is that increased parallelism leads to higher resource consumptions and low throughput, whereas low parallelism leads to higher response times. Here, we propose a technique which is based on a careful investigation of the currently available data in order to exploit parallelism up to a point, retaining low response times during query processing. The underlying access method is a variation of the R*-tree, which is distributed among the components of a disk array, whereas the system is simulated using event-driven simulation. The performance results conducted, demonstrate that the proposed approach outperforms by factors a previous branch-and-bound algorithm and a greedy algorithm which maximizes parallelism as much as possible. Moreover, the comparison of the proposed algorithm to a hypothetical (non-existing) optimal one (with respect to the number of disk accesses) shows that the former is on average two times slower than the latter.

#index 248804
#* Incremental distance join algorithms for spatial databases
#@ Gísli R. Hjaltason;Hanan Samet
#t 1998
#c 5
#% 4652
#% 61589
#% 68089
#% 68091
#% 83319
#% 83321
#% 86950
#% 144624
#% 152937
#% 172908
#% 172909
#% 198554
#% 201876
#% 214602
#% 227883
#% 227894
#% 317933
#% 340635
#% 427199
#% 462331
#% 462479
#% 462957
#% 463436
#% 463595
#% 479453
#% 479475
#% 503853
#% 527026
#! Two new spatial join operations, distance join and distance semi-join, are introduced where the join output is ordered by the distance between the spatial attribute values of the joined tuples. Incremental algorithms are presented for computing these operations, which can be used in a pipelined fashion, thereby obviating the need to wait for their completion when only a few tuples are needed. The algorithms can be used with a large class of hierarchical spatial data structures and arbitrary spatial data types in any dimensions. In addition, any distance metric may be employed. A performance study using R-trees shows that the incremental algorithms outperform non-incremental approaches by an order of magnitude if only a small part of the result is needed, while the penalty, if any, for the incremental processing is modest if the entire join result is required.

#index 248805
#* An alternative storage organization for ROLAP aggregate views based on cubetrees
#@ Yannis Kotidis;Nick Roussopoulos
#t 1998
#c 5
#% 18614
#% 64431
#% 152928
#% 191154
#% 198467
#% 201929
#% 210182
#% 227861
#% 227868
#% 227869
#% 227880
#% 286237
#% 286991
#% 427199
#% 462204
#% 464215
#% 464706
#% 479476
#% 481951
#! The Relational On-Line Analytical Processing (ROLAP) is emerging as the dominant approach in data warehousing with decision support applications. In order to enhance query performance, the ROLAP approach relies on selecting and materializing in summary tables appropriate subsets of aggregate views which are then engaged in speeding up OLAP queries. However, a straight forward relational storage implementation of materialized ROLAP views is immensely wasteful on storage and incredibly inadequate on query performance and incremental update speed. In this paper we propose the use of Cubetrees, a collection of packed and compressed R-trees, as an alternative storage and index organization for ROLAP views and provide an efficient algorithm for mapping an arbitrary set of OLAP views to a collection of Cubetrees that achieve excellent performance. Compared to a conventional (relational) storage organization of materialized OLAP views, Cubetrees offer at least a 2-1 storage reduction, a 10-1 better OLAP query performance, and a 100-1 faster updates. We compare the two alternative approaches with data generated from the TPC-D benchmark and stored in the Informix Universal Server (IUS). The straight forward implementation materializes the ROLAP views using IUS tables and conventional B-tree indexing. The Cubetree implementation materializes the same ROLAP views using a Cubetree Datablade developed for IUS. The experiments demonstrate that the Cubetree storage organization is superior in storage, query performance and update speed.

#index 248806
#* Caching multidimensional queries using chunks
#@ Prasad M. Deshpande;Karthikeyan Ramasamy;Amit Shukla;Jeffrey F. Naughton
#t 1998
#c 5
#% 191154
#% 210182
#% 227861
#% 227880
#% 462204
#% 463760
#% 464215
#% 464706
#% 479476
#% 481428
#% 481916
#% 481948
#% 481951
#% 482081
#% 566126
#! Caching has been proposed (and implemented) by OLAP systems in order to reduce response times for multidimensional queries. Previous work on such caching has considered table level caching and query level caching. Table level caching is more suitable for static schemes. On the other hand, query level caching can be used in dynamic schemes, but is too coarse for “large” query results. Query level caching has the further drawback for small query results in that it is only effective when a new query is subsumed by a previously cached query. In this paper, we propose caching small regions of the multidimensional space called “chunks”. Chunk-based caching allows fine granularity caching, and allows queries to partially reuse the results of previous queries with which they overlap. To facilitate the computation of chunks required by a query but missing from the cache, we propose a new organization for relational tables, which we call a “chunked file.” Our experiments show that for workloads that exhibit query locality, chunked caching combined with the chunked file organization performs better than query level caching. An unexpected benefit of the chunked file organization is that, due to its multidimensional clustering properties, it can significantly improve the performance of queries that “miss” the cache entirely as compared to traditional file organizations.

#index 248807
#* Simultaneous optimization and evaluation of multiple dimensional queries
#@ Yihong Zhao;Prasad M. Deshpande;Jeffrey F. Naughton;Amit Shukla
#t 1998
#c 5
#% 36117
#% 169337
#% 210182
#% 227861
#% 340319
#% 462025
#% 463760
#% 481288
#% 481428
#% 481604
#% 481608
#% 482082
#! Database researchers have made significant progress on several research issues related to multidimensional data analysis, including the development of fast cubing algorithms, efficient schemes for creating and maintaining precomputed group-bys, and the design of efficient storage structures for multidimensional data. However, to date there has been little or no work on multidimensional query optimization. Recently, Microsoft has proposed “OLE DB for OLAP” as a standard multidimensional interface for databases. OLE DB for OLAP defines Multi-Dimensional Expressions (MDX), which have the interesting and challenging feature of allowing clients to ask several related dimensional queries in a single MDX expression. In this paper, we present three algorithms to optimize multiple related dimensional queries. Two of the algorithms focus on how to generate a global plan from several related local plans. The third algorithm focuses on generating a good global plan without first generating local plans. We also present three new query evaluation primitives that allow related query plans to share portions of their evaluation. Our initial performance results suggest that the exploitation of common subtask evaluation and global optimization can yield substantial performance improvements when relational database systems are used as data sources for multidimensional analysis.

#index 248808
#* NoDoSE—a tool for semi-automatically extracting structured and semistructured data from text documents
#@ Brad Adelberg
#t 1998
#c 5
#% 43653
#% 75426
#% 464720
#% 511733
#! Often interesting structured or semistructured data is not in database systems but in HTML pages, text files, or on paper. The data in these formats is not usable by standard query processing engines and hence users need a way of extracting data from these sources into a DBMS or of writing wrappers around the sources. This paper describes NoDoSE, the Northwestern Document Structure Extractor, which is an interactive tool for semi-automatically determining the structure of such documents and then extracting their data. Using a GUI, the user hierarchically decomposes the file, outlining its interesting regions and then describing their semantics. This task is expedited by a mining component that attempts to infer the grammar of the file from the information the user has input so far. Once the format of a document has been determined, its data can be extracted into a number of useful forms. This paper describes both the NoDoSE architecture, which can be used as a test bed for structure mining algorithms in general, and the mining algorithms that have been developed by the author. The prototype, which is written in Java, is described and experiences parsing a variety of documents are reported.

#index 248809
#* Extracting schema from semistructured data
#@ Svetlozar Nestorov;Serge Abiteboul;Rajeev Motwani
#t 1998
#c 5
#% 36683
#% 55349
#% 210214
#% 237191
#% 282516
#% 384978
#% 395735
#% 459260
#% 462062
#% 464720
#% 464724
#% 479465
#% 481095
#% 482083
#! Semistructured data is characterized by the lack of any fixed and rigid schema, although typically the data has some implicit structure. While the lack of fixed schema makes extracting semistructured data fairly easy and an attractive goal, presenting and querying such data is greatly impaired. Thus, a critical problem is the discovery of the structure implicit in semistructured data and, subsequently, the recasting of the raw data in terms of this structure. In this paper, we consider a very general form of semistructured data based on labeled, directed graphs. We show that such data can be typed using the greatest fixpoint semantics of monadic datalog programs. We present an algorithm for approximate typing of semistructured data. We establish that the general problem of finding an optimal such typing is NP-hard, but present some heuristics and techniques based on clustering that allow efficient and near-optimal treatment of the problem. We also present some preliminary experimental results.

#index 248810
#* Enhanced hypertext categorization using hyperlinks
#@ Soumen Chakrabarti;Byron Dom;Piotr Indyk
#t 1998
#c 5
#% 5484
#% 12001
#% 40709
#% 64897
#% 142616
#% 142617
#% 151407
#% 164216
#% 165110
#% 186080
#% 197843
#% 203296
#% 211526
#% 219053
#% 223577
#% 232698
#% 288306
#% 396021
#% 406493
#% 434806
#% 448846
#% 449508
#% 459008
#% 465747
#% 481945
#% 482108
#% 482113
#! A major challenge in indexing unstructured hypertext databases is to automatically extract meta-data that enables structured search using topic taxonomies, circumvents keyword ambiguity, and improves the quality of search and profile-based routing and filtering. Therefore, an accurate classifier is an essential component of a hypertext database. Hyperlinks pose new problems not addressed in the extensive text classification literature. Links clearly contain high-quality semantic clues that are lost upon a purely term-based classifier, but exploiting link information is non-trivial because it is noisy. Naive use of terms in the link neighborhood of a document can even degrade accuracy. Our contribution is to propose robust statistical models and a relaxation labeling technique for better classification by exploiting link information in a small neighborhood around documents. Our technique also adapts gracefully to the fraction of neighboring documents having known topics. We experimented with pre-classified samples from Yahoo!1 and the US Patent Database2. In previous work, we developed a text classifier that misclassified only 13% of the documents in the well-known Reuters benchmark; this was comparable to the best results ever obtained. This classifier misclassified 36% of the patents, indicating that classifying hypertext can be more difficult than classifying text. Naively using terms in neighboring documents increased error to 38%; our hypertext classifier reduced it to 21%. Results with the Yahoo! sample were more dramatic: the text classifier showed 68% error, whereas our hypertext classifier reduced this to only 21%.

#index 248811
#* Cost-based optimization of decision support queries using transient-views
#@ Subbu N. Subramanian;Shivakumar Venkataraman
#t 1998
#c 5
#% 36117
#% 58377
#% 111913
#% 116043
#% 172927
#% 198465
#% 210182
#% 210208
#% 286991
#% 287667
#% 318049
#% 411554
#% 411750
#% 412935
#% 462204
#% 463919
#% 464056
#% 464706
#% 464720
#% 479449
#% 479452
#% 481916
#% 481923
#% 481944
#% 482081
#! Next generation decision support applications, besides being capable of processing huge amounts of data, require the ability to integrate and reason over data from multiple, heterogeneous data sources. Often, these data sources differ in a variety of aspects such as their data models, the query languages they support, and their network protocols. Also, typically they are spread over a wide geographical area. The cost of processing decision support queries in such a setting is quite high. However, processing these queries often involves redundancies such as repeated access of same data source and multiple execution of similar processing sequences. Minimizing these redundancies would significantly reduce the query processing cost. In this paper, we (1) propose an architecture for processing complex decision support queries involving multiple, heterogeneous data sources; (2) introduce the notion of transient-views — materialized views that exist only in the context of execution of a query — that is useful for minimizing the redundancies involved in the execution of these queries; (3) develop a cost-based algorithm that takes a query plan as input and generates an optimal “covering plan”, by minimizing redundancies in the original plan; (4) validate our approach by means of an implementation of the algorithms and a detailed performance study based on TPC-D benchmark queries on a commercial database system; and finally, (5) compare and contrast our approach with work in related areas, in particular, the areas of answering queries using views and optimization using common sub-expressions. Our experiments demonstrate the practicality and usefulness of transient-views in significantly improving the performance of decision support queries.

#index 248812
#* New sampling-based summary statistics for improving approximate query answers
#@ Phillip B. Gibbons;Yossi Matias
#t 1998
#c 5
#% 1331
#% 1682
#% 2833
#% 69273
#% 77967
#% 152585
#% 201921
#% 210190
#% 214073
#% 214602
#% 227883
#% 227917
#% 281768
#% 282401
#% 320240
#% 452838
#% 463285
#% 480953
#% 481266
#% 481290
#% 481749
#% 482095
#% 482123
#% 571294
#! In large data recording and warehousing environments, it is often advantageous to provide fast, approximate answers to queries, whenever possible. Before DBMSs providing highly-accurate approximate answers can become a reality, many new techniques for summarizing data and for estimating answers from summarized data must be developed. This paper introduces two new sampling-based summary statistics, concise samples and counting samples, and presents new techniques for their fast incremental maintenance regardless of the data distribution. We quantify their advantages over standard sample views in terms of the number of additional sample points for the same view size, and hence in providing more accurate query answers. Finally, we consider their application to providing fast approximate answers to hot list queries. Our algorithms maintain their accuracy in the presence of ongoing insertions to the data warehouse.

#index 248813
#* Integrating association rule mining with relational database systems: alternatives and implications
#@ Sunita Sarawagi;Shiby Thomas;Rakesh Agrawal
#t 1998
#c 5
#% 123589
#% 152934
#% 172958
#% 211931
#% 216508
#% 227917
#% 232102
#% 232136
#% 248784
#% 443091
#% 459006
#% 463883
#% 481758
#% 481779
#% 481954
#! Data mining on large data warehouses is becoming increasingly important. In support of this trend, we consider a spectrum of architectural alternatives for coupling mining with database systems. These alternatives include: loose-coupling through a SQL cursor interface; encapsulation of a mining algorithm in a stored procedure; caching the data to a file system on-the-fly and mining; tight-coupling using primarily user-defined functions; and SQL implementations for processing in the DBMS. We comprehensively study the option of expressing the mining algorithm in the form of SQL queries using Association rule mining as a case in point. We consider four options in SQL-92 and six options in SQL enhanced with object-relational extensions (SQL-OR). Our evaluation of the different architectural alternatives shows that from a performance perspective, the Cache-Mine option is superior, although the performance of the SQL-OR option is within a factor of two. Both the Cache-Mine and the SQL-OR approaches incur a higher storage penalty than the loose-coupling approach which performance-wise is a factor of 3 to 4 worse than Cache-Mine. The SQL-92 implementations were too slow to qualify as a competitive option. We also compare these alternatives on the basis of qualitative factors like automatic parallelization, development ease, portability and inter-operability.

#index 248814
#* Bitmap index design and evaluation
#@ Chee-Yong Chan;Yannis E. Ioannidis
#t 1998
#c 5
#% 136740
#% 191154
#% 201951
#% 223781
#% 227861
#% 227871
#% 466953
#! Bitmap indexing has been touted as a promising approach for processing complex adhoc queries in read-mostly environments, like those of decision support systems. Nevertheless, only few possible bitmap schemes have been proposed in the past and very little is known about the space-time tradeoff that they offer. In this paper, we present a general framework to study the design space of bitmap indexes for selection queries and examine the disk-space and time characteristics that the various alternative index choices offer. In particular, we draw a parallel between bitmap indexing and number representation in different number systems, and define a space of two orthogonal dimensions that captures a wide array of bitmap indexes, both old and new. Within that space, we identify (analytically or experimentally) the following interesting points: (1) the time-optimal bitmap index; (2) the space-optimal bitmap index; (3) the bitmap index with the optimal space-time tradeoff (knee); and (4) the time-optimal bitmap index under a given disk-space constraint. Finally, we examine the impact of bitmap compression and bitmap buffering on the space-time tradeoffs among those indexes. As part of this work, we also describe a bitmap-index-based evaluation algorithm for selection queries that represents an improvement over earlier proposals. We believe that this study offers a useful first set of guidelines for physical database design using bitmap indexes.

#index 248815
#* AutoAdmin “what-if” index analysis utility
#@ Surajit Chaudhuri;Vivek Narasayya
#t 1998
#c 5
#% 36119
#% 210182
#% 248821
#% 411717
#% 458523
#% 462079
#% 462204
#% 482100
#% 565429
#% 566118
#! As databases get widely deployed, it becomes increasingly important to reduce the overhead of database administration. An important aspect of data administration that critically influences performance is the ability to select indexes for a database. In order to decide the right indexes for a database, it is crucial for the database administrator (DBA) to be able to perform a quantitative analysis of the existing indexes. Furthermore, the DBA should have the ability to propose hypothetical (“what-if”) indexes and quantitatively analyze their impact on performance of the system. Such impact analysis may consist of analyzing workloads over the database, estimating changes in the cost of a workload, and studying index usage while taking into account projected changes in the sizes of the database tables. In this paper we describe a novel index analysis utility that we have prototyped for Microsoft SQL Server 7.0. We describe the interfaces exposed by this utility that can be leveraged by a variety of front-end tools and sketch important aspects of the user interfaces enabled by the utility. We also discuss the implementation techniques for efficiently supporting “what-if” indexes. Our framework can be extended to incorporate analysis of other aspects of physical database design.

#index 248816
#* On parallel processing of aggregate and scalar functions in object-relational DBMS
#@ Michael Jaedicke;Bernhard Mitschang
#t 1998
#c 5
#% 43162
#% 115661
#% 136740
#% 152940
#% 158047
#% 201883
#% 210170
#% 210206
#% 227875
#% 227934
#% 227962
#% 380546
#% 420053
#% 442706
#% 464007
#% 479452
#% 479460
#% 479467
#% 479469
#% 481599
#% 481608
#% 481915
#% 481919
#% 481930
#% 571294
#% 637779
#! Nowadays parallel object-relational DBMS are envisioned as the next great wave, but there is still a lack of efficient implementation concepts for some parts of the proposed functionality. Thus one of the current goals for parallel object-relational DBMS is to move towards higher performance. In this paper we develop a framework that allows to process user-defined functions with data parallelism. We will describe the class of partitionable functions that can be processed parallelly. We will also propose an extension which allows to speed up the processing of another large class of functions by means of parallel sorting. Functions that can be processed by means of our techniques are often used in decision support queries on large data volumes, for example. Hence a parallel execution is indispensable.

#index 248817
#* Secure and portable database extensibility
#@ Michael Godfrey;Tobias Mayr;Praveen Seshadri;Thorsten von Eicken
#t 1998
#c 5
#% 151540
#% 172939
#% 202153
#% 210177
#% 215948
#% 237225
#% 242180
#% 442705
#% 442706
#% 464007
#% 479467
#% 480260
#% 664555
#% 979323
#! The functionality of extensible database servers can be augmented by user-defined functions (UDFs). However, the server's security and stability are concerns whenever new code is incorporated. Recently, there has been interest in the use of Java for database extensibility. This raises several questions: Does Java solve the security problems? How does it affect efficiency?We explore the tradeoffs involved in extending the PREDATOR object-relational database server using Java. We also describe some interesting details of our implementation. The issues examined in our study are security, efficiency, and portability. Our performance experiments compare Java-based extensibility with traditional alternatives in the native language of the server. We explore a variety of UDFs that differ in the amount of computation involved and in the quantity of data accessed. We also qualitatively compare the security and portability of the different alternatives. Our conclusion is that Java-based UDFs are a viable approach in terms of performance. However, there may be challenging design issues in integrating Java UDFs with existing database systems.

#index 248818
#* A multi-similarity algebra
#@ S. Adali;P. Bonatti;M. L. Sapino;V. S. Subrahmanian
#t 1998
#c 5
#% 172922
#% 181409
#% 191581
#% 201832
#% 210172
#% 210176
#% 227891
#% 227894
#% 239581
#% 359751
#% 435141
#% 464203
#% 481439
#% 481956
#% 586808
#! The need to automatically extract and classify the contents of multimedia data archives such as images, video, and text documents has led to significant work on similarity based retrieval of data. To date, most work in this area has focused on the creation of index structures for similarity based retrieval. There is very little work on developing formalisms for querying multimedia databases that support similarity based computations and optimizing such queries, even though it is well known that feature extraction and identification algorithms in media data are very expensive. We introduce a similarity algebra that brings together relational operators and results of multiple similarity implementations in a uniform language. The algebra can be used to specify complex queries that combine different interpretations of similarity values and multiple algorithms for computing these values. We prove equivalence and containment relationships between similarity algebra expressions and develop query rewriting methods based on these results. We then provide a generic cost model for evaluating cost of query plans in the similarity algebra and query optimization methods based on this model. We supplement the paper with experimental results that illustrate the use of the algebra and the effectiveness of query optimization methods using the Integrated Search Engine (I.SEE) as the testbed.

#index 248819
#* Catching the boat with Strudel: experiences with a Web-site management system
#@ Mary Fernández;Daniela Florescu;Jaewoo Kang;Alon Levy;Dan Suciu
#t 1998
#c 5
#% 210176
#% 210214
#% 227995
#% 236409
#% 237184
#% 237190
#% 237191
#% 248799
#% 464717
#% 464720
#% 464724
#% 464825
#% 479452
#% 479471
#% 481923
#% 565262
#% 979026
#! The Strudel system applies concepts from database management systems to the process of building Web sites. Strudel's key idea is separating the management of the site's data, the creation and management of the site's structure, and the visual presentation of the site's pages. First, the site builder creates a uniform model of all data available at the site. Second, the builder uses this model to declaratively define the Web site's structure by applying a “site-definition query” to the underlying data. The result of evaluating this query is a “site graph”, which represents both the site's content and structure. Third, the builder specifies the visual presentation of pages in Strudel's HTML-template language. The data model underlying Strudel is a semi-structured model of labeled directed graphs.We describe Strudel's key characteristics, report on our experiences using Strudel, and present the technical problems that arose from our experience. We describe our experience constructing several Web sites with Strudel and discuss the impact of potential users' requirements on Strudel's design. We address two main questions: (1) when does a declarative specification of site structure provide significant benefits, and (2) what are the main advantages provided by the semi-structured data model.

#index 248820
#* Approximate medians and other quantiles in one pass and with limited memory
#@ Gurmeet Singh Manku;Sridhar Rajagopalan;Bruce G. Lindsay
#t 1998
#c 5
#% 2152
#% 210190
#% 281636
#% 340670
#% 411554
#% 427219
#% 482104
#% 669777
#! We present new algorithms for computing approximate quantiles of large datasets in a single pass. The approximation guarantees are explicit, and apply for arbitrary value distributions and arrival distributions of the dataset. The main memory requirements are smaller than those reported earlier by an order of magnitude.We also discuss methods that couple the approximation algorithms with random sampling to further reduce memory requirements. With sampling, the approximation guarantees are explicit but probabilistic, i.e. they apply with respect to a (user controlled) confidence parameter.We present the algorithms, their theoretical analysis and simulation results on different datasets.

#index 248821
#* Random sampling for histogram construction: how much is enough?
#@ Surajit Chaudhuri;Rajeev Motwani;Vivek Narasayya
#t 1998
#c 5
#% 36119
#% 58348
#% 82346
#% 99463
#% 102786
#% 116084
#% 145196
#% 190611
#% 201921
#% 210190
#% 268747
#% 277347
#% 411554
#% 427219
#% 464044
#% 481749
#% 482100
#% 482123
#! Random sampling is a standard technique for constructing (approximate) histograms for query optimization. However, any real implementation in commercial products requires solving the hard problem of determining “How much sampling is enough?” We address this critical question in the context of equi-height histograms used in many commercial products, including Microsoft SQL Server. We introduce a conservative error metric capturing the intuition that for an approximate histogram to have low error, the error must be small in all regions of the histogram. We then present a result establishing an optimal bound on the amount of sampling required for pre-specified error bounds. We also describe an adaptive page sampling algorithm which achieves greater efficiency by using all values in a sampled page but adjusts the amount of sampling depending on clustering of values in pages. Next, we establish that the problem of estimating the number of distinct values is provably difficult, but propose a new error metric which has a reliable estimator and can still be exploited by query optimizers to influence the choice of execution plans. The algorithm for histogram construction was prototyped on Microsoft SQL Server 7.0 and we present experimental results showing that the adaptive algorithm accurately approximates the true histogram over different data distributions.

#index 248822
#* Wavelet-based histograms for selectivity estimation
#@ Yossi Matias;Jeffrey Scott Vitter;Min Wang
#t 1998
#c 5
#% 1331
#% 2115
#% 18658
#% 43163
#% 82346
#% 116084
#% 168862
#% 190330
#% 210190
#% 227866
#% 242366
#% 248023
#% 248812
#% 257637
#% 411554
#% 427219
#% 464062
#% 482092
#% 482123
#! Query optimization is an integral part of relational database management systems. One important task in query optimization is selectivity estimation, that is, given a query P, we need to estimate the fraction of records in the database that satisfy P. Many commercial database systems maintain histograms to approximate the frequency distribution of values in the attributes of relations.In this paper, we present a technique based upon a multiresolution wavelet decomposition for building histograms on the underlying data distributions, with applications to databases, statistics, and simulation. Histograms built on the cumulative data distributions give very good approximations with limited space usage. We give fast algorithms for constructing histograms and using them in an on-line fashion for selectivity estimation. Our histograms also provide quick approximate answers to OLAP queries when the exact answers are not required. Our method captures the joint distribution of multiple attributes effectively, even when the attributes are correlated. Experiments confirm that our histograms offer substantial improvements in accuracy over random sampling and other previous approaches.

#index 248823
#* Efficient transparent application recovery in client-server information systems
#@ David Lomet;Gerhard Weikum
#t 1998
#c 5
#% 321
#% 1823
#% 50718
#% 86930
#% 114582
#% 116063
#% 122904
#% 185412
#% 193442
#% 268755
#% 381812
#% 403195
#% 435120
#% 452488
#% 464823
#% 479456
#% 481264
#% 481624
#% 602679
#% 602709
#% 656846
#% 660923
#! Database systems recover persistent data, providing high database availability. However, database applications, typically residing on client or “middle-tier” application-server machines, may lose work because of a server failure. This prevents the masking of server failures from the human user and substantially degrades application availability. This paper aims to enable high application availability with an integrated method for database server recovery and transparent application recovery in a client-server system. The approach, based on application message logging, is similar to earlier work on distributed system fault tolerance. However, we exploit advanced database logging and recovery techniques and request/reply messaging properties to significantly improve efficiency. Forced log I/Os, frequently required by other methods, are usually avoided. Restart time, for both failed server and failed client, is reduced by checkpointing and log truncation. Our method ensures that a server can recover independently of clients. A client may reduce logging overhead in return for dependency on server availability during client restart.

#index 248824
#* Memory management during run generation in external sorting
#@ Per-Åke Larson;Goetz Graefe
#t 1998
#c 5
#% 64730
#% 131062
#% 136740
#% 234905
#% 252608
#% 443033
#% 463126
#% 463759
#% 473237
#% 480272
#! If replacement selection is used in an external mergesort to generate initial runs, individual records are deleted and inserted in the sort operation's workspace. Variable-length records introduce the need for possibly complex memory management and extra copying of records. As a result, few systems employ replacement selection, even though it produces longer runs than commonly used algorithms. We experimentally compared several algorithms and variants for managing this workspace. We found that the simple best fit algorithm achieves memory utilization of 90% or better and run lengths over 1.8 times workspace size, with no extra copying of records and very little other overhead, for widely varying record sizes and for a wide range of memory sizes. Thus, replacement selection is a viable algorithm for commercial database systems, even for variable-length records.Efficient memory management also enables an external sort algorithm that degrades gracefully when its input is only slightly larger than or a small multiple of the available memory size. This is not the case with the usual implementations of external sorting, which incur I/O for the entire input even if it is as little as one record larger than memory. Thus, in some cases, our techniques may reduce I/O volume by a factor 10 compared to traditional database sort algorithms. Moreover, the gradual rather than abrupt growth in I/O volume for increasing input sizes significantly eases design and implementation of intra-query memory management policies.

#index 248825
#* Replication, consistency, and practicality: are these mutually exclusive?
#@ Todd Anderson;Yuri Breitbart;Henry F. Korth;Avishai Wool
#t 1998
#c 5
#% 9241
#% 102804
#% 112319
#% 210179
#% 237196
#% 237197
#% 380821
#% 403195
#% 435104
#% 461887
#% 461902
#% 531918
#! Previous papers have postulated that traditional schemes for the management of replicated data are doomed to failure in practice due to a quartic (or worse) explosion in the probability of deadlocks. In this paper, we present results of a simulation study for three recently introduced protocols that guarantee global serializability and transaction atomicity without resorting to the two-phase commit protocol. The protocols analyzed in this paper include a global locking protocol [10], a “pessimistic” protocol based on a replication graph [5], and an “optimistic” protocol based on a replication graph [7]. The results of the study show a wide range of practical applicability for the lazy replica-update approach employed in these protocols. We show that under reasonable contention conditions and sufficiently high transaction rate, both replication-graph-based protocols outperform the global locking protocol. The distinctions among the protocols in terms of performance are significant. For example, an offered load where 70% - 80% of transactions under the global locking protocol were aborted, only 10% of transactions were aborted under the protocols based on the replication graph. The results of the study suggest that protocols based on a replication graph offer practical techniques for replica management. However, it also shows that performance deteriorates rapidly and dramatically when transaction throughput reaches a saturation point.

#index 248826
#* Are we working on the right problems? (panel)
#@ Michael Stonebraker
#t 1998
#c 5
#! There appears to be a discrepancy between the research topics being pursued by the database research community and the key problems facing information systems decisions makers such as Chief Information Officers (CIOs). Panelists will present their view of the key problems that would benefit from a research focus in the database research community and will discuss perceived discrepancies. Based on personal experience, the most commonly discussed information systems problems facing CIOs today include:

#index 248827
#* Next generation database systems won't work without semantics! (panel)
#@ John Mylopoulos
#t 1998
#c 5
#! In the late '70s, while second generation DBMS products and technologies were entering the market, there was significant research activity whose aim was to make greater use of semantic information in database systems. The focus of that research was primarily on semantic data models and data modelling, including semantic query processing and integrity checking. However, few if any of the results of these efforts found their way in database technologies of the day, or database management practices. Instead, semantic issues were delegated to early phases of information system development (including requirements analysis and design), as well as application development.Today's database system technologies perform admirably well with semantically trivial operations and representations. At the same time, these technologies are being challenged in virtually every area of data management, with new applications which demand ways of dealing more explicitly with the meaning of the data being managed. For example, interoperation between applications requires that the underlying databases interoperate meaningfully. This currently requires a mammoth manual reverse engineering effort that simply cannot be sustained or funded by any large organization. The same applies to data warehouses, since they too require the correct semantic merging of data from semantically diverse sources. Errors in merging these sources can lead to significant problems of interpretation and potentially of the functions that the warehouse is designed to deliver. The effectiveness of database technologies to web-related information gathering and management applications is likewise limited by the degree to which they can accommodate semantics of the information being sought. Along the same lines, the emergence of organizational knowledge management as the next major application of computing in organizations clearly offers a tremendous opportunity for database technologies. But, again, this opportunity begs the question whether such technologies can succeed if they continue to ignore semantic issues.In summary, semantic issues were put aside by database technologies of the past. However, the database application challenges of the '90s seem to demand solutions to precisely such issues today. This panel intends to examine these long standing research issues on database semantics and their failures to penetrate database technologies. The discussion will also review emerging application areas and their need for mechanisms that deal with data semantics. Finally, the panelists will comment on relevant research tasks that need to be addressed in this long-ignored area of database modeling, management, access, and processing.

#index 248828
#* Electronic commerce: tutorial
#@ Nabil R. Adam;Yelena Yesha
#t 1998
#c 5
#! As we embark on the information age the use of electronic information is spreading through all sectors of society, both nationally and internationally. As a result, commercial organizations, educational institutions and government agencies are finding it essential to be linked by world wide networks, and commercial Internet usage is growing at an accelerating pace.

#index 248829
#* SAP R/3 (tutorial): a database application system
#@ Alfons Kemper;Donald Kossmann;Florian Matthes
#t 1998
#c 5
#! Many database applications in the real world are no longer built on top of a stand-alone database system. Rather, generic (standard) application systems are employed in which the database system is one integrated component. SAP is the market leader for integrated business administration systems, and its SAP R/3 product is a comprehensive software system which integrates modules for finance, material management, sales and distribution, etc. From an architectural point of view, SAP R/3 is a client/server application system with a relational database system as back-end. SAP supports a choice between a variety of commercial relational database products.

#index 248830
#* Java and relational databases (tutorial): SQLJ
#@ Gray Clossman;Phil Shaw;Mark Hapner;Johannes Klein;Richard Pledereder;Brian Becker
#t 1998
#c 5
#! This Tutorial presents the latest developments in the area of Java and Relational Databases. The material is based on the SQLJ consortium effort whose goal is to leverage Java technology for SQL processing. The SQLJ effort is driven by major industry vendors such as Oracle, Sybase, Tandem, JavaSoft, IBM, Informix and others. The SQLJ specifications describe Embedded SQL in Java, Java Stored Procedures, Java UDFs and Java Data Types.

#index 248831
#* High-dimensional index structures database support for next decade's applications (tutorial)
#@ Stefan Berchtold;Daniel A. Keim
#t 1998
#c 5

#index 248832
#* Microsoft universal data access platform
#@ José A. Blakeley;Michael J. Pizzo
#t 1998
#c 5
#% 210178
#! Microsoft Universal Data Access defines a platform for developing multi-tier enterprise applications that require efficient access to diverse relational or non-relational data sources across intranets or the Internet. Universal Data Access consists of a collection of software components that interact with each other using system-level interfaces defined by OLE DB and providing an application-level data access model called ActiveX Data Objects (ADO). This talk provides an overview of the platform.

#index 248833
#* Enterprise Java platform data access
#@ Seth White;Rick Cattell;Shel Finkelstein
#t 1998
#c 5
#! This paper describes alternative methods for data access that are available to developers using the Java™ platform and related technologies to create a new generation of enterprise applications. The paper highlights industry trends and describes Java technologies that are responsible for a new paradigm in data access. Java technology represents a new level of portability, scalability, and ease-of-use for applications that require data access.

#index 248834
#* SQL open heterogeneous data access
#@ Berthold Reinwald;Hamid Pirahesh
#t 1998
#c 5
#! We describe the open, extensible architecture of SQL for accessing data stored in external data sources not managed by the SQL engine. In this scenario, SQL engines act as middleware servers providing access to external data using SQL DML statements and joining external data with SQL tables in heterogeneous queries. We describe the state-of-the art in object-relational systems and their companion products, and provide an outlook on future directions.

#index 248835
#* Managing large systems with DB2 UDB
#@ Chris Eaton
#t 1998
#c 5
#! In this talk, we will describe the usability challenges facing large distributed corporations. As well, we will discuss what IBM's DB2 Universal Database is doing to address these complex issues.

#index 248836
#* Database systems management and Oracle8
#@ C. Gregory Doherty
#t 1998
#c 5
#! Oracle's corporate mission is to enable the Information Age through network computing, a vision of broader access to information for all and the empowerment and increased productivity that can result. The technology implications of the network computing vision are ubiquitous access via low-cost appliances to smaller numbers of larger databases, accessed via professionally managed networks compliant with open internetworking protocols. The latest release of the Oracle data server, Oracle8, provides new technology for management of very large databases containing rich and user-defined data types, and is continuing to evolve to make it economically beneficial to store all forms of digital information in a database.

#index 248837
#* Ubiquitous, self-tuning, scalable servers
#@ Peter Spiro
#t 1998
#c 5
#! Hardware developments allow wonderful reliability and essentially limitless capabilities in storage, networks, memory, and processing power. Costs have dropped dramatically. PCs are becoming ubiquitous.The features and scalability of DBMS software have advanced to the point where most commercial systems can solve virtually all OLTP and DSS requirements.The Internet and application software packages allow rapid deployment and facilitate a broad range of solutions.

#index 248838
#* “Data in your face”: push technology in perspective
#@ Michael Franklin;Stan Zdonik
#t 1998
#c 5
#% 32884
#% 66172
#% 151529
#% 172876
#% 201897
#% 227885
#% 237238
#% 978507

#index 248839
#* The PointCast network (abstract)
#@ Satish Ramakrishnan;Vibha Dayal
#t 1998
#c 5
#! PointCast Inc, the inventor and leader in broadcast news via the Internet and corporate intranets was founded in 1992 to deliver news as it happens from leading sources such as CNN, the New York Times, Wall Street Journal Interactive Edition and more, directly to a viewer's computer screen.The PointCast Network is an integrated client/server system. The system give users control of selecting kinds of information the client retrieves and, within limits, the frequency of those retrievals.The system is divided into client and server segments, referred to as “PointCast Client” and “the DataCenter” respectively. PointCast client is a program that runs on the user's Internet-connected computer, and performs a number of functions in addition to retrieving information from the DataCenter.The server side of the system, known as the PointCast DataCenter, supports the client by providing compelling content in a timely fashion. The Data Center is composed of multiple sites geographically distributed all over US, each carrying a number of industrial strength web servers called “PointServers”. The PointServers are highly customized and “infinitely” scalable to serve close to 200 million requests handled by the PointCast network in a day.The PointCast network receives content from over 100 different sources via satellite links or over the internet. A cluster of servers in the Data Center run customized processes round the clock which assimilate data from various sources to index, format and store it in multiple content databases.This presentation will describe the basic plumbing of the PointCast Network and how some of the challenges of establishing one of the busiest data centers in the world were addressed and implemented. It will focus on following issues:fault toleranceload balancingachieving scalability through pre-caching on serverspackaging information to optimize internet bandwidth usageminimizing data latency.

#index 248840
#* Transactional publish/subscribe: the proactive multicast of database changes (abstract)
#@ Arvola Chan
#t 1998
#c 5
#! For many years, TIBCO (the Information Bus Company) has pioneered the use of Publish/Subscribe—a form of push technology — to build flexible, real-time loosely-coupled distributed applications. Today, Publish/Subscribe is used by 300 of the world's largest financial institutions, deployed in 6 of the top 10 semiconductor manufacturer' factory floors, utilized in the implementation large-scale Internet services like Yahoo, Intuit, and ETrade, and chosen by many of the world's leading corporations as the enterprise infrastructure for integrating disparate applications. In this paper, we will:Contrast the Publish/Subscribe event-driven interaction paradigm against the traditional demand-driven request-reply interaction paradigm;Explain the concepts of subject-based addressing and self-describing messages, the cornerstones of Publish/Subscribe; Describe the scalable implementation of Publish/Subscribe via multicast and broadcast, and the proposed Pragmatic General Multicast Internet standard; andCategorize the qualities of service needed by different kinds of event-driven applications.Today, TIBCO products support:Reliable delivery for front-office applications which require update notifications only while they are online;Guaranteed delivery for back-office applications that cannot afford to lose messages despite network and application failures; andTransactionally guaranteed delivery for those applications that must update databases, consume messages on one set of subjects, and publish messages on another set of subjects, all within properly bracketed atomic transactions. Three different implementations of transactional Publish/Subscribe can be found in:A generic, database independent implementation embodied in TIBCO's Enterprise Transaction Express (ETX) product. ETX optimizes two-phase commit for those applications that span a single database and the messaging system by using the last resource manager optimization. It also supports more complicated transactions by playing the role of an XA-compliant resource manager, leaving the transaction coordination to standard-based transaction monitors.An Informix Universal Server specific extension package called the TIBCO Message Blade. This extends the SQL language with TIBCO-provided User Defined Routines (UDRs) for synchronous Publish / Subscribe operations. In general, UDRs can be used inside stored procedures and triggers to publish and consume (potentially complex) structured messages. The need for two-phase commit is finessed by storing messages in the same database that houses application tables.A bidirectional bridge between Oracle 8's Advanced Queueing (AQ) facility and TIBCO's TIB/Rendezvous guaranteed message delivery implementation. Oracle AQ supports enqueue and dequeue operations to queues (actually implemented as Oracle tables) that can be performed as part of database transactions. The bridge dequeues from Oracle queues and republishes on the Information Bus. Conversely, the bridge subscribes to TIB/Rendezvous messages and enqueues them to Oracle queues for consumption by Oracle applications. Multiple bridges can be used to route AQ messages from one Oracle database to another.

#index 248841
#* A data mining application: customer retention at the Port of Singapore Authority (PSA)
#@ KianSing Ng;Huan Liu;HweeBong Kwah
#t 1998
#c 5
#% 136350
#% 210160
#% 232106
#% 232170
#% 404362
#% 449588
#% 463903
#% 480940
#% 481290
#% 481588
#! “Customer retention” is an important real-world problem in many sales and services related industries today. This work illustrates how we can integrate the various techniques of data-mining, such as decision-tree induction, deviation analysis and multiple concept-level association rules to form an intuitive and novel approach to gauging customer's loyalty and predicting their likelihood of defection. Immediate action taken against these “early-warnings” is often the key to the eventual retention or loss of the customers involved.

#index 248842
#* Oracle Rdb's record caching model
#@ Richard Anderson;Gopalan Arun;Richard Frank
#t 1998
#c 5
#! In this paper we present a more efficient record based caching model than the conventional page (disk block) based scheme. In a record caching model, individual records are stored together in a section of shared memory to form the cache. Traditional relational database systems have individual pages that are stored together in shared memory to form the cache and records are then extracted from these pages on demand. The record cache model has better memory utilization than the page model and also helps reduce overheads like page fetches/writes, page locks and code path.In May 1996, Oracle Rdb announced a record breaking number of 14227 tpmC on a Digital AlphaServer 8400. At the time, this was the best TPC-C performance achieved on a single SMP machine. A total of 15 record caches, caching 19.5 million records, consuming almost 7 GB of memory, formed the bulk of the shared memory.

#index 248843
#* 50,000 users on an Oracle8 universal server database
#@ Tirthankar Lahiri;Ashok Joshi;Amit Jasuja;Sumanta Chatterjee
#t 1998
#c 5
#% 479468
#% 481937
#! In this paper, we describe the Oracle Large User Population Demonstration and highlight the scalability mechanisms in the Oracle8 Universal Data Server which make it possible to support as many as 50,000 concurrent users on a single Oracle8 database without any middle-tier TP-monitor software. Supporting such large user populations requires many mechanisms for high concurrency and throughput. Algorithms in all areas of the server ranging from process and buffer management to SQL compilation and execution must be designed to be highly scalable. Efficient resource sharing mechanisms are required to prevent server-side resource requirements from growing unboundedly with the number of users. Parallel execution across multiple systems is necessary to allow user-population and throughput to scale beyond the restrictions of a single system. In addition to scalability, mechanisms for high availability, ease-of-use, and rich functionality are necessary for supporting complex user applications typical of realistic workloads. All mechanisms must be portable to a wide variety of installations ranging from desk-top systems to large scale enterprise servers and to a wide variety of operating systems.

#index 248844
#* About Quark Digital Media System
#@ Kamar Aulakh
#t 1998
#c 5
#! In this paper, we describe the Oracle Large User Population Demonstration and highlight the scalability mechanisms in the Oracle8 Universal Data Server which make it possible to support as many as 50,000 concurrent users on a single Oracle8 database without any middle-tier TP-monitor software. Supporting such large user populations requires many mechanisms for high concurrency and throughput. Algorithms in all areas of the server ranging from process and buffer management to SQL compilation and execution must be designed to be highly scalable. Efficient resource sharing mechanisms are required to prevent server-side resource requirements from growing unboundedly with the number of users. Parallel execution across multiple systems is necessary to allow user-population and throughput to scale beyond the restrictions of a single system. In addition to scalability, mechanisms for high availability, ease-of-use, and rich functionality are necessary for supporting complex user applications typical of realistic workloads. All mechanisms must be portable to a wide variety of installations ranging from desk-top systems to large scale enterprise servers and to a wide variety of operating systems.

#index 248845
#* FileNet integrated document management database usage and issues
#@ Daniel S. Whelan
#t 1998
#c 5
#! The FileNet Integrated Document Management (IDM) products consists of a family of client applications and Imaging and Electronic Document Management (EDM) services. These services provide robust facilities for document creation, update, and deletion along with document search capabilities. Document properties are stored in an underlying relational database (RDBMS); document content is stored in files or in a specialized optical disk hierarchical storage manager. FileNet Corporation's Visual WorkFlo® and Ensemble® workflow products can be utilized in conjunction with FileNet's IDM technologies to automate production and ad hoc business processes respectively.This talk will discuss how Integrated Document Management requirements affect an IDM system's usage of a RDBMS. Some of the areas to be discussed include:

#index 248846
#* Developing a high traffic, read-only Web site
#@ John Nauman;Ray Suorsa
#t 1998
#c 5
#! In this paper, we describe some of the considerations for designing highly trafficked web sites with read-only or read mostly characteristics.

#index 248847
#* Real business processing meets the Web
#@ James Chong
#t 1998
#c 5
#! Charles Schwab and Co, Inc. is a major web trader generating a large proportion of its revenue from the Web. That revenue is based on both having a site with lots of useful facilities and also the speed of execution, ability to cope with peaks in demand volumes, and the reliability of the site and its underlying services. James Chong, VP Architecture and Planning at Schwab, will talk about the fundamental infrastructure that supports the Web trading, and his plans for its evolution.

#index 248848
#* Olympic records for data at the 1998 Nagano games
#@ Edwin R. Lassettre
#t 1998
#c 5
#! The 1998 Nagano Olympic games had more intensive demands on data management than any previous Olympics in history. This talk will take you behind the scenes to talk about the technical challenges and the architectures that made it possible to handle 4.5 Terabytes of data and sustain a total of almost 650 million web requests, reaching a peak of over 103K per minute. We will discuss the overall structure of the most comprehensive and heavily used Internet technology application in history. Many products were involved, both hardware and software, but this talk will focus in on the database and web challenges, the technology that made it possible to support this tremendous workload. High availability, data integrity, high performance, support of both SMPs and clustered architectures were among the features and functions that were critical. We will cover the Olympic Results System, the Commentator Information System, Info '98, Games Management, and the Olympic web site that made this information available to the Internet community. The speaker will be Ed Lassettre, IBM Fellow, and a key member of IBM's Olympic team.

#index 248849
#* Delivering high availability for Inktomi search engines
#@ Eric A. Brewer
#t 1998
#c 5
#! Inktomi provides the back-end for several well-known search engines, including Wired's HotBot and Microsoft's MS Start page. The services are supported by a highly available cluster with more than 300 CPUs and several hundred disks.

#index 248850
#* Microsoft.com: a high-scale data management and transaction processing solution
#@ Sherri Kennamer
#t 1998
#c 5
#! Microsoft.com, is the world's largest corporate website both in terms of site visitors and pages served. Overall, it is the fourth-largest website in total visitors behind America Online, Yahoo and Netscape. We offer 250,000 pages of content, viewable in all major browser versions (yes, we aggressively support Netscape), supported by three server farms internationally and featuring content updated as often as every three hours, seven days a week.

#index 248851
#* Free parallel data mining
#@ Bin Li;Dennis Shasha
#t 1998
#c 5
#% 55249
#% 136350
#% 172892
#% 489561
#% 594231
#! Data mining is computationally expensive. Since the benefits of data mining results are unpredictable, organizations may not be willing to buy new hardware for that purpose. We will present a system that enables data mining applications to run in parallel on networks of workstations in a fault-tolerant manner. We will describe our parallelization of a combinatorial pattern discovery algorithm and a classification tree algorithm. We will demonstrate the effectiveness of our system with two real applications: discovering active motifs in protein sequences and predicting foreign exchange rate movement.

#index 248852
#* The Araneus Web-based management system
#@ G. Mecca;P. Atzeni;A. Masci;G. Sindoni;P. Merialdo
#t 1998
#c 5
#% 227995
#% 237194
#% 458745
#% 458746
#% 479471

#index 248853
#* CQ: a personalized update monitoring toolkit
#@ Ling Liu;Calton Pu;Wei Tang;David Buttler;John Biggs;Tong Zhou;Paul Benninghoff;Wei Han;Fenghua Yu
#t 1998
#c 5
#% 631866
#% 635797
#! The CQ project at OGI, funded by DARPA, aims at developing a scalable toolkit and techniques for update monitoring and event-driven information delivery on the net. The main feature of the CQ project is a “personalized update monitoring” toolkit based on continual queries [3]. Comparing with the pure pull (such as DBMSs, various web search engines) and pure push (such as Pointcast, Marimba, Broadcast disks) technology, the CQ project can be seen as a hybrid approach that combines the pull and push technology by supporting personalized update monitoring through a combined client-pull and server-push paradigm.

#index 248854
#* DataSplash
#@ Chris Olston;Allison Woodruff;Alexander Aiken;Michael Chu;Vuk Ercegovac;Mark Lin;Mybrid Spalding;Michael Stonebraker
#t 1998
#c 5
#% 149109
#% 172757
#% 202038
#% 227927
#% 481100
#% 619989
#! Database visualization is an area of growing importance as database systems become larger and more accessible. DataSplash is an easy-to-use, integrated environment for navigating, creating, and querying visual representations of data. We will demonstrate the three main components which make up the DataSplash environment: a navigation system, a direct-manipulation interface for creating and modifying visualizations, and a direct-manipulation visual query system.

#index 248855
#* Microsoft index turning wizard for SQL Server 7.0
#@ Surajit Chaudhuri;Vivek Narasayya
#t 1998
#c 5
#% 248815
#% 482100

#index 248856
#* FlowBack: providing backward recovery for workflow management systems
#@ Bartek Kiepuszewski;Ralf Muhlberger;Maria E. Orlowska
#t 1998
#c 5
#! The Distributed Systems Technology Centre (DSTC) framework for workflow specification, verification and management captures workflows transaction-like behavior for long lasting processes. FlowBack is an advanced prototype functionally enhancing an existing workflow management system by providing process backward recovery. It is based on extensive theoretical research ([3],[4],[5],[6],[8],[9]), and its architecture and construction assumptions are product independent. FlowBack clearly demonstrates the extent to which generic backward recovery can be automated and system supported. The provision of a solution for handling exceptional business process behavior requiring backward recovery makes workflow solutions more suitable for a large class of applications, therefore opening up new dimensions within the market. For the demonstration purpose, FlowBack operates with IBM FlowMark, one of the leading workflow products.

#index 248857
#* Providing database-like access to the Web using queries based on textual similarity
#@ William W. Cohen
#t 1998
#c 5
#% 55490
#% 194290
#% 227994
#% 237190
#% 237192
#% 248801
#% 252834
#% 296931
#% 481923
#% 482083
#! Most databases contain “name constants” like course numbers, personal names, and place names that correspond to entities in the real world. Previous work in integration of heterogeneous databases has assumed that local name constants can be mapped into an appropriate global domain by normalization. Here we assume instead that the names are given in natural language text. We then propose a logic for database integration called WHIRL which reasons explicitly about the similarity of local names, as measured using the vector-space model commonly adopted in statistical information retrieval. An implemented data integration system based on WHIRL has been used to successfully integrate information from several dozen Web sites in two domains.

#index 248858
#* Ariadne: a system for constructing mediators for Internet sources
#@ José Luis Ambite;Naveen Ashish;Greg Barish;Craig A. Knoblock;Steven Minton;Pragnesh J. Modi;Ion Muslea;Andrew Philpot;Sheila Tejada
#t 1998
#c 5
#% 201976
#% 213437
#% 511733
#% 544775
#% 1478839
#! The Web is based on a browsing paradigm that makes it difficult to retrieve and integrate data from multiple sites. Today, the only way to achieve this integration is by building specialized applications, which are time-consuming to develop and difficult to maintain. We are addressing this problem by creating the technology and tools for rapidly constructing information mediators that extract, query, and integrate data from web sources. The resulting system, called Ariadne, makes it feasible to rapidly build information mediators that access existing web sources.

#index 248859
#* Capability based mediation in TSIMMIS
#@ Chen Li;Ramana Yerneni;Vasilis Vassalos;Hector Garcia-Molina;Yannis Papakonstantinou;Jeffrey Ullman;Murty Valiveti
#t 1998
#c 5
#% 116303
#% 198466
#% 227987
#% 229827
#% 340302
#% 463919
#% 464222
#% 479449
#% 479452
#% 481923

#index 248860
#* CONTROL: continuous output and navigation technology with refinement on-line
#@ Ron Avnur;Joseph M. Hellerstein;Bruce Lo;Chris Olston;Bhaskaran Raman;Vijayshankar Raman;Tali Roth;Kirk Wylie
#t 1998
#c 5
#% 227883
#! The CONTROL project at U.C. Berkeley has developed technologies to provide online behavior for data-intensive applications. Using new query processing algorithms, these technologies continuously improve estimates and confidence statistics. In addition, they react to user feedback, thereby giving the user control over the behavior of long-running operations. This demonstration displays the modifications to a database system and the resulting impact on aggregation queries, data visualization, and GUI widgets. We then compare this interactive behavior to batch-processing alternatives.

#index 248861
#* AMDB: an access method debugging tool
#@ Marcel Kornacker;Mehul Shah;Joseph M. Hellerstein
#t 1998
#c 5
#% 237204
#% 481599

#index 248862
#* User-oriented smart-cache for the Web: what you seek is what you get!
#@ Zoé Lacroix;Arnaud Sahuguet;Raman Chandrasekar
#t 1998
#c 5
#% 227995
#% 464825
#% 479465
#% 488117
#% 756766
#! Standard database approaches to querying information on the Web focus on the source(s) and provide a query language based on a given predefined organization (schema) of the data: this is the source-driven approach. However, can the Web be seen as a standard database? There is no super-user in charge of monitoring the source(s) (the data is constantly updated), there is no homogeneous structure (no common explicit structure thus), the Web itself never stops growing, etc. For these reasons, we believe that the source-driven standard approach is not suitable to the Web.As an alternative, we propose a user-oriented approach based on the idea that the schema is a posteriori expressed by the user's needs when asking a query. Given a user query, AKIRA (Agentive Knowledge-based Information Retrieval Architecture) [6] extracts a target structure (structure expressed in the query) and uses standard information retrieval and filtering techniques to access potentially relevant documents.The user-oriented paradigm means that the structure through which the data is viewed does not come from the source but is extracted from the user query. When a user asks a query, the relevant information is retrieved from the Web and stored as is in a cache. Then the information is extracted from the raw data using computational linguistic techniques. The AKIRA cache (smart-cache) represents these extracted layers of meta-information on top of the raw data. The smart-cache is an object-oriented database whose schema is inferred from the user's target structure. It is designed on demand through a library of concepts that can be assembled together to match concepts and meta-concepts required in the user's query. The smart cache can be seen as a view of the Web.To the best of our knowledge, AKIRA is the only system that uses information retrieval and extraction integrated with database techniques to provide maximum flexibility to the user and offer transparent access to the content of Web documents.

#index 248863
#* The multidimensional database system RasDaMan
#@ P. Baumann;A. Dehmel;P. Furtado;R. Ritsch;N. Widmann
#t 1998
#c 5
#% 125595
#% 227934
#% 395735
#% 461921
#% 482106
#! RasDaMan is a universal — i.e., domain-independent — array DBMS for multidimensional arrays of arbitrary size and structure. A declarative, SQL-based array query language offers flexible retrieval and manipulation. Efficient server-based query evaluation is enabled by an intelligent optimizer and a streamlined storage architecture based on flexible array tiling and compression.RasDaMan is being used in several international projects for the management of geo and healthcare data of various dimensionality.

#index 248864
#* Xmas: an extensible main-memory storage system for high-performance applications
#@ Jang Ho Park;Yong Sik Kwon;Ki Hong Kim;Sang Ho Lee;Byoung Dae Park;Sang Kyun Cha
#t 1998
#c 5
#% 12638
#% 114582
#% 187411
#% 240227
#% 442832
#% 479769
#% 480266
#! Xmas is an extensible main-memory storage system for high-performance embedded database applications. Xmas not only provides the core functionality of DBMS, such as data persistence, crash recovery, and concurrency control, but also pursues an extensible architecture to meet the requirements from various application areas. One crucial aspect of such extensibility is that an application developer can compose application-specific, high-level operations with a basic set of operations provided by the system. Called composite actions in Xmas, these operations are processed by a customized Xmas server with minimum interaction with application processes, thus improving the overall performance. This paper first presents the architecture and functionality of Xmas, and then demonstrates a simulation of mobile communication service.

#index 248865
#* MultiMediaMiner: a system prototype for multimedia data mining
#@ Osmar R. Zaïane;Jiawei Han;Ze-Nian Li;Sonny H. Chee;Jenny Y. Chiang
#t 1998
#c 5
#% 232102
#% 394418
#% 420056
#% 437405
#% 481438
#% 514728
#% 581572
#! Multimedia data mining is the mining of high-level multimedia information and knowledge from large multimedia databases. A multimedia data mining system prototype, MultiMediaMiner, has been designed and developed. It includes the construction of a multimedia data cube which facilitates multiple dimensional analysis of multimedia data, primarily based on visual content, and the mining of multiple kinds of knowledge, including summarization, comparison, classification, association, and clustering.

#index 248866
#* SuperSQL: an extended SQL for database publishing and presentation
#@ Motomichi Toyama
#t 1998
#c 5
#% 209414
#% 244105
#% 296929
#% 459019
#! SuperSQL is an extension of SQL that allows query results presented in various media for publishing and presentations with simple but sophisticated formatting capabilities. SuperSQL query can generate various kinds of materials, for example, a LaTeX source file to publish query results in a nested table, HTML or Java source files to present the result on WWW browsers, and other media including MS-Excel worksheet, Tcl/Tk, O2C, etc. O2C is a data manipulation language of O2 and thus useful to migrate data in a relational database to an object oriented database.SuperSQL is meant to provide a theoretical and practical foundation for 4GL-type applications such as report writers and DB/WWW coordinators.In this demonstration, we show how TFE reorganize the query results into various media in a universal way, first by grouping tuples according to an arbitrary tree structured schema, and by translating them with the constructors available in the target media.

#index 248867
#* The IDEA Web lab
#@ Stefano Ceri;Piero Fraternali;Stefano Paraboschi
#t 1998
#c 5
#% 220531
#% 459284
#! With the spreading of the World Wide Web as a uniform and ubiquitous interface to computer applications and information, novel opportunities are offered for introducing significant changes in all organizations and their processes. This demo presents the IDEA Web Laboratory (Web Lab), a Web-based software design environment available on the Internet, which demonstrates a novel approach to the software production process on the Web.

#index 248868
#* DTL's DataSpot: database exploration as easy as browsing the Web…
#@ Shaul Dar;Gadi Entin;Shai Geva;Eran Palmon
#t 1998
#c 5
#% 210214
#% 227995
#% 236416
#% 463919
#% 481602
#% 481923
#! DTL's DataSpot is an advanced, programming-free tool that lets Web designers and database developers automatically publish their databases for Web browser access. DataSpot enables non-technical end users to explore a database using free-form plain language queries combined with hypertext navigation, in a fashion similar to using search engines such as Alta Vista to search text files on the Internet.DataSpot is based on a novel representation of data in the form of a schema-less semi-structured graph called a Web View. The DataSpot Publisher takes one or more possibly heterogeneous databases, predefined knowledge banks such as a thesaurus, and user-defined associations, and creates the Web View. The DataSpot Search Server, which connects to any standard HTTP server, performs searches and navigation against the Web View, generating dynamic HTML pages that are returned to the user. The presentation and navigation of answers are controlled by templates that can be modified by the data provider.The DataSpot product has been successfully deployed in diverse Internet and Intranet application areas, including electronic catalogs, yellow pages, classified ads, help desks and finance.

#index 248869
#* A protein patent query system powered by Kleisli
#@ Jing Chen;Limsoon Wong;Louxin Zhang
#t 1998
#c 5
#% 115462
#% 163444

#index 252358
#* The ins and outs (and everything in between) of data warehousing
#@ Phil Fernandez;Donovan Schneider
#t 1996
#c 5

#index 252359
#* Repository system engineering
#@ Pillip A. Bernstein
#t 1996
#c 5

#index 252360
#* databases and visualization
#@ Daniel A. Keim
#t 1996
#c 5

#index 252361
#* State of the art in workflow management research and products
#@ C. Mohan
#t 1996
#c 5
#! In the last few years, workflow management has become a hot topic in the research community and, especially, in the commercial arena. Workflow management is multidisciplinary in nature encompassing many aspects of computing: database management, distributed client-server systems, transaction management, mobile computing, business process reengineering, integration of legacy and new applications, and heterogeneity of hardware and software. Many academic and industrial research projects are underway. Numerous successful products have been released. Standardization efforts are in progress under the auspices of the Workflow Management Coalition. As has happened in the RDBMS area with respect to some topics, in the workflow area also, some of the important real-life problems faced by customers and product developers are not being tackled by researchers. This tutorial will survey the state of the art in workflow management research and products.

#index 252362
#* Data mining techniques
#@ Jiawei Han
#t 1996
#c 5
#! Data mining, or knowledge discovery in databases, has been popularly recognized as an important research issue with broad applications. We provide a comprehensive survey, in database perspective, on the data mining techniques developed recently. Several major kinds of data mining methods, including generalization, characterization, classification, clustering, association, evolution, pattern matching, data visualization, and meta-rule guided mining, will be reviewed. Techniques for mining knowledge in different kinds of databases, including relational, transaction, object-oriented, spatial, and active databases, as well as global information systems, will be examined. Potential data mining applications and some research issues will also be discussed.

#index 252363
#* Thinksheet: a tool for tailoring complex documents
#@ Peter Piatko;Roman Yangarber;Daoi Lin;Dennis Shasha
#t 1996
#c 5

#index 252364
#* HyperStorM—administering structured documents using object-oriented database technology
#@ Klemens Böhm;Karl Aberer
#t 1996
#c 5
#% 172329
#% 535666

#index 252365
#* DBSim: a simulation tool for predicting database performance
#@ Mark Lefler;Mark Stokrp;Craig Wong
#t 1996
#c 5

#index 252366
#* LORE: a Lightweight Object REpository for semistructured data
#@ Dallan Quass;Jennifer Widom;Roy Goldman;Kevin Haas;Qingshan Luo;Jason McHugh;Svetlozar Nestorov;Anand Rajaraman;Hugo Rivero;Serge Abiteboul;Jeff Ullman;Janet Wiener
#t 1996
#c 5
#% 201976
#% 459260
#% 463919

#index 252367
#* DBMiner: interactive mining of multiple-level knowledge in relational databases
#@ Jaiwei Han;Youngjian Fu;Wei Wang;Jenny Chiang;Osmar R. Zaïane;Krzysztof Koperski
#t 1996
#c 5
#% 232147
#% 452747
#% 481588
#! Based on our years-of-research, a data mining system, DB-Miner, has been developed for interactive mining of multiple-level knowledge in large relational databases. The system implements a wide spectrum of data mining functions, including generalization, characterization, association, classification, and prediction. By incorporation of several interesting data mining techniques, including attribute-oriented induction, progressive deepening for mining multiple-level rules, and meta-rule guided knowledge mining, the system provides a user-friendly, interactive data mining environment with good performance.

#index 252368
#* prospector: a content-based multimedia server for massively parallel architectures
#@ S. Choo;W. O'Connell;G. Linerman;H. Chen;K. Ganapathy;A. Biliris;E. Panagos;D. Schrader
#t 1996
#c 5
#% 210170
#% 463884
#! The Prospector Multimedia Object Manager prototype is a general-purpose content analysis multimedia server designed for massively parallel processor environments. Prospector defines and manipulates user defined functions which are invoked in parallel to analyze/manipulate the contents of multimedia objects. Several computationally intensive applications of this technology based on large persistent datasets include: fingerprint matching, signature verification, face recognition, and speech recognition/translation [OIS96].

#index 252369
#* METU interoperable database system
#@ Asuman Dogac;Ugur Halici;Ebru Kilic;Gokhan Ozhan;Fatma Ozcan;Sena Nural;Cevdet Dengi;Sema Mancuhan;Budak Arpinar;Pinar Koksal;Cem Evrendilek
#t 1996
#c 5
#% 177829
#% 464723

#index 252370
#* SONAR: system for optimized numeric association rules
#@ Takeshi Fukuda;Yasuhiko Morimoto;Shinichi Morishita;Takeshi Tokuyama
#t 1996
#c 5
#% 210162
#% 213977

#index 252371
#* CapBasED-AMS: a capability-based and event-driven activity management system
#@ Patrick C. K. Hung;Helen P. Yeung;Kamalakar Karlapalem
#t 1996
#c 5

#index 252372
#* The MultiView project: object-oriented view technology and applications
#@ E. A. Rundensteiner;H. A. Kuno;Y.-G. Ra;V. Crestana-Taube;M. C. Jones;P. J. Marron
#t 1996
#c 5
#% 221756
#% 443145
#% 463896
#% 464234
#% 480958

#index 252373
#* BeSS: storage support for interactive visualization systems
#@ A. Biliris;T. A. Funkhouser;W. O'Connell;E. Panagos
#t 1996
#c 5
#% 131750
#% 463884

#index 252374
#* The Garlic project
#@ M. Tork Roth;M. Arya;L. Haas;M. Carey;W. Cody;R. Fagin;P. Schwarz;J. Thomas;E. Wimmers
#t 1996
#c 5
#% 238757
#% 614579
#! The goal of the Garlic [1] project is to build a multimedia information system capable of integrating data that resides in different database systems as well as in a variety of non-database data servers. This integration must be enabled while maintaining the independence of the data servers, and without creating copies of their data. "Multimedia" should be interpreted broadly to mean not only images, video, and audio, but also text and application specific data types (e.g., CAD drawings, medical objects, …). Since much of this data is naturally modeled by objects, Garlic provides an object-oriented schema to applications, interprets object queries, creates execution plans for sending pieces of queries to the appropriate data servers, and assembles query results for delivery back to the applications. A significant focus of the project is support for "intelligent" data servers, i.e., servers that provide media-specific indexing and query capabilities [2]. Database optimization technology is being extended to deal with heterogeneous collections of data servers so that efficient data access plans can be employed for multi-repository queries.A prototype of the Garlic system has been operational since January 1995. Queries are expressed in an SQL-like query language that has been extended to include object-oriented features such as reference-valued attributes and nested sets. In addition to a C++ API, Garlic supports a novel query/browser interface called PESTO [3]. This component of Garlic provides end users of the system with a friendly, graphical interface that supports interactive browsing, navigation, and querying of the contents of Garlic databases. Unlike existing interfaces to databases, PESTO allows users to move back and forth seamlessly between querying and browsing activities, using queries to identify interesting subsets of the database, browsing the subset, querying the content of a set-valued attribute of a particularly interesting object in the subset, and so on.

#index 261732
#* Report on the 5th international workshop on knowledge representation meets databases (KRDB'98)
#@ Alex Borgida;Vinay K. Chaudhri;Martin Staudt
#t 1998
#c 5
#% 2655
#% 90639
#% 159120
#% 244102
#% 266095
#% 266237
#% 479456

#index 261733
#* Enhanced nearest neighbour search on the R-tree
#@ King Lum Cheung;Ada Wai-Chee Fu
#t 1998
#c 5
#% 201876
#% 201893
#% 227856
#% 237187
#% 437405
#% 437406
#% 437407
#% 464195
#% 481460
#% 481956
#! Multimedia databases usually deal with huge amounts of data and it is necessary to have an indexing structure such that efficient retrieval of data can be provided. R-Tree with its variations, is a commonly cited indexing method. In this paper we propose an improved nearest neighbor search algorithm on the R-tree and its variants. The improvement lies in the removal of two hueristics that have been used in previous R*-tree work, which we prove cannot improve on the pruning power during a search.

#index 261734
#* Algebraic change propagation for semijoin and outerjoin queries
#@ Timothy Griffin;Bharat Kumar
#t 1998
#c 5
#! Many interesting examples in view maintenance involve semijoin and outerjoin queries. In this paper we develop algebraic change propagation algorithms for the following operators: semijoin, anti-semijoin, left outerjoin, right outerjoin, and full outerjoin.

#index 261735
#* B-tree page size when caching is considered
#@ David Lomet
#t 1998
#c 5
#% 244119

#index 261737
#* The TriGS active object-oriented database system— an overview
#@ G. Kappel;W. Retschitzegger
#t 1998
#c 5
#! The active object-oriented database system TriGS has been developed as part of a larger EC ESPRIT project aiming at the development of next-generation production scheduling and control systems [Huem93]. The goal of this paper is to summarize the work on TriGS which comprises both aspects concerning the development of the active system itself, and guidelines concerning the design of active databases.

#index 261738
#* A case for intelligent disks (IDISKs)
#@ Kimberly Keeton;David A. Patterson;Joseph M. Hellerstein
#t 1998
#c 5
#! Decision support systems (DSS) and data warehousing workloads comprise an increasing fraction of the database market today. I/O capacity and associated processing requirements for DSS workloads are increasing at a rapid rate, doubling roughly every nine to twelve months [38]. In response to this increasing storage and computational demand, we present a computer architecture for decision support database servers that utilizes “intelligent” disks (IDISKs). IDISKs utilize low-cost embedded general-purpose processing, main memory, and high-speed serial communication links on each disk. IDISKs are connected to each other via these serial links and high-speed crossbar switches, overcoming the I/O bus bottleneck of conventional systems. By off-loading computation from expensive desktop processors, IDISK systems may improve cost-performance. More importantly, the IDISK architecture allows the processing of the system to scale with increasing storage demand.

#index 261740
#* Standards in practice
#@ Andrew Eisenberg;Jim Melton
#t 1998
#c 5

#index 261741
#* Database techniques for the World-Wide Web: a survey
#@ Daniela Florescu;Alon Levy;Alberto Mendelzon
#t 1998
#c 5

#index 261743
#* Database research at Columbia University
#@ Shih-Fu Chang;Luis Gravano;Gail E. Kaiser;Kenneth A. Ross;Salvatore J. Stolfo
#t 1998
#c 5

#index 261744
#* The Microsoft database research group
#@ David Lomet;Roger Barga;Surajit Chaudhuri;Paul Larson;Vivek Narasayya
#t 1998
#c 5

#index 265692
#* The complexity of query reliability
#@ Erich Grädel;Yuri Gurevich;Colin Hirsch
#t 1998
#c 5
#% 17273
#% 101922
#% 165651
#% 181038
#% 191616
#% 228817
#% 235023
#% 257866

#index 273680
#* Proceedings of the eighteenth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems
#@ Victor Vianu;Christos Papadimitriou
#t 1999
#c 5

#index 273681
#* On views and XML
#@ Serge Abiteboul
#t 1999
#c 5
#% 663
#% 36683
#% 108499
#% 169812
#% 198465
#% 210214
#% 227995
#% 235914
#% 236416
#% 237191
#% 248038
#% 273924
#% 275423
#% 281149
#% 322880
#% 384978
#% 416003
#% 462212
#% 463919
#% 464056
#% 464720
#% 479621
#% 479629
#% 480121
#% 564419

#index 273682
#* Tracking join and self-join sizes in limited storage
#@ Noga Alon;Phillip B. Gibbons;Yossi Matias;Mario Szegedy
#t 1999
#c 5
#% 1331
#% 82346
#% 137885
#% 190330
#% 201921
#% 210188
#% 214073
#% 227883
#% 242366
#% 248812
#% 248820
#% 273909
#% 273910
#% 277347
#% 282942
#% 300195
#% 452838
#% 481749
#% 482123

#index 273683
#* Hypertree decompositions and tractable queries
#@ Georg Gottlob;Nicola Leone;Francesco Scarcello
#t 1999
#c 5
#% 451
#% 1324
#% 1675
#% 2028
#% 36683
#% 39265
#% 55926
#% 101922
#% 159244
#% 219474
#% 237054
#% 237180
#% 248033
#% 286995
#% 287031
#% 289287
#% 289424
#% 289425
#% 384978
#% 408396
#% 454204
#% 464203
#% 464717
#% 464727
#% 566718
#% 593867
#% 598376
#% 599549
#% 836134

#index 273684
#* Efficiently sequencing tape-resident jobs
#@ Sachin More;S. Muthukrishnan;Elizabeth Shriver
#t 1999
#c 5
#% 213861
#% 232678
#% 249414
#% 408396
#% 479461
#% 481613
#% 503721
#% 588457
#% 612073
#% 682353

#index 273685
#* Processing and optimization of multiway spatial joins using R-trees
#@ Dimitris Papadias;Nikos Mamoulis;Yannis Theodoridis
#t 1999
#c 5
#% 2115
#% 13041
#% 68183
#% 86950
#% 102765
#% 116064
#% 136740
#% 137887
#% 152937
#% 172909
#% 211087
#% 213974
#% 213975
#% 227932
#% 260064
#% 273886
#% 384872
#% 427199
#% 464831
#% 479453
#% 479620
#% 496086
#% 534159
#% 534160

#index 273686
#* Interaction between path and type constraints
#@ Peter Buneman;Wenfie Fan;Scott Weinstein
#t 1999
#c 5
#% 53390
#% 58356
#% 116090
#% 237191
#% 237192
#% 246333
#% 248024
#% 281149
#% 383546
#% 384978
#% 464720

#index 273687
#* Consistent query answers in inconsistent databases
#@ Marcelo Arenas;Leopoldo Bertossi;Jan Chomicki
#t 1999
#c 5
#% 36683
#% 69272
#% 109945
#% 194987
#% 217503
#% 223781
#% 264848
#% 264852
#% 264857
#% 464050
#% 519568
#% 591534

#index 273688
#* Type inference in the polymorphic relational algebra
#@ Jan van den Bussche;Emmanuel Waller
#t 1999
#c 5
#% 58346
#% 77665
#% 77710
#% 148219
#% 157913
#% 160054
#% 168050
#% 168073
#% 205243
#% 210214
#% 214091
#% 229827
#% 238450
#% 245878
#% 247458
#% 464724

#index 273689
#* Reasoning about nested functional dependencies
#@ Carmem S. Hara;Susan B. Davidson
#t 1999
#c 5
#% 3808
#% 6259
#% 10245
#% 114579
#% 215389
#% 248024
#% 286998
#% 287339
#% 287631
#% 287793
#% 374001
#% 384978
#% 700905
#% 836134

#index 273690
#* Open problems in electronic commerce
#@ J. D. Tygar
#t 1999
#c 5

#index 273691
#* Exact and approximate aggregation in constraint query languages
#@ Michael Benedikt;Leonid Libkin
#t 1999
#c 5
#% 7275
#% 55408
#% 56731
#% 66937
#% 90740
#% 101646
#% 114739
#% 166244
#% 184047
#% 190332
#% 191592
#% 213956
#% 213966
#% 220977
#% 224744
#% 227883
#% 237188
#% 245656
#% 246560
#% 248020
#% 248021
#% 248022
#% 248812
#% 273698
#% 384978
#% 483546
#% 587330
#% 593739
#% 675434
#% 758494

#index 273692
#* Minimal data upgrading to prevent inference and association attacks
#@ Steven Dawson;Sabrina De Capitani di Vimercati;Patrick Lincoln;Pierangela Samarati
#t 1999
#c 5
#% 32907
#% 51391
#% 69537
#% 70370
#% 102749
#% 164560
#% 176496
#% 221810
#% 228231
#% 261360
#% 442782
#% 442907
#% 443007
#% 664502
#% 664549

#index 273693
#* A framework for measuring changes in data characteristics
#@ Venkatesh Ganti;Johannes Gehrke;Raghu Ramakrishnan
#t 1999
#c 5
#% 15245
#% 201894
#% 210173
#% 216508
#% 227917
#% 232102
#% 232118
#% 232136
#% 248785
#% 248790
#% 248792
#% 404362
#% 443092
#% 452821
#% 459008
#% 464204
#% 479640
#% 479659
#% 479785
#% 479787
#% 479791
#% 481281
#% 481290
#% 481754
#% 481945
#% 631984
#% 631985

#index 273694
#* Least expected cost query optimization: an exercise in utility
#@ Francis Chu;Joseph Y. Halpern;Praveen Seshadri
#t 1999
#c 5
#% 3771
#% 22494
#% 44876
#% 58376
#% 86949
#% 116043
#% 136740
#% 145196
#% 172900
#% 210190
#% 248793
#% 248795
#% 411554
#% 463444
#% 480955

#index 273695
#* Inherent complexity of recursive queries
#@ Stavros Cosmadakis
#t 1999
#c 5
#% 11797
#% 36683
#% 50078
#% 63789
#% 64436
#% 101623
#% 102546
#% 137770
#% 175735
#% 190340
#% 194120
#% 245653
#% 399235
#% 408396
#% 587329

#index 273696
#* Rewriting aggregate queries using views
#@ Sara Cohen;Werner Nutt;Alexander Serebrenik
#t 1999
#c 5
#% 36683
#% 123118
#% 129572
#% 137867
#% 137871
#% 169844
#% 188853
#% 190638
#% 198465
#% 198473
#% 210208
#% 227949
#% 248034
#% 287336
#% 289266
#% 464056
#% 481604
#% 482081
#% 564419
#% 599549

#index 273697
#* On the complexity of the view-selection problem
#@ Howard Karloff;Milena Mihail
#t 1999
#c 5
#% 210182
#% 214219
#% 217812
#% 275929
#% 408396
#% 462204
#% 464706

#index 273698
#* Querying aggregate data
#@ Stéphane Grumbach;Maurizio Rafanelli;Leonardo Tininini
#t 1999
#c 5
#% 2247
#% 11906
#% 27056
#% 126329
#% 137867
#% 198465
#% 210182
#% 212253
#% 221375
#% 237182
#% 237198
#% 247577
#% 248034
#% 268788
#% 452799
#% 464215
#% 481604
#% 482081
#% 503731
#% 562301
#% 565259
#% 587366

#index 273699
#* Applications of linear algebra in information retrieval and hypertext analysis
#@ Jon Kleinberg;Andrew Tomkins
#t 1999
#c 5
#% 3188
#% 27049
#% 36672
#% 41374
#% 46803
#% 49501
#% 54453
#% 55490
#% 68091
#% 115462
#% 124009
#% 158687
#% 161722
#% 200694
#% 232912
#% 248027
#% 249110
#% 262059
#% 262061
#% 268073
#% 268079
#% 281214
#% 282102
#% 282481
#% 282905
#% 375017
#% 406493
#% 465754
#% 593842
#% 678759
#% 748465
#% 836019

#index 273700
#* Rewriting of regular expressions and regular path queries
#@ Diego Calvanese;Giuseppe De Giacomo;Maurizio Lenzerini;Moshe Y. Vardi
#t 1999
#c 5
#% 32904
#% 172927
#% 198465
#% 198466
#% 210176
#% 210214
#% 236409
#% 237189
#% 237190
#% 237191
#% 237192
#% 248024
#% 248025
#% 248026
#% 248038
#% 248819
#% 268797
#% 273696
#% 459260
#% 462235
#% 464056
#% 464717
#% 464720
#% 464724
#% 482081
#% 571169

#index 273701
#* Query automata
#@ Frank Neven;Thomas Schwentick
#t 1999
#c 5
#% 64414
#% 144038
#% 164221
#% 175524
#% 219492
#% 241159
#% 241166
#% 248011
#% 339373
#% 480084
#% 516520
#% 546132
#% 571069

#index 273702
#* Type inference for queries on semistructured data
#@ Tova Milo;Dan Suciu
#t 1999
#c 5
#% 90847
#% 101947
#% 210214
#% 214091
#% 237192
#% 248799
#% 248819
#% 266095
#% 281149
#% 340295
#% 459260
#% 462235
#% 463919
#% 464724
#% 479465
#% 479783
#% 481935
#% 598376

#index 273703
#* Queries with incomplete answers over semistructured data
#@ Yaron Kanza;Werner Nutt;Yehoshua Sagiv
#t 1999
#c 5
#% 32904
#% 172933
#% 210214
#% 213983
#% 221387
#% 236416
#% 237191
#% 237192
#% 237193
#% 248024
#% 248025
#% 248809
#% 248819
#% 248852
#% 252366
#% 264627
#% 462212
#% 463919
#% 464719
#% 464720
#% 464724
#% 464825
#% 479465
#% 479471
#% 481602
#% 562286
#% 565465
#% 599549
#% 614598

#index 273704
#* Fast time-series searching with scaling and shifting
#@ Kelvin Kam Wing Chu;Man Hon Wong
#t 1999
#c 5
#% 86950
#% 172949
#% 191581
#% 227857
#% 227939
#% 236692
#% 240182
#% 248798
#% 264633
#% 364536
#% 427199
#% 460862
#% 461885
#% 462231
#% 464196
#% 477479
#% 480093
#% 481609
#% 481956
#% 534183
#% 631923

#index 273705
#* Substring selectivity estimation
#@ H. V. Jagadish;Raymond T. Ng;Divesh Srivastava
#t 1999
#c 5
#% 13742
#% 43163
#% 201889
#% 201921
#% 210189
#% 210190
#% 238087
#% 268747
#% 289010
#% 411554
#% 479648
#% 481266

#index 273706
#* On indexing mobile objects
#@ George Kollios;Dimitrios Gunopulos;Vassilis J. Tsotras
#t 1999
#c 5
#% 5337
#% 8917
#% 41684
#% 68091
#% 77928
#% 83322
#% 86950
#% 116065
#% 137893
#% 144870
#% 153260
#% 237205
#% 248028
#% 252304
#% 281731
#% 282343
#% 287070
#% 317933
#% 370597
#% 427199
#% 443181
#% 461923
#% 462617
#% 464847
#% 480093
#% 481770
#% 493207
#% 503869
#% 526835
#% 571296
#% 618583
#% 656697

#index 273707
#* Using LDAP directory caches
#@ Sophie Cluet;Olga Kapitskaia;Divesh Srivastava
#t 1999
#c 5
#% 198465
#% 210214
#% 236409
#% 238087
#% 248806
#% 273897
#% 360716
#% 408396
#% 481916
#% 482081
#% 571216

#index 273708
#* Correctness in general configurations of transactional components
#@ Gustavo Alonso;Armin Feßler;Guy Pardon;Hans-Jörg Schek
#t 1999
#c 5
#% 1486
#% 9241
#% 54033
#% 91076
#% 237199
#% 403195
#% 458568

#index 273709
#* Workflow, transactions and datalog
#@ Anthony J. Bonner
#t 1999
#c 5
#% 1425
#% 1791
#% 9241
#% 29439
#% 55349
#% 89028
#% 101646
#% 102547
#% 111582
#% 122904
#% 158922
#% 182949
#% 185412
#% 248013
#% 248029
#% 264853
#% 273709
#% 289287
#% 384978
#% 404772
#% 456020
#% 458551
#% 472076
#% 532512
#% 562147
#% 562300
#% 565496
#% 598376
#% 598377

#index 273710
#* Atomicity with incompatible presumptions
#@ Yousef J. Al-Houmaily;Panos K. Chrysanthis
#t 1999
#c 5
#% 4619
#% 9241
#% 159735
#% 162309
#% 167271
#% 174328
#% 197715
#% 242463
#% 296438
#% 311785
#% 435104
#% 462073
#% 481129
#% 531907
#% 534393
#% 602803

#index 273711
#* Concurrency control and recovery in transactional process management
#@ Heiko Schuldt;Gustavo Alonso;Hans-Jörg Schek
#t 1999
#c 5
#% 9241
#% 54033
#% 83248
#% 122904
#% 122911
#% 137940
#% 166215
#% 172880
#% 185412
#% 237199
#% 247424
#% 273711
#% 383646
#% 463878
#% 464836
#% 614655

#index 273712
#* Window-accumulated subsequence matching problem is linear
#@ Luc Boasson;Patrick Cegielski;Irène Guessarian;Yuri Matiyasevich
#t 1999
#c 5
#% 93113
#% 101925
#% 120648
#% 120649
#% 121584
#% 183642
#% 229127
#% 289288
#% 408638
#% 464712
#% 474277
#% 546270
#% 546285
#% 600042

#index 273713
#* Locality preserving dictionaries: theory & application to clustering in databases
#@ Vijayshankar Raman
#t 1999
#c 5
#% 70370
#% 210174
#% 214241
#% 227706
#% 232767
#% 244119
#% 248796
#% 288467
#% 978284
#% 979674

#index 273714
#* On two-dimensional indexability and optimal range search indexing
#@ Lars Arge;Vasilis Samoladas;Jeffrey Scott Vitter
#t 1999
#c 5
#% 11274
#% 13041
#% 48321
#% 68089
#% 68091
#% 88056
#% 164362
#% 210355
#% 214112
#% 237204
#% 248015
#% 248016
#% 248028
#% 252304
#% 281731
#% 285932
#% 293712
#% 293719
#% 317933
#% 370597
#% 411694
#% 427199
#% 480093
#% 656697

#index 273715
#* Group updates for relaxed height-balanced trees
#@ Lauri Malmi;Eljas Soisalon-Soininen
#t 1999
#c 5
#% 1451
#% 5381
#% 6116
#% 23890
#% 116086
#% 252608
#% 289178
#% 319053
#% 321250
#% 443093
#% 458517
#% 458528
#% 481628
#% 496450

#index 273885
#* Proceedings of the 1999 ACM SIGMOD international conference on Management of data
#@ Susan B. Davidson;Christos Faloutsos
#t 1999
#c 5

#index 273886
#* Integration of spatial join algorithms for processing multiple inputs
#@ Nikos Mamoulis;Dimitris Papadias
#t 1999
#c 5
#% 2115
#% 3673
#% 13041
#% 86949
#% 86950
#% 136740
#% 152937
#% 153260
#% 172909
#% 201876
#% 201880
#% 210186
#% 210187
#% 227932
#% 227934
#% 248803
#% 252304
#% 260036
#% 273685
#% 286237
#% 289282
#% 318049
#% 427199
#% 462957
#% 463595
#% 464205
#% 464831
#% 479453
#% 479473
#% 479620
#% 479797
#% 503853
#! Several techniques that compute the join between two spatial datasets have been proposed during the last decade. Among these methods, some consider existing indices for the joined inputs, while others treat datasets with no index, providing solutions for the case where at least one input comes as an intermediate result of another database operator. In this paper we analyze previous work on spatial joins and propose a novel algorithm, called slot index spatial join (SISJ), that efficiently computes the spatial join between two inputs, only one of which is indexed by an R-tree. Going one step further, we show how SISJ and other spatial join algorithms can be implemented as operators in a database environment that joins more than two spatial datasets. We study the differences between relational and spatial multiway joins, and propose a dynamic programming algorithm that optimizes the execution of complex spatial queries.

#index 273887
#* Selectivity estimation in spatial databases
#@ Swarup Acharya;Viswanath Poosala;Sridhar Ramaswamy
#t 1999
#c 5
#% 68089
#% 82346
#% 86950
#% 152902
#% 172902
#% 172962
#% 210190
#% 213975
#% 242366
#% 273887
#% 411554
#% 427199
#% 427219
#% 480093
#% 480953
#% 481428
#% 481620
#% 481749
#% 481909
#% 482092
#% 494333
#% 562306
#% 673973
#% 689389
#! Selectivity estimation of queries is an important and well-studied problem in relational database systems. In this paper, we examine selectivity estimation in the context of Geographic Information Systems, which manage spatial data such as points, lines, poly-lines and polygons. In particular, we focus on point and range queries over two-dimensional rectangular data. We propose several techniques based on using spatial indices, histograms, binary space partitionings (BSPs), and the novel notion of spatial skew. Our techniques carefully partition the input rectangles into subsets and approximate each partition accurately. We present a detailed experimental study comparing the proposed techniques and the best known sampling and parametric techniques. We evaluate them using synthetic as well as real-life TIGER datasets. Based on our experiments, we identify a BSP based partitioning that we call Min-Skew which consistently provides the most accurate selectivity estimates for spatial queries. The Min-Skew partitioning can be constructed efficiently, occupies very little space, and provides accurate selectivity estimates over a broad range of spatial queries.

#index 273888
#* Efficient concurrency control in multidimensional access methods
#@ Kaushik Chakrabarti;Sharad Mehrotra
#t 1999
#c 5
#% 3645
#% 9241
#% 83183
#% 114582
#% 123589
#% 152902
#% 169940
#% 214065
#% 227864
#% 237187
#% 286244
#% 380546
#% 403195
#% 415957
#% 462240
#% 480093
#% 481256
#% 481599
#! The importance of multidimensional index structures to numerous emerging database applications is well established. However, before these index structures can be supported as access methods (AMs) in a “commercial-strength” database management system (DBMS), efficient techniques to provide transactional access to data via the index structure must be developed. Concurrent accesses to data via index structures introduce the problem of protecting ranges specified in the retrieval from phantom insertions and deletions (the phantom problem). This paper presents a dynamic granular locking approach to phantom protection in Generalized Search Trees(GiSTs), an index structure supporting an extensible set of queries and data types. The granular locking technique offers a high degree of concurrency and has a low lock overhead. Our experiments show that the granular locking technique (1) scales well under various system loads and (2) similar to the B-tree case, provides a significantly more efficient implementation compared to predicate locking for multidimensional AMs as well. Since a wide variety of multidimensional index structures can be implemented using GiST, the developed algorithms provide a general solution to concurrency control in multidimensional AMs. To the best of our knowledge, this paper provides the first such solution based on granular locking.

#index 273889
#* Snakes and sandwiches: optimal clustering strategies for a data warehouse
#@ H. V. Jagadish;Laks V. S. Lakshmanan;Divesh Srivastava
#t 1999
#c 5
#% 13032
#% 45766
#% 64431
#% 86951
#% 210182
#% 227706
#% 227861
#% 237204
#% 248015
#% 248806
#% 415957
#% 443128
#% 462204
#% 463760
#% 479476
#% 481604
#% 482081
#% 566126
#% 702963
#! Physical layout of data is a crucial determinant of performance in a data warehouse. The optimal clustering of data on disk, for minimizing expected I/O, depends on the query workload. In practice, we often have a reasonable sense of the likelihood of different classes of queries, e.g., 40% of the queries concern calls made from some specific telephone number in some month. In this paper, we address the problem of finding an optimal clustering of records of a fact table on disk, given an expected workload in the form of a probability distribution over query classes.Attributes in a data warehouse fact table typically have hierarchies defined on them (by means of auxiliary dimension tables). The product of the dimensional hierarchy levels forms a lattice and leads to a natural notion of query classes. Optimal clustering in this context is a combinatorially explosive problem with a huge search space (doubly exponential in number of hierarchy levels). We identify an important subclass of clustering strategies called lattice paths, and present a dynamic programming algorithm for finding the optimal lattice path clustering, in time linear in the lattice size. We additionally propose a technique called snaking, which when applied to a lattice path, always reduces its cost. For a representative class of star schemas, we show that for every workload, there is a snaked lattice path which is globally optimal. Further, we prove that the clustering obtained by applying snaking to the optimal lattice path is never much worse than the globally optimal snaked lattice path clustering. We complement our analyses and validate the practical utility of our techniques with experiments using TPC-D benchmark data.

#index 273890
#* OPTICS: ordering points to identify the clustering structure
#@ Mihael Ankerst;Markus M. Breunig;Hans-Peter Kriegel;Jörg Sander
#t 1999
#c 5
#% 36672
#% 86950
#% 210173
#% 221383
#% 248790
#% 248792
#% 252360
#% 365176
#% 443083
#% 479462
#% 479658
#% 479799
#% 481281
#% 481956
#% 527022
#% 551620
#% 937189
#! Cluster analysis is a primary method for database mining. It is either used as a stand-alone tool to get insight into the distribution of a data set, e.g. to focus further analysis and data processing, or as a preprocessing step for other algorithms operating on the detected clusters. Almost all of the well-known clustering algorithms require input parameters which are hard to determine but have a significant influence on the clustering result. Furthermore, for many real-data sets there does not even exist a global parameter setting for which the result of the clustering algorithm describes the intrinsic clustering structure accurately. We introduce a new algorithm for the purpose of cluster analysis which does not produce a clustering of a data set explicitly; but instead creates an augmented ordering of the database representing its density-based clustering structure. This cluster-ordering contains information which is equivalent to the density-based clusterings corresponding to a broad range of parameter settings. It is a versatile basis for both automatic and interactive cluster analysis. We show how to automatically and efficiently extract not only 'traditional' clustering information (e.g. representative points, arbitrary shaped clusters), but also the intrinsic clustering structure. For medium sized data sets, the cluster-ordering can be represented graphically and for very large data sets, we introduce an appropriate visualization technique. Both are suitable for interactive exploration of the intrinsic clustering structure offering additional insights into the distribution and correlation of the data.

#index 273891
#* Fast algorithms for projected clustering
#@ Charu C. Aggarwal;Joel L. Wolf;Philip S. Yu;Cecilia Procopiuc;Jong Soo Park
#t 1999
#c 5
#% 36672
#% 42408
#% 202011
#% 210173
#% 237187
#% 248790
#% 248792
#% 252400
#% 451052
#% 462243
#% 479659
#% 481281
#% 527022
#! The clustering problem is well known in the database literature for its numerous applications in problems such as customer segmentation, classification and trend analysis. Unfortunately, all known algorithms tend to break down in high dimensional spaces because of the inherent sparsity of the points. In such high dimensional spaces not all dimensions may be relevant to a given cluster. One way of handling this is to pick the closely correlated dimensions and find clusters in the corresponding subspace. Traditional feature selection algorithms attempt to achieve this. The weakness of this approach is that in typical high dimensional data mining applications different sets of points may cluster better for different subsets of dimensions. The number of dimensions in each such cluster-specific subspace may also vary. Hence, it may be impossible to find a single small subset of dimensions for all the clusters. We therefore discuss a generalization of the clustering problem, referred to as the projected clustering problem, in which the subsets of dimensions selected are specific to the clusters themselves. We develop an algorithmic framework for solving the projected clustering problem, and test its performance on synthetic data.

#index 273892
#* Logical logging to extend recovery to new domains
#@ David Lomet;Mark Tuttle
#t 1999
#c 5
#% 117
#% 1823
#% 114582
#% 245790
#% 317988
#% 403195
#% 464823
#% 481624
#! Recovery can be extended to new domains at reduced logging cost by exploiting “logical” log operations. During recovery, a logical log operation may read data values from any recoverable object, not solely from values on the log or from the updated object. Hence, we needn't log these values, a substantial saving. In [8], we developed a redo recovery theory that deals with general log operations and proved that the stable database remains recoverable when it is explained in terms of an installation graph. This graph was used to derived a write graph that determines a flush order for cached objects that ensures that the database remains recoverable. In this paper, we introduce a refined write graph that permits more flexible cache management that flushes smaller sets of objects. Using this write graph, we show how: (i) the cache manager can inject its own operations to break up atomic flush sets; and (ii) the recovery process can avoid redoing operations whose effects aren't needed by exploiting generalized recovery LSNs. These advances permit more cost-effective recovery for, e.g., files and applications.

#index 273893
#* Efficient concurrency control for broadcast environments
#@ Jayavel Shanmugasundaram;Arvind Nithrakashyap;Rajendran Sivasankaran;Krithi Ramamritham
#t 1999
#c 5
#% 3645
#% 8194
#% 9241
#% 32884
#% 77005
#% 83127
#% 102802
#% 102803
#% 151529
#% 175205
#% 175253
#% 201897
#% 286967
#% 340318
#% 479466
#% 480941
#% 481777
#% 571219
#% 615031
#% 661499
#! A crucial consideration in environments where data is broadcast to clients is the low bandwidth available for clients to communicate with servers. Advanced applications in such environments do need to read data that is mutually consistent as well as current. However, given the asymmetric communication capabilities and the needs of clients in mobile environments, traditional serializability-based approaches are too restrictive, unnecessary, and impractical. We thus propose the use of a weaker correctness criterion called update consistency and outline mechanisms based on this criterion that ensure (1) the mutual consistency of data maintained by the server and read by clients, and (2) the currency of data read by clients. Using these mechanisms, clients can obtain data that is current and mutually consistent “off the air”, i.e., without contacting the server to, say, obtain locks. Experimental results show a substantial reduction in response times as compared to existing (serializability-based) approaches. A further attractive feature of the approach is that if caching is possible at a client, weaker forms of currency can be obtained while still satisfying the mutual consistency of data.

#index 273894
#* Update propagation protocols for replicated databates
#@ Yuri Breitbart;Raghavan Komondoor;Rajeev Rastogi;S. Seshadri;Avi Silberschatz
#t 1999
#c 5
#% 210179
#% 237197
#% 244570
#% 248825
#% 282313
#% 289224
#% 408396
#% 422875
#% 461887
#! Replication is often used in many distributed systems to provide a higher level of performance, reliability and availability. Lazy replica update protocols, which propagate updates to replicas through independent transactions after the original transaction commits, have become popular with database vendors due to their superior performance characteristics. However, if lazy protocols are used indiscriminately, they can result in non-serializable executions. In this paper, we propose two new lazy update protocols that guarantee serializability but impose a much weaker requirement on data placement than earlier protocols. Further, many naturally occurring distributed systems, like distributed data warehouses, satisfy this requirement. We also extend our lazy update protocols to eliminate all requirements on data placement. The extension is a hybrid protocol that propagates as many updates as possible in a lazy fashion. We implemented our protocols on the Datablitz database system product developed at Bell Labs. We also conducted an extensive performance study which shows that our protocols outperform existing protocols over a wide range of workloads.

#index 273895
#* Belief reasoning in MLS deductive databases
#@ Hasan M. Jamil
#t 1999
#c 5
#% 57386
#% 69537
#% 102749
#% 176496
#% 189739
#% 227956
#% 236413
#% 442702
#% 442958
#% 461901
#% 464695
#% 480945
#% 482091
#% 482102
#% 501120
#% 535022
#% 664665
#! It is envisaged that the application of the multilevel security (MLS) scheme will enhance flexibility and effectiveness of authorization policies in shared enterprise databases and will replace cumbersome authorization enforcement practices through complicated view definitions on a per user basis. However, as advances in this area are being made and ideas crystallized, the concomitant weaknesses of the MLS databases are also surfacing. We insist that the critical problem with the current model is that the belief at a higher security level is cluttered with irrelevant or inconsistent data as no mechanism for attenuation is supported. Critics also argue that it is imperative for MLS database users to theorize about the belief of others, perhaps at different security levels, an apparatus that is currently missing and the absence of which is seriously felt.The impetus for our current research is this need to provide an adequate framework for belief reasoning in MLS databases. We demonstrate that a prudent application of the concept of inheritance in a deductive database setting will help capture the notion of declarative belief and belief reasoning in MLS databases in an elegant way. To this end, we develop a function to compute belief in multiple modes which can be used to reason about the beliefs of other users. We strive to develop a poised and practical logical characterization of MLS databases for the first time based on the inherently difficult concept of non-monotonic inheritance. We present an extension of the acclaimed Datalog language, called the MultiLog, and show that Datalog is a special case of our language. We also suggest an implementation scheme for MultiLog as a front-end for CORAL.

#index 273896
#* A multimedia presentation algebra
#@ S. Adali;M. L. Sapino;V. S. Subrahmanian
#t 1999
#c 5
#% 194180
#% 214693
#% 219919
#% 245787
#% 261741
#% 443220
#% 445270
#% 464069
#% 1180153
#! Over the last few years, there has been a tremendous increase in the number of interactive multimedia presentations prepared by different individuals and organizations. In this paper, we present an algebra for querying multimedia presentation databases. In contrast to the relational algebra, an algebra for interactive multimedia presentations must operate on trees whose branches reflect different possible playouts of a family of presentations. The query language supports selection type operations for locating objects and presentation paths that are of interest to the user, join type operations for combining presentations from multiple databases into a single presentation, and finally set theoretic operations for comparing different databases. The algebra operations can be used to locate presentations with specific properties and also for creating new presentations by borrowing different components from existing ones. We prove a host of equivalence results for queries in this algebra which may be used to build query optimizers for interactive presentation databases.

#index 273897
#* Querying network directories
#@ H. V. Jagadish;Laks V. S. Lakshmanan;Tova Milo;Divesh Srivastava;Dimitra Vista
#t 1999
#c 5
#% 3523
#% 58347
#% 154334
#% 210214
#% 236409
#% 238087
#% 255197
#% 259985
#% 259986
#% 266095
#% 268797
#% 289010
#% 374001
#% 459024
#% 464724
#% 479465
#% 1848842
#! Heirarchically structured directories have recently proliferated with the growth of the Internet, and are being used to store not only address books and contact information for people, but also personal profiles, network resource information, and network and service policies. These systems provide a means for managing scale and heterogeneity, while allowing for conceptual unity and autonomy across multiple directory servers in the network, in a way for superior to what conventional relational or object-oriented databases offer. Yet, in deployed systems today, much of the data is modeled in an ad hoc manner, and many of the more sophisticated “queries” involve navigational access.In this paper, we develop the core of a formal data model for network directories, and propose a sequence of efficiently computable query languages with increasing expressive power. The directory data model can naturally represent rich forms of heterogeneity exhibited in the real world. Answers to queries expressible in our query languages can exhibit the same kinds of heterogeneity. We present external memory algorithms for the evaluation of queries posed in our directory query languages, and prove the efficiency of each algorithm in terms of its I/O complexity. Our data model and query languages share the flexibility and utility of the recent proposals for semi-structured data models, while at the same time effectively addressing the specific needs of network directory applications, which we demonstrate by means of a representative real-life example.

#index 273898
#* Online association rule mining
#@ Christian Hidber
#t 1999
#c 5
#% 152934
#% 227883
#% 227917
#% 248791
#% 464204
#% 481290
#% 481754
#% 481779
#% 511333
#% 673707
#% 673970
#% 678172
#! We present a novel algorithm to compute large itemsets online. The user is free to change the support threshold any time during the first scan of the transaction sequence. The algorithm maintains a superset of all large itemsets and for each itemset a shrinking, deterministic interval on its support. After at most 2 scans the algorithm terminates with the precise support for each large itemset. Typically our algorithm is by an order of magnitude more memory efficient than Apriori or DIC.

#index 273899
#* Optimization of constrained frequent set queries with 2-variable constraints
#@ Laks V. S. Lakshmanan;Raymond Ng;Jiawei Han;Alex Pang
#t 1999
#c 5
#% 152934
#% 172386
#% 201894
#% 210160
#% 210162
#% 216508
#% 227919
#% 227953
#% 236414
#% 248784
#% 248785
#% 248791
#% 248813
#% 464204
#% 479482
#% 479484
#% 479627
#% 481290
#% 481588
#% 481758
#% 481779
#! Currently, there is tremendous interest in providing ad-hoc mining capabilities in database management systems. As a first step towards this goal, in [15] we proposed an architecture for supporting constraint-based, human-centered, exploratory mining of various kinds of rules including associations, introduced the notion of constrained frequent set queries (CFQs), and developed effective pruning optimizations for CFQs with 1-variable (1-var) constraints.While 1-var constraints are useful for constraining the antecedent and consequent separately, many natural examples of CFQs illustrate the need for constraining the antecedent and consequent jointly, for which 2-variable (2-var) constraints are indispensable. Developing pruning optimizations for CFQs with 2-var constraints is the subject of this paper. But this is a difficult problem because: (i) in 2-var constraints, both variables keep changing and, unlike 1-var constraints, there is no fixed target for pruning; (ii) as we show, “conventional” monotonicity-based optimization techniques do not apply effectively to 2-var constraints.The contributions are as follows. (1) We introduce a notion of quasi-succinctness, which allows a quasi-succinct 2-var constraint to be reduced to two succinct 1-var constraints for pruning. (2) We characterize the class of 2-var constraints that are quasi-succinct. (3) We develop heuristic techniques for non-quasi-succinct constraints. Experimental results show the effectiveness of all our techniques. (4) We propose a query optimizer for CFQs and show that for a large class of constraints, the computation strategy generated by the optimizer is ccc-optimal, i.e., minimizing the effort incurred w.r.t. constraint checking and support counting.

#index 273900
#* BOAT—optimistic decision tree construction
#@ Johannes Gehrke;Venkatesh Ganti;Raghu Ramakrishnan;Wei-Yin Loh
#t 1999
#c 5
#% 191910
#% 210162
#% 213977
#% 232102
#% 246747
#% 449529
#% 449588
#% 452821
#% 459008
#% 479640
#% 479643
#% 479787
#% 480940
#% 481945
#% 481949
#% 702615
#! Classification is an important data mining problem. Given a training database of records, each tagged with a class label, the goal of classification is to build a concise model that can be used to predict the class label of future, unlabeled records. A very popular class of classifiers are decision trees. All current algorithms to construct decision trees, including all main-memory algorithms, make one scan over the training database per level of the tree.We introduce a new algorithm (BOAT) for decision tree construction that improves upon earlier algorithms in both performance and functionality. BOAT constructs several levels of the tree in only two scans over the training database, resulting in an average performance gain of 300% over previous work. The key to this performance improvement is a novel optimistic approach to tree construction in which we construct an initial tree using a small subset of the data and refine it to arrive at the final tree. We guarantee that any difference with respect to the “real” tree (i.e., the tree that would be constructed by examining all the data in a traditional way) is detected and corrected. The correction step occasionally requires us to make additional scans over subsets of the data; typically, this situation rarely arises, and can be addressed with little added cost.Beyond offering faster tree construction, BOAT is the first scalable algorithm with the ability to incrementally update the tree with respect to both insertions and deletions over the dataset. This property is valuable in dynamic environments such as data warehouses, in which the training dataset changes over time. The BOAT update operation is much cheaper than completely rebuilding the tree, and the resulting tree is guaranteed to be identical to the tree that would be produced by a complete re-build.

#index 273901
#* Self-tuning histograms: building histograms without looking at data
#@ Ashraf Aboulnaga;Surajit Chaudhuri
#t 1999
#c 5
#% 43163
#% 82346
#% 172902
#% 210190
#% 248793
#% 248820
#% 248822
#% 285932
#% 482092
#% 482123
#% 689389
#! In this paper, we introduce self-tuning histograms. Although similar in structure to traditional histograms, these histograms infer data distributions not by examining the data or a sample thereof, but by using feedback from the query execution engine about the actual selectivity of range selection operators to progressively refine the histogram. Since the cost of building and maintaining self-tuning histograms is independent of the data size, self-tuning histograms provide a remarkably inexpensive way to construct histograms for large data sets with little up-front costs. Self-tuning histograms are particularly attractive as an alternative to multi-dimensional traditional histograms that capture dependencies between attributes but are prohibitively expensive to build and maintain. In this paper, we describe the techniques for initializing and refining self-tuning histograms. Our experimental results show that self-tuning histograms provide a low-cost alternative to traditional multi-dimensional histograms with little loss of accuracy for data distributions with low to moderate skew.

#index 273902
#* Approximate computation of multidimensional aggregates of sparse data using wavelets
#@ Jeffrey Scott Vitter;Min Wang
#t 1999
#c 5
#% 41684
#% 168862
#% 210182
#% 210190
#% 227866
#% 227880
#% 227883
#% 248812
#% 248822
#% 257637
#% 259995
#% 293712
#% 411554
#% 464215
#% 481775
#% 481951
#% 482092
#% 482095
#% 617012
#! Computing multidimensional aggregates in high dimensions is a performance bottleneck for many OLAP applications. Obtaining the exact answer to an aggregation query can be prohibitively expensive in terms of time and/or storage space in a data warehouse environment. It is advantageous to have fast, approximate answers to OLAP aggregation queries.In this paper, we present a novel method that provides approximate answers to high-dimensional OLAP aggregation queries in massive sparse data sets in a time-efficient and space-efficient manner. We construct a compact data cube, which is an approximate and space-efficient representation of the underlying multidimensional array, based upon a multiresolution wavelet decomposition. In the on-line phase, each aggregation query can generally be answered using the compact data cube in one I/O or a smalll number of I/Os, depending upon the desired accuracy.We present two I/O-efficient algorithms to construct the compact data cube for the important case of sparse high-dimensional arrays, which often arise in practice. The traditional histogram methods are infeasible for the massive high-dimensional data sets in OLAP applications. Previously developed wavelet techniques are efficient only for dense data. Our on-line query processing algorithm is very fast and capable of refining answers as the user demands more accuracy. Experiments on real data show that our method provides significantly more accurate results for typical OLAP aggregation queries than other efficient approximation techniques such as random sampling.

#index 273903
#* Multi-dimensional selectivity estimation using compressed histogram information
#@ Ju-Hong Lee;Deok-Hwan Kim;Chin-Wan Chung
#t 1999
#c 5
#% 54047
#% 85279
#% 115996
#% 137887
#% 152917
#% 172902
#% 201921
#% 210172
#% 210173
#% 210190
#% 213981
#% 239682
#% 248010
#% 248790
#% 248796
#% 261855
#% 435124
#% 460862
#% 479648
#% 479658
#% 481266
#% 481281
#% 481620
#% 481749
#% 481956
#% 482092
#! The database query optimizer requires the estimation of the query selectivity to find the most efficient access plan. For queries referencing multiple attributes from the same relation, we need a multi-dimensional selectivity estimation technique when the attributes are dependent each other because the selectivity is determined by the joint data distribution of the attributes. Additionally, for multimedia databases, there are intrinsic requirements for the multi-dimensional selectivity estimation because feature vectors are stored in multi-dimensional indexing trees. In the 1-dimensional case, a histogram is practically the most preferable. In the multi-dimensional case, however, a histogram is not adequate because of high storage overhead and high error rates.In this paper, we propose a novel approach for the multi-dimensional selectivity estimation. Compressed information from a large number of small-sized histogram buckets is maintained using the discrete cosine transform. This enables low error rates and low storage overheads even in high dimensions. In addition, this approach has the advantage of supporting dynamic data updates by eliminating the overhead for periodical reconstructions of the compressed information. Extensive experimental results show advantages of the proposed approach.

#index 273904
#* An efficient bitmap encoding scheme for selection queries
#@ Chee-Yong Chan;Yannis E. Ioannidis
#t 1999
#c 5
#% 191154
#% 227861
#% 248814
#% 462217
#% 466953
#! Bitmap indexes are useful in processing complex queries in decision support systems, and they have been implemented in several commercial database systems. A key design parameter for bitmap indexes is the encoding scheme, which determines the bits that are set to 1 in each bitmap in an index. While the relative performance of the two existing bitmap encoding schemes for simple selection queries of the form “v1 ≤ A ≤ v2” is known (specifically, one of the encoding schemes is better for processing equality queries; i.e., v1 = v2, while the other is better for processing range queries; i.e., v1 v2), it remains an open question whether these two encoding schemes are indeed optimal for their respective query classes in the sense that there is no other encoding scheme with better space-time tradeoff. In this paper, we establish a number of optimality results for the existing encoding schemes; in particular, we prove that neither of the two known schemes is optimal for the class of two-sided range queries. We also propose a new encoding scheme and prove that it is optimal for that class. Finally, we present an experimental study comparing the performance of the new encoding scheme with that of the existing ones as well as four hybrid encoding schemes for both simple selection queries and the more general class of membership queries of the form “A ∈ {v1, v2, .…, vk}”. These results demonstrate that the new encoding scheme has an overall better space-time performance than existing schemes.

#index 273905
#* Query optimization for selections using bitmaps
#@ Ming-Chuan Wu
#t 1999
#c 5
#% 3771
#% 3873
#% 63287
#% 121397
#% 136740
#% 191154
#% 227861
#% 248814
#% 317933
#% 427195
#% 462217
#% 466953
#! Bitmaps are popular indexes for data warehouse (DW) applications and most database management systems offer them today. This paper proposes query optimization strategies for selections using bitmaps. Both continuous and discrete selection criteria are considered. Query optimization strategies are categorized into static and dynamic. Static optimization strategies discussed are the optimal design of bitmaps, and algorithms based on tree and logical reduction. The dynamic optimization discussed is the approach of inclusion and exclusion for both bit-sliced indexes and encoded bitmap indexes.

#index 273906
#* A comparison of selectivity estimators for range queries on metric attributes
#@ Björn Blohsfeld;Dieter Korus;Bernhard Seeger
#t 1999
#c 5
#% 54047
#% 152585
#% 172902
#% 210190
#% 227883
#% 248821
#% 248822
#% 411554
#% 427219
#% 479648
#! In this paper, we present a comparison of nonparametric estimation methods for computing approximations of the selectivities of queries, in particular range queries. In contrast to previous studies, the focus of our comparison is on metric attributes with large domains which occur for example in spatial and temporal databases. We also assume that only small sample sets of the required relations are available for estimating the selectivity. In addition to the popular histogram estimators, our comparison includes so-called kernel estimation methods. Although these methods have been proven to be among the most accurate estimators known in statistics, they have not been considered for selectivity estimation of database queries, so far. We first show how to generate kernel estimators that deliver accurate approximate selectivities of queries. Thereafter, we reveal that two parameters, the number of samples and the so-called smoothing parameter, are important for the accuracy of both kernel estimators and histogram estimators. For histogram estimators, the smoothing parameter determines the number of bins (histogram classes). We first present the optimal smoothing parameter as a function of the number of samples and show how to compute approximations of the optimal parameter. Moreover, we propose a new selectivity estimator that can be viewed as an hybrid of histogram and kernel estimators. Experimental results show the performance of different estimators in practice. We found in our experiments that kernel estimators are most efficient for continuously distributed data sets, whereas for our real data sets the hybrid technique is most promising.

#index 273907
#* Random sampling techniques for space efficient online computation of order statistics of large datasets
#@ Gurmeet Singh Manku;Sridhar Rajagopalan;Bruce G. Lindsay
#t 1999
#c 5
#% 1331
#% 115608
#% 210190
#% 248812
#% 248820
#% 248821
#% 293714
#% 340670
#% 411554
#% 482104
#% 482123
#% 669777
#! In a recent paper [MRL98], we had described a general framework for single pass approximate quantile finding algorithms. This framework included several known algorithms as special cases. We had identified a new algorithm, within the framework, which had a significantly smaller requirement for main memory than other known algorithms. In this paper, we address two issues left open in our earlier paper.First, all known and space efficient algorithms for approximate quantile finding require advance knowledge of the length of the input sequence. Many important database applications employing quantiles cannot provide this information. In this paper, we present a novel non-uniform random sampling scheme and an extension of our framework. Together, they form the basis of a new algorithm which computes approximate quantiles without knowing the input sequence length.Second, if the desired quantile is an extreme value (e.g., within the top 1% of the elements), the space requirements of currently known algorithms are overly pessimistic. We provide a simple algorithm which estimates extreme values using less space than required by the earlier more general technique for computing all quantiles. Our principal observation here is that random sampling is quantifiably better when estimating extreme values than is the case with the median.

#index 273908
#* On random sampling over joins
#@ Surajit Chaudhuri;Rajeev Motwani;Vivek Narasayya
#t 1999
#c 5
#% 1331
#% 99463
#% 102786
#% 145196
#% 164361
#% 190611
#% 210188
#% 227883
#% 248821
#% 427219
#% 463577
#% 479931
#! A major bottleneck in implementing sampling as a primitive relational operation is the inefficiency of sampling the output of a query. It is not even known whether it is possible to generate a sample of a join tree without first evaluating the join tree completely. We undertake a detailed study of this problem and attempt to analyze it in a variety of settings. We present theoretical results explaining the difficulty of this problem and setting limits on the efficiency that can be achieved. Based on new insights into the interaction between join and sampling, we develop join sampling techniques for the settings where our negative results do not apply. Our new sampling algorithms are significantly more efficient than those known earlier. We present experimental evaluation of our techniques on Microsoft's SQL Server 7.0.

#index 273909
#* Join synopses for approximate query answering
#@ Swarup Acharya;Phillip B. Gibbons;Viswanath Poosala;Sridhar Ramaswamy
#t 1999
#c 5
#% 82346
#% 164361
#% 172902
#% 190330
#% 210188
#% 210190
#% 214073
#% 227883
#% 242366
#% 248812
#% 273887
#% 273909
#% 274152
#% 277347
#% 411554
#% 452838
#% 463285
#% 481749
#% 482123
#% 503719
#% 689389
#! In large data warehousing environments, it is often advantageous to provide fast, approximate answers to complex aggregate queries based on statistical summaries of the full data. In this paper, we demonstrate the difficulty of providing good approximate answers for join-queries using only statistics (in particular, samples) from the base relations. We propose join synopses as an effective solution for this problem and show how precomputing just one join synopsis for each relation suffices to significantly improve the quality of approximate answers for arbitrary queries with foreign key joins. We present optimal strategies for allocating the available space among the various join synopses when the query work load is known and identify heuristics for the common case when the work load is not known. We also present efficient algorithms for incrementally maintaining join synopses in the presence of updates to the base relations. Our extensive set of experiments on the TPC-D benchmark database show the effectiveness of join synopses and various other techniques proposed in this paper.

#index 273910
#* Ripple joins for online aggregation
#@ Peter J. Haas;Joseph M. Hellerstein
#t 1999
#c 5
#% 58348
#% 136740
#% 152251
#% 164361
#% 210206
#% 210353
#% 227883
#% 244119
#% 277347
#% 340635
#% 427195
#% 442882
#% 503719
#% 673995
#% 837639
#! We present a new family of join algorithms, called ripple joins, for online processing of multi-table aggregation queries in a relational database management system (DBMS). Such queries arise naturally in interactive exploratory decision-support applications.Traditional offline join algorithms are designed to minimize the time to completion of the query. In contrast, ripple joins are designed to minimize the time until an acceptably precise estimate of the query result is available, as measured by the length of a confidence interval. Ripple joins are adaptive, adjusting their behavior during processing in accordance with the statistical properties of the data. Ripple joins also permit the user to dynamically trade off the two key performance factors of on-line aggregation: the time between successive updates of the running aggregate, and the amount by which the confidence-interval length decreases at each update. We show how ripple joins can be implemented in an existing DBMS using iterators, and we give an overview of the methods used to compute confidence intervals and to adaptively optimize the ripple join “aspect-ratio” parameters. In experiments with an initial implementation of our algorithms in the POSTGRES DBMS, the time required to produce reasonably precise online estimates was up to two orders of magnitude smaller than the time required for the best offline join algorithms to produce exact answers.

#index 273911
#* An adaptive query execution system for data integration
#@ Zachary G. Ives;Daniela Florescu;Marc Friedman;Alon Levy;Daniel S. Weld
#t 1999
#c 5
#% 136740
#% 159337
#% 172900
#% 201947
#% 210176
#% 210178
#% 213437
#% 227719
#% 229827
#% 248793
#% 248795
#% 248801
#% 259996
#% 259997
#% 264263
#% 287667
#% 340635
#% 443235
#% 459027
#% 479452
#% 479485
#% 481923
#% 482108
#% 571217
#% 571294
#! Query processing in data integration occurs over network-bound, autonomous data sources. This requires extensions to traditional optimization and execution techniques for three reasons: there is an absence of quality statistics about the data, data transfer rates are unpredictable and bursty, and slow or unavailable data sources can often be replaced by overlapping or mirrored sources. This paper presents the Tukwila data integration system, designed to support adaptivity at its core using a two-pronged approach. Interleaved planning and execution with partial optimization allows Tukwila to quickly recover from decisions based on inaccurate estimates. During execution, Tukwila uses adaptive query operators such as the double pipelined hash join, which produces answers quickly, and the dynamic collector, which robustly and efficiently computes unions across overlapping data sources. We demonstrate that the Tukwila architecture extends previous innovations in adaptive execution (such as query scrambling, mid-execution re-optimization, and choose nodes), and we present experimental evidence that our techniques result in behavior desirable for a data integration system.

#index 273912
#* Query optimization in the presence of limited access patterns
#@ Daniela Florescu;Alon Levy;Ioana Manolescu;Dan Suciu
#t 1999
#c 5
#% 83154
#% 152940
#% 198466
#% 201936
#% 210206
#% 210207
#% 229827
#% 248834
#% 273923
#% 277328
#% 459014
#% 479452
#% 481101
#% 481915
#% 481923
#% 482115
#% 482116
#% 571091
#% 571169
#! We consider the problem of query optimization in the presence of limitations on access patterns to the data (i.e., when one must provide values for one of the attributes of a relation in order to obtain tuples). We show that in the presence of limited access patterns we must search a space of annotated query plans, where the annotations describe the inputs that must be given to the plan. We describe a theoretical and experimental analysis of the resulting search space and a novel query optimization algorithm that is designed to perform well under the different conditions that may arise. The algorithm searches the set of annotated query plans, pruning invalid and non-viable plans as early as possible in the search space, and it also uses a best-first search strategy in order to produce a first complete plan early in the search. We describe experiments to illustrate the performance of our algorithm.

#index 273913
#* Query processing techniques for arrays
#@ Arunprasad P. Marathe;Kenneth Salem
#t 1999
#c 5
#% 71862
#% 86929
#% 111368
#% 136740
#% 210184
#% 380546
#% 435138
#% 442826
#% 461921
#% 462072
#% 463760
#% 479459
#% 479467
#% 482093
#% 482106
#! Arrays are an appropriate data model for images, gridded output from computational models, and other types of data. This paper describes an approach to array query processing. Queries are expressed in AML, a logical algebra that is easily extended with user-defined functions to support a wide variety of array operations. For example, compression, filtering, and algebraic operations on images can be described. We show how AML expressions involving such operations can be treated declaratively and subjected to useful rewrite optimizations. We also describe a plan generator that produces efficient iterator-based plans from rewritten AML expressions.

#index 273914
#* Mind your vocabulary: query mapping across heterogeneous information sources
#@ Chen-Chuan K. Chang;Héctor García-Molina
#t 1999
#c 5
#% 116303
#% 189781
#% 198466
#% 213982
#% 227992
#% 249139
#% 267451
#% 340302
#% 408638
#% 443052
#% 459241
#% 464222
#% 464717
#% 479449
#% 479452
#% 481923
#% 632002
#% 672631
#% 707146
#% 1499471
#! In this paper we present a mechanism for translating constraint queries, i.e., Boolean expressions of constraints, across heterogeneous information sources. Integrating such systems is difficult in part because they use a wide range of constraints as the vocabulary for formulating queries. We describe algorithms that apply user-provided mapping rules to translate query constraints into ones that are understood and supported in another context, e.g., that use the proper operators and value formats. We show that the translated queries minimally subsume the original ones. Furthermore, the translated queries are also the most compact possible. Unlike other query mapping work, we effectively consider inter-dependencies among constraints, i.e., we handle constraints that cannot be translated independently. Furthermore, when constraints are not fully supported, our framework explores relaxations (semantic rewritings) into the closest supported version. Our most sophisticated algorithm (Algorithm TDQM) does not blindly convert queries to DNF (which would be easier to translate, but expensive); instead it performs a top-down mapping of a query tree, and does local query structure conversion only when necessary.

#index 273915
#* Client-site query extensions
#@ Tobias Mayr;Praveen Seshadri
#t 1999
#c 5
#% 554
#% 77944
#% 152940
#% 201936
#% 210177
#% 210206
#% 215948
#% 248817
#% 411554
#% 463574
#% 479937
#% 479938
#% 481101
#% 571071
#! We explore the execution of queries with client-site user-defined functions (UDFs). Many UDFs can only be executed at the client site, for reasons of scalability, security, confidentiality, or availability of resources. How should a query with client-site UDFs be executed? We demonstrate that the standard execution technique for server-site UDFs performs poorly. Instead, we adapt well-known distributed database algorithms and apply them to client-site UDFs. The resulting query execution techniques are implemented in the Cornell Predator database system, and we present performance results to demonstrate their effectiveness. We also consider the question of query optimization in the context of client-site UDFs. The known techniques for expensive UDFs are inadequate because they do not take the location of the UDF into account. We present an extension of traditional 'System-R' optimizers that suitably optimize queries with client-site operations.

#index 273916
#* Bottom-up computation of sparse and Iceberg CUBE
#@ Kevin Beyer;Raghu Ramakrishnan
#t 1999
#c 5
#% 90848
#% 210182
#% 227880
#% 248785
#% 462204
#% 464215
#% 464706
#% 479450
#% 479476
#% 479646
#% 479795
#% 481290
#% 481951
#! We introduce the Iceberg-CUBE problem as a reformulation of the datacube (CUBE) problem. The Iceberg-CUBE problem is to compute only those group-by partitions with an aggregate value (e.g., count) above some minimum support threshold. The result of Iceberg-CUBE can be used (1) to answer group-by queries with a clause such as HAVING COUNT(*) = X, where X is greater than the threshold, (2) for mining multidimensional association rules, and (3) to complement existing strategies for identifying interesting subsets of the CUBE for precomputation.We present a new algorithm (BUC) for Iceberg-CUBE computation. BUC builds the CUBE bottom-up; i.e., it builds the CUBE by starting from a group-by on a single attribute, then a group-by on a pair of attributes, then a group-by on three attributes, and so on. This is the opposite of all techniques proposed earlier for computing the CUBE, and has an important practical advantage: BUC avoids computing the larger group-bys that do not meet minimum support. The pruning in BUC is similar to the pruning in the Apriori algorithm for association rules, except that BUC trades some pruning for locality of reference and reduced memory requirements. BUC uses the same pruning strategy when computing sparse, complete CUBEs.We present a thorough performance evaluation over a broad range of workloads. Our evaluation demonstrates that (in contrast to earlier assumptions) minimizing the aggregations or the number of sorts is not the most important aspect of the sparse CUBE problem. The pruning in BUC, combined with an efficient sort method, enables BUC to outperform all previous algorithms for sparse CUBEs, even for computing entire CUBEs, and to dramatically improve Iceberg-CUBE computation.

#index 273917
#* DynaMat: a dynamic view management system for data warehouses
#@ Yannis Kotidis;Nick Roussopoulos
#t 1999
#c 5
#% 98469
#% 152928
#% 169844
#% 198467
#% 201929
#% 210182
#% 227868
#% 227869
#% 227880
#% 248805
#% 248806
#% 286237
#% 462204
#% 463760
#% 464215
#% 464706
#% 479476
#% 479622
#% 479646
#% 479792
#% 479904
#% 480930
#% 481916
#% 481951
#% 482110
#% 566126
#% 571216
#! Pre-computation and materialization of views with aggregate functions is a common technique in Data Warehouses. Due to the complex structure of the warehouse and the different profiles of the users who submit queries, there is need for tools that will automate the selection and management of the materialized data. In this paper we present DynaMat, a system that dynamically materializes information at multiple levels of granularity in order to match the demand (workload) but also takes into account the maintenance restrictions for the warehouse, such as down time to update the views and space availability. DynaMat unifies the view selection and the view maintenance problems under a single framework using a novel “goodness” measure for the materialized views. DynaMat constantly monitors incoming queries and materializes the best set of views subject to the space constraints. During updates, DynaMat reconciles the current materialized view selection and refreshes the most beneficial subset of it within a given maintenance window. We compare DynaMat against a system that is given all queries in advance and the pre-computed optimal static view selection. The comparison is made based on a new metric, the Detailed Cost Savings Ratio introduced for quantifying the benefits of view materialization against incoming queries. These experiments show that DynaMat's dynamic view selection outperforms the optimal static view selection and thus, any sub-optimal static algorithm that has appeared in the literature.

#index 273918
#* Shrinking the warehouse update Window
#@ Wilburt Juan Labio;Ramana Yerneni;Hector Garcia-Molina
#t 1999
#c 5
#% 152928
#% 201928
#% 201929
#% 210182
#% 210210
#% 227869
#% 227945
#% 227947
#% 458556
#% 464706
#% 479476
#% 482098
#% 482111
#% 682219

#index 273919
#* WALRUS: a similarity retrieval algorithm for image databases
#@ Apostol Natsev;Rajeev Rastogi;Kyuseok Shim
#t 1999
#c 5
#% 86950
#% 196977
#% 210173
#% 228351
#% 257637
#% 437405
#% 729437
#! Traditional approaches for content-based image querying typically compute a single signature for each image based on color histograms, texture, wavelet tranforms etc., and return as the query result, images whose signatures are closest to the signature of the query image. Therefore, most traditional methods break down when images contain similar objects that are scaled differently or at different locations, or only certain regions of the image match.In this paper, we propose WALRUS (WAveLet-based Retrieval of User-specified Scenes), a novel similarity retrieval algorithm that is robust to scaling and translation of objects within an image. WALRUS employs a novel similarity model in which each image is first decomposed into its regions, and the similarity measure between a pair of images is then defined to be the fraction of the area of the two images covered by matching regions from the images. In order to extract regions for an image, WALRUS considers sliding windows of varying sizes and then clusters them based on the proximity of their signatures. An efficient dynamic programming algorithm is used to compute wavelet-based signatures for the sliding windows. Experimental results on real-life data sets corroborate the effectiveness of WALRUS's similarity model that performs similarity matching at a region rather than an image granularity.

#index 273920
#* A new method for similarity indexing of market basket data
#@ Charu C. Aggarwal;Joel L. Wolf;Philip S. Yu
#t 1999
#c 5
#% 23321
#% 67565
#% 86950
#% 115462
#% 115466
#% 152934
#% 201876
#% 217292
#% 227939
#% 248796
#% 248797
#% 427199
#% 435141
#% 462239
#% 464195
#% 480093
#% 480274
#% 481290
#% 482109
#% 527028
#! In recent years, many data mining methods have been proposed for finding useful and structured information from market basket data. The association rule model was recently proposed in order to discover useful patterns and dependencies in such data. This paper discusses a method for indexing market basket data efficiently for similarity search. The technique is likely to be very useful in applications which utilize the similarity in customer buying behavior in order to make peer recommendations. We propose an index called the signature table, which is very flexible in supporting a wide range of similarity functions. The construction of the index structure is independent of the similarity function, which can be specified at query time. The resulting similarity search algorithm shows excellent scalability with increasing memory availability and database size.

#index 273921
#* Efficient geometry-based similarity search of 3D spatial databases
#@ Daniel A. Keim
#t 1999
#c 5
#% 49232
#% 68089
#% 68091
#% 73334
#% 86950
#% 169940
#% 427199
#% 463414
#% 463425
#% 479655
#% 480093
#% 481947
#! Searching a database of 3D-volume objects for objects which are similar to a given 3D search object is an important problem which arises in number of database applications — for example, in Medicine and CAD. In this paper, we present a new geometry-based solution to the problem of searching for similar 3D-volume objects. The problem is motivated from a real application in the medical domain where volume similarity is used as a basis for surgery decisions. Our solution for an efficient similarity search on large databases of 3D volume objects is based on a new geometric index structure. The basic idea of our new approach is to use the concept of hierarchical approximations of the 3D objects to speed up the search process. We formally show the correctness of our new approach and introduce two instantiations of our general idea, which are based on cuboid and octree approximations. We finally provide a performance evaluation of our new index structure revealing significant performance improvements over existing approaches.

#index 273922
#* Storing semistructured data with STORED
#@ Alin Deutsch;Mary Fernandez;Dan Suciu
#t 1999
#c 5
#% 36683
#% 152934
#% 172927
#% 198465
#% 210173
#% 210214
#% 236409
#% 237190
#% 248809
#% 248819
#% 262071
#% 408396
#% 459260
#% 463919
#% 464230
#% 481444
#% 481935
#% 482110
#% 571098
#% 840584
#! Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistructured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.

#index 273923
#* Computing capabilities of mediators
#@ Ramana Yerneni;Chen Li;Hector Garcia-Molina;Jeffrey Ullman
#t 1999
#c 5
#% 116303
#% 213437
#% 227992
#% 248859
#% 340302
#% 464222
#% 481923
#! Existing data-integration systems based on the mediation architecture employ a variety of mechanisms to describe the query-processing capabilities of sources. However, these systems do not compute the capabilities of the mediators based on the capabilities of the sources they integrate. In this paper, we proposed a framework to capture a rich variety of query-processing capabilities of data sources and mediators. We present algorithms to compute the set of supported queries of a mediator, based on the capability limitations of its sources. Our algorithms take into consideration a variety of query-processing techniques employed by mediators to enhance the set of supported queries.

#index 273924
#* Query rewriting for semistructured data
#@ Yannis Papakonstantinou;Vasilis Vassalos
#t 1999
#c 5
#% 36683
#% 100617
#% 123085
#% 198465
#% 201928
#% 210214
#% 213982
#% 227995
#% 229827
#% 236416
#% 237181
#% 237190
#% 242350
#% 248025
#% 248859
#% 261741
#% 273700
#% 289266
#% 462235
#% 463919
#% 464222
#% 464719
#% 464724
#% 479452
#% 479465
#% 481923
#% 564419
#% 571216
#% 599549
#% 631928
#! We address the problem of query rewriting for TSL, a language for querying semistructured data. We develop and present an algorithm that, given a semistructured query q and a set of semistructured views V, finds rewriting queries, i.e., queries that access the views and produce the same result as q. Our algorithm is based on appropriately generalizing containment mappings, the chase, and query composition — techniques that were developed for structured, relational data. We also develop an algorithm for equivalence checking of TSL queries.We show that the algorithm is sound and complete for TSL, i.e., it always finds every non-trivial TSL rewriting query of q, and we discuss its complexity. We extend the rewriting algorithm to use some forms of structural constraints (such as DTDs) and find more opportunities for query rewriting.

#index 273925
#* Record-boundary discovery in Web documents
#@ D. W. Embley;Y. Jiang;Y.-K. Ng
#t 1999
#c 5
#% 92505
#% 237194
#% 240955
#% 244103
#% 244118
#% 248808
#% 259991
#% 464724
#% 511733
#% 535703
#! Extraction of information from unstructured or semistructured Web documents often requires a recognition and delimitation of records. (By “record” we mean a group of information relevant to some entity.) Without first chunking documents that contain multiple records according to record boundaries, extraction of record information will not likely succeed. In this paper we describe a heuristic approach to discovering record boundaries in Web documents. In our approach, we capture the structure of a document as a tree of nested HTML tags, locate the subtree containing the records of interest, identify candidate separator tags within the subtree using five independent heuristics, and select a consensus separator tag based on a combined heuristic. Our approach is fast (runs linearly for practical cases within the context of the larger data-extraction problem) and accurate (100% in the experiments we conducted).

#index 273926
#* Automatic discovery of language models for text databases
#@ Jamie Callan;Margaret Connell;Aiqun Du
#t 1999
#c 5
#% 109209
#% 172898
#% 194246
#% 194275
#% 227891
#% 262063
#% 262065
#% 360914
#% 375076
#% 481748
#! The proliferation of text databases within large organizations and on the Internet makes it difficult for a person to know which databases to search. Given language models that describe the contents of each database, a database selection algorithm such as GIOSS can provide assistance by automatically selecting appropriate databases for an information need. Current practice is that each database provides its language model upon request, but this cooperative approach has important limitations.This paper demonstrates that cooperation is not required. Instead, the database selection service can construct its own language models by sampling database contents via the normal process of running queries and retrieving documents. Although random sampling is not possible, it can be approximated with carefully selected queries. This sampling approach avoids the limitations that characterize the cooperative approach, and also enables additional capabilities. Experimental results demonstrate that accurate language models can be learned from a relatively small number of queries and documents.

#index 273927
#* A layered architecture for querying dynamic Web content
#@ Hasan Davulcu;Juliana Freire;Michael Kifer;I. V. Ramakrishnan
#t 1999
#c 5
#% 116091
#% 169697
#% 189739
#% 198466
#% 210176
#% 229827
#% 244105
#% 248852
#% 248858
#% 252374
#% 255197
#% 261741
#% 266102
#% 285926
#% 287082
#% 411649
#% 459252
#% 462049
#% 479471
#% 481602
#% 481923
#% 614598
#% 836134
#% 1499471
#! The design of webbases, database systems for supporting Web-based applications, is currently an active area of research. In this paper, we propose a 3-year architecture for designing and implementing webbases for querying dynamic Web content(i.e., data that can only be extracted by filling out multiple forms). The lowest layer, virtual physical layer, provides navigation independence by shielding the user from the complexities associated with retrieving data from raw Web sources. Next, the traditional logical layer supports site independence. The top layer is analogous to the external schema layer in traditional databases.Within this architectural framework we address two problems unique to webbases — retrieving dynamic Web content in the virtual physical layer and querying of the external schema by the end user. The layered architecture makes it possible to automate data extraction to a much greater degree than in existing proposals. Wrappers for the virtual physical schema can be created semi-automatically, by asking the webbase designer to navigate through the sites of interest — we call this approach mapping by example. Thus, the webbase designer need not have expertise in the language that maps the physical schema to the raw Web (this should be contrasted to other approaches, which require expertise in various Web-enabled flavors of SQL). For the external schema layer, we propose a semantic extension of the universal relation interface. This interface provides powerful, yet reasonably simple, ad hoc querying capabilities for the end user compared to the currently prevailing “canned” form-based interfaces on the one hand or complex Web-enabling extensions of SQL on the other. Finally, we discuss the implementation of the proposed architecture.

#index 273928
#* “Honey, I shrunk the database”: footprint, mobility, and beyond
#@ Praveen Seshadri
#t 1999
#c 5
#! The small-scale mobile computing market (palmtops, handhelds, smartphones, smartcards, etc.) is showing the most explosive growth in the history of computing. Within the last year, most database vendors have announced plans to build “small footprint” versions of their DBMS products to run on these small and mobile platforms. What are the real database systems issues? What are the likely markets? What are the challenges, and the dangers? Every specific vendor seems to have a different answer to these questions. They all agree on only one thing: there is a tremendous opportunity here. Our motivation for this panel is to get a number of the central decision-makers into a room and debate the issues.

#index 273929
#* Of crawlers, portals, mice, and men: is there more to mining the Web?
#@ Minos N. Garofalakis;Sridhar Ramaswamy;Rajeev Rastogi;Kyuseok Shim
#t 1999
#c 5
#! The World Wide Web is rapidly emerging as an important medium for transacting commerce as well as for the dissemination of information related to a wide range of topics (e.g., business, government, recreation). According to most predictions, the majority of human information will be available on the Web in ten years. These huge amounts of data raise a grand challenge for the database community, namely, how to turn the Web into a more useful information utility. This is exactly the subject that will be addressed by this panel.

#index 273930
#* Data management issues in electronic commerce
#@ M. Tamer Özsu
#t 1999
#c 5
#% 206018
#% 223201
#% 535805

#index 273931
#* Petabyte databases
#@ Dirk Düllmann
#t 1999
#c 5
#! This paper describes the use of Object-Database Management Systems (ODBMS)for the storage of High-Energy Physics (HEP) data.

#index 273932
#* A database perspective on Lotus Domino/Notes
#@ C. Mohan
#t 1999
#c 5
#! In this one-page summary, I introduce the database aspects of Lotus Domino/Notes. These database features are covered in detail in the corresponding SIGMOD99 tutorial available at www.almaden.ibm.com/u/mohan/domino_sigmod99.ps.

#index 273933
#* Hypertext databases and data mining
#@ Soumen Chakrabarti
#t 1999
#c 5
#! The volume of unstructured text and hypertext data far exceeds that of structured data. Text and hypertext are used for digital libraries, product catalogs, reviews, newsgroups, medical reports, customer service reports, and the like. Currently measured in billions of dollars, the worldwide internet activity is expected to reach a trillion dollars by 2002. Database researchers have kept some cautious distance from this action. The goal of this tutorial is to expose database researchers to text and hypertext information retrieval (IR) and mining systems, and to discuss emerging issues in the overlapping areas of databases, hypertext, and data mining.

#index 273934
#* Clustering methods for large databases: from the past to the future
#@ Alexander Hinneburg;Daniel A. Keim
#t 1999
#c 5

#index 273935
#* Managing Web data
#@ Dan Suciu
#t 1999
#c 5

#index 273936
#* O-O, what's happening to DB2?
#@ M. Carey;D. Chamberlin;D. Doole;S. Rielau;N. Mattos;S. Narayanan;B. Vance;R. Swagerman
#t 1999
#c 5
#% 252481
#% 278619
#% 385828
#% 481919
#! In this presentation, we will describe a collection of new object-relational features that have been added to IBM's DB2 Universal Database (UDB) system. The features to be described include support for structured types, object references, and hierarchies of typed tables and views. These features will be covered from the perspective of a database designer or end user. In addition to presenting the features presently available in DB2 UDB V5.2, which became available in Fall 1998, we will discuss the expected evolution and impact of this technology over time.

#index 273937
#* Bringing object-relational technology to the mainstream
#@ Vishu Krishnamurthy;Sandeepan Banerjee;Anil Nori
#t 1999
#c 5
#! Over the last few years, Oracle has evolved its flagship relational database system into an Object-Relational system by adding an extensible type system, object storage, an object cache, an extensible query and indexing framework, support for multimedia datatypes, a server-based scalable Java virtual machine, as well as enhancing its SQL DDL and DML language. These extensions were done with the practical goal of bringing objects to mainstream use.

#index 273938
#* Implementing the spirit of SQL-99
#@ Paul Brown
#t 1999
#c 5
#% 55882
#% 123589
#% 248816
#% 380546
#% 535514
#! This paper describes the current INFORMIX IDS/UD release (9.2 or Centaur) and compares and contrasts its functionality with the features of the SQL-99 language standard. INFORMIX and Illustra have been shipping DBMSs implementing the spirit of the SQL-99 standard for five years. In this paper, we review our experience working with ORDBMS technology, and argue that while SQL-99 is a huge improvement over SQL-92, substantial further work is necessary to make object-relational DBMSs truly useful. Specifically, we describe several interesting pieces of functionality unique to IDS/UD, and several dilemmas our customers have encountered that the standard does not address.

#index 273940
#* DataBlitz storage manager: main-memory database performance for critical applications
#@ J. Baulier;P. Bohannon;S. Gogate;C. Gupta;S. Haldar
#t 1999
#c 5
#% 422875

#index 273941
#* Indexing medium-dimensionality data in Oracle
#@ K. V. Ravi Kanth;Siva Ravada;Jayant Sharma;Jay Banerjee
#t 1999
#c 5
#% 86950
#% 152937
#% 194253
#% 201876
#% 427199
#% 462059
#% 479453

#index 273942
#* EMC information sharing: direct access to MVS data from UNIX and NT
#@ Walt Kohler
#t 1999
#c 5
#! In this extended abstract we briefly describe EMC's information sharing technology that enables UNIX and NT systems to directly access MVS mainframe datasets and how this technology can be used to directly access an MVS DB2 database.

#index 273943
#* Daytona and the fourth-generation language Cymbal
#@ Rick Greer
#t 1999
#c 5
#! The Daytona™ data management system is used by AT&T to solve a wide spectrum of data management problems. For example, Daytona is managing a 4 terabyte data warehouse whose largest table contains over 10 billion rows. Daytona's architecture is based on translating its high-level query language Cymbal (which includes SQL as a subset) completely into C and then compiling that C into object code. The system resulting from this architecture is fast, powerful, easy to use and administer, reliable and open to UNIX™ tools. In particular, two forms of data compression plus robust horizontal partitioning enable Daytona to handle terabytes with ease.

#index 273944
#* Merge replication in Microsoft's SQL server 7.0
#@ Brad Hammond
#t 1999
#c 5
#! SQL Server 7.0 offers three different styles of replication that we call Transactional Replication, Snapshot Replication, and Merge Replication. Merge Replication means that data changes can be performed at any replica, and that the changes performed at multiple replicas are later merged together. Because Merge Replication allows updates to disconnected replicas, it is particularly well suited to applications that require a lot of autonomy. A special process called the Merge Agent propagates changes between replicas, filters data as appropriate, and detects and handles conflicts according to user-specified rules.

#index 273945
#* In-memory data management for consumer transactions the timesten approach
#@ CORPORATE TimesTen Team
#t 1999
#c 5
#% 427195
#% 442832
#% 442836
#% 464173
#% 479769
#% 481454

#index 273946
#* Microsoft site server (commerce ed.): Talk-slides available at the conference
#@ Bassel Ojjeh
#t 1999
#c 5

#index 273947
#* E-commerce database issues and experience: (talk-slides available at at the conference)
#@ Anand Rajaraman
#t 1999
#c 5

#index 273949
#* Versions and workspaces in Microsoft repository
#@ Thomas Bergstraesser;Philip A. Bernstein;Shankar Pal;David Shutt
#t 1999
#c 5
#% 86478
#% 236243
#% 281976
#% 435126
#% 479456
#% 479776
#! This paper describes the version and workspace features of Microsoft Repository, a layer that implements fine-grained objects and relationships on top of Microsoft SQL Server. It supports branching and merging of versions, delta storage, checkout-checkin, and single-version views for version-unaware applications.

#index 273951
#* The need for distributed asynchronous transactions
#@ Lyman Do;Prabhu Ram;Pamela Drew
#t 1999
#c 5
#% 1486
#% 9241
#% 122904
#% 443004
#% 479622
#! The theme of the paper is to promote research on asynchronous transactions. We discuss our experience of executing synchronous transactions on a large distributed production system in The Boeing Company. Due to the poor performance of synchronous transactions in our environment, it motivated the exploration of asynchronous transactions as an alternate solution. This paper presents the requirements and benefits/limitations of asynchronous transactions. Open issues related to large scale deployments of asynchronous transactions are also discussed.

#index 274139
#* Improving OLTP data quality using data warehouse mechanisms
#@ Matthias Jarke;Christoph Quix;Guido Blees;Dirk Lehmann;Gunter Michalk;Stefan Stierl
#t 1999
#c 5
#% 210211
#% 481933
#% 691652
#! Research and products for the integration of heterogeneous legacy source databases in data warehousing have addressed numerous data quality problems in or between the sources. Such a solution is marketed by Team4 for the decision support of mobile sales representatives, using advanced view maintenance and replication management techniques in an environment based on relational data warehouse technology and Lotus Notes-based client systems. However, considering total information supply chain management, the capture of poor operational data, to be cleaned later in the data warehouse, appears sub-optimal. Based on the observation that decision support clients are often closely linked to operational data entry, we have addressed the problem of mapping the data warehouse data quality techniques back to data quality measures for improving OLTP data. The solution requires a warehouse-to-OLTP workflow which employs a combination of view maintenance and view update techniques.

#index 274140
#* Data integration and warehousing in Telecom Italia
#@ Stefano M. Trisolini;Maurizio Lenzerini;Daniele Nardi
#t 1999
#c 5
#% 388009
#% 511906
#! We discuss the main methodological and technological issues arosen in the last years in the development of the enterprise integrated database of Telecom Italia and, subsequently in the management of the primary data store for Telecom Italia data warehouse applications.The two efforts, although driven by different needs and requirements can be regarded as a continous development of an integrated view of the enterprise data. We review the experience accumulated in the integration of over 50 internal databases, highlighting the benefits and drawbacks of this scenario for data warehousing and discuss the development of a large dedicated data store to support the analysis of data about customers and phone traffic.

#index 274141
#* An XJML-based wrapper generator for Web information extraction
#@ Ling Liu;Wei Han;David Buttler;Calton Pu;Wei Tang
#t 1999
#c 5
#% 227987
#% 248853
#% 266102
#% 443298

#index 274142
#* DBIS-toolkit: adaptable middleware for large scale data delivery
#@ Mehmet Altinel;Demet Aksoy;Thomas Baby;Michael Franklin;William Shapiro;Stan Zdonik
#t 1999
#c 5
#% 201897
#% 227885
#% 237238
#% 248838

#index 274143
#* DOMINO: databases fOr MovINg Objects tracking
#@ Ouri Wolfson;Prasad Sistla;Bo Xu;Jutai Zhou;Sam Chamberlain
#t 1999
#c 5
#% 503869
#! Consider a database that represents information about moving objects and their location. For example, for a database representing the location of taxi-cabs a typical query may be: retrieve the free cabs that are currently within 1 mile of 33 N. Michigan Ave., Chicago (to pick-up a customer); or for a trucking company database a typical query may be: retrieve the trucks that are currently within 1 mile of truck ABT312 (which needs assistance); or for a database representing the current location of objects in a battlefield a typical query may be: retrieve the friendly helicopters that are in a given region, or, retrieve the friendly helicopters that are expected to enter the region within the next 10 minutes. The queries may originate from the moving objects, or from stationary users. We will refer to applications with the above characteristics as moving-objects-database (MOD) applications, and to queries as the ones mentioned above as MOD queries.In the military MOD applications arise in the context of the digital battlefield (see [1]), and in the civilian industry they arise in transportation systems. For example, Omnitracs developed by Qualcomm (see[2]) is a commercial system used by the transportation industry, which enables MOD functionality. It provides location management by connecting vehicles (e.g. trucks), via satellites, to company databases. The vehicles are equipped with a Global Positioning System (GPS), and they automatically and periodically report their location.

#index 274144
#* Database patchwork on the Internet
#@ Reinhard Braumandl;Alfons Kemper;Donald Kossmann
#t 1999
#c 5
#% 86929
#% 136740
#% 229827
#% 479449
#% 479452
#! Naturally, data processing requires three kinds of resources:the data itself,the functionality (i.e. database operations) andthe machines on which to run the operations.Because of the Internet we believe that in the long run there will be alternative providers for all of these three resources for any given application. Data providers will bring more and more data and more and more different kinds of data to the net. Likewise, function providers will develop new methods to process and work with the data; e.g., function providers might develop new algorithms to compress data or to produce thumbnails out of large images and try to sell these on the Internet. It is also conceivable, that some people allow other people to use spare cycles of their idle machines in the Internet (as in the Condor system of the University of Wisconsin) or that some companies (cycle providers) even specialize on selling computing time to businesses that occasionally need to carry out very complex operations for which regular hardware is not sufficient. At the University of Passau, we are currently developing a distributed database system to be used in the Internet. The goal is to ultimately have a system which is able to run on any machine, manage any kind of data, import any kind of data from other systems and import any kind of database operations. The system is entirely written in Java. One of the most important features of the system is that it is capable of dynamically loading (external) query operators, written in Java and supplied by any function provider, and executing these query operators in concert with pre-defined and other external operators in order to evaluate a query. Compared to object-relational database systems, which allow to integrate external data and functionality by the means of extensions (datablades, extenders or cartridges) or heterogeneous database systems such as Garlic [MS97] or Tsimmis [GMPQ+97], our approach makes it possible to place external query operators anywhere in a query evaluation plan as opposed to restricting the placement of external operations to the “access level” of plans. It would, for example, be possible to make our system execute a completely new relational join method, if somebody finds a new join method which is worth-while implementing. Because our system is written in Java, it is highly portable and could be used by data, function and cycle providers with almost no effort. Furthermore, our query engine is, of course, completely distributed providing all the required infrastructure for server-server communication, name services, etc.

#index 274145
#* Evolvable view environment (EVE): non-equivalent view maintenance under schema changes
#@ E. A. Rundensteiner;A. Koeller;X. Zhang;A. J. Lee;A. Nica;A. Van Wyk;Y. Lee
#t 1999
#c 5
#% 227947
#% 264964
#% 565261
#% 581577
#% 589227
#% 631939
#! Supporting independent ISs and integrating them in distributed data warehouses (materialized views) is becoming more important with the growth of the WWW. However, views defined over autonomous ISs are susceptible to schema changes. In the EVE project we are developing techniques to support the maintenance of data warehouses defined over distributed dynamic ISs [5, 6, 7]. The EVE system is the first to allow views to survive schema changes of their underlying ISs while also adapting to changing data in those sources. EVE achieves this is two steps: applying view query rewriting algorithms that exploit information about alternative ISs and the information they contain, and incrementally adapting the view extent to the view definition changes. Those processes are referred to as view synchronization and view adaption, respectively. They increase the survivability of materialized views in changing environments and reduce the necessity of human interaction in system maintenance.

#index 274146
#* Exploratory mining via constrained frequent set queries
#@ Raymond Ng;Laks V. S. Lakshmanan;Jiawei Han;Teresa Mah
#t 1999
#c 5
#% 152934
#% 216508
#% 227919
#% 236414
#% 248784
#% 248785
#% 248813
#! Although there have been many studies on data mining, to date there have been few research prototypes or commercial systems supporting comprehensive query-driven mining, which encourages interactive exploration of the data. Our thesis is that constraint constructs and the optimization they induce play a pivotal role in mining queries, thus substantially enhancing the usefulness and performance of the mining system. This is based on the analogy of declarative query languages like SQL and query optimization which have made relational databases so successful. To this end, our proposed demo is not yet another data mining system, but of a new paradigm in data mining - mining with constraints, as the important first step towards supporting ad-hoc mining in DBMS.In this demo, we will show a prototype exploratory mining system that implements constraint-based mining query optimization methods proposed in [5]. We will demonstrate how a user can interact with the system for exploratory data mining and how efficiently the system may execute optimized data mining queries. The prototype system will include all the constraint pushing techniques for mining association rules outlined in [5], and will include additional capabilities for mining other kinds of rules for which the computation of constrained frequent sets forms the core first step.

#index 274147
#* Nodose version 2.0
#@ Brad Adelberg;Matthew Denny
#t 1999
#c 5
#% 248808
#% 511897
#! This paper describes a tool, called Nodose, we have developed to expedite the creation of robust wrappers. Nodose allows non-programmers to build components that can convert data from the source format to XML or another generic format. Further, the generated code performs a set of statistical checks at runtime that attempt to find extraction errors before they are propogated back to users.

#index 274148
#* Phoenix: making applications robust
#@ Roger Barga;David B. Lomet
#t 1999
#c 5
#% 248823
#% 464823
#% 481624

#index 274149
#* PowerBookmarks: a system for personalizable Web information organization, sharing, and management
#@ Wen-Syan Li;Quoc Vu;Edward Chang;Divyakant Agrawal;Kyoji Hirata;Sougata Mukherjea;Yi-Leh Wu;Corey Bufi;Chen-Chuan Kevin Chang;Yoshinori Hara;Reiko Ito;Yutaka Kimura;Kezuyuki Shimazu;Yukiyoshi Saito
#t 1999
#c 5
#% 198058
#% 232922
#% 268085
#% 281187
#% 584882

#index 274150
#* SERF: ODMG-based generic re-structuring facility
#@ E. A. Rundensteiner;K. Claypool;M. Li;L. Chen;Z. Zhang;C. Natarajan;J. Jin;S. De Lima;S. Weiner
#t 1999
#c 5
#% 32903
#% 260022
#% 487956
#% 582495
#% 677292
#! The age of information management and with it the advent of increasingly sophisticated technologies have kindled a need in the database community and others to re-structure existing systems and move forward to make use of these new technologies. Legacy application systems are being transformed to newer state-of-the-art systems, information sources are being mapped from one data model to another, a diversity of data sources are being transformed to load, cleanse and consolidate data into modern data-warehouses [CR99].Re-structuring is thus a critical task for a variety of applications. For this reason, most object-oriented database systems (OODB) today support some form of re-structuring support [Tec94, Obj93, BKKK87]. This existing support of current OODBs [BKKK87, Tec94, Obj93] is limited to a pre-defined taxonomy of simple fixed-semantic schema evolution operations. However, such simple changes, typically to individual types only, are not sufficient for many advanced applications [Bré96]. More radical changes, such as combining two types of redefining the relationship between two types, are either very difficult or even impossible to achieve with current commercial database technology [Tec94, Obj93]. In fact, most OODBs would typically require the user to write ad-hoc programs to accomplish such transformations. Research that has begun to look into the issue of complex changes [Bré96, Ler96] is still limited by providing a fixed set of some selected (even if now more complex) operations.To address these limitations of the current restructuring technology, we have proposed the SERF framework which aims at providing a rich environment for doing complex user-defined transformations flexibly, easily and correctly [CJR98b]. The goal of our work is to increase the usability and utility of the SERF framework and its applicability to re-structuring problems beyond OODB evolution. Towards that end, we provide re-usable transformations via the notion of SERF Templates that can be packaged into libraries, thereby increasing the portability of these transformations. We also now have a first cut at providing an assurance of consistency for the users of this system, a semantic optimizer that provides some performance improvements via enhanced query optimization techniques with emphasis on the re-structuring primitives [CNR99]. In this demo we give an overview of the SERF framework, its current status and the enhancements that are planned for the future. We also present an example of the application of SERF to a domain other than schema evolution, i.e., the web restructuring.

#index 274151
#* TAM: a system for dynamic transactional activity management
#@ Tong Zhou;Ling Liu;Calton Pu
#t 1999
#c 5
#% 81587
#% 122904
#% 209568
#% 260033
#% 340317
#% 403195
#% 462220
#% 480259
#% 481752
#% 535692

#index 274152
#* The Aqua approximate query answering system
#@ Swarup Acharya;Phillip B. Gibbons;Viswanath Poosala;Sridhar Ramaswamy
#t 1999
#c 5
#% 210190
#% 273909
#% 482123
#! Aqua is a system for providing fast, approximate answers to aggregate queries, which are very common in OLAP applications. It has been designed to run on top of any commercial relational DBMS. Aqua precomputes synopses (special statistical summaries) of the original data and stores them in the DBMS. It provides approximate answers along with quality guarantees by rewriting the queries to run on these synopses. Finally, Aqua keeps the synopses up-to-date as the database changes, using fast incremental maintenance techniques.

#index 274153
#* The CCUBE constraint object-oriented database system
#@ Alexander Brodsky;Victor E. Segal;Jia Chen;Paval A. Exarkhopoulo
#t 1999
#c 5

#index 274154
#* The Cornell Jaguar project: adding mobility to PREDATOR
#@ Phillippe Bonnet;Kyle Buza;Zhiyuan Chan;Victor Cheng;Randolph Chung;Takako Hickey;Ryan Kennedy;Daniel Mahashin;Tobias Mayr;Ivan Oprencak;Praveen Seshadri;Hubert Siu
#t 1999
#c 5
#% 273915
#% 571071
#! The Cornell Jaguar Project is exploring a variety of issues related to mobility and query processing. One broad theme is to break down the traditional client and server boundaries, leading to ubiquitous query processing. Another theme is to extend database and query processing techniques to small-scale and mobile devices. The project builds on and extends the Cornell PREDATOR database engine.

#index 274155
#* The active MultiSync controller of the cubetree storage organization
#@ Nick Roussopoulos;Yannis Kotidis;Yannis Sismanis
#t 1999
#c 5
#% 227868
#% 248805
#! The Cubetree Storage Organization (CSO)1 logically and physically clusters materialized-views data, multi-dimensional indices on them, and computed aggregate values all in one compact and tight storage structure that uses a fraction of the conventional table-based space. This is a breakthrough technology for storing and accessing multi-dimensional data in terms of storage reduction, query performance and incremental bulk update speed. CSO has been extended with an Active MultiSync controller for synchronizing multiple concurrent access and continuous asynchronous online updates for a non-stop data warehouse.

#index 274156
#* The Jungle database search engine
#@ Michael Böhlen;Linas Bukauskas;Curtis Dyreson
#t 1999
#c 5
#% 3643
#% 383594
#! Information spread in in databases cannot be found by current search engines. A database search engine is capable to access and advertise database on the WWW. Jungle is a database search engine prototype developed at Aalborg University. Operating through JDBC connections to remote databases, Jungle extracts and indexes database data and meta-data, building a data store of database information. This information is used to evaluate and optimize queries in the AQUA query language. AQUA is a natural and intuitive database query language that helps users to search for information without knowing how that information is structured. This paper gives an overview of AQUA and describes the implementation of Jungle.

#index 274157
#* The WASA2 object-oriented workflow management system
#@ Gottfried Vossen;Mathias Weske
#t 1999
#c 5
#% 266161

#index 274158
#* A user-centered interface for querying distributed multimedia databases
#@ Isabel F. Cruz;Kimberly M. James
#t 1999
#c 5
#% 199537
#% 229827
#% 239661
#% 249161
#% 249164
#% 249196
#% 443235
#% 584869
#% 637796
#! Facilitating information retrieval in the vastly growing realm of digital media has become increasingly difficult. DelaunayMM seeks to assist all users in finding relevant information through an interactive interface that supports pre- and post-query refinement, and a customizable multimedia information display. This project leverages the strengths of visual query languages with a resourceful framework to provide users with a single intuitive interface. The interface and its supporting framework are described in this paper.

#index 274159
#* World Wide Database—integrating the Web, CORBA and databases
#@ Athman Bouguettaya;Boualem Benatallah;Lily Hendra;James Beard;Kevin Smith;Mourad Quzzani
#t 1999
#c 5
#% 275367
#% 385766
#% 481598

#index 274160
#* XML-based information mediation with MIX
#@ Chaitan Baru;Amarnath Gupta;Bertram Ludäscher;Richard Marciano;Yannis Papakonstantinou;Pavel Velikhov;Vincent Chu
#t 1999
#c 5
#! The MIX mediator system, MIXm, is developed as part of the MIX Project at the San Diego Supercomputer Center, and the University of California, San Diego.1 MIXm uses XML as the common model for data exchange. Mediator views are expressed in XMAS (XML Matching And Structuring Language), a declarative XML query language. To facilitate user-friendly query formulation and for optimization purposes, MIXm employs XML DTDs as a structural description (in effect, a “schema”) of the exchanged data. The novel features of the system include:Data exchange and integration solely relies on XML, i.e., instance and schema information is represented by XML documents and XML DTDs, respectively. XML queries are denoted in XMAS, which builds upon ideas of languages like XML-QL, MSL, Yat, and UnQL. Additionally, XMAS features powerful grouping and order constructs for generating new integrated XML “objects” from existing ones.The graphical user interface BBQ (Blended Browsing and Querying) is driven by the mediator view DTD and integrates browsing and querying of XML data. Complex queries can be constructed in an intuitive way, resembling QBE. Due to the nested nature of XML data and DTDs, BBQ provides graphical means to specify the nesting and grouping of query results.Query evaluation can be demand-driven, i.e., by the user's navigation into the mediated view.

#index 275304
#* Sequences, datalog, transducers
#@ Anthony Bonner;Giansalvatore Mecca
#t 1998
#c 5

#index 275307
#* Safe locking policies for dynamic databases
#@ Vinay K. Chaudhri;Vassos Hadzilacos
#t 1998
#c 5

#index 275308
#* Algebras for querying text regions: expressive power and optimization
#@ Mariano P. Consens;Tova Milo
#t 1998
#c 5

#index 275310
#* Arity bounds in first-order incremental evaluation and definition of polynomial time database queries
#@ Guozhu Dong;Jianwen Su
#t 1998
#c 5

#index 275312
#* Increasing the resilience of distributed and replicated database systems
#@ Idit Keidar;Danny Dolev
#t 1998
#c 5

#index 275314
#* Semantics and expressiveness issues in active databases
#@ Philippe Picouet;Victor Vianu
#t 1998
#c 5

#index 275315
#* Learning with refutation
#@ Sanjay Jain
#t 1998
#c 5

#index 275316
#* Characterizing multiterminal flow networks and computing flows in networks of small treewidth
#@ Torben Hagerup;Jyrki Katajainen;Naomi Nishimura;Prabhakar Ragde
#t 1998
#c 5

#index 275317
#* Approximating hyper-rectangles: learning and pseudorandom sets
#@ Peter Auer;Philip M. Long;Aravind Srinivasan
#t 1998
#c 5

#index 275318
#* Achilles and the tortoise climbing up the arithmetical hierarchy
#@ Eugene Asarin;Oded Maler
#t 1998
#c 5

#index 275319
#* A three-party communication problem
#@ Leonard J. Schulman
#t 1998
#c 5

#index 275340
#* Component-based e-commerce: assessment of current practices and future directions
#@ Martin Bichler;Arie Segev;J. Leon Zhao
#t 1998
#c 5
#% 234856
#% 234953
#% 247069
#% 252384
#% 445077
#% 452528
#% 452542
#% 557050
#% 596212
#% 618955
#% 637757
#! Component-based e-commerce technology is a recent trend towards resolving the e-commerce challenge at both system and application levels. Instead of delivering a system as a prepacked monolith system containing any conceivable feature, component-based systems consist of a lightweight kernel to which new features can be added in the form of components. In order to identify the central problems in component-based e-commerce and ways to deal with them, we investigate prototypes, technologies, and frameworks that will transcend the current state of the practice in Internet commerce. In this paper, we first discuss the current practices and trends in component-based electronic commerce based on the International Workshop on Component-based Electronic Commerce. Then, we investigate a number of research issues and future directions in component-based development for electronic commerce.

#index 275341
#* Building database-driven electronic catalogs
#@ Sherif Danish
#t 1998
#c 5
#% 385275
#% 385594
#! This paper describes issues and solutions related to the creation of a product information database in the enterprise, and using this database as a foundation for deploying an electronic catalog. Today, product information is typically managed in document composition systems and communicated on paper. In the new wired world, these processes are undertaking fundamental changes to cope with the time to market pressure and the need for accurate, complete, and structured presentation of product information. Electronic catalogs are the answer.

#index 275343
#* XML and electronic commerce: enabling the network economy
#@ Bart Meltzer;Robert Glushko
#t 1998
#c 5
#! There has been a lot of talk about how the Internet is going to change the world economy. Companies will come together in a “plug and play” fashion to form trading partner networks. Virtual companies will be established and new business models can be created based on access to information and agents that can carry it around the world using computer networks.

#index 275347
#* A workflow-based electronic marketplace on the Web
#@ Asuman Dogac;Ilker Durusoy;Sena Arpinar;Nesime Tatbul;Pinar Koksal;Ibrahim Cingil;Nazife Dimililer
#t 1998
#c 5
#% 241284
#% 246009
#% 246011
#% 246349
#% 391633
#% 468194
#% 475879
#! In this paper, we describe an architecture for an open marketplace exploiting the workflow technology and the currently emerging data exchange and metadata representation standards on the Web.In this market architecture electronic commerce is realized through the adaptable workflow templates provided by the marketplace to its users. Having workflow templates for electronic commerce processes results in a component-based architecture where components can be agents (both buying and selling) as well as existing applications invoked by the workflows. Other advantages provided by the workflow technology are forward recovery, detailed logging of the processes through workflow history manager and being able to specify data and control flow among the workflow components.In the architecture proposed, the resources expose their metadata using Resource Description Framework (RDF) to be accessed by the resource discovery agents and their content through Extensible Markup Language (XML) to be accessed by the selling agents by using Document Object Model (DOM). The common set of Document Type Definitions (DTDs) are used to eliminate the need for an ontology.The marketplace contains an Intelligent Directory Service (IDS) which makes it possible for agents to find out about each other through a match making mechanism. References to the related Document Type Definitions (DTDs) are stored in IDS. The IDS also contains the template workflows for buying and selling processes.

#index 275349
#* ADEPT: an agent-based approach to business process management
#@ N. R. Jennings;T. J. Norman;P. Faratin
#t 1998
#c 5
#% 162305
#% 215476
#! Successful companies organise and run their business activities in an efficient manner. Core activities are completed on time and within specified resource constraints. However to stay competitive in today's markets, companies need to continually improve their efficiency — business activities need to be completed more quickly, to higher quality and at lower cost. To this end, there is an increasing awareness of the benefits and potential competitive advantage that well designed business process management systems can provide. In this paper we argue the case for an agent-based approach: showing how agent technology can improve efficiency by ensuring that business activities are better scheduled, executed, monitored, and coordinated.

#index 275351
#* A componentized architecture for dynamic electronic markets
#@ Benny Reich;Israel Ben-Shaul
#t 1998
#c 5
#% 168251
#% 241025
#% 252811
#% 259783
#% 260335
#% 274891
#% 390039
#% 437706
#! The emergence and growing popularity of Internet-based electronic market-places, in their various forms, has raised the challenge to explore genericity in market design. In this paper we present a domain-specific software architecture that delineates the abstract components of a generic market and specifies control and data-flow constraints between them, and a framework that allows convenient pluggability of components that implement specific market policies. The framework was realized in the GEM system. GEM provides infrastructure services that allow market designers to focus solely on market-issues. In addition, it allows dynamic (re)configuration of components. This functionality can be used to change market-policies as the environment or market trends change, adding another level of flexibility to market designers and administrators.

#index 275355
#* Design and implementation of RMP: a virtual electronic market place
#@ Susanne Boll;Wolfgang Klas;Bernard Battaglin
#t 1998
#c 5
#% 281972

#index 275360
#* Discovering Internet marketing intelligence through online analytical web usage mining
#@ Alex G. Büchner;Maurice D. Mulvenna
#t 1998
#c 5
#% 158200
#% 199539
#% 209662
#% 210182
#% 216509
#% 383669
#% 445120
#% 463903
#% 481588
#% 584891
#% 631914
#% 661023
#! This article describes a novel way of combining data mining techniques on Internet data in order to discover actionable marketing intelligence in electronic commerce scenarios. The data that is considered not only covers various types of server and web meta information, but also marketing data and knowledge. Furthermore, heterogeneity resolution thereof and Internet- and electronic commerce-specific pre-processing activities are embedded. A generic web log data hypercube is formally defined and schematic designs for analytical and predictive activities are given. From these materialised views, various online analytical web usage data mining techniques are shown, which include marketing expertise as domain knowledge and are specifically designed for electronic commerce purposes.

#index 275363
#* An anonymous electronic commerce scheme with an off-line authority and untrusted agents
#@ Josep Domingo-Ferrer;Jordi Herrera-Joancomartí
#t 1998
#c 5
#% 1714
#% 2147
#% 15406
#% 16014
#% 169421
#% 202153
#% 214986
#% 495547
#! In the last years, the exponential growth of computer networks has created an incredibly large offer of products and services in the net. Such a huge amount of information makes it impossible for a single person to analyze all existing offers of a product on the net and decide which of them fits better her requirements. This problem is solved with the intelligent trade agents (ITA), which are programs that have the ability to roam a network, collect business-related data and use them to make decisions to buy goods on their owners' behalf. Known ITA systems do not provide anonymity in transactions, require an on-line trusted third party and implicitly assume that the user trusts the ITA. We present a new scheme for an intelligent untrusted trade agent system allowing anonymous electronic transactions with an off-line trusted third party.

#index 275364
#* Electronic market: the roadmap for university libraries and members to survive in the information jungle
#@ Michael Christoffel;Sebastian Pulkowski;Bethina Schmitt;Peter C. Lockemann
#t 1998
#c 5
#% 43653
#% 232679
#% 244103
#% 247317
#% 248808
#% 499314
#! This contribution argues that electronic markets can serve as a powerful mechanism to entice providers to identify their customer base and to offer customer-oriented, high-quality and economical services and to induce customers to a more focused and price-conscious behavior. The paper claims that this should be particularly true for the provision and access to scientific literature where the tradition so far has been mostly free access by customers and non-transparent cost accounting and service procurement by university libraries. We report on a project for developing a technical network infrastructure that allows for a more cost-transparent access to scientific literature by campus users and attempts to add a competitive element to library services. Equally important, it provides added value to the users so that they can orient themselves in the vast expanses of scientific literature much faster and more economically. We cover three major elements of the infrastructure: user agents, traders and source wrappers.

#index 275367
#* The Asilomar report on database research
#@ Phil Bernstein;Michael Brodie;Stefano Ceri;David DeWitt;Mike Franklin;Hector Garcia-Molina;Jim Gray;Jerry Held;Joe Hellerstein;H. V. Jagadish;Michael Lesk;Dave Maier;Jeff Naughton;Hamid Pirahesh;Mike Stonebraker;Jeff Ullman
#t 1998
#c 5
#! The database research community is rightly proud of success in basic research, and its remarkable record of technology transfer. Now the field needs to radically broaden its research focus to attack the issues of capturing, storing, analyzing, and presenting the vast array of online data. The database research community should embrace a broader research agenda — broadening the definition of database management to embrace all the content of the Web and other online data stores, and rethinking our fundamental assumptions in light of technology shifts. To accelerate this transition, we recommend changing the way research results are evaluated and presented. In particular, we advocate encouraging more speculative and long-range work, moving conferences to a poster format, and publishing all research literature on the Web.

#index 275369
#* The middleware muddle
#@ David Ritter
#t 1998
#c 5
#! A new menagerie of middleware is emerging. These products promise great flexibility in partitioning enterprise applications across the diverse corporate computing landscape. What factors should you consider when choosing a solution, and how do current products stack up? More important to the focus of this article, what role should Web servers play?

#index 275372
#* SQLJ Part 0, now known as SQL/OLB (Object-Language Bindings)
#@ Andrew Eisenberg;Jim Melton
#t 1998
#c 5

#index 278397
#* Semantic interoperability in global information systems
#@ A. M. Ouksel;A. Sheth
#t 1999
#c 5
#! Internet, Web and distributed computing infrastructures continue to gain in popularity as a means of communication for organizations, groups and individuals alike. In such an environment, characterized by large distributed, autonomous, diverse, and dynamic information sources, access to relevant and accurate information is becoming increasingly complex. This complexity is exacerbated by the evolving system, semantic and structural heterogeneity of these potentially global, cross-disciplinary, multicultural and rich-media technologies. Clearly, solutions to these challenges require addressing directly a variety of interoperability issues.

#index 278400
#* Semantic integration of environmental models for application to global information systems and decision-making
#@ D. Scott Mackay
#t 1999
#c 5
#! Global information systems have the potential of providing decision makers with timely spatial information about earth systems. This information will come from diverse sources, including field monitoring, remotely sensed imagery, and environmental models. Of the three the latter has the greatest potential of providing regional and global scale information on the behavior of environmental systems, which may be vital for setting multi-governmental policy and for making decisions that are critical to quality of life. However, environmental models have limited prootocol for quality control and standardization. They tend to have weak or poorly defined semantics and so their output is often difficult to interpret outside a very limited range of applications for which they are designed. This paper considers this issue with respect to spatially distributed environmental models. A method of measuring the semantic proximity between components of large, integrated models is presented, along with an example illustrating its application. It is concluded that many of the issues associated with weak model semantics can be resolved with the addition of self-evaluating logic and context-based tools that present the semantic weaknesses to the end-user.

#index 278418
#* Semantic and pedagogic interoperability mechanisms in the ARIADNE educational repository
#@ E. Forte;F. Haenni;K. Warkentyne;E. Duval;K. Cardinaels;E. Vervaet;K. Hendrikx;M. Wentland Forte;F. Simillion
#t 1999
#c 5
#! This paper reports on the principles underlying the semantic and pedagogic interoperability mechanisms built in the European Knowledge Pool System, developed by the European research project ARIADNE. This system, which is the central feature of ARIADNE, consists in a distributed repository of pedagogical documents (or learning objects) of diverse granularity, origin, content, type, language, etc., which are stored in view of their use (and reuse) in telematics-based training or teaching curricula. The learning objects are indexed, usually by faculty staff, according to the ARIADNE metadata set. The principles embodied in the indexation tool, which interacts directly with the repository,stem from a few theoretical ideas but foremost from empirical, pragmatic considerations, suggested by the context of actual use. They tentatively address the stringent demands for semantic and pedagogic interoperability implied by a context of rather wide cultural and linguistic diversity, as well as those stemming from the very nature of the domain application itself: education and training. Possible extensions to the educational metadata scheme developed by ARIADNE on these basis, may accommodate corporate training/information needs. These extensions are briefly discussed as a mean for enhancing 'semantic' interoperability between different (kinds of) corporations. Finally, the architecture of the ARIADNE system, which heavily relies on this educational metadata system, is briefly reviewed.

#index 278426
#* Unpacking the semantics of source and usage to perform semantic reconciliation in large-scale information systems
#@ Ken Smith;Leo Obrst
#t 1999
#c 5
#! Semantic interoperability is a growing challenge in the United States Department of Defense (DoD). In this paper, we describe the basis of an infrastructure for the reconciliation of relevant, but semantically heterogeneous attribute values. Three types of information are described which can be used to infer the context of attributes, making explicit hidden semantic conflicts and making it possible to adjust values appropriately. Through an extended example, we show how an automated integration agent can derive the transformations necessary to perform four tasks in a simple semantic reconciliation.

#index 278431
#* Semantic video indexing: approach and issues
#@ Arun Hampapur
#t 1999
#c 5
#! Providing concept level access to video data requires, video management systems tailored to the domain of the data. Effective indexing and retrieval for high-level access mandates the use of domain knowledge. This paper proposes an approach based on the use of knowledge models to building domain specific video information systems. The key issues in such systems are identified and discussed.

#index 278434
#* Contextualizing the information space in federated digital libraries
#@ M. P. Papazoglou;J. Hoppenbrouwers
#t 1999
#c 5
#! Rapid growth in the volume of documents, their diversity, and terminological variations render federated digital libraries increasingly difficult to manage. Suitable abstraction mechanisms are required to construct meaningful and scalable document clusters, forming a cross-digital library information space for browsing and semantic searching. This paper addresses the above issues, proposes a distributed semantic framework that achieves a logical partitioning of the information space according to topic areas, and provides facilities to contextualize and landscape the available document sets in subject-specific categories.

#index 278443
#* Dynamic service matchmaking among agents in open information environments
#@ Katia Sycara;Matthias Klusch;Seth Widoff;Jianguo Lu
#t 1999
#c 5

#index 278445
#* Semantic integration of semistructured and structured data sources
#@ S. Bergamaschi;S. Castano;M. Vincini
#t 1999
#c 5
#! Providing an integrated access to multiple heterogeneous sources is a challenging issue in global information systems for cooperation and interoperability. In this context, two fundamental problems arise. First, how to determine if the sources contain semantically related information, that is, information related to the same or similar real-world concept(s). Second, how to handle semantic heterogeneity to support integration and uniform query interfaces. Complicating factors with respect to conventional view integration techniques are related to the fact that the sources to be integrated already exist and that semantic heterogeneity occurs on the large-scale, involving terminology, structure, and context of the involved sources, with respect to geographical, organizational, and functional aspects related to information use. Moreover, to meet the requirements of global, Internet-based information systems, it is important that tools developed for supporting these activities are semi-automatic and scalable as much as possible.The goal of this paper is to describe the MOMIS [4, 5] (Mediator envirOnment for Multiple Information Sources) approach to the integration and query of multiple, heterogeneous information sources, containing structured and semistructured data. MOMIS has been conceived as a joint collaboration between University of Milano and Modena in the framework of the INTERDATA national research project, aiming at providing methods and tools for data management in Internet-based information systems. Like other integration projects [1, 10, 14], MOMIS follows a “semantic approach” to information integration based on the conceptual schema, or metadata, of the information sources, and on the following architectural elements: i) a common object-oriented data model, defined according to the ODLI3 language, to describe source schemas for integration purposes. The data model and ODLI3 have been defined in MOMIS as subset of the ODMG-93 ones, following the proposal for a standard mediator language developed by the I3/POB working group [7]. In addition, ODLI3 introduces new constructors to support the semantic integration process [4, 5]; ii) one or more wrappers, to translate schema descriptions into the common ODLI3 representation; iii) a mediator and a query-processing component, based on two pre-existing tools, namely ARTEMIS [8] and ODB-Tools [3] (available on Internet at http://sparc20.dsi.unimo.it/), to provide an I3 architecture for integration and query optimization. In this paper, we focus on capturing and reasoning about semantic aspects of schema descriptions of heterogeneous information sources for supporting integration and query optimization. Both semistructured and structured data sources are taken into account [5]. A Common Thesaurus is constructed, which has the role of a shared ontology for the information sources. The Common Thesaurus is built by analyzing ODLI3 descriptions of the sources, by exploiting the Description Logics OLCD (Object Language with Complements allowing Descriptive cycles) [2, 6], derived from KL-ONE family [17]. The knowledge in the Common Thesaurus is then exploited for the identification of semantically related information in ODLI3 descriptions of different sources and for their integration at the global level. Mapping rules and integrity constraints are defined at the global level to express the relationships holding between the integrated description and the sources descriptions. ODB-Tools, supporting OLCD and description logic inference techniques, allows the analysis of sources descriptions for generating a consistent Common Thesaurus and provides support for semantic optimization of queries at the global level, based on defined mapping rules and integrity constraints.

#index 278604
#* Agent-based semantic interoperability in infosleuth
#@ Jerry Fowler;Brad Perry;Marian Nodine;Bruce Bargmeyer
#t 1999
#c 5

#index 278605
#* Semantic interoperability in information services: experiencing with CoopWARE
#@ Avigdor Gal
#t 1999
#c 5

#index 278606
#* Report on the 13th Brazilian symposium on database systems (SBBD'98)
#@ Mario A. Nascimento;Claudia Bauzer Medeiros
#t 1999
#c 5
#! The Brazilian Symposium on Database Systems (SBBD) is a traditional conference in Brazil, and is sponsored by the Brazilian Computer Society. SBBD's technical program contemplates the following activities: presentation of peer reviewed full technical papers, invited talks, tutorials (either invited and selected from submissions), discussion panels and presentation of tools.

#index 278607
#* Efficient materialization and use of views in data warehouses
#@ Márcio Farias de Souza;Marcus Costa Sampaio
#t 1999
#c 5
#! Given the complexity of many queries over a Data Warehouse (DW), it is interesting to precompute and store in the DW the answer sets of some demanding operations, so called materialized views. In this paper, we present an algorithm, including its experimental evaluation, which allows the materialization of several views simultaneously without losing sight of processing costs for queries using these materialized views.

#index 278608
#* Design principles for data-intensive Web sites
#@ Stefano Ceri;Piero Fraternali;Stefano Paraboschi
#t 1999
#c 5

#index 278609
#* A study on data point search for HG-trees
#@ Joseph Kuan;Paul Lewis
#t 1999
#c 5
#! A point data retrieval algorithm for the HG-tree is introduced which improves the number of nodes accessed. The HG-tree is a multidimensional indexing tree designed for point data and it is a simple modification from the Hilbert R-tree for indexing spatial data. The HG-tree data search method mainly makes use of the Hilbert index values to search for exact data, instead of using conventional point search methods as used in most of the R-tree papers. The use of Hilbert curve values and MBR can reduce the spatial cover of an MBR.Several R-tree variants have been developed; R*-tree, S-tree, Hilbert R-tree, and R*-tree combined with the linear split method by Ang et al. Our search method on the HG-tree gives a superior speed performance compared to all other R-tree variants.

#index 278610
#* The OASIS multidatabase prototype
#@ Mark Roantree;John Murphy;Wilhelm Hasselbring
#t 1999
#c 5
#! The OASIS Prototype is under development at Dublin City University in Ireland. We describe a multi-database architecture which uses the ODMG model as a canonical model and describe an extention for construction of virtual schemas within the multidatabase system. The OMG model is used to provide a standard distribution layer for data from local databases. This takes the form of CORBA objects representing export schemas from separate data sources.

#index 278611
#* Video anywhere: a system for searching and managing distributed heterogeneous video assets
#@ Amit Sheth;Clemens Bertram;Kshitij Shah
#t 1999
#c 5
#! Visual information, especially videos, plays an increasing role in our society for both work and entertainment as more sources become available to the user. Set-top boxes are poised to give home users access to videos that come not only from TV channels and personal recordings, but also from the Internet in the form of downloaded and streaming videos of various types. Current approaches such as Electronic Program Guides and video search engines search for video assets of one type or from one source. The capability to conveniently search through many types of video assets from a large number of video sources with easy-to-use user profiles cannot be found anywhere yet. VideoAnywhere has developed such a capability in the form of an extensible architecture as well as a specific implementation using the latest in Internet programming (Java, agents, XML, etc.) and applicable standards. It automatically extracts and manages an extensible set of metadata of major types of videos that can be queried using either attribute-based or keyword-based search. It also provides user profiling that can be combined with the query processing for filtering. A user-friendly interface provides management of all system functions and capabilities. VideoAnywhere can also be used as a video search engine for the Web, and a servlet-based version has also been implemented.

#index 278615
#* NSF workshop on industrial/academic cooperation in database systems
#@ Mike Carey;Len Seligman
#t 1999
#c 5

#index 278619
#* SQL: 1999, formerly known as SQL3
#@ Andrew Eisenberg;Jim Melton
#t 1999
#c 5
#! For several years now, you've been hearing and reading about an emerging standard that everybody has been calling SQL3. Intended as a major enhancement of the current second generation SQL standard, commonly called SQL-92 because of the year it was published, SQL3 was originally planned to be issued in about 1996…but things didn't go as planned.As you may be aware, SQL3 has been characterized as “object-oriented SQL” and is the foundation for several object-relational database management systems (including Oracle's ORACLE8, Informix' Universal Server, IBM's DB2 Universal Database, and Cloudscape's Cloudscape, among others). This is widely viewed as a “good thing”, but it has had a downside, too: it took nearly 7 years to develop, instead of the planned 3 or 4.As we shall show, SQL:1999 is much more than merely SQL-92 plus object technology. It involves additional features that we consider to fall into SQL's relational heritage, as well as a total restructuring of the standards documents themselves with an eye towards more effective standards progression in the future.

#index 298595
#* Engineering federated information systems: report of EEFIS '99 workshop
#@ S. Conrad;W. Hasselbring;U. Hohenstein;R.-D. Kutsche;M. Roantree;G. Saake;F. Saltor
#t 1999
#c 5
#! After the successful first International Workshop on Engineering Federated Database Systems (EFDBS'97) in Barcelona in June 1997 [CEH+ 97], the goal of this second workshop was to bring together researchers and practitioners interested in various issues in the development of federated information systems, whereby the scope has been extended to cover database and non-database information sources (the change from EFDBS to EFIS reflects this). This report provides details of the workshop content and the conclusions reached in the final discussion.

#index 298596
#* Chorochronos: a research network for spatiotemporal database systems
#@ Andrew Frank;Stephane Grumbach;Ralf Hartmut Güting;Christian S. Jensen;Manolis Koubarakis;Nikos Lorentzos;Yannis Manolopoulos;Enrico Nardelli;Barbara Pernici;Hans-Jörg Schek;Michel Scholl;Timos Sellis;Babis Theodoulidis;Peter Widmayer
#t 1999
#c 5

#index 298597
#* Cost estimation of user-defined methods in object-relational database systems
#@ Jihad Boulos;Kinji Ono
#t 1999
#c 5
#! In this paper we present a novel technique for cost estimation of user-defined methods in advanced database systems. This technique is based on multi-dimensional histograms. We explain how the system collects statistics on the method that a database user defines and adds to the system. From these statistics a multi-dimensional histogram is built. Afterwards, this histrogram can be used for estimating the cost of the target method whenever this method is referenced in a query. This cost estimation is needed by the optimizer of the database system since this cost estimation needs to know the cost of a method in order to place it at its optimal position in the Query Execution Plan (QEP). We explain here how our technique works and we provide an example to better verify its functionality.

#index 298598
#* First-class views: a key to user-centered computing
#@ Arnon Rosenthal;Edward Sciore
#t 1999
#c 5
#! Large database systems (e.g., federations, warehouses) are multi-layer — i.e., a combination of base databases and (virtual or physical) view databases1. Smaller systems use views for layers that hide detailed physical and conceptual structures. We argue that most databases would be more effective if they were more user-centered — i.e., if they allowed users, administrators, and application developers to work mostly within their native view. To do so, we need first class views — views that support most of the metadata and operations available on source tables.First class views could also make multi-tier object architectures (based on objects in multiple tiers of servers) easier to build and maintain. The views modularize code for data services (e.g., query, security) and for coordinating changes with neighboring tiers. When data in each tier is derived declaratively, one can generate some of these methods semi-automatically.Much of the functionality required to support first class views can be generated semi-automatically, if the derivations between layers are declarative (e.g., SQL, rather than Java). We present a framework where propagation rules can be defined, allowing the flexible and incremental specification of view semantics, even by non-programmers. Finally, we describe research areas opened up by this approach.

#index 298599
#* On mutli-resolution document transmission in mobile Web
#@ Stanley M. T. Yau;Hong Va Leong;Dennis McLeod;Antonio Si
#t 1999
#c 5
#! We propose a multi-resolution transmission mechanism that allows various organizational units of a web document to be transferred and browsed according to the amount of information captured. We define the notion of information content for each individual organizational unit of a web document as an indication of its captured information. The concept of information content is used as a foundation for defining the notion of relative information content which determines the transmission order of various units. Our mechanism allows a web client to explore the more content-bearing portion of a web document earlier so as to be able to terminate browsing a possibly irrelevant document sooner. This scheme is based on our observation that different organizational units of a document contribute to different amount of information to the document. Such a multi-resolution transmission paradigm is particularly useful in mobile web where the wireless bandwidth is a scarce resource and browsing every document in detail would consume the bandwidth unnecessarily. This is becoming more serious when the size of a web document is getting large, such as technical documents. We then present a prototype of the system in Java and CORBA to illustrate its feasibility.

#index 298600
#* Distributed transactions in practice
#@ Prabhu Ram;Lyman Do;Pamela Drew
#t 1999
#c 5
#! The concept of transactions and its application has found wide and often indiscriminate usage. In large enterprises, the model for distributed database applications has moved away from the client-server model to a multi-tier model with large database application software forming the middle tier. The software philosophy of “buy and not build” in large enterprises has had a major influence by extending functional requirements such as transactions and data consistency throughout the multiple tiers. In this article, we will discuss the effects of applying traditional transaction management techniques to multi-tier architectures in distributed environments. We will show the performance costs associated with distributed transactions and discuss ways by which enterprises really manage their distributed data to circumvent this performance hit. Our intent is to share our experience as an industrial customer with the database research and vendor community to create more usable and scalable designs.

#index 298601
#* A distributed scientific data archive using the Web, XML and SQL/MED
#@ Mark Papiani;Jasmin L. Wason;Alistair N. Dunlop;Denis A. Nicole
#t 1999
#c 5
#! We have developed a web-based architecture and user interface for fast storage, searching and retrieval of large, distributed, files resulting from scientific simulations. We demonstrate that the new DATALINK type defined in the draft SQL Management of External Data Standard can help to overcome problems associated with limited bandwidth when trying to archive large files using the web. We also show that separating the user interface specification from the user interface processing can provide a number of advantages. We provide a tool to generate automatically a default user interface specification, in the form of an XML document, for a given database. This facilitates deployment of our system by users with little web or database development experience. The XML document can be customised to change the appearance of the interface.

#index 298602
#* An overview and classification of mediated query systems
#@ Ruxandra Domenig;Klaus R. Dittrich
#t 1999
#c 5
#! Multimedia technology, global information infrastructures and other developments allow users to access more and more information sources of various types. However, the “technical” availability alone (by means of networks, WWW, mail systems, databases, etc.) is not sufficient for making meaningful and advanced use of all information available on-line. Therefore, the problem of effectively and efficiently accessing and querying heterogeneous and distributed data sources is an important research direction. This paper aims at classifying existing approaches which can be used to query heterogeneous data sources. We consider one of the approaches — the mediated query approach — in more detail and provide a classification framework for it as well.

#index 298603
#* Database research at the University of Oklahoma
#@ Le Gruenwald;Leonard Brown;Ravi Dirckze;Sylvain Guinepain;Carlos Sanchez;Brian Summers;Sirirut Vanichayobon
#t 1999
#c 5

#index 299939
#* Proceedings of the nineteenth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems
#@ Victor Vianu;Georg Gottlob
#t 2000
#c 5

#index 299941
#* The Web as a graph
#@ Ravi Kumar;Prabhakar Raghavan;Sridhar Rajagopalan;D. Sivakumar;Andrew Tompkins;Eli Upfal
#t 2000
#c 5
#% 197751
#% 248810
#% 249995
#% 255165
#% 255179
#% 255197
#% 261741
#% 262061
#% 268073
#% 268079
#% 268114
#% 281214
#% 281251
#% 290830
#% 300079
#% 309749
#% 479969
#% 614598
#! The pages and hyperlinks of the World-Wide Web may be viewed as nodes and edges in a directed graph. This graph has about a billion nodes today, several billion links, and appears to grow exponentially with time. There are many reasons—mathematical, sociological, and commercial—for studying the evolution of this graph. We first review a set of algorithms that operate on the Web graph, addressing problems from Web search, automatic community discovery, and classification. We then recall a number of measurements and properties of the Web graph. Noting that traditional random graph models do not explain these observations, we propose a new family of random graph models.

#index 299942
#* Typechecking for XML transformers
#@ Tova Milo;Dan Suciu;Victor Vianu
#t 2000
#c 5
#% 101943
#% 183411
#% 210214
#% 219492
#% 229827
#% 237192
#% 241134
#% 248799
#% 248819
#% 273701
#% 273702
#% 279367
#% 281149
#% 299944
#% 309851
#% 463919
#% 479806
#% 631928
#! We study the typechecking problem for XML transformers: given an XML transformation program and a DTD for the input XML documents, check whether every result of the program conforms to a specified output DTD. We model XML transformers using a novel device called a k-pebble transducer, that can express most queries without data-value joins in XML-QL, XSLT, and other XML query languages. Types are modeled by regular tree languages, a nobust extension of DTDs. The main result of the paper is that typechecking for k-pebble transducers is decidable. Consequently, typechecking can be performed for a broad range of XML transformation languages, including XML-QL and a fragment of XSLT.

#index 299943
#* Integrity constraints for XML
#@ Wenfei Fan;Jérôme Siméon
#t 2000
#c 5
#% 36683
#% 58356
#% 69283
#% 186993
#% 205398
#% 235914
#% 237192
#% 248024
#% 248799
#% 273686
#% 273689
#% 384978
#% 442879
#% 564416
#! Integrity constraints are useful for semantic specification, query optimization and data integration. The ID/IDREF mechanism provided by XML DTDs relics on a simple form of constraint to describe references. Yet, this mechanism is not sufficient to express semantic constraints, such as keys or inverse relationships, or stronger, object-style references. In this paper, we investigate integrity constraints for XML, both for semantic purposes and to improve its current reference mechanism. We extend DTDs with several families of constraints, including key, foreign key, inverse constraints and constraints specifying the semantics of object identities. These constraints are useful both for native XML documents and to preserve the semantics of data originating in relational or object databases. Complexity and axiomatization results are established for the (finite) implication problems associated with these constraints. These results also extend relational dependency theory on the interaction between (primary) keys and foreign keys. In addition, we investigate implication of more general constraints, such as functional, inclusion and inverse constraints defined in terms of navigation paths.

#index 299944
#* DTD inference for views of XML data
#@ Yannis Papakonstantinou;Victor Vianu
#t 2000
#c 5
#% 101947
#% 197751
#% 210214
#% 214091
#% 237191
#% 241159
#% 248011
#% 248025
#% 248799
#% 248809
#% 248819
#% 268797
#% 273701
#% 273702
#% 273924
#% 274160
#% 299942
#% 404772
#% 408396
#% 462062
#% 462235
#% 464720
#% 464724
#% 464825
#% 479465
#% 479783
#% 481602
#% 481935
#% 631928
#! We study the inference of Data Type Definitions (DTDs) for views of XML data, using an abstraction that focuses on document content structure. The views are defined by a query language that produces a list of documents selected from one or more input sources. The selection conditions involve vertical and horizontal navigation, thus querying explicitly the order present in input documents. We point several strong limitations in the descriptive ability of current DTDs and the need for extending them with (i) a subtyping mechanism and (ii) a more powerful specification mechanism than regular languages, such as context-free languages. With these extensions, we show that one can always infer tight DTDs, that precisely characterize a selection view on sources satisfying given DTDs. We also show important special cases where one can infer a tight DTD without requiring extension (ii). Finally we consider related problems such as verifying conformance of a view definition with a predefined DTD. Extensions to more powerful views that construct complex documents are also briefly discussed.

#index 299945
#* On the content of materialized aggregate views
#@ Stéphane Grumbach;Leonardo Tininini
#t 2000
#c 5
#% 27056
#% 100610
#% 152588
#% 198047
#% 198465
#% 199537
#% 210182
#% 221375
#% 227869
#% 237190
#% 237198
#% 248038
#% 273696
#% 273698
#% 273916
#% 384978
#% 435158
#% 461921
#% 462204
#% 464056
#% 464215
#% 464706
#% 479795
#% 481951
#% 482081
#% 503868
#! We consider the problem of answering queries using only materialized views. We first show that if the views subsume the query from the point of view of the information content, then the query can be answered using only the views, but the resulting query might be extremely inefficient. We then focus on aggregate views and queries over a single relation, which are fundamental in many applications such as data warehousing. We show that in this case, it is possible to guarantee that as soon as the views subsume the query, it can be completely rewritten in terms of the views in a simple query language. Our main contribution is the conception of various rewriting algorithms which run in polynomial time, and the proof of their completeness which relies on combinatorial arguments. Finally, we discuss the choice of materializing or not ratio views such as average and percentage, important for the design of materialized views. We show that it has an impact on the information content, which can be used to protect data, as well as on the maintenance of views.

#index 299967
#* View-based query processing for regular path queries with inverse
#@ Diego Calvanese;Moshe Y. Vardi;Giuseppe de Giacomo;Maurizio Lenzerini
#t 2000
#c 5
#% 36534
#% 55690
#% 198465
#% 198466
#% 210176
#% 210214
#% 237189
#% 237190
#% 237191
#% 248038
#% 248819
#% 264858
#% 273696
#% 273700
#% 273924
#% 404772
#% 462214
#% 462235
#% 464056
#% 464717
#% 464720
#% 481786
#% 482081
#% 571169
#% 598376
#% 632039
#! View-based query processing is the problem of computing the answer to a query based on a set of materialized views, rather than on the raw data in the database. The problem comes in two different forms, called query rewriting and query answering, respectively. In the first form, we are given a query and a set of view definitions, and the goal is to reformulate the query into an expression that refers only to the views. In the second form, besides the query and the view definitions, we are also given the extensions of the views and a tuple, and the goal is to check whether the knowledge on the view extensions logically implies that the tuple satisfies the query.In this paper we address the problem of view-based query processing in the context of semistructured data, in particular for the case of regular-path queries extended with the inverse operator. Several authors point out that the inverse operator is one of the fundamental extensions for making regular-path queries useful in real settings. We present a novel technique based on the use of two-way finite-state automata. Our approach demonstrates the power of this kind of automata in dealing with the inverse operator, allowing us to show that both query rewriting and query answering with the inverse operator has the same computational complexity as for the case of standard regular-path queries.

#index 299968
#* Query containment for data integration systems
#@ Todd Millstein;Alon Levy;Marc Friedman
#t 2000
#c 5
#% 36181
#% 122396
#% 123118
#% 140410
#% 164364
#% 169844
#% 198465
#% 198466
#% 210176
#% 229827
#% 237189
#% 237190
#% 248038
#% 248801
#% 248858
#% 273911
#% 283052
#% 289266
#% 296931
#% 464056
#% 479452
#% 481128
#% 481916
#% 481923
#% 496091
#% 504570
#% 564419
#% 571169
#% 571216
#% 599549
#% 1499470
#! The problem of query containment is fundamental to many aspects of database systems, including query optimization, determining independence of queries from updates, and rewriting queries using views. In the data integration framework, however, the standard notion of query containment does not suffice. We define relative containment, which formalizes the notion of query containment relative to the sources available to the integration system. First we provide optimal bounds for relative containment for several important classes of datalog queries, including the common case of conjunctive queries. Next we provide bounds for the case when sources enforce access restrictions in the form of binding pattern constraints. Surprisingly, we show that relative containment for conjunctive queries is still decidable in this case, even though it is known that finding all answers to such queries may require a recursive datalog program over the sources. Finally, we provide tight bounds for variants of relative containment when the queries and source descriptions may contain comparison predicates.

#index 299969
#* Constraint satisfaction and database theory: a tutorial
#@ Moshe Y. Vardi
#t 2000
#c 5
#% 36534
#% 36683
#% 39265
#% 55926
#% 65348
#% 70508
#% 101930
#% 115329
#% 125386
#% 145232
#% 150115
#% 159244
#% 160203
#% 172000
#% 172500
#% 190340
#% 191611
#% 198465
#% 198466
#% 210214
#% 237054
#% 237055
#% 237190
#% 248033
#% 248038
#% 248819
#% 268708
#% 273683
#% 273700
#% 289332
#% 299967
#% 320265
#% 384978
#% 464717
#% 481786
#% 566718
#% 568136
#% 593867
#% 598376
#% 599549
#% 600496
#% 632039
#% 705410
#% 836134
#! A large class of problems in AI and other areas of computer science can be viewed as constraint-satisfaction problems. This includes problems in machine vision, belief maintenance, scheduling, temporal reasoning, type reconstruction, graph theory, and satisfiability. In general, the constraint satisfaction-problem is NP-complete, so searching for tractable cases is an active research area. It turns out that constraint satisfaction has an intimate connection with database theory: constraint-satisfaction problems can be recast as database problems and database problems can be recast as constraint-satisfaction problems. In this tutorial, I will cover the fundamentals of constraints satisfaction and describe its intimate relationship with database theory from various perspectives.

#index 299970
#* Auditing Boolean attributes
#@ Jon Kleinberg;Christos Papadimitriou;Prabhakar Raghavan
#t 2000
#c 5
#% 149
#% 1868
#% 13742
#% 67453
#% 70370
#% 286828
#% 287297
#% 287462
#% 287795
#% 340475
#% 463560
#! We study the problem of auditing databases which support statistical sum queries to protect the security of sensitive information; we focus on the special case in which the sensitive information is Boolean. Principles and techniques developed for the security of statistical database in the case of continuous attributes do not apply here. We prove certain strong complexity results suggesting that there is no general efficient solution for the auditing problem in this case. We propose two efficient algorithms: The first is applicable when the sum queries are one-dimensional range queries (we prove that the problem is NP-hard even in the two-dimensional case). The second is an approximate algorithm that maintains security, although it may be too restrictive. Finally, we consider a “dual” variant, with continuous data but an aggregate function that is combinatorial in nature. Specifically, we provide algorithms for two natural definitions of the auditing condition when the aggregate function is MAX.

#index 299971
#* Verification of relational tranducers for electronic commerce
#@ Marc Spielmann
#t 2000
#c 5
#% 101955
#% 137871
#% 191611
#% 210761
#% 213957
#% 248029
#% 275314
#% 384978
#% 535805
#% 543850
#% 590310
#% 598376
#% 637757
#! Motivated by recent work of Abiteboul, Vianu, Fordham, and Yesha [3] we investigate the verifiability of transaction protocols specifying the interaction of multiple partiesvia a network, where each party is equipped with an (active) database that participates in the interaction. Such transaction protocols typically occur in the context of electronic commerce applications and can be formalized as relational transducers. We introduce a class of powerful relational transducers based on Gurevich's abstract state machines and show that several verification problems related to electronic commerce applications can be solved for these transducers. Our approach is, in some sense, complementary to the approach in [3].

#index 299972
#* Reachability and connectivity queries in constraint databases
#@ Michael Benedikt;Martin Grohe;Leonid Libkin;Luc Segoufin
#t 2000
#c 5
#% 6249
#% 28120
#% 101955
#% 190332
#% 213966
#% 246560
#% 248020
#% 262550
#% 310832
#% 384978
#% 410601
#% 464860
#% 477218
#% 505561
#% 542572
#% 587369
#! It is known that standard query languages for constraint databases lack the power to express connectivity properties. Such properties are important in the context of geographical databases, where one naturally wishes to ask queries about connectivity (what are the connected components of a given set?) or reachability (is there a path from A to B that lies entirely in a given region?). No existing constraint query languages that allow closed form evaluation can express these properties.In the first part of the paper, we show that in principle there is no obstacle to getting closed languages that can express connectivity and reachability queries. In fact, we show that adding any topological property to standard languages like FO + LIN and FO+POLY results in a closed language. In the second part of the paper, we look for tractable closed languages for expressing reachability and connectivity queries. We introduce path logic, which allows one to state properties of paths with respect to given regions. We show that it is closed, has polynomial time data complexity for linear and polynomial constraints, and can express a large number of reachability properties beyond simple connectivity. Query evaluation in the logic involves obtaining a discrete abstraction of a continuous path, and model-checking of temporal formulae on the discrete structure.

#index 299973
#* Fixed-point query languages for linear constraint databases
#@ Stephan Kreutzer
#t 2000
#c 5
#% 13742
#% 24108
#% 190332
#% 224744
#% 237186
#% 245656
#% 257440
#% 257866
#% 268788
#% 410601
#% 476167
#% 527014
#% 553832
#! We introduce a family of query languages for linear constraint databases over the reals. The languages are defined over two-sorted structures, the first sort being the real numbers and the second sort consisting of a decomposition of the input relation into regions. The languages are defined as extensions of first-order logic by transitive closure or fixed-point operators, where the fixed-point operators are defined over the set of regions only. It is shown that the query languages capture precisely the queries definable in various standard complexity classes including PTIME.

#index 299974
#* Linear approximation of planar spatial databases using transitive-closure logic
#@ Floris Geerts;Bart Kuijpers
#t 2000
#c 5
#% 2115
#% 78346
#% 113843
#% 190332
#% 213955
#% 224744
#% 237186
#% 248021
#% 248022
#% 248802
#% 273691
#% 410601
#% 455894
#% 455895
#% 455896
#% 464860
#% 505561
#% 527014
#% 534163
#% 542572
#% 553832
#% 562306
#! We consider spatial databases in the plane that can be defined by polynomial constraint formulas. Motivated by applications in geographic information systems, we investigate linear approximations of spatial databases and study in which language they can be expressed effectively. Specifically, we show that they cannot be expressed in the standard first-order query language for polynomial constraint databases but that an extension of this first-order language with transitive closure suffices to express the approximation query in an effective manner. Furthermore, we introduce an extension of transitive-closure logic and show that this logic is complete for the computable queries on linear spatial databases. This result together with our first result implies that this extension of transitive-closure logic can express all computable topological queries on arbitrary spatial databases in the plane.

#index 299975
#* Computational aspects of resilient data extraction from semistructured sources (extended abstract)
#@ Hasan Davulcu;Guizhen Yang;Michael Kifer;I. V. Ramakrishnan
#t 2000
#c 5
#% 210214
#% 227987
#% 229828
#% 237191
#% 244103
#% 248808
#% 287202
#% 287283
#% 300288
#% 383546
#% 404772
#% 450972
#% 464720
#% 511897
#% 600552
#! Automatic data extraction from semistructured sources such as HTML pages is rapidly growing into a problem of significant importance, spurred by the growing popularity of the so called “shopbots” that enable end users to compare prices of goods and other services at various web sites without having to manually browse and fill out forms at each one of these sites.The main problem one has to contend with when designing data extraction techniques is that the contents of a web page changes frequently, either because its data is generated dynamically, in response to filling out a form, or because of changes to its presentation format. This makes the problem of data extraction particularly challenging, since a desirable requirement of any data extraction technique is that it be “resilient”, i.e., using it we should always be able to locate the object of interest in a page (such as a form or an element in a table generated by a form fill-out) in spite of changes to the page's ntent and layout.In this paper we propose a formal computation model for developing resilient data extraction techniques from semistructured sources. Specifically we formalize the problem of data extraction as one of generating unambiguous extraction expressions, which are regular expressions with some additional structure. The problem of resilience is then formalized as one of generating a maximal extraction expression of this kind. We present characterization theorems for maximal extraction expressions, complexity results for testing them, and algorithms for synthesizing them.

#index 299976
#* Expressive and efficient pattern languages for tree-structured data (extended abstract)
#@ Frank Neven;Thomas Schwentick
#t 2000
#c 5
#% 1195
#% 197751
#% 210214
#% 237192
#% 241166
#% 248011
#% 248024
#% 248799
#% 248819
#% 273700
#% 273701
#% 281149
#% 291299
#% 299944
#% 384978
#% 544726
#! It would be desirable to have a query language for tree-structured data that is (1) as easily usable as SQL, (2) as expressive as monadic second-order logic (MSO), and (3) efficiently evaluable. The paper develops some ideas in this direction. Towards (1) the specification of sets of vertices of a tree by combining conditions on their induced subtree with conditions on their path to the root is proposed. Existing query languages allow regular expressions (hence MSO logic) in path conditions but are limited in expressing subtree conditions. It is shown that such query languages fall short of capturing all MSO queries. On the other hand, allowing a certain guarded fragment of MSO-logic in the specification of subtree conditions results in a language fulfilling (2), (3) and, anguably, (1).

#index 299977
#* Expressive power and data complexity of nonrecursive query languages for lists and trees (extended abstract)
#@ Evgeny Dantsin;Andrei Voronkov
#t 2000
#c 5
#% 6787
#% 28120
#% 36683
#% 101646
#% 101949
#% 139177
#% 145169
#% 154314
#% 164378
#% 173860
#% 190332
#% 237188
#% 248021
#% 248022
#% 248037
#% 384978
#% 399235
#% 565149
#% 587330
#% 598376
#! We extend the traditional query languages by primitives for handling lists and trees. Our main results characterize the expressive power and data complexity of the following extended languages: (1) relational algebra with lists and trees, (2) nonrecursive Datalog@@@@ with lists and trees, (3) nonrecursive Prolog with lists and trees, (4) first-order logic over lists and trees.Languages (2)-(4) turn out to have the same expressive power; their range-restricted fragments have the same expressive power as (1). Every query in these languages is a boolean combination of range-restricted queries.We also prove that these query languages have polynomial data complexity under any “reasonable” encoding of inputs. Furthermore, under a natural encoding of inputs, languages (2)-(4) have the same expressive power as first-order logic over finite structures, therefore their data complexity is in A Co. Thus, the use of lists and trees in nonrecursive query languages gives no gain in the expressiveness. This contrasts with a huge difference between the nonelementary program complexity of extended languages (2)-(4) and the PSPA CE program complexity of their relational counterparts.Our results partly explain why lists and trees are not so widely used in nonrecursive query languages as other collection types.

#index 299978
#* Indexing the edges—a simple and yet efficient approach to high-dimensional indexing
#@ Beng Chin Ooi;Kian-Lee Tan;Cui Yu;Stephane Bressan
#t 2000
#c 5
#% 86950
#% 248796
#% 317933
#% 382477
#% 427199
#% 479649
#! In this paper, we propose a new tunable index scheme, called iMinMax(&Ogr;), that maps points in high dimensional spaces to single dimension values determined by their maximum or minimum values among all dimensions. By varying the tuning “knob” &Ogr;, we can obtain different family of iMinMax structures that are optimized for different distributions of data sets. For a d-dimensional space, a range query need to be transformed into d subqueries. However, some of these subqueries can be pruned away without evaluation, further enhancing the efficiency of the scheme. Experimental results show that iMinMax(&Ogr;) can outperform the more complex Pyramid technique by a wide margin.

#index 299979
#* Indexing moving points (extended abstract)
#@ Pankaj K. Agarwal;Lars Arge;Jeff Erickson
#t 2000
#c 5
#% 56081
#% 144870
#% 164362
#% 201924
#% 210355
#% 211801
#% 236614
#% 248028
#% 252304
#% 260066
#% 273706
#% 273714
#% 274143
#% 282343
#% 282588
#% 295512
#% 300174
#% 370597
#% 443130
#% 461923
#% 464847
#% 503869
#% 560840
#% 571296
#% 588714
#% 618583
#% 656697
#! We propose three indexing schemes for storing a set S of N points in the plane, each moving along a linear trajectory, so that a query of the following form can be answered quickly: Given a rectangle R and a real value tq, report all K points of S that lie inside R at time tq. We first present an indexing structure that, for any given constant &egr; 0, uses O(N/B) disk blocks, where B is the block size, and answers a query in O((N/B)1/2+&egr; + K/B) I/Os. It can also report all the points of S that lie inside R during a given time interval. A point can be inserted or deleted, or the trajectory of a point can be changed, in O(log2B N) I/Os. Next, we present a general approach that improves the query time if the queries arrive in chronological order, by allowing the index to evolve over time. We obtain a trade off between the query time and the number of times the index needs to be updated as the points move. We also describe an indexing scheme in which the number of I/Os required to answer a query depends monotonically on the difference between tq and the current time. Finally, we develop an efficient indexing scheme to answer approximate nearest-neighbor queries among moving points.

#index 299980
#* On Herbrand semantics and conflict serializability of read-write transactions (extended abstract)
#@ Jens Lechtenbörger;Gottfried Vossen
#t 2000
#c 5
#% 748
#% 3645
#% 9241
#% 36180
#% 112319
#% 137940
#% 154315
#% 166215
#% 247424
#% 286836
#% 458568
#% 464000
#! The quest for unified correctness criteria in database concurrency control is addressed from a new perspective. A family of Herbrand semantics is presented, where each semantics provides an interpretation for operations in the read-write model of transactions. Using commutativity arguments, each semantics leads to a notion of conflict, which then gives rise to distinct classes of serializable schedules. Surprisingly, the classical notion of serializability with respect to two of these sematics, update-in-place and deferred-update semantics, already embodies a unified correctness criterion; moreover, prefix-closed variants of it allow for a higher degree of transaction parallelism than, for example, prefix-reducibility. Finally, it is shown that previous criteria may permit undesirable schedules, which are ruled out by a stronger notion of serializability that captures all intuitively correct schedules, but is incomparable to prefix-reducibility and the classes of schedules recognized by optimistic protocols.

#index 299981
#* Entrepreneurship for information systems researchers (invited tutorial) (abstract only)
#@ Ashish Gupta
#t 2000
#c 5
#! Today's environment offers tremendous opportunity for researchers to contribute to the advancement of commercial enterprises. Traditionally this involvement has been in the form of research grants from larger organizations and combined projects with research laboratories or advance development centers. More recently the involvement has been in the form of researchers starting or helping start small companies that are productising cutting edge technology.This tutorial will discuss logistics of start up companies, funding, and other issues associated with commercialising technology through the start-up route. Questions will be answered by the speaker and by knowledgeable members of the audience.

#index 299982
#* Optimal histograms for hierarchical range queries (extended abstract)
#@ Nick Koudas;S. Muthukrishnan;Divesh Srivastava
#t 2000
#c 5
#% 43163
#% 54047
#% 152585
#% 201921
#% 210190
#% 248822
#% 273901
#% 411554
#% 479648
#% 480123
#% 481266

#index 299983
#* (Almost) optimal parallel block access to range queries
#@ Mikhail J. Atallah;Sunil Prabhakar
#t 2000
#c 5
#% 43179
#% 227856
#% 250026
#% 286962
#% 339622
#% 462233
#% 464718
#% 480794
#% 609704
#% 632069
#! This guarantee is true for any number of dimensions. Subsequent to this work, Bhatia et al. [4] have proved that such a performance bound is essentially optimal for this kind of scheme, and have also extended our results to the case where the number of disks is a product of the form &kgr;1 * &kgr;2 * … * &kgr;t where the &kgr;ts need not all be 2.

#index 299984
#* Selectively estimation for Boolean queries
#@ Zhiyuan Chen;Nick Koudas;Flip Korn;S. Muthukrishnan
#t 2000
#c 5
#% 115462
#% 210189
#% 232644
#% 243166
#% 249238
#% 273705
#% 289010
#% 306494
#% 406493
#% 461918
#% 479648
#% 479958
#% 616528
#% 632029
#% 641107
#! In a variety of applications ranging from optimizing queries on alphanumeric attributes to providing approximate counts of documents containing several query terms, there is an increasing need to quickly and reliably estimate the number of strings (tuples, documents, etc.) matching a Boolean query. Boolean queries in this context consist of substring predicates composed using Boolean operators. While there has been some work in estimating the selectivity of substring queries, the more general problem of estimating the selectivity of Boolean queries over substring predicates has not been studied.Our approach is to extract selectivity estimates from relationships between the substring predicates of the Boolean query. However, storing the correlation between all possible predicates in order to provide an exact answer to such predicates is clearly infeasible, as there is a super-exponential number of possible combinations of these predicates. Instead, our novel idea is to capture correlations in a space-efficient but approximate manner. We employ a Monte Carlo technique called set hashing to succinctly represent the set of strings containing a given substring as a signature vector of hash values. Correlations among substring predicates can then be generated on-the-fly by operating on these signatures.We formalize our approach and propose an algorithm for estimating the selectivity of any Boolean query using the signatures of its substring predicates. We then experimentally demonstrate the superiority of our approach over a straight-forward approach based on the independence assumption wherein correlations are not explicitly captured.

#index 299985
#* Transversing itemset lattices with statistical metric pruning
#@ Shinichi Morishita;Jun Sese
#t 2000
#c 5
#% 152934
#% 201894
#% 210162
#% 213977
#% 227917
#% 227919
#% 248012
#% 248784
#% 248785
#% 248791
#% 273899
#% 280433
#% 280434
#% 280436
#% 408396
#% 479484
#% 479643
#% 481290
#% 481949
#% 482105
#! We study how to efficiently compute significant association rules according to common statistical measures such as a chi-squared value or correlation coefficient. For this purpose, one might consider to use of the Apriori algorithm, but the algorithm needs major conversion, because none of these statistical metrics are anti-monotone, and the use of higher support for reducing the search space cannot guarantee solutions in its the search space. We here present a method of estimating a tight upper bound on the statistical metric associated with any superset of an itemset, as well as the novel use of the resulting information of upper bounds to prune unproductive supersets while traversing itemset lattices. Experimental tests demonstrate the efficiency of this method.

#index 299986
#* Computational properties of metaquerying problems
#@ F. Angiulli;R. Ben-Eliyahu-Zohary;L. Palopoli;G. B. Ianni
#t 2000
#c 5
#% 84275
#% 152934
#% 232102
#% 232146
#% 289424
#% 384978
#% 408396
#% 443084
#% 496254
#% 593867
#% 598376
#% 599549
#! Metaquerying is a datamining technology by which hidden dependencies among several database relations can be discovered. This tool has already been successfully applied to several real-world applications. Recent papers provide only very preliminary results about the complexity of metaquerying. In this paper we define several variants of metaquerying that encompass, as far as we know, all variants defined in the literature. We study both the combined complexity and the data complexity of these variants. We show that, under the combined complexity measure, metaquerying is generally intractable (unless P=NP), but we are able to single out some tractable interesting metaquerying cases (whose combined complexity is LOGCFL-complete). As for the data complexity of metaquerying, we prove that, in general, this is in P, but lies within AC0 in some interesting cases. Finally, we discuss the issue of equivalence between metaqueries, which is useful for optimization purposes.

#index 299987
#* Uniform generation in spatial constraint databases and applications (Extended abstract)
#@ David Gross;Michel de Rougemont
#t 2000
#c 5
#% 8387
#% 8951
#% 13742
#% 77321
#% 90740
#% 95598
#% 101920
#% 164406
#% 190330
#% 190332
#% 213950
#% 224744
#% 237188
#% 242662
#% 245656
#% 248021
#% 248022
#% 273691
#% 277347
#% 292679
#% 296571
#% 463577
#% 483546
#% 552988
#% 593739
#! We study the efficient approximation of queries in linear constraint databases using sampling techniques. We define the notion of an almost uniform generator for a generalized relation and extend the classical generator of Dyer, Frieze and Kannan for convex sets to the union and the projection of relations. For the intersection and the difference, we give sufficient conditions for the existence of such generators. We show how such generators give relative estimations of the volume and approximations of generalized relations as the composition of convex hulls obtained from the samples.

#index 299988
#* Analysis and application of adaptive sampling
#@ James F. Lynch
#t 2000
#c 5
#% 70050
#% 77940
#% 122392
#% 190330
#% 243166
#! An estimation algorithm for a query is a probabilistic algorithm that computes an approximation for the size (number of tuples) of the query. The main question that is studied is which classes of logically definable queries have fast estimation algorithms. Evidence from descriptive complexity theory is provided that indicates not all such queries have fast estimation algorithms. However, it is shown that on classes of structures of bounded degree, all first-order queries have fast estimation algorithms.These estimation algorithms use a form of statistical sampling known as adaptive sampling. Several versions of adaptive sampling have been developed by other researchers. The original version has been surpassed in some ways by a newer version and a more specialized Monte-Carlo algorithm. An analysis of the average run time of the original version is given, and the different algorithms are compared. The analysis is used to compute what appears to be the best known upper bound on the efficiency of the original algorithm. Also, contrary to what seems to be a commonly held opinion, the two methods of adaptive sampling are incomparable. Which method is superior depends on the query being estimated and the criteria that are being applied. Lastly, adaptive sampling can be more efficient than the Monte-Carlo algorithm if knowledge about the maximum values of the data being sampled is available.

#index 299989
#* Towards estimation error guarantees for distinct values
#@ Moses Charikar;Surajit Chaudhuri;Rajeev Motwani;Vivek Narasayya
#t 2000
#c 5
#% 25208
#% 36119
#% 58348
#% 69273
#% 99463
#% 143971
#% 190611
#% 210190
#% 214073
#% 248821
#% 273908
#% 277347
#% 481749
#% 482100
#! We consider the problem of estimating the number of distinct values in a column of a table. For large tables without an index on the column, random sampling appears to be the only scalable approach for estimating the number of distinct values. We establish a powerful negative result stating that no estimator can guarantee small error across all input distributions, unless it examines a large fraction of the input data. In fact, any estimator must incur a significant error on at least some of a natural class of distributions. We then provide a new estimator which is provably optimal, in that its error is guaranteed to essentially match our negative result. A drawback of this estimator is that while its worst-case error is reasonable, it does not necessarily give the best possible error bound on any given distribution. Therefore, we develop heuristic estimators that are optimized for a class of typical input distributions. While these estimators lack strong guarantees on distribution-independent worst-case error, our extensive empirical comparison indicate their effectiveness both on real data sets and on synthetic data sets.

#index 300120
#* Mining frequent patterns without candidate generation
#@ Jiawei Han;Jian Pei;Yiwen Yin
#t 2000
#c 5
#% 172386
#% 201894
#% 227919
#% 248785
#% 248791
#% 248813
#% 280409
#% 329598
#% 420063
#% 461909
#% 463903
#% 479484
#% 481290
#% 481754
#% 631926
#! Mining frequent patterns in transaction databases, time-series databases, and many other kinds of databases has been studied popularly in data mining research. Most of the previous studies adopt an Apriori-like candidate set generation-and-test approach. However, candidate set generation is still costly, especially when there exist prolific patterns and/or long patterns.In this study, we propose a novel frequent pattern tree (FP-tree) structure, which is an extended prefix-tree structure for storing compressed, crucial information about frequent patterns, and develop an efficient FP-tree-based mining method, FP-growth, for mining the complete set of frequent patterns by pattern fragment growth. Efficiency of mining is achieved with three techniques: (1) a large database is compressed into a highly condensed, much smaller data structure, which avoids costly, repeated database scans, (2) our FP-tree-based mining adopts a pattern fragment growth method to avoid the costly generation of a large number of candidate sets, and (3) a partitioning-based, divide-and-conquer method is used to decompose the mining task into a set of smaller tasks for mining confined patterns in conditional databases, which dramatically reduces the search space. Our performance study shows that the FP-growth method is efficient and scalable for mining both long and short frequent patterns, and is about an order of magnitude faster than the Apriori algorithm and also faster than some recently reported new frequent pattern mining methods.

#index 300123
#* Data mining on an OLTP system (nearly) for free
#@ Erik Riedel;Christos Faloutsos;Gregory R. Ganger;David F. Nagle
#t 2000
#c 5
#% 159079
#% 164766
#% 199537
#% 201692
#% 223781
#% 248790
#% 261738
#% 420057
#% 443091
#% 479482
#% 479802
#% 481127
#! This paper proposes a scheme for scheduling disk requests that takes advantage of the ability of high-level functions to operate directly at individual disk drives. We show that such a scheme makes it possible to support a Data Mining workload on an OLTP system almost for free: there is only a small impact on the throughput and response time of the existing workload. Specifically, we show that an OLTP system has the disk resources to consistently provide one third of its sequential bandwidth to a background Data Mining task with close to zero impact on OLTP throughput and response time at high transaction loads. At low transaction loads, we show much lower impact than observed in previous work. This means that a production OLTP system can be used for Data Mining tasks without the expense of a second dedicated system. Our scheme takes advantage of close interaction with the on-disk scheduler by reading blocks for the Data Mining workload as the disk head “passes over” them while satisfying demand blocks from the OLTP request stream. We show that this scheme provides a consistent level of throughput for the background workload even at very high foreground loads. Such a scheme is of most benefit in combination with an Active Disk environment that allows the background Data Mining application to also take advantage of the processing power and memory available directly on the disk drives.

#index 300124
#* Turbo-charging vertical mining of large databases
#@ Pradeep Shenoy;Jayant R. Haritsa;S. Sudarshan;Gaurav Bhalotia;Mayank Bawa;Devavrat Shah
#t 2000
#c 5
#% 152934
#% 340289
#% 481290
#% 481754
#% 631986
#% 707637
#! In a vertical representation of a market-basket database, each item is associated with a column of values representing the transactions in which it is present. The association-rule mining algorithms that have been recently proposed for this representation show performance improvements over their classical horizontal counterparts, but are either efficient only for certain database sizes, or assume particular characteristics of the database contents, or are applicable only to specific kinds of database schemas. We present here a new vertical mining algorithm called VIPER, which is general-purpose, making no special requirements of the underlying database. VIPER stores data in compressed bit-vectors called “snakes” and integrates a number of novel optimizations for efficient snake generation, intersection, counting and storage. We analyze the performance of VIPER for a range of synthetic database workloads. Our experimental results indicate significant performance gains, especially for large databases, over previously proposed vertical and horizontal mining algorithms. In fact, there are even workload regions where VIPER outperforms an optimal, but practically infeasible, horizontal mining algorithm.

#index 300126
#* High speed on-line backup when using logical log operations
#@ David B. Lomet
#t 2000
#c 5
#% 117
#% 9241
#% 91620
#% 114582
#% 152926
#% 245790
#% 273892
#% 317988
#% 403195
#% 464823
#% 481126
#% 481624
#! Media recovery protects a database from failures of the stable medium by maintaining an extra copy of the database, called the backup, and a media recovery log. When a failure occurs, the database is “restored” from the backup, and the media recovery log is used to roll forward the database to the desired time, usually the current time. Backup must be both fast and “on-line”, i.e. concurrent with on-going update activity. Conventional online backup sequentially copies from the stable database, almost independent of the database cache manager, but requires page-oriented log operations. But results of logical operations must be flushed to a stable database (a backup is a stable database) in a constrained order to guarantee recovery. This order is not naturally achieved for the backup by a cache manager concerned only with crash recovery. We describe a “full speed” backup, only loosely coupled to the cache manager, and hence similar to current online backups, but effective for general logical log operations. This requires additional logging of cached objects to guarantee media recoverability. We then show how logging can be greatly reduced when log operations have a constrained form which nonetheless provides very useful additional logging efficiency for database systems.

#index 300127
#* Efficient resumption of interrupted warehouse loads
#@ Wilburt Juan Labio;Janet L. Wiener;Hector Garcia-Molina;Vlad Gorelik
#t 2000
#c 5
#% 86930
#% 116086
#% 235084
#% 403195
#% 481099
#% 481622
#! Data warehouses collect large quantities of data from distributed sources into a single repository. A typical load to create or maintain a warehouse processes GBs of data, takes hours or even days to execute, and involves many complex and user-defined transformations of the data (e.g., find duplicates, resolve data inconsistencies, and add unique keys). If the load fails, a possible approach is to “redo” the entire load. A better approach is to resume the incomplete load from where it was interrupted. Unfortunately, traditional algorithms for resuming the load either impose unacceptable overhead during normal operation, or rely on the specifics of transformations. We develop a resumption algorithm called DR that imposes no overhead and relies only on the high-level properties of the transformations. We show that DR can lead to a ten-fold reduction in resumption time by performing experiments using commercial software.

#index 300129
#* On-line reorganization in object databases
#@ Mohana K. Lakhamraju;Rajeev Rastogi;S. Seshadri;S. Sudarshan
#t 2000
#c 5
#% 32903
#% 102745
#% 116083
#% 116086
#% 152930
#% 172938
#% 172948
#% 210174
#% 210175
#% 422875
#% 458528
#% 463741
#% 473367
#% 479639
#% 480939
#% 481454
#% 481619
#% 481756
#% 482099
#! Reorganization of objects in an object databases is an important component of several operations like compaction, clustering, and schema evolution. The high availability requirements (24 × 7 operation) of certain application domains requires reorganization to be performed on-line with minimal interference to concurrently executing transactions.In this paper, we address the problem of on-line reorganization in object databases, where a set of objects have to be migrated from one location to another. Specifically, we consider the case where objects in the database may contain physical references to other objects. Relocating an object in this case involves finding the set of objects (parents) that refer to it, and modifying the references in each parent. We propose an algorithm called the Incremental Reorganization Algorithm (IRA) that achieves the above task with minimal interference to concurrently executing transactions. The IRA algorithm holds locks on at most two distinct objects at any point of time. We have implemented IRA on Brahma, a storage manager developed at IIT Bombay, and conducted an extensive performance study. Our experiments reveal that IRA makes on-line reorganization feasible, with very little impact on the response times of concurrently executing transactions and on overall system throughput. We also describe how the IRA algorithm can handle system failures.

#index 300131
#* Finding generalized projected clusters in high dimensional spaces
#@ Charu C. Aggarwal;Philip S. Yu
#t 2000
#c 5
#% 36672
#% 201893
#% 210173
#% 232764
#% 248790
#% 248792
#% 248798
#% 249321
#% 273891
#% 280417
#% 462243
#% 479962
#% 479973
#% 481281
#! High dimensional data has always been a challenge for clustering algorithms because of the inherent sparsity of the points. Recent research results indicate that in high dimensional data, even the concept of proximity or clustering may not be meaningful. We discuss very general techniques for projected clustering which are able to construct clusters in arbitrarily aligned subspaces of lower dimensionality. The subspaces are specific to the clusters themselves. This definition is substantially more general and realistic than currently available techniques which limit the method to only projections from the original set of attributes. The generalized projected clustering technique may also be viewed as a way of trying to redefine clustering for high dimensional applications by searching for hidden subspaces with clusters which are created by inter-attribute correlations. We provide a new concept of using extended cluster feature vectors in order to make the algorithm scalable for very large databases. The running time and space requirements of the algorithm are adjustable, and are likely ta tradeoff with better accuracy.

#index 300132
#* Density biased sampling: an improved method for data mining and clustering
#@ Christopher R. Palmer;Christos Faloutsos
#t 2000
#c 5
#% 1331
#% 3888
#% 86955
#% 115478
#% 210173
#% 227883
#% 248790
#% 248812
#% 262045
#% 375388
#% 376266
#% 480948
#% 481749
#% 481782
#! Data mining in large data sets often requires a sampling or summarization step to form an in-core representation of the data that can be processed more efficiently. Uniform random sampling is frequently used in practice and also frequently criticized because it will miss small clusters. Many natural phenomena are known to follow Zipf's distribution and the inability of uniform sampling to find small clusters is of practical concern. Density Biased Sampling is proposed to probabilistically under-sample dense regions and over-sample light regions. A weighted sample is used to preserve the densities of the original data. Density biased sampling naturally includes uniform sampling as a special case. A memory efficient algorithm is proposed that approximates density biased sampling using only a single scan of the data. We empirically evaluate density biased sampling using synthetic data sets that exhibit varying cluster size distributions finding up to a factor of six improvement over uniform sampling.

#index 300136
#* LOF: identifying density-based local outliers
#@ Markus M. Breunig;Hans-Peter Kriegel;Raymond T. Ng;Jörg Sander
#t 2000
#c 5
#% 2115
#% 210173
#% 230138
#% 248792
#% 273890
#% 300183
#% 420064
#% 479649
#% 479791
#% 479799
#% 479986
#% 481281
#% 481956
#% 566128
#! For many KDD applications, such as detecting criminal activities in E-commerce, finding the rare instances or the outliers, can be more interesting than finding the common patterns. Existing work in outlier detection regards being an outlier as a binary property. In this paper, we contend that for many scenarios, it is more meaningful to assign to each object a degree of being an outlier. This degree is called the local outlier factor (LOF) of an object. It is local in that the degree depends on how isolated the object is with respect to the surrounding neighborhood. We give a detailed formal analysis showing that LOF enjoys many desirable properties. Using real-world datasets, we demonstrate that LOF can be used to find outliers which appear to be meaningful, but can otherwise not be identified with existing approaches. Finally, a careful performance evaluation of our algorithm confirms we show that our approach of finding local outliers can be practical.

#index 300138
#* Answering complex SQL queries using automatic summary tables
#@ Markos Zaharioudakis;Roberta Cochrane;George Lapis;Hamid Pirahesh;Monica Urata
#t 2000
#c 5
#% 198465
#% 210182
#% 227869
#% 248034
#% 273696
#% 273698
#% 464056
#% 464215
#% 479792
#% 481604
#% 482081
#! We investigate the problem of using materialized views to answer SQL queries. We focus on modern decision-support queries, which involve joins, arithmetic operations and other (possibly user-defined) functions, aggregation (often along multiple dimensions), and nested subqueries. Given the complexity of such queries, the vast amounts of data upon which they operate, and the requirement for interactive response times, the use of materialized views (MVs) of similar complexity is often mandatory for acceptable performance. We present a novel algorithm that is able to rewrite a user query so that it will access one or more of the available MVs instead of the base tables. The algorithm extends prior work by addressing the new sources of complexity mentioned above, that is, complex expressions, multidimensional aggregation, and nested subqueries. It does so by relying on a graphical representation of queries and a bottom-up, pair-wise matching of nodes from the query and MV graphs. This approach offers great modularity and extensibility, allowing for the rewriting of a large class of queries.

#index 300139
#* Synchronizing a database to improve freshness
#@ Junghoo Cho;Hector Garcia-Molina
#t 2000
#c 5
#% 201928
#% 210182
#% 268079
#% 268087
#% 281251
#! In this paper we study how to refresh a local copy of an autonomous data source to maintain the copy up-to-date. As the size of the data grows, it becomes more difficult to maintain the copy \ fresh, “making it crucial to synchronize the copy effectively. We define two freshness metrics, change models of the underlying data, and synchronization policies. We analytically study how effective the various policies are. We also experimentally verify our analysis, based on data collected from 270 web sites for more than 4 months, and we show that our new policy improves the \ freshness” very significantly compared to current policies in use.

#index 300141
#* How to roll a join: asynchronous incremental view maintenance
#@ Kenneth Salem;Kevin Beyer;Bruce Lindsay;Roberta Cochrane
#t 2000
#c 5
#% 13016
#% 152928
#% 201928
#% 210210
#% 227869
#% 227944
#% 227947
#% 340300
#% 340301
#% 458556
#% 479792
#! Incremental refresh of a materialized join view is often less expensive than a full, non-incremental refresh. However, it is still a potentially costly atomic operation. This paper presents an algorithm that performs incremental view maintenance as a series of small, asynchronous steps. The size of each step can be controlled to limit contention between the refresh process and concurrent operations that access the materialized view or the underlying relations. The algorithm supports point-in-time refresh, which allows a materialized view to be refreshed to any time between the last refresh and the present.

#index 300143
#* On wrapping query languages and efficient XML integration
#@ Vassilis Christophides;Sophie Cluet;Jérǒme Simèon
#t 2000
#c 5
#% 83933
#% 123589
#% 136740
#% 172927
#% 194984
#% 210205
#% 235914
#% 248799
#% 265411
#% 273912
#% 274160
#% 360914
#% 459241
#% 462235
#% 463919
#% 479452
#% 479781
#% 479806
#% 481288
#% 481614
#% 481935
#% 511734
#% 562135
#% 591538
#% 614579
#% 631868
#! Modern applications (Web portals, digital libraries, etc.) require integrated access to various information sources (from traditional DBMS to semistructured Web repositories), fast deployment and low maintenance cost in a rapidly evolving environment. Because of its flexibility, there is an increasing interest in using XML as a middleware model for such applications. XML enables fast wrapping and declarative integration. However, query processing in XML-based integration systems is still penalized by the lack of an algebra with adequate optimization properties and the difficulty to understand source query capabilities. In this paper, we propose an algebraic approach to support efficient XML query evaluation. We define a general purpose algebra suitable for semistructured on XML query languages. We show how this algebra can be used, with appropriate type information, to also wrap more structured query languages such as OQL or SQL. Finally, we develop new optimization techniques for XML-based integration systems.

#index 300153
#* XMill: an efficient compressor for XML data
#@ Hartmut Liefke;Dan Suciu
#t 2000
#c 5
#% 68236
#% 146203
#% 157345
#% 179173
#% 243210
#% 308458
#% 404772
#% 443122
#% 464843
#% 479465
#% 481424
#% 740916
#! We describe a tool for compressing XML data, with applications in data exchange and archiving, which usually achieves about twice the compression ratio of gzip at roughly the same speed. The compressor, called XMill, incorporates and combines existing compressors in order to apply them to heterogeneous XML data: it uses zlib, the library function for gzip, a collection of datatype specific compressors for simple data types, and, possibly, user defined compressors for application specific data types.

#index 300157
#* XTRACT: a system for extracting document type descriptors from XML documents
#@ Minos Garofalakis;Aristides Gionis;Rajeev Rastogi;S. Seshadri;Kyuseok Shim
#t 2000
#c 5
#% 61792
#% 114261
#% 151246
#% 248809
#% 273922
#% 369349
#% 404772
#% 464720
#% 479465
#% 479956
#% 520221
#% 542161
#% 593913
#! XML is rapidly emerging as the new standard for data representation and exchange on the Web. An XML document can be accompanied by a Document Type Descriptor (DTD) which plays the role of a schema for an XML data collection. DTDs contain valuable information on the structure of documents and thus have a crucial role in the efficient storage of XML data, as well as the effective formulation and optimization of XML queries. In this paper, we propose XTRACT, a novel system for inferring a DTD schema for a database of XML documents. Since the DTD syntax incorporates the full expressive power of regular expressions, naive approaches typically fail to produce concise and intuitive DTDs. Instead, the XTRACT inference algorithms employ a sequence of sophisticated steps that involve: (1) finding patterns in the input sequences and replacing them with regular expressions to generate “general” candidate DTDs, (2) factoring candidate DTDs using adaptations of algorithms from the logic optimization literature, and (3) applying the Minimum Description Length (MDL) principle to find the best DTD among the candidates. The results of our experiments with real-life and synthetic DTDs demonstrate the effectiveness of XTRACT's approach in inferring concise and semantically meaningful DTD schemas for XML databases.

#index 300160
#* Spatial join selectivity using power laws
#@ Christos Faloutsos;Bernhard Seeger;Agma Traina;Caetano Traina, Jr.
#t 2000
#c 5
#% 13041
#% 152937
#% 164360
#% 172909
#% 172949
#% 210187
#% 213975
#% 227932
#% 242366
#% 273685
#% 273886
#% 273902
#% 273908
#% 285924
#% 411554
#% 435137
#% 437509
#% 461903
#% 462070
#% 462236
#% 463595
#% 464831
#% 479797
#% 480125
#% 480774
#% 481281
#% 481620
#% 481920
#% 631937
#! We discovered a surprising law governing the spatial join selectivity across two sets of points. An example of such a spatial join is “find the libraries that are within 10 miles of schools”. Our law dictates that the number of such qualifying pairs follows a power law, whose exponent we call “pair-count exponent” (PC). We show that this law also holds for self-spatial-joins (“find schools within 5 miles of other schools”) in addition to the general case that the two point-sets are distinct. Our law holds for many real datasets, including diverse environments (geographic datasets, feature vectors from biology data, galaxy data from astronomy).In addition, we introduce the concept of the Box-Occupancy-Product-Sum (BOPS) plot, and we show that it can compute the pair-count exponent in a timely manner, reducing the run time by orders of magnitude, from quadratic to linear. Due to the pair-count exponent and our analysis (Law 1), we can achieve accurate selectivity estimates in constant time (O(1)) without the need for sampling or other expensive operations. The relative error in selectivity is about 30% with our fast BOPS method, and even better (about 10%), if we use the slower, quadratic method.

#index 300161
#* MOCHA: a self-extensible database middleware system for distributed data sources
#@ Manuel Rodríguez-Martínez;Nick Roussopoulos
#t 2000
#c 5
#% 136740
#% 152902
#% 152940
#% 210177
#% 248817
#% 411554
#% 479449
#% 479452
#% 479467
#% 479937
#% 481915
#% 631868
#! We present MOCHA, a new self-extensible database middleware system designed to interconnect distributed data sources. MOCHA is designed to scale to large environments and is based on the idea that some of the user-defined functionality in the system should be deployed by the middleware system itself. This is realized by shipping Java code implementing either advanced data types or tailored query operators to remote data sources and have it executed remotely. Optimized query plans push the evaluation of powerful data-reducing operators to the data source sites while executing data-inflating operators near the client's site. The Volume Reduction Factor is a new and more explicit metric introduced in this paper to select the best site to execute query operators and is shown to be more accurate than the standard selectivity factor alone. MOCHA has been implemented in Java and runs on top of Informix and Oracle. We present the architecture of MOCHA, the ideas behind it, and a performance study using scientific data and queries. The results of this study demonstrate that MOCHA provides a more flexible, scalable and efficient framework for distributed query processing compared to those in existing middleware solutions.

#index 300162
#* Closest pair queries in spatial databases
#@ Antonio Corral;Yannis Manolopoulos;Yannis Theodoridis;Michael Vassilakopoulos
#t 2000
#c 5
#% 2115
#% 86950
#% 152902
#% 152937
#% 186897
#% 201876
#% 227934
#% 238060
#% 248804
#% 252304
#% 273685
#% 273886
#% 296159
#% 421044
#% 427199
#% 462218
#% 464831
#% 464859
#% 481428
#% 527178
#! This paper addresses the problem of finding the K closest pairs between two spatial data sets, where each set is stored in a structure belonging in the R-tree family. Five different algorithms (four recursive and one iterative) are presented for solving this problem. The case of 1 closest pair is treated as a special case. An extensive study, based on experiments performed with synthetic as well as with real point data sets, is presented. A wide range of values for the basic parameters affecting the performance of the algorithms, especially the effect of overlap between the two data sets, is explored. Moreover, an algorithmic as well as an experimental comparison with existing incremental algorithms addressing the same problem is presented. In most settings, the new algorithms proposed clearly outperform the existing ones.

#index 300163
#* Influence sets based on reverse nearest neighbor queries
#@ Flip Korn;S. Muthukrishnan
#t 2000
#c 5
#% 86950
#% 137887
#% 152937
#% 201876
#% 248804
#% 252304
#% 257472
#% 282516
#% 287466
#% 427199
#% 480093
#% 481947
#% 593937
#! Inherent in the operation of many decision support and continuous referral systems is the notion of the “influence” of a data point on the database. This notion arises in examples such as finding the set of customers affected by the opening of a new store outlet location, notifying the subset of subscribers to a digital library who will find a newly added document most relevant, etc. Standard approaches to determining the influence set of a data point involve range searching and nearest neighbor queries.In this paper, we formalize a novel notion of influence based on reverse neighbor queries and its variants. Since the nearest neighbor relation is not symmetric, the set of points that are closest to a query point (i.e., the nearest neighbors) differs from the set of points that have the query point as their nearest neighbor (called the reverse nearest neighbors). Influence sets based on reverse nearest neighbor (RNN) queries seem to capture the intuitive notion of influence from our motivating examples.We present a general approach for solving RNN queries and an efficient R-tree based method for large data sets, based on this approach. Although the RNN query appears to be natural, it has not been studied previously. RNN queries are of independent interest, and as such should be part of the suite of available queries for processing spatial and multimedia data. In our experiments with real geographical data, the proposed method appears to scale logarithmically, whereas straightforward sequential scan scales linearly. Our experimental study also shows that approaches based on range searching or nearest neighbors are ineffective at finding influence sets of our interest.

#index 300164
#* Towards self-tuning data placement in parallel database systems
#@ Mong Li Lee;Masaru Kitsuregawa;Beng Chin Ooi;Kian-Lee Tan;Anirban Mondal
#t 2000
#c 5
#% 116086
#% 152947
#% 172918
#% 210174
#% 210175
#% 238413
#% 261208
#% 299978
#% 340297
#% 347040
#% 376918
#% 382477
#% 439903
#% 462174
#% 463264
#% 479639
#% 571066
#! Parallel database systems are increasingly being deployed to support the performance demands of end-users. While declustering data across multiple nodes facilitates parallelism, initial data placement may not be optimal due to skewed workloads and changing access patterns. To prevent performance degradation, the placement of data must be reorganized, and this must be done on-line to minimize disruption to the system.In this paper, we consider a dynamic self-tuning approach to reorganization in a shared nothing system. We introduce a new index-based method that faciliates fast and efficient migration of data. Our solution incorporates a globally height-balanced structure and load tracking at different levels of granularity. We conducted an extensive performance study, and implemented the methods on the Fujitsu AP3000 machine. Both the simulation and empirical results demonstratic that our proposed method is indeed scalable and effective in correcting any deterioration in system throughput.

#index 300165
#* LH*RS: a high-availability scalable distributed data structure using Reed Solomon Codes
#@ Witold Litwin;Thomas Schwarz
#t 2000
#c 5
#% 132779
#% 152946
#% 172043
#% 172916
#% 183444
#% 188719
#% 213080
#% 237615
#% 238413
#% 252608
#% 261208
#% 382477
#% 458997
#% 462773
#% 591408
#% 591531
#% 614605
#! LH*RS is a new high-availability Scalable Distributed Data Structure (SDDS). The data storage scheme and the search performance of LH*RS are basically these of LH*. LH*RS manages in addition the parity information to tolerate the unavailability of k &gne; 1 server sites. The value of k scales with the file, to prevent the reliability decline. The parity calculus uses the Reed -Solomon Codes. The storage and access performance overheads to provide the high-availability are about the smallest possible. The scheme should prove attractive to data-intensive applications.

#index 300166
#* Efficient and extensible algorithms for multi query optimization
#@ Prasan Roy;S. Seshadri;S. Sudarshan;Siddhesh Bhobe
#t 2000
#c 5
#% 36117
#% 169337
#% 169844
#% 210208
#% 248787
#% 248807
#% 248811
#% 286991
#% 411750
#% 461897
#% 462025
#% 462204
#% 464056
#% 464706
#% 479956
#% 564419
#% 565457
#! Complex queries are becoming commonplace, with the growing use of decision support systems. These complex queries often have a lot of common sub-expressions, either within a single query, or across multiple such queries run as a batch. Multiquery optimization aims at exploiting common sub-expressions to reduce evaluation cost. Multi-query optimization has hither-to been viewed as impractical, since earlier algorithms were exhaustive, and explore a doubly exponential search space.In this paper we demonstrate that multi-query optimization using heuristics is practical, and provides significant benefits. We propose three cost-based heuristic algorithms: Volcano-SH and Volcano-RU, which are based on simple modifications to the Volcano search strategy, and a greedy heuristic. Our greedy heuristic incorporates novel optimizations that improve efficiency greatly. Our algorithms are designed to be easily added to existing optimizers. We present a performance study comparing the algorithms, using workloads consisting of queries from the TPC-D benchmark. The study shows that our algorithms provide significant benefits over traditional optimization, at a very acceptable overhead in optimization time.

#index 300167
#* Eddies: continuously adaptive query processing
#@ Ron Avnur;Joseph M. Hellerstein
#t 2000
#c 5
#% 554
#% 86929
#% 172900
#% 227914
#% 229827
#% 248793
#% 248795
#% 249985
#% 257655
#% 271664
#% 273910
#% 273911
#% 273912
#% 287664
#% 298597
#% 340305
#% 340635
#% 376266
#% 384911
#% 427195
#% 438135
#% 442700
#% 479452
#% 479938
#% 480120
#% 571088
#% 571294
#% 617867
#% 617870
#% 631918
#% 674743
#% 979675
#% 979712
#! In large federated and shared-nothing databases, resources can exhibit widely fluctuating characteristics. Assumptions made at the time a query is submitted will rarely hold throughout the duration of query processing. As a result, traditional static query optimization and execution techniques are ineffective in these environments.In this paper we introduce a query processing mechanism called an eddy, which continuously reorders operators in a query plan as it runs. We characterize the moments of symmetry during which pipelined joins can be easily reordered, and the synchronization barriers that require inputs from different sources to be coordinated. By combining eddies with appropriate join algorithms, we merge the optimization and execution phases of query processing, allowing each tuple to have a flexible ordering of the query operators. This flexibility is controlled by a combination of fluid dynamics and a simple learning algorithm. Our initial implementation demonstrates promising results, with eddies performing nearly as well as a static optimizer/executor in static scenarios, and providing dramatic improvements in dynamic execution environments.

#index 300168
#* A chase too far?
#@ Lucian Popa;Alin Deutsch;Arnaud Sahuguet;Val Tannen
#t 2000
#c 5
#% 18614
#% 69272
#% 86954
#% 116090
#% 145178
#% 427222
#% 462060
#% 464056
#% 479634
#% 479792
#% 479814
#% 562153
#% 564416
#% 564419
#% 571169
#% 1068338
#! In a previous paper we proposed a novel method for generating alternative query plans that uses chasing (and back-chasing) with logical constraints. The method brings together use of indexes, use of materialized views, semantic optimization and join elimination (minimization). Each of these techniques is known separately to be beneficial to query optimization. The novelty of our approach is in allowing these techniques to interact systematically, eg. non-trivial use of indexes and materialized views may be enabled only by semantic constraints.We have implemented our method for a variety of schemas and queries. We examine how far we can push the method in term of complexity of both schemas and queries. We propose a technique for reducing the size of the search space by “stratifying” the sets of constraints used in the (back)chase. The experimental results demonstrate that our method is practical (i.e., feasible and worthwhile).

#index 300169
#* WSQ/DSQ: a practical approach for combined querying of databases and the Web
#@ Roy Goldman;Jennifer Widom
#t 2000
#c 5
#% 77944
#% 86929
#% 136740
#% 198466
#% 201936
#% 210206
#% 248834
#% 273912
#% 281149
#% 387508
#% 442700
#% 459241
#% 461214
#% 464720
#% 479449
#% 479452
#% 479469
#% 481602
#% 481923
#% 979715
#! We present WSQ/DSQ (pronounced “wisk-disk”), a new approach for combining the query facilities of traditional databases with existing search engines on the Web. WSQ, for Web-Supported (Database) Queries, leverages results from Web searches to enhance SQL queries over a relational database. DSQ, for Database-Supported (Web) Queries, uses information stored in the database to enhance and explain Web searches. This paper focuses primarily on WSQ, describing a simple, low-overhead way to support WSQ in a relational DBMS, and demonstrating the utility of WSQ with a number of interesting queries and results. The queries supported by WSQ are enabled by two virtual tables, whose tuples represent Web search results generated dynamically during query execution. WSQ query execution may involve many high-latency calls to one or more search engines, during which the query processor is idle. We present a lightweight technique called asynchronous iteration that can be integrated easily into a standard sequential query processor to enable concurrency between query processing and multiple Web search requests. Asynchronous iteration has broader applications than WSQ alone, and it opens up many interesting query optimization issues. We have developed a prototype implementation of WSQ by extending a DBMS with virtual tables and asynchronous iteration; performance results are reported.

#index 300170
#* A framework for expressing and combining preferences
#@ Rakesh Agrawal;Edward L. Wimmers
#t 2000
#c 5
#% 220706
#% 248010
#% 406493
#% 564279
#% 656701
#! The advent of the World Wide Web has created an explosion in the available on-line information. As the range of potential choices expand, the time and effort required to sort through them also expands. We propose a formal framework for expressing and combining user preferences to address this problem. Preferences can be used to focus search queries and to order the search results. A preference is expressed by the user for an entity which is described by a set of named fields; each field can take on values from a certain type. The * symbol may be used to match any element of that type. A set of preferences can be combined using a generic combine operator which is instantiated with a value function, thus providing a great deal of flexibility. Same preferences can be combined in more than one way and a combination of preferences yields another preference thus providing the closure property. We demonstrate the power of our framework by illustrating how a currently popular personalization system and a real-life application can be realized as special cases of our framework. We also discuss implementation of the framework in a relational setting.

#index 300171
#* Microsoft TerraServer: a spatial data warehouse
#@ Tom Barclay;Jim Gray;Don Slutz
#t 2000
#c 5
#% 68091
#% 612073
#! Microsoft® TerraServer stores aerial, satellite, and topographic images of the earth in a SQL database available via the Internet. It is the world's largest online atlas, combining eight terabytes of image data from the United States Geological Survey (USGS) and SPIN-2. Internet browsers provide intuitive spatial and text interfaces to the data. Users need no special hardware, software, or knowledge to locate and browse imagery. This paper describes how terabytes of “Internet unfriendly” geo-spatial images were scrubbed and edited into hundreds of millions of “Internet friendly” image tiles and loaded into a SQL data warehouse. All meta-data and imagery are stored in the SQL database.TerraServer demonstrates that general-purpose relational database technology can manage large scale image repositories, and shows that web browsers can be a good geo-spatial image presentation system.

#index 300173
#* A data model and data structures for moving objects databases
#@ Luca Forlizzi;Ralf Hartmut Güting;Enrico Nardelli;Markus Schneider
#t 2000
#c 5
#% 172369
#% 248802
#% 260066
#% 315005
#% 421073
#% 435148
#% 464847
#% 503869
#% 503882
#% 527009
#% 527038
#% 527176
#% 618583
#% 641036
#! We consider spatio-temporal databases supporting spatial objects with continuously changing position and extent, termed moving objects databases. We formally define a data model for such databases that includes complex evolving spatial structures such as line networks or multi-component regions with holes. The data model is given as a collection of data types and operations which can be plugged as attribute types into any DBMS data model (e.g. relational, or object-oriented) to obtain a complete model and query language. A particular novel concept is the sliced representation which represents a temporal development as a set of units, where unit types for spatial and other data types represent certain “simple” functions of time. We also show how the model can be mapped into concrete physical data structures in a DBMS environment.

#index 300174
#* Indexing the positions of continuously moving objects
#@ Simonas Šaltenis;Christian S. Jensen;Scott T. Leutenegger;Mario A. Lopez
#t 2000
#c 5
#% 68091
#% 86950
#% 104346
#% 137887
#% 153260
#% 237205
#% 248028
#% 273706
#% 273714
#% 282343
#% 295512
#% 299979
#% 443181
#% 462218
#% 479796
#% 481599
#% 503869
#% 527176
#% 571296
#! The coming years will witness dramatic advances in wireless communications as well as positioning technologies. As a result, tracking the changing positions of objects capable of continuous movement is becoming increasingly feasible and necessary. The present paper proposes a novel, R*-tree based indexing technique that supports the efficient querying of the current and projected future positions of such moving objects. The technique is capable of indexing objects moving in one-, two-, and three-dimensional space. Update algorithms enable the index to accommodate a dynamic data set, where objects may appear and disappear, and where changes occur in the anticipated positions of existing objects. A comprehensive performance study is reported.

#index 300175
#* Adaptive multi-stage distance join processing
#@ Hyoseop Shin;Bongki Moon;Sukho Lee
#t 2000
#c 5
#% 2115
#% 86950
#% 86952
#% 152937
#% 172908
#% 172909
#% 198573
#% 201876
#% 210186
#% 210187
#% 227894
#% 248797
#% 248804
#% 427199
#% 462239
#% 479623
#% 479797
#% 479967
#% 481947
#% 481956
#% 527026
#! A spatial distance join is a relatively new type of operation introduced for spatial and multimedia database applications. Additional requirements for ranking and stopping cardinality are often combined with the spatial distance join in on-line query processing or internet search environments. These requirements pose new challenges as well as opportunities for more efficient processing of spatial distance join queries. In this paper, we first present an efficient k-distance join algorithm that uses spatial indexes such as R-trees. Bi-directional node expansion and plane-sweeping techniques are used for fast pruning of distant pairs, and the plane-sweeping is further optimized by novel strategies for selecting a sweeping axis and direction. Furthermore, we propose adaptive multi-stage algorithms for k-distance join and incremental distance join operations. Our performance study shows that the proposed adaptive multi-stage algorithms outperform previous work by up to an order of magnitude for both k-distance join and incremental distance join queries, under various operational conditions.

#index 300176
#* Finding replicated Web collections
#@ Junghoo Cho;Narayanan Shivakumar;Hector Garcia-Molina
#t 2000
#c 5
#% 70370
#% 204673
#% 232912
#% 255137
#% 266283
#% 281245
#% 406493
#% 479795
#% 616528
#! Many web documents (such as JAVA FAQs) are being replicated on the Internet. Often entire document collections (such as hyperlinked Linux manuals) are being replicated many times. In this paper, we make the case for identifying replicated documents and collections to improve web crawlers, archivers, and ranking functions used in search engines. The paper describes how to efficiently identify replicated documents and hyperlinked document collections. The challenge is to identify these replicas from an input data set of several tens of millions of web pages and several hundreds of gigabytes of textual data. We also present two real-life case studies where we used replication information to improve a crawler and a search engine. We report these results for a data set of 25 million web pages (about 150 gigabytes of HTML data) crawled from the web.

#index 300177
#* WebView materialization
#@ Alexandros Labrinidis;Nick Roussopoulos
#t 2000
#c 5
#% 241955
#% 245998
#% 248819
#% 261741
#% 273917
#% 275367
#% 279164
#% 281241
#% 284172
#% 308439
#% 458745
#% 458746
#% 464706
#% 479629
#% 479792
#% 479950
#% 479981
#% 504565
#% 978365
#% 1303525
#! A WebView is a web page automatically created from base data typically stored in a DBMS. Given the multi-tiered architecture behind database-backed web servers, we have the option of materializing a WebView inside the DBMS, at the web server, or not at all, always computing it on the fly (virtual). Since WebViews must be up to date, materialized WebViews are immediately refreshed with every update on the base data. In this paper we compare the three materialization policies (materialized inside the DBMS, materialized at the web server and virtual) analytically, through a detailed cost model, and quantitatively, through extensive experiments on an implemented system. Our results indicate that materializing at the web server is a more scalable solution and can facilitate an order of magnitude more users than the virtual and materialized inside the DBMS policies, even under high update workloads.

#index 300179
#* NiagaraCQ: a scalable continuous query system for Internet databases
#@ Jianjun Chen;David J. DeWitt;Feng Tian;Yuan Wang
#t 2000
#c 5
#% 58361
#% 86944
#% 86946
#% 116082
#% 248807
#% 443298
#% 479772
#% 480268
#% 480768
#% 481773
#% 631866
#% 631962
#! Continuous queries are persistent queries that allow users to receive new results when they become available. While continuous query systems can transform a passive web into an active environment, they need to be able to support millions of queries due to the scale of the Internet. No existing systems have achieved this level of scalability. NiagaraCQ addresses this problem by grouping continuous queries based on the observation that many web queries share similar structures. Grouped queries can share the common computation, tend to fit in memory and can reduce the I/O cost significantly. Furthermore, grouping on selection predicates can eliminate a large number of unnecessary query invocations. Our grouping technique is distinguished from previous group optimization approaches in the following ways. First, we use an incremental group optimization strategy with dynamic re-grouping. New queries are added to existing query groups, without having to regroup already installed queries. Second, we use a query-split scheme that requires minimal changes to a general-purpose query engine. Third, NiagaraCQ groups both change-based and timer-based queries in a uniform way. To insure that NiagaraCQ is scalable, we have also employed other techniques including incremental evaluation of continuous queries, use of both pull and push models for detecting heterogeneous data source changes, and memory caching. This paper presents the design of NiagaraCQ system and gives some experimental results on the system's performance and scalability.

#index 300180
#* The onion technique: indexing for linear optimization queries
#@ Yuan-Chi Chang;Lawrence Bergman;Vittorio Castelli;Chung-Sheng Li;Ming-Ling Lo;John R. Smith
#t 2000
#c 5
#% 2115
#% 86792
#% 127507
#% 137425
#% 213698
#% 237205
#% 248010
#% 248028
#% 248796
#! This paper describes the Onion technique, a special indexing structure for linear optimization queries. Linear optimization queries ask for top-N records subject to the maximization or minimization of linearly weighted sum of record attribute values. Such query appears in many applications employing linear models and is an effective way to summarize representative cases, such as the top-50 ranked colleges. The Onion indexing is based on a geometric property of convex hull, which guarantees that the optimal value can always be found at one or more of its vertices. The Onion indexing makes use of this property to construct convex hulls in layers with outer layers enclosing inner layers geometrically. A data record is indexed by its layer number or equivalently its depth in the layered convex hull. Queries with linear weightings issued at run time are evaluated from the outmost layer inwards. We show experimentally that the Onion indexing achieves orders of magnitude speedup against sequential linear scan when N is small compared to the cardinality of the set. The Onion technique also enables progressive retrieval, which processes and returns ranked results in a progressive manner. Furthermore, the proposed indexing can be extended into a hierarchical organization of data to accommodate both global and local queries.

#index 300181
#* On effective multi-dimensional indexing for strings
#@ H. V. Jagadish;Nick Koudas;Divesh Srivastava
#t 2000
#c 5
#% 68091
#% 86950
#% 115467
#% 143306
#% 203281
#% 232758
#% 252304
#% 252608
#% 271801
#% 273897
#% 282025
#% 286237
#% 287715
#% 288578
#% 289010
#% 386455
#% 411694
#% 427199
#% 479958
#% 480093
#% 481920
#! As databases have expanded in scope from storing purely business data to include XML documents, product catalogs, e-mail messages, and directory data, it has become increasingly important to search databases based on wild-card string matching: prefix matching, for example, is more common (and useful) than exact matching, for such data. In many cases, matches need to be on multiple attributes/dimensions, with correlations between the dimensions. Traditional multi-dimensional index structures, designed with (fixed length) numeric data in mind, are not suitable for matching unbounded length string data.In this paper, we describe a general technique for adapting a multi-dimensional index structure for wild-card indexing of unbounded length string data. The key ideas are (a) a carefully developed mapping function from strings to rational numbers, (b) representing an unbounded length string in an index leaf page by a fixed length offset to an external key, and (c) storing multiple elided tries, one per dimension, in an index page to prune search during traversal of index pages. These basic ideas affect all index algorithms. In this paper, we present efficient algorithms for different types of string matching.While our technique is applicable to a wide range of multi-dimensional index structures, we instantiate our generic techniques by adapting the 2-dimensional R-tree to string data. We demonstrate the space effectiveness and time benefits of using the string R-tree both analytically and experimentally.

#index 300182
#* Efficient and cost-effective techniques for browsing and indexing large video databases
#@ JungHwan Oh;Kien A. Hua
#t 2000
#c 5
#% 115462
#% 151305
#% 173678
#% 194185
#% 194209
#% 219839
#% 220109
#% 239587
#% 239697
#% 382491
#% 435932
#% 438054
#% 443245
#% 481272
#% 582005
#% 582008
#% 632189
#% 632385
#% 636411
#% 1180138
#! We present in this paper a fully automatic content-based approach to organizing and indexing video data. Our methodology involves three steps:Step 1: We segment each video into shots using a Camera-Tracking technique. This process also extracts the feature vector for each shot, which consists of two statistical variances VarBA and VarOA. These values capture how much things are changing in the background and foreground areas of the video shot.Step 2: For each video, We apply a fully automatic method to build a browsing hierarchy using the shots identified in Step 1.Step 3: Using the VarBA and VarOA values obtained in Step 1, we build an index table to support a variance-based video similarity model. That is, video scenes/shots are retrieved based on given values of VarBA and VarOA.The above three inter-related techniques offer an integrated framework for modeling, browsing, and searching large video databases. Our experimental results indicate that they have many advantages over existing methods.

#index 300183
#* Efficient algorithms for mining outliers from large data sets
#@ Sridhar Ramaswamy;Rajeev Rastogi;Kyuseok Shim
#t 2000
#c 5
#% 36672
#% 68091
#% 86950
#% 201876
#% 210173
#% 248790
#% 300136
#% 459025
#% 479640
#% 479791
#% 479986
#% 481281
#! In this paper, we propose a novel formulation for distance-based outliers that is based on the distance of a point from its kth nearest neighbor. We rank each point on the basis of its distance to its kth nearest neighbor and declare the top n points in this ranking to be outliers. In addition to developing relatively straightforward solutions to finding such outliers based on the classical nested-loop join and index join algorithms, we develop a highly efficient partition-based algorithm for mining outliers. This algorithm first partitions the input data set into disjoint subsets, and then prunes entire partitions as soon as it is determined that they cannot contain outliers. This results in substantial savings in computation. We present the results of an extensive experimental study on real-life and synthetic data sets. The results from a real-life NBA database highlight and reveal several expected and unexpected aspects of the database. The results from a study on synthetic data sets demonstrate that the partition-based algorithm scales well with respect to both data set size and data set dimensionality.

#index 300184
#* Privacy-preserving data mining
#@ Rakesh Agrawal;Ramakrishnan Srikant
#t 2000
#c 5
#% 149
#% 1868
#% 14277
#% 67453
#% 136350
#% 191910
#% 228355
#% 236410
#% 264246
#% 264267
#% 285061
#% 287297
#% 287298
#% 287794
#% 287795
#% 340475
#% 346931
#% 374401
#% 437974
#% 442709
#% 446277
#% 459008
#% 480940
#% 481945
#% 482049
#% 482071
#% 482095
#% 591487
#% 601649
#! A fruitful direction for future data mining research will be the development of techniques that incorporate privacy concerns. Specifically, we address the following question. Since the primary task in data mining is the development of models about aggregated data, can we develop accurate models without access to precise information in individual data records? We consider the concrete case of building a decision-tree classifier from training data in which the values of individual records have been perturbed. The resulting data records look very different from the original records and the distribution of data values is also very different from the original distribution. While it is not possible to accurately estimate original values in individual data records, we propose a novel reconstruction procedure to accurately estimate the distribution of original data values. By using these reconstructed distributions, we are able to build classifiers whose accuracy is comparable to the accuracy of classifiers built with the original data.

#index 300185
#* Designing and mining multi-terabyte astronomy archives: the Sloan Digital Sky Survey
#@ Alexander S. Szalay;Peter Z. Kunszt;Ani Thakar;Jim Gray;Don Slutz;Robert J. Brunner
#t 2000
#c 5
#% 68089
#% 68091
#% 82258
#% 115661
#% 136740
#% 152902
#% 171746
#% 201897
#% 271664
#% 610217
#! The next-generation astronomy digital archives will cover most of the sky at fine resolution in many wavelengths, from X-rays, through ultraviolet, optical, and infrared. The archives will be stored at diverse geographical locations. One of the first of these projects, the Sloan Digital Sky Survey (SDSS) is creating a 5-wavelength catalog over 10,000 square degrees of the sky (see http://www.sdss.org/). The 200 million objects in the multi-terabyte database will have mostly numerical attributes in a 100+ dimensional space. Points in this space have highly correlated distributions.The archive will enable astronomers to explore the data interactively. Data access will be aided by multidimensional spatial and attribute indices. The data will be partitioned in many ways. Small tag objects consisting of the most popular attributes will accelerate frequent searches. Splitting the data among multiple servers will allow parallel, scalable I/O and parallel data analysis. Hashing techniques will allow efficient clustering, and pair-wise comparison algorithms that should parallelize nicely. Randomly sampled subsets will allow de-bugging otherwise large queries at the desktop. Central servers will operate a data pump to support sweep searches touching most of the data. The anticipated queries will require special operators related to angular distances and complex similarity tests of object properties, like shapes, colors, velocity vectors, or temporal behaviors. These issues pose interesting data management challenges.

#index 300193
#* Approximating multi-dimensional aggregate range queries over real attributes
#@ Dimitrios Gunopulos;George Kollios;Vassilis J. Tsotras;Carlotta Domeniconi
#t 2000
#c 5
#% 43163
#% 77321
#% 82346
#% 116084
#% 210190
#% 213975
#% 227883
#% 248812
#% 248821
#% 248822
#% 259995
#% 273887
#% 273901
#% 273902
#% 273903
#% 273906
#% 273909
#% 280448
#% 282658
#% 411554
#% 479648
#% 479649
#% 479816
#% 479967
#% 479984
#% 480125
#% 482092
#% 482123
#% 504019
#% 617839
#! Finding approximate answers to multi-dimensional range queries over real valued attributes has significant applications in data exploration and database query optimization. In this paper we consider the following problem: given a table of d attributes whose domain is the real numbers, and a query that specifies a range in each dimension, find a good approximation of the number of records in the table that satisfy the query.We present a new histogram technique that is designed to approximate the density of multi-dimensional datasets with real attributes. Our technique finds buckets of variable size, and allows the buckets to overlap. Overlapping buckets allow more efficient approximation of the density. The size of the cells is based on the local density of the data. This technique leads to a faster and more compact approximation of the data distribution. We also show how to generalize kernel density estimators, and how to apply them on the multi-dimensional query approximation problem.Finally, we compare the accuracy of the proposed techniques with existing techniques using real and synthetic datasets.

#index 300194
#* Making B+- trees cache conscious in main memory
#@ Jun Rao;Kenneth A. Ross
#t 2000
#c 5
#% 59733
#% 122307
#% 238413
#% 252458
#% 275367
#% 317933
#% 318024
#% 443219
#% 479769
#% 479819
#% 479821
#% 480119
#% 566122
#! Previous research has shown that cache behavior is important for main memory index structures. Cache conscious index structures such as Cache Sensitive Search Trees (CSS-Trees) perform lookups much faster than binary search and T-Trees. However, CSS-Trees are designed for decision support workloads with relatively static data. Although B+-Trees are more cache conscious than binary search and T-Trees, their utilization of a cache line is low since half of the space is used to store child pointers. Nevertheless, for applications that require incremental updates, traditional B+-Trees perform well.Our goal is to make B+-Trees as cache conscious as CSS-Trees without increasing their update cost too much. We propose a new indexing technique called “Cache Sensitive B+-Trees” (CSB+-Trees). It is a variant of B+-Trees that stores all the child nodes of any given node contiguously, and keeps only the address of the first child in each node. The rest of the children can be found by adding an offset to that address. Since only one child pointer is stored explicitly, the utilization of a cache line is high. CSB+-Trees support incremental updates in a way similar to B+-Trees.We also introduce two variants of CSB+-Trees. Segmented CSB+-Trees divide the child nodes into segments. Nodes within the same segment are stored contiguously and only pointers to the beginning of each segment are stored explicitly in each node. Segmented CSB+-Trees can reduce the copying cost when there is a split since only one segment needs to be moved. Full CSB+-Trees preallocate space for the full node group and thus reduce the split cost. Our performance studies show that CSB+-Trees are useful for a wide range of applications.

#index 300195
#* Congressional samples for approximate answering of group-by queries
#@ Swarup Acharya;Phillip B. Gibbons;Viswanath Poosala
#t 2000
#c 5
#% 210190
#% 223781
#% 227883
#% 248812
#% 273902
#% 273908
#% 273909
#% 273910
#% 411554
#% 479804
#% 479984
#% 481288
#! In large data warehousing environments, it is often advantageous to provide fast, approximate answers to complex decision support queries using precomputed summary statistics, such as samples. Decision support queries routinely segment the data into groups and then aggregate the information in each group (group-by queries). Depending on the data, there can be a wide disparity between the number of data items in each group. As a result, approximate answers based on uniform random samples of the data can result in poor accuracy for groups with very few data items, since such groups will be represented in the sample by very few (often zero) tuples.In this paper, we propose a general class of techniques for obtaining fast, highly-accurate answers for group-by queries. These techniques rely on precomputed non-uniform (biased) samples of the data. In particular, we propose congressional samples, a hybrid union of uniform and biased samples. Given a fixed amount of space, congressional samples seek to maximize the accuracy for all possible group-by queries on a set of columns. We present a one pass algorithm for constructing a congressional sample and use this technique to also incrementally maintain the sample up-to-date without accessing the base relation. We also evaluate query rewriting strategies for providing approximate answers from congressional samples. Finally, we conduct an extensive set of experiments on the TPC-D database, which demonstrates the efficacy of the techniques proposed.

#index 300196
#* Counting, enumerating, and sampling of execution plans in a cost-based query optimizer
#@ Florian Waas;César Galindo-Legaria
#t 2000
#c 5
#% 32889
#% 83154
#% 102765
#% 147843
#% 152942
#% 464558
#% 479656
#% 482115
#% 565457
#! Testing an SQL database system by running large sets of deterministic or stochastic SQL statements is common practice in commercial database development. However, code defects often remain undetected as the query optimizer's choice of an execution plan is not only depending on the query but strongly influenced by a large number of parameters describing the database and the hardware environment. Modifying these parameters in order to steer the optimizer to select other plans is difficult since this means anticipating often complex search strategies implemented in the optimizer.In this paper we devise algorithms for counting, exhaustive generation, and uniform sampling of plans from the complete search space. Our techniques allow extensive validation of both generation of alternatives, and execution algorithms with plans other than the optimized one—if two candidate plans fail to produce the same results, then either the optimizer considered an invalid plan, or the execution code is faulty. When the space of alternatives becomes too large for exhaustive testing, which can occur even with a handful of joins, uniform random sampling provides a mechanism for unbiased testing.The technique is implemented in Microsoft's SQL Server, where it is an integral part of the validation and testing process.

#index 300197
#* Benchmarking queries over trees: learning the hard truth the hard way
#@ Fanny Wattez;Sophie Cluet;Véronique Benzaken;Guy Ferran;Christian Fiegel
#t 2000
#c 5
#% 86948
#% 235914
#% 248799
#% 264996
#% 365700

#index 300199
#* Maintenance of cube automatic summary tables
#@ Wolfgang Lehner;Richard Sidle;Hamid Pirahesh;Roberta Wolfgang Cochrane
#t 2000
#c 5
#% 210182
#% 227869
#% 300138
#! Materialized views (or Automatic Summary Tables—ASTs) are commonly used to improve the performance of aggregation queries by orders of magnitude. In contrast to regular tables, ASTs are synchronized by the database system. In this paper, we present techniques for maintaining cube ASTs. Our implementation is based on IBM DB2 UDB.

#index 300200
#* Challenges in automating declarative business rules to enable rapid business response
#@ Val Huber
#t 2000
#c 5

#index 300202
#* Expressing business rules
#@ Ronald G. Ross
#t 2000
#c 5
#% 208259
#% 252456
#% 356699
#! Point-and-Click Expression Builders, for instance limits and type consistency.Structured English, for more complex restrictions and logical inferences.Entity Life History or State Transition Diagrams, for both basic and more advanced state transition rules.Data Model or Class Model extensions, for basic property rules.No matter how the rules are captured, there should be a single, unified conceptual representation “inside” of the man-machine boundary. “Inside” here means transparent to the specifiers, but visible to analysis tools (e.g., for conflict analysis) and to rule engines or business logic servers (for run-time processing).Inside, there may be still other representations. For processing and performance reasons, there might be many physical representations of the rules, optimized for particular tools or hardware/software environments.The result is actually three layers of representation: external, conceptual, and internal. This is strongly reminiscent of the old ANSI/SPARC three-schema architecture for data. This should not be surprising since rules simply build on terms and facts, which can be ultimately represented by data. Where is this research now? A new, more concise representation scheme is under development. One focus of this scheme is a formal expression of how non-atomic rule types are derived from atomic ones. This would allow reduction of rules to a common base of fundamental rule types, in order to support automatic analysis of conflict and overlap in systematic fashion.This is opening exciting new avenues of research, and significant opportunities for those interested in getting involved.

#index 300204
#* Going beyond personalization: rule engines at work
#@ Eric Kintzer
#t 2000
#c 5

#index 300206
#* DLFM: a transactional resource manager
#@ Hui-I Hsiao;Inderpal Narang
#t 2000
#c 5
#% 117
#% 83183
#% 197715
#% 298601
#% 322880
#% 669894
#! The DataLinks technology developed at IBM Almaden Research Center and now available in DB2 UDB 5.2 introduces a new data type called DATALINK for a database to reference and manage files stored external to the database. An external file is put under a database control by “linking” the file to the database. Control to a file can also be removed by “unlinking” it. The technology provides transactional semantics with respect to linking or unlinking the file when DATALINK value is stored or updated. Further more, it provides the following set of properties: (1) managing access control to linked files, (2) enforcing referential integrity, such as referenced file cannot be deleted or renamed as long as it is referenced from the RDBMS, and (3) providing coordinated backup and recovery of RDBMS data with the file data.DataLinks File Manager (DLFM) is a key component of the DataLinks technology. DLFM is a sophisticated SQL application with a set of daemon processes residing at a file server node that work cooperatively with the host database server(s) to manage external files. To reduce the number of messages between database server and DLFM, DLFM maintains a set of meta data on the file system and the files that are under database control. One of the major decisions we made was to build DLFM on top of an existing database manager, such as DB2, instead of implementing a proprietary persistent data store. We have mixed feelings about using the RDBMS to build such a resource manager. One of the major challenges is to support transactional semantics for DLFM operations. To do this, we implemented the two-phase commit protocol in DLFM and designed an innovative scheme to enable rolling back transaction update after local database commit. Also a major gotchas is that the RDBMS' cost based optimizer generates the access plan, which does not take into account the locking costs of a concurrent workload. Using the RDBMS as a black box can cause “havoc” in terms of causing the lock timeouts and deadlocks and reducing the throughput of a concurrent workload. To solve the problem, we came up with a simple but effective way of influencing the optimizer to generate access plans matching the needs of DLFM implementation. Also several precautions had to be taken to ensure that lock escalation did not take place; next key locking was disabled to avoid deadlocks on heavily used indexes and SQL tables; and timeout mechanism was applied to break global deadlocks.We were able to run 100-client workload for 24 hours without much deadlock/timeout problem in system test. This paper describes the motivation for building the DLFM and the lessons that we have learned from this experience.

#index 300207
#* Online index rebuild
#@ Nagavamsi Ponnekanti;Hanuma Kodavalla
#t 2000
#c 5
#% 116087
#% 210174
#% 254177
#% 286929
#% 317933
#% 403195
#! In this paper we present an efficient method to do online rebuild of a B+-tree index. This method has been implemented in Sybase Adaptive Server Enterprise (ASE) Version 12.0. It provides high concurrency, does minimal amount of logging, has good performance and does not deadlock with other index operations. It copies the index rows to newly allocated pages in the key order so that good space utilization and clustering are achieved. The old pages are deallocated during the process. Our algorithm differs from the previously published online index rebuild algorithms in two ways. It rebuilds multiple leaf pages and then propagates the changes to higher levels. Also, while propagating the leaf level changes to higher levels, level 11 pages are reorganized, eliminating the need for a separate pass. Our performance study shows that our approach results in significant reduction in logging and CPU time. Also, our approach uses the same concurrency control mechanism as split and shrink operations, which made it attractive for implementation.

#index 300208
#* Indexing images in Oracle8i
#@ Melliyal Annamalai;Rajiv Chopra;Samuel DeFazio;Susan Mavris
#t 2000
#c 5
#% 169940
#% 227937
#% 228351
#% 248796
#% 437405
#% 464195
#! Content-based retrieval of images is the ability to retrieve images that are similar to a query image. Oracle8i Visual Information Retrieval provides this facility based on technology licensed from Virage, Inc. This product is built on top of Oracle8i interMedia which enables storage, retrieval and management of images, audios and videos. Images are matched using attributes such as color, texture and structure and efficient content-based retrieval is provided using indexes of an image index type. The design of the index type is based on a multi-level filtering algorithm. The filters reduce the search space so that the expensive comparison algorithm operates on a small subset of the data. Bitmap indexes are used to evaluate the first filter resulting in a design which performs well and is scalable. The image index type is built using Oracle8i extensible indexing technology, allowing users to create, use, and drop instances of this index type as they would any other standard index. In this paper we present an overview of the product, the design of the image index type, and some performance results of our product.

#index 300210
#* Handling very large databases with informix extended parallel server
#@ Andreas Weininger
#t 2000
#c 5
#! In this paper, we investigate which problems exist in very large real databases and describe which mechanisms are provided by Informix Extended Parallel Server (XPS) for dealing with these problems. Currently the largest customer XPS database contains 27 TB of data. A database server that has to handle such an amount of data has to provide mechanisms which allow achieving adequate performance and easing the usability. We will present mechanisms which address both of these issues and illustrate them with examples from real customer systems.

#index 300211
#* Internet traffic warehouse
#@ Chung-Min Chen;Munir Cochinwala;Claudio Petrone;Marc Pucci;Sunil Samtani;Patrizia Santa
#t 2000
#c 5
#% 208037
#% 632083
#! We report on a network traffic warehousing project at Telcordia. The warehouse supports a variety of applications that require access to Internet traffic data. The applications include Service Level Agreement (SLA), web traffic analysis, network capacity engineering and planning, and billing. We describe the design of the warehouse and the issues encountered in building the warehouse.

#index 300213
#* SQLEM: fast clustering in SQL using the EM algorithm
#@ Carlos Ordonez;Paul Cereghini
#t 2000
#c 5
#% 169358
#% 210173
#% 248792
#% 273891
#% 278011
#% 280402
#% 280521
#% 479962
#% 481281
#% 584926
#% 589738
#! Clustering is one of the most important tasks performed in Data Mining applications. This paper presents an efficient SQL implementation of the EM algorithm to perform clustering in very large databases. Our version can effectively handle high dimensional data, a high number of clusters and more importantly, a very large number of data records. We present three strategies to implement EM in SQL: horizontal, vertical and a hybrid one. We expect this work to be useful for data mining programmers and users who want to cluster large data sets inside a relational DBMS.

#index 300214
#* Anatomy of a real E-commerce system
#@ Anant Jhingran
#t 2000
#c 5
#! Today's E-Commerce systems are a complex assembly of databases, web servers, home grown glue code, and networking services for security and scalability. The trend is towards larger pieces of these coming together in bundled offerings from leading software vendors, and the networking/hardware being offered through service delivery companies. In this paper we examine the bundle by looking in detail at IBM's WebSphere, Commerce Edition, and its deployment at a major customer site.

#index 300216
#* From browsing to interacting: DBMS support for responsive websites
#@ Raghu Ramakrishnan
#t 2000
#c 5
#! Internet websites increasingly rely on database management systems. There are several reasons for this trend:As sites grow larger, managing the content becomes impossible without the use of a DBMS to keep track of the nature, origin, authorship, and modification history of each article.As sites become more interactive, tracking and logging user activity and user contributions creates valuable new data, which again is best managed using a DBMS. The emerging paradigm of Customer-Centric e-Business places a premium on engaging users, building a relationship with them across visits, and leveraging their expertise and feed-back. Supporting this paradigm means that we not only have to track what users visit on a site, we also have to enable them to offer opinions and contribute to the content of the website in various ways; naturally, this requires us to use a DBMS.In order to personalize a user's experience, a site must dynamically construct (or at least fine-tune) each page as it is delivered, taking into account information about the user's past activity and the nature of the content on the current page. In other words, personalization is made possible by utilizing the information (about content and user activity) that we already indicated is best managed using a DBMS.In summary, as websites go beyond a passive collection of pages to be browsed and seek to present users with a personalized, interactive experience, the role of database management systems becomes central.In this talk, I will present an overview of these issues, including a discussion of related techniques such as cookies and web server logs for tracking user activity.

#index 300218
#* Index research (panel session): forest or trees?
#@ Joseph M. Hellerstein;Hans-Peter Kriegel;David Comet;Christos Falsutsos;Raghu Ramakrishnan;Paul Brown
#t 2000
#c 5
#! Indexes and access methods have been a staple of database research — and indeed of computer science in general — for decades. A glance at the contents of this year's SIGMOD and PODS proceedings shows another bumper crop of indexing papers.Given the hundreds of indexing papers published in the database literature, a pause for reflection seems in order. From a scientific perspective, it is natural to ask why definitive indexing solutions have eluded us for so many years. What is the grand challenge in indexing? What basic complexities or intricacies underlie this large body of work? What would constitute a successful completion of this research agenda, and what steps will best move us in that direction? Or is it the case that the problem space branches in so many ways that we should expect to continuously need to solve variants of the indexing problem?From the practitioner's perspective, the proliferation of indexing solutions in the literature may be more confusing than helpful. Comprehensively evaluating the research to date is a near-impossible task. An evaluation has to include both functionality (applicability to the practitioner's problem, integration with other data management services like buffer management, query processing and transactions) as well as performance for the practitioner's workloads. Unfortunately, there are no standard benchmarks for advanced indexing problems, and there has been relatively little work on methodologies for index experimentation and customization. How should the research community promote technology transfer in this area? Are the new extensibility interfaces in object-relational DBMSs conducive to this effort?

#index 300219
#* Application architecture (panel session): 2Tier or 3Tier? What is DBMS's role?
#@ Anil K. Nori
#t 2000
#c 5
#! Experienced panelist will share their views on application architecture, specially, as it relates to database systems. The discussion will focus on what technologies and mechanisms are necessary for developing web applications, and where these mechanisms should reside.

#index 300221
#* Of XML and databases (panel session): where's the beef?
#@ Jennifer Widom;Adam Bosworth;Bruce Lindsay;Michael Stonebraker;Dan Suciu;Michael J. Carey
#t 2000
#c 5
#% 291299
#! This panel will examine the implications of the XML revolution, which is currently raging on the web, for database systems research and development.

#index 300222
#* Designing an ultra highly available DBMS (tutorial session)
#@ Svein Erik Bratsberg;Øystein Torbjørnsen
#t 2000
#c 5

#index 300224
#* Data management in eCommerce (tutorial session): the good, the bad, and the ugly
#@ Avigdor Gal
#t 2000
#c 5

#index 300225
#* Data access (tutorial session)
#@ José A. Blakeley;Anand Deshpande
#t 2000
#c 5
#! With an explosion of data on the web, consistent data access to diverse data sources has become a challenging task. In this tutorial will present topics of interest to database researchers and developers building: interoperable middle-ware, gateways, distributed heterogeneous query processors, federated databases, data source wrappers, mediators, and DBMS extensions.All of these require access to diverse information through common data access abstractions, powerful APIs, and common data exchange formats. With the emergence of the web, database applications are being run over the intranet and the extranet. This tutorial presents an overview of existing and emerging data access technologies. We will concentrate on some of the technical challenges that have to be addressed to enable uniform data access across various platforms and some of the issues that went into the design of these data access strategies.

#index 300227
#* LDAP directory services- just another database application? (tutorial session)
#@ Shridhar Shukla;Anand Deshpande
#t 2000
#c 5
#! The key driving force behind general-purpose enterprise directory services is for providing a central repository for commonly and widely used information such as users, groups, network service access information and profiles, security information, etc. Acceptance of the Lightweight Directory Access Protocol (LDAP) as an access protocol has facilitated widespread integration of these directory services into the network infrastructure and applications.Both directory and relational databases are data repositories sharing the characteristic that they have mechanisms for dealing with schema and structure of information and are suitable for systematically organized data. This tutorial describes characteristics of directories such as schema information, query language and support, storage mechanisms required, typical requirements imposed by applications, etc. We then explain the differences between a directory and relational database, and show how the two are required to co-exist in a typical enterprise.An essential characteristic assumed for information stored in directories is that it is relatively static and that the queries are mostly read only. We describe typical directory applications to validate this assumption and project the requirements imposed on them as these applications evolve. We then describe areas of overlap between traditional databases and directories, describe some database and directory integration solutions adopted in the market, and identify areas in which directory deployment can benefit from the experience gathered by the database community.

#index 300229
#* Research issues in moving objects databases (tutorial session)
#@ Ouri Wolfson
#t 2000
#c 5

#index 300237
#* Self-organizing data sharing communities with SAGRES
#@ Zachary Ives;Alon Levy;Jayant Madhavan;Rachel Pottinger;Stefan Saroiu;Igor Tatarinov;Shiori Betzler;Qiong Chen;Ewa Jaslikowska;Jing Su;Wai Tak Theodora Yeung
#t 2000
#c 5

#index 300239
#* &lgr;-DB: an ODMG-based object-oriented DBMS
#@ Leonidas Fegaras;Chandrasekhar Srinivasan;Arvind Rajendran;David Maier
#t 2000
#c 5
#% 172939
#% 201873
#% 235914
#% 248788
#! The &lgr;-DB project at the University of Texas at Arlington aims at developing frameworks and prototype systems that address the new query optimization challenges for object-oriented and object-relational databases, such as query nesting, multiple collection types, methods, and arbitrary nesting of collections. We have already developed a theoretical framework for query optimization based on an effective calculus, called the monoid comprehension calculus [4]. The system reported here is a fully operational ODMG 2.0 [2] OODB management system, based on this framework. Our system can handle most ODL declarations and can process most OQL query forms. &lgr;-DB is not ODMG compliant. Instead it supports its own C++ binding that provides a seamless integration between OQL and C++ with low impedance mismatch. It allows C++ variables to be used in queries and results of queries to be passed back to C++ programs. Programs expressed in our C++ binding are compiled by a preprocessor that performs query optimization at compile time, rather than run-time, as it is proposed by ODMG. In addition to compiled queries, &lgr;-DB provides an interpreter that evaluates ad-hoc OQL queries at run-time. The &lgr;-DB system architecture is shown in Figure 1. The &lgr;-DB evaluation engine is written in SDL (the SHORE Data Language) of the SHORE object management system [1], developed at the University of Wisconsin. ODL schemas are translated into SDL schemas in a straightforward way and are stored in the system catalog. The &lgr;-DB OQL compiler is a C++ preprocessor that accepts a language called &lgr;-OQL, which is C++ code with embedded DML commands to perform transactions, queries, updates, etc. The preprocessor translates &lgr;-OQL programs into C++ code that contains calls to the &lgr;-DB evaluation engine. We also provide a visual query formulation interface, called VOODOO, and a translator from visual queries to OQL text, which can be sent to the &lgr;-DB OQL interpreter for evaluation.Even though a lot of effort has been made to make the implementation of our system simple enough for other database researchers to use and extend, our system is quite sophisticated since it employs current state-of-the-art query optimization technologies as well as new advanced experimental optimization techniques which we have developed through the years, such as query unnesting [3]. The &lgr;-DB OODBMS is available as an open source software through the web at http://lambda.uta.edu/lambda-DB.html

#index 300240
#* An approximate search engine for structural databases
#@ Jason T. L. Wang;Xiong Wang;Dennis Shasha;Bruce A. Shapiro;Kaizhong Zhang;Qicheng Ma;Zasha Weinberg
#t 2000
#c 5
#! When a person interested in a topic enters a keyword into a Web search engine, the response is nearly instantaneous (and sometimes overwhelming). The impressive speed is due to clever inverted index structures, caching, and a domain-independent knowledge of strings. Our project seeks to construct algorithms, data structures, and software that approach the speed of keyword-based search engines for queries on structural databases.A structural database is one whose data objects include trees, graphs, or a set of interrelated labeled points in two, three, or higher dimensional space. Examples include databases holding (i) protein secondary and tertiary structure, (ii) phylogenetic trees, (iii) neuroanatomical networks, (iv) parse trees, (v) molecular diagrams, and (vi) XML documents. Comparison queries on such databases require solving variants of the graph isomorphism or subisomorphism problems (for which all known algorithms are exponential), so we have explored a large heuristic space.

#index 300241
#* SERFing the Web: web site management made easy
#@ Elke A. Rundensteiner;Kajal T. Claypool;Li Chen;Hong Su;Keiji Oenoki
#t 2000
#c 5

#index 300242
#* HOMER: a model-based CASE tool for data-intensive Web sites
#@ Paolo Merialdo;Paolo Atzeni;Marco Magnante;Giansalvatore Mecca;Marco Pecorone
#t 2000
#c 5
#% 458746
#% 479471

#index 300711
#* Information dependencies
#@ Mehmet M. Dalkilic;Edward L. Roberston
#t 2000
#c 5
#% 36683
#% 115210
#% 115608
#% 136350
#% 189872
#% 210183
#% 232106
#% 273916
#% 384978
#% 420053
#% 449588
#% 482095
#! This paper uses the tools of information theory to examine and reason about the information content of the attributes within a relation instance. For two sets of attributes X and Y, an information dependency measure (InD measure) characterizes the uncertainty remaining about the values for the set Y when the values for the set X are known. A variety of arithmetic inequalities (InD inequalities) are shown to hold among InD measures; InD inequalities hold in any relation instance. Numeric constraints (InD constraints) on InD measures, consistent with the InD inequalities, can be applied to relation instances. Remarkably, functional and multivalued dependencies correspond to setting certain constraints to zero, with Armstrong's axioms shown to be consequences of the arithmetic inequalities applied to constraints. As an analog of completeness, for any set of constraints consistent with the inequalities, we may construct a relation instance that approximates these constraints within any positive &egr;. InD measures suggest many valuable applications in areas such as data mining.

#index 301161
#* FACT: a learning based Web query processing system
#@ Songting Chen;Yanlei Diao;Hongjun Lu;Zengping Tian
#t 2000
#c 5
#% 244103
#! Though the query is posted in key words, the returned results contain exactly the information that the user is querying for, which may not be explicitly specified in the input query.The required information is often not contained in the Web pages whose URLs are returned by a search engine. FACT is capable of navigating in the neighborhood of these pages to find those that really contain the queried segments.The system does not require a prior knowledge about users such as user profiles [1] or preprocessing of Web pages such as wrapper generation [2].A prototype system has been implemented using the approach. It learns and applies two types of knowledge, navigation knowledge for following hyperlinks and classification knowledge for queried segment identification. For learning, it supports three training strategies, namely sequential training, random training and interleaved training. Yahoo! is currently the external search engine. The URLs of Web pages returned by the external search engine are used in processing. A set of experiments that are designed to evaluate the system, and compare different implementations, such as knowledge representations and training strategies.

#index 301163
#* javax.XXL: a prototype for a library of query processing algorithms
#@ Jochen van den Bercken;Jens-Peter Dittrich;Bernhard Seeger
#t 2000
#c 5
#% 172939
#% 296571
#% 442850
#% 481599
#% 632105
#! Therefore, index structures can easily be used in queries. A typical example is a join cursor which consumes the outputs of two underlying cursors. Most of our work is however not dedicated to the area of relational databases, but mainly refers to spatial and temporal data. For spatial databases, for example, we provide several implementations of spatial join algorithms [3]. The cursor-based processing is however the major advantage of XXL in contrast to approaches like LEDA [6] and TPIE [7]. For more information on XXL see http://www.mathematik.uni-marburg.de/DBS/xxl.We will demonstrate the latest version of XXL using examples to show its core functionality. We will concentrate on three key aspects of XXL.Usage: We show how easily state-of-the-art spatial join-algorithms can be implemented in XXL using data from different sources. Reuse: We will demonstrate how to support different joins, e.g. spatial and temporal joins, using the same generic algorithm like Plug&Join [1].Comparability: We will demonstrate how XXL serves as an ideal testbed to compare query processing algorithms and index structures.

#index 301165
#* i3: intelligent, interactive investigation of OLAP data cubes
#@ Sunita Sarawagi;Gayatri Sathe
#t 2000
#c 5
#% 479957
#! The goal of the i3(eye cube) project is to enhance multidimensional database products with a suite of advanced operators to automate data analysis tasks that are currently handled through manual exploration. Most OLAP products are rather simplistic and rely heavily on the user's intuition to manually drive the discovery process. Such ad hoc user-driven exploration gets tedious and error-prone as data dimensionality and size increases. We first investigated how and why analysts currently explore the data cube and then automated them using advanced operators that can be invoked interactively like existing simple operators.Our proposed suite of extensions appear in the form of a toolkit attached with a OLAP product. At this demo we will present three such operators: DIFF, RELAX and INFORM with illustrations from real-life datasets.

#index 301169
#* AJAX: an extensible data cleaning tool
#@ Helena Galhardas;Daniela Florescu;Dennis Shasha;Eric Simon
#t 2000
#c 5
#! @@@@ groups together matching pairs with a high similarity value by applying a given grouping criteria (e.g. by transitive closure). Finally, ging collapses each individual cluster into a tuple of the resulting data source. AJAX provides @@@@ for specifying data cleaning programs, which consists of SQL statements enriched with a set of specific primitives to express these transformations.AJAX also @@@@. It allows the user to interact with an executing data cleaning program to handle exceptional cases and to inspect intermediate results. Finally, AJAX provides @@@@ @@@@ that permits users to determine the source and processing of data for debugging purposes.We will present the AJAX system applied to two real world problems: the consolidation of a telecommunication database, and the conversion of a dirty database of bibliographic references into a set of clean, normalized, and redundancy free relational tables maintaining the same data.

#index 301171
#* Concept based design of data warehouses: the DWQ demonstrators
#@ M. Jarke;C. Quix;D. Calvanese;M. Lenzerini;E. Franconi;S. Ligoudistianos;P. Vassiliadis;Y. Vassiliou
#t 2000
#c 5
#% 482110
#% 503884
#! The ESPRIT Project DWQ (Foundations of Data Warehouse Quality) aimed at improving the quality of DW design and operation through systematic enrichment of the semantic foundations of data warehousing. Logic-based knowledge representation and reasoning techniques were developed to control accuracy, consistency, and completeness via advanced conceptual modeling techniques for source integration, data reconciliation, and multi-dimensional aggregation. This is complemented by quantitative optimization techniques for view materialization, optimizing timeliness and responsiveness without losing the semantic advantages from the conceptual approach. At the operational level, query rewriting and materialization refreshment algorithms exploit the knowledge developed at design time. The demonstration shows the interplay of these tools under a shared metadata repository, based on an example extracted from an application at Telecom Italia.

#index 301173
#* Towards data mining benchmarking: a test bed for performance study of frequent pattern mining
#@ Jian Pei;Runying Mao;Kan Hu;Hua Zhu
#t 2000
#c 5
#! Performance benchmarking has played an important role in the research and development in relational DBMS, object-relational DBMS, data warehouse systems, etc. We believe that benchmarking data mining algorithms is a long overdue task, and it will play an important role in the research and development of data mining systems as well.Frequent pattern mining forms a core component in mining associations, correlations, sequential patterns, partial periodicity, etc., which are of great potential value in applications. There have been a lot of methods proposed and developed for efficient frequent pattern mining in various kinds of databases, including transaction databases, time-series databases, etc. However, so far there is no serious performance benchmarking study of different frequent pattern mining methods.To facilitate an analytical comparison of different frequent mining methods, we have constructed an open test bed for performance study of a set of recently developed, popularly used methods for mining frequent patterns in transaction databases and mining sequential patterns in sequence databases, with different data characteristics. The testbed consists of the following components.A synthetic data generator, which can generate large sets of synthetic data in various kinds of data distributions. A few large data sets from real world applications will also be provided.A good set of typical frequent pattern mining methods, ranging from classical algorithms to recent studies. The method are grouped into three classes: frequent pattern mining, max-pattern mining, and sequential pattern mining. For frequent pattern mining, we will demonstrate Apriori, hashing, partitioning, sampling, TreeProjection, and FP-growth. For maximal pattern mining, we will demonstrate MaxMiner, TreeProjection, and FP-growth-max. For sequential pattern mining, we will demonstrate GSP and FreeSpan.A set of performance curves. These algorithms their running speeds, scalabilities, bottlenecks, and performance on different data distributions, will be compared and demonstrated upon request. Some performance curves from our pre-conference experimental evaluations will also be shown.An open testbed. Our goal is to construct an extensible test bed which integrates the above components and supports an open-ended testing service. Researchers can upload the object codes of their mining algorithms, and run them in the test bed using these data sets. The architecture is shown in Figure 1.This testbed is our first step towards benchmarking data mining algorithms. By doing so, performance of different algorithms can be reported consistently, on the same platform, and in the same environment. After the demo, we plan to make the testbed available on the WWW so that it may, hopefully, benefit further research and development of efficient data mining methods.

#index 301174
#* Image mining in IRIS: integrated retinal information system
#@ Wynne Hsu;Mong Li Lee;Kheng Guan Goh
#t 2000
#c 5
#! There is an increasing demand for systems that can automatically analyze images and extract semantically meaningful information. IRIS, an Integrated Retinal Information system, has been developed to provide medical professionals easy and unified access to the screening, trend and progression of diabetic-related eye diseases in a diabetic patient database. This paper shows how mining techniques can be used to accurately extract features in the retinal images. In particular, we apply a classification approach to determine the conditions for tortuousity in retinal blood vessels.

#index 301176
#* MOCHA: a database middleware system featuring automatic deployment of application-specific functionality
#@ Manuel Rodríguez-Martínez;Nick Roussopoulos;John M. McGann;Stephen Kelley;Vadim Katz;Zhexuan Song;Joseph JáJá
#t 2000
#c 5

#index 301178
#* A goal-driven auto-configuration tool for the distributed workflow management system mentorlite
#@ Michael Gillmann;Jeanine Weissenfels;German Shegalov;Wolfgang Wonner;Gerhard Weikum
#t 2000
#c 5
#! The Mentor-lite prototype has been developed within the research project “Architecture, Configuration, and Administration of Large Workflow Management Systems” funded by the German Science Foundation (DFG). It has evolved from its predecessor Mentor [1], but aims at a simpler architecture. The main goal of Mentor-lite has been to build a light-weight, extensible, and tailorable workflow management system (WFMS) with small footprint and easy-to-use administration capabilities. Our approach is to provide only kernel functionality inside the workflow engine, and consider system components like history management and worklist management as extensions on top of the kernel. The key point to retain the light-weight nature is that these extensions are implemented as workflows themselves.The workflow specifications are interpreted at runtime, which is a crucial prerequisite for flexible exception handling and dynamic modifications during runtime. The interpreter performs a stepwise execution of the workflow specification according to its formal semantics. For each step, the activities to be performed by the step are determined and started.Mentor-lite supports a protocol for distributed execution of workflows spread across multiple workflow engines. This support is crucial for workflows that span large, decentralized enterprises with largely autonomous organizational units or even cross multiple enterprises to form so-called “virtual enterprises”. A communication manager is responsible for sending and receiving synchronization messages between the engines. In order to guarantee a consistent global state even in the presence of site or network failures, we have built reliable message queues using the CORBA Object Transaction Services.For administration, Mentor-lite provides a Java-based workbench for workflow design, workflow partitioning across multiple workflow servers, and a Java-based runtime monitoring tool.

#index 301179
#* TIP: a temporal extension to Informix
#@ Jun Yang;Huacheng C. Ying;Jennifer Widom
#t 2000
#c 5
#% 198066
#% 225004
#% 319244
#% 361445
#% 459026
#% 481928
#% 631949
#! Commercial relational database systems today provide only limited temporal support. To address the needs of applications requiring rich temporal data and queries, we have built TIP (Temporal Information Processor), a temporal extension to the Informix database system based on its DataBlade technology. Our TIP DataBlade extends Informix with a rich set of datatypes and routines that facilitate temporal modeling and querying. TIP provides both C and Java libraries for client applications to access a TIP-enabled database, and provides end-users with a GUI interface for querying and browsing temporal data.

#index 301180
#* AQR-toolkit: an adaptive query routing middleware for distributed data intensive systems
#@ Ling Liu;Calton Pu;David Buttler;Wei Han;Henrique Paques;Wei Tang
#t 2000
#c 5
#! Query routing is an intelligent service that can direct query requests to appropriate servers that are capable of answering the queries. The goal of a query routing system is to provide efficient associative access to a large, heterogeneous, distributed collection of information providers by routing a user query to the most relevant information sources that can provide the best answer. Effective query routing not only minimizes the query response time and the overall processing cost, but also eliminates a lot of unnecessary communication overhead over the global networks and over the individual information sources.The AQR-Toolkit divides the query routing task into two cooperating processes: query refinement and source selection. It is well known that a broadly defined query inevitably produces many false positives. Query refinement provides mechanisms to help the user formulate queries that will return more useful results and that can be processed efficiently. As a complimentary process, source selection reduces false negatives by identifying and locating a set of relevant information providers from a large collection of available sources. By pruning irrelevant information sources, source selection also reduces the overhead of contacting the information servers that do not contribute to the answer of the query.The system architecture of AQR-Toolkit consists of a hierarchical network (a directed acyclic graph) with external information providers at the leaves and query routers as mediating nodes. The end-point information providers support query-based access to their documents. At a query router node, a user may browse and query the meta information about information providers registered at that query router or make use of the router's facilitates for query refinement and source selection.

#index 301184
#* SPIRE: a progressive content-based spatial image retrieval engine
#@ Chung-Sheng Li;Lawrence D. Bergman;Yuan-Chi Chang;Vittorio Castelli;John R. Smith
#t 2000
#c 5
#! In this demo, we will show the implementation of a content-based SPatial Image Retrieval Engine (SPIRE) for multimodal unstructured data. This architecture provides a framework for retrieving multi-modal data including image, image sequence, time series and parametric data from large archives. Dramatic speedup (from a factor of 4 to 35) has been achieved for many search operations such as template matching, texture feature extraction. This framework has been applied and validated in solar flares and petroleum exploration in which spatial and spatial-temporal phenomena are located.

#index 301188
#* Integrating replacement policies in StorM: an extensible approach
#@ Chong Leng Goh;Beng Chin Ooi;Stephane Bressan;Kian-Lee Tan
#t 2000
#c 5
#% 480967

#index 301190
#* DISIMA: a distributed and interoperable image database system
#@ Vincent Oria;M. Tamer Özsu;Paul J. Iglinski;Shu Lin;Bin Yao
#t 2000
#c 5
#% 1180249

#index 301191
#* The MLPQ/GIS constraint database system
#@ Peter Revesz;Rui Chen;Pradip Kanjamala;Yiming Li;Yuguo Liu;Yonghui Wang
#t 2000
#c 5
#% 248802
#% 274153
#! MLPQ/GIS [4,6] is a constraint database [5] system like CCUBE [1] and DEDALE [3] but with a special emphases on spatio-temporal data. Features include data entry tools (first four icons in Fig. 1), icon-based queries such as @@@@ Intersection, @@@@ Union, @@@@ Area, @@@@ Buffer, @@@@ Max and @@@@ Min, which optimize linear objective functions, and @@@@ for Datalog queries. For example, in Fig. 1 we loaded and displayed a constraint database that represents the midwest United States and loaded two contraint relations describing the movements of two persons. The query icon opened a dialog box into which we entered the query which finds (t, i) pairs such that the two people are in the same state i at the same time t.MLPQ/GIS can animate [2] spatio-temporal objects that are linear constraint relations over and t.Users can also display in discrete color zones (isometric maps) any spatially distributed variable z that is a linear function and For example, Fig. 2 shows the mean annual air temperature Nebraska. Animation and isometric map display can be combined.

#index 306033
#* Proceedings of the 2000 ACM SIGMOD international conference on Management of data
#@ Maggie Dunham;Jeffrey F. Naughton;Weidong Chen;Nick Koudas
#t 2000
#c 5

#index 308435
#* Evolution and change in data management — issues and directions
#@ John F. Roddick;Lina Al-Jadir;Leopoldo Bertossi;Marlon Dumas;Florida Estrella;Heidi Gregersen;Kathleen Hornsby;Jens Lufter;Federica Mandreoli;Tomi Männistö;Enric Mayol;Lex Wedemeijer
#t 2000
#c 5
#! One of the fundamental aspects of information and database systems is that they change. Moreover, in so doing they evolve, although the manner and quality of this evolution is highly dependent on the mechanisms in place to handle it. While changes in data are handled well, changes in other aspects, such as structure, rules, constraints, the model, etc., are handled to varying levels of sophistication and completeness.In order to study this in more detail a workshop on Evolution and Change in Data Management was held in Paris in November 1999. It brought together researchers from a wide range of disciplines with a common interest in handling the fundamental characteristics and the conceptual modelling of change in information and database systems. This short report of the workshop concentrates on some of the general lessons that emerged during the four days.

#index 308439
#* Generating dynamic content at database-backed web servers: cgi-bin vs. mod_perl
#@ Alexandros Labrinidis;Nick Roussopoulos
#t 2000
#c 5
#! Web servers are increasingly being used to deliver dynamic content rather than static HTML pages. In order to generate web pages dynamically, servers need to execute a script, which typically connects to a DBMS. Although CGI was the first approach at server side scripting, it has significant performance shortcomings. Currently, there are many alternative server side scripting architectures which offer better performance than CGI. In this paper, we report our experiences using mod_perl, an Apache Server module, which can improve the performance of CGI scripts by at least an order of magnitude. Except for presenting results from our experiments, we also briefly describe the implementation of an industrial strength database-backed web site that we recently built and give a quick overview of the various server-side scripting mechanisms.

#index 308444
#* Hierarchies and relative operators in the OLAP environment
#@ Elaheh Pourabbas;Maurizio Rafanelli
#t 2000
#c 5
#! In the last few years, numerous proposals for modelling and querying Multidimensional Databases (MDDB) are proposed. A rigorous classification of the different types of hierarchies is still an open problem. In this paper we propose and discuss some different types of hierarchies within a single dimension of a cube. These hierarchies divide in different levels of aggregation a single dimension. Depending on them, we discuss the characterization of some OLAP operators that refer to hierarchies in order to maintain the data cube consistency. Moreover, we propose a set of operators for changing the hierarchy structure. The issues discussed provide modelling flexibility during the scheme design phase and correct data analysis.

#index 308448
#* An optimisation scheme for coalesce/valid time selection operator sequences
#@ Costas Vassilakis
#t 2000
#c 5
#! Queries in temporal databases often employ the coalesce operator, either to coalesce results of projections, or data which are not coalesced upon storage. Therefore, the performance and the optimisation schemes utilised for this operator is of major importance for the performance of temporal DBMSs. Insofar, performance studies for various algorithms that implement this operator have been conducted, however, the joint optimisation of the coalesce operator with other algebraic operators that appear in the query execution plan has only received minimal attention. In this paper, we propose a scheme for combining the coalesce operator with selection operators which are applied to the valid time of the tuples produced from a coalescing operation. The proposed scheme aims at reducing the number of tuples that a coalescing operator must process, while at the same time allows the optimiser to exploit temporal indices on the valid time of the data.

#index 308452
#* Incremental maintenance of recursive views using relational calculus/SQL
#@ Guozhu Dong;Jianwen Su
#t 2000
#c 5
#! Views are a central component of both traditional database systems and new applications such as data warehouses. Very often the desired views (e.g. the transitive closure) cannot be defined in the standard language of the underlying database system. Fortunately, it is often possible to incrementally maintain these views using the standard language. For example, transitive closure of acyclic graphs, and of undirected graphs, can be maintained in relational calculus after both single edge insertions and deletions. Many such results have been published in the theoretical database community. The purpose of this survey is to make these useful results known to the wider database research and development community.There are many interesting issues involved in the maintenance of recursive views. A maintenance algorithm may be applicable to just one view, or to a class of views specified by a view definition language such as Datalog. The maintenance algorithm can be specified in a maintenance language of different expressiveness, such as the conjunctive queries, the relational calculus or SQL. Ideally, this maintenance language should be less expensive than the view definition language. The maintenance algorithm may allow updates of different kinds, such as just single tuple insertions, just single tuple deletions, special set-based insertions and/or deletions, or combinations thereof. The view maintenance algorithms may also need to maintain auxiliary relations to help maintain the views of interest. It is of interest to know the minimal arity necessary for these auxiliary relations and whether the auxiliary relations are deterministic. While many results are known about these issues for several settings, many further challenging research problems still remain to be solved.

#index 308458
#* An extensible compressor for XML data
#@ Hartmut Liefke;Dan Suciu
#t 2000
#c 5

#index 308460
#* SQL standardization: the next steps
#@ Andrew Eisenberg;Jim Melton
#t 2000
#c 5

#index 308461
#* Report on NGITS'99: The fourth international workshop on next generation information technologies and systems
#@ Opher Etzion
#t 1999
#c 5

#index 308463
#* Comparative analysis of five XML query languages
#@ Angela Bonifati;Stefano Ceri
#t 2000
#c 5
#! XML is becoming the most relevant new standard for data representation and exchange on the WWW. Novel languages for extracting and restructuring the XML content have been proposed, some in the tradition of database query languages (i.e. SQL, OQL), others more closely inspired by XML. No standard for XML query language has yet been decided, but the discussion is ongoing within the World Wide Web Consortium and within many academic institutions and Internet-related major companies. We present a comparison of five, representative query languages for XML, highlighting their common features and differences.

#index 308466
#* Design and management of data warehouses report on the DMDW'99 workshop
#@ Stella Gatziu;Manfred Jeusfeld;Martin Staudt;Yannis Vassiliou
#t 1999
#c 5

#index 308469
#* Beyond document similarity: understanding value-based search and browsing technologies
#@ Andreas Paepcke;Hector Garcia-Molina;Gerard Rodriguez-Mula;Junghoo Cho
#t 2000
#c 5
#! In the face of small, one or two word queries, high volumes of diverse documents on the Web are overwhelming search and ranking technologies that are based on document similarity measures. The increase of multimedia data within documents sharply exacerbates the shortcomings of these approaches. Recently, research prototypes and commercial experiments have added techniques that augment similarity-based search and ranking. These techniques rely on judgments about the 'value' of documents. Judgments are obtained directly from users, are derived by conjecture based on observations of user behavior, or are surmised from analyses of documents and collections. All these systems have been pursued independently, and no common understanding of the underlying processes has been presented. We survey existing value-based approaches, develop a reference architecture that helps compare the approaches, and categorize the constituent algorithms. We explain the options for collecting value metadata, and for using that metadata to improve search, ranking of results, and the enhancement of information browsing. Based on our survey and analysis, we then point to several open problems.

#index 308479
#* Timer-driven database triggers and alerters: semantics and a challenge
#@ Eric N. Hanson;Lloyd X. Noronha
#t 1999
#c 5
#! This paper proposes a simple model for a timer-driven triggering and alerting system. Such a system can be used with relational and object-relational databases systems. Timer-driven trigger systems have a number of advantages over traditional trigger systems that test trigger conditions and run trigger actions in response to update events. They are relatively easy to implement since they can be built using a middleware program that simply runs SQL statements against a DBMS. Also, they can check certain types of conditions, such as “a value did not change” or a “a value did not change by more than 10% in six months.” Such conditions may be of interest for a particular application, but cannot be checked correctly by an event-driven trigger system. Also, users may be perfectly happy being notified once a day, once a week, or even less often of certain conditions, depending on their application. Timer triggers are appropriate for these users. The semantics of timer triggers are defined here using a simple procedure. Timer triggers are meant to complement event-driven triggers, not replace them. We challenge the database research community to developed alternate algorithms and optimizations for processing timer triggers, provided that the semantics are the same as when using the simple procedure presented here.

#index 308483
#* Diluting ACID
#@ Tim Kempster;Colin Stirling;Peter Thanisch
#t 1999
#c 5
#! Several DBMS vendors have implemented the ANSI standard SQL isolation levels for transaction processing. This has created a gap between database practice and textbook accounts of transaction processing which simply equate isolation with serializability.We extend the notion of conflict to cover lower isolation levels and we present improved characterisations of classes of schedules achieving these levels.

#index 308490
#* Some remarks on variable independence, closure, and orthographic dimension in constraint databases
#@ Leonid Libkin
#t 1999
#c 5
#! The notion of variable independence was introduced by Chomicki, Goldin, and Kuper in their PODS'96 paper as a means of adding a limited form of aggregation to constraint query languages while retaining the closure property. Later, Grumbach, Rigoux and Segoufin showed in their ICDT'99 paper that variable independence and a related notion of orthographic dimension are useful tools for optimizing constraint queries.However, several results in those papers are incorrect as stated. As the notions of variable independence and orthographic dimension appear to be important for implementing constraint database prototypes, I explain in this short note the problems with the above mentioned papers and outline a solution for aggregate closure.

#index 308493
#* On views and XML
#@ Serge Abiteboul
#t 1999
#c 5

#index 308497
#* FinTime: a financial time series benchmark
#@ Kaippallimalil J. Jacob;Dennis Shasha
#t 1999
#c 5

#index 308500
#* Practical lessons in supporting large-scale computational science
#@ Ron Musick;Terence Critchlow
#t 1999
#c 5

#index 308504
#* SQLJ Part 1: SQL routines using the Java programming language
#@ Andrew Eisenberg;Jim Melton
#t 1999
#c 5

#index 308509
#* A survey of logical models for OLAP databases
#@ Panos Vassiliadis;Timos Sellis
#t 1999
#c 5
#! In this paper, we present different proposals for multidimensional data cubes, which are the basic logical model for OLAP applications. We have grouped the work in the field in two categories: commercial tools (presented along with terminology and standards) and academic efforts. We further divide the academic efforts in two subcategories: the relational model extensions and the cube-oriented approaches. Finally, we attempt a comparative analysis of the various efforts.

#index 321051
#* Path constraints in semistructured databases
#@ Peter Buneman;Wenfei Fan;Scott Weinstein
#t 2000
#c 5

#index 321052
#* Efficient searching with linear constraints
#@ Pankaj K. Agarwal;Lars Arge;Jeff Erickson;Paulo G. Franciosa;Jeffrey Scott Vitter
#t 2000
#c 5

#index 321053
#* Latent semantic indexing: a probabilistic analysis
#@ Christos H. Papadimitriou;Prabhakar Raghavan;Hisao Tamaki;Santosh Vempala
#t 2000
#c 5

#index 321054
#* Relational transducers for electronic commerce
#@ Serge Abiteboul;Victor Vianu;Brad Fordham;Yelena Yesha
#t 2000
#c 5

#index 321057
#* Querying spatial databases via topological invariants
#@ Luc Segoufin;Victor Vianu
#t 2000
#c 5

#index 321058
#* Conjunctive-query containment and constraint satisfaction
#@ Phokion G. Kolaitis;Moshe Y. Vardi
#t 2000
#c 5

#index 322368
#* Workshop on performance and architecture of web servers
#@ Krishna Kant;Prasant Mohapatra
#t 2000
#c 5

#index 322371
#* Report on ISDO '00: the CAiSE*00 workshop on “infrastructures for dynamic business-to-business service outsourcing”
#@ Heiko Ludwig;Paul Grefen
#t 2000
#c 5
#! The ISDO '00 workshop on "Infrastructures for Dynamic Business-to-Business Service Outsourcing" [1] was held as a preconference workshop of the 12th Conference on Advanced Information Systems Engineering (CAiSE *00) in Stockholm, Sweden, on June 5 and 6, 2000. C. Bussler (Net-fish Technologies), M. Bichler (Vienna University of Economics and Business Administration), and Y. Hoffner and H. Ludwig (IBM Zurich Research Laboratory) organised the workshop and chaired the program committee.The objective of the workshop was to provide a platform to discuss models and technologies for service outsourcing, with emphasis on the integration of the dynamic establishment, setup, and enactment of service relationships that connect the business processes of service provider and consumer businesses, thereby establishing virtual enterprises.Nowadays, many production companies integrate their procurement processes using online marketplaces and network-based supply chain management systems. However, this is not the case for the service industry. Whereas many service organisations have already automated their internal business process management (e.g. using workflow management systems or enterprise resource planning (ERP) systems), service marketplaces still remain an uncommon phenomenon. The reason for this is that the integration of service sales, service enactment, and customer interaction with the service process still appears to be highly difficult. This is particularly the case where complex services, such as insurance and complex logistics, involve considerable customer interaction.

#index 322373
#* Report on second international workshop on advanced issues of E-commerce and Web-based information systems
#@ Kun-Lung Wu;Philip S. Yu
#t 2000
#c 5
#! The Second International Workshop on Advanced Issues of E-Commerce and Web-Based Information Systems (WECWIS 2000) was held at the Crowne Plaza San Jose/Silicon Valley in Milpitas, California on June 8-9, 2000. The purpose of this workshop was to bring together leading practitioners, developers and researchers to explore the challenging technical issues and find feasible solutions for advancing the current state of the art in e-commerce and web-based information systems. In particular, the workshop was interested in the infrastructure issues to facilitate e-commerce and Web-based information systems.WECWIS 2000 was successful. There were three invited talks, one industrial panel discussion and six technical sessions. The keynote speech, "The global trading web: A strategic vision for the Internet economy," was delivered by Dr. Jay M. Tenebaum, VP and Chief Scientist, Commerce One, Inc., on June 8 immediately following the opening remarks by the conference chair. The banquet address, "Business issues in e-commerce," was delivered by Mr. Daniel Druker, General Manager, Hyperion e-Business Division. Finally, a lunch address, "B2C, B2B, N2N, N2M: Why 2 is so instrumental?" was delivered by Mr. Mstafa A. Syed, VP of Technology, VertialNet, Inc.The industrial panel was moderated by Dr. L. Mason and Dr. Z. Zhang, both of Blue Martini Software. The panelists included J. Becher, Accrue Software; L. Mellot, Business Objects; A. Srivastava, Blue Martini Software; and C. Zhou, IBM. The panel discussion topic was "Can e-business intelligence survive?" Among the many interesting issues being discussed were: Will privacy concerns stunt e-business intelligence utility? Will integrated e-commerce solutions be able to collect and analyze click steams, contents, products and sales data simultaneously? To what extent can out-of-the-box combined e-commerce and e-business intelligence solutions be useful? Is data mining useful in B2B e-commerce? Both positive and negative responses were hotly debated.There were a total of 30 papers included in the technical presentations, organized into six sessions. They were selected after rigorous reviews by the program committee members. The presented papers cover a wide range of topics, from framework, architecture and protocol issues of e-commerce to various types of e-services to web-based information systems for facilitating e-commerce. The rest of this report provides a brief summary of the technical presentations given in the workshop. The entire workshop proceedings is available from the IEEE Computer Society.

#index 322375
#* Provision of market services for eCo compliant electronic marketplaces
#@ Sena Arpinar;Asuman Dogac
#t 2000
#c 5
#! The progress and wider dissemination of electronic commerce will be facilitated through interoperability infrastructures. Commerce Net's eCo framework is a promising effort in this direction. In the eCo framework, businesses participate in a marketplace through standard interfaces for their services and by exchanging standardized documents. This framework does not specify any further interfaces for the services a marketplace itself may offer.In this paper, we demonstrate that a rich set of marketplace-specific services such as automated discovery of the needed services, comparison shopping, and negotiation can be offered to market participants by introducing a marketplace as an eCo business. For this purpose, a previously developed marketplace, namely MOPPET, is made eCo-compliant. We demonstrate that introducing MOPPET as an eCo business increases the functionality of the eCo market in the sense that several market specific services become available to the market participants.

#index 322378
#* Spatial operators
#@ Eliseo Clementini;Paolino Di Felice
#t 2000
#c 5
#! This paper discusses issues related to the integration of spatial operators into the new generation of SQL-like query languages. Starting from spatial data models, current spatial extensions of query languages are briefly reviewed and research directions are highlighted. A taxonomy of requirements to be satisfied by spatial operators is proposed with emphasis on users' needs and on the introduction of data uncertainty support. Further, spatial operators are classified into the three important categories of topological, projective, and metric operators and for each of them the state of the art is outlined.

#index 322381
#* Generating spatiotemporal datasets on the WWW
#@ Yannis Theodoridis;Mario A. Nascimento
#t 2000
#c 5
#! Efficient storage, indexing and retrieval of time-evolving spatial data are some of the tasks that a Spatiotemporal Database Management System (STDBMS) must support. Aiming at designers of indexing methods and access structures, in this article we review the GSTD algorithm for generating spatiotemporal datasets according to several user-defined parameters, and introduce a WWW-based environment for generating and visualizing such datasets. The GSTD interface is available at two main sites: http://www.cti.gr/RD3/GSTD/ and http://www.cs.ualberta.ca/~mn/GSTD/.

#index 322411
#* Constraint databases: a tutorial introduction
#@ Jan Van den Bussche
#t 2000
#c 5

#index 322412
#* The implementation and performance of compressed databases
#@ Till Westmann;Donald Kossmann;Sven Helmer;Guido Moerkotte
#t 2000
#c 5
#! In this paper, we show how compression can be integrated into a relational database system. Specifically, we describe how the storage manager, the query execution engine, and the query optimizer of a database system can be extended to deal with compressed data. Our main result is that compression can significantly improve the response time of queries if very light-weight compression techniques are used. We will present such light-weight compression techniques and give the results of running the TPC-D benchmark on a so compressed database and a non-compressed database using the AODB database system, an experimental database system that was developed at the Universities of Mannheim and Passau. Our benchmark results demonstrate that compression indeed offers high performance gains (up to 50%) for IO-intensive queries and moderate gains for CPU-intensive queries. Compression can, however, also increase the running time of certain update operations. In all, we recommend to extend today's database systems with light-weight compression techniques and to make extensive use of this feature.

#index 322413
#* Metadata standards for data warehousing: open information model vs. common warehouse metadata
#@ Thomas Vetterli;Anca Vaduva;Martin Staudt
#t 2000
#c 5
#! Metadata has been identified as a key success factor in data warehouse projects. It captures all kinds of information necessary to analyse, design, build, use, and interpret the data warehouse contents. In order to spread the use of metadata, enable the interoperability between repositories, and tool integration within data warehousing architectures, a standard for metadata representation and exchange is needed. This paper considers two standards and compares them according to specific areas of interest within data warehousing. Despite their incontestable similarities, there are significant differences between the two standards which would make their unification difficult.

#index 322415
#* Comparative analysis of six XML schema languages
#@ Dongwon Lee;Wesley W. Chu
#t 2000
#c 5
#! As XML [5] is emerging as the data format of the internet era, there is an substantial increase of the amount of data in XML format. To better describe such XML data structures and constraints, several XML schema languages have been proposed. In this paper, we present a comparative analysis of six noteworthy XML schema languages.

#index 322417
#* Knowledge discovery in data warehouses
#@ Themistoklis Palpanas
#t 2000
#c 5
#! As the size of data warehouses increase to several hundreds of gigabytes or terabytes, the need for methods and tools that will automate the process of knowledge extraction, or guide the user to subsets of the dataset that are of particular interest, is becoming prominent. In this survey paper we explore the problem of identifying and extracting interesting knowledge from large collections of data residing in data warehouses, by using data mining techniques. Such techniques have the ability to identify patterns and build succinct models to describe the data. These models can also be used to achieve summarization and approximation. We review the associated work in the OLAP, data mining, and approximate query answering literature. We discuss the need for the traditional data mining techniques to adapt, and accommodate the specific characteristics of OLAP systems. We also examine the notion of interestingness of data, as a tool to guide the analysis process. We describe methods that have been proposed in the literature for determining what is interesting to the user and what is not, and how these approaches can be incorporated in the data mining algorithms.

#index 328418
#* Report on the VLDB workshop on technologies for e-services (TES)
#@ Fabio Casati;Umesh Dayal;Ming-Chien Shan
#t 2000
#c 5

#index 328419
#* Research and practice in federated information systems
#@ W. Hasselbring;W.-J. van den Heuvel;G. J. Houben;R.-D. Kutsche;B. Rieger;M. Roantree;K. Subieta
#t 2000
#c 5
#% 244114
#% 298595
#! EFIS 2000 was held at Dublin City University in June 2000. The principal aim of this third workshop was to bring together new insights from academic research with industry-driven developments and perspectives in the area of federated information systems. This report describes the observations of the workshop together with the outcome and future research possibilities.

#index 328420
#* Using quantitative information for efficient association rule generation
#@ B. Pôssas;M. Carvalho;R. Resende;W. Meita, Jr.
#t 2000
#c 5
#% 152934
#% 201894
#% 210160
#% 227953
#% 232136
#% 280433
#% 280436
#% 280458
#% 321455
#% 452821
#% 481290
#% 481754
#% 631970
#% 1272179
#! The problem of mining association rules in categorical data presented in customer transactions was introduced by Agrawal, Imielinski and Swami [2]. This seminal work gave birth to several investigation efforts [4, 13] resulting in descriptions of how to extend the original concepts and how to increase the performance of the related algorithms.The original problem of mining association rules was formulated as how to find rules of the form set1 → set2. This rule is supposed to denote affinity or correlation among the two sets containing nominal or ordinal data items. More specifically, such an association rule should translate the following meaning: customers that buy the products in set1 also buy the products in set2. Statistical basis is represented in the form of minimum support and confidence measures of these rules with respect to the set of customer transactions.The original problem as proposed by Agrawal et al. [2] was extended in several directions such as adding or replacing the confidence and support by other measures, or filtering the rules during or after generation, or including quantitative attributes. Srikant e Agrawal [16] describe an new approach where quantitative data can be treated as categorical. This is very important since otherwise part of the customer transaction information is discarded. Whenever an extension is proposed it must be checked in terms of its performance. The algorithm efficiency is linked to the size of the database that is amenable to be treated. Therefore it is crucial to have efficient algorithms that enable us to examine and extract valuable decision-making information in the ever larger databases.In this paper we present an algorithm that can be used in the context of several of the extensions provided in the literature but at the same time preserves its performance, as demonstrated in a case study. The approach in our algorithm is to explore multidimensional properties of the data (provided such properties are present), allowing us to combine this additional information in a very efficient pruning phase. This results in a very flexible and efficient algorithm that was used with success in several experiments using categorical and quantitative databases.The paper is organized as follows. In the next section we describe the quantitative association rules and we present an algorithm to generate it. Section 3 presents an optimization of the pruning phase of the Apriori [4] algorithm based on quantitative information associated with the items. Section 4 presents our experimental results for mining four synthetic workloads, followed by some related work in Section 5. Finally we present some conclusions and future work in Section 6.

#index 328421
#* Object database evolution using separation of concerns
#@ Awais Rashid;Peter Sawyer
#t 2000
#c 5
#% 18606
#% 23960
#% 124639
#% 146201
#% 168251
#% 287616
#% 443145
#% 481768
#% 501937
#% 512521
#% 514397
#% 562687
#! This paper proposes an object database evolution approach based on separation of concerns. The lack of customisability and extensibility in existing evolution frameworks is a consequence of using attributes at the meta-object level to implement links among meta-objects and the injection of instance adaptation code directly into the class versions. The proposed approach uses dynamic relationships to separate the connection code from meta-objects and aspects - abstractions used by Aspect-Oriented Programming to localise cross-cutting concerns - to separate the instance adaptation code from class versions. The result is a customisable and extensible evolution framework with low maintenance overhead.

#index 328422
#* Cache invalidation scheme for mobile computing systems with real-time data
#@ Joe Chun-Hung Yuen;Edward Chan;Kam-Yiu Lam;H. W. Leung
#t 2000
#c 5
#% 172874
#% 201897
#% 462077
#% 464214
#% 610618
#% 615595
#! In this paper, we propose a cache invalidation scheme called Invalidation by Absolute Validity Interval (IAVI) for mobile computing systems. In IAVI, we define an absolute validity interval (AVI), for each data item based on its dynamic property such as the update interval. A mobile client can verify the validity of a cached item by comparing the last update time and its AVI. A cached item is invalidated if the current time is greater than the last update time plus its AVI. With this self-invalidation mechanism, the IAVI scheme uses the invalidation report to inform the mobile clients about changes in AVIs rather than the update event of the data items. As a result, the size of the invalidation report can be reduced significantly. Through extensive simulation experiments, we have found that the performance of the IVAI scheme is significantly better than other methods such as bit sequence and timestamp.

#index 328424
#* Theory of answering queries using views
#@ Alon Y. Halevy
#t 2000
#c 5
#% 663
#% 198465
#% 198466
#% 213982
#% 237189
#% 237190
#% 248038
#% 248800
#% 273696
#% 273698
#% 273700
#% 273924
#% 299945
#% 299967
#% 299968
#% 300138
#% 300237
#% 442977
#% 462214
#% 464727
#% 479792
#% 479950
#% 480149
#% 480158
#% 481916
#% 481923
#% 482116
#% 496091
#% 536422
#% 564416
#% 564419
#% 571169
#% 572311
#% 599549
#% 632039
#% 707146
#% 1499470
#! The problem of answering queries using views is to find efficient methods of answering a query using a set of previously materialized views over the database, rather than accessing the database relations. The problem has recently received significant attention because of its relevance to a wide variety of data management problems, such as query optimization, the maintenance of physical data independence, data integration and data warehousing. This article surveys the theoretical issues concerning the problem of answering queries using views.

#index 328427
#* Moving up the food chain: supporting e-commerce applications on databases
#@ Anant Jhingran
#t 2000
#c 5
#% 59697
#% 67453
#% 220706
#% 300184
#% 300214
#% 314933
#% 376266
#% 480134
#% 480293
#% 480317
#! Database systems have enjoyed a tremendous market because they have served many applications really well -- transaction processing in the beginning, and then decision support. Today, with over 200% cumulative growth rate in certain segments of E-Commerce, it is clear that this new class of applications will be a strong driver for databases to grow, commercially, as well as from a Research perspective. This paper outlines some of the issues that I have learnt in dealing with E-Commerce applications that may well be the focus of some of the research in database systems over the course of next few years.

#index 328429
#* A vision for management of complex models
#@ Phillip A. Bernstein;Alon Y. Halevy;Rachel A. Pottinger
#t 2000
#c 5
#% 11797
#% 13014
#% 32903
#% 62025
#% 77979
#% 84549
#% 88051
#% 90639
#% 139176
#% 172952
#% 173339
#% 188352
#% 189739
#% 210212
#% 227859
#% 227998
#% 245234
#% 260022
#% 278445
#% 281976
#% 287733
#% 294600
#% 308452
#% 442977
#% 479783
#% 480081
#% 480134
#% 480969
#% 572311
#% 641044
#% 1393677
#! Many problems encountered when building applications of database systems involve the manipulation of models. By "model," we mean a complex structure that represents a design artifact, such as a relational schema, object-oriented interface, UML model, XML DTD, web-site schema, semantic network, complex document, or software configuration. Many uses of models involve managing changes in models and transformations of data from one model into another. These uses require an explicit representation of "mappings" between models. We propose to make database systems easier to use for these applications by making "model" and "model mapping" first-class objects with special operations that simplify their use. We call this capability model management.In addition to making the case for model management, our main contribution is a sketch of a proposed data model. The data model consists of formal, object-oriented structures for representing models and model mappings, and of high-level algebraic operations on those structures, such as matching, differencing, merging, selection, inversion and instantiation. We focus on structure and semantics, not implementation.

#index 328431
#* New TPC benchmarks for decision support and web commerce
#@ Meikel Poess;Chris Floyd
#t 2000
#c 5
#! For as long as there have been DBMS's and applications that use them, there has been interest in the performance characteristics that these systems exhibit. This month's column describes some of the recent work that has taken place in TPC, the Transaction Processing Performance Council.TPC-A and TPC-B are obsolete benchmarks that you might have heard about in the past. TPC-C V3.5 is the current benchmark for OLTP systems. Introduced in 1992, it has been run on many hardware platforms and DBMS's. Indeed, the TPC web site currently lists 202 TPC-C benchmark results. Due to its maturity, TPC-C will not be discussed in this article.We've asked two very knowledgeable individuals to write this article. Meikel Poess is the chair of the TPC H and TPC-R Subcommittees and Chris Floyd is the chair of the TPC-W Subcommittee. We greatly appreciate their efforts.A wealth of information can be found at the TPC web site [ 1 ]. This information includes the benchmark specifications themselves, TPC membership information, and benchmark results.

#index 332089
#* EIHA?!?: deploying Web and WAP services using XML technology
#@ Chiara Biancheri;Jean-Christophe Pazzaglia;Gavino Paddeu
#t 2001
#c 5
#! The exponential growth of resources on the web, and the wide deployment of devices for multimodal access to the Internet, lead to new problems in information management. In this context, and as part of the European project Vision, we have built an interactive telematic handbook of the culture and the territory of Sardinia. A team of cultural experts browsed the web to get a large collection of Internet resources.The system built for the management of this data uses emerging Internet technologies such as the XML language suite and its applications. The result obtained is a multimodal service, called Eiha?!?, available through PCs and mobile phones.

#index 332094
#* Re-designing distance functions and distance-based applications for high dimensional data
#@ Charu C. Aggarwal
#t 2001
#c 5
#! In recent years, the detrimental effects of the curse of high dimensionality have been studied in great detail on several problems such as clustering, nearest neighbor search, and indexing. In high dimensional space the data becomes sparse, and traditional indexing and algorithmic techniques fail from the performance perspective. Recent research results show that in high dimensional space, the concept of proximity may not even be qualitatively meaningful [6]. In this paper, we try to outline the effects of generalizing low dimensional techniques to high dimensional applications and the natural effects of sparsity on distance based applications. We outline the guidelines required in order to re-design either the distance functions or the distance-based applications in a meaningful way for high dimensional domains. We provide novel perspectives and insights on some new lines of work for broadening application definitions in order to effectively deal with the dimensionality curse.

#index 332133
#* Querying multi-dimensional data indexed using the Hilbert space-filling curve
#@ J. K. Lawder;P. J. H. King
#t 2001
#c 5
#! Mapping to one-dimensional values and then using a one-dimensional indexing method has been proposed as a way of indexing multi-dimensional data. Most previous related work uses the Z-Order Curve but more recently the Hilbert Curve has been considered since it has superior clustering properties. Any approach, however, can only be of practical value if there are effective methods for executing range and partial match queries. This paper describes such a method for the Hilbert Curve.

#index 332135
#* Infosphere project: system support for information flow applications
#@ Calton Pu;Karsten Schwan;Jonathan Walpole
#t 2001
#c 5
#! We describe the Infosphere project, which is building the systems software support for information-driven applications such as digital libraries and electronic commerce. The main technical contribution is the Infopipe abstraction to support information flow with quality of service. Using building blocks such as program specialization, software feedback, domain-specific languages, and personalized information filtering, the Infopipe software generates code and manages resources to provide the specified quality of service with support for composition and restructuring.

#index 332138
#* Quality of service in multimedia digital libraries
#@ Elisa Bertino;Ahmed K. Elmagarmid;Mohand-Saïd Hacid
#t 2001
#c 5
#! There is currently considerable interest in developing multimedia digital libraries. However, it has become clear that existing architectures for management systems do not support the particular requirements of continuous media types. This is particularly the case in the important area of quality of service support. In this correspondence, we discuss quality of service issues within digital libraries and present a reference architecture able to support some quality aspects.

#index 332145
#* Towards knowledge-based digital libraries
#@ Ling Feng;Marfred A. Jeusfeld;Jeroen Hoppenbrouwers
#t 2001
#c 5
#! From the standpoint of satisfying human's information needs, the current digital library (DL) systems suffer from the following two shortcomings: (i) inadequate high-level cognition support; (ii) inadequate knowledge sharing facilities. In this article, we introduce a two-layered digital library architecture to support different levels of human cognitive acts. The model moves beyond simple information searching and browsing across multiple repositories, to inquiry of knowledge about the contents of digital libraries. To address users' high- order cognitive requests, we propose an information space consisting of a knowledge subspace and a document subspace. We extend the traditional indexing and searching schema of digital libraries from keyword-based to knowledge-based by adding knowledge to the documents into the DL information space. The distinguished features of such enhanced DL systems in comparison with the traditional knowledge-based systems are also discussed.

#index 332151
#* Constraints for semistructured data and XML
#@ Peter Buneman;Wenfei Fan;Jérôme Siméon;Scott Weinstein
#t 2001
#c 5
#! Integrity constraints play a fundamental role in database design. We review initial work on the expression of integrity constraints for semistructured data and XML.

#index 332153
#* Report on XEWA-00: the XML enabled wide-area searches for bioinformatics workshop
#@ Terence Critchlow
#t 2001
#c 5
#! The XEWA-00 workshop, held in December 2000 and sponsored by the IEEE Computer Society, was organized to bring together members of the bioinformatics community to determine if XML could simplify accessing large, heterogeneous, distributed collections of web-based data sources. The starting point for a series of breakout and group discussions was a proposed strawman of a grammar that described how to query a data source through its web interface. As a result of these discussions, the approach was validated, the strawman was refined, and several reference implementations are being generated as part of an ongoing effort. This article contains an overview of the workshop, including the proposed approach and a description of the strawman.

#index 332158
#* XML and information retrieval: a SIGIR 2000 workshop
#@ David Carmel;Yoelle Maarek;Aya Soffer
#t 2001
#c 5
#! XML - the eXtensible Markup Language has recently emerged as a new standard for data representation and exchange on the Interact. It is believed that it will become a universal format for data exchange on the Web and that in the near future we will find vast amounts of documents in XML format on the Web. As a result, it has become crucial to address the question of how large collections of XML documents can be sorted and retrieved efficiently and effectively.To date, most work on storing, indexing, querying, and searching documents in XML has stemmed from the database community's work on semi-structured data. An alternative approach, that has received less attention to date, is to view XML documents as a collection of text documents with additional tags and relations between these tags. IR techniques have traditionally been applied to search large sets of textual data and should thus be extended to encode the structure and semantics inherent in XML documents. Integrating IR and XML search techniques will enable more sophisticated search on the structure as well as the content of these documents, while leveraging the success of IR techniques in document similarity ranking and keyword search.The SIGIR workshop on XML and information retrieval was held July 28th, in Athens Greece. The goal of the workshop was to bring together researchers and practitioners interested in XML and IR to discuss and define the most relevant topics in the relation between these two technologies, present recent results, and propose future directions for research. The topics for discussion included:• How to extend IR technologies to search XML documents• How to integrate XML structure in IR indexing structures• How to query XML documents both on content and structure• How to introduce the semantics inherent in XML into the search process• How to adopt database indexing techniques in an IR frameworkThe opening session of the workshop consisted of a survey of search engines for XML documents. This was followed by three technical sessions: query languages, retrieval algorithms, and IR systems for XML documents. The final talk of the day, "Searching Annotated Language Resources in XML", by Nancy Ide was given from the perspective of potential users of XML search systems and opened many topics for discussion. The workshop was concluded with a panel discussion where the panelists outlined their vision of the future of XML search.

#index 332161
#* Report on the fourth international conference on flexible query answering systems (FQAS 2000)
#@ Patrick Bosc;Amihai Motro;Gabriella Pasi
#t 2001
#c 5
#! The fourth International Conference on Flexible Query Answering Systems (FQAS'2000) was held at the Academy of Sciences in Warsaw, Poland on October, 25-27, 2000. This series of conferences was launched in 1994 by Troels Andreasen, Henning Christiansen and Henrik Larsen from Roskilde University in Denmark, who have been the main driving force behind this series ever since. The previous FQAS events were held in Denmark in 1994, 1996, mad 1998. The next conference in this series will return to Denmark in 2002.This conference provides an opportunity for researchers, developers and practitioners to explore new ideas and approaches in a multidisciplinary forum to the aim of providing effective solutions to the problem of easy, flexible and intuitive access to electronically stored information. The focus of this conference is to add flexibility to the systems for the storage and access to information; as there exist several classes of such systems, the conference draws on several research areas, including information retrieval, database management, knowledge representation, multimedia systems and human-computer interaction.As a paradigm for flexible query answering we may think about an expert human intermediary who is able to analyze users information needs and to evaluate the relevant information items from the available information sources. The knowledge on the information sources and the capability to interpret the user requests enable the expert to perform a good estimate of the items possibly satisfying the users needs, though the query, per se, may be imprecise, incomplete, etc. Thus, one of the key issues for defining flexible query answering system is the tolerance to imprecision and uncertainty in the formulation of user queries as well as in the representation of information.The conference brought together more than 70 researchers and developers working in the area of information technology. The conference program included five invited talks, a keynote speech and 12 technical sessions. There was a total of 55 reviewed papers included in the technical sessions.In the proceedings, the contributions are gathered according to 6 main topics, each one corresponding to one or more technical sessions. These principal themes are the following: Flexibility in Database Management and Querying, Information Retrieval, Knowledge Representation and Reasoning, Natural Language and Multimedia Processing, Acquisition of Data and Knowledge, Cyberspace and distributed data.The rest of this report provides a brief summary of the three main thematic areas.

#index 332162
#* SQL and management of external data
#@ Jim Melton;Jan-Eike Michels;Vanja Josifovski;Krishna Kulkarni;Peter Schwarz;Kathy Zeidenstein
#t 2001
#c 5
#! In late 2000, work was completed on yet another part of the SQL standard [1], to which we introduced our readers in an earlier edition of this column [2].Although SQL database systems manage an enormous amount of data, it certainly has no monopoly on that task. Tremendous amounts of data remain in ordinary operating system files, in network and hierarchical databases, and in other repositories. The need to query and manipulate that data alongside SQL data continues to grow. Database system vendors have developed many approaches to providing such integrated access.In this (partly guested) article, SQL's new part, Management of External Data (SQL/MED), is explored to give readers a better notion of just how applications can use standard SQL to concurrently access their SQL data and their non-SQL data.

#index 332166
#* The Clio project: managing heterogeneity
#@ Renée J. Miller;Mauricio A. Hernández;Laura M. Haas;Lingling Yan;C. T. Howard Ho;Ronald Fagin;Lucian Popa
#t 2001
#c 5
#! Clio is a system for managing and facilitating the complex tasks of heterogeneous data transformation and integration. In Clio, we have collected together a powerful set of data management techniques that have proven invaluable in tackling these difficult problems. In this paper, we present the underlying themes of our approach and present a brief case study.

#index 333839
#* Proceedings of the twentieth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems
#@ Peter Buneman
#t 2001
#c 5

#index 333841
#* A Web Odyssey: from Codd to XML
#@ Victor Vianu
#t 2001
#c 5
#% 6787
#% 103705
#% 175464
#% 198466
#% 201976
#% 210214
#% 219492
#% 236416
#% 241160
#% 241166
#% 244109
#% 248011
#% 248024
#% 248038
#% 248799
#% 248819
#% 261370
#% 273686
#% 273688
#% 273700
#% 273701
#% 273927
#% 274160
#% 277343
#% 277344
#% 279367
#% 281149
#% 281935
#% 287037
#% 292677
#% 299941
#% 299942
#% 299943
#% 299944
#% 299967
#% 299971
#% 299976
#% 300143
#% 300179
#% 303887
#% 309493
#% 309851
#% 313640
#% 321054
#% 321058
#% 322415
#% 328424
#% 330627
#% 332151
#% 333855
#% 333857
#% 333858
#% 333982
#% 340295
#% 384978
#% 462062
#% 463919
#% 464720
#% 464724
#% 479465
#% 479971
#% 481602
#% 504443
#% 504575
#% 504578
#% 516520
#% 562141
#% 587566
#% 593696
#% 598376

#index 333843
#* Querying websites using compact skeletons
#@ Anand Rajaraman;Jeffrey D. Ullmann
#t 2001
#c 5
#% 172927
#% 210214
#% 229827
#% 237194
#% 240955
#% 244103
#% 248799
#% 248808
#% 248809
#% 300157
#% 368248
#% 459260
#% 462062
#% 462228
#% 464720
#% 464724
#% 479465
#% 479783
#% 479956
#% 481923
#% 504443
#% 511733
#% 535703
#! Several commercial applications, such as online comparison shopping and process automation, require integrating information that is scattered across multiple websites or XML documents. Much research has been devoted to this problem, resulting in several research prototypes and commercial implementations. Such systems rely on wrappers that provide relational or other structured interfaces to websites. Traditionally, wrappers have been constructed by hand on a per-website basis, constraining the scalability of the system.We introduce a website structure inference mechanism called compact skeletons that is a step in the direction of automated wrapper generation. Compact skeletons provide a transformation from websites or other hierarchical data, such as XML documents, to relational tables. We study several classes of compact skeletons and provide polynomial-time algorithms and heuristics for automated construction of compact skeletons from websites. Experimental results show that our heuristics work well in practice. We also argue that compact skeletons are a natural extension of commercially deployed techniques for wrapper construction.

#index 333844
#* Dynamically distributed query evaluation
#@ Trevor Jim;Dan Suciu
#t 2001
#c 5
#% 11797
#% 13025
#% 23902
#% 36683
#% 43200
#% 53391
#% 58344
#% 98469
#% 169844
#% 237190
#% 264263
#% 292120
#% 325745
#% 330305
#% 384978
#% 452753
#% 461214
#% 464555
#% 464717
#% 664709

#index 333845
#* Flexible queries over semistructured data
#@ Yaron Kanza;Yehoshua Sagiv
#t 2001
#c 5
#% 32904
#% 197751
#% 210214
#% 213983
#% 237191
#% 237192
#% 237193
#% 248024
#% 248852
#% 273703
#% 274160
#% 281149
#% 289321
#% 289425
#% 299976
#% 462212
#% 463919
#% 464720
#% 464724
#% 479465
#% 479471
#% 614598
#! Flexible queries facilitate, in a novel way, easy and concise querying of databases that have varying structures. Two different semantics, flexible and semiflexible, are introduced and investigated. The complexity of evaluating queries under the two semantics is analyzed. Query evaluation is polynomial in the size of the query, the database and the result in the following two cases. First, a semiflexible DAG query and a tree database. Second, a flexible tree query and a database that is any graph. Query containment and equivalence are also investigated. For the flexible semantics, query equivalence is always polynomial. For the semiflexible semantics, query equivalence is polynomial for DAG queries and exponential when the queries have cycles. Under the semiflexible and flexible semantics, two databases could be equivalent even when they are not isomorphic. Database equivalence is formally defined and characterized. The complexity of deciding equivalences among databases is analyzed. The implications of database equivalence on query evaluation are explained.

#index 333847
#* Multiobjective query optimization
#@ Christos H. Papadimitriou;Mihalis Yannakakis
#t 2001
#c 5
#% 34368
#% 191608
#% 416557
#% 481289
#% 571217
#% 593993
#% 656701
#! The optimization of queries in distributed database systems is known to be subject to delicate trade-offs. For example, the Mariposa database system allows users to specify a desired delay-cost tradeoff (that is, to supply a decreasing function u(d), specifying how much the user is willing to pay in order to receive the query results within time d); Mariposa divides a query graph into horizontal “strides,” analyzes each stride, and uses a greedy heuristic to find the “best” plan for all strides. We show that Mariposa's greedy heuristic can be arbitrarily far from the desired optimum. Applying a recent approach in multiobjective optimization algorithms to this problem, we show that the optimum cost-delay trade-off (Pareto) curve in Mariposa's framework can be approximated fast within any desired accuracy. We also present a polynomial algorithm for the general multiobjective query optimization problem, which approximates arbirarily well the optimum cost-delay tradeoff (without the restriction of Mariposa's heuristic stride subdivision).

#index 333848
#* Pipelining in multi-query optimization
#@ Nilesh N. Dalvi;Sumit K. Sanghai;Prasan Roy;S. Sudarshan
#t 2001
#c 5
#% 36117
#% 70370
#% 116041
#% 136740
#% 153041
#% 190153
#% 191608
#% 300166
#% 479646
#% 480268
#% 565457
#% 565469
#! Database systems frequently have to execute a set of related queries, which share several common subexpressions. Multi-query optimization exploits this, by finding evaluation plans that share common results. Current approaches to multi-query optimization assume that common subexpressions are materialized. Significant performance benefits can be had if common subexpressions are pipelined to their uses, without being materialized. However, plans with pipelining may not always be realizable with limited buffer space, as we show. We present a general model for schedules with pipelining, and present a necessary and sufficient condition for determining validity of a schedule under our model. We show that finding a valid schedule with minimum cost is NP-hard. We present a greedy heuristic for finding good schedules. Finally, we present a performance study that shows the benefit of our algorithms on batches of queries from the TPCD benchmark.

#index 333850
#* Optimization of sequence queries in database systems
#@ Reza Sadri;Carlo Zaniolo;Amir Zarkesh;Jafar Adibi
#t 2001
#c 5
#% 172949
#% 172950
#% 227951
#% 245996
#% 320454
#% 393792
#% 443061
#% 463903
#% 464058
#% 480144
#% 503878
#% 713238
#% 768815
#! The need to search for complex and recurring patterns in database sequences is shared by many applications. In this paper, we discuss how to express and support efficiently sophisticated sequential pattern queries in databases. Thus, we first introduce SQL-TS, an extension of SQL, to express these patterns, and then we study how to optimize search queries for this language. We take the optimal text search algorithm of Knuth, Morris and Pratt, and generalize it to handle complex queries on sequences. Our algorithm exploits the inter-dependencies between the elements of a sequential pattern to minimize repeated passes over the same data. Experimental results on typical sequence queries, such as double bottom queries, confirm that substantial speedups are achieved by our new optimization techniques.

#index 333851
#* The parameterized complexity of database queries
#@ Martin Grohe
#t 2001
#c 5
#% 38218
#% 93660
#% 101944
#% 139753
#% 182574
#% 190254
#% 237180
#% 248033
#% 250815
#% 273683
#% 333865
#% 338450
#% 343623
#% 414961
#% 464727
#% 540023
#% 593756
#% 598376
#% 599549
#% 1972413
#! This paper gives a short introduction into parameterized complexity theory, aimed towards database theorists interested in this area. The main results presented here classify the evaluation of first-order queries and conjunctive queries as hard parameterized problems.

#index 333853
#* Relaxed multi-way trees with group updates
#@ Kim S. Larsen
#t 2001
#c 5
#% 3623
#% 5381
#% 10112
#% 23890
#% 100604
#% 175742
#% 243173
#% 273715
#% 321250
#% 328892
#% 443093
#% 458517
#% 471317
#% 480415
#% 481337
#% 481628
#% 507729
#% 547800
#% 560660
#% 565688
#! Data structures with relaxed balance differ from standard structures in that rebalancing can be delayed and interspersed with updates. This gives extra flexibility in both sequential and parallel applications.We study the version of multi-way trees called (a,b)-trees (which includes B-trees) with the operations insertion, deletion, and group insertion. The latter has applications in for instance document databases and WWW search engines. We prove that we obtain the optimal asymptotic rebalancing complexities of amortized constant time for insertion and deletion and amortized logarithmic time in the size of the group for group insertion. These results hold even for the relaxed version.Our results also demonstrate that a binary tree scheme with the same complexities can be designed. This is an improvement over the existing results in the most interesting cases.

#index 333854
#* Optimal aggregation algorithms for middleware
#@ Ronald Fagin;Amnon Lotem;Moni Naor
#t 2001
#c 5
#% 1156
#% 67565
#% 190611
#% 217812
#% 261358
#% 278831
#% 290747
#% 480330
#% 591565
#% 614579
#% 631988
#% 659255
#! Assume that each object in a database has m grades, or scores, one for each of m attributes. For example, an object can have a color grade, that tells how red it is, and a shape grade, that tells how round it is. For each attribute, there is a sorted list, which lists each object and its grade under that attribute, sorted by grade (highest grade first). There is some monotone aggregation function, or combining rule, such as min or average, that combines the individual grades to obtain an overall grade.To determine objects that have the best overall grades, the naive algorithm must access every object in the database, to find its grade under each attribute. Fagin has given an algorithm (“Fagin's Algorithm”, or FA) that is much more efficient. For some distributions on grades, and for some monotone aggregation functions, FA is optimal in a high-probability sense.We analyze an elegant and remarkably simple algorithm (“the threshold algorithm”, or TA) that is optimal in a much stronger sense than FA. We show that TA is essentially optimal, not just for some monotone aggregation functions, but for all of them, and not just in a high-probability sense, but over every database. Unlike FA, which requires large buffers (whose size may grow unboundedly as the database size grows), TA requires only a small, constant-size buffer.We distinguish two types of access: sorted access (where the middleware system obtains the grade of an object in some sorted list by proceeding through the list sequentially from the top), and random access (where the middleware system requests the grade of object in a list, and obtains it in one step). We consider the scenarios where random access is either impossible, or expensive relative to sorted access, and provide algorithms that are essentially optimal for these cases as well.

#index 333855
#* On XML integrity constraints in the presence of DTDs
#@ Wenfei Fan;Leonid Libkin
#t 2001
#c 5
#% 36683
#% 69283
#% 123589
#% 186993
#% 237192
#% 248024
#% 273686
#% 273689
#% 274160
#% 289328
#% 299943
#% 299944
#% 330627
#% 384978
#% 408396
#% 712339
#! The paper investigates XML document specifications with DTDs and integrity constraints, such as keys and foreign keys. We study the consistency problem of checking whether a given specification is meaningful: that is, whether there exists an XML document that both conforms to the DTD and satisfies the constraints. We show that DTDs interact with constraints in a highly intricate way and as a result, the consistency problem in general is undecidable. When it comes to unary keys and foreign keys, the consistency problem is shown to be NP-complete. This is done by coding DTDs and integrity constraints with linear constraints on the integers. We consider the variations of the problem (by both restricting and enlarging the class of constraints), and identify a number of tractable cases, as well as a number of additional NP-complete ones. By incorporating negations of constraints, we establish complexity bounds on the implication problem, which is shown to be coNP-complete for unary keys and foreign keys.

#index 333856
#* Extended path expressions of XML
#@ Makoto Murata
#t 2001
#c 5
#% 138429
#% 197751
#% 237192
#% 241160
#% 241166
#% 248011
#% 248024
#% 273701
#% 291299
#% 299942
#% 299944
#% 299976
#% 320775
#% 339373
#% 462235
#% 478796
#% 516520
#! Query languages for XML often use path expressions to locate elements in XML documents. Path expressions are regular expressions such that underlying alphabets represent conditions on nodes. Path expressions represent conditions on paths from the root, but do not represent conditions on siblings, siblings of ancestors, and descendants of such siblings. In order to capture such conditions, we propose to extend underlying alphabets. Each symbol in an extended alphabet is a triplet (e1, a, e2), where a is a condition on nodes, and e1 (e2) is a condition on elder (resp. younger) siblings and their descendants; e1 and e2 are represented by hedge regular expressions, which are as expressive as hedge automata (hedges are ordered sequences of trees). Nodes matching such an extended path expression can be located by traversing the XML document twice. Furthermore, given an input schema and a query operation controlled by an extended path expression, it is possible to construct an output schema. This is done by identifying, where in the input schema the given extended path expression is satisfied.

#index 333857
#* XML with data values: typechecking revisited
#@ Noga Alon;Tova Milo;Frank Neven;Dan Suciu;Victor Vianu
#t 2001
#c 5
#% 16
#% 67851
#% 230142
#% 241166
#% 248799
#% 273702
#% 281149
#% 299942
#% 299944
#% 300143
#% 320775
#% 408396
#% 504578
#! We investigate the type checking problem for XML queries: statically verifying that every answer to a query conforms to a given output DTD, for inputs satisfying a given input DTD. This problem had been studied by a subset of the authors in a simplified framework that captured the structure of XML documents but ignored data values. We revisit here the type checking problem in the more realistic case when data values are present in documents and tested by queries. In this extended framework, type checking quickly becomes undecidable. However, it remains decidable for large classes of queries and DTDs of practical interest. The main contribution of the present paper is to trace a fairly tight boundary of decidability for type checking with data values. The complexity of type checking in the decidable cases is also considered.

#index 333858
#* Representing and querying XML with incomplete information
#@ Serge Abiteboul;Luc Segoufin;Victor Vianu
#t 2001
#c 5
#% 391
#% 663
#% 2984
#% 11817
#% 94459
#% 198465
#% 198466
#% 227997
#% 248038
#% 248799
#% 273703
#% 289321
#% 299942
#% 299944
#% 366807
#% 464056
#% 479621
#% 598376
#! We study the representation and querying of XML with incomplete information. We consider a simple model for XML data and their DTDs, a very simple query language, and a representation system for incomplete information in the spirit of the representations systems developed by Imielinski and Lipski for relational databases. In the scenario we consider, the incomplete information about an XML document is continuously enriched by successive queries to the document. We show that our representation system can represent partial information about the source document acquired by successive queries, and that it can be used to intelligently answer new queries. We also consider the impact on complexity of enriching our representation system or query language with additional features. The results suggest that our approach achieves a practically appealing balance between expressiveness and tractability. The research presented here was motivated by the Xyleme project at INRIA, whose objective it to develop a data warehouse for Web XML documents.

#index 333862
#* Querying partially sound and complete data sources
#@ Alberto O. Mendelzon;George A. Mihaila
#t 2001
#c 5
#% 47960
#% 198465
#% 242236
#% 248038
#% 265692
#% 384978
#% 406493
#% 481786
#% 482108
#! When gathering data from multiple data sources, users need uniform, transparent access to data. Also, when extracting data from several independent, often only partially sound and complete data sources, it is useful to present users with meta-information about the confidence in the answer to a query, based on the number and quality of the sources that participated in constructing the answer. We consider the problem of querying collections of sources with incomplete and partially sound data. We provide a method for checking the consistency of a source collection, we give a tableaux-based characterization for the set of possible worlds consistent with a given source collection and we propose a probabilistic semantics for query answers.

#index 333863
#* On computing functions with uncertainty
#@ Sanjeev Khanna;Wang-Chiew Tan
#t 2001
#c 5
#% 102802
#% 213981
#% 248010
#% 261358
#% 300134
#% 333969
#% 462222
#% 479816
#% 480332
#! We study the problem of computing a function f(x1,…, xn) given that the actual values of the variables xi's are known only with some uncertainty. For each variable xi, an interval Ii is known such that the value of xi is guaranteed to fall within this interval. Any such interval can be probed to obtain the actual value of the underlying variable; however, there is a cost associated with each such probe. The goal is to adaptively identify a minimum cost sequence of probes such that regardless of the actual values taken by the unprobed xi's, the value of the function f can be computed to within a specified precision.We design online algorithms for this problem when f is either the selection function or an aggregation function such as sum or average. We consider three natural models of precision and give algorithms for each model. We analyze our algorithms in the framework of competitive analysis and show that our algorithms are asymptotically optimal. Finally, we also study online algorithms for functions that are obtained by composing together selection and aggregation functions.

#index 333864
#* String operations in query languages
#@ Michael Benedikt;Leonid Libkin;Thomas Schwentick;Luc Segoufin
#t 2001
#c 5
#% 22959
#% 84275
#% 123111
#% 249228
#% 275304
#% 275308
#% 286504
#% 296535
#% 296540
#% 299977
#% 310832
#% 314839
#% 384978
#% 562296
#% 587518
#% 587563
#% 617174
#% 1782835
#! We study relational calculi with support for string operations. While SQL restricts the ability to mix string pattern-matching and relational operations, prior proposals for embedding SQL in a compositional calculus were based on adding the operation of concatenation to first-order logic. These latter proposals yield compositional query languages extending SQL, but are unfortunately computationally complete. The unbounded expressive power in turn implies strong limits on the ability to perform optimization and static analysis of properties such as query safety in these languages.In contrast, we look at compositional extensions of relational calculus that have nice expressiveness, decidability, and safety properties, while capturing string-matching queries used in SQL. We start with an extension based on the string ordering and LIKE predicates. This extension shares some of the attractive properties of relational calculus (e.g. effective syntax for safe queries, low data complexity), but lacks the full power of regular-expression pattern-matching. When we extend this basic model to include string length comparison, we get a natural string language with great expressiveness, but one which includes queries with high (albeit bounded) data complexity. We thus explore the space between these two languages. We consider two intermediate languages: the first extends our base language with functions that trim/add leading characters, and the other extends it by adding the full power of regular-expression pattern-matching. We show that both these extensions inherit many of the attractive properties of the basic model: they both have corresponding algebras expressing safe queries, and low complexity of query evaluation.

#index 333865
#* Robbers, marshals, and guards: game theoretic and logical characterizations of hypertree width
#@ Georg Gottlob;Nicola Leone;Francesco Scarcello
#t 2001
#c 5
#% 55926
#% 121602
#% 142111
#% 159244
#% 164325
#% 248038
#% 273683
#% 278054
#% 299969
#% 303884
#% 303886
#% 321058
#% 331899
#% 338450
#% 339937
#% 343623
#% 368248
#% 384978
#% 598376
#% 599549
#! In a previous paper [10], the authors introduced the notion of hypertree decomposition and the corresponding concept of hypertree width and showed that the conjunctive queries whose hypergraphs have bounded hypertree-width can be evaluated in polynomial time. Bounded hypertree-width generalizes the notions of acyclicity and bounded treewidth and corresponds to larger classes of tractable queries. In the present paper, we provide natural characterizations of hypergraphs and queries having bounded hypertree-width in terms of game-theory and logic.First we define the Robber and Marshals game, and prove that a hypergraph H has hypertree-width at most k iff k marshals have a winning strategy on H, allowing them to trap a robber who moves along the hyperedges. This game is akin the well-known Robber and Cops game (which characterizes bounded treewidth), except that marshals are more powerful than cops: They can control entire hyperedges instead of just vertices.Kolaitis and Vardi [17] recently gave an elegant characterization of the conjunctive queries having treewidth k in terms of the k-variable fragment of a certain logic L ( = existential-conjunctive fragment of positive FO). We use the Robber and Marshals game to derive a surprisingly simple and equally elegant characterization of the class HW[k] of queries of hypertree-width at most k in terms of guarded logic. In particular, we show that HW[k] = GFk (L), where GFk(L) denotes the k-guarded fragment of L. In this fragment, conjunctions of k atoms rather than just single atoms are allowed to act as guards. Note that, for the particular case k = 1, our results provide new characterizations of the class of acyclic queries.We extend the notion of bounded hypertreewidth to nonrecursive stratified datalog and show that the k-guarded fragment GFk(FO) of first order logic has the same expressive power as nonrecursive stratified datalog of hypertreewidth &lne; k.

#index 333866
#* On the complexity of join predicates
#@ Jin-Yi Cai;Venkatesan T. Chakaravarthy;Raghav Kaushik;Jeffrey F. Naughton
#t 2001
#c 5
#% 13041
#% 25998
#% 54199
#% 136740
#% 138493
#% 210187
#% 380546
#% 463595
#% 480463
#% 482121
#% 494780
#! We consider the complexity of join problems, focusing on equijoins, spatial-overlap joins, and set-containment joins. We use a graph pebbling model to characterize these joins combinatorially, by the length of their optimal pebbling strategies and computationally, by the complexity of discovering these strategies. Our results show that equijoins are the easiest of all joins, with optimal pebbling strategies that meet the lower bound over all join problems and that can be found in linear time. By contrast, spatial-overlap and set-containment joins are the hardest joins, with instances where optimal pebbling strategies reach the upper bound over all join problems and with the problem of discovering optimal pebbling strategies being NP-complete. For set-containment joins, we show that discovering the optimal pebbling is also MAX-SNP-Complete. As a consequence, we show that unless NP = P, there is a constant ∈o, such that this problem cannot be approximated within a factor of 1 + ∈&Ogr; in polynomial time. Our results shed some light on the difficulty the applied community has had in finding “good” algorithms for spatial-overlap and set-containment joins.

#index 333869
#* Equivalences among aggregate queries with negation
#@ Sara Cohen;Werner Nutt;Yehoshua Sagiv
#t 2001
#c 5
#% 36683
#% 123118
#% 137867
#% 190638
#% 198473
#% 248034
#% 273696
#% 273698
#% 287336
#% 289266
#% 481128
#% 587507
#% 599549
#% 1562065
#! Query equivalence is investigated for disjunctive aggregate queries with negated subgoals, constants and comparisons. A full characterization of equivalence is given for the aggregation functions count, max, sum, prod, top2 and parity. A related problem is that of determining, for a given natural number N, whether two given queries are equivalent over all databases with at most N constants. We call this problem bounded equivalence. A complete characterization of decidability of bounded equivalence is given. In particular, it is shown that this problem is decidable for all the above aggregation functions as well as for cntd (count distinct), stdev (standard deviation), median and avg. For quasilinear queries (i.e., queries in which predicates that occur positively are not repeated) it is shown that equivalence can be decided in polynomial time for the aggregation functions count, max, sum, prod, top2, parity, and avg. A similar result holds for cntd provided that a few additional conditions hold. The results are couched in terms of abstract characteristics of aggregation functions, and new proof techniques are used. Finally, the results presented also imply that equivalence, under bag-set semantics, is decidable for nonaggregate queries with negation.

#index 333872
#* Optimal and approximate computation of summary statistics for range aggregates
#@ Anna C. Gilbert;Yannis Kotidis;S. Muthukrishnan;Marin J. Strauss
#t 2001
#c 5
#% 28144
#% 54047
#% 116390
#% 168260
#% 227883
#% 248821
#% 248822
#% 273902
#% 274152
#% 299982
#% 299989
#% 427219
#% 479648
#% 481266
#! Fast estimates for aggregate queries are useful in database query optimization, approximate query answering and online query processing. Hence, there has been a lot of focus on “selectivity estimation”, that is, computing summary statistics on the underlying data and using that to answer aggregate queries fast and to a reasonable approximation. We present two sets of results for range aggregate queries, which are amongst the most common queries.First, we focus on a histogram as summary statistics and present algorithms for constructing histograms that are provably optimal (or provably approximate) for range queries; these algorithms take (pseudo-) polynomial time. These are the first known optimality or approximation results for arbitrary range queries; previously known results were optimal only for restricted range queries (such as equality queries, hierarchical or prefix range queries).Second, we focus on wavelet-based representations as summary statistics and present fast algorithms for picking wavelet statistics that are provably optimal for range queries. No previously-known wavelet-based methods have this property.We perform an experimental study of the various summary representations show the benefits of our algorithms over the known methods.

#index 333874
#* Efficient computation of temporal aggregates with range predicates
#@ Donhui Zhang;Alexander Markowetz;Vassilis Tsotras;Dimitrios Gunopulos;Bernhard Seeger
#t 2001
#c 5
#% 2115
#% 58371
#% 86950
#% 287070
#% 443181
#% 465010
#% 503375
#% 565462
#% 571296
#% 631922
#% 632071
#! A temporal aggregation query is an important but costly operation for applications that maintain time-evolving data (data warehouses, temporal databases, etc.). Due to the large volume of such data, performance improvements for temporal aggregation queries are critical. In this paper we examine techniques to compute temporal aggregates that include key-range predicates (range temporal aggregates). In particular we concentrate on SUM, COUNT and AVG aggregates. This problem is novel; to handle arbitrary key ranges, previous methods would need to keep a separate index for every possible key range. We propose an approach based on a new index structure called the Multiversion SB-Tree, which incorporates features from both the SB-Tree and the Multiversion B-Tree, to handle arbitrary key-range temporal SUM, COUNT and AVG queries. We analyze the performance of our approach and present experimental results that show its efficiency.

#index 333875
#* The challenges of delivering content on the Internet
#@ Tom Leighton
#t 2001
#c 5
#! In this talk, we will give an overview of how content is distributed on the internet, with an emphasis on the approach being used by Akamai. We will describe some of the technical challenges involved in operating a network of thousands of content servers across multiple geographies on behalf of thousands of customers. The talk will be introductory in nature and should be accessible to a broad audience.

#index 333876
#* On the design and quantification of privacy preserving data mining algorithms
#@ Dakshi Agrawal;Charu C. Aggarwal
#t 2001
#c 5
#% 1868
#% 264246
#% 264267
#% 285061
#% 300184
#% 365897
#% 840577
#! The increasing ability to track and collect large amounts of data with the use of current hardware technology has lead to an interest in the development of data mining algorithms which preserve user privacy. A recently proposed technique addresses the issue of privacy preservation by perturbing the data and reconstructing distributions at an aggregate level in order to perform the mining. This method is able to retain privacy while accessing the information implicit in the original attributes. The distribution reconstruction process naturally leads to some loss of information which is acceptable in many practical situations. This paper discusses an Expectation Maximization (EM) algorithm for distribution reconstruction which is more effective than the currently available method in terms of the level of information loss. Specifically, we prove that the EM algorithm converges to the maximum likelihood estimate of the original distribution based on the perturbed data. We show that when a large amount of data is available, the EM algorithm provides robust estimates of the original distribution. We propose metrics for quantification and measurement of privacy-preserving data mining algorithms. Thus, this paper provides the foundations for measurement of the effectiveness of privacy preserving data mining algorithms. Our privacy metrics illustrate some interesting results on the relative effectiveness of different perturbing distributions.

#index 333877
#* A condensed representation to find frequent patterns
#@ Artur Bykowski;Christophe Rigotti
#t 2001
#c 5
#% 232136
#% 248791
#% 279120
#% 300120
#% 329598
#% 420062
#% 481290
#% 481779
#! Given a large set of data, a common data mining problem is to extract the frequent patterns occurring in this set. The idea presented in this paper is to extract a condensed representation of the frequent patterns called disjunction-free sets, instead of extracting the whole frequent pattern collection. We show that this condensed representation can be used to regenerate all frequent patterns and their exact frequencies. Moreover, this regeneration can be performed without any access to the original data. Practical experiments show that this representation can be extracted very efficiently even in difficult cases. We compared it with another representation of frequent patterns previously investigated in the literature called frequent closed sets. In nearly all experiments we have run, the disjunction-free sets have been extracted much more efficiently than frequent closed sets.

#index 333881
#* Database-friendly random projections
#@ Dimitris Achlioptas
#t 2001
#c 5
#% 41374
#% 232764
#% 248027
#% 249321
#% 300121
#% 593787
#% 593926
#% 593928
#% 594029
#! A classic result of Johnson and Lindenstrauss asserts that any set of n points in d-dimensional Euclidean space can be embedded into k-dimensional Euclidean space where k is logarithmic in n and independent of d so that all pairwise distances are maintained within an arbitrarily small factor. All known constructions of such embeddings involve projecting the n points onto a random k-dimensional hyperplane. We give a novel construction of the embedding, suitable for database applications, which amounts to computing a simple aggregate over k random attribute partitions.

#index 333883
#* Two-dimensional substring indexing
#@ Paolo Ferragina;Nick Koudas;Divesh Srivastava;S. Muthukrishnan
#t 2001
#c 5
#% 68091
#% 86950
#% 88336
#% 115467
#% 143306
#% 189846
#% 203281
#% 252304
#% 271801
#% 273714
#% 282025
#% 287715
#% 288578
#% 289010
#% 300181
#% 341100
#% 411694
#% 427199
#% 480093
#% 481455
#% 481920
#! As databases have expanded in scope to storing string data (XML documents, product catalogs), it has become increasingly important to search databases based on matching substrings, often on multiple, correlated dimensions. While string B-trees are I/O optimal in one dimension, no index structure with non-trivial query bounds is known for two-dimensional substring indexing.In this paper, we present a technique for two-dimensional substring indexing based on a reduction to the geometric problem of identifying common colors in two ranges containing colored points. We develop an I/O efficient algorithm for solving the common colors problem, and use it to obtain an I/O efficient (poly-logarithmic query time) algorithm for the two-dimensional substring indexing problem. Our techniques result in a family of secondary memory index structures that trade space for time, with no loss of accuracy. We show how our technique can be practically realized using a combination of string B-trees and R-trees.

#index 333885
#* Process locking: a protocol based on ordered shared locks for the execution of transactional processes
#@ Heiko Schuldt
#t 2001
#c 5
#% 9241
#% 112319
#% 122904
#% 166215
#% 172880
#% 247424
#% 261267
#% 268752
#% 273711
#% 287303
#% 287470
#% 317988
#% 320902
#% 403195
#% 464211
#% 466948
#% 479477
#% 614655
#% 665528
#% 665581
#! In this paper, we propose process locking, a dynamic scheduling protocol based on ideas of ordered shared locks, that allows for the correct concurrent and fault-tolerant execution of transactional processes. Transactional processes are well defined, complex structured collections of transactional services. The process structure comprises flow of control between single process steps and also considers alternatives for failure handling purposes. Moreover, the individual steps of a process may have different termination characteristics, i.e., they cannot be compensated once they have committed. All these constraints have to be taken into consideration when deciding how to interleave processes. However, due to the higher level semantics of processes, standard locking techniques based on shared and exclusive locks on data objects cannot be applied. Yet, process locking addresses both atomicity and isolation simultaneously at the appropriate level, the scheduling of processes, and accounts for the various constraints imposed by processes. In addition, process locking aims at providing a high degree of concurrency while, at the same time, minimizing execution costs. This is done by allowing cascading aborts for rather simple processes white this is prevented for complex, long-running processes within the same framework.

#index 333924
#* Proceedings of the 2001 ACM SIGMOD international conference on Management of data
#@ Timos Sellis;Sharad Mehrotra
#t 2001
#c 5

#index 333925
#* Efficient computation of Iceberg cubes with complex measures
#@ Jiawei Han;Jian Pei;Guozhu Dong;Ke Wang
#t 2001
#c 5
#% 210182
#% 223781
#% 227880
#% 248785
#% 273899
#% 273916
#% 300120
#% 310558
#% 420053
#% 479450
#% 479795
#% 481290
#% 481951
#! It is often too expensive to compute and materialize a complete high-dimensional data cube. Computing an iceberg cube, which contains only aggregates above certain thresholds, is an effective way to derive nontrivial multi-dimensional aggregations for OLAP and data mining.In this paper, we study efficient methods for computing iceberg cubes with some popularly used complex measures, such as average, and develop a methodology that adopts a weaker but anti-monotonic condition for testing and pruning search space. In particular, for efficient computation of iceberg cubes with the average measure, we propose a top-k average pruning method and extend two previously studied methods, Apriori and BUC, to Top-k Apriori and Top-k BUC. To further improve the performance, an interesting hypertree structure, called H-tree, is designed and a new iceberg cubing method, called Top-k H-Cubing, is developed. Our performance study shows that Top-k BUC and Top-k H-Cubing are two promising candidates for scalable computation, and Top-k H-Cubing has better performance in most cases.

#index 333926
#* On computing correlated aggregates over continual data streams
#@ Johannes Gehrke;Flip Korn;Divesh Srivastava
#t 2001
#c 5
#% 227883
#% 248820
#% 256853
#% 273900
#% 273907
#% 273910
#% 278835
#% 282942
#% 300167
#% 302724
#% 310500
#% 465170
#% 480120
#% 480332
#% 482082
#% 482104
#% 482123
#% 593957
#% 594012
#% 632007
#! In many applications from telephone fraud detection to network management, data arrives in a stream, and there is a need to maintain a variety of statistical summary information about a large number of customers in an online fashion. At present, such applications maintain basic aggregates such as running extrema values (MIN, MAX), averages, standard deviations, etc., that can be computed over data streams with limited space in a straightforward way. However, many applications require knowledge of more complex aggregates relating different attributes, so-called correlated aggregates. As an example, one might be interested in computing the percentage of international phone calls that are longer than the average duration of a domestic phone call. Exact computation of this aggregate requires multiple passes over the data stream, which is infeasible.We propose single-pass techniques for approximate computation of correlated aggregates over both landmark and sliding window views of a data stream of tuples, using a very limited amount of space. We consider both the case where the independent aggregate (average duration in the example above) is an extrema value and the case where it is an average value, with any standard aggregate as the dependent aggregate; these can be used as building blocks for more sophisticated aggregates. We present an extensive experimental study based on some real and a wide variety of synthetic data sets to demonstrate the accuracy of our techniques. We show that this effectiveness is explained by the fact that our techniques exploit monotonicity and convergence properties of aggregates over data streams.

#index 333927
#* Iceberg-cube computation with PC clusters
#@ Raymond T. Ng;Alan Wagner;Yu Yin
#t 2001
#c 5
#% 210182
#% 227880
#% 227883
#% 248785
#% 273916
#% 420069
#% 420096
#% 434348
#% 462204
#% 464215
#% 479450
#% 479476
#% 479646
#% 479795
#% 479952
#% 479957
#% 481951
#! In this paper, we investigate the approach of using low cost PC cluster to parallelize the computation of iceberg-cube queries. We concentrate on techniques directed towards online querying of large, high-dimensional datasets where it is assumed that the total cube has net been precomputed. The algorithmic space we explore considers trade-offs between parallelism, computation and I/0. Our main contribution is the development and a comprehensive evaluation of various novel, parallel algorithms. Specifically: (1) Algorithm RP is a straightforward parallel version of BUC [BR99]; (2) Algorithm BPP attempts to reduce I/0 by outputting results in a more efficient way; (3) Algorithm ASL, which maintains cells in a cuboid in a skiplist, is designed to put the utmost priority on load balancing; and (4) alternatively, Algorithm PT load-balances by using binary partitioning to divide the cube lattice as evenly as possible.We present a thorough performance evaluation on all these algorithms on a variety of parameters, including the dimensionality of the cube, the sparseness of the cube, the selectivity of the constraints, the number of processors, and the size of the dataset. A key finding is that it is not a one-algorithm-fit-all situation. We recommend a “recipe” which uses PT as the default algorithm, but may also deploy ASL under specific circumstances.

#index 333929
#* Outlier detection for high dimensional data
#@ Charu C. Aggarwal;Philip S. Yu
#t 2001
#c 5
#% 114994
#% 152934
#% 210173
#% 248790
#% 248792
#% 273891
#% 300131
#% 300136
#% 300183
#% 332094
#% 369236
#% 459025
#% 479791
#% 479986
#% 480132
#% 480307
#% 481281
#% 686757
#! The outlier detection problem has important applications in the field of fraud detection, network robustness analysis, and intrusion detection. Most such applications are high dimensional domains in which the data can contain hundreds of dimensions. Many recent algorithms use concepts of proximity in order to find outliers based on their relationship to the rest of the data. However, in high dimensional space, the data is sparse and the notion of proximity fails to retain its meaningfulness. In fact, the sparsity of high dimensional data implies that every point is an almost equally good outlier from the perspective of proximity-based definitions. Consequently, for high dimensional data, the notion of finding meaningful outliers becomes substantially more complex and non-obvious. In this paper, we discuss new techniques for outlier detection which find the outliers by studying the behavior of projections from the data set.

#index 333930
#* Bit-sliced index arithmetic
#@ Denis Rinfret;Patrick O'Neil;Elizabeth O'Neil
#t 2001
#c 5
#% 67565
#% 90786
#% 185255
#% 201869
#% 213786
#% 227861
#% 248814
#% 253191
#% 273904
#% 273905
#% 290703
#% 292684
#% 301523
#% 462217
#% 466953
#% 580219
#! The bit-sliced index (BSI) was originally defined in [ONQ97]. The current paper introduces the concept of BSI arithmetic. For any two BSI's X and Y on a table T, we show how to efficiently generate new BSI's Z, V, and W, such that Z = X + Y, V = X - Y, and W = MIN(X, Y); this means that if a row r in T has a value x represented in BSI X and a value y in BSI Y, the value for r in BSI Z will be x + y, the value in V will be x - y and the value in W will be MIN(x, y). Since a bitmap representing a set of rows is the simplest bit-sliced index, BSI arithmetic is the most straightforward way to determine multisets of rows (with duplicates) resulting from the SQL clauses UNION ALL (addition), EXCEPT ALL (subtraction), and INTERSECT ALL (min) (see [OO00, DB2SQL] for definitions of these clauses). Another contribution of the current paper is to generalize BSI range restrictions from [ONQ97] to a new non-Boolean form: to determine the top k BSI-valued rows, for ally meaningful value k between one and the total number of rows in T. Together with bit-sliced addition, this permits us to solve a common basic problem of text retrieval: given an object-relational table T of rows representing documents, with a collection type column K representing keyword terms, we demonstrate an efficient algorithm to find k documents that share the largest number of terms with some query list Q of terms. A great deal of published work on such problems exists in the Information Retrieval (IR) field. The algorithm we introduce, which we call Bit-Sliced Term-Matching, or BSTM, uses an approach comparable in performance to the most efficient known IR algorithm, a major improvement on current DBMS text searching algorithms, with the advantage that it uses only indexing we propose for native database operations.

#index 333931
#* Space-efficient online computation of quantile summaries
#@ Michael Greenwald;Sanjeev Khanna
#t 2001
#c 5
#% 2152
#% 210190
#% 220551
#% 248820
#% 248821
#% 273907
#% 482104
#% 482123
#! An ∈-approximate quantile summary of a sequence of N elements is a data structure that can answer quantile queries about the sequence to within a precision of ∈N.We present a new online algorithm for computing∈-approximate quantile summaries of very large data sequences. The algorithm has a worst-case space requirement of &Ogr;(1÷∈ log(∈N)). This improves upon the previous best result of &Ogr;(1÷∈ log2(∈N)). Moreover, in contrast to earlier deterministic algorithms, our algorithm does not require a priori knowledge of the length of the input sequence.Finally, the actual space bounds obtained on experimental data are significantly better than the worst case guarantees of our algorithm as well as the observed space requirements of earlier algorithms.

#index 333932
#* Probe, count, and classify: categorizing hidden web databases
#@ Panagiotis G. Ipeirotis;Luis Gravano;Mehran Sahami
#t 2001
#c 5
#% 46803
#% 165110
#% 194283
#% 219052
#% 229828
#% 248228
#% 260001
#% 262063
#% 267454
#% 273926
#% 280817
#% 287463
#% 301225
#% 309783
#% 375017
#% 376266
#% 458379
#% 465747
#% 479642
#% 504579
#% 591588
#% 665561
#% 1499571
#! The contents of many valuable web-accessible databases are only accessible through search interfaces and are hence invisible to traditional web “crawlers.” Recent studies have estimated the size of this “hidden web” to be 500 billion pages, while the size of the “crawlable” web is only an estimated two billion pages. Recently, commercial web sites have started to manually organize web-accessible databases into Yahoo!-like hierarchical classification schemes. In this paper, we introduce a method for automating this classification process by using a small number of query probes. To classify a database, our algorithm does not retrieve or inspect any documents or pages from the database, but rather just exploits the number of matches that each query probe generates at the database in question. We have conducted an extensive experimental evaluation of our technique over collections of real documents, including over one hundred web-accessible databases. Our experiments show that our system has low overhead and achieves high classification accuracy across a variety of databases.

#index 333933
#* Data bubbles: quality preserving performance boosting for hierarchical clustering
#@ Markus M. Breunig;Hans-Peter Kriegel;Peer Kröger;Jörg Sander
#t 2001
#c 5
#% 40098
#% 210173
#% 273890
#% 280402
#! In this paper, we investigate how to scale hierarchical clustering methods (such as OPTICS) to extremely large databases by utilizing data compression methods (such as BIRCH or random sampling). We propose a three step procedure: 1) compress the data into suitable representative objects; 2) apply the hierarchical clustering algorithm only to these objects; 3) recover the clustering structure for the whole data set, based on the result for the compressed data. The key issue in this approach is to design compressed data items such that not only a hierarchical clustering algorithm can be applied, but also that they contain enough information to infer the clustering structure of the original data set in the third step. This is crucial because the results of hierarchical clustering algorithms, when applied naively to a random sample or to the clustering features (CFs) generated by BIRCH, deteriorate rapidly for higher compression rates. This is due to three key problems, which we identify. To solve these problems, we propose an efficient post-processing step and the concept of a Data Bubble as a special kind of compressed data item. Applying OPTICS to these Data Bubbles allows us to recover a very accurate approximation of the clustering structure of a large data set even for very high compression rates. A comprehensive performance and quality evaluation shows that we only trade very little quality of the clustering result for a great increase in performance.

#index 333934
#* Mining needle in a haystack: classifying rare classes via two-phase rule induction
#@ Mahesh V. Joshi;Ramesh C. Agarwal;Vipin Kumar
#t 2001
#c 5
#% 136350
#% 209021
#% 283138
#% 302391
#% 375017
#% 376266
#% 424801
#% 449566
#% 466744
#! Learning models to classify rarely occurring target classes is an important problem with applications in network intrusion detection, fraud detection, or deviation detection in general. In this paper, we analyze our previously proposed two-phase rule induction method in the context of learning complete and precise signatures of rare classes. The key feature of our method is that it separately conquers the objectives of achieving high recall and high precision for the given target class. The first phase of the method aims for high recall by inducing rules with high support and a reasonable level of accuracy. The second phase then tries to improve the precision by learning rules to remove false positives in the collection of the records covered by the first phase rules. Existing sequential covering techniques try to achieve high precision for each individual disjunct learned. In this paper, we claim that such approach is inadequate for rare classes, because of two problems: splintered false positives and error-prone small disjuncts. Motivated by the strengths of our two-phase design, we design various synthetic data models to identify and analyze the situations in which two state-of-the-art methods, RIPPER and C4.5 rules, either fail to learn a model or learn a very poor model. In all these situations, our two-phase approach learns a model with significantly better recall and precision levels. We also present a comparison of the three methods on a challenging real-life network intrusion detection dataset. Our method is significantly better or comparable to the best competitor in terms of achieving better balance between recall and precision.

#index 333935
#* Efficient evaluation of XML middle-ware queries
#@ Mary Fernandez;Atsuyuki Morishima;Dan Suciu
#t 2001
#c 5
#% 281149
#% 287295
#% 309851
#% 384978
#% 411554
#% 480152
#% 480317
#% 504573
#! We address the problem of efficiently constructing materialized XML views of relational databases. In our setting, the XML view is specified by a query in the declarative query language of a middle-ware system, called SilkRoute. The middle-ware system evaluates a query by sending one or more SQL queries to the target relational database, integrating the resulting tuple streams, and adding the XML tags. We focus on how to best choose the SQL queries, without having control over the target RDBMS.

#index 333938
#* Filtering algorithms and implementation for very fast publish/subscribe systems
#@ Françoise Fabret;H. Arno Jacobsen;François Llirbat;Joăo Pereira;Kenneth A. Ross;Dennis Shasha
#t 2001
#c 5
#% 86945
#% 116044
#% 271199
#% 275367
#% 297191
#% 300179
#% 307470
#% 319671
#% 479819
#% 480147
#% 480296
#% 511917
#% 631962
#! Publish/Subscribe is the paradigm in which users express long-term interests (“subscriptions”) and some agent “publishes” events (e.g., offers). The job of Publish/Subscribe software is to send events to the owners of subscriptions satisfied by those events. For example, a user subscription may consist of an interest in an airplane of a certain type, not to exceed a certain price. A published event may consist of an offer of an airplane with certain properties including price. Each subscription consists of a conjunction of (attribute, comparison operator, value) predicates. A subscription closely resembles a trigger in that it is a long-lived conditional query associated with an action (usually, informing the subscriber). However, it is less general than a trigger so novel data structures and implementations may enable the creation of more scalable, high performance publish/subscribe systems. This paper describes an attempt at the construction of such algorithms and its implementation. Using a combination of data structures, application-specific caching policies, and application-specific query processing our system can handle 600 events per second for a typical workload containing 6 million subscriptions.

#index 333939
#* Adaptable query optimization and evaluation in temporal middleware
#@ Giedrius Slivinskas;Christian S. Jensen;Richard Thomas Snodgrass
#t 2001
#c 5
#% 77323
#% 140389
#% 154750
#% 189779
#% 198066
#% 207552
#% 247425
#% 264263
#% 264998
#% 287268
#% 301163
#% 301179
#% 308448
#% 318049
#% 443257
#% 479449
#% 480788
#% 565457
#% 565462
#% 570888
#% 632008
#% 641038
#! Time-referenced data are pervasive in most real-world databases. Recent advances in temporal query languages show that such database applications may benefit substantially from built-in temporal support in the DBMS. To achieve this, temporal query optimization and evaluation mechanisms must be provided, either within the DBMS proper or as a source level translation from temporal queries to conventional SQL. This paper proposes a new approach: using a middleware component on top of a conventional DBMS. This component accepts temporal SQL statements and produces a corresponding query plan consisting of algebraic as well as regular SQL parts. The algebraic parts are processed by the middleware, while the SQL parts are processed by the DBMS. The middleware uses performance feedback from the DBMS to adapt its partitioning of subsequent queries into middleware and DBMS parts. The paper describes the architecture and implementation of the temporal middleware component, termed TANGO, which is based on the Volcano extensible query optimizer and the XXL query processing library. Experiments with the system demonstrate the utility of the middleware's internal processing capability and its cost-based mechanism for apportioning the processing between the middleware and the underlying DBMS.

#index 333940
#* Optimizing multidimensional index trees for main memory access
#@ Kihong Kim;Sang K. Cha;Keunjoo Kwon
#t 2001
#c 5
#% 86950
#% 144014
#% 153260
#% 164360
#% 172908
#% 248796
#% 252304
#% 252458
#% 300194
#% 300218
#% 427199
#% 462059
#% 464843
#% 479819
#% 479821
#% 480093
#% 480119
#% 481455
#! Recent studies have shown that cache-conscious indexes such as the CSB+-tree outperform conventional main memory indexes such as the T-tree. The key idea of these cache-conscious indexes is to eliminate most of child pointers from a node to increase the fanout of the tree. When the node size is chosen in the order of the cache block size, this pointer elimination effectively reduces the tree height, and thus improves the cache behavior of the index. However, the pointer elimination cannot be directly applied to multidimensional index structures such as the R-tree, where the size of a key, typically, an MBR (minimum bounding rectangle), is much larger than that of a pointer. Simple elimination of four-byte pointers does not help much to pack more entries in a node.This paper proposes a cache-conscious version of the R-tree called the CR-tree. To pack more entries in a node, the CR-tree compresses MBR keys, which occupy almost 80% of index data in the two-dimensional case. It first represents the coordinates of an MBR key relatively to the lower left corner of its parent MBR to eliminate the leading O's from the relative coordinate representation. Then, it quantizes the relative coordinates with a fixed number of bits to further cut off the trailing less significant bits. Consequently, the CR-tree becomes significantly wider and smaller than the ordinary R-tree. Our experimental and analytical study shows that the two-dimensional CR-tree performs search up to 2.5 times faster than the ordinary R-tree while maintaining similar update performance and consuming about 60% less memory space.

#index 333941
#* Locally adaptive dimensionality reduction for indexing large time series databases
#@ Eamonn Keogh;Kaushik Chakrabarti;Michael Pazzani;Sharad Mehrotra
#t 2001
#c 5
#% 172949
#% 201876
#% 214595
#% 227924
#% 237204
#% 248797
#% 248798
#% 251654
#% 257637
#% 260014
#% 260016
#% 264633
#% 273704
#% 280452
#% 280846
#% 285711
#% 286881
#% 316559
#% 316560
#% 427199
#% 460862
#% 461885
#% 462231
#% 464851
#% 477482
#% 477825
#% 480146
#% 480307
#% 481609
#% 481611
#% 501194
#% 527026
#% 571081
#% 586837
#% 616530
#% 617886
#% 631920
#% 631923
#% 632089
#! Similarity search in large time series databases has attracted much research interest recently. It is a difficult problem because of the typically high dimensionality of the data.. The most promising solutions involve performing dimensionality reduction on the data, then indexing the reduced data with a multidimensional index structure. Many dimensionality reduction techniques have been proposed, including Singular Value Decomposition (SVD), the Discrete Fourier transform (DFT), and the Discrete Wavelet Transform (DWT). In this work we introduce a new dimensionality reduction technique which we call Adaptive Piecewise Constant Approximation (APCA). While previous techniques (e.g., SVD, DFT and DWT) choose a common representation for all the items in the database that minimizes the global reconstruction error, APCA approximates each time series by a set of constant value segments of varying lengths such that their individual reconstruction errors are minimal. We show how APCA can be indexed using a multidimensional index structure. We propose two distance measures in the indexed space that exploit the high fidelity of APCA for fast searching: a lower bounding Euclidean distance approximation, and a non-lower bounding, but very tight Euclidean distance approximation and show how they can support fast exact searching, and even faster approximate searching on the same index structure. We theoretically and empirically compare APCA to all the other techniques and demonstrate its superiority.

#index 333942
#* Main-memory index structures with fixed-size partial keys
#@ Philip Bohannon;Peter Mcllroy;Rajeev Rastogi
#t 2001
#c 5
#% 115608
#% 115663
#% 202473
#% 273940
#% 273945
#% 287715
#% 300194
#% 408638
#% 427195
#% 440167
#% 442832
#% 442836
#% 443122
#% 464843
#% 479474
#% 479769
#% 479819
#% 479821
#% 481454
#% 979367
#! The performance of main-memory index structures is increasingly determined by the number of CPU cache misses incurred when traversing the index. When keys are stored indirectly, as is standard in main-memory databases, the cost of key retrieval in terms of cache misses can dominate the cost of an index traversal. Yet it is inefficient in both time and space to store even moderate sized keys directly in index nodes. In this paper, we investigate the performance of tree structures suitable for OLTP workloads in the face of expensive cache misses and non-trivial key sizes. We propose two index structures, pkT-trees and pkB-trees, which significantly reduce cache misses by storing partial-key information in the index. We show that a small, fixed amount of key information allows most cache misses to be avoided, allowing for a simple node structure and efficient implementation. Finally, we study the performance and cache behavior of partial-key trees by comparing them with other main-memory tree structures for a wide variety of key sizes and key value distributions.

#index 333943
#* Automatic segmentation of text into structured records
#@ Vinayak Borkar;Kaustubh Deshmukh;Sunita Sarawagi
#t 2001
#c 5
#% 137711
#% 201889
#% 211348
#% 248808
#% 271065
#% 273925
#% 275915
#% 278109
#% 283136
#% 438103
#% 466892
#% 479807
#% 531459
#% 632018
#% 632051
#% 742424
#! In this paper we present a method for automatically segmenting unformatted text records into structured elements. Several useful data sources today are human-generated as continuous text whereas convenient usage requires the data to be organized as structured records. A prime motivation is the warehouse address cleaning problem of transforming dirty addresses stored in large corporate databases as a single text field into subfields like “City” and “Street”. Existing tools rely on hand-tuned, domain-specific rule-based systems.We describe a tool DATAMOLD that learns to automatically extract structure when seeded with a small number of training examples. The tool enhances on Hidden Markov Models (HMM) to build a powerful probabilistic model that corroborates multiple sources of information including, the sequence of elements, their length distribution, distinguishing words from the vocabulary and an optional external data dictionary. Experiments on real-life datasets yielded accuracy of 90% on Asian addresses and 99% on US addresses. In contrast, existing information extraction methods based on rule-learning techniques yielded considerably lower accuracy.

#index 333945
#* Efficient and effective metasearch for text databases incorporating linkages among documents
#@ Clement Yu;Weiyi Meng;Wensheng Wu;King-Lup Liu
#t 2001
#c 5
#% 176501
#% 184496
#% 194246
#% 194275
#% 218982
#% 232701
#% 245788
#% 253188
#% 262063
#% 274483
#% 280839
#% 280854
#% 280856
#% 282905
#% 287237
#% 309133
#% 330704
#% 406493
#% 443556
#% 443561
#% 479451
#% 479642
#% 481748
#% 567255
#% 584914
#% 631997
#% 978381
#! Linkages among documents have a significant impact on the importance of documents, as it can be argued that important documents are pointed to by many documents or by other important documents. Metasearch engines can be used to facilitate ordinary users for retrieving information from multiple local sources (text databases). There is a search engine associated with each database. In a large-scale metasearch engine, the contents of each local database is represented by a representative. Each user query is evaluated against he set of representatives of all databases in order to determine the appropriate databases (search engines) to search (invoke) In previous word, the linkage information between documents has not been utilized in determining the appropriate databases to search. In this paper, such information is employed to determine the degree of relevance of a document with respect to a given query. Specifically, the importance (rank) of each document as determined by the linkages is integrated in each database representative to facilitate the selection of databases for each given query. We establish a necessary and sufficient condition to rank databases optimally, while incorporating the linkage information. A method is provided to estimate the desired quantities stated in the necessary and sufficient condition. The estimation method runs in time linearly proportional to the number of query terms. Experimental results are provided to demonstrate the high retrieval effectiveness of the method.

#index 333946
#* Independence is good: dependency-based histogram synopses for high-dimensional data
#@ Amol Deshpande;Minos Garofalakis;Rajeev Rastogi
#t 2001
#c 5
#% 42408
#% 44876
#% 70370
#% 152585
#% 210190
#% 245654
#% 300193
#% 480306
#% 482092
#! Approximating the joint data distribution of a multi-dimensional data set through a compact and accurate histogram synopsis is a fundamental problem arising in numerous practical scenarios, including query optimization and approximate query answering. Existing solutions either rely on simplistic independence assumptions or try to directly approximate the full joint data distribution over the complete set of attributes. Unfortunately, both approaches are doomed to fail for high-dimensional data sets with complex correlation patterns between attributes. In this paper, we propose a novel approach to histogram-based synopses that employs the solid foundation of statistical interaction models to explicitly identify and exploit the statistical characteristics of the data. Abstractly, our key idea is to break the synopsis into (1) a statistical interaction model that accurately captures significant correlation and independence patterns in data, and (2) a collection of histograms on low-dimensional marginals that, based on the model, can provide accurate approximations of the overall joint data distribution. Extensive experimental results with several real-life data sets verify the effectiveness of our approach. An important aspect of our general, model-based methodology is that it can be used to enhance the performance of other synopsis techniques that are based on data-space partitioning (e.g., wavelets) by providing an effective tool to deal with the “dimensionality curse”.

#index 333947
#* STHoles: a multidimensional workload-aware histogram
#@ Nicolas Bruno;Surajit Chaudhuri;Luis Gravano
#t 2001
#c 5
#% 43163
#% 77321
#% 132779
#% 137887
#% 172902
#% 201921
#% 210190
#% 248822
#% 273901
#% 273903
#% 300193
#% 427219
#% 479648
#% 479984
#% 482092
#% 482123
#% 504019
#% 632056
#! Attributes of a relation are not typically independent. Multidimensional histograms can be an effective tool for accurate multiattribute query selectivity estimation. In this paper, we introduce STHoles, a “workload-aware” histogram that allows bucket nesting to capture data regions with reasonably uniform tuple density. STHoles histograms are built without examining the data sets, but rather by just analyzing query results. Buckets are allocated where needed the most as indicated by the workload, which leads to accurate query selectivity estimations. Our extensive experiments demonstrate that STHoles histograms consistently produce good selectivity estimates across synthetic and real-world data sets and across query workloads, and, in many cases, outperform the best multidimensional histogram techniques that require access to and processing of the full data sets during histogram construction.

#index 333948
#* Global optimization of histograms
#@ H. V. Jagadish;Hui Jin;Beng Chin Ooi;Kian-Lee Tan
#t 2001
#c 5
#% 43163
#% 54047
#% 102784
#% 201921
#% 210190
#% 248812
#% 248822
#% 274152
#% 285924
#% 411554
#% 427219
#% 479648
#% 481266
#% 482092
#% 632048
#! Histograms are frequently used to represent the distribution of data values in an attribute of a relation. Most previous work has focused on identifying the optimal histogram (given a limited number of buckets) for a single attribute independent of other attributes/histograms. In this paper, we propose the idea of global optimization of histograms, i.e., single-attribute histograms for a set of attributes are optimized collectively so as to minimize the overall error in using the histograms. The idea is to allocate more buckets to histograms whose attributes are more frequently used and/or distributions are highly skewed. While the accuracy of some histograms is penalized (being assigned fewer buckets), we expect the global error to be low compared to the traditional method (of allocating equal number of buckets to each histogram).We propose two algorithms to determine the histograms to construct for a collection of attributes. The first is based on dynamic programming, and the second is a greedy algorithm. We compare the overall error of these algorithms against the traditional method. Extensive experiments are conducted and the results confirm the benefits of global optimal histograms in reducing the overall error. The extent of improvement depends on the data and query distributions, ranging from no benefit when there is no significant differences in the data distributions to over a factor of 100 reduction in error in some cases we tried.The time to compute global optimal histogram using dynamic programming is much longer than the time to compute optimal histograms separately for each attribute, and the difference widens at a faster rate as the number of histograms increases. With the greedy algorithm, the time penalty is small, but the error reduction is somewhat less as well. We propose a third algorithm, called greedy algorithm with remedy, that has running time similar to the greedy algorithm, but produces results close to global optimum. In fact, in every experiment that we tried, this algorithm found the exact global optimum.

#index 333949
#* Improving index performance through prefetching
#@ Shimin Chen;Phillip B. Gibbons;Todd C. Mowry
#t 2001
#c 5
#% 13033
#% 128287
#% 172911
#% 213501
#% 251473
#% 251474
#% 262154
#% 267990
#% 287715
#% 300194
#% 304012
#% 333942
#% 384872
#% 439991
#% 464843
#% 479769
#% 479819
#% 480119
#% 566122
#% 593968
#! This paper proposes and evaluate Prefetching B+-Trees (pB+-Trees), which use prefetching to accelerate two important operations on B+-Tree indices: searches and range scans. To accelerate searches, pB+-Trees use prefetching to effectively create wider nodes than the natural data transfer size: e.g., eight vs. one cache lines or disk pages. These wider nodes reduce the height of the B+-Tree, thereby decreasing the number of expensive misses when going from parent to child without significantly increasing the cost of fetching a given node. Our results show that this technique speeds up search and update times by a factor of 1.21-1.5 for main-memory B+-Trees. In addition, it outperforms and is complementary to “Cache-Sensitive B+-Trees.” To accelerate range scans, pB+-Trees provide arrays of pointers to their leaf nodes. These allow the pB+-Tree to prefetch arbitrarily far ahead, even for nonclustered indices, thereby hiding the normally expensive cache misses associated with traversing the leaves within the range. Our results show that this technique yields over a sixfold speedup on range scans of 1000+ keys. Although our experimental evaluation focuses on main memory databases, the techniques that we propose are also applicable to hiding disk latency.

#index 333950
#* Efficient and tumble similar set retrieval
#@ Aristides Gionis;Dimitrios Gunopulos;Nick Koudas
#t 2001
#c 5
#% 1921
#% 152938
#% 172949
#% 243166
#% 249238
#% 249321
#% 252304
#% 255137
#% 299984
#% 303021
#% 375017
#% 380546
#% 460862
#% 465018
#% 479973
#% 481599
#% 481609
#% 632029
#! Set value attributes are a concise and natural way to model complex data sets. Modern Object Relational systems support set value attributes and allow various query capabilities on them. In this paper we initiate a formal study of indexing techniques for set value attributes based on similarity, for suitably defined notions of similarity between sets. Such techniques are necessary in modern applications such as recommendations through collaborative filtering and automated advertising. Our techniques are probabilistic and approximate in nature. As a design principle we create structures that make use of well known and widely used data structuring techniques, as a means to ease integration with existing infrastructure.We show how the problem of indexing a collection of sets based on similarity can be reduced to the problem of indexing suitably encoded (in a way that preserves similarity) binary vectors in Hamming space thus, reducing the problem to one of similarity query processing in Hamming space. Then, we introduce and analyze two data structure primitives that we use in cooperation to perform similarity query processing in a Hamming space. We show how the resulting indexing technique can be optimized for properties of interest by formulating constraint optimization problems based on the space one is willing to devote for indexing. Finally we present experimental results from a prototype implementation of our techniques using real life datasets exploring the accuracy and efficiency of our overall approach as well as the quality of our solutions to problems related to the optimization of the indexing scheme.

#index 333951
#* PREFER: a system for the efficient execution of multi-parametric ranked queries
#@ Vagelis Hristidis;Nick Koudas;Yannis Papakonstantinou
#t 2001
#c 5
#% 25998
#% 198465
#% 213981
#% 217812
#% 237190
#% 248010
#% 273696
#% 273924
#% 300170
#% 300180
#% 427199
#% 464726
#% 480093
#% 482081
#! Users often need to optimize the selection of objects by appropriately weighting the importance of multiple object attributes. Such optimization problems appear often in operations' research and applied mathematics as well as everyday life; e.g., a buyer may select a home as a weighted function of a number of attributes like its distance from office, its price, its area, etc.We capture such queries in our definition of preference queries that use a weight function over a relation's attributes to derive a score for each tuple. Database systems cannot efficiently produce the top results of a preference query because they need to evaluate the weight function over all tuples of the relation. PREFER answers preference queries efficiently by using materialized views that have been pre-processed and stored.We first show how the result of a preference query can be produced in a pipelined fashion using a materialized view. Then we show that excellent performance can be delivered given a reasonable number of materialized views and we provide an algorithm that selects a number of views to precompute and materialize given space constraints.We have implemented the algorithms proposed in this paper in a prototype system called PREFER, which operates on top of a commercial database management system. We present the results of a performance comparison, comparing our algorithms with prior approaches using synthetic datasets. Our results indicate that the proposed algorithms are superior in performance compared to other approaches, both in preprocessing (preparation of materialized views) as well as execution time.

#index 333953
#* Query optimization in compressed database systems
#@ Zhiyuan Chen;Johannes Gehrke;Flip Korn
#t 2001
#c 5
#% 13033
#% 146203
#% 146207
#% 152940
#% 193743
#% 193923
#% 273943
#% 287461
#% 290703
#% 300153
#% 322412
#% 411554
#% 461896
#% 463895
#% 464843
#% 479812
#% 479821
#% 480329
#% 481424
#% 632026
#% 632055
#! Over the last decades, improvements in CPU speed have outpaced improvements in main memory and disk access rates by orders of magnitude, enabling the use of data compression techniques to improve the performance of database systems. Previous work describes the benefits of compression for numerical attributes, where data is stored in compressed format on disk. Despite the abundance of string-valued attributes in relational schemas there is little work on compression for string attributes in a database context. Moreover, none of the previous work suitably addresses the role of the query optimizer: During query execution, data is either eagerly decompressed when it is read into main memory, or data lazily stays compressed in main memory and is decompressed on demand onlyIn this paper, we present an effective approach for database compression based on lightweight, attribute-level compression techniques. We propose a IIierarchical Dictionary Encoding strategy that intelligently selects the most effective compression method for string-valued attributes. We show that eager and lazy decompression strategies produce sub-optimal plans for queries involving compressed string attributes. We then formalize the problem of compression-aware query optimization and propose one provably optimal and two fast heuristic algorithms for selecting a query plan for relational schemas with compressed attributes; our algorithms can easily be integrated into existing cost-based query optimizers. Experiments using TPC-H data demonstrate the impact of our string compression methods and show the importance of compression-aware query optimization. Our approach results in up to an order speed up over existing approaches.

#index 333954
#* SPARTAN: a model-based semantic compression system for massive data tables
#@ Shivnath Babu;Minos Garofalakis;Rajeev Rastogi
#t 2001
#c 5
#% 101217
#% 129987
#% 240222
#% 297171
#% 302725
#% 356227
#% 408396
#% 479640
#% 479787
#% 480124
#% 482105
#% 1650289
#! While a variety of lossy compression schemes have been developed for certain forms of digital data (e.g., images, audio, video), the area of lossy compression techniques for arbitrary data tables has been left relatively unexplored. Nevertheless, such techniques are clearly motivated by the ever-increasing data collection rates of modern enterprises and the need for effective, guaranteed-quality approximate answers to queries over massive relational data sets. In this paper, we propose SPARTAN, a system that takes advantage of attribute semantics and data-mining models to perform lossy compression of massive data tables. SPARTAN is based on the novel idea of exploiting predictive data correlations and prescribed error tolerances for individual attributes to construct concise and accurate Classification and Regression Tree (CaRT) models for entire columns of a table. More precisely, SPARTAN selects a certain subset of attributes for which no values are explicitly stored in the compressed table; instead, concise CaRTs that predict these values (within the prescribed error bounds) are maintained. To restrict the huge search space and construction cost of possible CaRT predictors, SPARTAN employs sophisticated learning techniques and novel combinatorial optimization algorithms. Our experimentation with several real-life data sets offers convincing evidence of the effectiveness of SPARTAN's model-based approach — SPARTAN is able to consistently yield substantially better compression ratios than existing semantic or syntactic compression tools (e.g., gzip) while utilizing only small data samples for model inference.

#index 333955
#* A robust, optimization-based approach for approximate answering of aggregate queries
#@ Surajit Chaudhuri;Gautam Das;Vivek Narasayya
#t 2001
#c 5
#% 1331
#% 36349
#% 217812
#% 227883
#% 236410
#% 259995
#% 273902
#% 273908
#% 273909
#% 280501
#% 300195
#% 465162
#% 479984
#% 480306
#% 480471
#% 504019
#! The ability to approximately answer aggregation queries accurately and efficiently is of great benefit for decision support and data mining tools. In contrast to previous sampling-based studies, we treat the problem as an optimization problem whose goal is to minimize the error in answering queries in the given workload. A key novelty of our approach is that we can tailor the choice of samples to be robust even for workloads that are “similar” but not necessarily identical to the given workload. Finally, our techniques recognize the importance of taking into account the variance in the data distribution in a principled manner. We show how our solution can be implemented on a database system, and present results of extensive experiments on Microsoft SQL Server 2000 that demonstrate the superior quality of our method compared to previous work.

#index 333962
#* Materialized view selection and maintenance using multi-query optimization
#@ Hoshi Mistry;Prasan Roy;S. Sudarshan;Krithi Ramamritham
#t 2001
#c 5
#% 13016
#% 36117
#% 201929
#% 210182
#% 210208
#% 286991
#% 300166
#% 340301
#% 459016
#% 464706
#% 480158
#% 565457
#% 565469
#! Materialized views have been found to be very effective at speeding up queries, and are increasingly being supported by commercial databases and data warehouse systems. However, whereas the amount of data entering a warehouse and the number of materialized views are rapidly increasing, the time window available for maintaining materialized views is shrinking. These trends necessitate efficient techniques for the maintenance of materialized views.In this paper, we show how to find an efficient plan for the maintenance of a set of materialized views, by exploiting common subexpressions between different view maintenance expressions. In particular, we show how to efficiently select (a) expressions and indices that can be effectively shared, by transient materialization; (b) additional expressions and indices for permanent materialization; and (c) the best maintenance plan — incremental or recomputation — for each view. These three decisions are highly interdependent, and the choice of one affects the choice of the others. We develop a framework that cleanly integrates the various choices in a systematic and efficient manner. Our evaluations show that many-fold improvement in view maintenance time can be achieved using our techniques. Our algorithms can also be used to efficiently select materialized views to speed up workloads containing queries and updates.

#index 333964
#* Generating efficient plans for queries using views
#@ Foto N. Afrati;Chen Li;Jeffrey D. Ullman
#t 2001
#c 5
#% 23902
#% 36683
#% 70370
#% 198465
#% 227886
#% 237190
#% 248038
#% 273911
#% 300138
#% 379503
#% 387508
#% 411554
#% 464056
#% 464203
#% 464717
#% 479452
#% 479950
#% 480149
#% 481923
#% 482110
#% 571091
#% 599549
#! We study the problem or generating efficient, equivalent rewritings using views to compute the answer to a query. We take the closed-world assumption, in which views are materialized from base relations, rather than views describing sources in terms of abstract predicates, as is common when the open-world assumption is used. In the closed-world model, there can be an infinite number of different rewritings that compute the same answer, yet have quite different performance. Query optimizers take a logical plan (a rewriting of the query) as an input, and generate efficient physical plans to compute the answer. Thus our goal is to generate a small subset of the possible logical plans without missing an optimal physical plan.We first consider a cost model that counts the number of subgoals in a physical plan, and show a search space that is guaranteed to include an optimal rewriting, if the query has a rewriting in terms of the views. We also develop an efficient algorithm for finding rewritings with the minimum number of subgoals. We then consider a cost model that counts the sizes of intermediate relations of a physical plan, without dropping any attributes, and give a search space for finding optimal rewritings. Our final cost model allows attributes to be dropped in intermediate relations. We show that, by careful variable renaming, it is possible to do better than the standard “supplementary relation” approach, by dropping attributes that the latter approach would retain. Experiments show that our algorithm of generating optimal rewritings has good efficiency and scalability.

#index 333965
#* Optimizing queries using materialized views: a practical, scalable solution
#@ Jonathan Goldstein;Per-Åke Larson
#t 2001
#c 5
#% 198465
#% 248034
#% 264963
#% 273696
#% 273698
#% 300138
#% 464056
#% 479792
#% 480149
#% 480158
#% 481604
#% 481608
#% 482081
#% 564419
#% 565457
#! Materialized views can provide massive improvements in query processing time, especially for aggregation queries over large tables. To realize this potential, the query optimizer must know how and when to exploit materialized views. This paper presents a fast and scalable algorithm for determining whether part or all of a query can be computed from materialized views and describes how it can be incorporated in transformation-based optimizers. The current version handles views composed of selections, joins and a final group-by. Optimization remains fully cost based, that is, a single “best” rewrite is not selected by heuristic rules but multiple rewrites are generated and the optimizer chooses the best alternative in the normal way. Experimental results based on an implementation in Microsoft SQL Server show outstanding performance and scalability. Optimization time increases slowly with the number of views but remains low even up to a thousand.

#index 333968
#* Dynamic buffer allocation in video-on-demand systems
#@ Sang-Ho Lee;Kyu-Young Whang;Yang-Sae Moon;Il-Yeol Song
#t 2001
#c 5
#% 151340
#% 159079
#% 172881
#% 173593
#% 173594
#% 193237
#% 208749
#% 239654
#% 241295
#% 247928
#% 479479
#% 593027
#% 614633
#! In video-on-demand (VOD) systems, as the size of the buffer allocated to user requests increases, initial latency and memory requirements increase. Hence, the buffer size must be minimized. The existing static buffer allocation scheme, however, determines the buffer size based on the assumption that the system is in the fully loaded state. Thus, when the system is in a partially loaded state, the scheme allocates a buffer larger than necessary to a user request. This paper proposes a dynamic buffer allocation scheme that allocates to user requests buffers of the minimum size in a partially loaded state as well as in the fully loaded state. The inherent difficulty in determining the buffer size in the dynamic buffer allocation scheme is that the size of the buffer currently being allocated is dependent on the number of and the sizes of the buffers to be allocated in the next service period. We solve this problem by the predict-and-enforce strategy, where we predict the number and the sizes of future buffers based on inertia assumptions and enforce these assumptions at runtime. Any violation of these assumptions is resolved by deferring service to the violating new user request until the assumptions are satisfied. Since the size of the current buffer is dependent on the sizes of the future buffers, the size is represented by a recurrence equation. We provide a solution to this equation, which can be computed at the system initialization time for runtime efficiency. We have performed extensive analysis and simulation. The results show that the dynamic buffer allocation scheme reduces initial latency (averaged over the number of user requests in service from one to the maximum capacity) to 1 ÷ 29.4 &nsim; 1 ÷ 11.0 of that for the static one and, by reducing the memory requirement, increases the number of concurrent user requests to 2.36 ∼ 3.25 times that of the static one when averaged over the amount of system memory available. These results demonstrate that the dynamic buffer allocation scheme significantly improves the performance and capacity of VOD systems.

#index 333969
#* Adaptive precision setting for cached approximate values
#@ Chris Olston;Boon Thau Loo;Jennifer Widom
#t 2001
#c 5
#% 78695
#% 188026
#% 225006
#% 300134
#% 340607
#% 458535
#% 458601
#% 464847
#% 480299
#% 480332
#% 503869
#% 566135
#! Caching approximate values instead of exact values presents an opportunity for performance gains in exchange for decreased precision. To maximize the performance improvement, cached approximations must be of appropriate precision: approximations that are too precise easily become invalid, requiring frequent refreshing, while overly imprecise approximations are likely to be useless to applications, which must then bypass the cache. We present a parameterized algorithm for adjusting the precision of cached approximations adaptively to achieve the best performance as data values, precision requirements, or workload vary. We consider interval approximations to numeric values but our ideas can be extended to other kinds of data and approximations. Our algorithm strictly generalizes previous adaptive caching algorithms for exact copies: we can set parameters to require that all approximations be exact, in which case our algorithm dynamically chooses whether or not to cache each data value.We have implemented our algorithm and tested it on synthetic and real-world data. A number of experimental results are reported, showing the effectiveness of our algorithm at maximizing performance, and also showing that in the special case of exact caching our algorithm performs as well as previous algorithms. In cases where bounded imprecision is acceptable, our algorithm easily outperforms previous algorithms for exact caching.

#index 333971
#* Proxy-server architectures for OLAP
#@ Panos Kalnis;Dimitris Papadias
#t 2001
#c 5
#% 210182
#% 210211
#% 227868
#% 227869
#% 227880
#% 248806
#% 250053
#% 264263
#% 273917
#% 462204
#% 464215
#% 464706
#% 479476
#% 479646
#% 481916
#% 481948
#% 481951
#% 566126
#% 571216
#% 571217
#% 641013
#! Data warehouses have been successfully employed for assisting decision making by offering a global view of the enterprise data and providing mechanisms for On-Line Analytical processing. Traditionally, data warehouses are utilized within the limits of an enterprise or organization. The growth of Internet and WWW however, has created new opportunities for data sharing among ad-hoc, geographically spanned and possibly mobile users. Since it is impractical for each enterprise to set up a worldwide infrastructure, currently such applications are handled by the central warehouse. This often yields poor performance, due to overloading of the central server and low transfer rate of the network.In this paper we propose an architecture for OLAP cache servers (OCS). An OCS is the equivalent of a proxy-server for web documents, but it is designed to accommodate data from warehouses and support OLAP operations. We allow numerous OCSs to be connected via an arbitrary network, and present a centralized, a semi-centralized and an autonomous control policy. We experimentally evaluate these policies and compare the performance gain against the existing systems where caching is performed only at the client side. Our architecture offers increased autonomy at remote clients, substantial network traffic savings, better scalability, lower response time and is complementary both to existing OLAP cache systems and distributed OLAP approaches.

#index 333973
#* Epsilon grid order: an algorithm for the similarity join on massive high-dimensional data
#@ Christian Böhm;Bernhard Braunmüller;Florian Krebs;Hans-Peter Kriegel
#t 2001
#c 5
#% 102772
#% 152937
#% 169940
#% 172909
#% 210186
#% 210187
#% 227932
#% 237187
#% 248790
#% 273890
#% 420078
#% 421052
#% 435141
#% 443083
#% 458741
#% 460862
#% 462070
#% 462236
#% 464205
#% 465000
#% 479453
#% 479473
#% 479791
#% 481455
#% 481947
#% 527021
#% 632009
#! The similarity join is an important database primitive which has been successfully applied to speed up applications such as similarity search, data analysis and data mining. The similarity join combines two point sets of a multidimensional vector space such that the result contains all point pairs where the distance does not exceed a parameter &egr;. In this paper, we propose the Epsilon Grid Order, a new algorithm for determining the similarity join of very large data sets. Our solution is based on a particular sort order of the data points, which is obtained by laying an equi-distant grid with cell length &egr; over the data space and comparing the grid cells lexicographically. A typical problem of grid-based approaches such as MSJ or the &egr;-kdB-tree is that large portions of the data sets must be held simultaneously in main memory. Therefore, these approaches do not scale to large data sets. Our technique avoids this problem by an external sorting algorithm and a particular scheduling strategy during the join phase. In the experimental evaluation, a substantial improvement over competitive techniques is shown.

#index 333975
#* Modeling high-dimensional index structures using sampling
#@ Christian A. Lang;Ambuj K. Singh
#t 2001
#c 5
#% 32898
#% 32913
#% 82346
#% 86950
#% 88056
#% 137885
#% 153260
#% 164360
#% 198573
#% 213975
#% 227939
#% 237187
#% 248796
#% 248797
#% 273887
#% 285932
#% 299989
#% 326337
#% 411694
#% 427199
#% 435141
#% 458741
#% 464195
#% 464859
#% 479462
#% 479649
#% 481956
#% 617839
#% 631963
#% 632043
#% 632104
#! A large number of index structures for high-dimensional data have been proposed previously. In order to tune and compare such index structures, it is vital to have efficient cost prediction techniques for these structures. Previous techniques either assume uniformity of the data or are not applicable to high-dimensional data. We propose the use of sampling to predict the number of accessed index pages during a query execution. Sampling is independent of the dimensionality and preserves clusters which is important for representing skewed data. We present a general model for estimating the index page layout using sampling and show how to compensate for errors. We then give an implementation of our model under restricted memory assumptions and show that it performs well even under these constraints. Errors are minimal and the overall prediction time is up to two orders of magnitude below the time for building and probing the full index without sampling.

#index 333977
#* Progressive approximate aggregate queries with a multi-resolution tree structure
#@ Iosif Lazaridis;Sharad Mehrotra
#t 2001
#c 5
#% 201876
#% 227883
#% 273902
#% 300193
#% 318051
#% 411694
#% 427199
#% 479984
#% 480306
#% 481599
#% 617867
#! Answering aggregate queries like SUM, COUNT, MIN, MAX, AVG in an approximate manner is often desirable when the exact answer is not needed or too costly to compute. We present an algorithm for answering such queries in multi-dimensional databases, using selective traversal of a Multi-Resolution Aggregate (MRA) tree structure storing point data. Our approach provides 100% intervals of confidence on the value of the aggregate and works iteratively, coming up with improving quality answers, until some error requirement is satisfied or time constraint as reached. Using the same technique we can also answer aggregate queries exactly and our experiments indicate that even for exact answering the proposed data structure and algorithm are very fast.

#index 333979
#* Updating XML
#@ Igor Tatarinov;Zachary G. Ives;Alon Y. Halevy;Daniel S. Weld
#t 2001
#c 5
#% 86954
#% 273922
#% 281149
#% 300179
#% 309851
#% 463594
#% 479956
#% 480152
#! As XML has developed over the past few years, its role has expanded beyond its original domain as a semantics-preserving markup language for online documents, and it is now also the de facto format for interchanging data between heterogeneous systems. Data sources expert XML “views” over their data, and other system can directly import or query these views. As a result, there has been great interest in languages and systems for expressing queries over XML data, whether the XML is stored in a repository or generated as a view over some other data storage format.Clearly, in order to fully evolve XML into a universal data representation and sharing format, we must allow users to specify updates to XML documents and must develop techniques to process them efficiently. Update capabilities are important not only for modifying XML documents, but also for propagating changes through XML view and for expressing and transmitting changes to documents. This paper begins by proposing a set of basic update operations for both ordered and unordered XML data. We next describe extensions to the proposed standard XML query language, XQuery, to incorporate the update operations. We then consider alternative methods for implementing update operations when the XML data is mapped into a relational database. Finally, we describe an experimental evaluation of the alternative techniques for implementing our extensions.

#index 333981
#* On supporting containment queries in relational database management systems
#@ Chun Zhang;Jeffrey Naughton;David DeWitt;Qiong Luo;Guy Lohman
#t 2001
#c 5
#% 236416
#% 281149
#% 300169
#% 300194
#% 309726
#% 339373
#% 406493
#% 479469
#% 479956
#% 480119
#% 480152
#% 481418
#% 504578
#% 566122
#! Virtually all proposals for querying XML include a class of query we term “containment queries”. It is also clear that in the foreseeable future, a substantial amount of XML data will be stored in relational database systems. This raises the question of how to support these containment queries. The inverted list technology that underlies much of Information Retrieval is well-suited to these queries, but should we implement this technology (a) in a separate loosely-coupled IR engine, or (b) using the native tables and query execution machinery of the RDBMS? With option (b), more than twenty years of work on RDBMS query optimization, query execution, scalability, and concurrency control and recovery immediately extend to the queries and structures that implement these new operations. But all this will be irrelevant if the performance of option (b) lags that of (a) by too much. In this paper, we explore some performance implications of both options using native implementations in two commercial relational database systems and in a special purpose inverted list engine. Our performance study shows that while RDBMSs are generally poorly suited for such queries, under conditions they can outperform an inverted list engine. Our analysis further identifies two significant causes that differentiate the performance of the IR and RDBMS implementations: the join algorithms employed and the hardware cache utilization. Our results suggest that contrary to most expectations, with some modifications, a native implementations in an RDBMS can support this class of query much more efficiently.

#index 333982
#* Monitoring XML data on the Web
#@ Benjamin Nguyen;Serge Abiteboul;Grégory Cobena;Mihaí Preda
#t 2001
#c 5
#% 199537
#% 299489
#% 300179
#% 354016
#% 361445
#% 394417
#% 443298
#% 462212
#% 631962
#! We consider the monitoring of a flow of incoming documents. More precisely, we present here the monitoring used in a very large warehouse built from XML documents found on the web. The flow of documents consists in XML pages (that are warehoused) and HTML pages (that are not). Our contributions are the following:a subscription language which specifies the monitoring of pages when fetched, the periodical evaluation of continuous queries and the production of XML reports.the description of the architecture of the system we implemented that makes it possible to monitor a flow of millions of pages per day with millions of subscriptions on a single PC, and scales up by using more machines.a new algorithm for processing alerts that can be used in a wider context.We support monitoring at the page level (e.g., discovery of a new page within a certain semantic domain) as well as at the element level (e.g., insertion of a new electronic product in a catalog).This work is part of the Xyleme system. Xyleme is developed on a cluster of PCs under Linux with Corba communications. The part of the system described in this paper has been implemented. We mention first experiments.

#index 333983
#* Applying the golden rule of sampling for query estimation
#@ Yi-Leh Wu;Divyakant Agrawal;Amr El Abbadi
#t 2001
#c 5
#% 82346
#% 193396
#% 201921
#% 210190
#% 227866
#% 248812
#% 248822
#% 259995
#% 273887
#% 273908
#% 437510
#! Query size estimation is crucial for many database system components. In particular, query optimizers need efficient and accurate query size estimation when deciding among alternative query plans. In this paper we propose a novel sampling technique based on the golden rule of sampling, introduced by von Neumann in 1947, for estimating range queries. The proposed technique randomly samples the frequency domain using the cumulative frequency distribution and yields good estimates without any a priori knowledge of the actual underlying distribution of spatial objects. We show experimentally that the proposed sampling technique gives smaller approximation error than the Min-Skew histogram based and wavelet based approaches for both synthetic and real datasets. Moreover, the proposed technique can be easily extended for higher dimensional datasets.

#index 333986
#* Selectivity estimation using probabilistic models
#@ Lise Getoor;Benjamin Taskar;Daphne Koller
#t 2001
#c 5
#% 44876
#% 82346
#% 115608
#% 144070
#% 209725
#% 248822
#% 266230
#% 273902
#% 273909
#% 277396
#% 277480
#% 480306
#% 482092
#% 496116
#! Estimating the result size of complex queries that involve selection on multiple attributes and the join of several relations is a difficult but fundamental task in database query processing. It arises in cost-based query optimization, query profiling, and approximate query answering. In this paper, we show how probabilistic graphical models can be effectively used for this task as an accurate and compact approximation of the joint frequency distribution of multiple attributes across multiple relations. Probabilistic Relational Models (PRMs) are a recent development that extends graphical statistical models such as Bayesian Networks to relational domains. They represent the statistical dependencies between attributes within a table, and between attributes across foreign-key joins. We provide an efficient algorithm for constructing a PRM front a database, and show how a PRM can be used to compute selectivity estimates for a broad class of queries. One of the major contributions of this work is a unified framework for the estimation of queries involving both select and foreign-key join operations. Furthermore, our approach is not limited to answering a small set of predetermined queries; a single model can be used to effectively estimate the sizes of a wide collection of potential queries across multiple tables. We present results for our approach on several real-world databases. For both single-table multi-attribute queries and a general class of select-join queries, our approach produces more accurate estimates than standard approaches to selectivity estimation, using comparable space and time.

#index 333987
#* Communication-efficient distributed mining of association rules
#@ Assaf Schuster;Ran Wolff
#t 2001
#c 5
#% 201894
#% 280436
#% 328271
#% 340291
#% 443091
#% 443348
#% 481290
#% 481588
#% 481758
#% 481945
#% 501207
#! Mining for associations between items in large transactional databases is a central problem in the field of knowledge discovery. When the database is partitioned among several share-nothing machines, the problem can be addressed using distributed data mining algorithms. One such algorithm, called CD, was proposed by Agrawal and Shafer in [1] and was later enhanced by the FDM algorithm of Cheung, Han et al. [5].The main problem with these algorithms is that they do not scale well with the number of partitions. They are thus impractical for use in modern distributed environments such as peer-to-peer systems, in which hundreds or thousands of computers may interact. In this paper we present a set of new algorithms that solve the Distributed Association Rule Mining problem using far less communication. In addition to being very efficient, the new algorithms are also extremely robust. Unlike existing algorithms, they continue to be efficient even when the data is skewed or the partition sizes are imbalanced. We present both experimental and theoretical results concerning the behavior of these algorithms and explain how they can be implemented in different settings.

#index 333988
#* Data-driven understanding and refinement of schema mappings
#@ Ling Ling Yan;Renée J. Miller;Laura M. Haas;Ronald Fagin
#t 2001
#c 5
#% 471
#% 36683
#% 172933
#% 201927
#% 213983
#% 220425
#% 269088
#% 286995
#% 291861
#% 332166
#% 480134
#% 564416
#! At the heart of many data-intensive applications is the problem of quickly and accurately transforming data into a new form. Database researchers have long advocated the use of declarative queries for this process. Yet tools for creating, managing and understanding the complex queries necessary for data transformation are still too primitive to permit widespread adoption of this approach. We present a new framework that uses data examples as the basis for understanding and refining declarative schema mappings. We identify a small set of intuitive operators for manipulating examples. These operators permit a user to follow and refine an example by walking through a data source. We show that our operators are powerful enough both to identify a large class of schema mappings and to distinguish effectively between alternative schema mappings. These operators permit a user to quickly and intuitively build and refine complex data transformation queries that map one data source into another.

#index 333989
#* Minimization of tree pattern queries
#@ Sihem Amer-Yahia;SungRan Cho;Laks V. S. Lakshmanan;Divesh Srivastava
#t 2001
#c 5
#% 36683
#% 53390
#% 123085
#% 237181
#% 248025
#% 248026
#% 248032
#% 248033
#% 268758
#% 273897
#% 281149
#% 299968
#% 300143
#% 300168
#% 386455
#% 479806
#% 504578
#% 599549
#! Tree patterns forms a natural basis to query tree-structured data such as XML and LDAP. Since the efficiency of tree pattern matching against a tree-structured database depends on the size of the pattern, it is essential to identify and eliminate redundant nodes in the pattern and do so as quickly as possible. In this paper, we study tree pattern minimization both in the absence and in the presence of integrity constraints (ICs) on the underlying tree-structured database.When no ICs are considered, we call the process of minimizing a tree pattern, constraint-independent minimization. We develop a polynomial time algorithm called CIM for this purpose. CIM's efficiency stems from two key properties: (i) a node cannot be redundant unless its children are, and (ii) the order of elimination of redundant nodes is immaterial. When ICs are considered for minimization, we refer to it as constraint-dependent minimization. For tree-structured databases, required child/descendant and type co-occurrence ICs are very natural. Under such ICs, we show that the minimal equivalent query is unique. We show the surprising result that the algorithm obtained by first augmenting the tree pattern using ICS, and then applying CIM, always finds the unique minimal equivalent query; we refer to this algorithm as ACIM. While ACIM is also polynomial time, it can be expensive in practice because of its inherent non-locality. We then present a fast algorithm, CDM, that identifies and eliminates local redundancies due to ICs, based on propagating “information labels” up the tree pattern. CDM can be applied prior to ACIM for improving the minimization efficiency. We complement our analytical results with an experimental study that shows the effectiveness of our tree pattern minimization techniques.

#index 333990
#* Reconciling schemas of disparate data sources: a machine-learning approach
#@ AnHai Doan;Pedro Domingos;Alon Y. Halevy
#t 2001
#c 5
#% 132938
#% 229827
#% 246831
#% 266102
#% 273911
#% 283238
#% 307632
#% 310556
#% 312860
#% 443235
#% 479452
#% 479783
#% 480134
#% 481923
#% 637829
#% 641044
#% 709066
#% 802590
#% 1272397
#% 1275347
#! A data-integration system provides access to a multitude of data sources through a single mediated schema. A key bottleneck in building such systems has been the laborious manual construction of semantic mappings between the source schemas and the mediated schema. We describe LSD, a system that employs and extends current machine-learning techniques to semi-automatically find such mappings. LSD first asks the user to provide the semantic mappings for a small set of data sources, then uses these mappings together with the sources to train a set of learners. Each learner exploits a different type of information either in the source schemas or in their data. Once the learners have been trained, LSD finds semantic mappings for a new data source by applying the learners, then combining their predictions using a meta-learner. To further improve matching accuracy, we extend machine learning techniques so that LSD can incorporate domain constraints as an additional source of knowledge, and develop a novel learner that utilizes the structural information in XML documents. Our approach thus is distinguished in that it incorporates multiple types of knowledge. Importantly, its architecture is extensible to additional learners that may exploit new kinds of information. We describe a set of experiments on several real-world domains, and show that LSD proposes semantic mappings with a high degree of accuracy.

#index 333992
#* StorHouse metanoia - new applications for database, storage & data warehousing
#@ Felipe Cariño, Jr.;Pekka Kostamaa;Art Kaufmann;John Burgess
#t 2001
#c 5
#% 256700
#% 462069
#% 479802
#% 479982
#% 480281
#% 499803
#% 612123
#! This paper describes the StorHouse/Relational Manager (RM) database system that uses and exploits an active storage hierarchy. By active storage hierarchy, we mean that StorHouse/RM executes SQL queries directly against data stored on all hierarchical storage (i.e. disk, optical, and tape) without post processing a file or a DBA having to manage a data set. We describe and analyze StorHouse/RM features and internals. We also describe how StorHouse/RM differs from traditional HSM (Hierarchical Storage Management) systems. For commercial applications we describe an evolution to the Data Warehouse concept, called Atomic Data Store, whereby atomic data is stored in the database system. Atomic data is defined as storing all the historic data values and executing queries against them. We also describe a Hub-and-Spoke Data Warehouse architecture, which is used to feed or fuel data into Data Marts.Furthermore, we provide analysis how StorHouse/RM can be federated with DB2, Oracle and Microsoft SQL Server 7 (SS7) and thus provide these databases an active storage hierarchy (i.e. tape). We then show two federated data modeling techniques (a) logical horizontal partitioning (LHP) of tuples and (b) logical vertical partitioning (LVP) of columns to demonstrate our database extension capabilities. We conclude with a TPC-like performance analysis of data stored on tape and disk.

#index 333995
#* Enabling dynamic content caching for database-driven web sites
#@ K. Selçuk Candan;Wen-Syan Li;Qiong Luo;Wang-Pin Hsiung;Divyakant Agrawal
#t 2001
#c 5
#% 249968
#% 283823
#% 300177
#% 302484
#% 309705
#% 309708
#% 309711
#% 480474
#% 622611
#% 635787
#% 979356
#! Web performance is a key differentiation among content providers. Snafus and slowdowns at major web sites demonstrate the difficulty that companies face trying to scale to a large amount of web traffic. One solution to this problem is to store web content at server-side and edge-caches for fast delivery to the end users. However, for many e-commerce sites, web pages are created dynamically based on the current state of business processes, represented in application servers and databases. Since application servers, databases, web servers, and caches are independent components, there is no efficient mechanism to make changes in the database content reflected to the cached web pages. As a result, most application servers have to mark dynamically generated web pages as non-cacheable. In this paper, we describe the architectural framework of the CachePortal system for enabling dynamic content caching for database-driven e-commerce sites. We describe techniques for intelligently invalidating dynamically generated web pages in the caches, thereby enabling caching of web pages generated based on database contents. We use some of the most popular components in the industry to illustrate the deployment and applicability of the proposed architecture.

#index 333996
#* The network is the database: data management for highly distributed systems
#@ Julio C. Navas;Michael Wynblatt
#t 2001
#c 5
#% 127517
#% 235853
#% 264263
#% 281556
#% 281557
#% 308760
#! This paper describes the methodology and implementation of a data management system for highly distributed systems, which was built to solve the scalability and reliability problems faced in a wide area postal logistics application developed at Siemens. The core of the approach is to borrow from Internet routing protocols, and their proven scalability and robustness, to build a network-embedded dynamic database index, and to augment schema definition to optimize the use of this index. The system was developed with an eye toward future applications in the area of sensor networks.

#index 333997
#* Content integration for e-business
#@ Michael Stonebraker;Joseph M. Hellerstein
#t 2001
#c 5
#% 266216
#% 279164
#% 328427
#% 480158
#% 480286
#% 481916
#% 571217
#% 674103
#! We define the problem of content integration for E-Business, and show how it differs in fundamental ways from traditional issues surrounding data integration, application integration, data warehousing and OLTP. Content integration includes catalog integration as a special case, but encompasses a broader set of applications and challenges. We explore the characteristics of content integration and required services for any solution. In addition, we explore architectural alternatives and discuss the use of XML in this arena.

#index 334000
#* Catalog management in websphere commerce suite
#@ Thomas Maguire
#t 2001
#c 5

#index 334001
#* Experiences in mining aviation safety data
#@ Zohreh Nazeri;Eric Bloedorn;Paul Ostwald
#t 2001
#c 5
#% 232136
#% 420056
#! The goal of data analysis in aviation safety is simple: improve safety. However, the path to this goal is hard to identify. What data mining methods are most applicable to this task? What data are available and how should they be analyzed? How do we focus on the most interesting results? Our answers to these questions are based on a recent research project we completed. The encouraging news is that we found a number of aviation safety offices doing commendable work to collect and analyze safety-related data. But we also found a number of areas where data mining techniques could provide new tools that either perform analyses that were not considered before, or that can now be done more easily.Currently, Aviation Safety offices collect and analyze the incident reports by a combination of manual and automated methods. Data analysis is done by safety officers who are well familiar with the domain, but not with data mining methods. Some Aviation Safety officers have tools to automate the database query and report generation process. However, the actual analysis is done by the officer with only fairly rudimentary tools to help extract the useful information from the data.Our research project looked at the application of data mining techniques to aviation safety data to help Aviation Safety officers with their analysis task. This effort led to the creation of a tool called the “Aviation Safety Data Mining Workbench”. This paper describes the research effort, the workbench, the experience with data mining of Aviation Safety data, and lessons learned.

#index 334002
#* The nimble integration engine
#@ Denise Draper;Alon Y. Halevy;Daniel S. Weld
#t 2001
#c 5
#% 210176
#% 229827
#% 237184
#% 261741
#% 479452
#% 481923
#! The consensus that XML has become the de facto standard for data interchange will spur demand for technology that allows users to integrate data from a variety of applications, repositories, and legacy systems which are located across the corporate intranet or at partner companies on the Internet.In the past two years, Nimble Technology has developed a product for this market. Spawned from over a person-decade of data integration research, the product has been deployed at several Fortune-500 beta-customer sites. This abstract reports on the key challenges we faced in the design of our product and highlights some issues we think require more attention from the research community.

#index 334003
#* Data management: lasting impact on wild, wild, web
#@ Reed M. Meseck
#t 2001
#c 5
#! This paper describes some of the ways the Internet and World Wide Web have affected databases and data warehousing and the lasting impact in these areas.

#index 334006
#* Orthogonal optimization of subqueries and aggregation
#@ César Galindo-Legaria;Milind Joshi
#t 2001
#c 5
#% 83537
#% 116043
#% 116090
#% 123589
#% 220425
#% 287005
#% 461897
#% 461924
#% 479460
#% 480091
#% 481288
#% 481608
#% 562122
#% 565457
#! There is considerable overlap between strategies proposed for subquery evaluation, and those for grouping and aggregation. In this paper we show how a number of small, independent primitives generate a rich set of efficient execution strategies —covering standard proposals for subquery evaluation suggested in earlier literature. These small primitives fall into two main, orthogonal areas: Correlation removal, and efficient processing of outerjoins and GroupBy. An optimization approach based on these pieces provides syntax-independence of query processing with respect to subqueries, i. e. equivalent queries written with or without subquery produce the same efficient plan.We describe techniques implemented in Microsoft SQL Server (releases 7.0 and 8.0) for queries containing sub-queries and/or aggregations, based on a number of orthogonal optimizations. We concentrate separately on removing correlated subqueries, also called “query flattening,” and on efficient execution of queries with aggregations. The end result is a modular, flexible implementation, which produces very efficient execution plans. To demonstrate the validity of our approach, we present results for some queries from the TPC-H benchmark. From all published TPC-H results in the 300GB scale, at the time of writing (November 2000), SQL Server has the fastest results on those queries, even on a fraction of the processors used by other systems.

#index 334007
#* Exploiting constraint-like data characterizations in query optimization
#@ Parke Godfrey;Jarek Gryz;Calisto Zuzarte
#t 2001
#c 5
#% 54047
#% 69272
#% 82346
#% 83315
#% 152585
#% 167258
#% 169370
#% 210169
#% 210190
#% 232151
#% 264857
#% 442678
#% 452824
#% 462472
#% 462942
#% 463761
#% 464837
#% 465163
#% 479814
#% 480964
#% 481266
#% 481749
#% 499681
#! Query optimizers nowadays draw upon many sources of information about the database to optimize queries. They employ runtime statistics in cost-based estimation of query plans. They employ integrity constraints in the query rewrite process. Primary and foreign key constraints have long played a role in the optimizer, both for rewrite opportunities and for providing more accurate cost predictions. More recently, other types of integrity constraints are being exploited by optimizers in commercial systems, for which certain semantic query optimization techniques have now been implemented.These new optimization strategies that exploit constraints hold the promise for good improvement. Their weakness, however, is that often the “constraints” that would be useful for optimization for a given database and workload are not explicitly available for the optimizer. Data mining tools can find such “constraints” that are true of the database, but then there is the question of how this information can be kept by the database system, and how to make this information available to, and effectively usable by, the optimizer.We present our work on soft constraints in DB2. A soft constraint is a syntactic statement equivalent to an integrity constraint declaration. A soft constraint is not really a constraint, per se, since future updates may undermine it. While a soft constraint is valid, however, it can be used by the optimizer in the same way integrity constraints are. We present two forms of soft constraint: absolute and statistical. An absolute soft constraint is consistent with respect to the current state of the database, just in the same way an integrity constraint must be. They can be used in rewrite, as well as in cost estimation. A statistical soft constraint differs in that it may have some degree of violation with respect to the state of the database. Thus, statistical soft constraints cannot be used in rewrite, but they can still be used in cost estimation.We are working long-term on absolute soft constraints. We discuss the issues involved in implementing a facility for absolute soft constraints in a database system (and in DB2), and the strategies that we are researching. The current DB2 optimizer is more amenable to adding facilities for statistical soft constraints. In the short-term, we have been implementing pathways in the optimizer for statistical soft constraints. We discuss this implementation.

#index 334009
#* Fast-Start: quick fault recovery in oracle
#@ Tirthankar Lahiri;Amit Ganesh;Ron Weiss;Ashok Joshi
#t 2001
#c 5
#% 117
#% 248843
#% 479468
#% 479480
#% 479641
#% 481264
#% 481937
#! Availability requirements for database systems are more stringent than ever before with the widespread use of databases as the foundation for ebusiness. This paper highlights Fast-Start™ Fault Recovery, an important availability feature in Oracle, designed to expedite recovery from unplanned outages. Fast-Start allows the administrator to configure a running system to impose predictable bounds on the time required for crash recovery. For instance, fast-start allows fine-grained control over the duration of the roll-forward phase of crash recovery by adaptively varying the rate of checkpointing with minimal impact on online performance. Persistent transaction locking in Oracle allows normal online processing to be resumed while the rollback phase of recovery is still in progress, and fast-start allows quick and transparent rollback of changes made by uncommitted transactions prior to a crash.

#index 334010
#* Dissemination of dynamic data
#@ Pavan Deolasee;Amol Katkar;Ankur Panchbudhe;Krithi Ramamritham;Prashant Shenoy
#t 2001
#c 5
#% 330682

#index 334012
#* COSIMA- your smart, speaking E-salesperson
#@ Werner Kieβling;Stefan Holland;Stefan Fischer;Thorsten Ehm
#t 2001
#c 5
#! We present a new cooperative user interface for e-shopping. COSIMA is an intelligent Internet avatar with dynamic voice output that assists customers through their e-shopping tours and advises them like a real salesperson. COSIMA uses a meta search engine based on Preference SQL, computing best matching results to the customer's wishes. COSIMA can qualify these results and generates proper voice output. Our presentation shows COSIMA in action for comparison shopping.

#index 334013
#* REVIEW: a real-time virtual walkthrough system
#@ L. Shou;C. H. Chionh;Z. Huang;Y. Ruan;Kian-Lee Tan
#t 2001
#c 5

#index 334015
#* Kweelt: more than just “yet another framework to query XML!”
#@ Arnaud Sahuguet
#t 2001
#c 5
#% 504578

#index 334017
#* Monitoring business processes through event correlation based on dependency model
#@ Asaf Adii;David Botzer;Opher Etzion;Tali Yatzkar-Haham
#t 2001
#c 5
#! Events are at the core of reactive and proactive applications, which have become popular in many domains.This demo shows the monitoring of incoming events as a means to detect possible problems in the course of business processes using a dependency model.Contemporary modeling tools lack the capability to express the event semantics and relationships to other entities. This capability is useful when the events are based on a dependency model among business processes, applications and resources. The ability to express an event by employing a general dependency model, an to use it through a designated event correlation monitoring tool, enables the accomplishment of tasks such as impact analysis and business processes monitoring, including prediction of violation of constraints (such as: service level agreements).This demonstrated tool provides the system designer with the ability to define and describe events and their relationships to other events, objects and tasks. The model employs various conditional dependencies that are specific to the event domain. The demo shows how systems (business processes) are monitored using the dependency / event model, by applying rules using an event correlation engine with strong expressive power.This demo proposal describes the generic application development tool, the middleware architecture and framework and the demo.

#index 334019
#* VQBD: exploring semistructured data
#@ Sudarshan S. Chawathe;Thomas Baby;Jihwang Yoo
#t 2001
#c 5

#index 334020
#* OminiSearch: a method for searching dynamic content on the Web
#@ David Buttler;Ling Liu;Calton Pu;Henrique Paques;Wei Han;Wei Tang
#t 2001
#c 5

#index 334022
#* Securing XML documents: the author-X project demonstration
#@ Elisa Bertino;Silvana Castano;Elena Ferrari
#t 2001
#c 5

#index 334023
#* Sangam - a solution to support multiple data models, their mappings and maintenance
#@ Kajal T. Claypool;Elke A. Rundensteiner;Xin Zhang;Su Hong;Harumi Kuno;Wang-chien Lee;Gail Mitchell
#t 2001
#c 5
#% 664055

#index 334025
#* Clio: a semi-automatic tool for schema mapping
#@ Mauricio A. Hernández;Renée J. Miller;Laura M. Haas
#t 2001
#c 5
#% 333988
#% 480134

#index 334026
#* Materialized view and index selection tool for Microsoft SQL server 2000
#@ Sanjay Agrawal;Surajit Chaudhuri;Vivek Narasayya
#t 2001
#c 5
#% 248815
#% 480158
#% 482100

#index 334028
#* The prototype of the DARE system
#@ Tiziana Catarci;Giuseppe Santucci
#t 2001
#c 5
#% 587706

#index 334030
#* Fault-tolerant, load-balancing queries in telegraph
#@ Mehul A. Shah;Sirish Chandrasekaran
#t 2001
#c 5
#% 248795
#% 248813
#% 271664
#% 300127
#% 300167

#index 334031
#* Snowball: a prototype system for extracting relations from large text collections
#@ Eugene Agichtein;Luis Gravano;Jeff Pavel;Viktoriya Sokolova;Aleksandr Voskoboynik
#t 2001
#c 5
#% 301241
#% 504443

#index 334033
#* PBIR - perception-based image retrieval
#@ Edward Chang;Kwang-Ting Cheng;Lihyuarn L. Chang
#t 2001
#c 5
#% 443517
#% 589970
#! We demonstrate a system that we have built on our proposed perception-based image retrieval (PBIR) paradigm. This PBIR system achieves accurate similarity measurements by rooting image characterization in human perception and by learning user's query concept through an intelligent sampling process. We show that our system can usually grasp a user's query concept with a small number of labeled instances.

#index 334034
#* Spatial data management for computer aided design
#@ Hans-Peter Kriegel;Andreas Müller;Marco Pötke;Thomas Seidl
#t 2001
#c 5
#% 279885
#% 480299
#% 530352
#! This demonstration presents a spatial database integration for novel CAD applications into off-the-shelf database systems. Spatial queries on even large product databases for digital mockup or haptic rendering are performed at interactive response times.

#index 334035
#* RETINA: a real-time traffic navigation system
#@ Kam-Yiu Lam;Edward Chan;Tei-Wei Kuo;S. W. Ng;Dick Hung
#t 2001
#c 5

#index 334036
#* Dynamic content acceleration: a caching solution to enable scalable dynamic Web page generation
#@ Anindya Datta;Kaushik Dutta;Krithi Ramamritham;Helen Thomas;Debra VanderMeer
#t 2001
#c 5

#index 334038
#* Lots o'Ticks: real time high performance time series queries on billions of trades and quotes
#@ Arthur Whitney;Dennis Shasha
#t 2001
#c 5
#% 503878
#! Financial mathematicians think they can predict the future by looking at time series of trades and quotes (called ticks) from the past. The main evidence for this hypothesis is that prices fluctuate only by a small amount in a given day and more or less obey the mathematics of a random walk. The hypothesis allows traders to price options and to speculate on stocks. This demonstration presents a query language and a parallel database (50-way parallelism) to support traders who want to analyze every tick, not just end-of-day ticks, using temporal statistical queries such as time-delayed correlations and tick trends. This is the first attempt that we know of to store and analyze hundreds of gigabytes of time series data and to query that data using a declarative time series extension to SQL (available at www.kx.com).

#index 334041
#* DNA-miner: a system prototype for mining DNA sequences
#@ Jiawei Han;Hasan Jamil;Ying Lu;Liangyou Chen;Yaqin Liao;Jian Pei
#t 2001
#c 5

#index 334043
#* DyDa: data warehouse maintenance in fully concurrent environments
#@ Jun Chen;Xin Zhang;Songting Chen;Andreas Koeller;Elke A. Rundensteiner
#t 2001
#c 5

#index 334046
#* XML data management (panel session): go native or spruce up relational systems?
#@ Per-Åke Larson;Dana Florescu;Goetz Graefe;Guido Moerkotte;Hamid Pirahesh;Harald Schöning
#t 2001
#c 5
#! XML data is likely to be widely used as a data exchange format but users also need to store and query XML data. The purpose of this panel is to explore whether and how to best provide this functionality.

#index 334050
#* Will database researchers have any role in data security? (panel session)
#@ Arnon Rosenthal;Klaus Dittrich;Jim Donahue;Bill Maimone
#t 2001
#c 5
#! Data security issues today go far beyond the traditional questions of grant/revoke in an RDBMS. We will discuss what the new research agenda should be.

#index 334052
#* Application servers (panel session): born-again TP monitors for the Web
#@ C. Mohan;Larry Cable;Matthieu Devin;Scott Dietzen;Pat Helland;Dan Wolfson
#t 2001
#c 5

#index 334053
#* Online query processing: a tutorial
#@ Peter J. Haas;Joseph M. Hellerstein
#t 2001
#c 5

#index 334059
#* Time series similarity measures and time series indexing (abstract only)
#@ Dimitrios Gunopulos;Gautam Das
#t 2001
#c 5
#! Time series is the simplest form of temporal data. A time series is a sequence of real numbers collected regularly in time, where each number represents a value. Time series data come up in a variety of domains, including stock market analysis, environmental data, telecommunications data, medical data and financial data. Web data that count the number of clicks on given cites, or model the usage of different pages are also modeled as time series. Therefore time series account for a large fraction of the data stored in commercial databases. There is recently increasing recognition of this fact, and support for time series as a different data type in commercial data bases management systems is increasing. IBM DB2 for example implements support for time series using data-blades.The pervasiveness and importance of time series data has sparked a lot of research work on the topic. While the statistics literature on time series is vast, it has not studied methods that would be appropriate for the time series similarity and indexing problems we discuss here; much of the relevant work on these problems has been done by the computer science community.One interesting problem with time series data is finding whether different time series display similar behavior. More formally, the problem can be stated as: Given two time series X and Y, determine whether they are similar or not (in other words, define and compute a distance function dist(X, Y)). Typically each time series describes the evolution of an object, for example the price of a stock, or the levels of pollution as a function of time at a given data collection station. The objective can be to cluster the different objects to similar groups, or to classify an object based on a set of known object examples. The problem is hard because the similarity model should allow for imprecise matches. One interesting variation is the subsequence similarity problem, where given two time series X and Y, we have to determine those subsequences of X that are similar to pattern Y. To answer these problems, different notions of similarity between time series have been proposed in data mining research.In the tutorial we examine the different time series similarity models that have been proposed, in terms of efficiency and accuracy. The solutions encompass techniques from a wide variety of disciplines, such as databases, signal processing, speech recognition, pattern matching, combinatorics and statistics. We survey proposed similarity techniques, including the Lp norms, time warping, longest common subsequence measures, baselines, moving averaging, or deformable Markov model templates.Another problem that comes up in applications is the indexing problem: given a time series X, and a set of time series S = {Y1,…,YN}, find the time series in S that are most similar to the query X. A variation is the subsequence indexing problem, where given a set of sequences S, and a query sequence (pattern) X, find the sequences in S that contain subsequences that are similar to X. To solve these problems efficiently, appropriate indexing techniques have to be used. Typically, the similarity problem is related to the indexing problem: simple (and possibly inaccurate) similarity measures are usually easy to build indexes for, while more sophisticated similarity measures make the indexing problem hard and interesting.We examine the indexing techniques that can be used for different models, and the dimensionality reduction techniques that have been proposed to improve indexing performance. A time series of length n can be considered as a tuple in an n-dimensional space. Indexing this space directly is inefficient because of the very high dimensionality. The main idea to improve on it is to use a dimensionality reduction technique that takes the n item long time series, and maps it to a lower dimensional space with k dimensions (hopefully, k n).We give a detailed description of the most important techniques used for dimensionality reduction. These include: the SVD decomposition, the Fourier transform (and the similar Discrete Cosine transform), the Wavelet decomposition, Multidimensional Scaling, random projection techniques, FastMap (and variants), and Linear partitioning. These techniques have specific strengths and weaknesses, making some of them better suited for specific applications and settings.Finally we consider extensions to the problem of indexing subsequences, as well as to the problem of finding similar high-dimensional sequences, such as trajectories or video frame sequences.

#index 334061
#* Semantic B2B integration
#@ Christoph Bussler
#t 2001
#c 5
#! The tutorial “Semantic B2B Integration” will give an introduction to the field of business-to-business (B2B) integration from a technical viewpoint with the focus on semantic integration aspects. The set of B2B integration concepts is introduced as well as their implementation in form of a technical semantic B2B integration architecture. A mix of examples is taken illustrating the problems that need to be solved in semantic B2B integration projects. The tutorial enables the audience to identify semantic B2B integration problems as well as to determine the benefits and deficiencies of various technical integration architecture approaches or B2B integration technologies.

#index 334063
#* Models and languages for describing and discovering E-services
#@ Fabio Casati;Ming-Chien Shan
#t 2001
#c 5

#index 334065
#* Standard for multimedia databases
#@ John R. Smith
#t 2001
#c 5
#% 1857848
#! The Moving Picture Experts Group (MPEG) is developing a new standard called the “Multimedia Content Description Interface,” also known as MPEG-7. The goal of MPEG-7 is to enable fast and effective searching and filtering of multimedia content. The effort is being driven by requirements taken from a large number of applications related to multimedia databases, interactive media services (music, TV programs), video libraries, and so forth. MPEG-7 is achieving this goal by developing an XML-Schema based standard for describing features of multimedia content. In this tutorial, we study the emerging MPEG-7 standard and describe the new challenges for MPEG-7 multimedia databases.

#index 339349
#* Advances in real-time database systems research
#@ Azer Bestavros
#t 1996
#c 5
#% 24871
#% 37968
#% 37972
#% 37973
#% 37974
#% 42882
#% 77988
#% 77990
#% 83122
#% 86938
#% 112163
#% 116053
#% 117903
#% 124815
#% 124816
#% 124818
#% 158051
#% 158898
#% 166509
#% 172910
#% 187413
#% 198775
#% 199027
#% 268791
#% 339355
#% 339357
#% 339359
#% 339362
#% 339363
#% 339364
#% 339365
#% 339366
#% 339372
#% 442012
#% 463253
#% 463279
#% 480266
#% 480436
#% 480766
#% 481585
#% 481601
#% 511025
#% 614960
#% 615509
#% 615514
#% 692762

#index 339355
#* Integrating temporal, real-time, an active databases
#@ Krithi Ramamritham;Raju Sivasankaran;John A. Stankovic;Don T. Towsley;Ming Xiong
#t 1996
#c 5
#% 37972
#% 77988
#% 86939
#% 158051
#% 480766
#% 571215
#! To meet the needs of many real-world control applications, concepts from Temporal, Real-Time, and Active Databases must be integrated:Since the system's data is supposed to reflect the environment being controlled, they must be updated frequently to maintain temporal validity;Many activities, including those that perform the updates, work under time constraints;The occurrence of events, for example, emergency events, trigger actions.In these systems, meeting timeliness, predictability, and QoS guarantee requirements — through appropriate resource and overload management — become very important. So, algorithms and protocols for concurrency control, recovery, and scheduling are needed. These algorithms must exploit semantics of the data and the transactions to be responsive and efficient. Whereas time cognizant scheduling, concurrency control and conflict resolution have been studied in the literature, recovery issues have not. We have developed strategies for data placement at the appropriate level of memory hierarchy, for avoiding undoing/redoing by exploiting data/transaction characteristics, and for placing logs at the appropriate level in the memory hierarchy. Another issue that we have studied deals with the assignment of priority to transactions in active real-time database systems. We are also studying concurrency control for temporal and multi-media data. We have built RADEx, a simulation environment to evaluate our solutions.

#index 339357
#* Real-time index concurrency control
#@ Jayant R. Haritsa;S. Seshadri
#t 1996
#c 5
#% 36118
#% 102808
#% 116087
#% 124815
#% 172910
#% 268786
#% 286929
#% 291189
#% 317933
#% 481601
#% 561427

#index 339359
#* Real-time database — similarity and resource scheduling
#@ Tei-Wei Kuo;Aloysius K. Mok
#t 1996
#c 5
#% 3645
#% 43206
#% 200675
#% 286967
#% 288821
#% 442012
#% 614974
#! While much recent work has focussed on the performance of transaction systems where individual transactions have deadlines, our research addresses the semantics of data usage in real-time applications and its integration with real-time resource management, in particular, the timeless value of real-time data and the inherent path and not state-based constraints on concurrency control. Central to our research is the idea of similarity which is a reflexive, symmetric relation over the domain of a data object. By exploiting the similarity relation, we propose a class of efficient data-access policies for real-time data objects. We shall also discuss the design of a distributed real-time data-access interface. Our goal is to build a database facility which can support predictable real-time applications involving high-speed communication, information access, and multimedia.

#index 339362
#* Exploiting main memory DBMS features to improve real-time concurrency control protocols
#@ Özgür Ulusoy;Alejandro Buchmann
#t 1996
#c 5
#% 32885
#% 112163
#% 117903
#% 442832

#index 339363
#* Enhancing external consistency in real-time transactions
#@ Kwei-Jay Lin;Ching-Shan Peng
#t 1996
#c 5
#% 201922
#% 442012

#index 339364
#* Improving timeliness in real-time secure database systems
#@ Sang H. Son;Rasikan David;Bhavani Thuraisingham
#t 1996
#c 5
#% 9241
#% 112163
#% 117903
#% 124818
#% 149627
#% 194950
#% 238731
#% 322637
#% 462628
#% 615194
#% 664471
#% 664535
#! Database systems for real-time applications must satisfy timing constraints associated with transactions, while maintaining data consistency. In addition to real-time requirements, security is usually required in many applications. Multilevel security requirements introduce a new dimension to transaction processing in real-time database systems. In this paper, we argue that because of the complexities involved, trade-offs need to be made between security and timeliness. We briefly present the secure two-phase locking protocol and discuss an adaptive method to support trading off security for timeliness, depending on the current state of the system. The performance of the adaptive secure two-phase locking protocol shows improved timeliness. We also discuss future research direction to improve timeliness of secure database systems.

#index 339365
#* DeeDS towards a distributed and active real-time database system
#@ S. F. Andler;J. Hansson;J. Eriksson;J. Mellin;M. Berndtsson;B. Eftring
#t 1996
#c 5
#% 69084
#% 177557
#% 177567
#% 177755
#% 187413
#% 319290
#% 440783
#% 442832
#% 464043
#% 562027

#index 339366
#* Overview of the STanford Real-time Information Processor (STRIP)
#@ Brad Adelberg;Ben Kao;Hector Garcia-Molina
#t 1996
#c 5
#% 201922
#% 458544
#! We believe that the greatest growth potential for soft real-time databases is not as isolated monolithic databases but as components in open systems consisting of many heterogenous databases. In such environments, the flexibility to deal with unpredictable situations and the ability to cooperate with other databases (often non-real-time databases) is just as important as the guarantee of stringent timing constraints. In this paper, we describe a database designed explicitly for heterogeneous environments, the STanford Real-time Information Processor (STRIP). STRIP, which runs on standard Posix Unix, is a soft real-time main memory database with special facilities for importing and exporting data as well as handling derived data. We will describe the architecture of STRIP, its unique features, and its potential uses in overall system architectures.

#index 339369
#* Database research: achievements and opportunities into the 1st century
#@ Avi Silberschatz;Mike Stonebraker;Jeff Ullman
#t 1996
#c 5
#% 111378

#index 339372
#* Workshop report: the first international workshop on active and real-time database systems (ARTDB-95)
#@ Mikael Berndtsson;Jörgen Hansson
#t 1996
#c 5
#% 168765
#% 380644

#index 339373
#* Integrating contents and structure in text retrieval
#@ Ricardo Baeza-Yates;Gonzalo Navarro
#t 1996
#c 5
#% 24624
#% 36188
#% 38696
#% 55234
#% 55490
#% 58366
#% 71755
#% 107243
#% 109188
#% 115462
#% 118743
#% 118744
#% 129659
#% 131392
#% 144038
#% 154334
#% 163445
#% 172924
#% 172927
#% 187659
#% 191574
#% 194254
#% 282431
#% 287434
#% 318455
#% 406493
#% 480084
#! The purpose of a textual database is to store textual documents. These documents have not only textual contents, but also structure. Many traditional text database systems have focused only on querying by contents or by structure. Recently, a number of models integrating both types of queries have appeared. We argue in favor of that integration, and focus our attention on these recent models, covering a representative sampling of the proposals in the field. We pay special attention to the tradeoffs between expressiveness and efficiency, showing the compromises taken by the models. We argue in favor of achieving a good compromise, since being weak in any of these two aspects makes the model useless for many applications.

#index 339374
#* Database research at Arizona State University
#@ Susan D. Urban;Suzanne W. Dietrich;Forouzan Golshani
#t 1996
#c 5
#% 124666
#% 126371
#% 150462
#% 173638
#% 190650
#% 212913
#% 222158
#% 434687
#% 461891
#% 463568
#% 535831

#index 339375
#* Lifestreams: a storage model for personal data
#@ Eric Freeman;David Gelernter
#t 1996
#c 5
#% 55249
#% 107693
#% 108512
#% 199528
#% 224305
#% 318453
#% 319244
#% 441058
#! Conventional software systems, such as those based on the “desktop metaphor,” are ill-equipped to manage the electronic information and events of the typical computer user. We introduce a new metaphor, Lifestreams, for dynamically organizing a user's personal workspace. Lifestreams uses a simple organizational metaphor, a time-ordered stream of documents, as an underlying storage system. Stream filters are used to organize, monitor and summarize information for the user. Combined, they provide a system that subsumes many separate desktop applications. This paper describes the Lifestreams model and our prototype system.

#index 339376
#* Object query standards
#@ Andrew E. Wade
#t 1996
#c 5
#! As object technology is adopted by software systems for analysis and design, language, GUI, and frameworks, the database community also is working to support objects, and to develop standards for that support. A key benefit of object technology is the ability for different objects and object tools to interoperate, so it's critical that such DBMS object standards interoperate with those of the rest of the object world. Starting with a discussion of the new issues objects bring to query standards, we present the efforts of various groups relevant to this, including ODMG, OMG, ANSI X3H2 (SQL3), and recent merger efforts feeding into SQL3.What's different with Objects?ODMG's OQLOMG's Query ServiceSQL3's Object extensionsEfforts to merge

#index 339377
#* Database research at the Indian Institute of Technology, Bombay
#@ D. B. Phatak;N. L. Sarda;S. Seshadri;S. Sudarshan
#t 1996
#c 5
#% 137885
#% 163440
#% 183944
#% 194116
#% 205092
#% 210353
#% 339357
#% 481441
#% 481449
#% 481601
#% 481749

#index 339378
#* An Illustra technical white paper
#@ John Gaffney
#t 1996
#c 5
#! Illustra's Web DataBlade module is a comprehensive toolset for creating Web-enabled database applications that dynamically retrieve and update Illustra database content. You can construct simple query front ends in a matter of minutes and powerful Web applications in just a few hours with the Web DataBlade module. The Illustra Web DataBlade makes it easy for you to take full advantage of the Illustra server's many important features, including extensible data types, an underlying rules system, and Time Travel capabilities, all of which make Illustra the database of choice for managing all types of content on the World Wide Web.

#index 339379
#* MQseries and CICS link for Lotus Notes
#@ Lotus Development Corp.
#t 1996
#c 5

#index 339380
#* Shutdown, budget, and funding
#@ Xiaolei Qian
#t 1996
#c 5
#! There are few new funding announcements and requests for proposals, mostly due to the partial government shutdown and the budget impasse. We will report on the potential impact on NSF of the government shutdown and a 7-year balanced budget. We then briefly discuss some BAAs from ARPA, Rome Laboratory, and the Air Force.

#index 340309
#* On the effects of dimensionality reduction on high dimensional similarity search
#@ Charu C. Aggarwal
#t 2001
#c 5
#% 68091
#% 201876
#% 201893
#% 248027
#% 248798
#% 273699
#% 280822
#% 300131
#% 310509
#% 427199
#% 435141
#% 479649
#% 480093
#% 480132
#% 480307
#% 481956
#! The dimensionality curse has profound effects on the effectiveness of high-dimensional similarity indexing from the performance perspective. One of the well known techniques for improving the indexing performance is the method of dimensionality reduction. In this technique, the data is transformed to a lower dimensional space by finding a new axis-system in which most of the data variance is preserved in a few dimensions. This reduction may also have a positive effect on the quality of similarity for certain data domains such as text. For other domains, it may lead to loss of information and degradation of search quality. Recent research indicates that the improvement for the text domain is caused by the re-enforcement of the semantic concepts in the data. In this paper, we provide an intuitive model of the effects of dimensionality reduction on arbitrary high dimensional problems. We provide an effective diagnosis of the causality behind the qualitative effects of dimensionality reduction on a given data set. The analysis suggests that these effects are very data dependent. Our analysis also indicates that currently accepted techniques of picking the reduction which results in the least loss of information are useful for maximizing precision and recall, but are not necessarily optimum from a qualitative perspective. We demonstrate that by making simple changes to the implementation details of dimensionality reduction techniques, we can considerably improve the quality of similarity search.

#index 341172
#* Reminiscences in influential papers
#@ Richard Snodgrass
#t 1998
#c 5
#% 114582
#% 286929
#% 287670
#% 411663
#% 482070

#index 345741
#* Data management issues in electronic commerce: guest editor's introduction
#@ Asuman Dogac
#t 2002
#c 5

#index 345742
#* The design and performance evaluation of alternative XML storage strategies
#@ Feng Tian;David J. DeWitt;Jianjun Chen;Chun Zhang
#t 2002
#c 5
#% 172939
#% 227875
#% 236416
#% 237191
#% 273922
#% 479956
#% 481125
#! This paper studies five strategies for storing XML documents including one that leaves documents in the file system, three that use a relational database system, and one that uses an object manager. We implement and evaluate each approach using a number of XQuery queries. A number of interesting insights are gained from these experiments and a summary of the advantages and disadvantages of the approaches is presented.

#index 345743
#* Supporting global user profiles through trusted authorities
#@ Ibrahim Cingil
#t 2002
#c 5
#% 275360
#% 308768
#! Personalization generally refers to making a Web site more responsive to the unique and individual needs of each user. We argue that for personalization to work effectively, detailed and interoperable user profiles should be globally available for authorized sites, and these profiles should dynamically reflect the changes in user interests.Creating user profiles from user click-stream data seems to be an effective way of generating detailed and dynamic user profiles. However a user profile generated in this way is available only on the computer where the user accesses his browser, and is inaccessable when the same user works on a different computer. On the other hand, the integration of Internet with telecommunication networks have made it possible for the users to connect to Web with a variety of mobile devices as well as desk tops. This requires that user profiles should be available to any desktop or mobile device on the Internet that users choose to work with.In this paper, we address these problems through the concept of "Trusted Authority". A user agent at the client side that captures the user click stream, dynamically generates a navigational history 'log' file in Extensible Markup Language (XML). This log files is then used to produce the 'user profiles' in Resource Description Framework (RDF). User's right to privacy is provided through the Platform for Privacy Preferences (P3P) standard. User profiles are uploaded to the trusted authority and served next time the user connects to the Web.The trusted authority concept, serving as a namespace qualifier, provides globally unique userid/password identification for users. Furthermore user profiles dynamically reflect the changes in their interests since the data generated while they are surfing the Web contribute to their profile. Also since the user profiles are defined in RDF, they are interoperable and available to any type of authorized device on the Internet.

#index 345744
#* The n-tier hub technology
#@ Rainer A. Sommer;Thomas R. Gulledge;David Bailey
#t 2002
#c 5
#% 315847
#% 448037
#! During 2001, the Enterprise Engineering Laboratory at George Mason University was contracted by the Boeing Company to develop an eHub capability for aerospace suppliers in Taiwan. In a laboratory environment, the core technology was designed, developed, and tested, and now a large first-tier aerospace supplier in Taiwan is commercializing the technology. The project objective was to provide layered network and application services for transporting XML-based business transaction flows across multi-tier, heterogeneous data processing environments. This paper documents the business scenario, the eHub application, and the network transport mechanisms that were used to build the n-tier hub. Contrary to most eHubs, this solution takes the point of view of suppliers, pushing data in accordance with supplier requirements; hence, enhancing the probability of supplier adoption. The unique contribution of this project is the development of an eHub that meets the needs of Small and Medium Enterprises (SMEs) and first-tier suppliers.

#index 345745
#* An active functionality service for e-business applications
#@ M. Cilia;A. P. Buchmann
#t 2002
#c 5
#% 201897
#% 246000
#% 385995
#% 544951
#% 591579
#% 665399
#! Service based architectures are a powerful approach to meet the fast evolution of business rules and the corresponding software. An active functionality service that detects events and involves the appropriate business rules is a critical component of such a service-based middleware architecture. In this paper we present an active functionality service that is capable of detecting events in heterogeneous environments, it uses an integral ontology-based approach for the semantic interpretation of heterogeneous events and data, and provides notifications through a publish/subscribe notification mechanism. The power of this approach is illustrated with the help of an auction application and through the personalization of car and driver portals in Internet-enabled vehicles.

#index 345746
#* Contracting in the days of eBusiness
#@ W. Hümmer;W. Lehner;H. Wedekind
#t 2002
#c 5
#% 146399
#% 168251
#% 266736
#% 390905
#% 462221
#! Putting electronic business on a sound foundation --- model theoretically as well as technologically --- has to be seen as a central challenge for research as well as for commercial development. This paper concentrates on the discovery and the negotiation phase of concluding an agreement based on a contract. We present a methodology how to come seamlessly from a many-to-many relationship in the discovery phase to a one-to-one relationship in the contract negotiation phase. Making the content of the contracts persistent is achieved by reconstructing contract templates by means of mereologic (logic of the whole-part relation). Possibly nested sub-structures of the contract template are taken as a basis for negotiation in a dialogical way. For the negotiation itself the contract templates are extended by implications (logical) and sequences (topical).

#index 345747
#* Conceptual modeling and specification generation for B2B business processes based on ebXML
#@ HyoungDo Kim
#t 2002
#c 5
#% 1393716
#! In order to support dynamic setup of business processes among independent organizations, a formal standard schema for describing the business processes is basically required. The ebXML framework provides such a specification schema called BPSS (Business Process Specification Schema) which is available in two stand-alone representations: a UML version, and an XML version. The former, however, is not intended for the direct creation of business process specifications, but for defining specification elements and their relationships required for creating an ebXML-compliant business process specification. For this reason, it is very important to support conceptual modeling that is well organized and directly matched with major modeling concepts. This paper deals with how to represent and manage B2B business processes using UML-compliant diagrams. The major challenge is to organize UML diagrams in a natural way that is well suited with the business process meta-model and then to transform the diagrams into an XML version. This paper demonstrates the usefulness of conceptually modeling business processes by prototyping a business process editor tool called ebDesigner.

#index 345748
#* Semantic B2B integration: issues in ontology-based approaches
#@ Zhan Cui;Dean Jones;Paul O'Brien
#t 2002
#c 5
#% 266237
#% 333406
#% 482855
#! Solving queries to support e-commerce transactions can involve retrieving and integrating information from multiple information resources. Often, users don't care which resources are used to answer their query. In such situations, the ideal solution would be to hide from the user the details of the resources involved in solving a particular query. An example would be providing seamless access to a set of heterogeneous electronic product catalogues. There are many problems that must be addressed before such a solution can be provided. In this paper, we discuss a number of these problems, indicate how we have addressed these and go on to describe the proof-of-concept demonstration system we have developed.

#index 345749
#* Business data management for business-to-business electronic commerce
#@ Christoph Quix;Mareike Schoop;Manfred Jeusfeld
#t 2002
#c 5
#% 85086
#% 223189
#% 328427
#% 348248
#% 443410
#% 545557
#! Business-to-business electronic commerce (B2B EC) opens up new possibilities of trade. For example, new business partners from around the globe can be found, their offers can be compared, even complex negotiations can be conducted electronically, and a contract can be drawn up and fulfilled via an electronic marketplace. However, a sophisticated data management is required to provide such factilities. In this paper, the results of a multi-national project on creating a business-to-business electronic marketplace for small and medium-sized enterprises are presented. Tools for information discovery, protocol-based negotiations, and monitored contract enactment are provided and based on a business data repository. The repository integrates heterogeneous business data with business communication. Specific problems such as multi-linguality, data ownership, and traceability of contracts and related negotiations are addressed and it will be shown that the present approach provides efficient business data management for B2B EC.

#index 345750
#* A multi-agent system infrastructure for software component market-place: an ontological perspective
#@ Riza Cenk Erdur;Oğuz Dikenelli
#t 2002
#c 5
#% 333553
#% 431525
#! In this paper, we introduce a multi-agent system architecture and an implemented prototype for software component market-place. We emphasize the ontological perspective by discussing the ontology modeling for component market-place, UML extensions for ontology modeling, and the idea of ontology transfer which makes the multi-agent system to adapt itself to the dynamically changing ontologies.

#index 345751
#* Supply chain infrastructures: system integration and information sharing
#@ Michael O. Ball;Meng Ma;Louiqa Raschid;Zhengying Zhao
#t 2002
#c 5
#% 232323
#% 273694
#% 294608
#% 322405
#% 376515
#% 383661
#% 479951
#% 657273
#% 657316
#! The need for supply chain integration (SCI) methodologies has been increasing as a consequence of the globalization of production and sales, and the advancement of enabling information technologies. In this paper, we describe our experience with implementing and modeling SCIs. We present the integration architecture and the software components of our prototype implementation. We then discuss a variety of information sharing methodologies. Then, within the framework of a multi-echelon supply chain process model spanning multiple organizations, we summarize research on the benefits of intra-organizational knowledge sharing, and we discuss performance scalability.

#index 345752
#* The role of B2B engines in B2B integration architectures
#@ Christoph Bussler
#t 2002
#c 5
#% 397475
#% 664063
#! Semantic B2B Integration architectures must enable enterprises to communicate standards-based B2B events like purchase orders with any potential trading partner. This requires not only back end application integration capabilities to integrate with e.g. enterprise resource planning (ERP) systems as the company-internal source and destination of B2B events, but also a capability to implement every necessary B2B protocol like Electronic Data Interchange (EDI), RosettaNet as well as more generic capabilities like web services (WS). This paper shows the placement and functionality of B2B engines in semantic B2B integration architectures that implement a generic framework for modeling and executing any B2B protocol. A detailed discussion shows how a B2B engine can provide the necessary abstractions to implement any standard-based B2B protocol or any trading partner specific specialization.

#index 345753
#* Databases and transaction processing: an application-oriented approach
#@ Philip M. Lewis;Arthur Bernstein;Michael Kifer
#t 2002
#c 5

#index 345754
#* Data mining: practical machine learning tools and techniques with Java implementations
#@ Ian H. Witten;Eibe Frank
#t 2002
#c 5

#index 345755
#* What will be
#@ Michael Dertouzos
#t 2002
#c 5

#index 345756
#* Avi Silberschatz speaks out: on academia versus industrial labs, startup fever, database PhD quality, the future of data mining as a research area, IP issues, how he wouldn't change a thing, and more
#@ Marianne Winslett
#t 2002
#c 5

#index 345757
#* The XML typechecking problem
#@ Dan Suciu
#t 2002
#c 5
#% 70235
#% 101940
#% 101943
#% 291299
#% 299942
#% 299944
#% 309851
#% 313640
#% 333857
#% 342438
#% 382963
#% 480152
#% 504575
#% 562461
#% 653704
#! When an XML document conforms to a given type (e.g. a DTD or an XML Schema type) it is called a valid document. Checking if a given XML document is valid is called the validation problem, and is typically performed by a parser (hence, validating parser), more precisely it is performed right after parsing, by the same program module. In practice however XML documents are often generated dynamically, by some program: checking whether all XML documents generated by the program are valid w.r.t. a given type is called the typechecking problem. While a validation analyzes an XML document, a type checker analyzes a program, and the problem's difficulty is a function of the language in which that program is expressed. The XML typechecking problem has been investigated recently in [MSV00, HP00, HVP00, AMN+01a, AMN+01b] and the XQuery Working Group adopted some of these techniques for typechecking XQuery [FFM+01]). All these techniques, however, have limitations which need to be understood and further explored and investigated. In this paper we define the XML typechecking problem, and present current approaches to typechecking, discussing their limitations.

#index 345758
#* Too much middleware
#@ Michael Stonebraker
#t 2002
#c 5
#% 86930
#% 208032
#! The movement from client-server computing to multi-tier computing has created a potpourri of so-called middleware systems, including application servers, workflow products, EAI systems, ETL systems and federated data systems. In this paper we argue that the explosion in middleware has created a myriad of poorly integrated systems with overlapping functionality. The world would be well served by considerable consolidation, and we present some of the ways this might happen. Some of the points covered in this paper have been previously explored in [BERN96].

#index 345759
#* Report on the 18th British National Conference on Databases (BNCOD)
#@ Carole Goble;Brian Read
#t 2002
#c 5
#! The annual series of the British National Conference on Databases has been a forum for UK database practitioners and a focus for database research since 1981. In recent years, interest in this conference series has extended well beyond the UK.BNCOD 2001, the 18th conference in the series, was held at the CLRC Rutherford Appleton Laboratory (RAL) from 9th -11th July 2001. RAL hosts national large-scale facilities for advanced scientific research. The Information Technology Department collaborates with the Laboratory's data centres that manage terabytes of data in remote sensing, high-energy physics and astronomy.BNCOD 2001 included scientific papers, invited talks, a panel and a poster session. The BNCOD Programme Committee, chaired by Professor Carole Goble of Manchester University, selected for presentation at the meeting eleven papers, about one third of those papers submitted. Contributors were drawn from the Netherlands, Germany, Sweden, Canada and USA, as well as the UK. The audience of 60 attendees was chiefly drawn from the UK database community. The Proceedings are published by Springer-Verlag in the Lecture Notes in Computer Science series, and are available online at: http://link.springer.de/link/service/series/0558/tocs/t2097.htm.

#index 345760
#* Advances in databases and information systems: report of 5th East European Conference ADBIS'2001
#@ Albertas Caplinskas;Johann Eder;Olegas Vasilecas
#t 2002
#c 5
#% 330034
#% 454734
#! The 5th East European Conference ADBIS'2001 was organized by the Vilnius Gediminas Technical University, Institute of Mathematics and Informatics (Lithuania), Lithuanian Computer Society in cooperation with Moscow ACM SIGMOD Chapter and Law University of Lithuania in Vilnius, Lithuania, September 25-28, 2001. The call for papers attracted 82 submissions from 30 countries. The international program committee, consisting of 47 researchers from 21 countries, selected 25 papers for long presentations and 19 research communications for regular sessions. Additionally, 9 professional communications and reports have been selected for industrial sessions. The authors of accepted papers come from 29 countries, indicating the truly international recognition of the ADBIS conference series. The conference had 127 registered participants from 23 countries and included invited lectures, tutorials, regular sessions, and industrial sessions. This report describes the goals of the conference and summarizes the issues discussed during the sessions.

#index 345761
#* Why I like working in academia
#@ Richard Thomas Snodgrass
#t 2002
#c 5
#! When Alex Labrinidis asked me to write this essay, I initially balked. I was loathe to speak for academics worldwide, or even just those in SIGMOD. But I then realized that I could speak from personal experience. So these random musings will be of necessity entirely subjective, highly individualistic, and unrepresentative---attributes that a scholar normally attempts to vigorously avoid in his writing. I'm definitely not a "typical" academic (I don't know such an animal), but I can speak with some authority as to what motivates me.As another caveat, I make few comparisons with alternatives such as working in a research lab or as a developer. I won't even attempt to speak for them.The final caveat (distrust all commentaries that start with caveats, but perhaps more so those that don't!) is that my assumed audience comprises students who are considering such a profession. Current academics will find some of my observations trite or may disagree loudly, as academics are oft to do (see below).That said, I have been an academic for exactly twenty years, and I deeply love the academic life. While I have consulted for and written papers with those away from the ivory tower, my professional life has been entirely as a professor. I went forthwith from undergraduate school to graduate school, then directly to the University of North Carolina, then to the University of Arizona, where I am happily ensconced.I open with some disadvantages to this seemingly ideal life, then turn to the advantages. With each I start with those that I expected when I was a doctoral student, and then consider those I (naïvely or otherwise) was not aware of from that early vantage point.

#index 378387
#* Proceedings of the twenty-first ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems
#@ Lucian Popa;Serge Abiteboul;Phokion G. Kolaitis
#t 2002
#c 5
#! This volume contains the proceedings of the Twenty-first ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems (PODS 2002), held at Madison, Wisconsin on June 3-5, 2002 in conjunction with the 2002 ACM SIGMOD International Conference on Management of Data. It consists of a paper based on the invited talk by Rajeev Motwani, two papers based on the invited tutorials by Maurizio Lenzerini and Dennis Shasha, and 24 contributed papers that were selected by the program committee for presentation at the symposium.The contributed papers were selected from 109 submissions. Most of the papers are "extended abstracts" and are preliminary reports on work in progress. While they have been read by program committee members, they have not been formally refereed. It is expected that much of the research described in these papers will be publihsed in detail in computer science journals.The program committee selected "Monadic Datalog over Trees and the Expressive Power of Languages for Web Information Extraction" by Georg Gottlob and Christoph Koch for the PODS 2002 Best Paper Award and "From Discrepancy to Declustering: Near optimal multidimensional declustering strategies for range queries" by Chung-Min Chen and Christine T. Cheng for the PODS 2002 Best Newcomer Award. Warmest congratulations to the authors of these papers.

#index 378388
#* Models and issues in data stream systems
#@ Brian Babcock;Shivnath Babu;Mayur Datar;Rajeev Motwani;Jennifer Widom
#t 2002
#c 5
#% 1331
#% 36117
#% 59350
#% 116082
#% 172949
#% 172950
#% 190611
#% 198467
#% 214073
#% 227883
#% 234797
#% 238182
#% 248820
#% 248821
#% 248822
#% 259995
#% 273682
#% 273902
#% 273907
#% 273908
#% 273909
#% 273911
#% 286256
#% 293712
#% 299989
#% 300167
#% 300179
#% 300195
#% 302724
#% 310488
#% 310500
#% 310900
#% 333926
#% 333931
#% 333955
#% 333982
#% 336610
#% 338394
#% 338425
#% 340301
#% 342600
#% 347223
#% 347226
#% 378408
#% 379443
#% 379444
#% 379445
#% 397352
#% 397353
#% 397354
#% 428155
#% 443298
#% 458556
#% 464058
#% 479621
#% 479648
#% 479795
#% 479984
#% 480120
#% 480296
#% 480306
#% 480465
#% 480628
#% 480768
#% 481749
#% 481943
#% 482088
#% 482100
#% 504019
#% 593957
#% 594012
#% 594029
#% 660003
#% 660004
#! In this overview paper we motivate the need for and research issues arising from a new model of data processing. In this model, data does not take the form of persistent relations, but rather arrives in multiple, continuous, rapid, time-varying data streams. In addition to reviewing past work relevant to data stream systems and current projects in the area, the paper explores topics in stream query languages, new requirements and challenges in query processing, and algorithmic issues.

#index 378389
#* Monadic datalog and the expressive power of languages for web information extraction
#@ Georg Gottlob;Christoph Koch
#t 2002
#c 5
#% 36683
#% 49315
#% 73005
#% 101944
#% 237192
#% 237194
#% 241136
#% 241160
#% 241166
#% 255197
#% 275922
#% 299976
#% 331772
#% 342829
#% 401124
#% 404772
#% 424931
#% 459241
#% 464825
#% 474871
#% 480648
#% 614598
#! Research on information extraction from Web pages (wrapping) has seen much activity in recent times (particularly systems implementations), but little work has been done on formally studying the expressiveness of the formalisms proposed or on the theoretical foundations of wrapping.In this paper, we first study monadic datalog as a wrapping language (over ranked or unranked tree structures). Using previous work by Neven and Schwentick, we show that this simple language is equivalent to full monadic second order logic (MSO) in its ability to specify wrappers. We believe that MSO has the right expressiveness required for Web information extraction and thus propose MSO as a yardstick for evaluating and comparing wrappers.Using the above result, we study the kernel fragment Elog- of the Elog wrapping language used in the Lixto system (a visual wrapper generator). The striking fact here is that Elog- exactly captures MSO, yet is easier to use. Indeed, programs in this language can be entirely visually specified. We also formally compare Elog to other wrapping languages proposed in the literature.

#index 378390
#* From discrepancy to declustering: near-optimal multidimensional declustering strategies for range queries
#@ Chung-Min Chen;Christine T. Cheng
#t 2002
#c 5
#% 43179
#% 45766
#% 153400
#% 215403
#% 286962
#% 299983
#% 319327
#% 339622
#% 387088
#% 443200
#% 443374
#% 461922
#% 462233
#% 463598
#% 464718
#% 479936
#% 480794
#% 481109
#% 609704
#% 632069
#% 637794
#% 664833
#! Declustering schemes allocate data blocks among multiple disks to enable parallel retrieval. Given a declustering scheme D, its response time with respect to a query Q, rt(Q), is defined to be the maximum number of disk blocks of the query stored by the scheme in any one of the disks. If &verbar;Q&verbar; is the number of data blocks in Q and M is the number of disks then rt(Q) is at least &verbar;Q&verbar;/M. One way to evaluate the performance of D with respect to a set of queries &#119876; is to measure its additive error - the maximum difference between rt(Q) from &verbar;Q&verbar;/M over all range queries Q ε &#119876;.In this paper, we consider the problem of designing declustering schemes for uniform multidimensional data arranged in a d-dimensional grid so that their additive errors with respect to range queries are as small as possible. It has been shown that such declustering schemes will have an additive error of Ω(log M) when d = 2 and Ω(log d-1/2 M) when d 2 with respect to range queries.Asymptotically optimal declustering schemes exist for 2-dimensional data. For data in larger dimensions, however, the best bound for additive errors is O(Md-1), which is extremely large. In this paper, we propose the two declustering schemes based on low discrepancy points in d-dimensions. When d is fixed, both schemes have an additive error of O(logd-1 M) with respect to range queries provided certain conditions are satisfied: the first scheme requires d ≥ 3 and M to be a power of a prime where the prime is at least d while the second scheme requires the size of the data to grow within some polynomial of M, with no restriction on M. These are the first known multidimensional declustering schemes with additive errors near optimal.

#index 378391
#* Algorithmics and applications of tree and graph searching
#@ Dennis Shasha;Jason T. L. Wang;Rosalba Giugno
#t 2002
#c 5
#% 25470
#% 45753
#% 60137
#% 65341
#% 68610
#% 114567
#% 115462
#% 120649
#% 121462
#% 136318
#% 144038
#% 162626
#% 187659
#% 197751
#% 207612
#% 210189
#% 210212
#% 210214
#% 212287
#% 223567
#% 227859
#% 227998
#% 237191
#% 248024
#% 248819
#% 268782
#% 268797
#% 268799
#% 273705
#% 273897
#% 275423
#% 281149
#% 282470
#% 284562
#% 287434
#% 288652
#% 288990
#% 289335
#% 291299
#% 292677
#% 299976
#% 299984
#% 333841
#% 333845
#% 333989
#% 335005
#% 342303
#% 342878
#% 344552
#% 378393
#% 387427
#% 388776
#% 408396
#% 442886
#% 443143
#% 443663
#% 443891
#% 458861
#% 461918
#% 462062
#% 462212
#% 464041
#% 464720
#% 465018
#% 479465
#% 479806
#% 479958
#% 480084
#% 480296
#% 480489
#% 481434
#% 516319
#% 546132
#% 562122
#% 569211
#% 571040
#% 625933
#% 627101
#% 659999
#! Modern search engines answer keyword-based queries extremely efficiently. The impressive speed is due to clever inverted index structures, caching, a domain-independent knowledge of strings, and thousands of machines. Several research efforts have attempted to generalize keyword search to keytree and keygraph searching, because trees and graphs have many applications in next-generation database systems. This paper surveys both algorithms and applications, giving some emphasis to our own work.

#index 378392
#* Validating streaming XML documents
#@ Luc Segoufin;Victor Vianu
#t 2002
#c 5
#% 241159
#% 248799
#% 262724
#% 279367
#% 299944
#% 300179
#% 333982
#% 404772
#% 428155
#% 483878
#% 840584
#! This paper investigates the on-line validation of streaming XML documents with respect to a DTD, under memory constraints. We first consider validation using constant memory, formalized by a finite-state automaton (FSA). We examine two flavors of the problem, depending on whether or not the XML document is assumed to be well-formed. The main results of the paper provide conditions on the DTDs under which validation of either flavor can be done using an FSA. For DTDs that cannot be validated by an FSA, we investigate two alternatives. The first relaxes the constant memory requirement by allowing a stack bounded in the depth of the XML document, while maintaining the deterministic, one-pass requirement. The second approach consists in refining the DTD to provide additional information that allows validation by an FSA.

#index 378393
#* Containment and equivalence for an XPath fragment
#@ Gerome Miklau;Dan Suciu
#t 2002
#c 5
#% 140410
#% 187659
#% 248025
#% 282470
#% 289287
#% 289335
#% 333989
#% 342372
#% 400361
#% 408396
#% 562451
#% 599549
#% 600179
#! XPath is a simple language for navigating an XML document and selecting a set of element nodes. XPath expressions are used to query XML data, describe key constraints, express transformations, and reference elements in remote documents. This paper studies the containment and equivalence problems for a fragment of the XPath query language, with applications in all these contexts.In particular, we study a class of XPath queries that contain branching, label wildcards and can express descendant relationships between nodes. Prior work has shown that languages which combine any two of these three features have efficient containment algorithms. However, we show that for the combination of features, containment is coNP-complete. We provide a sound and complete EXPTIME algorithm for containment, and study parameterized PTIME special cases. While we identify two parameterized classes of queries for which containment can be decided efficiently, we also show that even with some bounded parameters, containment is coNP-complete. In response to these negative results, we describe a sound algorithm which is efficient for all queries, but may return false negatives in some cases.

#index 378394
#* On the power of walking for querying tree-structured data
#@ Frank Neven
#t 2002
#c 5
#% 6787
#% 46269
#% 154314
#% 257866
#% 273701
#% 278829
#% 279367
#% 291299
#% 299942
#% 315302
#% 344425
#% 427874
#% 483878
#% 598376
#! XSLT is the prime example of an XML query language based on tree-walking. Indeed, stripped down, XSLT is just a tree-walking tree-transducer equipped with registers and look-ahead. Motivated by this connection, we want to pinpoint the computational power of devices based on tree-walking. We show that in the absence of unique identifiers even very powerful extensions of the tree-walking paradigm are not relationally complete. That is, these extensions do not capture all of first-order logic. In contrast, when unique identifiers are available, we show that various restrictions allow to capture LOGSPACE, PTIME, PSPACE, and EXPTIME. These complexity classes are defined w.r.t. a Turing machine model working directly on (attributed) trees. When no attributes are present, relational storage does not add power; whether look-ahead adds power is related to the open question whether tree-walking captures the regular tree languages.

#index 378395
#* A normal form for XML documents
#@ Marcelo Arenas;Leonid Libkin
#t 2002
#c 5
#% 445
#% 663
#% 10245
#% 11284
#% 109995
#% 119792
#% 205246
#% 228660
#% 250473
#% 267604
#% 289305
#% 299943
#% 330627
#% 332909
#% 333855
#% 333979
#% 343978
#% 366807
#% 384978
#% 479956
#% 562455
#% 632058
#% 993272
#! This paper takes a first step towards the design and normalization theory for XML documents. We show that, like relational databases, XML documents may contain redundant information, and may be prone to update anomalies. Furthermore, such problems are caused by certain functional dependencies among paths in the document. Our goal is to find a way of converting an arbitrary DTD into a well-designed one, that avoids these problems. We first introduce the concept of a functional dependency for XML, and define its semantics via a relational representation of XML. We then define an XML normal form, XNF, that avoids update anomalies and redundancies. We study its properties and show that it generalizes BCNF and a normal form for nested relations when those are appropriately coded as XML documents. Finally, we present a lossless algorithm for converting any DTD into one in XNF.

#index 378396
#* Distributed computation of web queries using automata
#@ Marc Spielmann;Jerzy Tyszkiewicz;Jan Van den Bussche
#t 2002
#c 5
#% 28120
#% 175464
#% 210761
#% 223887
#% 261139
#% 261741
#% 275928
#% 292677
#% 303887
#% 321075
#% 330995
#% 424304
#% 514137
#% 632052
#! We introduce and investigate a distributed computation model for querying the Web. Web queries are computed by interacting automata running at different nodes in the Web. The automata which we are concerned with can be viewed as register automata equipped with an additional communication component. We identify conditions necessary and sufficient for systems of automata to compute Web queries, and investigate the computational power of such systems.

#index 378397
#* Conjunctive selection conditions in main memory
#@ Kenneth A. Ross
#t 2002
#c 5
#% 68142
#% 68611
#% 83335
#% 152940
#% 214021
#% 300194
#% 319671
#% 333938
#% 442832
#% 442836
#% 479819
#% 479821
#% 480119
#% 480147
#% 480464
#% 480821
#% 566122
#% 632034
#% 640783
#% 742722
#! We consider the fundamental operation of applying a conjunction of selection conditions to a set of records. With large main memories available cheaply, systems may choose to keep the data entirely in main memory, in order to improve query and/or update performance.The design of a data-intensive algorithm in main memory needs to take into account the architectural characteristics of modern processors, just as a disk-based method needs to consider the physical characteristics of disk devices. An important architectural feature that influences the performance of main memory algorithms is the branch misprediction penalty. We demonstrate that branch misprediction has a substantial impact on the performance of an algorithm for applying selection conditions.We describe a space of "query plans" that are logically equivalent, but differ in terms of performance due to variations in their branch prediction behavior. We propose a cost model that takes branch prediction into account, and develop a query optimization algorithm that chooses a plan with optimal estimated cost. We also develop an efficient heuristic optimization algorithm.We provide experimental results for a case study based on an event notification system. Our results show the effectiveness of the proposed optimization techniques. Our results also demonstrate that significant improvements in performance can be obtained by applying a methodology that takes branch misprediction latency into account.

#index 378398
#* Efficient aggregation over objects with extent
#@ Donghui Zhang;Vassilis J. Tsotras;Dimitrios Gunopulos
#t 2002
#c 5
#% 37861
#% 86950
#% 178068
#% 227866
#% 227868
#% 227880
#% 227883
#% 235114
#% 252304
#% 259995
#% 273887
#% 273902
#% 280448
#% 300195
#% 319601
#% 333874
#% 333977
#% 341100
#% 370597
#% 378399
#% 458843
#% 458858
#% 458863
#% 465010
#% 479822
#% 480801
#% 481951
#% 527189
#% 565462
#% 617881
#% 631922
#% 632071
#! We examine the problem of efficiently computing sum/count/avg aggregates over objects with non-zero extent. Recent work on computing multi-dimensional aggregates has concentrated on objects with zero extent (points) on a multi-dimensional grid, or one-dimensional intervals. However, in many spatial and/or spatio-temporal applications objects have extent in various dimensions, while they can be located anywhere in the application space. The aggregation predicate is typically described by a multi-dimensional box (box-sum aggregation). We examine two variations of the problem. In the simple case an object's value contributes to the aggregation result as a whole as long as the object intersects the query box. More complex is the functional box-sum aggregation introduced in this paper, where objects participate in the aggregation proportionally to the size of their intersection with the query box. We first show that both problems can he reduced to dominance-sum queries. Traditionally dominance-sum queries are addressed in main memory by a static structure, the ECDF-tree. We then propose two extensions, namely the ECDF-B-trees, that make this structure disk-based and dynamic. Finally, we introduce the DA-tree that combines the advantages from each ECDF-B-tree. We run experiments comparing the performance of the ECDF-B-trees, the BA-tree and a traditional R*-tree (which has been augmented to include aggregation information on its index nodes) over spatial datasets. Our evaluation reaffirms that the BA-tree has more robust performance. Compared against the augmented R*-tree, the BA-tree offers drastic improvement in query performance at the expense of some limited extra space.

#index 378399
#* How to evaluate multiple range-sum queries progressively
#@ Rolfe R. Schmidt;Cyrus Shahabi
#t 2002
#c 5
#% 227866
#% 227883
#% 248807
#% 273902
#% 280448
#% 300193
#% 316551
#% 333872
#% 333977
#% 458875
#% 459022
#% 479822
#% 480306
#% 480628
#% 504019
#% 617881
#! Users of decision support system typically submit batches of range-sum queries simultaneously rather than issuing individual, unrelated queries. We propose a wavelet based technique that exploits T/O sharing across a query batch to evaluate the set of queries progressively and efficiently. The challenge is that now controlling the structure of errors across query results becomes more critical than minimizing error per individual query. Consequently, we define a class of structural error penalty functions and show how they are controlled by our technique Experiments demonstrate that our technique is efficient as an exact algorithm, and the progressive estimates are accurate, even after less than one I/O per query.

#index 378400
#* On the computation of relational view complements
#@ Jens Lechtenbörger;Gottfried Vossen
#t 2002
#c 5
#% 664
#% 84615
#% 164796
#% 286901
#% 289266
#% 384978
#% 399235
#% 427208
#% 495115
#% 572312
#! Views as a means to describe parts of a given data collection play an important role in many database applications. In dynamic environments, where data is updated, not only information provided by views, but also information provided by data sources but missing from views turns out to be relevant: Previously, this missing information was characterized in terms of view complements; recently, it was shown that view complements can be exploited in the context of data warehouses to guarantee desirable warehouse properties such as independence and self-maintainability. As the complete source information is a trivial complement for any given view, a natural interest for "small" or even "minimal" complements arises. However, the computation of minimal complements is still not too well understood. In this paper, we show how to compute reasonably small (and in special cases even minimal) complements for monotonic relational views, where the complexity of constructing complements is polynomial in the size of schema information.

#index 378401
#* On propagation of deletions and annotations through views
#@ Peter Buneman;Sanjeev Khanna;Wang-Chiew Tan
#t 2002
#c 5
#% 256685
#% 286901
#% 287000
#% 291869
#% 318704
#% 408396
#% 416004
#% 479915
#% 600496
#! We study two classes of view update problems in relational databases. We are given a source database S, a monotone query Q, and the view Q(S) generated by the query. The first problem that we consider is the classical view deletion problem where we wish to identify a minimal set T of tuples in S whose deletion will eliminate a given tuple t from the view. We study the complexity of optimizing two natural objectives in this setting, namely, find T to minimize the side-effects on the view, and the source, respectively. For both objective functions, we show a dichotomy in the complexity. Interestingly, the problem is either in P or is NP-hard, for queries in the same class in either objective function.The second problem in our study is the annotation placement problem. Suppose we annotate an attribute of a tuple in S. The rules for carrying the annotation forward through a query are easily stated. On the other hand, suppose we annotate an attribute of a tuple in the view Q(S), what annotation(s) in S will cause this annotation to appear in the view, minimizing the propagation to other attributes in Q(S)? View annotation is becoming an increasingly useful method of communicating meta-data among users of shared scientific data sets, and to our knowledge, there has been no formal study of this problem.Our study of these problems gives us important insights into computational issues involved in data provenance or lineage --- the process by which data moves through databases. We show that the two problems correspond to two fundamentally distinct notions of provenance, why and where-provenance.

#index 378402
#* The view-selection problem has an exponential-time lower bound for conjunctive queries and views
#@ Rada Chirkova
#t 2002
#c 5
#% 36683
#% 198465
#% 210182
#% 210208
#% 273697
#% 328424
#% 333964
#% 462204
#% 464706
#% 479476
#% 480158
#% 480670
#% 482110
#% 482111
#! The view-selection problem is to design and materialize a set of views over a database schema, such that the choice of views minimizes the cost of evaluating the selected workload of queries, and the combined size of the materialized views does not exceed a prespecified storage limit. Important applications of the view-selection problem include query optimization, data warehouse design, and information integration.We consider the view-selection problem in relational databases, for conjunctive queries and views. Suppose somebody wants to design a view-selection algorithm that outputs a polynomial number of views for all query workloads and storage limits and produces optimal selections of views independently of actual database contents. In previous work it was shown that it is impossible to design such an algorithm when the product (as for nested-loop joins) cost model is used. That is, there exist databases for which the number of views in an optima] viewset is exponential in the size of the database schema and query workload. As a consequence, under the product-cost model the view-selection problem has an exponential-time lower bound.Efficient join algorithms have a cost that is proportional to the sum of the sizes of the input and output relations. In this paper we prove that under the more practical sum-cost model, the view-selection problem also has an exponential time lower bound. As a consequence, under the sum-cost model it is also impossible to come up with a view-selection algorithm that outputs a polynomial number of views for all query workloads and databases, yet produces optimal selections of views.

#index 378403
#* OLAP dimension constraints
#@ Carlos A. Hurtado;Alberto O. Mendelzon
#t 2002
#c 5
#% 237192
#% 248024
#% 479948
#% 480123
#% 503731
#% 562299
#% 631925
#! In multidimensional data models intended for online analytic processing (OLAP), data are viewed as points in a multidimensional space. Each dimension has structure, described by a directed graph of categories, a set of members for each category, and a child/parent relation between members. An important application of this structure is to use it to infer summarizability, that is, whether an aggregate view defined for some category can be correctly derived from a set of precomputed views defined for other categories. A dimension is called heterogeneous if two members in a given category are allowed to have ancestors in different categories. In previous work, we studied the problem of inferring summarizability in a particular class of heterogeneous dimensions. In this paper, we propose a class of integrity constraints and schemas that allow us to reason about summarizability in general heterogeneous dimensions. We introduce the notion of frozen dimensions, which are minimal homogeneous dimension instances representing the different structures that are implicitly combined in a heterogeneous dimension. Frozen dimensions provide the basis for efficiently testing implication of dimension constraints, and are useful aid to understanding heterogeneous dimensions. We give a sound and complete algorithm for solving the implication of dimension constraints, that uses heuristics based on the structure of the dimension and the constraints to speed up its execution. We study the intrinsic complexity of the implication problem, and the running time of our algorithm.

#index 378404
#* Fast algorithms for hierarchical range histogram construction
#@ Sudipto Guha;Nick Koudas;Divesh Srivastava
#t 2002
#c 5
#% 43163
#% 54047
#% 152585
#% 201921
#% 210190
#% 248822
#% 299982
#% 333872
#% 411554
#% 479648
#% 480123
#% 481266
#! Data Warehousing and OLAP applications typically view data an having multiple logical dimensions (e.g., product, location) with natural hierarchies defined on each dimension. OLAP queries usually involve hierarchical selections on some of the dimensions, and often aggregate measure attributes (e.g., sales, volume). Accurately estimating the distribution of measure attributes, under hierarchical selections, is important in a variety of scenarios, including approximate query evaluation and cost-based optimization of queries.In this paper, we propose fast (near linear time) algorithms for the problem of approximating the distribution of measure attributes with hierarchies defined on them, using histograms. Our algorithms are based on dynamic programming and a novel notion of sparse intervals that we introduce, and are the first practical algorithms for this problem. They effectively trade space for construction time without compromising histogram accuracy. We complement our analytical contributions with an experimental evaluation using real data sets, demonstrating the superiority of our approach.

#index 378405
#* On moving object queries: (extended abstract)
#@ Hoda Mokhtar;Jianwen Su;Oscar Ibarra
#t 2002
#c 5
#% 6249
#% 117448
#% 126329
#% 152928
#% 188352
#% 190332
#% 201929
#% 210210
#% 273706
#% 275310
#% 299979
#% 300173
#% 315005
#% 356378
#% 356568
#% 410601
#% 421073
#% 442767
#% 461923
#% 480473
#% 503869
#% 505561
#% 527187
#% 527198
#% 527321
#% 618583
#! Database applications for moving objects pose new challenges in modeling, querying, and maintenance of objects whose locations are rapidly changing over time. Previous work on modeling and querying spatio-temporal databases and constraint databases focus primarily on snapshots of changing databases. In this paper we study query evaluation techniques for moving object databases where moving objects are being updated frequently. We consider a constraint database approach to moving objects and queries. We classify moving object queries into: "past", "continuing", and "future" queries. We argue that while traditional constraint query evaluation techniques are suitable for past queries, new techniques are needed for continuing and future queries. Motivated by nearest-neighbor queries, we define a query language based on a single "generalized distance" function f mapping from objects to continuous functions from time to ℝ. Queries in this language may be past, continuing, or future. We show that if f maps to polynomials, queries can be evaluated efficiently using the plane sweeping technique from computational geometry. Consequently, many known distance based queries can be evaluated efficiently.

#index 378406
#* Knowledge compilation = query rewriting + view synthesis
#@ Marco Cadoli;Toni Mancini
#t 2002
#c 5
#% 90860
#% 136678
#% 172928
#% 183609
#% 204396
#% 207777
#% 211581
#% 216990
#% 556918
#% 594022
#% 598376
#% 600823
#% 936786
#% 1275338
#% 1289171
#! In Knowledge Compilation (KC) an intractable deduction problem KB ⊨ f is split into two phases: 1) KB is preprocessed, thus obtaining a data structure DKB; 2) the problem is efficiently solved using DKB and f. Our goal is to study KC in the context of relational databases: Both KB and f are represented as databases, and '⊨' is represented as a query Q in second-order logic. DKB is a database, to be synthesized from KB by means of an appropriate view. Q is rewritten, thus obtaining Qr. We show syntactic restrictions on Q implying that a polynomial-size DKB and a first-order Qr exist, which imply that phase 2 can be done in polynomial time. We also present classes of queries (in some sense complementary to the former ones) for which either no polynomial-size DKB or no first-order Qr exist (unless the PH collapses). Compilation to other complexity classes is also addressed.

#index 378407
#* Answering queries using views with arithmetic comparisons
#@ Foto Afrati;Chen Li;Prasenjit Mitra
#t 2002
#c 5
#% 11819
#% 36181
#% 122396
#% 123118
#% 164364
#% 198465
#% 227886
#% 237189
#% 237190
#% 248032
#% 248038
#% 273911
#% 328424
#% 333964
#% 368248
#% 378407
#% 379503
#% 443173
#% 464056
#% 464203
#% 464717
#% 479452
#% 479950
#% 480149
#% 481923
#% 482110
#% 531450
#% 598678
#% 599549
#% 707146
#% 714229
#! We consider the problem of answering queries using views, where queries and views are conjunctive queries with arithmetic comparisons (CQACs) over dense orders. Previous work only considered limited variants of this problem, without giving a complete solution. We have developed a novel algorithm to obtain maximally-contained rewritings (MCRs) for queries having left (or right) semi-interval-comparison predicates. For semi-interval queries, we show that the language of finite unions of CQAC rewritings is not sufficient to find a maximally-contained solution, and identify cases where datalog is sufficient. Finally, we show that it is decidable to obtain equivalent rewritings for CQAC queries.

#index 378408
#* Characterizing memory requirements for queries over continuous data streams
#@ Arvind Arasu;Brian Babcock;Shivnath Babu;Jon McAlister;Jennifer Widom
#t 2002
#c 5
#% 36683
#% 59350
#% 116082
#% 198467
#% 214073
#% 248795
#% 273911
#% 300179
#% 310900
#% 333926
#% 333982
#% 338425
#% 340301
#% 378388
#% 379444
#% 379445
#% 428155
#% 458556
#% 479621
#% 480628
#% 480768
#% 593957
#% 594029
#! We consider conjunctive queries with arithmetic comparisons over multiple continuous data streams. We specify an algorithm for determining whether or not a query can be evaluated using a bounded amount of memory for all possible instances of the data streams. When a query can be evaluated using bounded memory, we produce an execution strategy based on constant-sized synopses of the data streams.

#index 378409
#* Data integration: a theoretical perspective
#@ Maurizio Lenzerini
#t 2002
#c 5
#% 33376
#% 36181
#% 122396
#% 123085
#% 154067
#% 198465
#% 198466
#% 210176
#% 210214
#% 213982
#% 221878
#% 229827
#% 237181
#% 237184
#% 237189
#% 237190
#% 237191
#% 248025
#% 248026
#% 248038
#% 248801
#% 248819
#% 248859
#% 263136
#% 264858
#% 273687
#% 273696
#% 273698
#% 273700
#% 273912
#% 273924
#% 282425
#% 283052
#% 289266
#% 299945
#% 299967
#% 299968
#% 342980
#% 378410
#% 442977
#% 462214
#% 462235
#% 464056
#% 464717
#% 464915
#% 479813
#% 480468
#% 480670
#% 480822
#% 481786
#% 481923
#% 482081
#% 488620
#% 519568
#% 531450
#% 536353
#% 562451
#% 562454
#% 571169
#% 572311
#% 572314
#% 587566
#% 599549
#% 614579
#% 632039
#% 632086
#% 665856
#% 707146
#! Data integration is the problem of combining data residing at different sources, and providing the user with a unified view of these data. The problem of designing data integration systems is important in current real world applications, and is characterized by a number of issues that are interesting from a theoretical point of view. This document presents on overview of the material to be presented in a tutorial on data integration. The tutorial is focused on some of the theoretical issues that are relevant for data integration. Special attention will be devoted to the following aspects: modeling a data integration application, processing queries in data integration, dealing with inconsistent data sources, and reasoning on queries.

#index 378410
#* Lossless regular views
#@ Diego Calvanese;Giuseppe De Giacomo;Maurizio Lenzerini;Moshe Y. Vardi
#t 2002
#c 5
#% 198465
#% 237190
#% 237191
#% 241166
#% 248038
#% 268708
#% 273700
#% 291299
#% 299945
#% 299967
#% 299968
#% 328424
#% 408396
#% 464717
#% 464720
#% 480670
#% 587566
#% 632039
#! If the only information we have on a certain database is through a set of views, the question arises of whether this is sufficient to answer completely a given query. We say that the set of views is lossless with respect to the query, if, no matter what the database is, we can answer the query by solely relying on the content of the views. The question of losslessness has various applications, for example in query optimization, mobile computing, data warehousing, and data integration. We study this problem in a context where the database is semistructured, and both the query and the views are expressed as regular path queries. The form of recursion present in this class prevents us from applying known results to our case.We first address the problem of checking losslessness in the case where the views are materialized. The fact that we have the view extensions available makes this case solvable by extending known techniques. We then study a more complex version of the problem, namely the one where we abstract from the specific view extension. More precisely, we address the problem of checking whether, for every database, the answer to the query over such a database can be obtained by relying only on the view extensions. We show that the problem is solvable by utilizing, via automata-theoretic techniques, the known connection between view-based query answering and constraint satisfaction. We also investigate the computational complexity of both versions of the problem.

#index 378411
#* On verifying consistency of XML specifications
#@ Marcelo Arenas;Wenfei Fan;Leonid Libkin
#t 2002
#c 5
#% 69283
#% 147801
#% 164387
#% 274160
#% 289328
#% 292677
#% 321051
#% 330627
#% 333855
#% 384978
#% 408396
#% 463730
#% 479956
#% 480152
#% 495632
#% 562455
#% 572298
#% 587380
#% 1393700
#! XML specifications often consist of a type definition (typically, a DTD) and a set of integrity constraints. It has been shown previously that such specifications can be inconsistent, and thus it is often desirable to check consistency at compile-time. It is known that for general keys and foreign keys, and DTDs, the consistency problem is undecidable; however, it becomes NP-complete when all keys are one-attribute (unary), and tractable, if no foreign keys are used.In this paper, we consider a variety of constraints for XML data, and study the complexity of the consistency problem. Our main conclusion is that in the presence of foreign keys, compile-time verification of consistency is usually infeasible. We look at two types of constraints: absolute (that hold in the entire document), and relative (that only hold in a part of the document). For absolute constraints, we extend earlier decidability results to the case of multi-attribute keys and unary foreign keys, and to the case of constraints involving regular expressions, providing lower and upper bounds in both cases. For relative constraints, we show that even for unary constraints, the consistency problem is undecidable. We also establish a number of restricted decidable cases.

#index 378412
#* Labeling dynamic XML trees
#@ Edith Cohen;Haim Kaplan;Tova Milo
#t 2002
#c 5
#% 19616
#% 148890
#% 291299
#% 325384
#% 336509
#% 379483
#% 379484
#% 480659
#! We present algorithms to label the nodes of an XML tree which is subject to insertions and deletions of nodes. The labeling is done such that (1) we label each node immediately when it is inserted and this label remains unchanged, and (2) from a pair of labels alone, we can decide whether one node is an ancestor of the other. This problem arises in the context of XML databases that support queries on the structure of the documents as well us on the changes made to the documents over time. We prove that our algorithms assign the shortest possible labels (up to a constant factor) which satisfy these requirements.We also consider the same problem when "clues" that provide guarantees on possible future insertions are given together with newly inserted nodes. Such clues can be derived from the DTD or from statistics on similar XML trees. We present algorithms that use the clues to assign shorter labels. We also prove that the length of our labels is close to the minimum possible.

#index 378413
#* On the complexity of approximate query optimization
#@ S. Chatterji;S. S. K. Evani;S. Ganguly;M. D. Yemmanuru
#t 2002
#c 5
#% 554
#% 408396
#% 443065
#% 464700
#% 479938
#! In this work, we study the complexity of the problem of approximate query optimization. We show that, for any δ 0, the problem of finding a join order sequence whose cost is within a factor 2Θ(log1-δ(K)) of K, where K is the cost of the optimal join order sequence is NP-Hard. The complexity gap remains if the number of edges in the query graph is constrained to be a given function e(n) of the number of vertices n of the query graph, where n(n - 1)/2 - Θ(nτ) ≥ e(n) ≥ n + Θ(nτ) and τ is any constant between 0 and 1. These results show that, unless P=NP, the query optimization problem cannot be approximately solved by an algorithm that runs in polynomial time and has a competitive ratio that is within some polylogarithmic factor of the optimal cost.

#index 378414
#* Least expected cost query optimization: what can we expect?
#@ Francis Chu;Joseph Halpern;Johannes Gehrke
#t 2002
#c 5
#% 227883
#% 248824
#% 273694
#% 368248
#% 411554
#% 479452
#% 479617
#% 479951
#% 480803
#% 480955
#% 632048
#! A standard assumption in the database query optimization literature is that it suffices to optimize for the "typical" case---that is, the case in which various parameters (e.g., the amount of available memory, the selectivities of predicates, etc.) take on their "typical" values. It was claimed in [CHS99] that we could do better by choosing plans based on their expected cost. Here we investigate this issue more thoroughly. We show that in many circumstances of interest, a "typical" value of the parameter often does give acceptable answers, provided that it is chosen carefully and we are interested only in minimizing expected running time. However, by minimizing the expected running time, we are effectively assuming that if plan p1 runs three times as long as plan p2, then p1 is exactly three times as bad as p2. An assumption like this is not always appropriate. We show that focusing on least expected cost can lead to significant improvement for a number of cost functions of interest.

#index 397348
#* Proceedings of the 2002 ACM SIGMOD international conference on Management of data
#@ Bongki Moon;David DeWitt;Michael Franklin
#t 2002
#c 5
#! The 2002 ACM SIGMOD International Conference on Management of Data, was held June 4-6, 2002 at the spectacular Frank Lloyd Wright-designed Monona Terrace conference center in Madison, Wisconsin. The SIGMOD conference has long held its status a leading forum for database researchers, practitioners, developers, and users to explore cutting-edge ideas and to exchange techniques, tools, and experiences. This year we are pleased to have a particularly strong program, including significant representation from both industry and academia and contributions from around the world. The SIGMOD conference is sponsored by the Association for Computing Machinery (ACM) and its Special Interest Group on Management of Data (SIGMOD). As has become customary, two days of the conference were overlapped with the Symposium on Principles of Database Systems (PODS). In addition, there were a number of important events co-located with SIGMOD this year, including the 5th International Workshop on the Web and Databases (WebDB), the Workshop on Research Issues in Data Mining and Knowledge Discovery (DMKD), and the New Database Faculty Symposium.As with previous years, acceptance into the conference proceedings was extremely competitive. From the 240 research program submissions, the program committee selected 42 papers for presentation and inclusion in the proceedings. These papers span the range of traditional database topics as well as issues of emerging interest such as data streaming, middle-tier data management, and integration with web services and the WWW. The: program committee worked hard to select these papers through a detailed review process and active discussion both electronically and at the program committee meeting held in January at Berkeley. We are deeply indebted to Surajit Chaudhuri and Jonathan Simon for their efforts in developing and hosting the Conference Management Tool at Microsoft. We used the CMT to run the entire submissions and review process for research papers, including the program committee meeting. We would also like to thank Umesh Dayal, who took on extra work to help manage the review process.In addition to the research track, we had 41 submissions to the Demonstrations track, of which 21 projects were invited to present. The Demonstrations track has become a key venue for the early dissemination of cutting-edge prototype and systems development experience. Paul Aoki chaired the demos program committee, which put together a diverse and exciting program. The Industrial program committee, chaired by Mike Carey took an active role in soliciting quality submissions and have put together a program that is a key component of this year's conference. Likewise, Donald Kossmann and Peter Scheuermann and the tutorial committee organized an excellent slate of Panels and Tutorials to round out the program.

#index 397349
#* Archiving scientific data
#@ Peter Buneman;Sanjeev Khanna;Keishi Tajima;Wang-Chiew Tan
#t 2002
#c 5
#% 56081
#% 66654
#% 84549
#% 210212
#% 227859
#% 300153
#% 330627
#% 390132
#% 480659
#% 480827
#% 650962
#! We present an archiving technique for hierarchical data with key structure. Our approach is based on the notion of timestamps whereby an element appearing in multiple versions of the database is stored only once along with a compact description of versions in which it appears. The basic idea of timestamping was discovered by Driscoll et. al. in the context of persistent data structures where one wishes to track the sequences of changes made to a data structure. We extend this idea to develop an archiving tool for XML data that is capable of providing meaningful change descriptions and can also efficiently support a variety of basic functions concerning the evolution of data such as retrieval of any specific version from the archive and querying the temporal history of any element. This is in contrast to diff-based approaches where such operations may require undoing a large number of changes or significant reasoning with the deltas. Surprisingly, our archiving technique does not incur any significant space overhead when contrasted with other approaches. Our experimental results support this and also show that the compacted archive file interacts well with other compression techniques. Finally, another useful property of our approach is that the resulting archive is also in XML and hence can directly leverage existing XML tools.

#index 397350
#* Efficient integration and aggregation of historical information
#@ Mirek Riedewald;Divyakant Agrawal;Amr El Abbadi
#t 2002
#c 5
#% 56081
#% 223781
#% 227866
#% 333874
#% 333977
#% 420053
#% 443181
#% 452329
#% 458741
#% 479822
#% 480817
#% 565462
#% 571296
#% 617881
#% 631922
#% 632071
#% 738915
#! Data warehouses support the analysis of historical data. This often involves aggregation over a period of time. Furthermore, data is typically incorporated in the warehouse in the increasing order of a time attribute, e.g., date of sale or time of a temperature measurement. In this paper we propose a framework to take advantage of this append only nature of updates due to a time attribute. The framework allows us to integrate large amounts of new data into the warehouse and generate historical summaries efficiently. Query and update costs are virtually independent from the extent of the data set in the time dimension, making our framework an attractive aggregation approach for append-only data streams. A specific instantiation of the general approach is developed for MOLAP data cubes, involving a new data structure for append-only arrays with pre-aggregated values. Our framework is applicable to point data and data with extent, e.g., hyper-rectangles.

#index 397351
#* An adaptive peer-to-peer network for distributed caching of OLAP results
#@ Panos Kalnis;Wee Siong Ng;Beng Chin Ooi;Dimitris Papadias;Kian-Lee Tan
#t 2002
#c 5
#% 113704
#% 210182
#% 210211
#% 227880
#% 248806
#% 250053
#% 264263
#% 273917
#% 330305
#% 333971
#% 479646
#% 480647
#% 481916
#% 564270
#% 566126
#% 571216
#% 571217
#% 641013
#% 1303525
#! Peer-to-Peer (P2P) systems are becoming increasingly popular as they enable users to exchange digital information by participating in complex networks. Such systems are inexpensive, easy to use, highly scalable and do not require central administration. Despite their advantages, however, limited work has been done on employing database systems on top of P2P networks.Here we propose the PeerOLAP architecture for supporting On-Line Analytical Processing queries. A large number low-end clients, each containing a cache with the most useful results, are connected through an arbitrary P2P network. If a query cannot be answered locally (i.e. by using the cache contents of the computer where it is issued), it is propagated through the network until a peer that has cached the answer is found. An answer may also be constructed by partial results from many peers. Thus PeerOLAP acts as a large distributed cache, which amplifies the benefits of traditional client-side caching. The system is fully distributed and can reconfigure itself on-the-fly in order to decrease the query cost for the observed workload. This paper describes the core components of PeerOLAP and presents our results both from simulation and a prototype installation running on geographically remote peers.

#index 397352
#* Rate-based query optimization for streaming information sources
#@ Stratis D. Viglas;Jeffrey F. Naughton
#t 2002
#c 5
#% 3771
#% 9240
#% 159337
#% 248793
#% 248795
#% 249985
#% 273911
#% 300167
#% 300179
#% 333981
#% 379445
#% 411554
#% 428155
#% 479461
#% 480152
#% 511910
#% 571217
#% 617870
#! Relational query optimizers have traditionally relied upon table cardinalities when estimating the cost of the query plans they consider. While this approach has been and continues to be successful, the advent of the Internet and the need to execute queries over streaming sources requires a different approach, since for streaming inputs the cardinality may not be known or may not even be knowable (as is the case for an unbounded stream.) In view of this, we propose shifting from a cardinality-based approach to a rate-based approach, and give an optimization framework that aims at maximizing the output rate of query evaluation plans. This approach can be applied to cases where the cardinality-based approach cannot be used. It may also be useful for cases where cardinalities are known, because by focusing on rates we are able not only to optimize the time at which the last result tuple appears, but also to optimize for the number of answers computed at any specified time after the query evaluation commences. We present a preliminary validation of our rate-based optimization framework on a prototype XML query engine, though it is generic enough to be used in other database contexts. The results show that rate-based optimization is feasible and can indeed yield correct decisions.

#index 397353
#* Continuously adaptive continuous queries over streams
#@ Samuel Madden;Mehul Shah;Joseph M. Hellerstein;Vijayshankar Raman
#t 2002
#c 5
#% 116082
#% 248795
#% 273911
#% 281537
#% 281557
#% 300166
#% 300167
#% 300179
#% 333962
#% 336865
#% 340635
#% 428410
#% 443298
#% 480774
#% 482088
#% 715191
#% 979303
#! We present a continuously adaptive, continuous query (CACQ) implementation based on the eddy query processing framework. We show that our design provides significant performance benefits over existing approaches to evaluating continuous queries, not only because of its adaptivity, but also because of the aggressive cross-query sharing of work and space that it enables. By breaking the abstraction of shared relational algebra expressions, our Telegraph CACQ implementation is able to share physical operators --- both selections and join state --- at a very fine grain. We augment these features with a grouped-filter index to simultaneously evaluate multiple selection predicates. We include measurements of the performance of our core system, along with a comparison to existing continuous query approaches.

#index 397354
#* Processing complex aggregate queries over data streams
#@ Alin Dobra;Minos Garofalakis;Johannes Gehrke;Rajeev Rastogi
#t 2002
#c 5
#% 1331
#% 214073
#% 223781
#% 273682
#% 273902
#% 273907
#% 273909
#% 273910
#% 310500
#% 333926
#% 333931
#% 338425
#% 379445
#% 428155
#% 479984
#% 480306
#% 480465
#% 480628
#% 480810
#% 482123
#% 593957
#% 594012
#! Recent years have witnessed an increasing interest in designing algorithms for querying and analyzing streaming data (i.e., data that is seen only once in a fixed order) with only limited memory. Providing (perhaps approximate) answers to queries over such continuous data streams is a crucial requirement for many application environments; examples include large telecom and IP network installations where performance data from different parts of the network needs to be continuously collected and analyzed.In this paper, we consider the problem of approximately answering general aggregate SQL queries over continuous data streams with limited memory. Our method relies on randomizing techniques that compute small "sketch" summaries of the streams that can then be used to provide approximate answers to aggregate queries with provable guarantees on the approximation error. We also demonstrate how existing statistical information on the base data (e.g., histograms) can be used in the proposed framework to improve the quality of the approximation provided by our algorithms. The key idea is to intelligently partition the domain of the underlying attribute(s) and, thus, decompose the sketching problem in a way that provably tightens our guarantees. Results of our experimental study with real-life as well as synthetic data streams indicate that sketches provide significantly more accurate answers compared to histograms for aggregate queries. This is especially true when our domain partitioning methods are employed to further boast the accuracy of the final estimates.

#index 397355
#* Best-effort cache synchronization with source cooperation
#@ Chris Olston;Jennifer Widom
#t 2002
#c 5
#% 102804
#% 158051
#% 201922
#% 259082
#% 259086
#% 268079
#% 273951
#% 281557
#% 297915
#% 300139
#% 330682
#% 333969
#% 344154
#% 458544
#% 480332
#% 480816
#% 660273
#% 682414
#% 963655
#! In environments where exact synchronization between source data objects and cached copies is not achievable due to bandwidth or other resource constraints, stale (out-of-date) copies are permitted. It is desirable to minimize the overall divergence between source objects and cached copies by selectively refreshing modified objects. We call the online process of selecting which objects to refresh in order to minimize divergence best-effort synchronization. In most approaches to best-effort synchronization, the cache coordinates the process and selects objects to refresh. In this paper, we propose a best-effort synchronization scheduling policy that exploits cooperation between data sources and the cache. We also propose an implementation of our policy that incurs low communication overhead even in environments with very large numbers of sources. Our algorithm is adaptive to wide fluctuations in available resources and data update rates. Through experimental simulation over synthetic and real-world data, we demonstrate the effectiveness of our algorithm, and we quantify the significant decrease in divergence achievable with source cooperation.

#index 397356
#* Efficient evaluation of queries in a mediator for WebSources
#@ Vladimir Zadorozhny;Louiqa Raschid;Maria Esther Vidal;Tolga Urhan;Laura Bright
#t 2002
#c 5
#% 36683
#% 86949
#% 194984
#% 210176
#% 248795
#% 264691
#% 273910
#% 273911
#% 273912
#% 300167
#% 300169
#% 300179
#% 340302
#% 479452
#% 479467
#% 479951
#% 480642
#% 480788
#% 481101
#% 481923
#% 482116
#% 565470
#% 571037
#% 589376
#% 631868
#% 632002
#% 632086
#% 772131
#! We consider an architecture of mediators and wrappers for Internet accessible WebSources of limited query capability. Each call to a source is a WebSource Implementation (WSI) and it is associated with both a capability and (a possibly dynamic) cost. The multiplicity of WSIs with varying costs and capabilities increases the complexity of a traditional optimizer that must assign WSIs for each remote relation in the query while generating an (optimal) plan. We present a two-phase Web Query Optimizer (WQO). In a pre-optimization phase, the WQO selects one or more WSIs for a pre-plan; a pre-plan represents a space of query evaluation plans (plans) based on this choice of WSIs. The WQO uses cost-based heuristics to evaluate the choice of WSI assignment in the pre-plan and to choose a good pre-plan. The WQO uses the pre-plan to drive the extended relational optimizer to obtain the best plan for a pre-plan. A prototype of the WQO has been developed. We compare the effectiveness of the WQO, i.e., its ability to efficiently search a large space of plans and obtain a low cost plan, in comparison to a traditional optimizer. We also validate the cost-based heuristics by experimental evaluation of queries in the noisy Internet environment.

#index 397357
#* Proxy-based acceleration of dynamically generated content on the world wide web: an approach and implementation
#@ Anindya Datta;Kaushik Dutta;Helen Thomas;Debra VanderMeer; Suresha;Krithi Ramamritham
#t 2002
#c 5
#% 168251
#% 281237
#% 333995
#% 334036
#% 340296
#% 480474
#% 480495
#% 480814
#% 480818
#% 504584
#% 609912
#% 642534
#! As Internet traffic continues to grow and web sites become increasingly complex, performance and scalability are major issues for web sites. Web sites are increasingly relying on dynamic content generation applications to provide web site visitors with dynamic, interactive, and personalized experiences. However, dynamic content generation comes at a cost --- each request requires computation as well as communication across multiple components.To address these issues, various dynamic content caching approaches have been proposed. Proxy-based caching approaches store content at various locations outside the site infrastructure and can improve Web site performance by reducing content generation delays, firewall processing delays, and bandwidth requirements. However, existing proxy-based caching approaches either (a) cache at the page level, which does not guarantee that correct pages are served and provides very limited reusability, or (b) cache at the fragment level, which requires the use of pre-defined page layouts. To address these issues, several back end caching approaches have been proposed, including query result caching and fragment level caching. While back end approaches guarantee the correctness of results and offer the advantages of fine-grained caching, they neither address firewall delays nor reduce bandwidth requirements.In this paper, we present an approach and an implementation of a dynamic proxy caching technique which combines the benefits of both proxy-based and back end caching approaches, yet does not suffer from their above-mentioned limitations. Our dynamic proxy caching technique allows granular, proxy-based caching where both the content and layout can be dynamic. Our analysis of the performance of our approach indicates that it is capable of providing significant reductions in bandwidth. We have also deployed our proposed dynamic proxy caching technique at a major financial institution. The results of this implementation indicate that our technique is capable of providing order-of-magnitude reductions in bandwidth and response times in real-world dynamic Web applications.

#index 397358
#* Accelerating XPath location steps
#@ Torsten Grust
#t 2002
#c 5
#% 23651
#% 153260
#% 313889
#% 333981
#% 427199
#% 465018
#% 479465
#% 480299
#% 480489
#% 480656
#% 481599
#% 650962
#! This work is a proposal for a database index structure that has been specifically designed to support the evaluation of XPath queries. As such, the index is capable to support all XPath axes (including ancestor, following, preceding-sibling, descendant-or-self, etc.). This feature lets the index stand out among related work on XML indexing structures which had a focus on regular path expressions (which correspond to the XPath axes children and descendant-or-self plus name tests). Its ability to start traversals from arbitrary context nodes in an XML document additionally enables the index to support the evaluation of path traversals embedded in XQuery expressions. Despite its flexibility, the new index can be implemented and queried using purely relational techniques, but it performs especially well if the underlying database host provides support for R-trees. A performance assessment which shows quite promising results completes this proposal.

#index 397359
#* APEX: an adaptive path index for XML data
#@ Chin-Wan Chung;Jun-Ki Min;Kyuseok Shim
#t 2002
#c 5
#% 139837
#% 210214
#% 236409
#% 248785
#% 333981
#% 404772
#% 462062
#% 462235
#% 463903
#% 463919
#% 464720
#% 464724
#% 479465
#% 479971
#% 480489
#% 480656
#! The emergence of the Web has increased interests in XML data. XML query languages such as XQuery and XPath use label paths to traverse the irregularly structured data. Without a structural summary and efficient indexes, query processing can be quite inefficient due to an exhaustive traversal on XML data. To overcome the inefficiency, several path indexes have been proposed in the research community. Traditional indexes generally record all label paths from the root element in XML data. Such path indexes may result in performance degradation due to large sizes and exhaustive navigations for partial matching path queries start with the self-or-descendent axis("//").In this paper, we propose APEX, an adaptive path index for XML data. APEX does not keep all paths starting from the root and utilizes frequently used paths to improve the query performance. APEX also has a nice property that it can be updated incrementally according to the changes of query workloads. Experimental results with synthetic and real-life data sets clearly confirm that APEX improves query processing cost typically 2 to 54 times better than the existing indexes, with the performance gap increasing with the irregularity of XML data.

#index 397360
#* Covering indexes for branching path queries
#@ Raghav Kaushik;Philip Bohannon;Jeffrey F Naughton;Henry F Korth
#t 2002
#c 5
#% 31484
#% 115513
#% 250208
#% 291299
#% 462062
#% 479465
#% 479956
#% 480489
#% 480656
#% 528124
#% 600179
#! In this paper, we ask if the traditional relational query acceleration techniques of summary tables and covering indexes have analogs for branching path expression queries over tree- or graph-structured XML data. Our answer is yes --- the forward-and-backward index already proposed in the literature can be viewed as a structure analogous to a summary table or covering index. We also show that it is the smallest such index that covers all branching path expression queries. While this index is very general, our experiments show that it can be so large in practice as to offer little performance improvement over evaluating queries directly on the data. Likening the forward-and-backward index to a covering index on all the attributes of several tables, we devise an index definition scheme to restrict the class of branching path expressions being indexed. The resulting index structures are dramatically smaller and perform better than the full forward-and-backward index for these classes of branching path expressions. This is roughly analogous to the situation in multidimensional or OLAP workloads, in which more highly aggregated summary tables can service a smaller subset of queries but can do so at increased performance. We evaluate the performance of our indexes on both relational decompositions of XML and a native storage technique. As expected, the performance benefit of an index is maximized when the query matches the index definition.

#index 397361
#* Implementing database operations using SIMD instructions
#@ Jingren Zhou;Kenneth A. Ross
#t 2002
#c 5
#% 227861
#% 238060
#% 252458
#% 289196
#% 300194
#% 317933
#% 333940
#% 333949
#% 378397
#% 411694
#% 427199
#% 464987
#% 479819
#% 479821
#% 480119
#% 480464
#% 480774
#% 480821
#% 674112
#! Modern CPUs have instructions that allow basic operations to be performed on several data elements in parallel. These instructions are called SIMD instructions, since they apply a single instruction to multiple data elements. SIMD technology was initially built into commodity processors in order to accelerate the performance of multimedia applications. SIMD instructions provide new opportunities for database engine design and implementation. We study various kinds of operations in a database context, and show how the inner loop of the operations can be accelerated using SIMD instructions. The use of SIMD instructions has two immediate performance benefits: It allows a degree of parallelism, so that many operands can be processed at once. It also often leads to the elimination of conditional branch instructions, reducing branch mispredictions.We consider the most important database operations, including sequential scans, aggregation, index operations, and joins. We present techniques for implementing these using SIMD instructions. We show that there are significant benefits in redesigning traditional query processing algorithms so that they can make better use of SIMD technology. Our study shows that using a SIMD parallelism of four, the CPU time for the new algorithms is from 10% to more than four times less than for the traditional algorithms. Superlinear speedups are obtained as a result of the elimination of branch misprediction effects.

#index 397362
#* Fractal prefetching B+-Trees: optimizing both cache and disk performance
#@ Shimin Chen;Phillip B. Gibbons;Todd C. Mowry;Gary Valentin
#t 2002
#c 5
#% 244119
#% 251473
#% 261735
#% 271492
#% 300194
#% 304012
#% 333942
#% 333949
#% 428152
#% 439991
#% 464987
#% 479769
#% 479819
#% 479985
#% 480119
#% 480821
#% 593968
#! B+-Trees have been traditionally optimized for I/O performance with disk pages as tree nodes. Recently, researchers have proposed new types of B+-Trees optimized for CPU cache performance in main memory environments, where the tree node sizes are one or a few cache lines. Unfortunately, due primarily to this large discrepancy in optimal node sizes, existing disk-optimized B+-Trees suffer from poor cache performance while cache-optimized B+-Trees exhibit poor disk performance. In this paper, we propose fractal prefetching B+-Trees (fpB+-Trees), which embed "cache-optimized" trees within "disk-optimized" trees, in order to optimize both cache and I/O performance. We design and evaluate two approaches to breaking disk pages into cache-optimized nodes: disk-first and cache-first. These approaches are somewhat biased in favor of maximizing disk and cache performance, respectively, as demonstrated by our results. Both implementations of fpB+-Trees achieve dramatically better cache performance than disk-optimized B+-Trees: a factor of 1.1-1.8 improvement for search, up to a factor of 4.2 improvement for range scans, and up to a 20-fold improvement for updates, all without significant degradation of I/O performance. In addition, fpB+-Trees accelerate I/O performance for range scans by using jump-pointer arrays to prefetch leaf pages, thereby achieving a speed-up of 2.5-5 on IBM's DB2 Universal Database.

#index 397363
#* Skew handling techniques in sort-merge join
#@ Wei Li;Dengfeng Gao;Richard Thomas Snodgrass
#t 2002
#c 5
#% 77323
#% 77963
#% 114577
#% 136740
#% 333981
#% 387508
#% 411554
#% 442918
#% 463751
#% 463759
#% 480272
#% 480489
#% 480608
#% 480761
#% 480774
#% 480966
#% 659999
#% 689389
#! Joins are among the most frequently executed operations. Several fast join algorithms have been developed and extensively studied; these can be categorized as sort-merge, hash-based, and index-based algorithms. While all three types of algorithms exhibit excellent performance over most data, ameliorating the performance degradation in the presence of skew has been investigated only for hash-based algorithms. However, for sort-merge join, even a small amount of skew present in realistic data can result in a significant performance hit on a commercial DBMS. This paper examines the negative ramifications of skew in sort-merge join and proposes several refinements that deal effectively with data skew. Experiments show that some of these algorithms also impose virtually no penalty in the absence of data skew and are thus suitable for replacing existing sort-merge implementations. We also show how sort-merge band join performance is significantly enhanced with these refinements.

#index 397364
#* StatiX: making XML count
#@ Juliana Freire;Jayant R. Haritsa;Maya Ramanath;Prasan Roy;Jérôme Siméon
#t 2002
#c 5
#% 83148
#% 152585
#% 152835
#% 210190
#% 257873
#% 273922
#% 382963
#% 397379
#% 411554
#% 427219
#% 458836
#% 465018
#% 479806
#% 479956
#% 480488
#% 482092
#% 565457
#% 659924
#! The availability of summary data for XML documents has many applications, from providing users with quick feedback about their queries, to cost-based storage design and query optimization. StatiX is a novel XML Schema-aware statistics framework that exploits the structure derived by regular expressions (which define elements in an XML Schema) to pinpoint places in the schema that are likely sources of structural skew. As we discuss below, this information can be used to build concise, yet accurate, statistical summaries for XML data. StatiX leverages standard XML technology for gathering statistics, notably XML Schema validators, and it uses histograms to summarize both the structure and values in an XML document. In this paper we describe the StatiX system. We develop algorithms that decompose schemas to obtain statistics at different granularities and discuss how statistics can be gathered as documents are validated. We also present an experimental evaluation which demonstrates the accuracy and scalability of our approach and show an application of these statistics to cost-based XML storage design.

#index 397365
#* QURSED: querying and reporting semistructured data
#@ Yannis Papakonstantinou;Michalis Petropoulos;Vasilis Vassalos
#t 2002
#c 5
#% 26722
#% 108503
#% 123589
#% 213982
#% 229827
#% 248799
#% 275367
#% 281150
#% 296736
#% 297181
#% 318990
#% 330773
#% 333935
#% 334019
#% 459260
#% 479471
#% 480152
#% 482087
#% 504571
#! QURSED enables the development of web-based query forms and reports (QFRs) that query and report semistructured XML data, i.e., data that are characterized by nesting, irregularities and structural variance. The query aspects of a QFR are captured by its query set specification, which formally encodes multiple parameterized condition fragments and can describe large numbers of queries. The run-time component of QURSED produces XQuery-compliant queries by synthesizing fragments from the query set specification that have been activated during the interaction of the end-user with the QFR. The design-time component of QURSED, called QURSED Editor, semi-automates the development of the query set specification and its association with the visual components of the QFR by translating visual actions into appropriate query set specifications. We describe QURSED and illustrate how it accommodates the intricacies that the semistructured nature of the underlying database introduces. We specifically focus on the formal model of the query set specification, its generation via the QURSED Editor and its coupling with the visual aspects of the web-based form and report.

#index 397366
#* Storing and querying ordered XML using a relational database system
#@ Igor Tatarinov;Stratis D. Viglas;Kevin Beyer;Jayavel Shanmugasundaram;Eugene Shekita;Chun Zhang
#t 2002
#c 5
#% 172950
#% 273922
#% 286256
#% 333935
#% 333979
#% 333981
#% 340144
#% 428146
#% 465012
#% 479956
#% 480152
#% 480656
#! XML is quickly becoming the de facto standard for data exchange over the Internet. This is creating a new set of data management requirements involving XML, such as the need to store and query XML documents. Researchers have proposed using relational database systems to satisfy these requirements by devising ways to "shred" XML documents into relations, and translate XML queries into SQL queries over these relations. However, a key issue with such an approach, which has largely been ignored in the research literature, is how (and whether) the ordered XML data model can be efficiently supported by the unordered relational data model. This paper shows that XML's ordered data model can indeed be efficiently supported by a relational database system. This is accomplished by encoding order as a data value. We propose three order encoding methods that can be used to represent XML order in the relational data model, and also propose algorithms for translating ordered XPath expressions into SQL using these encoding methods. Finally, we report the results of an experimental study that investigates the performance of the proposed order encoding methods on a workload of ordered XML queries and updates.

#index 397367
#* Executing SQL over encrypted data in the database-service-provider model
#@ Hakan Hacigümüş;Bala Iyer;Chen Li;Sharad Mehrotra
#t 2002
#c 5
#% 319994
#% 333948
#% 384014
#% 393907
#% 427219
#% 479984
#% 548379
#% 659992
#! Rapid advances in networking and Internet technologies have fueled the emergence of the "software as a service" model for enterprise computing. Successful examples of commercially viable software services include rent-a-spreadsheet, electronic mail services, general storage services, disaster protection services. "Database as a Service" model provides users power to create, store, modify, and retrieve data from anywhere in the world, as long as they have access to the Internet. It introduces several challenges, an important issue being data privacy. It is in this context that we specifically address the issue of data privacy.There are two main privacy issues. First, the owner of the data needs to be assured that the data stored on the service-provider site is protected against data thefts from outsiders. Second, data needs to be protected even from the service providers, if the providers themselves cannot be trusted. In this paper, we focus on the second challenge. Specifically, we explore techniques to execute SQL queries over encrypted data. Our strategy is to process as much of the query as possible at the service providers' site, without having to decrypt the data. Decryption and the remainder of the query processing are performed at the client site. The paper explores an algebraic framework to split the query to minimize the computation at the client site. Results of experiments validating our approach are also presented.

#index 397368
#* Workflow management with service quality guarantees
#@ Michael Gillmann;Gerhard Weikum;Wolfgang Wonner
#t 2002
#c 5
#% 207505
#% 264062
#% 266179
#% 391633
#% 437739
#% 456017
#% 459005
#% 511743
#% 511920
#% 572302
#% 614638
#% 617398
#! Workflow management systems (WFMS) that are geared for the orchestration of business processes across multiple organizations are complex distributed systems: they consist of multiple workflow engines, application servers, and communication middleware servers such as ORBs, where each of these server types can be replicated on multiple computers for scalability and availability.Finding an appropriate system configuration with guaranteed application-specific quality of service in terms of throughput, response time, and tolerable downtime is a major challenge for human system administrators. This paper presents a tool that largely automates the task of configuring a distributed WFMS. Based on a suite of mathematical models, the tool derives the necessary degrees of replication for the various server types in order to meet specified goals for performance and availability as well as "performability" when service is degraded due to outages of individual servers. The paper describes the configuration tool, with emphasis on how to capture the load behavior of workflows in a realistic manner. We also present extensive experiments that evaluate the accuracy of the tool's underlying models and demonstrate the practical feasibility of automating the task of configuring a distributed WFMS. The experiments use a detailed simulation which in turn has been validated through measurements with the Mentor-lite prototype system.

#index 397369
#* Mining database structure; or, how to build a data quality browser
#@ Tamraparni Dasu;Theodore Johnson;S. Muthukrishnan;Vladislav Shkapenyuk
#t 2002
#c 5
#% 22948
#% 210188
#% 243166
#% 248882
#% 252608
#% 273682
#% 282942
#% 299984
#% 301169
#% 307632
#% 333881
#% 333990
#% 334025
#% 420072
#% 462472
#% 464837
#% 480134
#% 480156
#% 480499
#% 480628
#% 480654
#% 480805
#% 480810
#% 571297
#% 616528
#! Data mining research typically assumes that the data to be analyzed has been identified, gathered, cleaned, and processed into a convenient form. While data mining tools greatly enhance the ability of the analyst to make data-driven discoveries, most of the time spent in performing an analysis is spent in data identification, gathering, cleaning and processing the data. Similarly, schema mapping tools have been developed to help automate the task of using legacy or federated data sources for a new purpose, but assume that the structure of the data sources is well understood. However the data sets to be federated may come from dozens of databases containing thousands of tables and tens of thousands of fields, with little reliable documentation about primary keys or foreign keys.We are developing a system, Bellman, which performs data mining on the structure of the database. In this paper, we present techniques for quickly identifying which fields have similar values, identifying join paths, estimating join directions and sizes, and identifying structures in the database. The results of the database structure mining allow the analyst to make sense of the database content. This information can be used to e.g., prepare data for data mining, find foreign key joins for schema mapping, or identify steps to be taken to prevent the database from collapsing under the weight of its complexity.

#index 397370
#* A scalable hash ripple join algorithm
#@ Gang Luo;Curt J. Ellmann;Peter J. Haas;Jeffrey F. Naughton
#t 2002
#c 5
#% 58352
#% 136740
#% 201883
#% 210353
#% 227883
#% 252608
#% 273910
#% 273911
#% 479976
#% 482070
#% 503719
#! Recently, Haas and Hellerstein proposed the hash ripple join algorithm in the context of online aggregation. Although the algorithm rapidly gives a good estimate for many join-aggregate problem instances, the convergence can be slow if the number of tuples that satisfy the join predicate is small or if there are many groups in the output. Furthermore, if memory overflows (for example, because the user allows the algorithm to run to completion for an exact answer), the algorithm degenerates to block ripple join and performance suffers. In this paper, we build on the work of Haas and Hellerstein and propose a new algorithm that (a) combines parallelism with sampling to speed convergence, and (b) maintains good performance in the presence of memory overflow. Results from a prototype implementation in a parallel DBMS show that its rate of convergence scales with the number of processors, and that when allowed to run to completion, even in the presence of memory overflow, it is competitive with the traditional parallel hybrid hash join algorithm.

#index 397371
#* Exploiting statistics on query expressions for optimization
#@ Nicolas Bruno;Surajit Chaudhuri
#t 2002
#c 5
#% 32889
#% 43163
#% 58377
#% 210190
#% 248014
#% 248815
#% 273901
#% 273909
#% 300193
#% 333947
#% 333965
#% 411554
#% 427219
#% 464056
#% 480149
#% 480803
#% 482092
#% 482100
#% 565457
#% 632048
#! Statistics play an important role in influencing the plans produced by a query optimizer. Traditionally, optimizers use statistics built over base tables and assume independence between attributes while propagating statistical information through the query plan. This approach can introduce large estimation errors, which may result in the optimizer choosing inefficient execution plans. In this paper, we show how to extend a generic optimizer so that it also exploits statistics built on expressions corresponding to intermediate nodes of query plans. We show that in some cases, the quality of the resulting plans is significantly better than when only base-table statistics are available. Unfortunately, even moderately-sized schemas may have too many relevant candidate statistics. We introduce a workload-driven technique to identify a small subset of statistics that can provide significant benefits over just maintaining base-table statistics. Finally, we present experimental results on an implementation of our approach in Microsoft SQL Server 2000.

#index 397372
#* Partial results for online query processing
#@ Vijayshankar Raman;Joseph M. Hellerstein
#t 2002
#c 5
#% 152251
#% 227883
#% 248793
#% 248795
#% 273911
#% 277328
#% 300167
#% 300169
#% 300179
#% 334053
#% 461214
#% 464215
#% 479452
#% 479976
#% 480479
#% 480642
#% 480810
#% 571048
#% 571294
#% 619478
#% 715191
#! Traditional query processors generate full, accurate query results, either in batch or in pipelined fashion. We argue that this strict model is too rigid for exploratory queries over diverse and distributed data sources, such as sources on the Internet. Instead, we propose a looser model of querying in which a user submits a broad initial query outline, and the system continually generates partial result tuples that may contain values for only some of the output fields. The user can watch these partial results accumulate at the user interface, and accordingly refine the query by specifying their interest in different kinds of partial results.After describing our querying model and user interface, we present a query processing architecture for this model which is implemented in the Telegraph dataflow system. Our architecture is designed to generate partial results quickly, and to adapt query execution to changing user interests. The crux of this architecture is a dataflow operator that supports two kinds of reorderings: reordering of intermediate tuples within a dataflow, and reordering of query plan operators through which tuples flow. We study reordering policies that optimize for the quality of partial results delivered over time, and experimentally demonstrate the benefits of our architecture in this context.

#index 397373
#* Approximate XML joins
#@ Sudipto Guha;H. V. Jagadish;Nick Koudas;Divesh Srivastava;Ting Yu
#t 2002
#c 5
#% 210173
#% 210212
#% 227859
#% 244328
#% 244371
#% 248790
#% 281750
#% 333679
#% 479462
#% 480654
#% 480659
#% 546105
#% 659923
#! XML is widely recognized as the data interchange standard for tomorrow, because of its ability to represent data from a wide variety sources. Hence, XML is likely to be the format through which data from multiple sources is integrated.In this paper we study the problem of integrating XML data sources through correlations realized as join operations. A challenging aspect of this operation is the XML document structure. Two documents might convey approximately or exactly the same information but may be quite different in structure. Consequently approximate match in structure, in addition to, content has to be folded in the join operation. We quantify approximate match in structure and content using well defined notions of distance. For structure, we propose computationally inexpensive lower and upper bounds for the tree edit distance metric between two trees. We then show how the tree edit distance, and other metrics that quantify distance between trees, can be incorporated in a join framework. We introduce the notion of reference sets to facilitate this operation. Intuitively, a reference set consists of data elements used to project the data space. We characterize what constitutes a good choice of a reference set and we propose sampling based algorithms to identify them. This gives rise to a variety of algorithmic approaches for the problem, which we formulate and analyze. We demonstrate the practical utility of our solutions using large collections of real and synthetic XML data sets.

#index 397374
#* Efficient algorithms for minimizing tree pattern queries
#@ Prakash Ramanan
#t 2002
#c 5
#% 36683
#% 193212
#% 248025
#% 248026
#% 273897
#% 281149
#% 287029
#% 287339
#% 299943
#% 299944
#% 333989
#% 378393
#% 386455
#% 408396
#% 464724
#% 504578
#% 593696
#% 599549
#! We consider the problem of minimizing tree pattern queries (TPQ) that arise in XML and in LDAP-style network directories. In [Minimization of Tree Pattern Queries, Proc. ACM SIGMOD Intl. Conf. Management of Data, 2001, pp. 497-508], Amer-Yahia, Cho, Lakshmanan and Srivastava presented an O(n4) algorithm for minimizing TPQs in the absence of integrity constraints (Case 1); n is the number of nodes in the query. Then they considered the problem of minimizing TPQs in the presence of three kinds of integrity constraints: required-child, required-descendant and subtype (Case 2). They presented an O(n6) algorithm for minimizing TPQs in the presence of only required-child and required-descendant constraints (i.e., no subtypes allowed; Case 3). We present O(n2), O(n4) and O(n2) algorithms for minimizing TPQs in these three cases, respectively, based on the concept of graph simulation. We believe that our O(n2) algorithms for Cases 1 and 3 are runtime optimal.

#index 397375
#* Holistic twig joins: optimal XML pattern matching
#@ Nicolas Bruno;Nick Koudas;Divesh Srivastava
#t 2002
#c 5
#% 136740
#% 172924
#% 191574
#% 210186
#% 210187
#% 227932
#% 236416
#% 252366
#% 309851
#% 333981
#% 406493
#% 479806
#% 479956
#% 480152
#% 480317
#% 504578
#% 504580
#% 659999
#! XML employs a tree-structured data model, and, naturally, XML queries specify patterns of selection predicates on multiple elements related by a tree structure. Finding all occurrences of such a twig pattern in an XML database is a core operation for XML query processing. Prior work has typically decomposed the twig pattern into binary structural (parent-child and ancestor-descendant) relationships, and twig matching is achieved by: (i) using structural join algorithms to match the binary relationships against the XML database, and (ii) stitching together these basic matches. A limitation of this approach for matching twig patterns is that intermediate result sizes can get large, even when the input and output sizes are more manageable.In this paper, we propose a novel holistic twig join algorithm, TwigStack, for matching an XML query twig pattern. Our technique uses a chain of linked stacks to compactly represent partial results to root-to-leaf query paths, which are then composed to obtain matches for the twig pattern. When the twig pattern uses only ancestor-descendant relationships between elements, TwigStack is I/O and CPU optimal among all sequential algorithms that read the entire input: it is linear in the sum of sizes of the input lists and the final result list, but independent of the sizes of intermediate results. We then show how to use (a modification of) B-trees, along with TwigStack, to match query twig patterns in sub-linear time. Finally, we complement our analysis with experimental results on a range of real and synthetic data, and query twig patterns.

#index 397376
#* Efficient k-NN search on vertically decomposed data
#@ Arjen P. de Vries;Nikos Mamoulis;Niels Nes;Martin Kersten
#t 2002
#c 5
#% 120270
#% 201876
#% 201893
#% 227861
#% 239709
#% 248010
#% 248797
#% 286258
#% 300208
#% 427199
#% 479649
#% 479788
#% 480132
#% 480133
#% 480330
#% 481947
#% 571056
#% 632035
#! Applications like multimedia retrieval require efficient support for similarity search on large data collections. Yet, nearest neighbor search is a difficult problem in high dimensional spaces, rendering efficient applications hard to realize: index structures degrade rapidly with increasing dimensionality, while sequential search is not an attractive solution for repositories with millions of objects. This paper approaches the problem from a different angle. A solution is sought in an unconventional storage scheme, that opens up a new range of techniques for processing k-NN queries, especially suited for high dimensional spaces. The suggested (physical) database design accommodates well a novel variant of branch-and-bound search, that reduces the high dimensional space quickly to a small candidate set. The paper provides insight in applying this idea to k-NN search using two similarity metrics commonly encountered in image database applications, and discusses techniques for its implementation in relational database systems. The effectiveness of the proposed method is evaluated empirically on both real and synthetic data sets, reporting the significant improvements in response time yielded.

#index 397377
#* Time-parameterized queries in spatio-temporal databases
#@ Yufei Tao;Dimitris Papadias
#t 2002
#c 5
#% 86950
#% 116082
#% 152937
#% 201876
#% 237187
#% 248017
#% 248797
#% 273706
#% 287466
#% 300162
#% 300174
#% 300179
#% 461923
#% 462239
#% 479649
#% 479796
#% 479816
#% 480632
#% 481947
#! Time-parameterized queries (TP queries for short) retrieve (i) the actual result at the time that the query is issued, (ii) the validity period of the result given the current motion of the query and the database objects, and (iii) the change that causes the expiration of the result. Due to the highly dynamic nature of several spatio-temporal applications, TP queries are important both as standalone methods, as well as building blocks of more complex operations. However, little work has been done towards their efficient processing. In this paper, we propose a general framework that covers time-parameterized variations of the most common spatial queries, namely window queries, k-nearest neighbors and spatial joins. In particular, each of these TP queries is reduced to nearest neighbor search where the distance functions are defined according to the query type. This reduction allows the application and extension of well-known branch and bound techniques to the current problem. The proposed methods can be applied with mobile queries, mobile objects or both, given a suitable indexing method. Our experimental evaluation is based on R-trees and their extensions for dynamic objects.

#index 397378
#* Minimal probing: supporting expensive predicates for top-k queries
#@ Kevin Chen-Chuan Chang;Seung-won Hwang
#t 2002
#c 5
#% 55490
#% 70370
#% 152940
#% 172931
#% 174161
#% 191175
#% 210172
#% 213981
#% 227894
#% 248801
#% 300170
#% 333854
#% 333951
#% 479623
#% 479816
#% 480819
#% 591565
#! This paper addresses the problem of evaluating ranked top-k queries with expensive predicates. As major DBMSs now all support expensive user-defined predicates for Boolean queries, we believe such support for ranked queries will be even more important: First ranked queries often need to model user-specific concepts of preference, relevance, or similarity, which call for dynamic user-defined functions. Second, middleware systems must incorporate external predicates for integrating autonomous sources typically accessible only by per-object queries. Third, fuzzy joins are inherently expensive, as they are essentially user-defined operations that dynamically associate multiple relations. These predicates, being dynamically defined or externally accessed, cannot rely on index mechanisms to provide zero-time sorted output, and must instead require per-object probe to evaluate. The current standard sort-merge framework for ranked queries cannot efficiently handle such predicates because it must completely probe all objects, before sorting and merging them to produce top-k answers. To minimize expensive probes, we thus develop the formal principle of "necessary probes," which determines if a probe is absolutely required. We then propose Algorithm MPro which, by implementing the principle, is provably optimal with minimal probe cost. Further, we show that MPro can scale well and can be easily parallelized. Our experiments using both a real-estate benchmark database and synthetic datasets show that MPro enables significant probe reduction, which can be orders of magnitude faster than the standard scheme using complete probing.

#index 397379
#* Statistical synopses for graph-structured XML databases
#@ Neoklis Polyzotis;Minos Garofalakis
#t 2002
#c 5
#% 31484
#% 44876
#% 210190
#% 273902
#% 465018
#% 479806
#% 480488
#% 482092
#% 650962
#% 660000
#! Effective support for XML query languages is becoming increasingly important with the emergence of new applications that access large volumes of XML data. All existing proposals for querying XML (e.g., XQuery) rely on a pattern-specification language that allows path navigation and branching through the XML data graph in order to reach the desired data elements. Optimizing such queries depends crucially on the existence of concise synopsis structures that enable accurate compile-time selectivity estimates for complex path expressions over graph-structured XML data. In this paper, We introduce a novel approach to building and using statistical summaries of large XML data graphs for effective path-expression selectivity estimation. Our proposed graph-synopsis model (termed XSKETCH) exploits localized graph stability to accurately approximate (in limited space) the path and branching distribution in the data graph. To estimate the selectivities of complex path expressions over concise XSKETCH synopses, we develop an estimation framework that relies on appropriate statistical (uniformity and independence) assumptions to compensate for the lack of detailed distribution information. Given our estimation framework, we demonstrate that the problem of building an accuracy-optimal XSKETCH for a given amount of space is &#119977;&#119979;-hard, and propose an efficient heuristic algorithm based on greedy forward selection. Briefly, our algorithm constructs an XSKETCH synopsis by successive refinements of the label-split graph, the coarsest summary of the XML data graph. Our refinement operations act locally and attempt to capture important statistical correlations between data paths. Extensive experimental results with synthetic as well as real-life data sets verify the effectiveness of our approach. To the best of our knowledge, ours is the first work to address this timely problem in the most general setting of graph-structured data and complex (branching) path expressions.

#index 397380
#* Continually evaluating similarity-based pattern queries on a streaming time series
#@ Like Gao;X. Sean Wang
#t 2002
#c 5
#% 67552
#% 116082
#% 172949
#% 227857
#% 248831
#% 273704
#% 285711
#% 300179
#% 333941
#% 404335
#% 428155
#% 443298
#% 443369
#% 460862
#% 461885
#% 462500
#% 464851
#% 471511
#% 481609
#% 631866
#% 632089
#% 659996
#% 660004
#% 1809954
#% 1860826
#! In many applications, local or remote sensors send in streams of data, and the system needs to monitor the streams to discover relevant events/patterns and deliver instant reaction correspondingly. An important scenario is that the incoming stream is a continually appended time series, and the patterns are time series in a database. At each time when a new value arrives (called a time position), the system needs to find, from the database, the nearest or near neighbors of the incoming time series up to the time position. This paper attacks the problem by using Fast Fourier Transform (FFT) to efficiently find the cross correlations of time series, which yields, in a batch mode, the nearest and near neighbors of the incoming time series at many time positions. To take advantage of this batch processing in achieving fast response time, this paper uses prediction methods to predict future values. FFT is used to compute the cross correlations of the predicted series (with the values that have already arrived) and the database patterns, and to obtain predicted distances between the incoming time series at many future time positions and the database patterns. When the actual data value arrives, the prediction error together with the predicted distances is used to filter out patterns that are not possible to be the nearest or near neighbors, which provides fast responses. Experiments show that with reasonable prediction errors, the performance gain is significant.

#index 397381
#* General match: a subsequence matching method in time-series databases based on generalized windows
#@ Yang-Sae Moon;Kyu-Young Whang;Wook-Shin Han
#t 2002
#c 5
#% 67552
#% 86950
#% 164360
#% 172949
#% 191581
#% 227857
#% 248796
#% 273704
#% 460862
#% 462231
#% 464994
#% 479649
#% 481609
#% 631920
#% 631923
#% 632088
#! We generalize the method of constructing windows in subsequence matching. By this generalization, we can explain earlier subsequence matching methods as special cases of a common framework. Based on the generalization, we propose a new subsequence matching method, General Match. The earlier work by Faloutsos et al. (called FRM for convenience) causes a lot of false alarms due to lack of point-filtering effect. Dual Match, recently proposed as a dual approach of FRM, improves performance significantly over FRM by exploiting point filtering effect. However, it has the problem of having a smaller allowable window size---half that of FRM---given the minimum query length. A smaller window increases false alarms due to window size effect. General Match offers advantages of both methods: it can reduce window size effect by using large windows like FRM and, at the same time, can exploit point-filtering effect like Dual Match. General Match divides data sequences into generalized sliding windows (J-sliding windows) and the query sequence into generalized disjoint windows (J-disjoint windows). We formally prove that General Match is correct, i.e., it incurs no false dismissal. We then propose a method of estimating the optimal value of the sliding factor J that minimizes the number of page accesses. Experimental results for real stock data show that, for low selectivities (10-6∼10-4), General Match improves average performance by 117% over Dual Match and by 998% over FRM; for high selectivities (10-3∼10-1), by 45% over Dual Match and by 64% over FRM. The proposed generalization provides an excellent theoretical basis for understanding the underlying mechanisms of subsequence matching.

#index 397382
#* Clustering by pattern similarity in large data sets
#@ Haixun Wang;Wei Wang;Jiong Yang;Philip S. Yu
#t 2002
#c 5
#% 80995
#% 202011
#% 210173
#% 248792
#% 273891
#% 280417
#% 300131
#% 310507
#% 451052
#% 469422
#% 480124
#% 481281
#% 659967
#! Clustering is the process of grouping a set of objects into classes of similar objects. Although definitions of similarity vary from one clustering model to another, in most of these models the concept of similarity is based on distances, e.g., Euclidean distance or cosine distance. In other words, similar objects are required to have close values on at least a set of dimensions. In this paper, we explore a more general type of similarity. Under the pCluster model we proposed, two objects are similar if they exhibit a coherent pattern on a subset of dimensions. For instance, in DNA microarray analysis, the expression levels of two genes may rise and fall synchronously in response to a set of environmental stimuli. Although the magnitude of their expression levels may not be close, the patterns they exhibit can be very much alike. Discovery of such clusters of genes is essential in revealing significant connections in gene regulatory networks. E-commerce applications, such as collaborative filtering, can also benefit from the new model, which captures not only the closeness of values of certain leading indicators but also the closeness of (purchasing, browsing, etc.) patterns exhibited by the customers. Our paper introduces an effective algorithm to detect such clusters, and we perform tests on several real and synthetic data sets to show its effectiveness.

#index 397383
#* Mining long sequential patterns in a noisy environment
#@ Jiong Yang;Wei Wang;Philip S. Yu;Jiawei Han
#t 2002
#c 5
#% 18658
#% 172892
#% 248791
#% 300120
#% 310500
#% 310542
#% 310559
#% 329537
#% 338586
#% 342642
#% 420062
#% 420063
#% 459006
#% 459020
#% 463903
#% 464403
#% 464714
#% 465003
#% 479971
#% 481290
#% 481758
#% 481779
#% 565487
#% 614619
#! Pattern discovery in long sequences is of great importance in many applications including computational biology study, consumer behavior analysis, system performance analysis, etc. In a noisy environment, an observed sequence may not accurately reflect the underlying behavior. For example, in a protein sequence, the amino acid N is likely to mutate to D with little impact to the biological function of the protein. It would be desirable if the occurrence of D in the observation can be related to a possible mutation from N in an appropriate manner. Unfortunately, the support measure (i.e., the number of occurrences) of a pattern does not serve this purpose. In this paper, we introduce the concept of compatibility matrix as the means to provide a probabilistic connection from the observation to the underlying true value. A new metric match is also proposed to capture the "real support" of a pattern which would be expected if a noise-free environment is assumed. In addition, in the context we address, a pattern could be very long. The standard pruning technique developed for the market basket problem may not work efficiently. As a result, a novel algorithm that combines statistical sampling and a new technique (namely border collapsing) is devised to discover long patterns in a minimal number of scans of the sequence database with sufficiently high confidence. Empirical results demonstrate the robustness of the match model (with respect to the noise) and the efficiency of the probabilistic algorithm.

#index 397384
#* A Monte Carlo algorithm for fast projective clustering
#@ Cecilia M. Procopiuc;Michael Jones;Pankaj K. Agarwal;T. M. Murali
#t 2002
#c 5
#% 210173
#% 247889
#% 248790
#% 248792
#% 273891
#% 300131
#% 479962
#% 480132
#% 480307
#% 481281
#! We propose a mathematical formulation for the notion of optimal projective cluster, starting from natural requirements on the density of points in subspaces. This allows us to develop a Monte Carlo algorithm for iteratively computing projective clusters. We prove that the computed clusters are good with high probability. We implemented a modified version of the algorithm, using heuristics to speed up computation. Our extensive experiments show that our method is significantly more accurate than previous approaches. In particular, we use our techniques to build a classifier for detecting rotated human faces in cluttered images.

#index 397385
#* Dynamic multidimensional histograms
#@ Nitin Thaper;Sudipto Guha;Piotr Indyk;Nick Koudas
#t 2002
#c 5
#% 116084
#% 137885
#% 201921
#% 210190
#% 248822
#% 259995
#% 273901
#% 273902
#% 273903
#% 273907
#% 274152
#% 300193
#% 333931
#% 333947
#% 333983
#% 338425
#% 347226
#% 379444
#% 379445
#% 428155
#% 464215
#% 464876
#% 479648
#% 480465
#% 480628
#% 481749
#% 482092
#% 482123
#% 494333
#% 594012
#% 594029
#% 660004
#% 689389
#! Histograms are a concise and flexible way to construct summary structures for large data sets. They have attracted a lot of attention in database research due to their utility in many areas, including query optimization, and approximate query answering. They are also a basic tool for data visualization and analysis.In this paper, we present a formal study of dynamic multidimensional histogram structures over continuous data streams. At the heart of our proposal is the use of a dynamic summary data structure (vastly different from a histogram) maintaining a succinct approximation of the data distribution of the underlying continuous stream. On demand, an accurate histogram is derived from this dynamic data structure. We propose algorithms for extracting such an accurate histogram and we analyze their behavior and tradeoffs. The proposed algorithms are able to provide approximate guarantees about the quality of the estimation of the histograms they extract.We complement our analytical results with a thorough experimental evaluation using real data sets.

#index 397386
#* Selectivity estimation for spatio-temporal queries to moving objects
#@ Yong-Jin Choi;Chin-Wan Chung
#t 2002
#c 5
#% 152902
#% 273706
#% 273887
#% 299979
#% 300173
#% 300174
#% 461923
#% 464847
#% 480473
#% 480817
#% 482123
#! A query optimizer requires selectivity estimation of a query to choose the most efficient access plan. An effective method of selectivity estimation for the future locations of moving objects has not yet been proposed. Existing methods for spatial selectivity estimation do not accurately estimate the selectivity of a query to moving objects, because they do not consider the future locations of moving objects, which change continuously as time passes.In this paper, we propose an effective method for spatio-temporal selectivity estimation to solve this problem. We present analytical formulas which accurately calculate the selectivity of a spatio-temporal query as a function of spatio-temporal information. Extensive experimental results show that our proposed method accurately estimates the selectivity over various queries to spatio-temporal data combining real-life spatial data and synthetic temporal data. When Tiger/lines is used as real-life spatial data, the application of an existing method for spatial selectivity estimation to the estimation of the selectivity of a query to moving objects has the average error ratio from 14% to 85%, whereas our method for spatio-temporal selectivity estimation has the average error ratio from 9% to 23%.

#index 397387
#* Hierarchical subspace sampling: a unified framework for high dimensional data reduction, selectivity estimation and nearest neighbor search
#@ Charu C. Aggarwal
#t 2002
#c 5
#% 86950
#% 201876
#% 201893
#% 248822
#% 300131
#% 300193
#% 333881
#% 333946
#% 333954
#% 342828
#% 479648
#% 480124
#% 480307
#% 481290
#% 482092
#! With the increased abilities for automated data collection made possible by modern technology, the typical sizes of data collections have continued to grow in recent years. In such cases, it may be desirable to store the data in a reduced format in order to improve the storage, transfer time, and processing requirements on the data. One of the challenges of designing effective data compression techniques is to be able to preserve the ability to use the reduced format directly for a wide range of database and data mining applications. In this paper, we propose the novel idea of hierarchical subspace sampling in order to create a reduced representation of the data. The method is naturally able to estimate the local implicit dimensionalities of each point very effectively, and thereby create a variable dimensionality reduced representation of the data. Such a technique has the advantage that it is very adaptive about adjusting its representation depending upon the behavior of the immediate locality of a data point. An interesting property of the subspace sampling technique is that unlike all other data reduction techniques, the overall efficiency of compression improves with increasing database size. This is a highly desirable property for any data reduction system since the problem itself is motivated by the large size of data sets. Because of its sampling approach, the procedure is extremely fast and scales linearly both with data set size and dimensionality. Furthermore, the subspace sampling technique is able to reveal important local subspace characteristics of high dimensional data which can be harnessed for effective solutions to problems such as selectivity estimation and approximate nearest neighbor search.

#index 397388
#* Dwarf: shrinking the PetaCube
#@ Yannis Sismanis;Antonios Deligiannakis;Nick Roussopoulos;Yannis Kotidis
#t 2002
#c 5
#% 210182
#% 227868
#% 227880
#% 227883
#% 248805
#% 248812
#% 259995
#% 273916
#% 280448
#% 300195
#% 316978
#% 462204
#% 464215
#% 464706
#% 479450
#% 479476
#% 479646
#% 479795
#% 481951
#! Dwarf is a highly compressed structure for computing, storing, and querying data cubes. Dwarf identifies prefix and suffix structural redundancies and factors them out by coalescing their store. Prefix redundancy is high on dense areas of cubes but suffix redundancy is significantly higher for sparse areas. Putting the two together fuses the exponential sizes of high dimensional full cubes into a dramatically condensed data structure. The elimination of suffix redundancy has an equally dramatic reduction in the computation of the cube because recomputation of the redundant suffixes is avoided. This effect is multiplied in the presence of correlation amongst attributes in the cube. A Petabyte 25-dimensional cube was shrunk this way to a 2.3GB Dwarf Cube, in less than 20 minutes, a 1:400000 storage reduction ratio. Still, Dwarf provides 100% precision on cube queries and is a self-sufficient structure which requires no access to the fact table. What makes Dwarf practical is the automatic discovery,in a single pass over the fact table, of the prefix and suffix redundancies without user involvement or knowledge of the value distributions.This paper describes the Dwarf structure and the Dwarf cube construction algorithm. Further optimizations are then introduced for improving clustering and query performance. Experiments with the current implementation include comparisons on detailed measurements with real and synthetic datasets against previously published techniques. The comparisons show that Dwarfs by far out-perform these techniques on all counts: storage space, creation time, query response time, and updates of cubes.

#index 397389
#* Wavelet synopses with error guarantees
#@ Minos Garofalakis;Phillip B. Gibbons
#t 2002
#c 5
#% 38894
#% 116084
#% 168862
#% 190611
#% 248822
#% 257637
#% 273902
#% 273909
#% 480306
#% 480465
#% 480628
#! Recent work has demonstrated the effectiveness of the wavelet decomposition in reducing large amounts of data to compact sets of wavelet coefficients (termed "wavelet synopses") that can be used to provide fast and reasonably accurate approximate answers to queries. A major criticism of such techniques is that unlike, for example, random sampling, conventional wavelet synopses do not provide informative error guarantees on the accuracy of individual approximate answers. In fact, as this paper demonstrates, errors can vary widely (without bound) and unpredictably, even for identical queries on nearly-identical values in distinct parts of the data. This lack of error guarantees severely limits the practicality of traditional wavelets as an approximate query-processing tool, because users have no idea of the quality of any particular approximate answer. In this paper, we introduce Probabilistic Wavelet Synopses, the first wavelet-based data reduction technique with guarantees on the accuracy of individual approximate answers. Whereas earlier approaches rely on deterministic thresholding for selecting a set of "good" wavelet coefficients, our technique is based on a novel, probabilistic thresholding scheme that assigns each coefficient a probability of being retained based on its importance to the reconstruction of individual data values, and then flips coins to select the synopsis. We show how our scheme avoids the above pitfalls of deterministic thresholding, providing highly-accurate answers for individual data values in a data vector. We propose several novel optimization algorithms for tuning our probabilistic thresholding scheme to minimize desired error metrics. Experimental results on real-world and synthetic data sets evaluate these algorithms, and demonstrate the effectiveness of our probabilistic wavelet synopses in providing fast, highly-accurate answers with error guarantees.

#index 397390
#* Compressing SQL workloads
#@ Surajit Chaudhuri;Ashish Kumar Gupta;Vivek Narasayya
#t 2002
#c 5
#% 1932
#% 115562
#% 249176
#% 271130
#% 273901
#% 300195
#% 316709
#% 333947
#% 333955
#% 408396
#% 465162
#% 480471
#% 480803
#% 481749
#% 482100
#% 482123
#% 632048
#% 632056
#% 632100
#! Recently several important relational database tasks such as index selection, histogram tuning, approximate query processing, and statistics selection have recognized the importance of leveraging workloads. Often these tasks are presented with large workloads, i.e., a set of SQL DML statements, as input. A key factor affecting the scalability of such tasks is the size of the workload. In this paper, we present the novel problem of workload compression which helps improve the scalability of such tasks. We present a principled solution to this challenging problem. Our solution is broadly applicable to a variety of workload-driven tasks, while allowing for incorporation of task specific knowledge. We have implemented this solution and our experiments illustrate its effectiveness in the context of two workload-driven tasks: index selection and approximate query processing.

#index 397391
#* Coordinating backup/recovery and data consistency between database and file systems
#@ Suparna Bhattacharya;C. Mohan;Karen W. Brannon;Inderpal Narang;Hui-I Hsiao;Mahadevan Subramanian
#t 2002
#c 5
#% 114582
#% 152926
#% 172939
#% 210178
#% 210196
#% 298601
#% 300206
#% 481762
#! Managing a combined store consisting of database data and file data in a robust and consistent manner is a challenge for database systems and content management systems. In such a hybrid system, images, videos, engineering drawings, etc. are stored as files on a file server while meta-data referencing/indexing such files is created and stored in a relational database to take advantage of efficient search. In this paper we describe solutions for two potentially problematic aspects of such a data management system: backup/recovery and data consistency. We present algorithms for performing backup and recovery of the DBMS data in a coordinated fashion with the files on the file servers. Our algorithms for coordinated backup and recovery have been implemented in the IBM DB2/DataLinks product [1]. We also propose an efficient solution to the problem of maintaining consistency between the content of a file and the associated meta-data stored in the DBMS from a reader's point of view without holding long duration locks on meta-data tables. In the model, an object is directly accessed and edited in-place through normal file system APIs using a reference obtained via an SQL Query on the database. To relate file modifications to meta-data updates, the user issues an update through the DBMS, and commits both file and meta-data updates together.

#index 397392
#* An ebXML infrastructure implementation through UDDI registries and RosettaNet PIPs
#@ Asuman Dogac;Yusuf Tambag;Pinar Pembecioglu;Sait Pektas;Gokce Laleci;Gokhan Kurt;Serkan Toprak;Yildiray Kabak
#t 2002
#c 5
#! Today's Internet based businesses need a level of interoperability which will allow trading partners to seamlessly and dynamically come together and do business without ad hoc and proprietary integrations. Such a level of interoperability involves being able to find potential business partners, discovering their services and business processes, and conducting business "on the fly". This process of dynamic interoperation is only possible through standard B2B frameworks. Indeed a number of B2B electronic commerce standard frameworks have emerged recently. Although most of these standards are overlapping and competing, each with its own strenghts and weeknesses, a closer investigation reveals that they can be used in a manner to complement one another.In this paper we describe such an implementation where an ebXML infrastructure is developed by exploiting the Universal Description, Discovery and Integration (UDDI) registries and RosettaNet Partner Interface Processes (PIPs). ebXML is an ambitious effort and produced detailed specifications of an infrastructure both for B2B and B2C e-commerce. However a public ebXML compliant registry/repository mechanism is not available yet. On the other hand, UDDI's approach to developing a registry has been a lot simpler and public registries are available. In ebXML, trading parties collaborate by agreeing on the same business process with complementary roles. Therefore there is a need for standardized business processes. In this respect, exploiting the already developed expertise through RosettaNet PIPs becomes indispensable. We show how to create and use ebXML "Binary Collaborations" based on RosettaNet PIPs and provide a GUI tool to allow users to graphically build their ebXML business processes by combining RosettaNet PIPs. In ebXML, trading parties reveal essential information about themselves through Collaboration Protocol Profiles (CPPs). To conduct business, an agreement between parties is necessary and this is expressed

#index 397393
#* Garlic: a new flavor of federated query processing for DB2
#@ Vanja Josifovski;Peter Schwarz;Laura Haas;Eileen Lin
#t 2002
#c 5
#% 116043
#% 340625
#% 479449
#% 479452
#% 479951
#% 480128
#% 480134
#% 614579
#! In a large modern enterprise, information is almost inevitably distributed among several database management systems. Despite considerable attention from the research community, relatively few commercial systems have attempted to address this issue. This paper describes new technology that enables clients of IBM's DB2 Universal Database to access the data and specialized computational capabilities of a wide range of non-relational data sources. This technology, based on the Garlic prototype developed at the Almaden Research Center, complements and extends DB2's existing ability to federate relational data sources.The paper focuses on three topics. Firstly, we show how the DB2 catalogs are used as an extensible repository for the metadata needed to access remotely-stored information. Secondly, we describe how the Garlic approach to query planning, in which source-specific modules and the federated server cooperate to develop an optimized execution plan, has been realized in DB2. Lastly, we describe how DB2's query execution engine has been extended to support queries and functions that are evaluated remotely.

#index 397394
#* A compact B-tree
#@ Peter Bumbulis;Ivan T. Bowman
#t 2002
#c 5
#% 115663
#% 134526
#% 154310
#% 271801
#% 282025
#% 286835
#% 288578
#% 300181
#% 317933
#% 330927
#% 333942
#% 356652
#% 464987
#% 465020
#% 480656
#! In this paper we describe a Patricia tree-based B-tree variant suitable for OLTP. In this variant, each page of the B-tree contains a local Patricia tree instead of the usual sorted array of keys. It has been implemented in iAnywhere ASA Version 8.0. Preliminary experience has shown that these indexes can provide significant space and performance benefits over existing ASA indexes.

#index 397395
#* Efficient execution of joins in a star schema
#@ Andreas Weininger
#t 2002
#c 5
#% 191154
#% 208037
#% 262309
#% 279164
#% 348640
#% 355051
#% 390850
#! A star schema is very popular for modeling data warehouses and data marts. Therefore, it is important that a database system which is used for implementing such a data warehouse or data mart is able to efficiently handle operations on such a schema. In this paper we will describe how one of these operations, the join operation --- probably the most important operation --- is implemented in the IBM Informix Extended Parallel Server (XPS).

#index 397396
#* Quadtree and R-tree indexes in oracle spatial: a comparison using GIS data
#@ Ravi Kanth V Kothuri;Siva Ravada;Daniel Abugov
#t 2002
#c 5
#% 25152
#% 68091
#% 78374
#% 86950
#% 88056
#% 171554
#% 194253
#% 201876
#% 201880
#% 213975
#% 227864
#% 227939
#% 248798
#% 252304
#% 260073
#% 273941
#% 287379
#% 427199
#% 435141
#% 442745
#% 443329
#% 462059
#% 464195
#% 465014
#% 467248
#% 480093
#% 480632
#% 481956
#% 526851
#% 527004
#% 527026
#% 527193
#! Spatial indexing has been one of the active focus areas in recent database research. Several variants of Quadtree and R-tree indexes have been proposed in database literature. In this paper, we first describe briefly our implementation of Quadtree and R-tree index structures and related optimizations in Oracle Spatial. We then examine the relative merits of two structures as implemented in Oracle Spatial and compare their performance for different types of queries and other operations. Finally, we summarize experiences with these different structures in indexing large GIS datasets in Oracle Spatial.

#index 397397
#* Automating physical database design in a parallel database
#@ Jun Rao;Chun Zhang;Nimrod Megiddo;Guy Lohman
#t 2002
#c 5
#% 1327
#% 36119
#% 43171
#% 83154
#% 102765
#% 115661
#% 164372
#% 170893
#% 188719
#% 248855
#% 321250
#% 369236
#% 411554
#% 480158
#% 480298
#% 481116
#% 632100
#% 697170
#! Physical database design is important for query performance in a shared-nothing parallel database system, in which data is horizontally partitioned among multiple independent nodes. We seek to automate the process of data partitioning. Given a workload of SQL statements, we seek to determine automatically how to partition the base data across multiple nodes to achieve overall optimal (or close to optimal) performance for that workload. Previous attempts use heuristic rules to make those decisions. These approaches fail to consider all of the interdependent aspects of query performance typically modeled by today's sophisticated query optimizers.We present a comprehensive solution to the problem that has been tightly integrated with the optimizer of a commercial shared-nothing parallel database system. Our approach uses the query optimizer itself both to recommend candidate partitions for each table that will benefit each query in the workload, and to evaluate various combinations of these candidates. We compare a rank-based enumeration method with a random-based one. Our experimental results show that the former is more effective.

#index 397398
#* The SDSS skyserver: public access to the sloan digital sky server data
#@ Alexander S. Szalay;Jim Gray;Ani R. Thakar;Peter Z. Kunszt;Tanu Malik;Jordan Raddick;Christopher Stoughton;Jan vandenBerg
#t 2002
#c 5
#% 300171
#% 300185
#! The SkyServer provides Internet access to the public Sloan Digital Sky Survey (SDSS) data for both astronomers and for science education. This paper describes the SkyServer goals and architecture. It also describes our experience operating the SkyServer on the Internet. The SDSS data is public and well-documented so it makes a good test platform for research on database algorithms and performance.

#index 397399
#* TPC-DS, taking decision support benchmarking to the next level
#@ Meikel Poess;Bryan Smith;Lubor Kollar;Paul Larson
#t 2002
#c 5
#% 328431
#! TPC-DS is a new decision support benchmark currently under development by the Transaction Processing Performance Council (TPC). This paper provides a brief overview of the new benchmark. The benchmark models the decision support functions of a retail product supplier, including data loading, multiple types of queries and data maintenance. The database consists of multiple snowflake schemas with shared dimension tables; data is skewed; and the query set is large. Overall, the benchmark is considerably more realistic than previous decision support benchmarks.

#index 397400
#* Mid-tier caching: the TimesTen approach
#@  The TimesTen Team
#t 2002
#c 5
#% 397402
#% 480293
#% 480818
#! TimesTen is an in-memory, application-tier data manager that delivers low response time and high throughput. Applications may create tables and manage them exclusively in TimesTen, and they may optionally cache frequently used subsets of a disk-based relational database in TimesTen. Cached tables and tables managed exclusively by TimesTen may coexist in the same database. Queries and updates to the cache are performed by the application through SQL. Applications running on different mid-tier servers may cache different or overlapping subsets of the same back-end database. TimesTen keeps the caches synchronized with each other and with the back-end database.

#index 397401
#* Web caching for database applications with Oracle Web Cache
#@ Jesse Anton;Lawrence Jacobs;Xiang Liu;Jordan Parker;Zheng Zeng;Tie Zhong
#t 2002
#c 5
#% 302760
#% 333995
#% 480818
#! We discuss several important issues specific to Web caching for content dynamically generated from database applications. We present the techniques employed by Oracle Web Cache to address these issues. They include: content disambiguation based on information in addition to the URL, transparent session management, partial-page caching for personalization, and broad-scope invalidation with performance assurance heuristics.

#index 397402
#* Middle-tier database caching for e-business
#@ Qiong Luo;Sailesh Krishnamurthy;C. Mohan;Hamid Pirahesh;Honguk Woo;Bruce G. Lindsay;Jeffrey F. Naughton
#t 2002
#c 5
#% 210176
#% 248806
#% 279164
#% 281184
#% 300177
#% 333995
#% 335726
#% 479951
#% 480495
#% 480814
#% 480818
#% 481916
#% 566137
#% 571216
#! While scaling up to the enormous and growing Internet population with unpredictable usage patterns, E-commerce applications face severe challenges in cost and manageability, especially for database servers that are deployed as those applications' backends in a multi-tier configuration. Middle-tier database caching is one solution to this problem. In this paper, we present a simple extension to the existing federated features in DB2 UDB, which enables a regular DB2 instance to become a DBCache without any application modification. On deployment of a DBCache at an application server, arbitrary SQL statements generated from the unchanged application that are intended for a backend database server, can be answered: at the cache, at the backend database server, or at both locations in a distributed manner. The factors that determine the distribution of workload include the SQL statement type, the cache content, the application requirement on data freshness, and cost-based optimization at the cache. We have developed a research prototype of DBCache, and conducted an extensive set of experiments with an E-Commerce benchmark to show the benefits of this approach and illustrate tradeoffs in caching considerations.

#index 397403
#* DBCache: database caching for web application servers
#@ Mehmet Altinel;Qiong Luo;Sailesh Krishnamurthy;C. Mohan;Hamid Pirahesh;Bruce G. Lindsay;Honguk Woo;Larry Brown
#t 2002
#c 5
#% 397402
#% 480814
#! Many e-Business applications today are being developed and deployed on multi-tier environments involving browser-based clients, web application servers and backend databases. The dynamic nature of these applications necessitates generating web pages on-demand, making middle-tier database caching an effective approach to achieve high scalability and performance [3]. In the DBCache project, we are incorporating a database cache feature in DB2 UDB by modifying the engine code and leveraging existing federated database functionality. This allows us to take advantage of DB2's sophisticated distributed query processing power for database caching. As a result, the user queries can be executed at either the local database cache or the remote backend server, or more importantly, the query can be partitioned and then distributed to both databases for cost optimum execution.DBCache also includes a cache initialization component that takes a backend database schema and SQL queries in the workload, and generates a middle-tier database schema for the cache. We have implemented an initial prototype of the system that supports table level caching. As DB2's functionality is extended, we will be able to support subtable level caching, XML data caching and caching of execution results of web services.

#index 397404
#* Learning table access cardinalities with LEO
#@ Volker Markl;Guy Lohman
#t 2002
#c 5
#! LEO is a comprehensive way to repair incorrect statistics and cardinality estimates of a query execution plan. LEO introduces a feedback loop to query optimization that enhances the available information on the database where the most queries have occurred, allowing the optimizer to actually learn from its past mistakes. We demonstrate how LEO learns outdated table access statistics on a TPC-H like database schema and show that LEO improves the estimates for table cardinalities as well as filter factors for single predicates. Thus LEO enables the query optimizer to choose a better query execution plan, resulting in more efficient query processing. We not only demonstrate learning by repetitive execution of a single query, but also illustrate how similar, but not identical queries benefit from learned knowledge. In addition, we show the effect of both learning cardinalities and adjusting related statistics.

#index 397405
#* Rainbow: mapping-driven XQuery processing system
#@ Xin Zhang;Mukesh Mulchandani;Steffen Christ;Brian Murphy;Elke A. Rundensteiner
#t 2002
#c 5
#% 479330
#% 480317

#index 397406
#* The XXL search engine: ranked retrieval of XML data using indexes and ontologies
#@ Anja Theobald;Gerhard Weikum
#t 2002
#c 5
#% 458829
#% 504581

#index 397407
#* ToXgene: a template-based data generator for XML
#@ Denilson Barbosa;Alberto Mendelzon;John Keenleyside;Kelly Lyons
#t 2002
#c 5

#index 397408
#* Visual COKO: a debugger for query optimizer development
#@ Daniel J. Abadi;Mitch Cherniack
#t 2002
#c 5
#% 210203
#% 248789
#! Query optimization generates plans to retrieve data requested by queries. Query rewriting, which is the first step of this process, rewrites a query expression into an equivalent form to prepare it for plan generation. COKO-KOLA introduced a new approach to query rewriting that enables query rewrites to be formally verified using an automated theorem prover [1]. KOLA is a language for expressing term rewriting rules that can be "fired" on query expressions. COKO is a language for expressing query rewriting transformations that are too complex to express with simple KOLA rules [2].COKO is a programming language designed for query optimizer development. Programming languages require debuggers, and in this demonstration, we illustrate our COKO debugger: Visual COKO. Visual COKO enables a query optimization developer to visually trace the execution of a COKO transformation. At every step of the transformation, the developer can view a tree-display that illustrates how the original query expression has evolved.

#index 397409
#* XCache: a semantic caching system for XML queries
#@ Li Chen;Elke A. Rundensteiner;Song Wang
#t 2002
#c 5
#% 313640
#% 481916
#% 504585
#! A wide range of Web applications retrieve desired information from remote XML data sources across the Internet, which is usually costly due to transmission delays for large volumes of data. Therefore we propose to apply the ideas of semantic caching to XML query processing systems [2], in particular the XQuery engine. Semantic caching [3] implies view-based query answering and cache management. While it is well studied in the traditional database context, query containment for XQuery is left unexplored due to its complexity coming with the powerful expressiveness of hierarchy, recursion and result construction. We hence have developed the first solution for XQuery processing using cached views.We exploit the connections between XML and tree automata, and use subtype relations between two regular expression types to tackle the XQuery containment mapping problem. Inspired by XDuce [1], which explores the use of tree-automata-based regular expression types for XML processing, we have designed a containment mapping process to incorporate type inference and subtyping mechanisms provided by XDuce to establish containment mappings between regular-expression-type-based pattern variables of two queries. We have implemented a semantic caching system called XCache (see Figure 1), to realize the proposed containment and rewriting techniques for XQuery.The main modules of XCache include: (1) Query Decomposer. An input query is is decomposed into source-specific subqueries explicitly represented by matching patterns and return structures. (2) Query Pattern Register. By registering a few queries into semantic regions, we warm up XCache at its initialization phase. (3) Query Containment Mapper. The XDuce subtyper is incorporated into the containment mapper for establishing query containment mappings between variables of a new query and each cached query. (4) Query Rewriter. We implement the classical bucket algorithm and further apply heuristics to decide on an "optimal" rewriting plan if several valid ones exist. (5) Replacement Manager. We free space for new regions by both complete and partial replacement. (6) Region Coalescer. We apply a coalescing strategy to control the region granularity over time.

#index 397410
#* ACDN: a content delivery network for applications
#@ Pradnya Karbhari;Michael Rabinovich;Zhen Xiao;Fred Douglis
#t 2002
#c 5

#index 397411
#* COMMIX: towards effective web information extraction, integration and query answering
#@ Tengjiao Wang;Shiwei Tang;Dongqing Yang;Jun Gao;Yuqing Wu;Jian Pei
#t 2002
#c 5
#! As WWW becomes more and more popular and powerful, how to search information on the web in database way becomes an important research topic. COMMIX, which is developed in the DB group in Peking University (China), is a system towards building very large database using data from the Web for information extraction, integration and query answering. COMMIX has some innovative features, such as ontology-based wrapper generation, XML-based information integration, view-based query answering, and QBE-style XML query interface.

#index 397412
#* COUGAR: the network is the database
#@ Wai Fu Fung;David Sun;Johannes Gehrke
#t 2002
#c 5
#% 281556
#% 309433
#% 318109
#! The widespread distribution and availability of small-scale sensors, actuators, and embedded processors is transforming the physical world into a computing platform. One such example is a sensor network consisting of a large number of sensor nodes that combine physical sensing capabilities such as temperature, light, or seismic sensors with networking and computation capabilities [1]. Applications range from environmental control, warehouse inventory, health care to military environments. Existing sensor networks assume that the sensors are preprogrammed and send data to a central frontend where the data is aggregated and stored for offfsline querying and analysis. This approach has two major draw-backs. First, the user cannot change the behavior of the system on the fly. Second, communication in today's networks is orders of magnitude more expensive than local computation, thus in-network processing can vastly reduce resource usage and thus extend the lifetime of a sensor network.This demo demonstrates a database approach to unite the seemingly conflicting requirements of scalability and flexibility in monitoring the physical world. We demonstrate the COUGAR System, a new distributed data management infrastructure that scales with the growth of sensor interconnectivity and computational power on the sensors over the next decades. Our system resides directly on the sensor nodes and creates the abstraction of a single processing node without centralizing data or computation.

#index 397413
#* Distributing queries over low-power wireless sensor networks
#@ Samuel Madden;Joseph M. Hellerstein
#t 2002
#c 5
#% 143040
#% 228299
#% 281557
#% 428410

#index 397414
#* Gigascope: high performance network monitoring with an SQL interface
#@ Chuck Cranor;Yuan Gao;Theodore Johnson;Vlaidslav Shkapenyuk;Oliver Spatscheck
#t 2002
#c 5
#! Operators of large networks and providers of network services need to monitor and analyze the network traffic flowing through their systems. Monitoring requirements range from the long term (e.g., monitoring link utilizations, computing traffic matrices) to the ad-hoc (e.g. detecting network intrusions, debugging performance problems). Many of the applications are complex (e.g., reconstruct TCP/IP sessions), query layer-7 data (find streaming media connections), operate over huge volumes of data (Gigabit and higher speed links), and have real-time reporting requirements (e.g., to raise performance or intrusion alerts).We have found that existing network monitoring technologies have severe limitations. One option is to use TCPdump to monitor a network port and a user-level application program to process the data. While this approach is very flexible, it is not fast enough to handle gigabit speeds on inexpensive equipment. Another approach is to use network monitoring devices. While these devices are capable of high speed monitoring, they are inflexible as the set of monitoring tasks is pre-defined. Adding new functionality is expensive and has long lead times. A similar approach is to use monitoring tools built into routers, such as SNMP, RMON, or NetFlow. These tools have similar characteristics --- fast but inflexible.A further problem with all of these tools is their lack of a query interface. The data from the monitors are dumped to a file or piped through a file stream without an association to the semantics of the data. The burden of managing and interpreting the data is left to the analyst. Due to the volume and complexity of the data, the burden can be severe. These problems make developing new applications needlessly slow and difficult. Also, many mistakes are made leading to incorrect analyses.

#index 397415
#* RoadRunner: automatic data extraction from data-intensive web sites
#@ Valter Crescenzi;Giansalvatore Mecca;Paolo Merialdo
#t 2002
#c 5
#% 480824

#index 397416
#* XL: a platform for web services
#@ Daniela Florescu;Andreas Grünhagen;Donald Kossmann;Steffen Rost
#t 2002
#c 5
#! We present a platform for Web services. Web services are implemented in a special XML programming language called XL [1, 2]. A Web service receives an XML message as input and returns an XML message as output. The platform supports a number of features that are particularly useful to implement Web services; e.g., logging, timetables, conversations, workflow management, automatic transactions, security. Our platform is going to be compliant with all W3C standards and emerging proposals. The programming language is very abstract and can be optimized automatically (like SQL). Furthermore, the platform allows to integrate Web services that are written in XL and other programming languages.

#index 397417
#* CubeExplorer: online exploration of data cubes
#@ Jiawei Han;Jianyong Wang;Guozhu Dong;Jian Pei;Ke Wang
#t 2002
#c 5
#% 333925
#% 480630
#! Data cube enables fast online analysis of large data repositories which is attractive in many applications. Although there are several kinds of available cube-based OLAP products, users may still encounter challenges on effectiveness and efficiency in the exploration of large data cubes due to the huge computation space as well as the huge observation space in a data cube. CubeExplorer is an integrated environment for online exploration of data cubes. It integrates our newly developed techniques on iceberg cube computation [2], cube-based feature extraction, and gradient analysis [1], and makes cube exploration effective and efficient. In this demo, we will show the features of CubeExplorer, especially its power and flexibility at exploring and mining of large databases.

#index 397418
#* DBXplorer: enabling keyword search over relational databases
#@ Sanjay Agrawal;Surajit Chaudhuri;Gautam Das
#t 2002
#c 5
#% 659990

#index 397419
#* GEA: a toolkit for gene expression analysis
#@ Jessica M. Phan;Raymond Ng
#t 2002
#c 5
#% 480318
#! Currently gene expression data are being produced at a phenomenal rate. The general objective is to try to gain a better understanding of the functions of cellular tissues. In particular, one specific goal is to relate gene expression to cancer diagnosis, prognosis and treatment. However, a key obstacle is that the availability of analysis tools or lack thereof, impedes the use of the data, making it difficult for cancer researchers to perform analysis efficiently and effectively.

#index 397420
#* HD-Eye: visual clustering of high dimensional data
#@ Alexander Hinneburg;Daniel A. Keim;Markus Wawryniuk
#t 2002
#c 5
#! Clustering of large data bases is an important research area with a large variety of applications in the data base context. Missing in most of the research efforts are means for guiding the clustering process and understanding the results, which is especially important for high dimensional data. Visualization technology may help to solve this problem since it provides effective support of different clustering paradigms and allows a visual inspection of the results. The HD-Eye (high-dim. eye) system shows that a tight integration of advanced clustering algorithms and state-of-the-art visualization techniques is powerful for a better understanding and effective guidance of the clustering process, and therefore can help to significantly improve the clustering results.

#index 397421
#* XBase: making your gigabyte disk queriable
#@ Hongjun Lu;Guoren Wang;Ge Yu;Yubin Bao;Jianhua Lv;Yaxin Yu
#t 2002
#c 5
#! With the rapid development of the Internet and the World Wide Web (WWW), very large amount of information is available and ready for downloading, most of which are free of charge. At the same time, hard disks with large capacity are available at affordable prices. Most of us nowadays often dump a large number of various types of documents into our computers without much thinking. On the other hand, file systems have not changed too much during the past decades. Most of them organize files in directories that form a tree structure, and a file is identified by its name and pathname in the directory tree. Remembering name of files created sometime ago and digging them out from a disk with dozen gigabytes of data in hundred thousands of files becomes never an easy task. Tools available for helping such a search are still far from satisfactory.Xbase (XML-based document BASE) is a prototype system aiming at addressing the above problem. By XML-based, we meant that XML is used to define the metadata. The current version of XBase stores text-based files, including semi-structured data such as XML, HTML, plain text documents (e.g., tex files, computer programs) and those files that can be converted into text (e.g., postscript files, PDF files). In XBase, file name is optional. Users can just load a file into XBase without giving a name and the directory where it should be stored. XBase will automatically associate it with attributes such as the time when the file was saved, its source, its size and type, and etc., To retrieve those files, XBase provides three access methods, explorative browsing, querying using query languages, and keyword based search.

#index 397422
#* XmdvTool: visual interactive data exploration and trend discovery of high-dimensional data sets
#@ Elke A. Rundensteiner;Matthew O. Ward;Jing Yang;Punit R. Doshi
#t 2002
#c 5
#% 286639
#% 434566

#index 397423
#* Data streams: fresh current or stagnant backwater? (panel)
#@ Joseph M. Hellerstein;Jennifer Widom
#t 2002
#c 5

#index 397424
#* Going public: open-source databases and database research
#@ Philippe Bonnet
#t 2002
#c 5
#! There are a number of database systems available free of charge for the research community, with complete access to the source code. Some of these systems result from completed research projects, others have been developed outside the research community. How can the database community best take advantage of these publically available systems? The most widely used open-source database is MySQL. Their objective is to become the 'best and most used database in the world'. Can they do it without the database research community?

#index 397425
#* Implementing XQuery
#@ Paul Cotton
#t 2002
#c 5

#index 397426
#* Querying and mining data streams: you only get one look a tutorial
#@ Minos Garofalakis;Johannes Gehrke;Rajeev Rastogi
#t 2002
#c 5

#index 397427
#* Tutorial: application servers and associated technologies
#@ C. Mohan
#t 2002
#c 5
#% 345657
#% 390048
#% 480814
#! Application Servers (ASs), which have become very popular in the last few years, provide the platforms for the execution of transactional, server-side applications in the online world. ASs are the modern cousins of traditional transaction processing monitors (TPMs) like CICS. In this tutorial, I will provide an introduction to different ASs and their technologies. ASs play a central role in enabling electronic commerce in the web context. They are built on the basis of more standardized protocols and APIs than were the traditional TPMs. The emergence of Java, XML and OMG standards has played a significant role in this regard. Consequently, I will also briefly introduce the related XML, Java and OMG technologies like SOAP, J2EE and CORBA. One of the most important features of ASs is their ability to integrate the modern application environments with legacy data sources like IMS, CICS, VSAM, etc. They provide a number of connectors for this purpose, typically using asynchronous transactional messaging technologies like MQSeries and JMS. Traditional TPM-style requirements for industrial strength features like scalability, availability, reliability and high performance are equally important for ASs also. Security and authentication issues are additional important requirements in the web context. ASs support DBMSs not only as storage engines for user data but also as repositories for tracking their own state. Recently, the ECPerf benchmark has been developed via the Java Community Process to evaluate in a standardized way the cost performance of J2EE-compliant ASs. Several caching technologies have been developed to improve performance of ASs.Soon after this conference is over, the slides of this tutorial will be available on the web at the following URL: http://www.almaden.ibm.com/u/mohan/AppServersTutorial_SIGMOD2002_Slides.pdf

#index 397428
#* Database tuning: principles, experiments, and troubleshooting techniques (part I)
#@ Dennis Shasha;Philippe Bonnet
#t 2002
#c 5

#index 397429
#* Database tuning: principles, experiments, and troubleshooting techniques (part II)
#@ Dennis Shasha;Philippe Bonnet
#t 2002
#c 5

#index 397430
#* Software as a service: ASP and ASP aggregation
#@ Christoph Bussler
#t 2002
#c 5
#% 569807
#! The tutorial "Software as a Service: ASP and ASP aggregation" will give an introduction and overview of the concept of "renting" access to software to customers (subscribers). Application service providers (ASPs) are enterprises hosting one or more applications and provide access to subscribers over the Internet by means of browser technology. Furthermore, the underlying technologies are discussed to enable application hosting. The concept of ASP aggregation is introduced to provide a single access point and a single sign-on capability to subscribers sub-scribing to more than one hosted application in more than one ASP.

#index 397592
#* Bringing order to query optimization
#@ Giedrius Slivinskas;Christian S. Jensen;Richard Thomas Snodgrass
#t 2002
#c 5
#% 172950
#% 289370
#% 416029
#% 443391
#% 462058
#% 464058
#% 480764
#% 480946
#% 565457
#! A variety of developments combine to highlight the need for respecting order when manipulating relations. For example, new functionality is being added to SQL to support OLAP-style querying in which order is frequently an important aspect. The set- or multiset-based frameworks for query optimization that are currently being taught to database students are increasingly inadequate.This paper presents a foundation for query optimization that extends existing frameworks to also capture ordering. A list-based relational algebra is provided along with three progressively stronger types of algebraic equivalences, concrete query transformation rules that obey the different equivalences, and a procedure for determining which types of transformation rules are applicable for optimizing a query. The exposition follows the style chosen by many textbooks, making it relatively easy to teach this material in continuation of the material covered in the textbooks, and to integrate this material into the textbooks.

#index 397593
#* The design of a retrieval technique for high-dimensional data on tertiary storage
#@ Ratko Orlandic;Jack Lukaszuk;Craig Swietlik
#t 2002
#c 5
#% 248796
#% 252304
#% 261139
#% 317933
#% 411694
#% 427199
#% 479649
#% 480133
#% 481956
#% 495399
#% 617842
#% 617889
#% 664831
#! In high-energy physics experiments, large particle accelerators produce enormous quantities of data, measured in hundreds of terabytes or petabytes per year, which are deposited onto tertiary storage. The experiments are designed to study the collisions of fundamental particles, called "events", each of which is represented as a point in a multi-dimensional universe. In these environments, the best retrieval performance can be achieved only if the data is clustered on the tertiary storage by all searchable attributes of the events. Since the number of these attributes is high, the underlying data-management facility must be able to cope with extremely large volumes and very high dimensionalities of data at the same time. The proposed indexing technique is designed to facilitate both clustering and efficient retrieval of high-dimensional data on tertiary storage. The structure uses an original space-partitioning scheme, which has numerous advantages over other space-partitioning techniques. While the main objective of the design is to support high-energy physics experiments, the proposed solution is appropriate for many other scientific applications.

#index 397594
#* A pictorial query language for querying geographic databases using positional and OLAP operators
#@ Elaheh Pourabbas;Maurizio Rafanelli
#t 2002
#c 5
#% 308444
#% 442847
#% 464215
#% 617893
#! The authors propose a declarative Pictorial Query Language (called PQL) that is able to express queries on an Object-Oriented geographic database drawing the features which form the query. These features refer to the classic ones of a geographic environment (geo-null, geo-points, geo-polyline, and geo-region) and define the alphabet of the above mentioned language. This language, extended with respect to a previous one, considers twelve positional operators and a set of their specifications. Moreover, the possibility to use the mentioned language to query multidimensional databases is discussed. Finally, the characteristic of the mentioned language by a query example is shown.

#index 397595
#* Investigating XQuery for querying across database object types
#@ Nancy Wiegand
#t 2002
#c 5
#% 83580
#% 116091
#% 168696
#% 210214
#% 235914
#% 539294
#! In addition to facilitating querying over the Web, XML query languages may provide high level constructs for useful facilities in traditional DBMSs that do not currently exist. In particular, current DBMS query languages do not allow querying across database object types to yield heterogeneous results. This paper motivates the usefulness of heterogeneous querying in traditional DBMSs and investigates XQuery, an emerging standard for XML query languages, to express such queries. The usefulness of querying and storing heterogeneous types is also applied to XML data within a Web information system.

#index 397596
#* MPEG-7 and multimedia database systems
#@ Harald Kosch
#t 2002
#c 5
#% 297155
#% 308463
#% 334561
#% 390631
#% 422950
#% 428409
#% 434690
#% 480808
#% 636411
#% 1180249
#% 1857839
#! The Multimedia Description Standard MPEG-7 is an International Standard since February 2002. It defines a huge set of description classes for multimedia content, for its creation and its communication. This article investigates what MPEG-7 means to Multimedia Database Systems (MMDBSs) and vice versa. We argue that MPEG-7 has to be considered complementary to, rather than competing with, data models employed in MMDBSs. Finally we show by an example scenario how these technologies can reasonably complement one another.

#index 397597
#* Cluster validity methods: part I
#@ Maria Halkidi;Yannis Batistakis;Michalis Vazirgiannis
#t 2002
#c 5
#% 58646
#% 104472
#% 207945
#% 232102
#% 260970
#% 296738
#% 316709
#% 393792
#% 631985
#! Clustering is an unsupervised process since there are no predefined classes and no examples that would indicate grouping properties in the data set. The majority of the clustering algorithms behave differently depending on the features of the data set and the initial assumptions for defining groups. Therefore, in most applications the resulting clustering scheme requires some sort of evaluation as regards its validity. Evaluating and assessing the results of a clustering algorithm is the main subject of cluster validity. In this paper we present a review of the clustering validity and methods. More specifically, Part I of the paper discusses the cluster validity approaches based on external and internal criteria.

#index 397598
#* From ternary relationship to relational tables: a case against common beliefs
#@ Rafael Camps Paré
#t 2002
#c 5
#% 106916
#% 108252
#% 207730
#% 234797
#% 256153
#% 260116
#% 264771
#% 287631
#% 307951
#% 387089
#% 388000
#! The transformation from n-ary relationships to a relational database schema has never been really fully analyzed. This paper presents one of the several ternary cases ignored by the ER-to-RM literature. The case shows that the following common belief is wrong: Given a set of FDs over a table resulting in a non-3NF situation, it is always possible to obtain a fully equivalent set of 3NF tables, without adding other restrictions than candidate keys and elementary inclusion dependencies.

#index 397599
#* David DeWitt speaks out: on rethinking the CS curriculum, why the database community should be proud, why query optimization doesn't work, how supercomputing funding is sometimes very poorly spent, how he's not a good coder and isn't smart enough to do DB theory, and more
#@ Marianne Winslett
#t 2002
#c 5

#index 397600
#* Data mining: concepts and techniques by Jiawei Han and Micheline Kamber
#@ Fernando Berzal;Nicolfás Matín
#t 2002
#c 5

#index 397601
#* Mining the world wide web: an information search approach by George Chang, Marcus J. Healey (editor), James A. M. McHugh, Jason T. L. Wang
#@ Aris Ouksel
#t 2002
#c 5

#index 397602
#* Data warehousing and business intelligence for e-commerce: by Alan R. Simon & Steven L. Shaffer
#@ Frank G. Goethals
#t 2002
#c 5

#index 397603
#* On building an infrastructure for mobile and wireless systems: report on the NSF workshop on an infrastructure for mobile and wireless systems, Oct. 15, 2001
#@ Birgitta König-Ries;Kia Makki;Sam Makki;Charles Perkins;Niki Pissinou;Peter Reiher;Peter Scheuermann;Jari Veijalainen;Ouri Wolfson
#t 2002
#c 5

#index 397604
#* Report on the ACM fourth international workshop on data warehousing and OLAP (DOLAP 2001)
#@ Joachim Hammer
#t 2002
#c 5
#! The Fourth Annual ACM International Workshop on Data Warehousing and Online Analytical Processing (DOLAP 2001) was held in Atlanta, GA, USA, in November 2001, in conjunction with the Tenth International Conference on Information and Knowledge Management (CIKM 2001). Although this was only the fourth annual meeting, DOLAP has already become an important and broadly accepted forum for researchers and practitioners to share their findings in theoretical foundations, current methodologies, practical experiences, and new research directions in the areas of data warehousing and online analytical processing (OLAP). Despite the fact that conference attendance has been down since the horrific events of September 11, DOLAP 2001 attracted researchers from Europe, Asia and the Americas.The DOLAP 2001 program, which occupied a full day immediately following CIKM 2001, included a keynote talk, technical presentations, and a final discussion involving all workshop participants. As in previous years, the quality of the submitted papers was high and the program committee had a difficult time deciding which of the 31 submissions from 16 different countries should be accepted for presentation. The submitted research papers covered the state-of-the-art in data warehousing and related fields including data warehouse architecture, design and evolution, multi-dimensional modeling, query optimization, indexing, view materialization and maintenance, data warehouse quality, XML- and object-based warehouses, and data warehousing and the Web. In addition, the committee received a number of industrial submissions describing ongoing data warehousing projects and novel applications for warehouses. After careful review, 12 research and industrial papers were selected for presentation at DOLAP 2001. The proceedings are published by ACM Press, and are also available online at www.informatik.uni-trier.de/~ley/db/conf/dolap/dolap2001.html.In addition, the authors of the three highest-ranked papers have been invited to submit an updated, more detailed version to the Special Issue on Advances in Data Warehousing of the Data and Knowledge Engineering Journal to be published in early 2003.

#index 397605
#* A brief survey of web data extraction tools
#@ Alberto H. F. Laender;Berthier A. Ribeiro-Neto;Altigran S. da Silva;Juliana S. Teixeira
#t 2002
#c 5
#% 227987
#% 248808
#% 248852
#% 261741
#% 273925
#% 275915
#% 275917
#% 275922
#% 278109
#% 283136
#% 287202
#% 300288
#% 311037
#% 312860
#% 331772
#% 342703
#% 424931
#% 431536
#% 463919
#% 464720
#% 464825
#% 480648
#% 480824
#% 511902
#% 617167
#% 632051
#% 1782837
#! In the last few years, several works in the literature have addressed the problem of data extraction from Web pages. The importance of this problem derives from the fact that, once extracted, the data can be handled in a way similar to instances of a traditional database. The approaches proposed in the literature to address the problem of Web data extraction use techniques borrowed from areas such as natural language processing, languages and grammars, machine learning, information retrieval, databases, and ontologies. As a consequence, they present very distinct features and capabilities which make a direct comparison difficult to be done. In this paper, we propose a taxonomy for characterizing Web data extraction fools, briefly survey major Web data extraction tools described in the literature, and provide a qualitative analysis of them. Hopefully, this work will stimulate other studies aimed at a more comprehensive analysis of data extraction approaches and tools for Web data.

#index 397606
#* Phoenix project: fault-tolerant applications
#@ Roger Barga;David Lomet
#t 2002
#c 5
#% 86930
#% 114582
#% 248823
#% 273892
#% 274148
#% 403195
#% 464823
#% 465023
#% 660002
#! After a system crash, databases recover to the last committed transaction, but applications usually either crash or cannot continue. The Phoenix purpose is to enable application state to persist across system crashes, transparent to the application program. This simplifies application programming, reduces operational costs, masks failures from users, and increases application availability, which is critical in many scenarios, e.g., e-commerce. Within the Phoenix project, we have explored how to provide application recovery efficiently and transparently via redo logging. This paper describes the conceptual framework for the Phoenix project, and the software infrastructure that we are building.

#index 397607
#* SQL/XML is making good progress
#@ Andrew Eisenberg;Jim Melton
#t 2002
#c 5
#! Not very long ago, we discussed the creation of a new part of SQL, XML-Related Specifications (SQL/XML), in this column [1]. At the time, we referred to the work that had been done as "infrastructure". We are pleased to be able to say that significant progress has been made, and SQL/XML [2] is now going out for the first formal stage of processing, Final Committee Draft (FCD) ballot, in ISO/IEC JTC1.In our previous column, we described the mapping of SQL 〈identifier〉s to XML Names, SQL data types to XML Schema data types, and SQL values to XML values. There have been a few small corrections and enhancements in these areas, but for the most part the descriptions in our previous column are still accurate.Thc new work that we will discuss in this column comes in three parts. The first part provides a mapping from a single table, all tables in a schema, or all tables in a catalog to an XML document. The second of these parts includes the creation of an XML data type in SQL and adds functions that create values of this new type. These functions allow a user to produce XML from existing SQL data. Finally, the "infrastructure" work that we described in our previous article included the mapping of SQL's predefined data types to XML Schema data types. This mapping has been extended to include the mapping of domains, distinct types, row types, arrays, and multisets.The FCD ballot that we mentioned began in early April. This will allow the comments contained in the ballot responses to be discussed at the Editing Meeting in September or October of this year. We expect the Editing Meeting to recommend progression to Final Draft International Status (FDIS) ballot, which suggests that an International Standard will be published by the middle of 2003.

#index 397608
#* Combining fuzzy information: an overview
#@ Ronald Fagin
#t 2002
#c 5
#% 210172
#% 278831
#% 333854
#% 480330
#% 480819
#% 591565
#% 631988
#% 659255
#% 659993
#! Assume that each object in a database has m grades, or scores, one for each of m attributes. For example, an object can have a color grade, that tells how red it is, and a shape grade, that tells how round it is. For each attribute, there is a sorted list, which lists each object and its grade under that attribute, sorted by grade (highest grade first). Each object is assigned an overall grade, that is obtained by combining the attribute grades using a fixed monotone aggregation function, or combining rule, such as min or average. In this overview, we discuss and compare algorithms for determining the top k objects, that is, k objects with the highest overall grades.

#index 427021
#* Treasurer's message
#@ Joachim Hammer
#t 2002
#c 5

#index 427022
#* The cougar approach to in-network query processing in sensor networks
#@ Yong Yao;Johannes Gehrke
#t 2002
#c 5
#% 121
#% 1758
#% 83132
#% 83933
#% 85088
#% 152913
#% 171449
#% 172902
#% 214683
#% 227883
#% 234756
#% 245788
#% 248783
#% 248793
#% 248795
#% 248824
#% 259642
#% 259665
#% 273910
#% 281537
#% 281541
#% 281556
#% 281557
#% 297915
#% 300167
#% 318109
#% 330305
#% 340305
#% 464058
#% 479463
#% 479617
#% 480120
#% 480272
#% 482088
#% 566857
#% 641426
#% 659937
#% 660004
#! The widespread distribution and availability of small-scale sensors, actuators, and embedded processors is transforming the physical world into a computing platform. One such example is a sensor network consisting of a large number of sensor nodes that combine physical sensing capabilities such as temperature, light, or seismic sensors with networking and computation capabilities. Applications range from environmental control, warehouse inventory, and health care to military environments. Existing sensor networks assume that the sensors are preprogrammed and send data to a central frontend where the data is aggregated and stored for offline querying and analysis. This approach has two major drawbacks. First, the user cannot change the behavior of the system on the fly. Second, conservation of battery power is a major design factor, but a central system cannot make use of in-network programming, which trades costly communication for cheap local computation.In this paper, we introduce the Cougar approach to tasking sensor networks through declarative queries. Given a user query, a query optimizer generates an efficient query plan for in-network query processing, which can vastly reduce resource usage and thus extend the lifetime of a sensor network. In addition, since queries are asked in a declarative language, the user is shielded from the physical characteristics of the network. We give a short overview of sensor networks, propose a natural architecture for a data management system for sensor networks, and describe open research problems in this area.

#index 427023
#* Clustering validity checking methods: part II
#@ Maria Halkidi;Yannis Batistakis;Michalis Vazirgiannis
#t 2002
#c 5
#% 58646
#% 104472
#% 140654
#% 203462
#% 207945
#% 248790
#% 260970
#% 393792
#% 466481
#% 922066
#! Clustering results validation is an important topic in the context of pattern recognition. We review approaches and systems in this context. In the first part of this paper we presented clustering validity checking approaches based on internal and external criteria. In the second, current part, we present a review of clustering validity approaches based on relative criteria. Also we discuss the results of an experimental study based on widely known validity indices. Finally the paper illustrates the issues that are under-addressed by the recent approaches and proposes the research directions in the field.

#index 427024
#* A study on the management of semantic transaction for efficient data retrieval
#@ Shi Ming Huang;Irene Kwan;Chih He Li
#t 2002
#c 5
#% 140613
#% 340666
#% 437126
#% 463101
#% 511491
#% 617260
#% 657444
#% 660942
#! Mobile computing technology is developing rapidly due to the advantages of information access through mobile devices and the need to retrieve information at remote locations. However, many obstacles within the discipline of wireless computing are yet to be resolved. One of the most significant of these issues is the speed of data retrieval, which directly affects the performance of mobile database applications. To remedy this problem, we propose here a revised methodology focusing on the management of mobile transactions. This paper investigates an extended semantic-based transaction management mechanism, and applies a model-based approach for developing a simulation model to evaluate the performance of our approach.

#index 427025
#* Introduction to constraint databases
#@ Bart Kuijpers
#t 2002
#c 5

#index 427026
#* Foundations of statistical natural language processing
#@ Gerhard Weikum
#t 2002
#c 5

#index 427027
#* Automata theory for XML researchers
#@ Frank Neven
#t 2002
#c 5
#% 46269
#% 64414
#% 175464
#% 275308
#% 279367
#% 299942
#% 299944
#% 299976
#% 320775
#% 333841
#% 333856
#% 344425
#% 345757
#% 378389
#% 378392
#% 378393
#% 378394
#% 401124
#% 427874
#% 483878
#% 516520
#% 562461
#% 1395000

#index 427028
#* Hector Garcia-Molina speaks out: regarding startups, how life is getting harder, delta papers, CS in Mexico, life as a department chair, and more
#@ Marianne Winslett
#t 2002
#c 5

#index 427029
#* Toward autonomic computing with DB2 universal database
#@ Sam S. Lightstone;Guy Lohman;Danny Zilio
#t 2002
#c 5
#% 36119
#% 116043
#% 170893
#% 218149
#% 248815
#% 248855
#% 300138
#% 342729
#% 397397
#% 458523
#% 462058
#% 462204
#% 465007
#% 480158
#% 480803
#% 481459
#% 632100
#! As the cost of both hardware and software falls due to technological advancements and economies of scale, the cost of ownership for database applications is increasingly dominated by the cost of people to manage them. Databases are growing rapidly in scale and complexity, while skilled database administrators (DBAs) are becoming rarer and more expensive. This paper describes the self-managing or autonomic technology in IBM's DB2 Universal Database® for UNIX and Windows to illustrate how self-managing technology can reduce complexity, helping to reduce the total cost of ownership (TCO) of DBMSs and improve system performance.

#index 427030
#* SQL/MED: a status report
#@ Jim Melton;Jan Eike Michels;Vanja Josifovski;Krishna Kulkarni;Peter Schwarz
#t 2002
#c 5
#% 332162

#index 427031
#* Interviewing during a tight job market
#@ Zachary G. Ives;Qiong Luo
#t 2002
#c 5
#! The following collection of articles aims to provide a sequel to Ugur Cetintemel's December 2001 interviewing advice article, with specific tips for times when the economy is in recession. The contributions are based on the personal experience of recent database graduates who were on the job market in the 2001-2002 season.

#index 427032
#* Database research at the University of Illinois at Urbana-Champaign
#@ M. Winslett;K. Chang;A. Doan;J. Han;C. Zhai;Y. Zhou
#t 2002
#c 5
#% 273914
#% 274580
#% 309198
#% 333990
#% 340899
#% 342346
#% 348187
#% 379241
#% 379290
#% 397129
#% 397378
#% 397383
#% 480476
#% 523890
#% 577234
#% 577235
#% 993958
#% 993996

#index 427033
#* Research activities in database management and information retrieval at University of Illinois at Chicago
#@ Isabel Cruz;Ashfaq Khokhar;Bing Liu;Prasad Sistla;Ouri Wolfson;Clement Yu
#t 2002
#c 5
#% 116048
#% 249933
#% 309103
#% 316253
#% 348190
#% 443450
#% 461923
#% 464641
#% 481600
#% 500896
#% 538889
#% 577232
#% 620060
#% 637796

#index 428144
#* Advanced XML data processing: guest editor's introduction
#@ Karl Aberer
#t 2001
#c 5
#% 244102
#% 332135
#% 454532
#% 479327
#% 566023

#index 428145
#* XQuery formal semantics state and challenges
#@ Peter Fankhauser
#t 2001
#c 5
#% 313640
#% 320775
#% 464720
#% 504581
#! The XQuery formalization is an ongoing effort of the W3C XML Query working group to define a precise formal semantics for XQuery. This paper briefly introduces the current state of the formalization and discusses some of the more demanding remaining challenges in formally describing an expressive query language for XML.

#index 428146
#* A general technique for querying XML documents using a relational database system
#@ Jayavel Shanmugasundaram;Eugene Shekita;Jerry Kiernan;Rajasekar Krishnamurthy;Efstratios Viglas;Jeffrey Naughton;Igor Tatarinov
#t 2001
#c 5
#% 198465
#% 273922
#% 300143
#% 309851
#% 479956
#% 480152
#% 480657
#% 480822
#% 504574
#! There has been recent interest in using relational database systems to store and query XML documents. Each of the techniques proposed in this context works by (a) creating tables for the purpose of storing XML documents (also called relational schema generation), (b) storing XML documents by shredding them into rows in the created tables, and (c) converting queries over XML documents into SQL queries over the created tables. Since relational schema generation is a physical database design issue -- dependent on factors such as the nature of the data, the query workload and availability of schemas -- there have been many techniques proposed for this purpose. Currently, each relational schema generation technique requires its own query processor to efficiently convert queries over XML documents into SQL queries over the created tables. In this paper, we present an efficient technique whereby the same query-processor can be used for all such relational schema generation techniques. This greatly simplifies the task of relational schema generation by eliminating the need to write a special-purpose query processor for each new solution to the problem. In addition, our proposed technique enables users to query seamlessly across relational data and XML documents. This provides users with unified access to both relational and XML data without them having to deal with separate databases.

#index 428147
#* Why and how to benchmark XML databases
#@ Albrecht Schmidt;Florian Waas;Martin Kersten;Daniela Florescu;Michael J. Carey;Ioana Manolescu;Ralph Busse
#t 2001
#c 5
#% 236416
#% 273922
#% 322415
#% 328431
#% 333965
#% 334006
#% 479956
#% 504574
#% 541480
#% 632058
#% 650962
#! Benchmarks belong to the very standard repertory of tools deployed in database development. Assessing the capabilities of a system, analyzing actual and potential bottlenecks, and, naturally, comparing the pros and cons of different systems architectures have become indispensable tasks as databases management systems grow in complexity and capacity. In the course of the development of XML databases the need for a benchmark framework has become more and more evident: a great many different ways to store XML data have been suggested in the past, each with its genuine advantages, disadvantages and consequences that propagate through the layers of a complex database system and need to be carefully considered. The different storage schemes render the query characteristics of the data variably different. However, no conclusive methodology for assessing these differences is available to date.In this paper, we outline desiderata for a benchmark for XML databases drawing from our own experience of developing an XML repository, involvement in the definition of the standard query language, and experience with standard benchmarks for relational databases.

#index 428148
#* Wrapping web data into XML
#@ Wei Han;David Buttler;Calton Pu
#t 2001
#c 5
#% 227987
#% 248853
#% 266102
#% 443298
#% 632051
#% 660272
#! The vast majority of online information is part of the World Wide Web. In order to use this information for more than human browsing, web pages in HTML must be converted into a format meaningful to software programs. Wrappers have been a useful technique to convert HTML documents into semantically meaningful XML files. However, developing wrappers is slow and labor-intensive. Further, frequent changes on the HTML documents typically require frequent changes in the wrappers. This paper describes XWRAP Elite, a tool to automatically generate robust wrappers. XWRAP breaks down the conversion process into three steps. First, discover where the data is located in an HTML page and separating the data into individual objects. Second, decompose objects into data elements. Third, mark objects and elements in an output format. XWRAP Elite automates the first two steps and minimizes human involvement in marking output data. Our experience shows that XWRAP is able to create useful wrapper software for a wide variety of real world HTML documents.

#index 428149
#* On database theory and XML
#@ Dan Suciu
#t 2001
#c 5
#% 663
#% 1834
#% 5379
#% 6787
#% 11797
#% 13014
#% 23873
#% 23898
#% 23899
#% 28120
#% 32889
#% 36181
#% 43162
#% 58354
#% 58356
#% 116043
#% 122396
#% 123118
#% 136740
#% 164377
#% 164406
#% 189739
#% 191573
#% 191614
#% 213950
#% 213966
#% 237181
#% 248025
#% 248033
#% 248034
#% 261370
#% 268745
#% 268788
#% 271908
#% 273922
#% 289266
#% 299942
#% 299969
#% 309851
#% 320775
#% 330627
#% 333857
#% 333935
#% 411554
#% 415958
#% 416030
#% 416034
#% 464727
#% 464988
#% 479956
#% 480152
#% 480656
#% 481128
#% 504575
#% 565457
#% 598376
#% 599549
#% 653704
#! Over the years, the connection between database theory and database practice has weakened. We argue here that the new challenges posed by XML and its applications are strengthening this connection today. We illustrate three examples of theoretical problems arising from XML applications, based on our own research.

#index 428150
#* XML document versioning
#@ Shu Yao Chien;Vassilis J. Tsotras;Carlo Zaniolo
#t 2001
#c 5
#% 2011
#% 58371
#% 182672
#% 210212
#% 442967
#% 479336
#% 480489
#% 480659
#% 480827
#% 504576
#% 526866
#! Managing multiple versions of XML documents represents an important problem, because of many applications ranging from traditional ones, such as software configuration control, to new ones, such as link permanence of web documents. Research on managing multiversion XML documents seeks to provide efficient and robust techniques for (i) storing and retrieving, (ii) exchanging, and (iii) querying such documents. In this paper, we first show that traditional version control methods, such as RCS, and SCCS, fall short from satisfying these three requirements, and discuss alternative solutions. First, we enhance RCS with a temporal page clustering policy to achieve objective (i). Then, we discuss a reference-based versioning scheme that achieves both objectives (i) and (ii) and is also effective at supporting simple queries. The topic of supporting complex queries, including temporal ones, meshes with the burgeoning interest of database researchers in XML as a database description language, and in XML query languages. In this context, the XML versioning problems are akin to those of transaction time management for databases of objects and semistructured information. Nevertheless, the need to preserve the natural ordering of XML documents frequently requires different techniques.

#index 428151
#* Preservation of digital data with self-validating, self-instantiating knowledge-based archives
#@ Bertram Ludäscher;Richard Marciano;Reagan Moore
#t 2001
#c 5
#% 189739
#% 281404
#% 612107
#! Digital archives are dedicated to the long-term preservation of electronic information and have the mandate to enable sustained access despite rapid technology changes. Persistent archives are confronted with heterogeneous data formats, helper applications, and platforms being used over the lifetime of the archive. This is not unlike the interoperability challenges, for which mediators are devised. To prevent technological obsolescence over time and across platforms, a migration approach for persistent archives is proposed based on an XML infrastructure.We extend current archival approaches that build upon standardized data formats and simple metadata mechanisms for collection management, by involving high-level conceptual models and knowledge representations as an integral part of the archive and the ingestion/migration processes. Infrastructure independence is maximized by archiving generic, executable specifications of (i) archival constraints (i.e., "model validators"), and (ii) archival transformations that are part of the ingestion process. The proposed architecture facilitates construction of self-validating and self-instantiating knowledge-based archives. We illustrate our overall approach and report on first experiences using a sample collection from a collaboration with the National Archives and Records Administration (NARA).

#index 428152
#* The evolution of effective B-tree: page organization and techniques: a personal account
#@ David Lomet
#t 2001
#c 5
#% 172911
#% 244119
#% 252608
#% 261735
#% 287715
#% 300194
#% 320483
#% 333949
#% 411555
#% 461896
#% 464012
#% 464987
#% 480821
#! An under-appreciated facet of index search structures is the importance of high performance search within B-tree internal nodes. Much attention has been focused on improving node fanout, and hence minimizing the tree height [BU77, LL86]. [GG97, Lo98] have discussed the importance of B-tree page size. A recent article [GL2001] discusses internal node architecture, but the subject is buried in a single section of the paper.In this short note, I want to describe the long evolution of good internal node architecture and techniques, including an understanding of what problem was being solved during each of the incremental steps that have led to much improved node organizations.

#index 428153
#* The Ecobase project: database and web technologies for environmental information systems
#@ Luc Bouganim;Maria Claudia Cavalcanti;Françoise Fabret;Maria Luiza Campos;François Llirbat;Marta Mattoso;Rubens Melo;Ana Maria Moura;Esther Pacitti;Fabio Porto;Margareth Simoes;Eric Simon;Asterio Tanaka;Patrick Valduriez
#t 2001
#c 5
#% 223771
#% 443235
#% 465016
#% 479978
#% 588786
#% 588818
#% 635831
#! A very large number of data sources on environment, energy, and natural resources are available worldwide. Unfortunately, users usually face several problems when they want to search and use relevant information. In the Ecobase project, we address these problems in the context of several environmental applications in Brazil and Europe. We propose a distributed architecture for environmental information systems (EIS) based on the Le Select middleware developed at INRIA. In this paper, we present this architecture and its capabilities, and discuss the lessons learned and open issues.

#index 428154
#* Data analysis and mining in the life sciences
#@ Nam Huyn
#t 2001
#c 5
#% 2163
#% 152934
#% 223781
#% 243727
#% 308509
#% 385563
#% 480295
#! Biotech companies routinely generate vast amounts of biological measurement data that must be analyzed rapidly and mined for diagnostic, prognostic, or drug evaluation purposes. While these data analysis tasks are critical to their success, they have not benefited from recent advances that emerged from database and KDD research. In this paper, we focus on two such tasks: on-line analysis of clinical study data, and mining broad datasets for biomarkers. We examine the new requirements that are not met by current data analysis technologies and we identify new database and KDD research to address these needs. We describe our experience implementing a Scientific OLAP system and a data mining platform for the support of biomarker discovery at SurroMed, and we outline some key technical challenges that must be overcome before data analysis and data mining technologies can be widely adopted in the biotech industry.

#index 428155
#* Continuous queries over data streams
#@ Shivnath Babu;Jennifer Widom
#t 2001
#c 5
#% 1331
#% 36117
#% 59350
#% 86929
#% 116082
#% 136740
#% 172949
#% 172950
#% 187411
#% 198467
#% 214073
#% 223887
#% 227883
#% 234797
#% 273898
#% 273902
#% 273907
#% 273908
#% 273909
#% 273910
#% 273911
#% 273945
#% 293714
#% 300167
#% 300179
#% 300195
#% 310488
#% 310500
#% 310900
#% 333926
#% 333931
#% 333954
#% 333982
#% 340301
#% 340635
#% 342600
#% 394417
#% 411750
#% 443298
#% 456040
#% 458556
#% 479621
#% 479984
#% 480296
#% 480306
#% 480465
#% 480628
#% 480642
#% 480768
#% 481943
#% 482123
#% 504019
#% 594012
#% 632090
#! In many recent applications, data may take the form of continuous data streams, rather than finite stored data sets. Several aspects of data management need to be reconsidered in the presence of data streams, offering a new research direction for the database community. In this paper we focus primarily on the problem of query processing, specifically on how to define and evaluate continuous queries over data streams. We address semantic issues as well as efficiency concerns. Our main contributions are threefold. First, we specify a general and flexible architecture for query processing in the presence of data streams. Second, we use our basic architecture as a tool to clarify alternative semantics and processing techniques for continuous queries. The architecture also captures most previous work on continuous queries and data streams, as well as related concepts such as triggers and materialized views. Finally, we map out research topics in the area of query processing over data streams, showing where previous work is relevant and describing problems yet to be addressed.

#index 428156
#* Career-enhancing services at SIGMOD online
#@ Alexandros Labrinidis;Alberto O. Mendelzon
#t 2001
#c 5
#% 308439
#! This article serves three purposes. First of all, to introduce dbjobs, the database of database jobs, and also describe its functionality and architecture. Secondly, to present statistics for the dbgrads system, after 18 months of continuous operation. Finally, to describe exciting future projects for SIGMOD Online.

#index 428398
#* Reminiscences on Influential Papers
#@ Kenneth A. Ross
#t 2001
#c 5

#index 428399
#* Special issue on data mining for intrusion detection and threat analysis
#@ Daniel Barbará
#t 2001
#c 5

#index 428400
#* Data mining-based intrusion detectors: an overview of the columbia IDS project
#@ Salvatore J. Stolfo;Wenke Lee;Philip K. Chan;Wei Fan;Eleazar Eskin
#t 2001
#c 5
#% 280429
#% 332082
#% 340031
#% 423630
#% 445344
#% 466745
#% 523873
#% 536882
#% 664713
#% 664717
#% 703747
#% 709657
#% 978636

#index 428401
#* ADAM: a testbed for exploring the use of data mining in intrusion detection
#@ Daniel Barbará;Julia Couto;Sushil Jajodia;Ningning Wu
#t 2001
#c 5
#% 18528
#% 152934
#% 232102
#% 290482
#% 365897
#% 488479
#% 583712
#% 978636
#! Intrusion detection systems have traditionally been based on the characterization of an attack and the tracking of the activity on the system to see if it matches that characterization. Recently, new intrusion detection systems based on data mining are making their appearance in the field. This paper describes the design and experiences with the ADAM (Audit Data Analysis and Mining) system, which we use as a testbed to study how useful data mining techniques can be in intrusion detection.

#index 428402
#* Detection and classification of intrusions and faults using sequences of system calls
#@ João B. D. Cabrera;Lundy Lewis;Raman K. Mehra
#t 2001
#c 5
#% 80995
#% 275738
#% 275741
#% 325926
#% 332082
#% 488479
#% 536877
#% 536883
#% 611162
#% 664547
#% 664713
#% 1001648
#% 1001828
#! This paper investigates the use of sequences of system calls for classifying intrusions and faults induced by privileged processes in Unix. Classification is an essential capability for responding to an anomaly (attack or fault), since it gives the ability to associate appropriate responses to each anomaly type. Previous work using the well known dataset from the University of New Mexico (UNM) has demonstrated the usefulness of monitoring sequences of system calls for detecting anomalies induced by processes corresponding to several Unix Programs, such as sendmail, lpr, ftp, etc. Specifically, previous work has shown that the Anomaly Count of a running process, i.e., the number of sequences spawned by the process which are not found in the corresponding dictionary of normal activity for the Program, is a valuable feature for anomaly detection. To achieve Classification, in this paper we introduce the concept of Anomaly Dictionaries, which are the sets of anomalous sequences for each type of anomaly. It is verified that Anomaly Dictionaries for the UNM's sendmail Program have very little overlap, and can be effectively used for Anomaly Classification. The sequences in the Anomalous Dictionary enable a description of Self for the Anomalies, analogous to the definition of Self for Privileged Programs given by the Normal Dictionaries. The dependence of Classification Accuracy with sequence length is also discussed. As a side result, it is also shown that a hybrid scheme, combining the proposed classification strategy with the original Anomaly Counts can lead to a substantial improvement in the overall detection rates for the sendmail dataset. The methodology proposed is rather general, and can be applied to any situation where sequences of symbols provide an effective characterization of a phenomenon.

#index 428403
#* Mining system audit data: opportunities and challenges
#@ Wenke Lee;Wei Fan
#t 2001
#c 5
#% 152934
#% 183319
#% 280429
#% 289519
#% 340031
#% 355792
#% 428400
#% 709657
#! Intrusion detection is an essential component of computer security mechanisms. It requires accurate and efficient analysis of a large amount of system and network audit data. It can thus be an application area of data mining. There are several characteristics of audit data: abundant raw data, rich system and network semantics, and ever "streaming". Accordingly, when developing data mining approaches, we need to focus on: feature extraction and construction, customization of (general) algorithms according to semantic information, and optimization of execution efficiency of the output models. In this paper, we describe a data mining framework for mining audit data for intrusion detection models. We discuss its advantages and limitations, and outline the open research problems.

#index 428404
#* Using unknowns to prevent discovery of association rules
#@ Yücel Saygin;Vassilios S. Verykios;Chris Clifton
#t 2001
#c 5
#% 279334
#% 300184
#% 329858
#% 333876
#% 539744
#% 586838
#% 740764
#! Data mining technology has given us new capabilities to identify correlations in large data sets. This introduces risks when the data is to be made public, but the correlations are private. We introduce a method for selectively removing individual values from a database to prevent the discovery of a set of rules, while preserving the data for other applications. The efficacy and complexity of this method are discussed. We also present an experiment showing an example of this methodology.

#index 428405
#* Mining e-mail content for author identification forensics
#@ O. de Vel;A. Anderson;M. Corney;G. Mohay
#t 2001
#c 5
#% 66075
#% 161111
#% 190581
#% 232653
#% 280817
#% 290482
#% 318412
#% 376266
#% 406493
#% 458379
#% 578558
#% 1860547
#! We describe an investigation into e-mail content mining for author identification, or authorship attribution, for the purpose of forensic investigation. We focus our discussion on the ability to discriminate between authors for the case of both aggregated e-mail topics as well as across different e-mail topics. An extended set of e-mail document features including structural characteristics and linguistic patterns were derived and, together with a Support Vector Machine learning algorithm, were used for mining the e-mail content. Experiments using a number of e-mail documents generated by different authors on a set of topics gave promising results for both aggregated and multi-topic author categorisation.

#index 428406
#* Transactional information systems: theory, algorithms, and the practice of concurrency control and recovery
#@ Marc H. Scholl
#t 2001
#c 5

#index 428407
#* Information warfare and security
#@ H. V. Jagadish
#t 2001
#c 5

#index 428408
#* Describing semistructured data
#@ Luca Cardelli
#t 2001
#c 5
#% 210214
#% 227995
#% 248819
#% 281149
#% 292125
#% 299976
#% 304816
#% 384978
#% 463919
#% 504575
#% 504578
#! We introduce a rich language of descriptions for semistructured tree-like data, and we explain how such descriptions relate to the data they describe. Various query languages and data schemas can be based on such descriptions.

#index 428409
#* SQL multimedia and application packages (SQL/MM)
#@ Jim Melton;Andrew Eisenberg
#t 2001
#c 5
#% 332162
#! Regular readers of this column will have become familiar with database language SQL -- indeed, most readers are already familiar with it. We have also discussed the fact that the SQL standard is being published in multiple parts and have even discussed one of those parts in some detail[l].Another standard, based on SQL and its structured user-defined types[2], has been developed and published by the International Organization for Standardization (ISO). This standard, like SQL, is divided into multiple parts (more independent than the parts of SQL, in fact). Some parts of this other standard, known as SQL/MM, have already been published and are currently in revision, while others are still in preparation for initial publication.In this issue, we introduce SQL/MM and review each of its parts, necessarily at a high level.

#index 428410
#* Java support for data-intensive systems: experiences building the telegraph dataflow system
#@ Mehul A. Shah;Michael J. Franklin;Samuel Madden;Joseph M. Hellerstein
#t 2001
#c 5
#% 114582
#% 116087
#% 202153
#% 239951
#% 248817
#% 273796
#% 300167
#% 319473
#% 319581
#% 342377
#% 403195
#% 979070
#! Database system designers have traditionally had trouble with the default services and interfaces provided by operating systems. In recent years, developers and enthusiasts have increasingly promoted Java as a serious platform for building data-intensive servers. Java provides a number of very helpful language features, as well as a full run-time environment reminiscent of a traditional operating system. This combination of features and community support raises the question of whether Java is better or worse at supporting data-intensive server software than a traditional operating system coupled with a weakly-typed language such as C or C++.In this paper, we summarize and discuss our experience building the Telegraph dataflow system in Java. We highlight some of the pleasures of coding with Java, and some of the pains of coding around Java in order to obtain good performance in a data-intensive server. For those issues that were painful, we present concrete suggestions for evolving Java's interfaces to better suit serious software systems development. We believe these experiences can provide insight for other designers to avoid pitfalls we encountered and to decide if Java is a suitable platform for their system.

#index 428411
#* On the academic interview circuit: an end-to-end discussion
#@ Uğur Çetintemel
#t 2001
#c 5
#! This article summarizes my recent job search that effectively began in the late fall of 2000 and ended in the early summer of 2001. The opinions I express here are largely based on what I experienced, heard and read from various sources, and should be taken as mere tips or suggestions for Ph.D. students who are soon to graduate and look for a position in a research-oriented academic institution.This is by no means a comprehensive guide to job searching: in limited space, I address only the issues that I deem more relevant or important, in an effort to provide information and insight that I believe is not readily available elsewhere. I do, however, try to provide pointers to (hopefully) complementary information throughout the text wherever appropriate and in Section 13.Figure 1 illustrates the typical timeline for the entire process, from pre-application to final decision, and the documents and activities required at each stage. The rest of the article briefly discusses each of these stages.

#index 428412
#* Research in multi-organizational processes and semantic information brokering at the LSDIS lab
#@ Amit Sheth;John Miller;Krys Kochut;Budak Arpinar
#t 2001
#c 5
#% 261273
#% 433803
#% 434337
#% 535977
#% 571297
#% 1561718

#index 449212
#* Data modelling versus ontology engineering
#@ Peter Spyns;Robert Meersman;Mustafa Jarrar
#t 2002
#c 5
#% 150854
#% 198016
#% 331535
#% 342984
#% 497618
#% 509707
#% 509713
#% 535820
#! Ontologies in current computer science parlance are computer based resources that represent agreed domain semantics. Unlike data models, the fundamental asset of ontologies is their relative independence of particular applications, i.e. an ontology consists of relatively generic knowledge that can be reused by different kinds of applications/tasks. The first part of this paper concerns some aspects that help to understand the differences and similarities between ontologies and data models. In the second part we present an ontology engineering framework that supports and favours the genericity of an ontology. We introduce the DOGMA ontology engineering approach that separates "atomic" conceptual relations from "predicative" domain rules. A DOGMA ontology consists of an ontology base that holds sets of intuitive context-specific conceptual relations and a layer of "relatively generic" ontological commitments that hold the domain rules. This constitutes what we shall call the double articulation of a DOGMA ontology 1.

#index 449213
#* Methodology for development and employment of ontology based knowledge management applications
#@ York Sure;Steffen Staab;Rudi Studer
#t 2002
#c 5
#% 198016
#% 310424
#% 342984
#% 426885
#% 445229
#% 445230
#% 445289
#% 445430
#% 455668
#% 459492
#% 509877
#% 519554
#% 523654
#% 644923
#! In this article we illustrate a methodology for introducing and maintaining ontology based knowledge management applications into enterprises with a focus on Knowledge Processes and Knowledge Meta Processes. While the former process circles around the usage of ontologies, the latter process guides their initial set up. We illustrate our methodology by an example from a case study on skills management.

#index 449214
#* A conceptual architecture for semantic web enabled web services
#@ Christoph Bussler;Dieter Fensel;Alexander Maedche
#t 2002
#c 5
#% 116303
#% 459496
#% 509855
#% 519426
#% 743482
#! Semantic Web Enabled Web Services (SWWS) will transform the web from a static collection of information into a distributed device of computation on the basis of Semantic technology making content within the World Wide Web machine-processable and machine-interpretable. Semantic Web Enabled Web Services will allow the automatic discovery, selection and execution of inter-organization business logic making areas like dynamic supply chain composition a reality. In this paper we introduce the vision of Semantic Web Enabled Web Services, describe requirements for building semantics-driven web services and sketch a first draft of conceptual architecture for implementing semantic web enabled web services.

#index 449215
#* Agents, trust, and information access on the semantic web
#@ Tim Finin;Anupam Joshi
#t 2002
#c 5
#% 413603
#% 419699
#% 445510
#% 524957
#% 664539

#index 449216
#* Conceptual model of web service reputation
#@ E. Michael Maximilien;Munindar P. Singh
#t 2002
#c 5
#% 391644
#% 434004
#% 445446
#% 445529
#% 629258
#! Current Web services standards enable publishing service descriptions and finding services on a match based on criteria such as method signatures or service category. However, current approaches provide no basis for selecting a good service or for comparing ratings of services. We describe a conceptual model for reputation using which reputation information can be organized and shared and service selection can be facilitated and automated.

#index 449217
#* The ρ operator: discovering and ranking associations on the semantic web
#@ Kemafor Anyanwu;Amit Sheth
#t 2002
#c 5
#% 45255
#% 172381
#% 197751
#% 289315
#% 341658
#% 348181
#% 434033
#% 481434
#% 993987
#! In this paper, we introduce an approach that supports querying for Semantic Associations on the Semantic Web. Semantic Associations capture complex relationships between entities involving sequences of predicates, and sets of predicate sequences that interact in complex ways. Detecting such associations is at the heart of many research and analytical activities that are crucial to applications in national security and business intelligence. This in combination with the improving ability to identify entities in documents as part of automatic semantic annotation, gives a very powerful capability for semantic analysis of large amounts of heterogeneous content.The approach for supporting Semantic Associations discussed in this paper has four main facets. First, it generalizes these associations into three main classes based on their structural properties, allowing us to reason about them in a domain-independent manner. The second is the provision of an operator ρ for expressing queries about such associations. Third, it uses a graph data model for knowledge representation, allowing the semantic associations search techniques to be built upon the graph algorithms for paths, while integrating knowledge from the schema into the search process. The fourth facet is the use of a notion of context, which allows for restricting the search space and for context-driven ranking of results. Just as a Web search engine looks for relevant documents in the current Web, ρ can be seen as discovering and ranking complex relationships in the Semantic Web.In this paper, we demonstrate the need for supporting such complex semantic relationships. We also give a formal basis to the notion of Semantic Associations and give a brief discussion on our overall approach for discovering and ranking them.

#index 449218
#* A framework for semantic gossiping
#@ Karl Aberer;Philippe Cudré-Mauroux;Manfred Hauswirth
#t 2002
#c 5
#% 237184
#% 278397
#% 309493
#% 309678
#% 430420
#% 433931
#% 516100
#% 572314
#! Today the problem of semantic interoperability in information search on the Internet is solved mostly by means of centralization, both at a system and at a logical level. This approach has been successful to a certain extent. Peer-to-peer systems as a new brand of system architectures indicate that the principle of decentralization might offer new solutions to many problems that scale well to very large numbers of users.In this paper we outline how the peer-to-peer system architectures can be applied to tackle the problem of semantic interoperability in the large, driven in a bottom-up manner by the participating peers. Such a system can readily be used to study semantic interoperability as a global scale phenomenon taking place in a social network of information sharing peers.

#index 449219
#* Emergent semantics and the multimedia semantic web
#@ W. I. Grosky;D. V. Sreenath;F. Fotouhi
#t 2002
#c 5
#% 202011
#% 437404
#% 443413
#% 635923
#% 1775153
#! It is well known that context plays an important role in the meaning of a work of art. This paper addresses the dynamic context of a collection of linked multimedia documents, of which the web is a perfect example. Contextual document semantics emerge through identification of various users' browsing paths though this multimedia collection. In this paper, we present techniques that use multimedia information as part of this determination. Some implications of our approach are that the author of a webpage cannot completely define that document's semantics and that semantics emerge through use.

#index 449220
#* Querying multiple bioinformatics information sources: can semantic web research help?
#@ David Buttler;Matthew Coleman;Terence Critchlow;Renato Fileto;Wei Han;Calton Pu;Daniel Rocco;Li Xiong
#t 2002
#c 5
#% 465159
#% 533933
#% 591540
#% 772133
#! Advances in Semantic Web and Ontologies have pushed the role of semantics to a new frontier: Semantic Composition of Web Services. A good example of such compositions is the querying of multiple bioinformatics data sources. Supporting effective querying over a large collection of bioinformatics data sources presents a number of unique challenges. First, queries over bioinformatics data sources are often complex associative queries over multiple Web documents. Most associations are defined by string matching of textual fragments in two documents. Second, most of the queries required by Genomics researchers involve complex data extraction, and sophisticated workflows that implement the complex associative access. Third but not the least, complex Genomics-specific queries are often reused many times by Genomics researchers, either directly or through some refinements, and are considered as a part of the research results by Genomics researchers. In this short article we present a list of challenging issues in supporting effective querying over bioinformatics data sources and illustrate them through a selection of representative search scenarios provided by biologists. We end the article with a discussion on how the state-of-art research and technological development in Semantic Web, Ontology, Internet Data Management, and Internet Computing Systems can help addressing these issues.

#index 449221
#* The Grid: an application of the semantic web
#@ Carole Goble;David De Roure
#t 2002
#c 5
#% 397349
#% 419640
#% 445444
#% 458868
#% 504161
#% 519421
#% 569916
#% 610818
#% 657652
#% 822359
#! The Grid is an emerging platform to support on-demand "virtual organisations" for coordinated resource sharing and problem solving on a global scale. The application thrust is large-scale scientific endeavour, and the scale and complexity of scientific data presents challenges for databases. The Grid is beginning to exploit technologies developed for Web Services and to realise its potential it also stands to benefit from Semantic Web technologies; conversely, the Grid and its scientific users provide application pull which will benefit the Semantic Web.

#index 449222
#* Review of "Ontologies: a silver bullet for knowledge management and electronic commerce" by Dieter Fensel. Springer-Verlag, 2002.
#@ Antonio Badia
#t 2002
#c 5
#% 269079

#index 449223
#* Small worlds: the dynamics of networks between order and randomness
#@ Jie Wu;Duncan J. Watts
#t 2002
#c 5

#index 449224
#* Parameterized complexity for the database theorist
#@ Martin Grohe
#t 2002
#c 5
#% 29440
#% 101944
#% 182574
#% 190254
#% 285967
#% 292675
#% 299976
#% 333851
#% 344155
#% 414961
#% 473117
#% 473125
#% 504524
#% 593756
#% 598376
#% 599549

#index 449225
#* Amicalola report: database and information systems research challenges and opportunities in semantic web and enterprises
#@ Amit Sheth;Robert Meersman
#t 2002
#c 5

#index 449226
#* Report on the EDBT'02 panel on scientific data integration
#@ Omar Boucelma;Silvana Castano;Carole Goble;Vanja Josifovski;Zoé Lacroix;Bertram Ludäscher
#t 2002
#c 5
#% 189739
#% 331769
#% 465159
#% 480319
#% 487716
#% 772133

#index 449227
#* An early look at XQuery
#@ Andrew Eisenberg;Jim Melton
#t 2002
#c 5

#index 449228
#* Research in information managment at Dublin City University
#@ Mark Roantree;Alan F. Smeaton
#t 2002
#c 5
#% 250966
#% 278610
#% 360717
#% 396740
#% 478060
#% 488458
#% 511909
#% 562117
#% 1783083

#index 451546
#* Performing joins without decompression in a compressed database system
#@ S. J. O'Connell;N. Winterbottom
#t 2003
#c 5
#% 223778
#% 322412
#% 333953
#% 390132
#% 403195
#% 443121
#! There has been much work on compressing database indexes, but less on compressing the data itself. We examine the performance gains to be made by compression outside the index. A novel compression algorithm is reported, which enables the processing of queries without decompressing data needed to perform join operations in a database built on a triple store. The results of modelling the performance of the database with and without compression are given and compared with other recent work in this area. It is found that for some applications, gains in performance of over 50% are achievable, and in OLTP-like situations, there are also gains to be made.

#index 451547
#* XPath processing in a nutshell
#@ Georg Gottlob;Christoph Koch;Reinhard Pichler
#t 2003
#c 5
#% 378393
#% 473117
#% 576108
#% 993939
#! We provide a concise yet complete formal definition of the semantics of XPath 1 and summarize e cient algorithms for processing queries in this language. Our presentation is intended both for the reader who is looking for a short but comprehensive formal account of XPath as well as the software developer in need of material that facilitates the rapid implementation of XPath engines.

#index 451548
#* A graphical query language for mobile information systems
#@ Ya-Hui Chang
#t 2003
#c 5
#% 295512
#% 300174
#% 361445
#% 435137
#% 442847
#% 445703
#% 461923
#% 464224
#% 632015
#! The advance of the mobile computing environment allows data to be accessed in any place at any time, but currently only simple and ad-hoc queries are supported. People are eager for mobile information systems with more functionality and powerful querying facilities. In this paper, a graphical query language called MoSQL is proposed to be the basis of general mobile information systems. It provides a uniform way for users to access alphanumerical data and to query current or future location information, based on an icon-based interface. The interface is particularly suitable for the mobile environment, since it is easily operated by clicking or dragging the mouse. An example and the underlying theoretical framework will be presented in this paper to demonstrate the functionality of MoSQL.

#index 451549
#* A multi-paradigm querying approach for a generic multimedia database management system
#@ Ji-Rong Wen;Qing Li;Wei-Ying Ma;Hong-Jiang Zhang
#t 2003
#c 5
#% 86952
#% 162626
#% 206633
#% 210172
#% 232703
#% 248010
#% 309726
#% 316148
#% 340934
#% 374001
#% 383260
#% 406493
#% 442887
#% 452796
#% 464720
#% 479469
#% 479803
#% 480330
#! To truly meet the requirements of multimedia database (MMDB) management, an integrated framework for modeling, managing and retrieving various kinds of media data in a uniform way is necessary. MediaLand is an experimental MMDB platform being developed at Microsoft Research Asia for users with different levels of experiences and expertise to manage and search multimedia repositories easily, efficiently, and cooperatively. Key features of MediaLand include a uniform data model for describing all kinds of media objects and their relationships, and a 4-tier architecture based on this data model. In this paper, a multi-paradigm querying approach of MediaLand is presented, in which multimedia queries are processed based on a seamless integration of various existing search approaches. In doing so, MediaLand also offers the feature of "media independence" which is analogous to the notion of "data independence" from the classic ANSI SPARC standard. By incorporating a rich set of facilities and techniques, MediaLand lays down a good foundation for addressing further research issues, such as multimedia query rewriting, optimization, and presentation.

#index 451550
#* A database approach to quality of service specification in video databases
#@ Elisa Bertino;Ahmed K. Elmagarmid;Mohand-Saïd Hacid
#t 2003
#c 5
#% 29089
#% 197910
#% 264854
#% 434694
#% 520127
#! Quality of Service (QoS) is defined as a set of perceivable attributes expressed in a user-friendly language with parameters that may be subjective or objective. Objective parameters are those related to a particular service and are measurable and verifiable. Subjective parameters are those based on the opinions of the end-users. We believe that quality of service should become an integral part of multimedia database systems and users should be able to query by requiring a quality of service from the system. The specification and enforcement of QoS presents an interesting challenge in multimedia systems development. A deal of effort has been done on QoS specification and control at the system and the network levels, but less work has been done at the application/user level. In this paper, we propose a language, in the style of constraint database languages, for formal specification of QoS constraints. The satisfaction by the system of the user quality requirements can be viewed as a constraint satisfaction problem. We believe this paper represents a first step towards the development of a database framework for quality of service management in video databases. The contribution of this paper lies in providing a logical framework for specifying and enforcing quality of service in video databases. To our knowledge, this work is the first from a database perspective on quality of service management.

#index 451551
#* ANSI SQL hierarchical processing can fully integrate native XML
#@ Michael M. David
#t 2003
#c 5
#% 119279
#% 333935
#% 354924
#% 397607
#% 428146
#! Most SQL-based XML vendor support is through interoperation and not integration. One reason for this is that XML is inherently hierarchical and SQL is supposedly not. This paper demonstrates how ANSI SQL along with its relational Cartesian product model can naturally perform complete and flexible hierarchical query processing. With this ANSI SQL inherent hierarchical processing capability, native XML data can be fully and seamlessly integrated into SQL processing and operated on at a full hierarchical level. This paper will describe the basic stages involved in this hierarchical SQL processing: hierarchical data modeling, hierarchical working set creation, and hierarchical Cartesian product processing. These processes enable a complete relational, XML, and legacy data integration which maintains ANSI SQL compatibility even while performing the most complex multi-leg hierarchical processing, and includes the dynamic, direct, and controlled hierarchical joining of hierarchical structures. Also covered are ANSI SQL hierarchical support features: hierarchical SQL views, hierarchical data filtering, and hierarchical optimization. These make standard SQL a well rounded and complete hierarchical processor. With this full hierarchical level of processing established, it will be shown how the relational Cartesian product engine can be seamlessly replaced with a hierarchical engine, greatly increasing processing and memory utilization, and enabling advanced XML hierarchical data capabilities.

#index 451552
#* Analysis of existing databases at the logical level: the DBA companion project
#@ Fabien De Marchi;Stéphane Lopes;Jean-Marc Petit;Farouk Toumani
#t 2003
#c 5
#% 6710
#% 169370
#% 275367
#% 289446
#% 332166
#% 332931
#% 400530
#% 427873
#% 458869
#% 462214
#% 464199
#% 479814
#% 498787
#! Whereas physical database tuning has received a lot of attention over the last decade, logical database tuning seems to be under-studied. We have developed a project called DBA Companion devoted to the understanding of logical database constraints from which logical database tuning can be achieved.In this setting, two main data mining issues need to be addressed: the first one is the design of efficient algorithms for functional dependencies and inclusion dependencies inference and the second one is about the interestingness of the discovered knowledge. In this paper, we point out some relationships between database analysis and data mining. In this setting, we sketch the underlying themes of our approach. Some database applications that could benefit from our project are also described, including logical database tuning.

#index 451553
#* Jim Gray speaks out
#@ Marianne Winslett
#t 2003
#c 5

#index 451554
#* Report on the first international workshop on efficient web-based information systems
#@ Zoé Lacroix;Omar Boucelma
#t 2003
#c 5

#index 451555
#* Report on the first international conference on ontologies, databases and applications of semantics (ODBASE): part of the federated conference "On the Move to Meaningful Internet Systems 2002"
#@ Karl Aberer
#t 2003
#c 5
#! ODBASE is a new conference series focussing on semantics, databases and the Web. In this article we provide a short overview of the history and the program of the first ODBASE event.

#index 451556
#* Database research at UT Arlington
#@ Sharma Chakravarthy;Alp Aslandogan;Ramez Elmasri;Leonidas Fegaras;JungHwan Oh
#t 2003
#c 5
#% 201873
#% 248788
#% 300239
#% 316253
#% 335725
#% 413563
#% 458834
#% 480663
#% 528196
#% 566376
#% 591270
#% 618574
#% 708379
#% 728373
#% 1388501
#% 1774632

#index 576090
#* Proceedings of the twenty-second ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems
#@ Frank Neven;Catriel Beeri;Tova Milo
#t 2003
#c 5
#! This volume contains the proceedings of the Twenty-second ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems (PODS 2003), held at San Diego, California on June 9-11, 2003 in conjunction with the 2003 ACM SIGMOD International Conference on Management of Data and as part of the Federated Computing Research Conference FCRC 2003. It consists of a paper based on the invited talk by Rick Hull, a paper based on the invited tutorial by Rakesh Agrawal, and 27 contributed papers that were selected by the program committee for presentation at the symposium.The contributed papers were selected from 136 submissions. Most of the papers are "extended abstracts" and are preliminary reports on work in progress. While they have been read by program committee members, they have not been formally refereed. It is expected that much of the research described in these papers will be published in detail in computer science journals.The program committee selected \An Information-Theoretic Approach to Normal Forms for Relational and XML Data" by Marcelo Arenas and Leonid Libkin for the PODS 2003 Best Paper Award and "Algorithms for Data Migration with Cloning" by Samir Khuller, Yoo-Ah Kim, Yung-Chun (Justin) Wan for the PODS 2003 Best Newcomer Award. Warmest congratulations to the authors of these papers.

#index 576091
#* E-services: a look behind the curtain
#@ Richard Hull;Michael Benedikt;Vassilis Christophides;Jianwen Su
#t 2003
#c 5
#% 38696
#% 70235
#% 92951
#% 101955
#% 261139
#% 274160
#% 289415
#% 291299
#% 297770
#% 317871
#% 320204
#% 320775
#% 321054
#% 333857
#% 343015
#% 348130
#% 365308
#% 374130
#% 404772
#% 458995
#% 460499
#% 465063
#% 479452
#% 489794
#% 509525
#% 519432
#% 526098
#% 541593
#% 543819
#% 577343
#% 587397
#% 653685
#% 654465
#% 654485
#% 657654
#! The emerging paradigm of electronic services promises to bring to distributed computation and services the flexibility that the web has brought to the sharing of documents. An understanding of fundamental properties of e-service composition is required in order to take full advantage of the paradigm. This paper examines proposals and standards for e-services from the perspectives of XML, data management, workflow, and process models. Key areas for study are identified, including behavioral service signatures, verification and synthesis techniques for composite services, analysis of service data manipulation commands, and XML analysis applied to service specifications. We give a sample of the relevant results and techniques in each of these areas.

#index 576092
#* An information-theoretic approach to normal forms for relational and XML data
#@ Marcelo Arenas;Leonid Libkin
#t 2003
#c 5
#% 10245
#% 11284
#% 30077
#% 101956
#% 115608
#% 205246
#% 241515
#% 286860
#% 287339
#% 287754
#% 292680
#% 300711
#% 333841
#% 333979
#% 378395
#% 384978
#% 411570
#% 428149
#% 443343
#% 480102
#% 527113
#% 533902
#% 572459
#% 993272
#! Normalization as a way of producing good database designs is a well-understood topic. However, the same problem of distinguishing well-designed databases from poorly designed ones arises in other data models, in particular, XML. While in the relational world the criteria for being well-designed are usually very intuitive and clear to state, they become more obscure when one moves to more complex data models.Our goal is to provide a set of tools for testing when a condition on a database design, specified by a normal form, corresponds to a good design. We use techniques of information theory, and define a measure of information content of elements in a database with respect to a set of constraints. We first test this measure in the relational context, providing information-theoretic justification for familiar normal forms such as BCNF, 4NF, PJ/NF, 5NFR, DK/NF. We then show that the same measure applies in the XML context, which gives us a characterization of a recently introduced XML normal form called XNF. Finally, we look at information-theoretic criteria for justifying normalization algorithms.

#index 576093
#* Algorithms for data migration with cloning
#@ Samir Khuller;Yoo-Ah Kim;Yung-Chun (Justin) Wan
#t 2003
#c 5
#% 73823
#% 137637
#% 176535
#% 269745
#% 302731
#% 325310
#% 325404
#% 493844
#% 836124
#! Our work is motivated by the problem of managing data on storage devices, typically a set of disks. Such high demand storage servers are used as web servers, or multimedia servers for handling high demand for data. As the system is running, it needs to dynamically respond to changes in demand for different data items. In this work we study the data migration problem, which arises when we need to quickly change one storage configuration into another. We show that this problem is NP-hard. In addition, we develop polynomial-time approximation algorithms for this problem and prove a worst case bound of 9.5 on the approximation factor achieved by our algorithm. We also compare the algorithm to several heuristics for this problem.

#index 576094
#* Privacy in data systems
#@ Rakesh Agrawal
#t 2003
#c 5
#% 67453
#% 300184
#% 333876
#% 577233
#% 577289
#% 577366
#% 654448
#% 993943
#% 993944
#! The explosive progress in networking, storage, and processor technologies is resulting in an unprecedented amount of digitization of information. In concert with this dramatic increase in digital data, concerns about the privacy of personal information have emerged globally. The concerns over massive collection of data are naturally extending to analytic tools applied to data. Data mining, with its promise to efficiently discover valuable, non-obvious information from large databases, is particularly vulnerable to misuse.The challenge for the database community is to design information systems that protect the privacy and ownership of individual data without impeding information flow. One way of preserving privacy of individual data values is to perturb them. Since the primary task in data mining is the development of models about aggregated data, we explore if we can develop accurate models without access to precise information in individual data records. We consider the concrete case of building a decision-tree classifier from perturbed data. While it is not possible to accurately estimate original values in individual data records, we describe a reconstruction procedure to accurately estimate the distribution of original data values. By using these reconstructed distributions, we are able to build classifiers whose accuracy is comparable to the accuracy of classifiers built with the original data. We also discuss how to discover association rules over privacy preserved data.Inspired by the privacy tenet of the Hippocratic Oath, we argue that future database systems must include responsibility for the privacy of data they manage as a founding tenet. We enunciate the key principles for such Hippocratic database systems, distilled from the principles behind current privacy legislations and guidelines. We identify the technical challenges and problems in designing Hippocratic databases, and also outline some solution approaches.

#index 576095
#* Materializing views with minimal size to answer queries
#@ Rada Chirkova;Chen Li
#t 2003
#c 5
#% 121
#% 36683
#% 116303
#% 198465
#% 210182
#% 264263
#% 286916
#% 289266
#% 300138
#% 333964
#% 333965
#% 378402
#% 378409
#% 397367
#% 411554
#% 464706
#% 479452
#% 479476
#% 479792
#% 480149
#% 480158
#% 480670
#% 482110
#% 482111
#% 531450
#% 536422
#% 572311
#% 599549
#% 632026
#% 632039
#! In this paper we study the following problem. Given a database and a set of queries, we want to find, in advance, a set of views that can compute the answers to the queries, such that the size of the viewset (i.e., the amount of space, in bytes, required to store the viewset) is minimal on the given database. This problem is important for many applications such as distributed databases, data warehousing, and data integration. We explore the decidability and complexity of the problem for workloads of conjunctive queries. We show that results differ significantly depending on whether the workload queries have self-joins. If queries can have self-joins, then a disjunctive viewset can be a better solution than any set of conjunctive views. We show that the problem of finding a minimal-size disjunctive viewset is decidable, and give an upper bound on its complexity. If workload queries cannot have self-joins, there is no need to consider disjunctive viewsets, and we show that the problem is in NP. We describe a very compact search space of conjunctive views, which contains all views in at least one optimal disjunctive viewset. We give a dynamic-programming algorithm for finding minimal-size disjunctive viewsets for queries without self-joins, and discuss heuristics to make the algorithm efficient.

#index 576096
#* The impact of the constant complement approach towards view updating
#@ Jens Lechtenbörger
#t 2003
#c 5
#% 84615
#% 286901
#% 287730
#% 291869
#% 315853
#% 335500
#% 378400
#% 384978
#% 562676
#% 572312
#! Views play an important role as a means to structure information with respect to specific users' needs. While read access through views is easy to handle, update requests through views are dificult in the sense that they have to be translated into appropriate updates on database relations. In this paper the "constant complement translator" approach towards view updating proposed by Bancilhon and Spyratos is revisited within the realm of SQL databases, and a novel characterization is established showing that constant complement translators exist precisely if users have a chance to undo all effects of their view updates using further view updates. Based on this characterization view updates with and without constant complement translators are presented. As it turns out that users cannot fully understand updates on views violating the constant complement principle, the application of this principle in the context of external schema design is discussed.

#index 576097
#* View-based query containment
#@ Diego Calvanese;Giuseppe De Giacomo;Maurizio Lenzerini;Moshe Y. Vardi
#t 2003
#c 5
#% 55690
#% 198465
#% 198473
#% 210176
#% 210214
#% 211987
#% 237191
#% 248033
#% 248038
#% 261741
#% 268708
#% 283052
#% 291299
#% 292677
#% 299967
#% 299968
#% 378409
#% 378410
#% 464056
#% 464720
#% 495632
#% 562451
#% 572311
#% 587566
#! Query containment is the problem of checking whether for all databases the answer to a query is a subset of the answer to a second query. In several data management tasks, such as data integration, mobile computing, etc., the data of interest are only accessible through a given set of views. In this case, containment of queries should be determined relative to the set of views, as already noted in the literature. Such a form of containment, which we call view-based query containment, is the subject of this paper. The problem comes in various forms, depending on whether each of the two queries is expressed over the base alphabet or the alphabet of the view names. We present a thorough analysis of view-based query containment, by discussing all possible combinations from a semantic point of view, and by showing their mutual relationships. In particular, for the two settings of conjunctive queries and two-way regular path queries, we provide both techniques and complexity bounds for the different variants of the problem. Finally, we study the relationship between view-based query containment and view-based query rewriting.

#index 576098
#* The view selection problem for XML content based routing
#@ Ashish Kumar Gupta;Dan Suciu;Alon Y. Halevy
#t 2003
#c 5
#% 124009
#% 158911
#% 217812
#% 297191
#% 342372
#% 465061
#% 480296
#% 556654
#% 654476
#% 656763
#% 659987
#% 659995
#! We consider the view selection problem for XML content based routing: given a network, in which a stream of XML documents is routed and the routing decisions are taken based on results of evaluating XPath predicates on these documents, select a set of views that maximize the throughput of the network. While in view selection for relational queries the speedup comes from eliminating joins, here the speedup is obtained from gaining direct access to data values in an XML packet, without parsing that packet. The views in our context can be seen as a binary representation of the XML document, tailored for the network's workload.In this paper we define formally the view selection problem in the context of XML content based routing, and provide a practical solution for it. First, we formalize the problem; while the exact formulation is too complex to admit practical solutions, we show that it can be simplified to a manageable optimization problem, without loss in precision. Second we show that the simplified problem can be reduced to the Integer Cover problem. The Integer Cover problem is known to be NP-hard, and to admit a log n greedy approximation algorithm. Third, we show that the same greedy approximation algorithm performs much better on a class of work-loads called 'hierarchical workloads', which are typical in XML stream processing. Namely, it returns an optimal solution for hierarchical workloads, and degrades gracefully to the log n general bound as the workload becomes less hierarchical.

#index 576099
#* Computing full disjunctions
#@ Yaron Kanza;Yehoshua Sagiv
#t 2003
#c 5
#% 663
#% 2984
#% 11817
#% 90744
#% 94459
#% 172933
#% 201927
#% 213983
#% 220425
#% 252366
#% 273703
#% 286417
#% 286995
#% 287223
#% 287333
#% 289321
#% 289368
#% 289424
#% 289425
#% 333858
#% 463919
#% 479614
#% 598376
#% 750877
#% 1499471
#! Under either the OR-semantics or the weak semantics, the answer to a query over semistructured data consists of maximal rather than complete matchings, i.e., some query variables may be assigned null values. In the relational model, a similar effect is achieved by computing the full disjunction (rather than the natural join or equijoin) of the given relations. It is shown that under either the OR-semantics or the weak semantics, query evaluation has a polynomial-time complexity in the size of the query, the database and the result. It is also shown that the evaluation of full disjunctions is reducible to query evaluation under the weak semantics and hence can be done in polynomial time in the size of the input and the output. Complexity results are also given for two related problems. One is evaluating a projection of the full disjunction and the other is evaluating the set of all tuples in the full disjunction that are non-null on some given attributes. In the special case of γ-acyclic relation schemes, both problems have polynomial-time algorithms in the size of the input and the output. In the general case, such algorithms do not exist, assuming that P ≠ NP. Finally, it is shown that the weak semantics can generalize full disjunctions by allowing tuples to be joined according to general types of conditions, rather than just equalities among attributes.

#index 576100
#* Data exchange: getting to the core
#@ Ronald Fagin;Phokion G. Kolaitis;Lucian Popa
#t 2003
#c 5
#% 391
#% 583
#% 101956
#% 129217
#% 248038
#% 264858
#% 283052
#% 287339
#% 287733
#% 378409
#% 384978
#% 465053
#% 465057
#% 480134
#% 564416
#% 572311
#% 598389
#% 599549
#% 993981
#! Data exchange is the problem of taking data structured under a source schema and creating an instance of a target schema that reflects the source data as accurately as possible. Given a source instance, there may be many solutions to the data exchange problem, that is, many target instances that satisfy the constraints of the data exchange problem. In an earlier paper, we identified a special class of solutions that we call universal. A universal solution has homomorphisms into every possible solution, and hence is a "most general possible" solution. Nonetheless, given a source instance, there may be many universal solutions. This naturally raises the question of whether there is a "best" universal solution, and hence a best solution for data exchange. We answer this question by considering the well-known notion of the core of a structure, a notion that was first studied in graph theory, but has also played a role in conjunctive-query processing. The core of a structure is the smallest substructure that is also a homomorphic image of the structure. All universal solutions have the same core (up to isomorphism); we show that this core is also a universal solution, and hence the smallest universal solution. The uniqueness of the core of a universal solution together with its minimality make the core an ideal solution for data exchange. Furthermore, we show that the core is the best among all universal solutions for answering unions of conjunctive queries with inequalities. After this, we investigate the computational complexity of producing the core. Well-known results by Chandra and Merlin imply that, unless P = NP, there is no polynomial-time algorithm that, given a structure as input, returns the core of that structure as output. In contrast, in the context of data exchange, we identify natural and fairly broad conditions under which there are polynomial-time algorithms for computing the core of a universal solution. Finally, we analyze the computational complexity of the following decision problem that underlies the computation of cores: given two graphs G and H, is H the core ofG? Earlier results imply that this problem is both NP-hard and coNP-hard. Here, we pinpoint its exact complexity by establishing that it is a DP-complete problem.

#index 576101
#* Soft stratification for magic set based query evaluation in deductive databases
#@ Andreas Behrend
#t 2003
#c 5
#% 11797
#% 13014
#% 17084
#% 58363
#% 64407
#% 64410
#% 86943
#% 101623
#% 103705
#% 105239
#% 105243
#% 123123
#% 154317
#% 196695
#% 196699
#! In this paper we propose a new bottom-up query evaluation method for stratified deductive databases based on the Magic Set approach. As the Magic Sets rewriting may lead to unstratifiable rules, we propose to use Kerisit's weak consequence operator to compute the well-founded model of magic rules (guaranteed to be two-valued). We show that its application in combination with the concept weak stratification, however, may lead to a set of answers which is neither sound nor complete with respect to the well-founded model. This problem is cured by introducing the new concept soft stratification instead.

#index 576102
#* Query containment and rewriting using views for regular path queries under constraints
#@ Gösta Grahne;Alex Thomo
#t 2003
#c 5
#% 69495
#% 119917
#% 197751
#% 198465
#% 241136
#% 248024
#% 248025
#% 273700
#% 273924
#% 291299
#% 292677
#% 299967
#% 359443
#% 404772
#% 465048
#% 504583
#% 528141
#% 562454
#% 632039
#! In this paper we consider general path constraints for semistructured databases. Our general constraints do not suffer from the limitations of the path constraints previously studied in the literature. We investigate the containment of regular path queries under general path constraints. We show that when the path constraints and queries are expressed by words, as opposed to languages, the containment problem becomes equivalent to the word rewrite problem for a corresponding semi-Thue system. Consequently, if the corresponding semi-Thue system has an undecidable word problem, the word query containment problem will be undecidable too. Also, we show that there are word constraints, where the corresponding semi-Thue system has a decidable word rewrite problem, but the general query containment under these word constraints is undecidable. In order to overcome this, we exhibit a large, practical class of word constraints with a decidable general query containment problem.Based on the query containment under constraints, we reason about constrained rewritings -using views- of regular path queries. We give a constructive characterization for computing optimal constrained rewritings using views.

#index 576103
#* Concise descriptions of subsets of structured sets
#@ Alberto O. Mendelzon;Ken Q. Pu
#t 2003
#c 5
#% 248792
#% 248807
#% 271239
#% 408396
#% 516354
#% 519534
#% 993995
#! We study the problem of economical representation of subsets of structured sets, that is, sets equipped with a set cover. Given a structured set U, and a language L whose expressions define subsets of U, the problem of Minimum Description Length in L (L-MDL) is: "given a subset V of U, find a shortest string in L that defines V".We show that the simple set cover is enough to model a number of realistic database structures. We focus on two important families: hierarchical and multidimensional organizations. The former is found in the context of semistructured data such as XML, the latter in the context of statistical and OLAP databases. In the case of general OLAP databases, data organization is a mixture of multidimensionality and hierarchy, which can also be viewed naturally as a structured set. We study the complexity of the L-MDL problem in several settings, and provide an efficient algorithm for the hierarchical case.Finally, we illustrate the application of the theory to summarization of large result sets, (multi) query optimization for ROLAP queries, and XML queries.

#index 576104
#* On producing join results early
#@ Jens-Peter Dittrich;Bernhard Seeger;David Scot Taylor;Peter Widmayer
#t 2003
#c 5
#% 2194
#% 13041
#% 112323
#% 136740
#% 210187
#% 252608
#% 273908
#% 273910
#% 273911
#% 333973
#% 342595
#% 397363
#% 411554
#% 462070
#% 463759
#% 479797
#% 479928
#% 526847
#% 632105
#% 659919
#% 993956
#! Support for exploratory interaction with databases in applications such as data mining requires that the first few results of an operation be available as quickly as possible. We study the algorithmic side of what can and what cannot be achieved for processing join operations. We develop strategies that modify the strict two-phase processing of the sort-merge paradigm, intermingling join steps with selected merge phases of the sort. We propose an algorithm that produces early join results for a broad class of join problems, including many not addressed well by hash-based algorithms. Our algorithm has no significant increase in the number of I/O operations needed to complete the join compared to standard sort-merge algorithms.

#index 576105
#* Correlating XML data streams using tree-edit distance embeddings
#@ Minos Garofalakis;Amit Kumar
#t 2003
#c 5
#% 214073
#% 234905
#% 244328
#% 273682
#% 333931
#% 379449
#% 397354
#% 397373
#% 397385
#% 480628
#% 492912
#% 519953
#% 565342
#% 593957
#% 594029
#% 656804
#% 659995
#% 768815
#% 993959
#% 993960
#% 993969
#! We propose the first known solution to the problem of correlating, in small space, continuous streams of XML data through approximate (structure and content) matching, as defined by a general tree-edit distance metric. The key element of our solution is a novel algorithm for obliviously embedding tree-edit distance metrics into an L1 vector space while guaranteeing an upper bound of O(log2 n log* n) on the distance distortion between any data trees with at most n nodes. We demonstrate how our embedding algorithm can be applied in conjunction with known random sketching techniques to: (1) build a compact synopsis of a massive, streaming XML data tree that can be used as a concise surrogate for the full tree in approximate tree-edit distance computations; and, (2) approximate the result of tree-edit distance similarity joins over continuous XML document streams. To the best of our knowledge, these are the first algorithmic results on low-distortion embeddings for tree-edit distance metrics, and on correlating (e.g., through similarity joins) XML data in the streaming model.

#index 576106
#* Numerical document queries
#@ Helmut Seidl;Thomas Schwentick;Anca Muscholl
#t 2003
#c 5
#% 288469
#% 292125
#% 299942
#% 344425
#% 378389
#% 401124
#% 494499
#% 505610
#% 516520
#% 526961
#% 529241
#% 559320
#! A query against a database behind a site like Napster may search, e.g., for all users who have downloaded more jazz titles than pop music titles. In order to express such queries, we extend classical monadic second-order logic by Presburger predicates which pose numerical restrictions on the children (content) of an element node and provide a precise automata-theoretic characterization. While the existential fragment of the resulting logic is decidable, it turns out that satisfiability of the full logic is undecidable. Decidable satisfiability and a querying algorithm even with linear data complexity can be obtained if numerical constraints are only applied to those contents of elements where ordering is irrelevant. Finally, it is sketched how these techniques can be extended also to answer questions like, e.g., whether the total price of the jazz music downloaded so far exceeds a user's budget.

#index 576107
#* Typing and querying XML documents: some complexity bounds
#@ Luc Segoufin
#t 2003
#c 5
#% 23627
#% 27615
#% 59787
#% 60869
#% 84275
#% 101643
#% 107906
#% 115525
#% 231873
#% 248799
#% 262724
#% 279367
#% 288513
#% 288711
#% 289137
#% 299944
#% 339937
#% 355864
#% 378392
#% 382963
#% 465059
#% 483878
#% 487257
#% 545382
#% 562461
#% 576108
#% 653706
#% 993939
#! We study the complexity bound of validating XML documents, viewed as labeled unranked ordered trees, against various typing systems like DTDs, XML schemas, tree automata ... We also consider query evaluation complexities for various fragments of XPath. For both problems, validation and query evaluation, we consider data and combined complexity bounds.

#index 576108
#* The complexity of XPath query evaluation
#@ Georg Gottlob;Christoph Koch;Reinhard Pichler
#t 2003
#c 5
#% 27615
#% 59787
#% 101643
#% 101922
#% 183411
#% 576107
#% 598376
#% 993939
#! In this paper, we study the precise complexity of XPath 1.0 query processing. Even though heavily used by its incorporation into a variety of XML-related standards, the precise cost of evaluating an XPath query is not yet wellunderstood. The first polynomial-time algorithm for XPath processing (with respect to combined complexity) was proposed only recently, and even to this day all major XPath engines take time exponential in the size of the input queries. From the standpoint of theory, the precise complexity of XPath query evaluation is open, and it is thus unknown whether the query evaluation problem can be parallelized.In this work, we show that both the data complexity and the query complexity of XPath 1.0 fall into lower (highly parallelizable) complexity classes, but that the combined complexity is PTIME-hard. Subsequently, we study the sources of this hardness and identify a large and practically important fragment of XPath 1.0 for which the combined complexity is LOGCFL-complete and, therefore, in the highly parallelizable complexity class NC2.

#index 576109
#* Query-preserving watermarking of relational databases and XML documents
#@ David Gross-Amblard
#t 2003
#c 5
#% 66937
#% 172928
#% 212253
#% 245655
#% 268707
#% 278408
#% 299942
#% 303051
#% 307258
#% 336497
#% 342904
#% 389077
#% 401124
#% 504361
#% 539561
#% 562301
#% 604688
#% 993944
#! Watermarking allows robust and unobtrusive insertion of information in a digital document. Very recently, techniques have been proposed for watermarking relational databases or XML documents, where information insertion must preserve a specific measure on data (e.g. mean and variance of numerical attributes.)In this paper we investigate the problem of watermarking databases or XML while preserving a set of parametric queries in a specified language, up to an acceptable distortion.We first observe that unrestricted databases can not be watermarked while preserving trivial parametric queries. We then exhibit query languages and classes of structures that allow guaranteed watermarking capacity, namely 1) local query languages on structures with bounded degree Gaifman graph, and 2) monadic second-order queries on trees or tree-like structures. We relate these results to an important topic in computational learning theory, the VC-dimension. We finally consider incremental aspects of query-preserving watermarking.

#index 576110
#* Revealing information while preserving privacy
#@ Irit Dinur;Kobbi Nissim
#t 2003
#c 5
#% 149
#% 1868
#% 23638
#% 67453
#% 243632
#% 287298
#% 287794
#% 287795
#% 299970
#% 300184
#% 333876
#% 340475
#% 374401
#% 482049
#% 482071
#% 593827
#! We examine the tradeoff between privacy and usability of statistical databases. We model a statistical database by an n-bit string d1,..,dn, with a query being a subset q ⊆ [n] to be answered by Σiεq di. Our main result is a polynomial reconstruction algorithm of data from noisy (perturbed) subset sums. Applying this reconstruction algorithm to statistical databases we show that in order to achieve privacy one has to add perturbation of magnitude (Ω√n). That is, smaller perturbation always results in a strong violation of privacy. We show that this result is tight by exemplifying access algorithms for statistical databases that preserve privacy while adding perturbation of magnitude Õ(√n).For time-T bounded adversaries we demonstrate a privacypreserving access algorithm whose perturbation magnitude is ≈ √T.

#index 576111
#* Limiting privacy breaches in privacy preserving data mining
#@ Alexandre Evfimievski;Johannes Gehrke;Ramakrishnan Srikant
#t 2003
#c 5
#% 224752
#% 300184
#% 333876
#% 481290
#% 577233
#% 577289
#% 993988
#! There has been increasing interest in the problem of building accurate data mining models over aggregate data, while protecting privacy at the level of individual records. One approach for this problem is to randomize the values in individual records, and only disclose the randomized values. The model is then built over the randomized data, after first compensating for the randomization (at the aggregate level). This approach is potentially vulnerable to privacy breaches: based on the distribution of the data, one may be able to learn with high confidence that some of the randomized records satisfy a specified property, even though privacy is preserved on average.In this paper, we present a new formulation of privacy breaches, together with a methodology, "amplification", for limiting them. Unlike earlier approaches, amplification makes it is possible to guarantee limits on privacy breaches without any knowledge of the distribution of the original data. We instantiate this methodology for the problem of mining association rules, and modify the algorithm from [9] to limit privacy breaches without knowledge of the data distribution. Next, we address the problem that the amount of randomization required to avoid privacy breaches (when mining association rules) results in very long transactions. By using pseudorandom generators and carefully choosing seeds such that the desired items from the original transaction are present in the randomized transaction, we can send just the seed instead of the transaction, resulting in a dramatic drop in communication and storage cost. Finally, we define new information measures that take privacy breaches into account when quantifying the amount of privacy preserved by randomization.

#index 576112
#* Maintaining time-decaying stream aggregates
#@ Edith Cohen;Martin Strauss
#t 2003
#c 5
#% 45094
#% 152804
#% 281257
#% 320240
#% 378388
#% 379445
#% 397443
#% 449105
#% 1852712
#! We formalize the problem of maintaining time-decaying aggregates and statistics of a data stream: the relative contribution of each data item to the aggregate is scaled down by a factor that depends on, and is non-decreasing with, elapsed time. Time-decaying aggregates are used in applications where the significance of data items decreases over time. We develop storage-efficient algorithms, and establish upper and lower bounds. Surprisingly, even though maintaining decayed aggregates have become a widely-used tool, our work seems to be the first both to explore it formally and to provide storage-efficient algorithms for important families of decay functions, including polynomial decay.

#index 576113
#* Maintaining variance and k-medians over data stream windows
#@ Brain Babcock;Mayur Datar;Rajeev Motwani;Liadan O'Callaghan
#t 2003
#c 5
#% 210173
#% 248790
#% 271236
#% 280463
#% 300132
#% 310500
#% 325357
#% 338344
#% 342600
#% 378388
#% 379444
#% 379445
#% 397443
#% 480628
#% 580668
#% 593913
#% 593937
#% 594010
#% 594012
#% 659972
#! The sliding window model is useful for discounting stale data in data stream applications. In this model, data elements arrive continually and only the most recent N elements are used when answering queries. We present a novel technique for solving two important and related problems in the sliding window model---maintaining variance and maintaining a k--median clustering. Our solution to the problem of maintaining variance provides a continually updated estimate of the variance of the last N values in a data stream with relative error of at most ε using O(1/ε2 log N) memory. We present a constant-factor approximation algorithm which maintains an approximate k--median solution for the last N data points using O(k/τ4 N2τ log2 N) memory, where τ O(2O(1/τ)).

#index 576114
#* Optimal indexing using near-minimal space
#@ C. Heeren;H. V. Jagadish;L. Pitt
#t 2003
#c 5
#% 115562
#% 145224
#% 210182
#% 222814
#% 248815
#% 273697
#% 397359
#% 397360
#% 462204
#% 464706
#% 480158
#% 480489
#% 482100
#% 632100
#! We consider the index selection problem. Given either a fixed query workload or an unknown probability distribution on possible future queries, and a bound B on how much space is available to build indices, we seek to build a collection of indices for which the average query response time is minimized. We give strong negative and positive peformance bounds.Let m be the number of queries in the workload. We show how to obtain with high probability a collection of indices using space O(B ln m) for which the average query cost is optB, the optimal performance possible for indices using at most B total space. Moreover, this space relaxation is necessary: unless NP ⊆ nO(log log n), no polynomial time algorithm can guarantee average query cost less than M1--ε optB using space αB, for any constant α, where M is the size of the dataset. We quantify the error in performance introduced by running the algorithm on a sample drawn from a query distribution.

#index 576115
#* On nearest neighbor indexing of nonlinear trajectories
#@ Charu C. Aggarwal;Dakshi Agrawal
#t 2003
#c 5
#% 2115
#% 86950
#% 227939
#% 273706
#% 287466
#% 299979
#% 300174
#% 353133
#% 427199
#% 480093
#% 481956
#% 503869
#% 527195
#! In recent years, the problem of indexing mobile objects has assumed great importance because of its relevance to a wide variety of applications. Most previous results in this area have proposed indexing schemes for objects with linear trajectories in one or two dimensions. In this paper, we present methods for indexing objects with nonlinear trajectories. Specifically, we identify a useful condition called the convex hull property and show that any trajectory satisfying this condition can be indexed by storing a careful representation of these objects in a traditional index structure. Since a wide variety of relevant nonlinear trajectories satisfy this condition, our result significantly expands the class of trajectories for which nearest neighbor indexing schemes can be devised. We also show that even though many non-linear trajectories do not satisfy the convex hull condition, an approximate representation can often be found which satisfies it. We discuss examples of techniques which can be utilized to find representations that satisfy the convex hull property. We present empirical results to demonstrate the effectiveness of our indexing method.

#index 576116
#* On the decidability and complexity of query answering over inconsistent and incomplete databases
#@ Andrea Calì;Domenico Lembo;Riccardo Rosati
#t 2003
#c 5
#% 131559
#% 222826
#% 273687
#% 378409
#% 384978
#% 416007
#% 464915
#% 465057
#% 519568
#% 536353
#% 572311
#% 591534
#! In databases with integrity constraints, data may not satisfy the constraints. In this paper, we address the problem of obtaining consistent answers in such a setting, when key and inclusion dependencies are expressed on the database schema. We establish decidability and complexity results for query answering under different assumptions on data (soundness and/or completeness). In particular, after showing that the problem is in general undecidable, we identify the maximal class of inclusion dependencies under which query answering is decidable in the presence of key dependencies. Although obtained in a single database context, such results are directly applicable to data integration, where multiple information sources may provide data that are inconsistent with respect to the global view of the sources.

#index 576117
#* How to quickly find a witness
#@ Daniel Kifer;Johannes Gehrke;Cristian Bucila;Walker White
#t 2003
#c 5
#% 152934
#% 232136
#% 237200
#% 248785
#% 273899
#% 274146
#% 310558
#% 399793
#% 399794
#% 399795
#% 399796
#% 464989
#% 481290
#% 495270
#% 577215
#! The subfield of itemset mining is essentially a collection of algorithms. Whenever a new type of constraint is discovered, a specialized algorithm is proposed to handle it. All of these algorithms are highly tuned to take advantage of the unique properties of their associated constraints, and so they are not very compatible with other constraints. In this paper we present a more unified view of mining constrained itemsets such that most existing algorithms can be easily extended to handle constraints for which they were not designed a-priori. We apply this technique to mining itemsets with restrictions on their variance --- a problem that has been open for several years in the data mining community.

#index 576118
#* Feasible itemset distributions in data mining: theory and application
#@ Ganesh Ramesh;William A. Maniatty;Mohammed J. Zaki
#t 2003
#c 5
#% 227917
#% 232136
#% 237200
#% 248791
#% 300120
#% 310507
#% 316709
#% 342643
#% 459020
#% 465003
#% 466487
#% 466664
#% 481754
#! Computing frequent itemsets and maximally frequent item-sets in a database are classic problems in data mining. The resource requirements of all extant algorithms for both problems depend on the distribution of frequent patterns, a topic that has not been formally investigated. In this paper, we study properties of length distributions of frequent and maximal frequent itemset collections and provide novel solutions for computing tight lower bounds for feasible distributions. We show how these bounding distributions can help in generating realistic synthetic datasets, which can be used for algorithm benchmarking.

#index 576119
#* What's hot and what's not: tracking most frequent items dynamically
#@ Graham Cormode;S. Muthukrishnan
#t 2003
#c 5
#% 152585
#% 190611
#% 201921
#% 214073
#% 248812
#% 273682
#% 347226
#% 407822
#% 479795
#% 482123
#% 492912
#% 548479
#% 569754
#% 632090
#% 654443
#% 993960
#% 993969
#! Most database management systems maintain statistics on the underlying relation. One of the important statistics is that of the "hot items" in the relation: those that appear many times (most frequently, or more than some threshold). For example, end-biased histograms keep the hot items as part of the histogram and are used in selectivity estimation. Hot items are used as simple outliers in data mining, and in anomaly detection in networking applications.We present a new algorithm for dynamically determining the hot items at any time in the relation that is undergoing deletion operations as well as inserts. Our algorithm maintains a small space data structure that monitors the transactions on the relation, and when required, quickly outputs all hot items, without rescanning the relation in the database. With user-specified probability, it is able to report all hot items. Our algorithm relies on the idea of "group testing", is simple to implement, and has provable quality, space and time guarantees. Previously known algorithms for this problem that make similar quality and performance guarantees can not handle deletions, and those that handle deletions can not make similar guarantees without rescanning the database. Our experiments with real and synthetic data shows that our algorithm is remarkably accurate in dynamically tracking the hot items independent of the rate of insertions and deletions.

#index 578560
#* Issues in data stream management
#@ Lukasz Golab;M. Tamer Özsu
#t 2003
#c 5
#% 214073
#% 273907
#% 300167
#% 300179
#% 310488
#% 333926
#% 333931
#% 336610
#% 340635
#% 342600
#% 344400
#% 378388
#% 378408
#% 379444
#% 379445
#% 397352
#% 397353
#% 397354
#% 397389
#% 397414
#% 397426
#% 397443
#% 443298
#% 480628
#% 492912
#% 492932
#% 548479
#% 576113
#% 580668
#% 593957
#% 594012
#% 622760
#% 629097
#% 632090
#% 654462
#% 654482
#% 654488
#% 660004
#% 979303
#% 993948
#% 993949
#% 993958
#% 993959
#% 993960
#% 993961
#% 993999
#! Traditional databases store sets of relatively static records with no pre-defined notion of time, unless timestamp attributes are explicitly added. While this model adequately represents commercial catalogues or repositories of personal information, many current and emerging applications require support for on-line analysis of rapidly changing data streams. Limitations of traditional DBMSs in supporting streaming applications have been recognized, prompting research to augment existing technologies and build new systems to manage streaming data. The purpose of this paper is to review recent work in data stream management systems, with an emphasis on application requirements, data models, continuous query languages, and query evaluation.

#index 578561
#* Closing the key loophole in MLS databases
#@ Nenad Jukic;Svetlozar Nestorov;Susan Vrbsky
#t 2003
#c 5
#% 102749
#% 236413
#% 261360
#% 273895
#% 366272
#% 443009
#% 443156
#% 480945
#% 507241
#% 566384
#% 993558
#! There has been an abundance of research within the last couple of decades in the area of multilevel secure (MLS) databases. Recent work in this field deals with the processing of multilevel transactions, expanding the logic of MLS query languages, and utilizing MLS principles within the realm of E-Business. However, there is a basic flaw within the MLS logic, which obstructs the handling of clearance-invariant aggregate queries and physical-entity related queries where some of the information in the database may be gleaned from the outside world. This flaw stands in the way of a more pervasive adoption of MLS models by the developers of practical applications. This paper clearly identifies the cause of this impediment -- the cover story dependence on the value of a user-defined key -- and proposes a practical solution.

#index 578562
#* XPath processing in a nutshell
#@ Georg Gottlob;Christoph Koch;Reinhard Pichler
#t 2003
#c 5
#% 378393
#% 473117
#% 993939
#! We provide a concise yet complete formal definition of the semantics of XPath 1 and summarize efficient algorithms for processing queries in this language. Our presentation is intended both for the reader who is looking for a short but comprehensive formal account of XPath as well as the software developer in need of material that facilitates the rapid implementation of XPath engines.

#index 578563
#* An approach to confidence based page ranking for user oriented Web search
#@ Debajyoti Mukhopadhyay;Debasis Giri;Sanasam Ranbir Singh
#t 2003
#c 5
#% 268079
#% 309779
#% 320930
#% 348178
#% 438136
#% 438553
#% 656794

#index 578564
#* Modelling temporal thematic map contents
#@ Alberto d'Onofrio;Elaheh Pourabbas
#t 2003
#c 5
#% 78365
#% 116335
#% 319244
#% 564075

#index 578565
#* Research issues for data communication in mobile ad-hoc network database systems
#@ Leslie D. Fife;Le Gruenwald
#t 2003
#c 5
#% 172876
#% 235863
#% 259665
#% 281557
#% 286575
#% 310282
#% 313635
#% 331031
#% 340722
#% 340730
#% 341471
#% 347962
#% 413193
#% 495557
#% 511171
#% 536045
#% 608489
#! Mobile Ad-hoc Networks (MANET) is an emerging area of research. Most current work is centered on routing issues. This paper discusses the issues associated with data communication with MANET database systems. While data push and data pull methods have been previously addressed in mobile networks, the proposed methods do not handle the unique requirements associated with MANET. Unlike traditional mobile networks, all nodes within the MANET are mobile and battery powered. Existing wireless algorithms and protocols are insufficient primarily because they do not consider the mobility and power requirements of both clients and servers. This paper will present some of the critical tasks facing this research.

#index 578566
#* Towards a database body of knowledge: a study from Spain
#@ Coral Calero;Mario Piattini;Francisco Ruiz
#t 2003
#c 5
#% 140389
#% 207552
#% 282431
#% 301809
#% 339328
#% 387427
#% 392275
#% 392975
#% 410004
#% 410461
#! Databases, the center of today's information systems, are becoming more and more important judging by the huge volume of business they generate. In fact, database related material is included in a variety of curricula proposed by international organizations and prestigious universities. However, a systemized database body of knowledge (DBBOK), analogous to other works in Software Engineering (SWEBOK) or in Project Management (PMBOK) is needed. In this paper, we propose a first draft for this DBBOK based on degree programs from a variety of universities, the most relevant international curricula and the contents of the latest editions of principle books on databases.

#index 578567
#* Fundamentals of data warehouses: 2nd revised and extended edition
#@ Vernon Hoffner
#t 2003
#c 5

#index 578568
#* Peer-to-peer: harnessing the power of disruptive technologies
#@ Mario A. Nascimento
#t 2003
#c 5

#index 578569
#* Information rules
#@ Dale A. Stirling
#t 2003
#c 5

#index 578570
#* A Web odyssey: from codd to XML
#@ Victor Vianu
#t 2003
#c 5
#% 103705
#% 175464
#% 201976
#% 210214
#% 219492
#% 236416
#% 241160
#% 248024
#% 248799
#% 248819
#% 261370
#% 273686
#% 274160
#% 277343
#% 277344
#% 279367
#% 281149
#% 292677
#% 299942
#% 299943
#% 299944
#% 299976
#% 300143
#% 303887
#% 309851
#% 313640
#% 322415
#% 330627
#% 332151
#% 333841
#% 333857
#% 340295
#% 344425
#% 345757
#% 378392
#% 378394
#% 378395
#% 378411
#% 398752
#% 401124
#% 427027
#% 462062
#% 463919
#% 464720
#% 465059
#% 479465
#% 481602
#% 504575
#% 504578
#% 516520
#% 545382
#% 562141
#% 653704
#% 993950

#index 578571
#* A mapping mechanism to support bitmap index and other auxiliary structures on tables stored as primary B+-trees
#@ Eugene Inseok Chong;Jagannathan Srinivasan;Souripriya Das;Chuck Freiwald;Aravind Yalamanchi;Mahesh Jagannath;Anh-Tuan Tran;Ramkumar Krishnan;Richard Jiang
#t 2003
#c 5
#% 143796
#% 223781
#% 227861
#% 248814
#% 317933
#% 427218
#% 465011
#% 466944
#% 480458
#% 632099
#! Any auxiliary structure, such as a bitmap or a B+-tree index, that refers to rows of a table stored as a primary B+-tree (e.g., tables with clustered index in Microsoft SQL Server, or index-organized tables in Oracle) by their physical addresses would require updates due to inherent volatility of those addresses. To address this problem, we propose a mapping mechanism that 1) introduces a single mapping table, with each row holding one key value from the primary B+-tree, as an intermediate structure between the primary B+-tree and the associated auxiliary structures, and 2) augments the primary B+-tree structure to include in each row the physical address of the corresponding mapping table row. The mapping table row addresses can then be used in the auxiliary structures to indirectly refer to the primary B+-tree rows. The two key benefits are: 1) the mapping table shields the auxiliary structures from the volatility of the primary B+-tree row addresses, and 2) the method allows reuse of existing conventional table mechanisms for supporting auxiliary structures on primary B+- trees.This paper presents the mapping mechanism, its possible application in supporting various auxiliary structures on primary B+-trees, and a case study describing its use for supporting bitmap indexes on index-organized tables in Oracle9i. The case study demonstrates that the proposed mapping mechanism allows us to reuse existing bitmap index technologies with minimal changes. It also includes a comparison between bitmap and non-bitmap (B+- tree) index performance on index-organized tables for both single-table queries and star queries. The analytical and experimental studies show that the method is storage efficient, and (despite the mapping table overhead) provides performance benefits that are similar to those provided by bitmap indexes implemented on conventional tables.

#index 578572
#* XML schema
#@ Charles E. Campbell;Andrew Eisenberg;Jim Melton
#t 2003
#c 5

#index 578573
#* Time management for new faculty
#@ Anastassia Ailamaki;Johannes Gehrke
#t 2003
#c 5
#! In this article, we describe techniques for time management for new faculty members, covering a wide range of topics ranging from advice on scheduling meetings, email, to writing grant proposals and teaching.

#index 642523
#* Advances in Real-Time Database Systems Research: Special Section on RTDBS of ACM SIGMOD Record 25(1), March 1996.
#@ Azer Bestavros
#t 1996
#c 5
#! Abstract A Real-Time DataBase System (RTDBS) can be viewed as an amalgamation of a conventional DataBase Management System (DBMS) and a real-time system. Like a DBMS, it has to process transactions and guarantee ACID database properties. Furthermore, it has to operate in real-time, satisfying time constraints imposed on transaction commitments. A RTDBS may exist as a stand-alone system or as an embedded component in a larger multidatabase system. The publication in 1988 of a special issue of ACM SIGMOD Record on Real-Time DataBases signaled the birth of the RTDBS research area---an area that brings together researchers from both the database and real-time systems communities. Today, almost eight years later, I am pleased to present in this special section of ACM SIGMOD Record a review of recent advances in RTDBS research. There were 18 submissions to this special section, of which eight papers were selected for inclusion to provide the readers of ACM SIGMOD Record with an overview of current and future research directions within the RTDBS community. In this paper, I will summarize these directions and provide the reader with pointers to other publications for further information.

#index 654439
#* Proceedings of the 2003 ACM SIGMOD international conference on Management of data
#@ Zachary Ives;Yannis Papakonstantinou;Alon Halevy
#t 2003
#c 5
#! The 2003 ACM SIGMOD International Conference on Management of Data, was held June 10-12, 2003 in San Diego, California. This year, SIGMOD was part of the Federated Computing Research Conference (FCRC-2003), which included a total of 16 conferences and workshops. The SIGMOD conference is sponsored by the Association for Computing Machinery (ACM) and its Special Interest Group on Management of Data (SIGMOD). As has become customary, two days of the conference were overlapped with the Symposium on Principles of Database Systems (PODS). In addition, there were a number of important events co-located with SIGMOD this year, including the 6th International Workshop on the Web and Databases (WebDB), the Workshop on Research Issues in Data Mining and Knowledge Discovery (DMKD), The Second International Workshop on Distributed Event-Based Systems (MPDS), The Workshop on Management and Processing of Data Streams, the Paris C. Kanellakis Memorial Workshop, and the Second Database Faculty Symposium.As in previous years, SIGMOD held its status as a leading forum for exchange of ideas and publication of new results on data management. Acceptance into the conference proceedings was extremely competitive. We received 342 submissions to the conference (up from 240 in 2002, i.e., over 40% increase). The program committee selected 53 papers for presentation and inclusion in the proceedings. These papers span the range of traditional database topics such as query processing, indexing and OLAP, to current issues of signifcant interest such as XML data management, data streaming, large-scale data sharing, and metadata management. The program committee worked hard to select these papers through a detailed review process and active discussion both electronically and at the program committee meeting held in January in Seattle. We used the CMT to run the entire submissions and review process for research papers, including the program committee meeting.In addition to the research track, we had 73 submissions to the Demonstrations track, of which 22 projects were invited to present. The Demonstrations track has become a key venue for the early dissemination of cutting-edge prototype and systems development experience. Juliana Freire chaired the demos program committee, which put together a diverse and exciting program. The Industrial program committee, chaired by Surajit Chaudhuri, took an active role in soliciting quality submissions and have put together a program that is a key component of this year's conference: not only were there 4 papers accepted through the submission process, but there were an additional 10 invited talks. Likewise, Mary Fernandez and Joe Hellerstein organized an excellent slate of Tutorials and Panels to round out the program. As a result of all these efforts, we have an especially strong and innovative program this year, with a good balance of academic and industrial representation.

#index 654440
#* Improving the efficiency of database-system teaching
#@ Jeffrey D. Ullman
#t 2003
#c 5
#! The education industry has a very poor record of productivity gains. In this brief article, I outline some of the ways the teaching of a college course in database systems could be made more efficient, and staff time used more productively. These ideas carry over to other programming-oriented courses, and many of them apply to any academic subject whatsoever. After proposing a number of things that could be done, I concentrate here on a system under development, called OTC (On-line Testing Center), and on its methodology of "root questions." These questions encourage students to do homework of the long-answer type, yet we can have their work checked and graded automatically by a simple multiple-choice-question grader. OTC also offers some improvement in the way we handle SQL homework, and could be used with other languages as well.

#index 654441
#* Querying structured text in an XML database
#@ Shurug Al-Khalifa;Cong Yu;H. V. Jagadish
#t 2003
#c 5
#% 215225
#% 248801
#% 289335
#% 300170
#% 333951
#% 333981
#% 340914
#% 397375
#% 397378
#% 406493
#% 458829
#% 465155
#% 562456
#% 993936
#% 993953
#% 993985
#! XML databases often contain documents comprising structured text. Therefore, it is important to integrate "information retrieval style" query evaluation, which is well-suited for natural language text, with standard "database style" query evaluation, which handles structured queries efficiently. Relevance scoring is central to information retrieval. In the case of XML, this operation becomes more complex because the data required for scoring could reside not directly in an element itself but also in its descendant elements.In this paper, we propose a bulk-algebra, TIX, and describe how it can be used as a basis for integrating information retrieval techniques into a standard pipelined database query evaluation engine. We develop new evaluation strategies essential to obtaining good performance, including a stack-based TermJoin algorithm for efficiently scoring composite elements. We report results from an extensive experimental evaluation, which show, among other things, that the new TermJoin access method outperforms a direct implementation of the same functionality using standard operators by a large factor.

#index 654442
#* XRANK: ranked keyword search over XML documents
#@ Lin Guo;Feng Shao;Chavdar Botev;Jayavel Shanmugasundaram
#t 2003
#c 5
#% 186
#% 67565
#% 169817
#% 172922
#% 172927
#% 204662
#% 228097
#% 259985
#% 262069
#% 266553
#% 268079
#% 273897
#% 290830
#% 309726
#% 333854
#% 340886
#% 340914
#% 340919
#% 397366
#% 458829
#% 465155
#% 479782
#% 479803
#% 481439
#% 511343
#% 571098
#% 650962
#% 654442
#% 659990
#% 660011
#% 993987
#! We consider the problem of efficiently producing ranked results for keyword search queries over hyperlinked XML documents. Evaluating keyword search queries over hierarchical XML documents, as opposed to (conceptually) flat HTML documents, introduces many new challenges. First, XML keyword search queries do not always return entire documents, but can return deeply nested XML elements that contain the desired keywords. Second, the nested structure of XML implies that the notion of ranking is no longer at the granularity of a document, but at the granularity of an XML element. Finally, the notion of keyword proximity is more complex in the hierarchical XML data model. In this paper, we present the XRANK system that is designed to handle these novel features of XML keyword search. Our experimental results show that XRANK offers both space and performance benefits when compared with existing approaches. An interesting feature of XRANK is that it naturally generalizes a hyperlink based HTML search engine such as Google. XRANK can thus be used to query a mix of HTML and XML documents.

#index 654443
#* Distributed top-k monitoring
#@ Brian Babcock;Chris Olston
#t 2003
#c 5
#% 152911
#% 188026
#% 248812
#% 268793
#% 297915
#% 300179
#% 330769
#% 333854
#% 336610
#% 378388
#% 397353
#% 397443
#% 419918
#% 453464
#% 458535
#% 480628
#% 492912
#% 620720
#% 636014
#% 654488
#% 659993
#% 993949
#% 993960
#% 993998
#! The querying and analysis of data streams has been a topic of much recent interest, motivated by applications from the fields of networking, web usage analysis, sensor instrumentation, telecommunications, and others. Many of these applications involve monitoring answers to continuous queries over data streams produced at physically distributed locations, and most previous approaches require streams to be transmitted to a single location for centralized processing. Unfortunately, the continual transmission of a large number of rapid data streams to a central location can be impractical or expensive. We study a useful class of queries that continuously report the k largest values obtained from distributed data streams ("top-k monitoring queries"), which are of particular interest because they can be used to reduce the overhead incurred while running other types of monitoring queries. We show that transmitting entire data streams is unnecessary to support these queries and present an alternative approach that reduces communication significantly. In our approach, arithmetic constraints are maintained at remote stream sources to ensure that the most recently provided top-k answer remains valid to within a user-specified error tolerance. Distributed communication is only necessary on occasion, when constraints are violated, and we show empirically through extensive simulation on real-world data that our approach reduces overall communication cost by an order of magnitude compared with alternatives that o er the same error guarantees.

#index 654444
#* Approximate join processing over data streams
#@ Abhinandan Das;Johannes Gehrke;Mirek Riedewald
#t 2003
#c 5
#% 224218
#% 273908
#% 273911
#% 300179
#% 333931
#% 338425
#% 341700
#% 375017
#% 378388
#% 378408
#% 379445
#% 397353
#% 397385
#% 428155
#% 479984
#% 480628
#% 660004
#% 718437
#% 993948
#% 993949
#% 993999
#! We consider the problem of approximating sliding window joins over data streams in a data stream processing system with limited resources. In our model, we deal with resource constraints by shedding load in the form of dropping tuples from the data streams. We first discuss alternate architectural models for data stream join processing, and we survey suitable measures for the quality of an approximation of a set-valued query result. We then consider the number of generated result tuples as the quality measure, and we give optimal offline and fast online algorithms for it. In a thorough experimental study with synthetic and real data we show the efficacy of our solutions. For applications with demand for exact results we introduce a new Archive-metric which captures the amount of work needed to complete the join in case the streams are archived for later processing.

#index 654445
#* Spreadsheets in RDBMS for OLAP
#@ Andrew Witkowski;Srikanth Bellamkonda;Tolga Bozkaya;Gregory Dorman;Nathan Folkert;Abhinav Gupta;Lei Shen;Sankar Subramanian
#t 2003
#c 5
#% 13016
#% 86943
#% 86950
#% 123113
#% 152928
#% 152940
#% 172930
#% 397388
#% 427199
#% 477212
#% 479792
#% 480470
#% 481293
#% 482082
#% 654446
#% 1120943
#! One of the critical deficiencies of SQL is lack of support for n-dimensional array-based computations which are frequent in OLAP environments. Relational OLAP (ROLAP) applications have to emulate them using joins, recently introduced SQL Window Functions [18] and complex and inefficient CASE expressions. The designated place in SQL for specifying calculations is the SELECT clause, which is extremely limiting and forces the user to generate queries using nested views, subqueries and complex joins. Furthermore, SQL-query optimizer is pre-occupied with determining efficient join orders and choosing optimal access methods and largely disregards optimization of complex numerical formulas. Execution methods concentrated on efficient computation of a cube [11], [16] rather than on random access structures for inter-row calculations. This has created a gap that has been filled by spreadsheets and specialized MOLAP engines, which are good at formulas for mathematical modeling but lack the formalism of the relational model, are difficult to manage, and exhibit scalability problems. This paper presents SQL extensions involving array based calculations for complex modeling. In addition, we present optimizations, access structures and execution models for processing them efficiently.

#index 654446
#* QC-trees: an efficient summary structure for semantic OLAP
#@ Laks V. S. Lakshmanan;Jian Pei;Yan Zhao
#t 2003
#c 5
#% 152928
#% 198465
#% 210182
#% 227868
#% 227869
#% 227880
#% 227944
#% 236410
#% 259995
#% 273696
#% 273916
#% 280448
#% 287047
#% 340301
#% 397388
#% 459026
#% 464215
#% 479450
#% 480460
#% 480470
#% 480820
#% 481951
#% 993996
#! Recently, a technique called quotient cube was proposed as a summary structure for a data cube that preserves its semantics, with applications for online exploration and visualization. The authors showed that a quotient cube can be constructed very efficiently and it leads to a significant reduction in the cube size. While it is an interesting proposal, that paper leaves many issues unaddressed. Firstly, a direct representation of a quotient cube is not as compact as possible and thus still wastes space. Secondly, while a quotient cube can in principle be used for answering queries, no specific algorithms were given in the paper. Thirdly, maintaining any summary structure incrementally against updates is an important task, a topic not addressed there. In this paper, we propose an efficient data structure called QC-tree and an efficient algorithm for directly constructing it from a base table, solving the first problem. We give efficient algorithms that address the remaining questions. We report results from an extensive performance study that illustrate the space and time savings achieved by our algorithms over previous ones (wherever they exist).

#index 654447
#* Winnowing: local algorithms for document fingerprinting
#@ Saul Schleimer;Daniel S. Wilkerson;Alex Aiken
#t 2003
#c 5
#% 201935
#% 255137
#% 340141
#% 616528
#% 622212
#% 768815
#% 979316
#! Digital content is for copying: quotation, revision, plagiarism, and file sharing all create copies. Document fingerprinting is concerned with accurately identifying copying, including small partial copies, within large sets of documents.We introduce the class of local document fingerprinting algorithms, which seems to capture an essential property of any finger-printing technique guaranteed to detect copies. We prove a novel lower bound on the performance of any local algorithm. We also develop winnowing, an efficient local fingerprinting algorithm, and show that winnowing's performance is within 33% of the lower bound. Finally, we also give experimental results on Web data, and report experience with MOSS, a widely-used plagiarism detection service.

#index 654448
#* Information sharing across private databases
#@ Rakesh Agrawal;Alexandre Evfimievski;Ramakrishnan Srikant
#t 2003
#c 5
#% 31034
#% 67453
#% 151501
#% 152980
#% 169589
#% 232722
#% 249181
#% 269079
#% 271185
#% 287297
#% 287298
#% 300184
#% 301569
#% 301584
#% 325359
#% 338439
#% 350873
#% 406493
#% 482049
#% 545454
#% 557515
#% 577233
#% 577289
#% 577366
#% 593711
#% 593800
#% 993943
#% 993944
#% 993964
#% 1386192
#! Literature on information integration across databases tacitly assumes that the data in each database can be revealed to the other databases. However, there is an increasing need for sharing information across autonomous entities in such a way that no information apart from the answer to the query is revealed. We formalize the notion of minimal information sharing across private databases, and develop protocols for intersection, equijoin, intersection size, and equijoin size. We also show how new applications can be built using the proposed protocols.

#index 654449
#* Rights protection for relational data
#@ Radu Sion;Mikhail Atallah;Sunil Prabhakar
#t 2003
#c 5
#% 227956
#% 275836
#% 488310
#% 539745
#% 566390
#% 583804
#% 592726
#% 660348
#% 664665
#% 993944
#! Protecting rights over relational data is of ever increasing interest, especially considering areas where sensitive, valuable content is to be outsourced. A good example is a data mining application, where data is sold in pieces to parties specialized in mining it.Different avenues for rights protection are available, each with its own advantages and drawbacks. Enforcement by legal means is usually ineffective in preventing theft of copyrighted works, unless augmented by a digital counter-part, for example watermarking.Recent research of the authors introduces the issue of digital watermarking for generic number sets. In the present paper we expand on this foundation and introduce a solution for relational database content rights protection through watermarking.Our solution addresses important attacks, such as data re-sorting, subset selection, linear data changes (applying a linear transformation on arbitrary subsets of the data). Our watermark also survives up to 50% and above data loss.Finally we present wmdb.*, a proof-of-concept implementation of our algorithm and its application to real life data, namely in watermarking the outsourced Wal-Mart sales data that we have available at our institute.

#index 654450
#* ViST: a dynamic index method for querying XML data by tree structures
#@ Haixun Wang;Sanghyun Park;Wei Fan;Philip S. Yu
#t 2003
#c 5
#% 281149
#% 289010
#% 291299
#% 325384
#% 378412
#% 379483
#% 379484
#% 397359
#% 397360
#% 479465
#% 480489
#% 480656
#% 504578
#% 650962
#! With the growing importance of XML in data exchange, much research has been done in providing flexible query facilities to extract data from structured XML documents. In this paper, we propose ViST, a novel index structure for searching XML documents. By representing both XML documents and XML queries in structure-encoded sequences, we show that querying XML data is equivalent to finding subsequence matches. Unlike index methods that disassemble a query into multiple sub-queries, and then join the results of these sub-queries to provide the final answers, ViST uses tree structures as the basic unit of query to avoid expensive join operations. Furthermore, ViST provides a unified index on both content and structure of the XML documents, hence it has a performance advantage over methods indexing either just content or structure. ViST supports dynamic index update, and it relies solely on B+ Trees without using any specialized data structures that are not well supported by DBMSs. Our experiments show that ViST is effective, scalable, and efficient in supporting structural queries.

#index 654451
#* XPRESS: a queriable compression for XML data
#@ Jun-Ki Min;Myung-Jae Park;Chin-Wan Chung
#t 2003
#c 5
#% 193743
#% 243210
#% 300153
#% 309851
#% 397358
#% 397359
#% 397366
#% 462235
#% 479465
#% 480152
#% 480488
#% 659997
#% 993938
#! Like HTML, many XML documents are resident on native file systems. Since XML data is irregular and verbose, the disk space and the network bandwidth are wasted. To overcome the verbosity problem, the research on compressors for XML data has been conducted. However, some XML compressors do not support querying compressed data, while other XML compressors which support querying compressed data blindly encode tags and data values using predefined encoding methods. Thus, the query performance on compressed XML data is degraded.In this paper, we propose XPRESS, an XML compressor which supports direct and efficient evaluations of queries on compressed XML data. XPRESS adopts a novel encoding method, called reverse arithmetic encoding, which is intended for encoding label paths of XML data, and applies diverse encoding methods depending on the types of data values. Experimental results with real life data sets show that XPRESS achieves significant improvements on query performance for compressed XML data and reasonable compression ratios. On the average, the query performance of XPRESS is 2.83 times better than that of an existing XML compressor and the compression ratio of XPRESS is 73%.

#index 654452
#* D(k)-index: an adaptive structural summary for graph-structured data
#@ Qun Chen;Andrew Lim;Kian Win Ong
#t 2003
#c 5
#% 31484
#% 281149
#% 302617
#% 333981
#% 397359
#% 397360
#% 397379
#% 464724
#% 479465
#% 480489
#% 480656
#% 504578
#% 593696
#% 993951
#% 993970
#! To facilitate queries over semi-structured data, various structural summaries have been proposed. Structural summaries are derived directly from the data and serve as indices for evaluating path expressions on semi-structured or XML data. We introduce the D(k) index, an adaptive structural summary for general graph structured documents. Building on previous work, 1-index and A(k) index, the D(k)-index is also based on the concept of bisimilarity. However, as a generalization of the 1-index and A(k)-index, the D(k) index possesses the adaptive ability to adjust its structure according to the current query load. This dynamism also facilitates efficient update algorithms, which are crucial to practical applications of structural indices, but have not been adequately addressed in previous index proposals. Our experiments show that the D(k) index is a more effective structural summary than previous static ones, as a result of its query load sensitivity. In addition, update operations on the D(k) index can be performed more efficiently than on its predecessors.

#index 654453
#* Containment join size estimation: models and methods
#@ Wei Wang;Haifeng Jiang;Hongjun Lu;Jeffrey Xu Yu
#t 2003
#c 5
#% 58348
#% 137885
#% 210188
#% 210190
#% 214073
#% 243166
#% 268747
#% 273682
#% 277347
#% 299988
#% 300160
#% 333981
#% 397354
#% 397364
#% 397379
#% 458836
#% 458863
#% 464850
#% 465018
#% 480488
#% 480489
#% 481266
#% 527190
#% 541480
#% 588504
#% 650962
#% 659999
#! Recent years witnessed an increasing interest in researches in XML, partly due to the fact that XML has now become the de facto standard for data interchange over the internet. A large amount of work has been reported on XML storage models and query processing techniques. However, few works have addressed issues of XML query optimization. In this paper, we report our study on one of the challenges in XML query optimization: containment join size estimation. Containment join is well accepted as an important operation in XML query processing. Estimating the size of its results is no doubt essential to generate efficient XML query processing plans. We propose two models, the interval model and the position model, and a set of estimation methods based on these two models. Comprehensive performance studies were conducted. The results not only demonstrate the advantages of our new algorithms over existing algorithms, but also provide valuable insights into the tradeoff among various parameters.

#index 654454
#* Efficient processing of joins on set-valued attributes
#@ Nikos Mamoulis
#t 2003
#c 5
#% 115466
#% 152938
#% 188587
#% 227783
#% 249989
#% 300162
#% 380546
#% 427199
#% 435137
#% 443210
#% 480463
#% 480926
#% 481107
#% 482121
#! Object-oriented and object-relational DBMS support set valued attributes, which are a natural and concise way to model complex information. However, there has been limited research to-date on the evaluation of query operators that apply on sets. In this paper we study the join of two relations on their set-valued attributes. Various join types are considered, namely the set containment, set equality, and set overlap joins. We show that the inverted file, a powerful index for selection queries, can also facilitate the efficient evaluation of most join predicates. We propose join algorithms that utilize inverted files and compare them with signature-based methods for several set-comparison predicates.

#index 654455
#* Temporal coalescing with now granularity, and incomplete information
#@ Curtis E. Dyreson
#t 2003
#c 5
#% 43191
#% 84398
#% 123589
#% 225003
#% 225004
#% 238946
#% 259487
#% 287268
#% 287313
#% 319244
#% 333939
#% 361445
#% 390187
#% 442919
#% 443363
#% 480934
#% 481928
#% 527793
#% 527801
#% 565265
#% 665630
#! This paper presents a novel strategy for temporal coalescing. Temporal coalescing merges the temporal extents of value-equivalent tuples. A temporal extent is usually coalesced offline and stored since coalescing is an expensive operation. But the temporal extent of a tuple with now, times at different granularities, or incomplete times cannot be determined until query evaluation. This paper presents a strategy to partially coalesce temporal extents by identifying regions that are potentially covered. The covered regions can be used to evaluate temporal predicates and constructors on the coalesced extent. Our strategy uses standard relational database technology. We quantify the cost using the Oracle DBMS.

#index 654456
#* Warping indexes with envelope transforms for query by humming
#@ Yunyue Zhu;Dennis Shasha
#t 2003
#c 5
#% 86950
#% 172949
#% 194192
#% 227857
#% 227924
#% 232122
#% 248797
#% 261882
#% 261908
#% 286744
#% 316259
#% 316560
#% 333941
#% 341300
#% 397381
#% 451658
#% 460862
#% 462231
#% 480146
#% 481599
#% 481947
#% 534183
#% 555460
#% 577221
#% 631923
#% 993961
#% 993965
#! A Query by Humming system allows the user to find a song by humming part of the tune. No musical training is needed. Previous query by humming systems have not provided satisfactory results for various reasons. Some systems have low retrieval precision because they rely on melodic contour information from the hum tune, which in turn relies on the error-prone note segmentation process. Some systems yield better precision when matching the melody directly from audio, but they are slow because of their extensive use of Dynamic Time Warping (DTW). Our approach improves both the retrieval precision and speed compared to previous approaches. We treat music as a time series and exploit and improve well-developed techniques from time series databases to index the music for fast similarity queries. We improve on existing DTW indexes technique by introducing the concept of envelope transforms, which gives a general guideline for extending existing dimensionality reduction methods to DTW indexes. The net result is high scalability. We confirm our claims through extensive experiments.

#index 654457
#* Rondo: a programming platform for generic model management
#@ Sergey Melnik;Erhard Rahm;Philip A. Bernstein
#t 2003
#c 5
#% 11284
#% 166203
#% 248799
#% 278445
#% 281976
#% 328429
#% 458607
#% 458995
#% 480322
#% 488766
#% 527108
#% 562447
#% 572314
#% 578668
#% 660001
#% 993981
#% 1393677
#! Model management aims at reducing the amount of programming needed for the development of metadata-intensive applications. We present a first complete prototype of a generic model management system, in which high-level operators are used to manipulate models and mappings between models. We define the key conceptual structures: models, morphisms, and selectors, and describe their use and implementation. We specify the semantics of the known model-management operators applied to these structures, suggest new ones, and develop new algorithms for implementing the individual operators. We examine the solutions for two model-management tasks that involve manipulations of relational schemas, XML schemas, and SQL views.

#index 654458
#* On schema matching with opaque column names and data values
#@ Jaewoo Kang;Jeffrey F. Naughton
#t 2003
#c 5
#% 115608
#% 223567
#% 307632
#% 328429
#% 333986
#% 333988
#% 333990
#% 334025
#% 443408
#% 479783
#% 480134
#% 480468
#% 480645
#% 481280
#% 488766
#% 572314
#% 1650289
#! Most previous solutions to the schema matching problem rely in some fashion upon identifying "similar" column names in the schemas to be matched, or by recognizing common domains in the data stored in the schemas. While each of these approaches is valuable in many cases, they are not infallible, and there exist instances of the schema matching problem for which they do not even apply. Such problem instances typically arise when the column names in the schemas and the data in the columns are "opaque" or very difficult to interpret. In this paper we propose a two-step technique that works even in the presence of opaque column names and data values. In the first step, we measure the pair-wise attribute correlations in the tables to be matched and construct a dependency graph using mutual information as a measure of the dependency between attributes. In the second stage, we find matching node pairs in the dependency graphs by running a graph matching algorithm. We validate our approach with an experimental study, the results of which suggest that such an approach can be a useful addition to a set of (semi) automatic schema matching techniques.

#index 654459
#* Statistical schema matching across web query interfaces
#@ Bin He;Kevin Chen-Chuan Chang
#t 2003
#c 5
#% 22948
#% 55294
#% 248801
#% 262096
#% 333990
#% 410276
#% 480645
#% 481911
#% 572314
#! Schema matching is a critical problem for integrating heterogeneous information sources. Traditionally, the problem of matching multiple schemas has essentially relied on finding pairwise-attribute correspondence. This paper proposes a different approach, motivated by integrating large numbers of data sources on the Internet. On this "deep Web," we observe two distinguishing characteristics that offer a new view for considering schema matching: First, as the Web scales, there are ample sources that provide structured information in the same domains (e.g., books and automobiles). Second, while sources proliferate, their aggregate schema vocabulary tends to converge at a relatively small size. Motivated by these observations, we propose a new paradigm, statistical schema matching: Unlike traditional approaches using pairwise-attribute correspondence, we take a holistic approach to match all input schemas by finding an underlying generative schema model. We propose a general statistical framework MGS for such hidden model discovery, which consists of hypothesis modeling, generation, and selection. Further, we specialize the general framework to develop Algorithm MGSsd, targeting at synonym discovery, a canonical problem of schema matching, by designing and discovering a model that specifically captures synonym attributes. We demonstrate our approach over hundreds of real Web sources in four domains and the results show good accuracy.

#index 654460
#* Extended wavelets for multiple measures
#@ Antonios Deligiannakis;Nick Roussopoulos
#t 2003
#c 5
#% 1331
#% 214073
#% 248822
#% 257637
#% 259995
#% 273902
#% 273919
#% 333946
#% 397385
#% 397389
#% 479984
#% 480306
#% 480465
#% 480805
#% 482092
#% 504019
#! While work in recent years has demonstrated that wavelets can be efficiently used to compress large quantities of data and provide fast and fairly accurate answers to queries, little emphasis has been placed on using wavelets in approximating datasets containing multiple measures. Existing decomposition approaches will either operate on each measure individually, or treat all measures as a vector of values and process them simultaneously. We show in this paper that the resulting individual or combined storage approaches for the wavelet coefficients of different measures that stem from these existing algorithms may lead to suboptimal storage utilization, which results to reduced accuracy to queries. To alleviate this problem, we introduce in this work the notion of an extended wavelet coefficient as a flexible storage method for the wavelet coefficients, and propose novel algorithms for selecting which extended wavelet coefficients to retain under a given storage constraint. Experimental results with both real and synthetic datasets demonstrate that our approach achieves improved accuracy to queries when compared to existing techniques.

#index 654461
#* Spectral bloom filters
#@ Saar Cohen;Yossi Matias
#t 2003
#c 5
#% 137885
#% 163073
#% 210188
#% 248812
#% 256883
#% 293714
#% 319415
#% 322884
#% 340933
#% 378388
#% 379445
#% 446438
#% 479795
#% 479937
#% 654461
#% 993960
#! A Bloom Filter is a space-efficient randomized data structure allowing membership queries over sets with certain allowable errors. It is widely used in many applications which take advantage of its ability to compactly represent a set, and filter out effectively any element that does not belong to the set, with small error probability. This paper introduces the Spectral Bloom Filter (SBF), an extension of the original Bloom Filter to multi-sets, allowing the filtering of elements whose multiplicities are below a threshold given at query time. Using memory only slightly larger than that of the original Bloom Filter, the SBF supports queries on the multiplicities of individual keys with a guaranteed, small error probability. The SBF also supports insertions and deletions over the data set. We present novel methods for reducing the probability and magnitude of errors. We also present an efficient data structure and algorithms to build it incrementally and maintain it over streaming data, as well as over materialized data with arbitrary insertions and deletions. The SBF does not assume any a priori filtering threshold and effectively and efficiently maintains information over the entire data-set, allowing for ad-hoc queries with arbitrary parameters and enabling a range of new applications.

#index 654462
#* Chain: operator scheduling for memory minimization in data stream systems
#@ Brian Babcock;Shivnath Babu;Rajeev Motwani;Mayur Datar
#t 2003
#c 5
#% 116082
#% 160390
#% 188026
#% 248793
#% 248795
#% 259996
#% 259997
#% 264691
#% 273911
#% 300167
#% 310488
#% 340635
#% 378388
#% 397352
#% 397353
#% 428410
#% 462500
#% 480642
#% 480960
#% 481943
#% 577220
#% 993948
#% 993949
#% 994014
#! In many applications involving continuous data streams, data arrival is bursty and data rate fluctuates over time. Systems that seek to give rapid or real-time query responses in such an environment must be prepared to deal gracefully with bursts in data arrival without compromising system performance. We discuss one strategy for processing bursty streams --- adaptive, load-aware scheduling of query operators to minimize resource consumption during times of peak load. We show that the choice of an operator scheduling strategy can have significant impact on the run-time system memory usage. We then present Chain scheduling, an operator scheduling strategy for data stream systems that is near-optimal in minimizing run-time memory usage for any collection of single-stream queries involving selections, projections, and foreign-key joins with stored relations. Chain scheduling also performs well for queries with sliding-window joins over multiple streams, and multiple queries of the above types. A thorough experimental evaluation is provided where we demonstrate the potential benefits of Chain scheduling, compare it with competing scheduling strategies, and validate our analytical conclusions.

#index 654463
#* Processing set expressions over continuous update streams
#@ Sumit Ganguly;Minos Garofalakis;Rajeev Rastogi
#t 2003
#c 5
#% 2833
#% 70370
#% 123589
#% 125830
#% 214073
#% 238182
#% 243166
#% 249238
#% 273682
#% 282505
#% 299984
#% 299989
#% 336610
#% 397354
#% 480805
#% 481749
#% 519953
#% 593957
#% 594029
#% 993969
#! There is growing interest in algorithms for processing and querying continuous data streams (i.e., data that is seen only once in a fixed order) with limited memory resources. In its most general form, a data stream is actually an update stream, i.e., comprising data-item deletions as well as insertions. Such massive update streams arise naturally in several application domains (e.g., monitoring of large IP network installations, or processing of retail-chain transactions).Estimating the cardinality of set expressions defined over several (perhaps, distributed) update streams is perhaps one of the most fundamental query classes of interest; as an example, such a query may ask "what is the number of distinct IP source addresses seen in passing packets from both router R1 and R2 but not router R3?". Earlier work has only addressed very restricted forms of this problem, focusing solely on the special case of insert-only streams and specific operators (e.g., union). In this paper, we propose the first space-efficient algorithmic solution for estimating the cardinality of full-fledged set expressions over general update streams. Our estimation algorithms are probabilistic in nature and rely on a novel, hash-based synopsis data structure, termed "2-level hash sketch". We demonstrate how our 2-level hash sketch synopses can be used to provide low-error, high-confidence estimates for the cardinality of set expressions (including operators such as set union, intersection, and difference) over continuous update streams, using only small space and small processing time per update. Furthermore, our estimators never require rescanning or resampling of past stream items, regardless of the number of deletions in the stream. We also present lower bounds for the problem, demonstrating that the space usage of our estimation algorithms is within small factors of the optimal. Preliminary experimental results verify the effectiveness of our approach.

#index 654464
#* Capturing both types and constraints in data integration
#@ Michael Benedikt;Chee-Yong Chan;Wenfei Fan;Juliana Freire;Rajeev Rastogi
#t 2003
#c 5
#% 248799
#% 274160
#% 294600
#% 299944
#% 330305
#% 333935
#% 344425
#% 378409
#% 378411
#% 408396
#% 462051
#% 465057
#% 479452
#% 479783
#% 480657
#% 572305
#% 572314
#% 993981
#% 994001
#! We propose a framework for integrating data from multiple relational sources into an XML document that both conforms to a given DTD and satisfies predefined XML constraints. The framework is based on a specification language, AIG, that extends a DTD by (1) associating element types with semantic attributes (inherited and synthesized, inspired by the corresponding notions from Attribute Grammars), (2) computing these attributes via parameterized SQL queries over multiple data sources, and (3) incorporating XML keys and inclusion constraints. The novelty of AIG consists in semantic attributes and their dependency relations for controlling context-dependent, DTD-directed construction of XML documents, as well as for checking XML constraints in parallel with document-generation. We also present cost-based optimization techniques for efficiently evaluating AIGs, including algorithms for merging queries and for scheduling queries on multiple data sources. This provides a new grammar-based approach for data integration under both syntactic and semantic constraints.

#index 654465
#* Exchanging intensional XML data
#@ Tova Milo;Serge Abiteboul;Bernd Amann;Omar Benjelloun;Fred Dang Ngoc
#t 2003
#c 5
#% 83933
#% 101947
#% 201928
#% 229827
#% 248799
#% 299942
#% 333844
#% 333990
#% 393907
#% 404772
#% 461901
#% 464706
#% 482111
#% 504575
#% 654485
#% 994034
#! XML is becoming the universal format for data exchange between applications. Recently, the emergence of Web services as standard means of publishing and accessing data on the Web introduced a new class of XML documents, which we call intensional documents. These are XML documents where some of the data is given explicitly while other parts are defined only intensionally by means of embedded calls to Web services.When such documents are exchanged between applications, one has the choice to materialize the intensional data (i.e. to invoke the embedded calls) or not, before the document is sent. This choice may be influenced by various parameters, such as performance and security considerations. This paper addresses the problem of guiding this materialization process.We argue that, just like for regular XML data, schemas (ala DTD and XML Schema) may be used to control the exchange of intensional data and, in particular, to determine which data should be materialized before sending a document, and which should not. We formalize the problem and provide algorithms to solve it. We also present an implementation that complies with real life standards for XML data, schemas, and Web services, and is used in the Active XML system [3, 1].

#index 654466
#* Efficient similarity search and classification via rank aggregation
#@ Ronald Fagin;Ravi Kumar;D. Sivakumar
#t 2003
#c 5
#% 232764
#% 249321
#% 278831
#% 330769
#% 333854
#% 347225
#% 397387
#% 397608
#% 415033
#% 464195
#% 479973
#% 480304
#% 481956
#% 494173
#% 994013
#! We propose a novel approach to performing efficient similarity search and classification in high dimensional data. In this framework, the database elements are vectors in a Euclidean space. Given a query vector in the same space, the goal is to find elements of the database that are similar to the query. In our approach, a small number of independent "voters" rank the database elements based on similarity to the query. These rankings are then combined by a highly efficient aggregation algorithm. Our methodology leads both to techniques for computing approximate nearest neighbors and to a conceptually rich alternative to nearest neighbors.One instantiation of our methodology is as follows. Each voter projects all the vectors (database elements and the query) on a random line (different for each voter), and ranks the database elements based on the proximity of the projections to the projection of the query. The aggregation rule picks the database element that has the best median rank. This combination has several appealing features. On the theoretical side, we prove that with high probability, it produces a result that is a (1 + ε) factor approximation to the Euclidean nearest neighbor. On the practical side, it turns out to be extremely efficient, often exploring no more than 5% of the data to obtain very high-quality results. This method is also database-friendly, in that it accesses data primarily in a pre-defined order without random accesses, and, unlike other methods for approximate nearest neighbors, requires almost no extra storage. Also, we extend our approach to deal with the k nearest neighbors.We conduct two sets of experiments to evaluate the efficacy of our methods. Our experiments include two scenarios where nearest neighbors are typically employed---similarity search and classification problems. In both cases, we study the performance of our methods with respect to several evaluation criteria, and conclude that they are uniformly excellent, both in terms of quality of results and in terms of efficiency.

#index 654467
#* Robust and efficient fuzzy match for online data cleaning
#@ Surajit Chaudhuri;Kris Ganjam;Venkatesh Ganti;Rajeev Motwani
#t 2003
#c 5
#% 190611
#% 201889
#% 243166
#% 248801
#% 249321
#% 252304
#% 282323
#% 314740
#% 387427
#% 479462
#% 480654
#% 572265
#% 577238
#% 616528
#% 993980
#! To ensure high data quality, data warehouses must validate and cleanse incoming data tuples from external sources. In many situations, clean tuples must match acceptable tuples in reference tables. For example, product name and description fields in a sales record from a distributor must match the pre-recorded name and description fields in a product reference relation.A significant challenge in such a scenario is to implement an efficient and accurate fuzzy match operation that can effectively clean an incoming tuple if it fails to match exactly with any tuple in the reference relation. In this paper, we propose a new similarity function which overcomes limitations of commonly used similarity functions, and develop an efficient fuzzy match algorithm. We demonstrate the effectiveness of our techniques by evaluating them on real datasets.

#index 654468
#* Mapping data in peer-to-peer systems: semantics and algorithmic issues
#@ Anastasios Kementsietsidis;Marcelo Arenas;Renée J. Miller
#t 2003
#c 5
#% 188077
#% 273914
#% 340175
#% 340176
#% 345086
#% 378409
#% 384978
#% 442692
#% 465159
#% 480647
#% 496291
#% 578668
#% 993981

#index 654469
#* Extracting structured data from Web pages
#@ Arvind Arasu;Hector Garcia-Molina
#t 2003
#c 5
#% 229827
#% 271065
#% 275915
#% 300157
#% 330784
#% 384978
#% 397605
#% 464717
#% 479452
#% 480824
#% 481923
#% 504443
#% 542161
#% 632051
#! Many web sites contain large sets of pages generated using a common template or layout. For example, Amazon lays out the author, title, comments, etc. in the same way in all its book pages. The values used to generate the pages (e.g., the author, title,...) typically come from a database. In this paper, we study the problem of automatically extracting the database values from such template-generated web pages without any learning examples or other similar human input. We formally define a template, and propose a model that describes how values are encoded into pages using a template. We present an algorithm that takes, as input, a set of template-generated pages, deduces the unknown template used to generate the pages, and extracts, as output, the values encoded in the pages. Experimental evaluation on a large number of real input page collections indicates that our algorithm correctly extracts data in most cases.

#index 654470
#* Scientific data repositories: designing for a moving target
#@ Etzard Stolte;Christoph von Praun;Gustavo Alonso;Thomas Gross
#t 2003
#c 5
#% 201980
#% 223774
#% 300171
#% 300185
#% 411251
#% 458837
#% 480140
#% 480295
#% 570889
#% 617861
#% 993983
#! Managing scientific data warehouses requires constant adaptations to cope with changes in processing algorithms, computing environments, database schemas, and usage patterns. We have faced this challenge in the RHESSI Experimental Data Center (HEDC), a datacenter for the RHESSI NASA spacecraft. In this paper we describe our experience in developing HEDC and discuss in detail the design choices made. To successfully accommodate typical adaptations encountered in scientific data management systems, HEDC (i) clearly separates generic from domain specific code in all tiers, (ii) uses a file system for the actual data in combination with a DBMS to manage the corresponding meta data, and (iii) revolves around a middle tier designed to scale if more browsing or processing power is required. These design choices are valuable contributions as they address common concerns in a wide range of scientific data management systems.

#index 654471
#* Factorizing complex predicates in queries to exploit indexes
#@ Surajit Chaudhuri;Prasanna Ganesan;Sunita Sarawagi
#t 2003
#c 5
#% 77648
#% 116090
#% 136740
#% 152940
#% 172931
#% 210172
#% 277346
#% 287461
#% 288449
#% 303968
#% 320217
#% 411554
#% 459014
#% 480091
#% 480944
#% 481771
#% 659966
#% 768808
#! Decision-support applications generate queries with complex predicates. We show how the factorization of complex query expressions exposes significant opportunities for exploiting available indexes. We also present a novel idea of relaxing predicates in a complex condition to create possibilities for factoring. Our algorithms are designed for easy integration with existing query optimizers and support multiple optimization levels, providing different trade-offs between plan complexity and optimization time.

#index 654472
#* Estimating compilation time of a query optimizer
#@ Ihab F. Ilyas;Jun Rao;Guy Lohman;Dengfeng Gao;Eileen Lin
#t 2003
#c 5
#% 32889
#% 43162
#% 83154
#% 102765
#% 116040
#% 167302
#% 188719
#% 210169
#% 220425
#% 248793
#% 248855
#% 287461
#% 300196
#% 397393
#% 397397
#% 411554
#% 464558
#% 480158
#% 565457
#! A query optimizer compares alternative plans in its search space to find the best plan for a given query. Depending on the search space and the enumeration algorithm, optimizers vary in their compilation time and the quality of the execution plan they can generate. This paper describes a compilation time estimator that provides a quantified estimate of the optimizer compilation time for a given query. Such an estimator is useful for automatically choosing the right level of optimization in commercial database systems. In addition, compilation time estimates can be quite helpful for mid-query reoptimization, for monitoring the progress of workload analysis tools where a large number queries need to be compiled (but not executed), and for judicious design and tuning of an optimizer.Previous attempts to estimate optimizer compilation complexity used the number of possible binary joins as the metric and overlooked the fact that each join often translates into a different number of join plans because of the presence of "physical" properties. We use the number of plans (instead of joins) to estimate query compilation time, and employ two novel ideas: (1) reusing an optimizer's join enumerator to obtain actual number of joins, but bypassing plan generation to save estimation overhead; (2) maintaining a small number of "interesting" properties to facilitate plan counting. We prototyped our approach in a commercial database system and our experimental results show that we can achieve good compilation time estimates (less than 30% error, on average) for complex real queries, using a small fraction (within 3%) of the actual compilation time.

#index 654473
#* A characterization of the sensitivity of query optimization to storage access cost parameters
#@ Frederick R. Reiss;Tapas Kanungo
#t 2003
#c 5
#% 2115
#% 36160
#% 116040
#% 159079
#% 172900
#% 172902
#% 201692
#% 211087
#% 273694
#% 318049
#% 378414
#% 411554
#% 443390
#% 479786
#% 480803
#% 480955
#% 481593
#% 571091
#% 674157
#% 978826
#% 993933
#% 993945
#! Most relational query optimizers make use of information about the costs of accessing tuples and data structures on various storage devices. This information can at times be off by several orders of magnitude due to human error in configuration setup, sudden changes in load, or hardware failure. In this paper, we attempt to answer the following questions:• Are inaccurate access cost estimates likely to cause a typical query optimizer to choose a suboptimal query plan?• If an optimizer chooses a suboptimal plan as a result of inaccurate access cost estimates, how far from optimal is this plan likely to be?To address these issues, we provide a theoretical, vector-based framework for analyzing the costs of query plans under various storage parameter costs. We then use this geometric framework to characterize experimentally a commercial query optimizer. We develop algorithms for extracting detailed information about query plans through narrow optimizer interfaces, and we perform the characterization using database statistics from a published run of the TPC-H benchmark and a wide range of storage parameters.We show that, when data structures such as tables, indexes, and sorted runs reside on different storage devices, the optimizer can derive significant benefits from having accurate and timely information regarding the cost of accessing storage devices.

#index 654474
#* A theory of redo recovery
#@ David Lomet;Mark Tuttle
#t 2003
#c 5
#% 117
#% 713
#% 9241
#% 114582
#% 245790
#% 273892
#% 317988
#% 403195
#% 464523
#% 464823
#% 481624
#% 531907
#! Our goal is to understand redo recovery. We define an installation graph of operations in an execution, an ordering significantly weaker than conflict ordering from concurrency control. The installation graph explains recoverable system state in terms of which operations are considered installed. This explanation and the set of operations replayed during recovery form an invariant that is the contract between normal operation and recovery. It prescribes how to coordinate changes to system components such as the state, the log, and the cache. We also describe how widely used recovery techniques are modeled in our theory, and why they succeed in providing redo recovery.

#index 654475
#* Formal semantics and analysis of object queries
#@ G. M. Bierman
#t 2003
#c 5
#% 135428
#% 172934
#% 228000
#% 242186
#% 249985
#% 261370
#% 287092
#% 292231
#% 294600
#% 294618
#% 320793
#% 335725
#% 340698
#% 343164
#% 383394
#% 481919
#% 489541
#% 562135
#% 646741
#% 737772
#% 750967
#! Modern database systems provide not only powerful data models but also complex query languages supporting powerful features such as the ability to create new database objects and invocation of arbitrary methods (possibly written in a third-party programming language).In this sense query languages have evolved into powerful programming languages. Surprisingly little work exists utilizing techniques from programming language research to specify and analyse these query languages. This paper provides a formal, high-level operational semantics for a complex-value OQL-like query language that can create fresh database objects, and invoke external methods. We define a type system for our query language and prove an important soundness property.We define a simple effect typing discipline to delimit the computational effects within our queries. We prove that this effect system is correct and show how it can be used to detect cases of non-determinism and to define correct query optimizations.

#index 654476
#* Stream processing of XPath queries with predicates
#@ Ashish Kumar Gupta;Dan Suciu
#t 2003
#c 5
#% 132396
#% 187659
#% 205614
#% 282470
#% 289287
#% 289335
#% 300179
#% 321327
#% 333982
#% 342372
#% 382963
#% 404772
#% 465061
#% 480296
#% 503364
#% 659987
#% 659995
#% 659996
#% 993939
#! We consider the problem of evaluating large numbers of XPath filters, each with many predicates, on a stream of XML documents. The solution we propose is to lazily construct a single deterministic pushdown automata, called the XPush Machine from the given XPath fllters. We describe a number of optimization techniques to make the lazy XPush machine more efficient, both in terms of space and time. The combination of these optimizations results in high, sustained throughput. For example, if the total number of atomic predicates in the filters is up to 200000, then the throughput is at least 0.5 MB/sec: it increases to 4.5 MB/sec when each fllter contains a single predicate.

#index 654477
#* XPath queries on streaming data
#@ Feng Peng;Sudarshan S. Chawathe
#t 2003
#c 5
#% 289287
#% 299942
#% 300179
#% 378388
#% 378392
#% 458847
#% 465061
#% 480296
#% 487257
#% 659987
#% 659995
#% 993950
#! We present the design and implementation of the XSQ system for querying streaming XML data using XPath 1.0. Using a clean design based on a hierarchical arrangement of pushdown transducers augmented with buffers, XSQ supports features such as multiple predicates, closures, and aggregation. XSQ not only provides high throughput, but is also memory efficient: It buffers only data that must be buffered by any streaming XPath processor. We also present an empirical study of the performance characteristics of XPath features, as embodied by XSQ and several other systems.

#index 654478
#* Location-based spatial queries
#@ Jun Zhang;Manli Zhu;Dimitris Papadias;Yufei Tao;Dik Lun Lee
#t 2003
#c 5
#% 86950
#% 103743
#% 121114
#% 201876
#% 237187
#% 252304
#% 273887
#% 287466
#% 318703
#% 397377
#% 443327
#% 479649
#% 495433
#% 993955
#! In this paper we propose an approach that enables mobile clients to determine the validity of previous queries based on their current locations. In order to make this possible, the server returns in addition to the query result, a validity region around the client's location within which the result remains the same. We focus on two of the most common spatial query types, namely nearest neighbor and window queries, define the validity region in each case and propose the corresponding query processing algorithms. In addition, we provide analytical models for estimating the expected size of the validity region. Our techniques can significantly reduce the number of queries issued to the server, while introducing minimal computational and network overhead compared to traditional spatial queries.

#index 654479
#* Hardware acceleration for spatial selections and joins
#@ Chengyu Sun;Divyakant Agrawal;Amr El Abbadi
#t 2003
#c 5
#% 119597
#% 172908
#% 235114
#% 260653
#% 279852
#% 287379
#% 324364
#% 397396
#% 427199
#% 479653
#% 527193
#% 527197
#! Spatial database operations are typically performed in two steps. In the filtering step, indexes and the minimum bounding rectangles (MBRs) of the objects are used to quickly determine a set of candidate objects, and in the refinement step, the actual geometries of the objects are retrieved and compared to the query geometry or each other. Because of the complexity of the computational geometry algorithms involved, the CPU cost of the refinement step is usually the dominant cost of the operation for complex geometries such as polygons. In this paper, we propose a novel approach to address this problem using efficient rendering and searching capabilities of modern graphics hardware. This approach does not require expensive pre-processing of the data or changes to existing storage and index structures, and it applies to both intersection and distance predicates. Our experiments with real world datasets show that by combining hardware and software methods, the overall computational cost can be reduced substantially for both spatial selections and joins.

#index 654480
#* An optimal and progressive algorithm for skyline queries
#@ Dimitris Papadias;Yufei Tao;Greg Fu;Bernhard Seeger
#t 2003
#c 5
#% 2115
#% 62323
#% 86950
#% 100803
#% 201876
#% 248010
#% 287466
#% 288976
#% 300180
#% 333951
#% 438135
#% 443327
#% 480671
#% 480819
#% 993954
#! The skyline of a set of d-dimensional points contains the points that are not dominated by any other point on all dimensions. Skyline computation has recently received considerable attention in the database community, especially for progressive (or online) algorithms that can quickly return the first skyline points without having to read the entire data file. Currently, the most efficient algorithm is NN (nearest neighbors), which applies the divide -and-conquer framework on datasets indexed by R-trees. Although NN has some desirable features (such as high speed for returning the initial skyline points, applicability to arbitrary data distributions and dimensions), it also presents several inherent disadvantages (need for duplicate elimination if d2, multiple accesses of the same node, large space overhead). In this paper we develop BBS (branch-and-bound skyline), a progressive algorithm also based on nearest neighbor search, which is IO optimal, i.e., it performs a single access only to those R-tree nodes that may contain skyline points. Furthermore, it does not retrieve duplicates and its space overhead is significantly smaller than that of NN. Finally, BBS is simple to implement and can be efficiently applied to a variety of alternative skyline queries. An analytical and experimental comparison shows that BBS outperforms NN (usually by orders of magnitude) under all problem instances.

#index 654481
#* Contorting high dimensional data for efficient main memory KNN processing
#@ Bin Cui;Beng Chin Ooi;Jianwen Su;Kian-Lee Tan
#t 2003
#c 5
#% 227937
#% 300194
#% 333940
#% 333942
#% 333949
#% 342828
#% 435141
#% 479462
#% 479649
#% 480307
#% 480632
#! In this paper, we present a novel index structure, called Δ-tree, to speed up processing of high-dimensional K-nearest neighbor (KNN) queries in main memory environment. The Δ-tree is a multi-level structure where each level represents the data space at different dimensionalities: the number of dimensions increases towards the leaf level which contains the data at their full dimensions. The remaining dimensions are obtained using Principal Component Analysis, which has the desirable property that the first few dimensions capture most of the information in the dataset. Each level of the tree serves to prune the search space more efficiently as the reduced dimensions can better exploit the small cache line size. Moreover, the distance computation on lower dimensionality is less expensive. We also propose an extension, called Δ+-tree, that globally clusters the data space and then further partitions clusters into small regions to reduce the search space. We conducted extensive experiments to evaluate the proposed structures against existing techniques on different kinds of datasets. Our results show that the Δ+-tree is superior in most cases.

#index 654482
#* The design of an acquisitional query processor for sensor networks
#@ Samuel Madden;Michael J. Franklin;Joseph M. Hellerstein;Wei Hong
#t 2003
#c 5
#% 554
#% 111368
#% 152954
#% 249985
#% 273911
#% 274143
#% 297915
#% 300167
#% 300179
#% 309433
#% 330305
#% 330413
#% 333926
#% 333953
#% 336865
#% 339223
#% 397353
#% 397355
#% 401228
#% 427022
#% 443019
#% 443298
#% 479938
#% 480810
#% 480965
#% 481448
#% 571048
#% 572308
#% 636008
#% 805466
#% 805467
#% 993949
#% 1394366
#! We discuss the design of an acquisitional query processor for data collection in sensor networks. Acquisitional issues are those that pertain to where, when, and how often data is physically acquired (sampled) and delivered to query processing operators. By focusing on the locations and costs of acquiring data, we are able to significantly reduce power consumption over traditional passive systems that assume the a priori existence of data. We discuss simple extensions to SQL for controlling data acquisition, and show how acquisitional issues influence query optimization, dissemination, and execution. We evaluate these issues in the context of TinyDB, a distributed query processor for smart sensor devices, and show how acquisitional techniques can provide significant reductions in power consumption on our sensor devices.

#index 654483
#* Cache-and-query for wide area sensor databases
#@ Amol Deshpande;Suman Nath;Phillip B. Gibbons;Srinivasan Seshan
#t 2003
#c 5
#% 45076
#% 77005
#% 100593
#% 102804
#% 111368
#% 136740
#% 189430
#% 210179
#% 232771
#% 239969
#% 397351
#% 397355
#% 452756
#% 461902
#% 479629
#% 496291
#% 567011
#% 636008
#% 665535
#% 805466
#! Webcams, microphones, pressure gauges and other sensors provide exciting new opportunities for querying and monitoring the physical world. In this paper we focus on querying wide area sensor databases, containing (XML) data derived from sensors spread over tens to thousands of miles. We present the first scalable system for executing XPATH queries on such databases. The system maintains the logical view of the data as a single XML document, while physically the data is fragmented across any number of host nodes. For scalability, sensor data is stored close to the sensors, but can be cached elsewhere as dictated by the queries. Our design enables self starting distributed queries that jump directly to the lowest common ancestor of the query result, dramatically reducing query response times. We present a novel query-evaluate gather technique (using XSLT) for detecting (1) which data in a local database fragment is part of the query result, and (2) how to gather the missing parts. We define partitioning and cache invariants that ensure that even partial matches on cached data are exploited and that correct answers are returned, despite our dynamic query-driven caching. Experimental results demonstrate that our techniques dramatically increase query throughputs and decrease query response times in wide area sensor databases.

#index 654484
#* Composing XSL transformations with XML publishing views
#@ Chengkai Li;Philip Bohannon;P. P. S. Narayan
#t 2003
#c 5
#% 287005
#% 309851
#% 333935
#% 348183
#% 480657
#% 572305
#% 993940
#% 993941
#! While the XML Stylesheet Language for Transformations (XSLT) was not designed as a query language, it is well-suited for many query-like operations on XML documents including selecting and restructuring data. Further, it actively fulfills the role of an XML query language in modern applications and is widely supported by application platform software. However, the use of database techniques to optimize and execute XSLT has only recently received attention in the research community. In this paper, we focus on the case where XSL transformations are to be run on XML documents defined as views of relational databases. For a subset of XSLT, we present an algorithm to compose a transformation with an XML view, eliminating the need for the XSLT execution. We then describe how to extend this algorithm to handle several additional features of XSLT, including a proposed approach for handling recursion.

#index 654485
#* Dynamic XML documents with distribution and replication
#@ Serge Abiteboul;Angela Bonifati;Grégory Cobéna;Ioana Manolescu;Tova Milo
#t 2003
#c 5
#% 121
#% 38688
#% 210177
#% 264263
#% 330305
#% 333979
#% 340949
#% 397351
#% 397364
#% 480488
#% 480647
#% 480822
#% 481935
#% 487257
#% 654465
#% 993939
#% 994034
#! The advent of XML as a universal exchange format, and of Web services as a basis for distributed computing, has fostered the apparition of a new class of documents: dynamic XML documents. These are XML documents where some data is given explicitly while other parts are given only intensionally by means of embedded calls to web services that can be called to generate the required information. By the sole presence of Web services, dynamic documents already include inherently some form of distributed computation. A higher level of distribution that also allows (fragments of) dynamic documents to be distributed and/or replicated over several sites is highly desirable in today's Web architecture, and in fact is also relevant for regular (non dynamic) documents.The goal of this paper is to study new issues raised by the distribution and replication of dynamic XML data. Our study has originated in the context of the Active XML system [1, 3, 22] but the results are applicable to many other systems supporting dynamic XML data. Starting from a data model and a query language, we describe a complete framework for distributed and replicated dynamic XML documents. We provide a comprehensive cost model for query evaluation and show how it applies to user queries and service calls. Finally, we describe an algorithm that, for a given peer, chooses data and services that the peer should replicate to improve the efficiency of maintaining and querying its dynamic data.

#index 654486
#* Dynamic sample selection for approximate query processing
#@ Brian Babcock;Surajit Chaudhuri;Gautam Das
#t 2003
#c 5
#% 1331
#% 210182
#% 227883
#% 273908
#% 273909
#% 273911
#% 300195
#% 333955
#% 397372
#% 397390
#% 420053
#% 462204
#% 464706
#% 465162
#% 479804
#% 479984
#% 480158
#% 480306
#% 480465
#% 480471
#% 482100
#% 482123
#! In decision support applications, the ability to provide fast approximate answers to aggregation queries is desirable. One commonly-used technique for approximate query answering is sampling. For many aggregation queries, appropriately constructed biased (non-uniform) samples can provide more accurate approximations than a uniform sample. The optimal type of bias, however, varies from query to query. In this paper, we describe an approximate query processing technique that dynamically constructs an appropriately biased sample for each query by combining samples selected from a family of non-uniform samples that are constructed during a pre-processing phase. We show that dynamic selection of appropriate portions of previously constructed samples can provide more accurate approximate answers than static, non-adaptive usage of uniform or non-uniform samples.

#index 654487
#* Evaluating probabilistic queries over imprecise data
#@ Reynold Cheng;Dmitri V. Kalashnikov;Sunil Prabhakar
#t 2003
#c 5
#% 188641
#% 248812
#% 273909
#% 295512
#% 333863
#% 333969
#% 335047
#% 397355
#% 442615
#% 480332
#% 840577
#! Many applications employ sensors for monitoring entities such as temperature and wind speed. A centralized database tracks these entities to enable query processing. Due to continuous changes in these values and limited resources (e.g., network bandwidth and battery power), it is often infeasible to store the exact values at all times. A similar situation exists for moving object environments that track the constantly changing locations of objects. In this environment, it is possible for database queries to produce incorrect or invalid results based upon old data. However, if the degree of error (or uncertainty) between the actual value and the database value is controlled, one can place more confidence in the answers to queries. More generally, query answers can be augmented with probabilistic estimates of the validity of the answers. In this paper we study probabilistic query evaluation based upon uncertain data. A classification of queries is made based upon the nature of the result set. For each class, we develop algorithms for computing probabilistic answers. We address the important issue of measuring the quality of the answers to these queries, and provide algorithms for efficiently pulling data from relevant sensors or moving objects in order to improve the quality of the executing queries. Extensive experiments are performed to examine the effectiveness of several data update policies.

#index 654488
#* Adaptive filters for continuous queries over distributed data streams
#@ Chris Olston;Jing Jiang;Jennifer Widom
#t 2003
#c 5
#% 91210
#% 152911
#% 188026
#% 247117
#% 268793
#% 281557
#% 297915
#% 300179
#% 320187
#% 333969
#% 340635
#% 397353
#% 397355
#% 419918
#% 428155
#% 443298
#% 458535
#% 458601
#% 480332
#% 566135
#% 620720
#% 636014
#% 660004
#% 993949
#! We consider an environment where distributed data sources continuously stream updates to a centralized processor that monitors continuous queries over the distributed data. Significant communication overhead is incurred in the presence of rapid update streams, and we propose a new technique for reducing the overhead. Users register continuous queries with precision requirements at the central stream processor, which installs filters at remote data sources. The filters adapt to changing conditions to minimize stream rates while guaranteeing that all continuous queries still receive the updates necessary to provide answers of adequate precision at all times. Our approach enables applications to trade precision for communication overhead at a fine granularity by individually adjusting the precision constraints of continuous queries over streams in a multi-query workload. Through experiments performed on synthetic data simulations and a real network monitoring implementation, we demonstrate the effectiveness of our approach in achieving low communication overhead compared with alternate approaches.

#index 654489
#* A framework for diagnosing changes in evolving data streams
#@ Charu C. Aggarwal
#t 2003
#c 5
#% 227859
#% 273693
#% 297183
#% 308435
#% 310488
#% 310500
#% 345857
#% 464204
#% 481290
#% 481931
#% 527177
#% 630974
#% 632056
#% 632090
#! In recent years, the progress in hardware technology has made it possible for organizations to store and record large streams of transactional data. This results in databases which grow without limit at a rapid rate. This data can often show important changes in trends over time. In such cases, it is useful to understand, visualize and diagnose the evolution of these trends. When the data streams are fast and continuous, it becomes important to analyze and predict the trends quickly in online fashion. In this paper, we discuss the concept of velocity density estimation, a technique used to understand, visualize and determine trends in the evolution of fast data streams. We show how to use velocity density estimation in order to create both temporal velocity profiles and spatial velocity profiles at periodic instants in time. These profiles are then used in order to predict three kinds of data evolution: dissolution, coagulation and shift. Methods are proposed to visualize the changing data trends in a single online scan of the data stream, and a computational requirement which is linear in the number of data points. In addition, batch processing techniques are proposed in order to identify combinations of dimensions which show the greatest amount of global evolution. The techniques discussed in this paper can be easily extended to spatio-temporal data, changes in data snapshots at fixed instances in time, or any other data which has a temporal component during its evolution.

#index 654490
#* Using sets of feature vectors for similarity search on voxelized CAD objects
#@ Hans-Peter Kriegel;Stefan Brecheisen;Peer Kröger;Martin Pfeifle;Matthias Schubert
#t 2003
#c 5
#% 36672
#% 102772
#% 124447
#% 142639
#% 169940
#% 172949
#% 227999
#% 248797
#% 273890
#% 273921
#% 435141
#% 460862
#% 463414
#% 479462
#% 481609
#% 481947
#% 481956
#% 527158
#% 527186
#% 571100
#% 587733
#% 632035
#! In modern application domains such as multimedia, molecular biology and medical imaging, similarity search in database systems is becoming an increasingly important task. Especially for CAD applications, suitable similarity models can help to reduce the cost of developing and producing new parts by maximizing the reuse of existing parts. Most of the existing similarity models are based on feature vectors. In this paper, we shortly review three models which pursue this paradigm. Based on the most promising of these three models, we explain how sets of feature vectors can be used for more effective and still efficient similarity search. We first introduce an intuitive distance measure on sets of feature vectors together with an algorithm for its efficient computation. Furthermore, we present a method for accelerating the processing of similarity queries on vector set data. The experimental evaluation is based on two real world test data sets and points out that our new similarity approach yields more meaningful results in comparatively short time.

#index 654491
#* QCluster: relevance feedback using adaptive clustering for content-based image retrieval
#@ Deok-Hwan Kim;Chin-Wan Chung
#t 2003
#c 5
#% 51647
#% 219847
#% 232768
#% 286881
#% 319273
#% 341272
#% 437405
#% 479788
#% 480302
#% 480480
#% 631963
#% 632060
#% 729437
#% 993935
#% 1775090
#% 1857498
#! The learning-enhanced relevance feedback has been one of the most active research areas in content-based image retrieval in recent years. However, few methods using the relevance feedback are currently available to process relatively complex queries on large image databases. In the case of complex image queries, the feature space and the distance function of the user's perception are usually different from those of the system. This difference leads to the representation of a query with multiple clusters (i.e., regions) in the feature space. Therefore, it is necessary to handle disjunctive queries in the feature space.In this paper, we propose a new content-based image retrieval method using adaptive classification and cluster-merging to find multiple clusters of a complex image query. When the measures of a retrieval method are invariant under linear transformations, the method can achieve the same retrieval quality regardless of the shapes of clusters of a query. Our method achieves the same high retrieval quality regardless of the shapes of clusters of a query since it uses such measures. Extensive experiments show that the result of our method converges to the user's true information need fast, and the retrieval quality of our method is about 22% in recall and 20% in precision better than that of the query expansion approach, and about 34% in recall and about 33% in precision better than that of the query point movement approach, in MARS.

#index 654492
#* On relational support for XML publishing: beyond sorting and tagging
#@ Surajit Chaudhuri;Raghav Kaushik;Jeffrey F. Naughton
#t 2003
#c 5
#% 334006
#% 411554
#% 461924
#% 463735
#% 479460
#% 480152
#% 480657
#% 481288
#% 481604
#% 481608
#% 482082
#% 565457
#% 993941
#! In this paper, we study whether the need for efficient XML publishing brings any new requirements for relational query engines, or if sorting query results in the relational engine and tagging them in middleware is sufficient. We observe that the mismatch between the XML data model and the relational model requires relational engines to be enhanced for efficiency. Specifically, they need to support relation valued variables. We discuss how such support can be provided through the addition of an operator, GApply, with minimal extensions to existing relational engines. We discuss how the operator may be exposed in SQL syntax and provide a comprehensive study of optimization rules that govern this operator. We report the results of a preliminary performance evaluation showing the speedup obtained through our approach and the effectiveness of our optimization rules.

#index 654493
#* A comprehensive XQuery to SQL translation using dynamic interval encoding
#@ David DeHaan;David Toman;Mariano P. Consens;M. Tamer Özsu
#t 2003
#c 5
#% 163442
#% 172924
#% 191574
#% 236416
#% 266553
#% 333981
#% 340144
#% 378412
#% 397366
#% 397375
#% 411759
#% 459296
#% 479956
#% 480152
#% 480317
#% 480822
#% 504580
#% 659924
#% 659999
#% 770338
#! The W3C XQuery language recommendation, based on a hierarchical and ordered document model, supports a wide variety of constructs and use cases. There is a diversity of approaches and strategies for evaluating XQuery expressions, in many cases only dealing with limited subsets of the language. In this paper we describe an implementation approach that handles XQuery with arbitrarily-nested FLWR expressions, element constructors and built-in functions (including structural comparisons). Our proposal maps an XQuery expression to a single equivalent SQL query using a novel dynamic interval encoding of a collection of XML documents as relations, augmented with information tied to the query evaluation environment. The dynamic interval technique enables (suitably enhanced) relational engines to produce predictably good query plans that do not preclude the use of sort-merge join query operators. The benefits are realized despite the challenges presented by intermediate results that create arbitrary documents and the need to preserve document order as prescribed by semantics of XQuery. Finally, our experimental results demonstrate that (native or relational) XML systems can benefit from the above technique to avoid a quadratic scale up penalty that effectively prevents the evaluation of nested FLWR expressions for large documents.

#index 654494
#* Abstracts of invited industrial track presentations
#@ Zachary Ives
#t 2003
#c 5

#index 654495
#* Multi-dimensional clustering: a new data layout scheme in DB2
#@ Sriram Padmanabhan;Bishwaranjan Bhattacharjee;Tim Malkemus;Leslie Cranston;Matthew Huras
#t 2003
#c 5
#% 208037
#% 223781
#! We describe the design and implementation of a new data layout scheme, called multi-dimensional clustering, in DB2 Universal Database Version 8. Many applications, e.g., OLAP and data warehousing, process a table or tables in a database using a multi-dimensional access paradigm. Currently, most database systems can only support organization of a table using a primary clustering index. Secondary indexes are created to access the tables when the primary key index is not applicable. Unfortunately, secondary indexes perform many random I/O accesses against the table for a simple operation such as a range query. Our work in multi-dimensional clustering addresses this important deficiency in database systems. Multi-Dimensional Clustering is based on the definition of one or more orthogonal clustering attributes (or expressions) of a table. The table is organized physically by associating records with similar values for the dimension attributes in a cluster. We describe novel techniques for maintaining this physical layout efficiently and methods of processing database operations that provide significant performance improvements. We show results from experiments using a star-schema database to validate our claims of performance with minimal overhead.

#index 654496
#* Integration of electronic tickets and personal guide system for public transport using mobile terminals
#@ Koichi Goto;Yahiko Kambayashi
#t 2003
#c 5
#% 175253
#% 198038
#% 443263
#% 509886
#% 994009
#! We have been developing a mobile passenger guide system for public transports. Passengers can make their travel plans and purchase necessary electronic tickets using mobile terminals via Internet. During the travel, the mobile terminal, which also works as an electronic ticket, compares the stored travel plan with the passenger's actual activities and offers appropriate guide messages. To execute this task, the mobile terminal collects various kinds of information about the travel fields (routes, fares, area maps, station maps, operation schedule, timetables, facilities of stations and vehicles etc.) using multi-channel data communications. The mobile terminal contains a personal database for the passenger by selecting and integrating necessary data according to the user's situation and characteristics.

#index 654497
#* Gigascope: a stream database for network applications
#@ Chuck Cranor;Theodore Johnson;Oliver Spataschek;Vladislav Shkapenyuk
#t 2003
#c 5
#% 273943
#% 378388
#% 397414
#% 465170
#% 979303
#% 993949
#! We have developed Gigascope, a stream database for network applications including traffic analysis, intrusion detection, router configuration analysis, network research, network monitoring, and performance monitoring and debugging. Gigascope is undergoing installation at many sites within the AT&T network, including at OC48 routers, for detailed monitoring. In this paper we describe our motivation for and constraints in developing Gigascope, the Gigascope architecture and query language, and performance issues. We conclude with a discussion of stream database research problems we have found in our application.

#index 654498
#* WinMagic: subquery elimination using window aggregation
#@ Calisto Zuzarte;Hamid Pirahesh;Wenbin Ma;Qi Cheng;Linqi Liu;Kwai Wong
#t 2003
#c 5
#% 32878
#% 83144
#% 248787
#% 271935
#% 287005
#% 300138
#% 334006
#% 461897
#% 479460
#% 480091
#% 482082
#! Database queries often take the form of correlated SQL queries. Correlation refers to the use of values from the outer query block to compute the inner subquery. This is a convenient paradigm for SQL programmers and closely mimics a function invocation paradigm in a typical computer programming language. Queries with correlated subqueries are also often created by SQL generators that translate queries from application domain-specific languages into SQL. Another significant class of queries that use this correlated subquery form is that involving temporal databases using SQL. Performance of these queries is an important consideration particularly in large databases. Several proposals to improve the performance of SQL queries containing correlated subqueries can be found in database literature. One of the main ideas in many of these proposals is to suitably decorrelate the subquery internally to avoid a tuple-at-a-time invocation of the subquery. Magic decorrelation is one method that has been successfully used. Another proposal is to cache the portion of the subquery that is invariant with the changing values of the outer query block. What we propose here is a new technique to handle some typical correlated queries. We go a step further than to simply decorrelate the subquery. By making use of extended window aggregation capabilities, we eliminate redundant access to common tables referenced in the outer query block and the subquery. This technique can be exploited even for non-correlated subqueries. It is possible to get a huge boost in performance for queries that can exploit this technique, which we call WinMagic. This technique was implemented in IBM® DB2® Universal Database" Version 7 and Version 8. In addition to improving DB2 customer queries that contain aggregation subqueries, it has provided significant improvements in a number of TPCH benchmarks that IBM has published since late in 2001.

#index 654499
#* CMVF: a novel dimension reduction scheme for efficient indexing in a large image database
#@ Jialie Shen;Anne H. H. Ngu;John Shepherd;Du Q. Huynh;Quan Z. Sheng
#t 2003
#c 5
#% 571050

#index 654500
#* SOCQET: semantic OLAP with compressed cube and summarization
#@ Laks V. S. Lakshmanan;Jian Pei;Yan Zhao
#t 2003
#c 5
#% 654446
#% 993996

#index 654501
#* PeerDB: peering into personal databases
#@ Beng Chin Ooi;Kian-Lee Tan;Aoying Zhou;Chin Hong Goh;Yingguang Li;Chu Yee Liau;Bo Ling;Wee Siong Ng;Yanfeng Shu;Xiaoyu Wang;Ming Zhang
#t 2003
#c 5
#% 659957

#index 654502
#* GridDB: a relational interface for the grid
#@ David T. Liu;Michael J. Franklin;Devesh Parekh
#t 2003
#c 5
#% 480120

#index 654503
#* Transparent mid-tier database caching in SQL server
#@ Per-Åke Larson;Jonathan Goldstein;Jingren Zhou
#t 2003
#c 5
#% 397400
#% 397401
#% 397402
#% 397403
#% 480293

#index 654504
#* DBCache: middle-tier database caching for highly scalable e-business architectures
#@ Christof Bornhövd;Mehmet Altinel;Sailesh Krishnamurthy;C. Mohan;Hamid Pirahesh;Berthold Reinwald
#t 2003
#c 5

#index 654505
#* QXtract: a building block for efficient information extraction from text databases
#@ Eugene Agichtein;Luis Gravano
#t 2003
#c 5

#index 654506
#* PIX: exact and approximate phrase matching in XML
#@ Sihem Amer-Yahia;Mary Fernández;Divesh Srivastava;Yu Xu
#t 2003
#c 5

#index 654507
#* STREAM: the stanford stream data manager (demonstration description)
#@ Arvind Arasu;Brian Babcock;Shivnath Babu;Mayur Datar;Keith Ito;Itaru Nishizawa;Justin Rosenstein;Jennifer Widom
#t 2003
#c 5
#% 654462

#index 654508
#* Aurora: a data stream management system
#@ D. Abadi;D. Carney;U. Çetintemel;M. Cherniack;C. Convey;C. Erwin;E. Galvez;M. Hatoun;A. Maskey;A. Rasin;A. Singer;M. Stonebraker;N. Tatbul;Y. Xing;R. Yan;S. Zdonik
#t 2003
#c 5
#% 993949

#index 654509
#* IrisNet: Internet-scale resource-intensive sensor services
#@ Amol Deshpande;Suman Nath;Phillip B. Gibbons;Srinivasan Seshan
#t 2003
#c 5
#% 654483

#index 654510
#* TelegraphCQ: continuous dataflow processing
#@ Sirish Chandrasekaran;Owen Cooper;Amol Deshpande;Michael J. Franklin;Joseph M. Hellerstein;Wei Hong;Sailesh Krishnamurthy;Samuel R. Madden;Fred Reiss;Mehul A. Shah
#t 2003
#c 5

#index 654511
#* LockX: a system for efficiently querying secure XML
#@ SungRan Cho;Sihem Amer-Yahia;Laks V. S. Lakshmanan;Divesh Srivastava
#t 2003
#c 5
#% 993972

#index 654512
#* TREX: DTD-conforming XML to XML transformations
#@ Aoying Zhou;Qing Wang;Zhimao Guo;Xueqing Gong;Shihui Zheng;Hongwei Wu;Jianchang Xiao;Kun Yue;Wenfei Fan
#t 2003
#c 5
#% 504578

#index 654513
#* Rainbow: multi-XQuery optimization using materialized XML views
#@ Xin Zhang;Katica Dimitrova;Ling Wang;Maged El Sayed;Brian Murphy;Bradford Pielech;Mukesh Mulchandani;Luping Ding;Elke A. Rundensteiner
#t 2003
#c 5
#% 397405
#% 413650

#index 654514
#* TIMBER: a native system for querying XML
#@ Stelios Paparizos;Shurug Al-Khalifa;Adriane Chapman;H. V. Jagadish;Laks V. S. Lakshmanan;Andrew Nierman;Jignesh M. Patel;Divesh Srivastava;Nuwee Wiwatwattana;Yuqing Wu;Cong Yu
#t 2003
#c 5
#% 413564
#% 458836
#% 562456
#% 570875
#% 654441
#% 659999
#% 993985
#! XML has become ubiquitous, and XML data has to be managed in databases. The current industry standard is to map XML data into relational tables and store this information in a relational database. Such mappings create both expressive power problems and performance problems.In the TIMBER [7] project we are exploring the issues involved in storing XML in native format. We believe that the key intellectual contribution of this system is a comprehensive set-at-a-time query processing ability in a native XML store, with all the standard components of relational query processing, including algebraic rewriting and a cost-based optimizer.

#index 654515
#* ROLEX: relational on-line exchange with XML
#@ Philip Bohannon;Xin (Luna) Dong;Sumit Ganguly;Henry F. Korth;Chengkai Li;P. P. S. Narayan;Pradeep Shenoy
#t 2003
#c 5
#% 273940
#% 654484
#% 993941

#index 654516
#* A system for watermarking relational databases
#@ Rakesh Agrawal;Peter J. Haas;Jerry Kiernan
#t 2003
#c 5
#% 726623
#% 993944

#index 654517
#* Query by humming: in action with its technology revealed
#@ Yunyue Zhu;Dennis Shasha;Xiaojian Zhao
#t 2003
#c 5
#% 194192

#index 654518
#* PLASTIC: reducing query optimization overheads through plan recycling
#@ Vibhuti S. Sengar;Jayant R. Haritsa
#t 2003
#c 5
#% 993946

#index 654519
#* IPSOFACTO: a visual correlation tool for aggregate network traffic data
#@ Flip Korn;S. Muthukrishnan;Yunyue Zhu
#t 2003
#c 5
#! IP network operators collect aggregate traffic statistics on network interfaces via the Simple Network Management Protocol (SNMP). This is part of routine network operations for most ISPs; it involves a large infrastructure with multiple network management stations polling information from all the network elements and collating a real time data feed. This demo will present a tool that manages the live SNMP data feed on a fully operational large ISP at industry scale. The tool primarily serves to study correlations in the network traffic, by providing a rich mix of ad-hoc querying based on a user-friendly correlation interface and as well as canned queries, based on the expertise of the network operators with field experience. The tool is called IPSOFACTO for IP Stream-Oriented FAst Correlation TOol.

#index 654520
#* BIRN-M: a semantic mediator for solving real-world neuroscience problems
#@ Amarnath Gupta;Bertram Ludäscher;Maryann E. Martone
#t 2003
#c 5

#index 654521
#* Panel: querying networked databases
#@ Nick Koudas;Divesh Srivastava
#t 2003
#c 5

#index 654522
#* The Lowell report
#@ Jim Gray;Hans Schek;Michael Stonebraker;Jeff Ullman
#t 2003
#c 5
#% 111378
#% 275367
#% 339369

#index 654523
#* Data quality and data cleaning: an overview
#@ Theodore Johnson;Tamraparni Dasu
#t 2003
#c 5
#! Data quality is a serious concern in any data-driven enterprise, often creating misleading findings during data mining, and causing process disruptions in operational databases. The manifestations of data quality problems can be very expensive- "losing" customers, "misplacing" billions of dollars worth of equipment, misallocated resources due to glitched forecasts, and so on. Solving data quality problems typically requires a very large investment of time and energy -- often 80% to 90% of a data analysis project is spent in making the data reliable enough that the results can be trusted.In this tutorial, we present a multi disciplinary approach to data quality problems. We start by discussing the meaning of data quality and the sources of data quality problems. We show how these problems can be addressed by a multidisciplinary approach, combining techniques from management science, statistics, database research, and metadata management. Next, we present an updated definition of data quality metrics, and illustrate their application with a case study. We conclude with a survey of recent database research that is relevant to data quality problems, and suggest directions for future research.

#index 654524
#* XQuery: a query language for XML
#@ Don Chamberlin
#t 2003
#c 5
#! XQuery is the XML query language currently under development in the World Wide Web Consortium (W3C). XQuery specifications have been published in a series of W3C working drafts, and several reference implementations of the language are already available on the Web. If successful, XQuery has the potential to be one of the most important new computer languages to be introduced in several years. This tutorial will provide an overview of the syntax and semantics of XQuery, as well as insight into the principles that guided the design of the language.

#index 654525
#* Data grid management systems
#@ Arun Jagatheesan;Arcot Rajasekar
#t 2003
#c 5
#! Data Grids are being built across the world as the next generation data handling systems to manage peta-bytes of inter organizational data and storage space. A data grid (datagrid) is a logical name space consisting of storage resources and digital entities that is created by the cooperation of autonomous organizations and its users based on the coordination of local and global policies. Data Grid Management Systems (DGMSs) provide services for the confluence of organizations and management of inter-organizational data and resources in the datagrid.The objective of the tutorial is to provide an introduction to the opportunities and challenges of this emerging technology. Novices and experts would benefit from this tutorial. The tutorial would cover introduction, use cases, design philosophies, architecture, research issues, existing technologies and demonstrations. Hands on sessions for the participants to use and feel the existing technologies could be provided based on the availability of internet connections.

#index 662748
#* Proceedings of the 8th ACM SIGMOD workshop on Research issues in data mining and knowledge discovery
#@ Mohammed J. Zaki;Charu C. Aggarwal
#t 2003
#c 5

#index 662749
#* Analyzing massive data streams: past, present, and future
#@ Minos Garofalakis
#t 2003
#c 5
#! Continuous data streams arise naturally, for example, in the installations of large telecom and Internet service providers where detailed usage information (Call-Detail-Records, SNMP-/RMON packet-flow data, etc.) from different parts of the underlying network needs to be continuously collected and analyzed for interesting trends. Such environments raise a critical need for effective stream-processing algorithms that can provide (typically, approximate) answers to data-analysis queries while utilizing only small space (to maintain concise stream synopses) and small processing time per stream item. In this talk, I will discuss the basic pseudo-random sketching mechanism for building stream synopses and our ongoing work that exploits sketch synopses to build an approximate SQL (multi) query processor. I will also describe our recent results on extending sketching to handle more complex forms of queries and streaming data (e.g., similarity joins over streams of XML trees), and try to identify some challenging open problems in the data-streaming area.

#index 662750
#* A symbolic representation of time series, with implications for streaming algorithms
#@ Jessica Lin;Eamonn Keogh;Stefano Lonardi;Bill Chiu
#t 2003
#c 5
#% 172949
#% 237204
#% 285711
#% 310488
#% 328321
#% 333941
#% 378388
#% 397629
#% 451127
#% 466507
#% 477482
#% 480146
#% 481611
#% 548654
#% 577221
#% 577275
#% 594012
#% 617888
#% 629648
#% 631923
#% 659971
#% 715230
#! The parallel explosions of interest in streaming data, and data mining of time series have had surprisingly little intersection. This is in spite of the fact that time series data are typically streaming data. The main reason for this apparent paradox is the fact that the vast majority of work on streaming data explicitly assumes that the data is discrete, whereas the vast majority of time series data is real valued.Many researchers have also considered transforming real valued time series into symbolic representations, nothing that such representations would potentially allow researchers to avail of the wealth of data structures and algorithms from the text processing and bioinformatics communities, in addition to allowing formerly "batch-only" problems to be tackled by the streaming community. While many symbolic representations of time series have been introduced over the past decades, they all suffer from three fatal flaws. Firstly, the dimensionality of the symbolic representation is the same as the original data, and virtually all data mining algorithms scale poorly with dimensionality. Secondly, although distance measures can be defined on the symbolic approaches, these distance measures have little correlation with distance measures defined on the original time series. Finally, most of these symbolic approaches require one to have access to all the data, before creating the symbolic representation. This last feature explicitly thwarts efforts to use the representations with streaming algorithms.In this work we introduce a new symbolic representation of time series. Our representation is unique in that it allows dimensionality/numerosity reduction, and it also allows distance measures to be defined on the symbolic approach that lower bound corresponding distance measures defined on the original series. As we shall demonstrate, this latter feature is particularly exciting because it allows one to run certain data mining algorithms on the efficiently manipulated symbolic representation, while producing identical results to the algorithms that operate on the original data. Finally, our representation allows the real valued data to be converted in a streaming fashion, with only an infinitesimal time and space overhead.We will demonstrate the utility of our representation on the classic data mining tasks of clustering, classification, query by content and anomaly detection.

#index 662751
#* Clustering binary data streams with K-means
#@ Carlos Ordonez
#t 2003
#c 5
#% 152934
#% 210173
#% 248790
#% 248792
#% 273891
#% 278011
#% 280419
#% 300120
#% 300131
#% 320942
#% 333933
#% 413619
#% 420081
#% 430881
#% 466513
#% 479962
#% 480812
#% 481290
#% 594012
#% 631985
#% 857390
#! Clustering data streams is an interesting Data Mining problem. This article presents three variants of the K-means algorithm to cluster binary data streams. The variants include On-line K-means, Scalable K-means, and Incremental K-means, a proposed variant introduced that finds higher quality solutions in less time. Higher quality of solutions are obtained with a mean-based initialization and incremental learning. The speedup is achieved through a simplified set of sufficient statistics and operations with sparse matrices. A summary table of clusters is maintained on-line. The K-means variants are compared with respect to quality of results and speed. The proposed algorithms can be used to monitor transactions.

#index 662752
#* Processing frequent itemset discovery queries by division and set containment join operators
#@ Ralf Rantzau
#t 2003
#c 5
#% 189638
#% 196825
#% 248813
#% 316709
#% 333981
#% 342643
#% 453190
#% 458838
#% 471366
#% 480463
#% 480629
#% 480825
#% 481290
#% 569755
#% 654454
#% 713791
#! SQL-based data mining algorithms are rarely used in practice today. Most performance experiments have shown that SQL-based approaches are inferior to main-memory algorithms. Nevertheless, database vendors try to integrate analysis functionalities to some extent into their query execution and optimization components in order to narrow the gap between data and processing. Such a database support is particularly important when data mining applicatons need to analyze very large datasets or when they need access current data, not a possibly outdated copy of it.We investigate approaches based on SQL for the problem of finding frequent itemsets in a transaction table, including an algorithm that we recently proposed, called Quiver, which employs universal and existential quantifications. This approach employs a table schema for itemsets that is similar to the commonly used vertical layout for transactions: each item of an itemset is stored in a separate row. We argue that expressing the frequent itemset discovery problem using quantifications offers interesting opportunities to process such queries using set containment join or set containment division operators, which are not yet available in commercial database systems. Initial performance experiments reveal that Quiver cannot be processed efficiently by commercial DBMS. However, our experiments with query execution plans that use operators realizing set containment tests suggest that an efficient processing of Quiver is possible.

#index 662753
#* Efficient OLAP operations for spatial data using peano trees
#@ Baoying Wang;Fei Pan;Dongmei Ren;Yue Cui;Qiang Ding;William Perrizo
#t 2003
#c 5
#% 18614
#% 191154
#% 227880
#% 420053
#% 435141
#% 462217
#% 480329
#% 481956
#% 502134
#! Online Analytical Processing (OLAP) is an important application of data warehouses. With more and more spatial data being collected, such as remotely sensed images, geographical information, digital sky survey data, efficient OLAP for spatial data is in great demand. In this paper, we build up a new data warehouse structure -- PD-cube, With PD-cube, OLAP operations and queries can be efficiently implemented. All these are accomplished based on the fast logical operations of Peano Trees (P-Trees*). One of the P-tree variations, Predicate P-tree, is used to efficiently reduce data accesses by filtering out "bit holes" consisting of consecutive 0's. Experiments show that OLAP operations can be executed much faster than with traditional OLAP methods.

#index 662754
#* Clustering gene expression data in SQL using locally adaptive metrics
#@ Dimitris Papadopoulos;Carlotta Domeniconi;Dimitrios Gunopulos;Sheng Ma
#t 2003
#c 5
#% 143194
#% 210173
#% 248792
#% 260008
#% 273891
#% 278040
#% 333941
#% 397382
#% 397384
#% 466414
#% 469422
#% 480307
#% 481281
#% 715706
#! The clustering problem concerns the discovery of homogeneous groups of data according to a certain similarity measure. Clustering suffers from the curse of dimensionality. It is not meaningful to look for clusters in high dimensional spaces as the average density of points anywhere in input space is likely to be low. As a consequence, distance functions that equally use all input features may be ineffective. We introduce an algorithm that discovers clusters in subspaces spanned by different combinations of dimensions via local weightings of features. This approach avoids the risk of loss of information encountered in global dimensionality reduction techniques. Our method associates to each cluster a weight vector, whose values capture the relevance of features within the corresponding cluster. In this paper we present an efficient SQL implementation of our algorithm, that enables the discovery of clusters on data residing inside a relational DBMS.

#index 662755
#* Graph-based ranking algorithms for e-mail expertise analysis
#@ Byron Dom;Iris Eiron;Alex Cozzi;Yi Zhang
#t 2003
#c 5
#% 146494
#% 220708
#% 240973
#% 290830
#% 549563
#% 1499467
#! In this paper we study graph--based ranking measures for the purpose of using them to rank email correspondents according to their degree of expertise on subjects of interest. While this complete expertise analysis consists of several steps, in this paper we focus on the analysis of digraphs whose nodes correspond to correspondents (people), whose edges correspond to the existence of email correspondence between the people corresponding to the nodes they connect and whose edge directions point from the member of the pair whose relative expertise has been estimated to be higher. We perform our analysis on both synthetic and real data and we introduce a new error measure for comparing ranked lists.

#index 662756
#* Deriving link-context from HTML tag tree
#@ Gautam Pant
#t 2003
#c 5
#% 268073
#% 268079
#% 268106
#% 281251
#% 309145
#% 311040
#% 330599
#% 340928
#% 348138
#! HTML anchors are often surrounded by text that seems to describe the destination page appropriately. The text surrounding a link or the link-context is used for a variety of tasks associated with Web information retrieval. These tasks can benefit by identifying regularities in the manner in which "good" contexts appear around links. In this paper, we describe a framework for conducting such a study. The framework serves as an evaluation platform for comparing various link-context derivation methods. We apply the framework to a sample of Web pages obtained from more than 10,000 different categories of the ODP. Our focus is on understanding the potential merits of using a Web page's tag tree structure, for deriving link-contexts. We find that good link-context can be associated with tag tree hierarchy. Our results show that climbing up the tag tree when the link-context provided by greater depths is too short can provide better performance than some of the traditional techniques.

#index 662757
#* Clustering of streaming time series is meaningless
#@ Jessica Lin;Eamonn Keogh;Wagner Truppel
#t 2003
#c 5
#% 152934
#% 260016
#% 280482
#% 310580
#% 397631
#% 430746
#% 430767
#% 443515
#% 463948
#% 466083
#% 494958
#% 498625
#% 501995
#% 528055
#% 546415
#% 577221
#% 594012
#% 630989
#% 858452
#% 993965
#! Time series data is perhaps the most frequently encountered type of data examined by the data mining community. Clustering is perhaps the most frequently used data mining algorithm, being useful in it's own right as an exploratory technique, and also as a subroutine in more complex data mining algorithms such as rule discovery, indexing, summarization, anomaly detection, and classification. Given these two facts, it is hardly surprising that time series clustering has attracted much attention. The data to be clustered can be in one of two formats: many individual time series, or a single time series, from which individual time series are extracted with a sliding window. Given the recent explosion of interest in streaming data and online algorithms, the latter case has received much attention.In this work we make a surprising claim. Clustering of streaming time series is completely meaningless. More concretely, clusters extracted from streaming time series are forced to obey a certain constraint that is pathologically unlikely to be satisfied by any dataset, and because of this, the clusters extracted by any clustering algorithm are essentially random. While this constraint can be intuitively demonstrated with a simple illustration and is simple to prove, it has never appeared in the literature.We can justify calling our claim surprising, since it invalidates the contribution of dozens of previously published papers. We will justify our claim with a theorem, illustrative examples, and a comprehensive set of experiments on reimplementations of previous work. Although the primary contribution of our work is to draw attention to the fact that an apparent solution to an important problem is incorrect and should no longer be used, we also introduce a novel method which, based on the concept of time series motifs, is able to meaningfully cluster some streaming time series datasets.

#index 662758
#* A learning-based approach to estimate statistics of operators in continuous queries: a case study
#@ Like Gao;Min Wang;X. Sean Wang;Sriram Padmanabhan
#t 2003
#c 5
#% 67552
#% 82346
#% 116082
#% 136350
#% 152917
#% 172902
#% 172949
#% 227857
#% 248822
#% 300179
#% 312054
#% 378388
#% 397352
#% 397353
#% 397380
#% 397426
#% 411554
#% 413606
#% 428155
#% 443298
#% 460862
#% 481609
#% 511349
#% 631866
#% 659996
#% 689389
#! Statistic estimation such as output size estimation of operators is a well-studied subject in the database research community, mainly for the purpose of query optimization. The assumption, however, is that queries are ad-hoc and therefore the emphasis has been on capturing the data distribution. When long standing continuous queries on a changing database are concerned, a more direct approach, namely building an estimation model for each operator, is possible. In this paper, we propose a novel learning-based method. Our method consists of two steps. The first step is to design a dedicated feature extraction algorithm that can be used incrementally to obtain feature values from the underlying data. The second step is to use a data mining algorithm to generate an estimation model based on the feature values extracted from the historical data. To illustrate the approach, this paper studies the case of similarity-based searches over streaming time series. Experimental results show this approach provides accurate statistic estimates with a low overhead.

#index 662759
#* Using transposition for pattern discovery from microarray data
#@ François Rioult;Jean-François Boulicaut;Bruno Crémilleux;Jérémy Besson
#t 2003
#c 5
#% 232136
#% 248791
#% 279120
#% 310494
#% 338594
#% 420062
#% 431033
#! We analyze expression matrices to identify a priori interesting sets of genes, e.g., genes that are frequently co-regulated. Such matrices provide expression values for given biological situations (the lines) and given genes (columns). The frequent itemset (sets of columns) extraction technique enables to process difficult cases (millions of lines, hundreds of columns) provided that data is not too dense. However, expression matrices can be dense and have generally only few lines w.r.t. the number of columns. Known algorithms, including the recent algorithms that compute the so-called condensed representations can fail. Thanks to the properties of Galois connections, we propose an original technique that processes the transposed matrices while computing the sets of genes. We validate the potential of this framework by looking for the closed sets in two microarray data sets.

#index 662760
#* Weave amino acid sequences for protein secondary structure prediction
#@ Xiaochun Yang;Bin Wang
#t 2003
#c 5
#% 190581
#% 669214
#% 1860547
#! Given a known protein sequence, predicting its secondary structure can help understand its three-dimensional (tertiary) structure, i.e., the folding. In this paper, we present an approach for predicting protein secondary structures. Different from the existing prediction methods, our approach proposes an encoding schema that weaves physio-chemical information in encoded vectors and a prediction framework that combines the context information with secondary structure segments. We employed Support Vector Machine (SVM) for training the CB513 and RS126 data sets, which are collections of protein secondary structure sequences, through sevenfold cross validation to uncover the structural differences of protein secondary structures. Hereafter, we apply the sliding window technique to test a set of protein sequences based on the group classification learned from the training set. Our approach achieves 77.8% segment overlap accuracy (SOV) and 75.2% three-state overall per-residue accuracy (Q3), which outperform other prediction methods.

#index 662761
#* Assuring privacy when big brother is watching
#@ Murat Kantarcioĝlu;Chris Clifton
#t 2003
#c 5
#% 23638
#% 176257
#% 300184
#% 577289
#% 635219
#% 809530
#! Homeland security measures are increasing the amount of data collected, processed and mined. At the same time, owners of the data raised legitimate concern about their privacy and potential abuses of the data. Privacy-preserving data mining techniques enable learning models without violating privacy. This paper addresses a complementary problem: What if we want to apply a model without revealing it? This paper presents a method to apply classification rules without revealing either the data or the rules. In addition, the rules can be verified not to use "forbidden" criteria.

#index 662762
#* Dynamic inference control
#@ Jessica Staddon
#t 2003
#c 5
#% 55179
#% 65525
#% 76569
#% 151163
#% 190611
#% 386006
#% 488015
#% 488144
#% 488145
#% 507406
#% 536498
#% 575965
#% 592708
#% 664495
#% 664502
#% 664673
#! An inference problem exists in a multilevel database if knowledge of some objects in the database allows information with a higher security level to be inferred. Many such inferences may be prevented prior to any query processing by raising the security level of some of the objects, however this inevitably impedes information access, as a user with low authorization who queries just one of the objects with raised security must seek clearance even when not in danger of making the inference. More flexible access control is possible when inferences are prevented during query processing, however this practice can result in slow query response times. We demonstrate that access control can be made sufficiently dynamic to ensure easy access to the information users are entitled to, while retaining fast query processing. Our inference control schemes provide collusion resistance and have a query processing time that depends only on the length of the inference channels (not on the length of user query histories). In addition, our schemes provide a property we call crowd control that goes beyond collusion resistance to ensure that if a large number of users have queried all but one of the objects in an inference channel, then no one will be able to query the remaining object regardless of the level of collusion resistance provided by the scheme.

#index 723439
#* Analysis of SIGMOD's co-authorship graph
#@ Mario A. Nascimento;Jörg Sander;Jeffrey Pound
#t 2003
#c 5
#% 287267
#% 590527
#! In this paper we investigate the co-authorship graph obtained from all papers published at SIGMOD between 1975 and 2002. We find some interesting facts, for instance, the identity of the authors who, on average, are "closest" to all other authors at a given time. We also show that SIGMOD's co-authorship graph is yet another example of a small world---a graph topology which has received a lot of attention recently. A companion web site for this paper can be found at http://db.cs.ualberta.ca/coauthorship.

#index 723440
#* Journal relevance
#@ Richard Snodgrass
#t 2003
#c 5

#index 723441
#* Learning about data integration challenges from day one
#@ Alon Y. Halevy
#t 2003
#c 5
#! I describe the format of the new version of an introductory database course that I taught at the University ofWashington inWinter, 2003. The key idea underlying the course is to expose the students to some of the challenges that arise when working with and integrating data from multiple database systems and applications.

#index 723442
#* Exposing undergraduate students to database system internals
#@ Anastassia Ailamaki;Joseph M. Hellerstein
#t 2003
#c 5
#% 210206
#! In Spring 2003, Joe Hellerstein at Berkeley and Natassa Ailamaki at CMU collaborated in designing and running parallel editions of an undergraduate database course that exposed students to developing code in the core of a ful-function database system. As part of this exercise, our course teams developed new programming projects based on the PostgreSQL open-source DBMS. This report describes our experience with this effort.

#index 723443
#* Guest editor's introduction
#@ Karl Aberer
#t 2003
#c 5

#index 723444
#* Peer-to-peer research at Stanford
#@ Mayank Bawa;Brian F. Cooper;Arturo Crespo;Neil Daswani;Prasanna Ganesan;Hector Garcia-Molina;Sepandar Kamvar;Sergio Marti;Mario Schlosser;Qi Sun;Patrick Vinograd;Beverly Yang
#t 2003
#c 5
#% 300078
#% 322884
#% 337484
#% 340175
#% 340176
#% 345086
#% 348160
#% 414381
#% 505869
#% 577361
#% 577367
#% 612645
#% 636008
#% 636009
#% 636037
#% 636117
#% 643013
#% 963875

#index 723445
#* P-Grid: a self-organizing structured P2P system
#@ Karl Aberer;Philippe Cudré-Mauroux;Anwitaman Datta;Zoran Despotovic;Manfred Hauswirth;Magdalena Punceva;Roman Schmidt
#t 2003
#c 5
#% 340175
#% 342695
#% 349973
#% 433980
#% 433982
#% 449218
#% 496164
#% 577320
#% 593985
#% 636116
#% 721359
#% 1386260

#index 723446
#* Toward network data independence
#@ Joseph M. Hellerstein
#t 2003
#c 5
#% 107693
#% 151529
#% 159733
#% 191573
#% 232640
#% 232771
#% 286477
#% 299862
#% 300167
#% 322880
#% 340175
#% 340176
#% 397353
#% 411695
#% 446421
#% 456101
#% 496147
#% 505869
#% 612643
#% 646237
#% 646239
#% 674136
#% 1015281
#% 1394366
#% 1830308
#! A number of researchers have become interested in the design of global-scale networked systems and applications. Our thesis here is that the database community's principles and technologies have an important role to play in the design of these systems. The point of departure is at the roots of database research: we generalize Codd's notion of data independence to physical environments beyond storage systems. We note analogies between the development of database indexes and the new generation of structured peer-to-peer networks. We illustrate the emergence of data independence in networks by surveying a number of recent network facilities and applications, seen through a database lens. We present a sampling of database query processing techniques that can contribute in this arena, and discuss methods for adoption of these technologies.

#index 723447
#* Design issues and challenges for RDF- and schema-based peer-to-peer systems
#@ Wolfgang Nejdl;Wolf Siberski;Michael Sintek
#t 2003
#c 5
#% 340175
#% 340176
#% 348182
#% 449218
#% 480658
#% 519557
#% 572300
#% 577320
#% 577357
#% 577359
#% 636008
#% 636009
#% 993960
#% 1386265
#% 1388072
#! Databases have employed a schema-based approach to store and retrieve structured data for decades. For peer-to-peer (P2P) networks, similar approaches are just beginning to emerge. While quite a few database techniques can be re-used in this new context, a P2P data management infrastructure poses additional challenges which have to be solved before schema-based P2P networks become as common as schema-based databases. We will describe some of these challenges and discuss approaches to solve them. Our discussion will be based on the design decisions we have employed in our Edutella infrastructure, a schema-based P2P network based on RDF and RDF schemas, and will also point out additional work addressing the issues discussed.

#index 723448
#* The Piazza peer data management project
#@ Igor Tatarinov;Zachary Ives;Jayant Madhavan;Alon Halevy;Dan Suciu;Nilesh Dalvi;Xin (Luna) Dong;Yana Kadiyska;Gerome Miklau;Peter Mork
#t 2003
#c 5
#% 36683
#% 229827
#% 248038
#% 321455
#% 330305
#% 333990
#% 449218
#% 480134
#% 481923
#% 571217
#% 572311
#% 577320
#% 577359
#% 654468
#% 1015302
#% 1015329
#! A major problem in today's information-driven world is that sharing heterogeneous, semantically rich data is incredibly difficult. Piazza is a peer data management system that enables sharing heterogeneous data in a distributed and scalable way. Piazza assumes the participants to be interested in sharing data, and willing to define pairwise mappings between their schemas. Then, users formulate queries over their preferred schema, and a query answering system expands recursively any mappings relevant to the query, retrieving data from other peers. In this paper, we provide a brief overview of the Piazza project including our work on developing mapping languages and query reformulation algorithms, assisting the users in defining mappings, indexing, and enforcing access control over shared data.

#index 723449
#* The hyperion project: from data integration to data coordination
#@ Marcelo Arenas;Vasiliki Kantere;Anastasios Kementsietsidis;Iluju Kiringa;Renée J. Miller;John Mylopoulos
#t 2003
#c 5
#% 340175
#% 340176
#% 378409
#% 465057
#% 465159
#% 543690
#% 577320
#% 589484
#% 654468
#% 659957
#% 993981
#! We present an architecture and a set of challenges for peer database management systems. These systems team up to build a network of nodes (peers) that coordinate at run time most of the typical DBMS tasks such as the querying, updating, and sharing of data. Such a network works in a way similar to conventional multidatabases. Conventional multidatabase systems are founded on key concepts such as those of a global schema, central administrative authority, data integration, global access to multiple databases, permanent participation of databases, etc. Instead, our proposal assumes total absence of any central authority or control, no global schema, transient participation of peer databases, and constantly evolving coordination rules among databases. In this work, we describe the status of the Hyperion project, present our current solutions, and outline remaining research issues.

#index 723450
#* Relational data sharing in peer-based data management systems
#@ Beng Chin Ooi;Yanfeng Shu;Kian-Lee Tan
#t 2003
#c 5
#% 340175
#% 340176
#% 397418
#% 449218
#% 505869
#% 654468
#% 659957
#% 660011
#% 674136
#% 993987
#! Data sharing in current P2P systems is very much restricted to file-system-like capabilities. In this paper, we present the strategies that we have adopted in our BestPeer project to support more fine-grained data sharing, especially, relational data sharing, in a P2P context. First, we look at some of the issues in designing a peer-based data management system, and discuss some possible solutions to address these issues. Second, we present the design of our first prototype system, PeerDB, and report our experience with it. Finally, we discuss our current extensions to PeerDB to support keyword-based queries.

#index 723451
#* In-context peer-to-peer information filtering on the Web
#@ Aris M. Ouksel
#t 2003
#c 5
#% 2298
#% 3708
#% 28185
#% 95617
#% 117871
#% 163046
#% 181622
#% 188978
#% 430420
#% 577320
#% 654468
#% 1386262

#index 723452
#* Selective information dissemination in P2P networks: problems and solutions
#@ Manolis Koubarakis;Christos Tryfonopoulos;Stratos Idreos;Yannis Drougas
#t 2003
#c 5
#% 158911
#% 248838
#% 267451
#% 297191
#% 312870
#% 338354
#% 339055
#% 340175
#% 340911
#% 379013
#% 384978
#% 465063
#% 480296
#% 508414
#% 516235
#% 543556
#% 636010
#% 636108
#% 659995
#% 1386254
#! We study the problem of selective dissemination of information in P2P networks. We present our work on data models and laiguages for textual information dissemination and discuss a relemnt P2P architecture that motivates our efforts. We also survey our results on the computational complexity of three related algorithmic problems (query satisfiability, entailment and filtering) and present efficient algorithms for the most crucial of these problems (filtering). Finally, we discuss the features of P2P-DIET, a super-peer system we have implemented at the Technical Lniversity of Crete, that realizes our vision and is able to support both ad-hoc querying and selective information dissemination scenarios in a P2P framework.

#index 723453
#* DBGlobe: a service-oriented P2P system for global computing
#@ Evaggelia Pitoura;Serge Abiteboul;Dieter Pfoser;George Samaras;Michalis Vazirgiannis
#t 2003
#c 5
#% 413793
#% 527176
#% 654465
#% 654485
#% 724549
#! The challenge of peer-to-peer computing goes beyond simple file sharing. In the DBGlobe project, we view the multitude of peers carrying data and services as a superdatabase. Our goal is to develop a data management system for modeling, indexing and querying data hosted by such massively distributed, autonomous and possibly mobile peers. We employ a service-oriented approach, in that data are encapsulated in services. Direct querying of data is also supported by an XML-based query language. In this paper, we present our research results along the following topics: (a) infrastructure support, including mobile peers and the creation of context-dependent communities, (b) metadata management for services and peers, including location-dependent data, (c) filters for efficiently routing path queries on hierarchical data, and (d) querying using the AXML language that incorporates service calls inside XML documents.

#index 723454
#* Standards for databases on the grid
#@ Susan Malaika;Andrew Eisenberg;Jim Melton
#t 2003
#c 5

#index 723455
#* Review of The data warehouse toolkit: the complete guide to dimensional modeling (2nd edition) by Ralph Kimball, Margy Ross. John Wiley & Sons, Inc. 2002.
#@ Alexander A. Anisimov
#t 2003
#c 5

#index 731472
#* Edgar F. Codd: a tribute and personal memoir
#@ C. J. Date
#t 2003
#c 5

#index 731473
#* Developments at ACM TODS
#@ Richard Snodgrass
#t 2003
#c 5

#index 731474
#* Sensor: the atomic computing particle
#@ Vijay Kumar
#t 2003
#c 5
#% 297913
#% 309433
#% 411796
#% 438658
#% 444568
#% 1772138
#% 1831294
#! We visualize the world as a fully connected information space where each object communicates with all other objects without any temporal and geographical constraints. We can model this fully connected space using fine granularity processing which can be implemented using sensors technology. We regard sensors as atomic computing particles which can deployed to geographical locations for capturing and processing data of their surrounding. This report introduces a number of excellent research articles which present unique problems and their success in finding efficient solutions for them. It also peeks in to the future of ever changing information processing discipline.

#index 731475
#* The sensor spectrum: technology, trends, and requirements
#@ Joseph M. Hellerstein;Wei Hong;Samuel R. Madden
#t 2003
#c 5
#% 250244
#% 300167
#% 309433
#% 330413
#% 336865
#% 401228
#% 427022
#% 429194
#% 480965
#% 654482
#% 761322
#% 805466
#% 1394365
#% 1394366
#! Though physical sensing instruments have long been used in astronomy, biology, and civil engineering, the recent emergence of wireless sensor networks and RFID has spurred a renaissance in sensor interest in both academia and industry. In this paper, we examine the spectrum of sensing platforms, from billion dollar satellites to tiny RF tags, and discuss the technological differences between them. We show that battery powered sensor networks, with low-power multihop radios and low-cost processors, occupy a sweet spot in this spectrum that is rife with opportunity for novel database research. We briefly summarize some of our research work in this space and present a number of examples of interesting sensor network-related problems that the database community is uniquely equipped to address.

#index 731476
#* Understanding the semantics of sensor data
#@ Murali Mani
#t 2003
#c 5
#% 333926
#% 397352
#% 397353
#% 427022
#% 453192
#% 462500
#% 549156
#% 654507
#% 660004
#% 848763
#% 993949
#% 993960
#! Our system architecture to manage sensor data is described. Our data mining applications require past history of the sensor data. Therefore, unlike most present systems that focus on streaming data, and cache a small window of historic data, we store the entire historic data. Several interesting problems arise in these scenarios. We study two of them: (a) Given that a sensor can send data corresponding to its current configuration at any particular instant, how do we define the data that should be stored in the database? (b) Sensors try to minimize the amount of data transmitted. Also there could be data loss in the network. So the data stored will have lots of "holes". In this case, how can an application make sense of the stored data? In this paper, we describe our approach to solve these problems that enables an application to recreate the environment that generated the data as precisely as possible.

#index 731477
#* Bluetooth-based sensor networks
#@ Philippe Bonnet;Allan Beaufour;Mads Bondo Dydensborg;Martin Leopold
#t 2003
#c 5
#% 281556
#% 297915
#% 336865
#% 401226
#% 654482
#% 731086
#% 731094
#! It is neither desirable nor possible to abstract sensor network software from the characteristics of the underlying hardware components. In particular the radio has a major impact on higher level software. In this paper, we review the lessons we learnt using Bluetooth radios in the context of sensor networks. These lessons are relevant for (a) application designers choosing the best radio given a set of requirements and for (b) researchers in the data management community who need to formulate assumptions about underlying sensor networks.

#index 731478
#* Managing uncertainty in sensor database
#@ Reynold Cheng;Sunil Prabhakar
#t 2003
#c 5
#% 295512
#% 296159
#% 333863
#% 333969
#% 397355
#% 458849
#% 480299
#% 480332
#% 654487
#% 772835
#! Sensors are often employed to monitor continuously changing entities like locations of moving objects and temperature. The sensor readings are reported to a centralized database system, and are subsequently used to answer queries. Due to continuous changes in these values and limited resources (e.g., network bandwidth and battery power), the database may not be able to keep track of the actual values of the entities, and use the old values instead. Queries that use these old values may produce incorrect answers. However, if the degree of uncertainty between the actual data value and the database value is limited, one can place more confidence in the answers to the queries. In this paper, we present a frame-work that represents uncertainty of sensor data. Depending on the amount of uncertainty information given to the application, different levels of imprecision are presented in a query answer. We examine the situations when answer imprecision can be represented qualitatively and quantitatively. We propose a new kind of probabilistic queries called Probabilistic Threshold Query, which requires answers to have probabilities larger than a certain threshold value. We also study techniques for evaluating queries under different details of uncertainty, and investigate the tradeoff between data uncertainty, answer accuracy and computation costs.

#index 731479
#* An environmental sensor network to determine drinking water quality and security
#@ Anastassia Ailamaki;Christos Faloutos;Paul S. Fischbeck;Mitchell J. Small;Jeanne VanBriesen
#t 2003
#c 5
#% 172949
#% 227857
#% 316709
#% 427199
#% 463903
#% 480146
#% 482095
#% 503870
#% 632090
#! Finding patterns in large, real, spatio/temporal data continues to attract high interest (e.g., sales of products over space and time, patterns in mobile phone users; sensor networks collecting operational data from automobiles, or even from humans with wearable computers). In this paper, we describe an interdisciplinary research effort to couple knowledge discovery in large environmental databases with biological and chemical sensor networks, in order to revolutionize drinking water quality and security decision making. We describe a distribution and operation protocol for the placement and utilization of in situ environmental sensors by combining (1) new algorithms for spatialtemporal data mining, (2) new methods to model water quality and security dynamics, and (3) a sophisticated decision-analysis framework. The project was recently funded by NSF and represents application of these research areas to the critical current issue of ensuring safe and secure drinking water to the population of the United States.

#index 731480
#* The Cougar Project: a work-in-progress report
#@ Alan Demers;Johannes Gehrke;Rajmohan Rajaraman;Niki Trigoni;Yong Yao
#t 2003
#c 5
#% 171449
#% 198465
#% 201883
#% 248815
#% 259642
#% 259665
#% 273917
#% 274937
#% 297915
#% 309433
#% 330305
#% 333965
#% 339210
#% 342371
#% 397413
#% 401165
#% 401227
#% 408396
#% 424925
#% 453525
#% 464706
#% 479476
#% 480149
#% 480158
#% 481608
#% 554915
#% 622760
#% 654482
#% 805466
#% 805467
#% 1394366
#! We present an update on the status of the Cougar Sensor Database Project, in which we are investigating a database approach to sensor networks: Clients "program" the sensors through queries in a high-level declarative language (such as a variant of SQL). In this paper, we give an overview of our activities on energy-efficient data dissemination and query processing. Due to space constraints, we cannot present a full menu of results; instead, we decided to only whet the reader's appetite with some problems in energy-efficient routing and in-network aggregation and some thoughts on how to approach them.

#index 731481
#* Energy and rate based MAC protocol for wireless sensor networks
#@ Rajgopal Kannan;Ram Kalidindi;S. S. Iyengar;Vijay Kumar
#t 2003
#c 5
#% 264669
#% 310285
#% 339223
#% 355963
#% 360833
#% 414174
#! Sensor networks are typically unattended because of their deployment in hazardous, hostile or remote environments. This makes the problem of conserving energy at individual sensor nodes challenging. S-MAC and PAMAS are two MAC protocols which periodically put nodes (selected at random) to sleep in order to achieve energy savings. Unlike these protocols, we propose an approach in which node duty cycles (i.e sleep and wake schedules) are based on their criticality. A distributed algorithm is used to find sets of winners and losers, who are then assigned appropriate slots in our TDMA based MAC protocol. We introduce the concept of of energy-criticality of a sensor node as a function of energies and traffic rates. Our protocol makes more critical nodes sleep longer, thereby balancing the energy consumption. Simulation results show that the performance of the protocol with increase in traffic load is better than existing protocols with increase in traffic load is better than existing protocols, thereby illustrating the energy balancing nature of the approach.

#index 731482
#* Power efficient data gathering and aggregation in wireless sensor networks
#@ Hüseyin Özgür Tan;Ibrahim Körpeoǧlu
#t 2003
#c 5
#% 259665
#% 281537
#% 309433
#% 318291
#% 414174
#% 439324
#% 444550
#% 608160
#% 778363
#! Recent developments in processor, memory and radio technology have enabled wireless sensor networks which are deployed to collect useful information from an area of interest. The sensed data must be gathered and transmitted to a base station where it is further processed for end-user queries. Since the network consists of low-cost nodes with limited battery power, power efficient methods must be employed for data gathering and aggregation in order to achieve long network lifetimes.In an environment where in a round of communication each of the sensor nodes has data to send to a base station, it is important to minimize the total energy consumed by the system in a round so that the system lifetime is maximized. With the use of data fusion and aggregation techniques, while minimizing the total energy per round, if power consumption per node can be balanced as well, a near optimal data gathering and routing scheme can be achieved in terms of network lifetime.So far, besides the conventional protocol of direct transmission, two elegant protocols called LEACH and PEGASIS have been proposed to maximize the lifetime of a sensor network. In this paper, we propose two new algorithms under name PEDAP (Power Efficient Data gathering and Aggregation Protocol), which are near optimal minimum spanning tree based routing schemes, where one of them is the power-aware version of the other. Our simulation results show that our algorithms perform well both in systems where base station is far away from and where it is in the center of the field. PEDAP achieves between 4x to 20x improvement in network lifetime compared with LEACH, and about three times improvement compared with PEGASIS.

#index 731483
#* Multimedia streaming in large-scale sensor networks with mobile swarms
#@ Mario Gerla;Kaixin Xu
#t 2003
#c 5
#% 250311
#% 281345
#% 309433
#% 622728
#! Sensor networking technologies have developed very rapidly in the last ten years. In many situations, high quality multimedia streams may be required for providing detailed information of the hot spots in a large scale network. With the limited capabilities of sensor node and sensor network, it is very difficult to support multimedia streams in current sensor network structure. In this paper, we propose to enhance the sensor network by deploying limited number of mobile "swarms". The swarm nodes have much higher capabilities than the sensor nodes in terms of both hardware functionalities and networking capabilities. The mobile swarms can be directed to the hot spots in the sensor network to provide detailed information of the intended area. With the help of mobile swarms, high quality of multimedia streams can be supported in the large scale sensor network without too much cost. The wireless backbone network for connecting different swarms and the routing schemes for supporting such a unified architecture is also discussed and verified via simulations.

#index 731484
#* Distributed deviation detection in sensor networks
#@ Themistoklis Palpanas;Dimitris Papadopoulos;Vana Kalogeraki;Dimitrios Gunopulos
#t 2003
#c 5
#% 300136
#% 300183
#% 300193
#% 309433
#% 333926
#% 342600
#% 379444
#% 438354
#% 459025
#% 479791
#% 480628
#% 548654
#% 576113
#% 579676
#% 615136
#% 631651
#% 635986
#% 660004
#% 664842
#% 721137
#% 729966
#% 745513
#% 805466
#% 993959
#! Sensor networks have recently attracted much attention, because of their potential applications in a number of different settings. The sensors can be deployed in large numbers in wide geographical areas, and can be used to monitor physical phenomena, or to detect certain events.An interesting problem which has not been adequately addressed so far is that of distributed online deviation detection in streaming data. The identification of deviating values provides an efficient way to focus on the interesting events in the sensor network.In this work, we propose a technique for online deviation detection in streaming data. We discuss how these techniques can operate efficiently in the distributed environment of a sensor network, and discuss the tradeoffs that arise in this setting. Our techniques process as much of the data as possible in a decentralized fashion, so as to avoid unnecessary communication and computational effort.

#index 731485
#* Reasoning on regular path queries
#@ D. Calvanese;G. De Giacomo;M. Lenzerini;M. Y. Vardi
#t 2003
#c 5
#% 36181
#% 64284
#% 140410
#% 154067
#% 198465
#% 198473
#% 210176
#% 211987
#% 237191
#% 248038
#% 268708
#% 273700
#% 283052
#% 289266
#% 291299
#% 292677
#% 299967
#% 299968
#% 328424
#% 378409
#% 378410
#% 404772
#% 408396
#% 464056
#% 464717
#% 562451
#% 562454
#% 572311
#% 576097
#% 576102
#% 587566
#% 598376
#% 599549
#% 1499552
#! Current information systems are required to deal with more complex data with respect to traditional relational data. The database community has already proposed abstractions for these kinds of data, in particular in terms of semistructured data models. A semistructured model conceives a database essentially as a finite directed labeled graph whose nodes represent objects, and whose edges represent relationships between objects. In the same way as conjunctive queries form the core of any query language for the relational model, regular path queries (RPQs) and their variants are considered the basic querying mechanisms for semistructured data.Besides the basic task of query answering, i.e., evaluating a query over a database, databases should support other reasoning services related to querying. One of the most important is query containment, i.e., verifying whether for all databases the answer to a query is a subset of the answer to a second query. Another important reasoning service that has received considerable attention in the recent years is view-based query processing, which amounts to processing queries based on a set of materialized views, rather than on the raw data in the database.The goal of this paper is to describe basic results and techniques concerning query containment and view based query processing for the class of two-way regular-path queries (which extend RPQs with the inverse operator). We will demonstrate that the basic services for reasoning about two way regular path queries are decidable, thus showing that the limited form of recursion expressible by these queries does not endanger the decidability of reasoning. Besides the specific results, our methods show the power of two-way automata in reasoning on complex queries.

#index 731486
#* Review of Web caching and replication by Michael Rabinovich and Oliver Spatscheck. Addison Wesley 2002.
#@ Qiang Wang;Brian D. Davison
#t 2003
#c 5

#index 731487
#* Review of Data on the Web: from relational to semistructured data and XML by Serge Abiteboul, Peter Buneman, and Dan Suciu. Morgan Kaufmann 1999.
#@ Fernando Berzal;Nicolás Marín
#t 2003
#c 5

#index 731488
#* Review of Spatial databases with application to GIS by Philippe Rigaux, Michel Scholl, and Agnes Voisard. Morgan Kaufmann 2002.
#@ Nancy Wiegand
#t 2003
#c 5

#index 731489
#* Report on the 5th international workshop on the design and management of data warehouses (DMDW'03)
#@ Hans J. Lenz;Panos Vassiliadis;Manfred Jeusfeld;Martin Staudt
#t 2003
#c 5

#index 731490
#* Agent-Oriented software engineering report on the 4th AOSE workshop (AOSE 2003)
#@ Paolo Giorgini
#t 2003
#c 5
#! Agent-Orientation is emerging as a powerful new paradigm in computing. Concepts, methodologies and tools from the agents paradigm are one of the best candidates for the foundations of the next generation of mainstream software systems. The Agent-Oriented Software Engineering (AOSE) workshop is an international event that brings together researchers and groups active in the area of agent-based software development. Here we briefly report on the fourth edition of the AOSE workshop.

#index 731491
#* Report on FQAS 2002: fifth international conference on flexible query answering systems
#@ Amihai Motro;Troels Andreasen
#t 2003
#c 5

#index 731492
#* Information integration on the Web: a view from AI and databases (report on IIWeb-03)
#@ Subbarao Kambhampati;Craig A. Knoblock
#t 2003
#c 5

#index 731493
#* Research in database engineering at the University of Namur
#@ Jean-Luc Hainaut
#t 2003
#c 5
#% 207787
#% 281977
#% 536361
#% 562805
#% 564867
#% 622426
#% 665365

#index 742041
#* A secure hierarchical model for sensor network
#@ Malik Tubaishat;Jian Yin;Biswajit Panja;Sanjay Madria
#t 2004
#c 5
#% 281537
#% 281556
#% 309433
#% 349938
#% 411796
#% 622757
#% 662360
#% 1772138
#! In a distributed sensor network, large number of sensors deployed which communicate among themselves to self-organize a wireless ad hoc network. We propose an energy-efficient level-based hierarchical system. We compromise between the energy consumption and shortest path route by utilizing number of neighbors (NBR) of a sensor and its level in the hierarchical clustering. In addition, we design a Secure Routing Protocol for Sensor Networks (SRPSN) to safeguard the data packet passing on the sensor networks under different types of attacks. We build the secure route from the source node to sink node. The sink node is guaranteed to receive correct information using our SRPSN. We also propose a group key management scheme, which contains group communication policies, group membership requirements and an algorithm for generating a distributed group key for secure communication.

#index 742042
#* Robust key establishment in sensor networks
#@ Yongge Wang
#t 2004
#c 5
#% 123243
#% 411796
#% 414367
#% 523958
#% 616979
#! Secure communication guaranteeing reliability, authenticity, and privacy in sensor networks with active adversaries is a challenging research problem since asymmetric key cryptosystems are not suitable for sensor nodes with limited computation and communication capabilities. In most proposed secure communication protocols, sensor nodes need to contact the base station to get a session key first if two sensor nodes want to establish a secure communication channel (e.g., SPINS). In several environments, this may be impractical. In this paper, we study key agreement protocols for which two sensor nodes (who do not necessarily have a shared key from the key predistribution phase) could establish a secure communication channel against active adversaries (e.g., denial of service attacks) without the involvement of the base station.

#index 742043
#* Team communications among autonomous sensor swarms
#@ Mario Gerla;Yunjung Yi
#t 2004
#c 5
#% 281345
#% 281543
#% 349932
#! In this paper, we consider team (swarm) of unmanned vehicles (UVs) equipped with various sensors (videos, chemicals, etc). Those swarms need efficient communication to feed sensed data, communicate data to other swarms, to navigate and, more generally, to carry out complex mission autonomously. We focus on a particular aspect of mission oriented communications, namely, team multicast. In team multicast, the multicast group does not consist of individual members, rather, of teams. In our case, the teams may consist of special UVs that have been established to launch a search and rescue mission. Simulation results illustrate the performance benefits of the team multicast solution as compared with more traditional multicast approaches.

#index 742044
#* QUASAR: quality aware sensing architecture
#@ Iosif Lazaridis;Qi Han;Xingbo Yu;Sharad Mehrotra;Nalini Venkatasubramanian;Dmitri V. Kalashnikov;Weiwen Yang
#t 2004
#c 5
#% 333969
#% 378388
#% 414174
#% 439324
#% 654487
#% 654488
#% 729793
#% 745503
#% 1112417
#! Sensor devices are promising to revolutionize our interaction with the physical world by allowing continuous monitoring and reaction to natural and artificial processes at an unprecedented level of spatial and temporal resolution. As sensors become smaller, cheaper and more configurable, systems incorporating large numbers of them become feasible. Besides the technological aspects of sensor design, a critical factor enabling future sensor-driven applications will be the availability of an integrated infrastructure taking care of the onus of data management. Ideally, accessing sensor data should be no difficult or inconvenient than using simple SQL.In this paper we investigate some of the issues that such an infrastructure must address. Unlike conventional distributed database systems, a sensor data architecture must handle extremely high data generation rates from a large number of small autonomous components. And, unlike the emerging paradigm of data streams, it is infeasible to think that all this data can be streamed into the query processing site, due to severe bandwidth and energy constraints of battery-operated wireless sensors. Thus, sensing data architectures must become quality-aware, regulating the quality of data at all levels of the distributed system, and supporting user applications' quality requirements in the most efficient manner possible.

#index 742045
#* Statistical grid-based clustering over data streams
#@ Nam Hun Park;Won Suk Lee
#t 2004
#c 5
#% 248790
#% 280417
#% 379445
#% 479658
#% 659972
#% 993960
#! A data stream is a massive unbounded sequence of data elements continuously generated at a rapid rate. Due to this reason, most algorithms for data streams sacrifice the correctness of their results for fast processing time. The processing time is greatly influenced by the amount of information that should be maintained. This paper proposes a statistical grid-based approach to clustering data elements of a data stream. Initially, the multidimensional data space of a data stream is partitioned into a set of mutually exclusive equal-size initial cells. When the support of a cell becomes high enough, the cell is dynamically divided into two mutually exclusive intermediate cells based on its distribution statistics. Three different ways of partitioning a dense cell are introduced. Eventually, a dense region of each initial cell is recursively partitioned until it becomes the smallest cell called a unit cell. A cluster of a data stream is a group of adjacent dense unit cells. In order to minimize the number of cells, a sparse intermediate or unit cell is pruned if its support becomes much less than a minimum support. Furthermore, in order to confine the usage of memory space, the size of a unit cell is dynamically minimized such that the result of clustering becomes as accurate as possible. The proposed algorithm is analyzed by a series of experiments to identify its various characteristics.

#index 742046
#* Replica allocation for correlated data items in ad hoc sensor networks
#@ Takahiro Hara;Norishige Murakami;Shojiro Nishio
#t 2004
#c 5
#% 171449
#% 259642
#% 309433
#% 408638
#% 579660
#% 617427
#% 622728
#% 720827
#% 1831268
#! To improve data accessibility in ad hoc networks, in our previous work we proposed three methods of replicating data items by considering the data access frequencies from mobile nodes to each data item and the network topology. In this paper, we extend our previously proposed methods to consider the correlation among data items. Under these extended methods, the data priority of each data item is de-fined based on the correlation among data items, and data items are replicated at mobile nodes with the data priority. We employ simulations to show that the extended methods are more efficient than the original ones.

#index 742047
#* An initial study of overheads of eddies
#@ Amol Deshpande
#t 2004
#c 5
#% 136740
#% 300167
#% 397353
#% 479938
#! An eddy [2] is a highly adaptive query processing operator that continuously reoptimizes a query in response to changing runtime conditions. It does this by treating query processing as routing of tuples through operators and making per-tuple routing decisions. The benefits of such adaptivity can be significant, especially in highly dynamic environments such as data streams, sensor query processing, web querying, etc. Various parties have asserted that the cost of making per-tuple routing decisions is prohibitive. We have implemented eddies in the PostgreSQL open source database system [1] in the context of the TelegraphCQ project. In this paper, we present an "apples-to-apples" comparison of PostgreSQL query processing overhead with and without eddies. Our results show that with some minor tuning, the overhead of the eddy mechanism is negligible.

#index 742048
#* State-of-the-art in privacy preserving data mining
#@ Vassilios S. Verykios;Elisa Bertino;Igor Nai Fovino;Loredana Parasiliti Provenza;Yucel Saygin;Yannis Theodoridis
#t 2004
#c 5
#% 67453
#% 279334
#% 300184
#% 333876
#% 340291
#% 428404
#% 488643
#% 539744
#% 575969
#% 577233
#% 577289
#% 586838
#% 635214
#% 635220
#% 637062
#% 664070
#% 740764
#% 993988
#! We provide here an overview of the new and rapidly emerging research area of privacy preserving data mining. We also propose a classification hierarchy that sets the basis for analyzing the work which has been performed in this context. A detailed review of the work accomplished in this area is also given, along with the coordinates of each work to the classification hierarchy. A brief evaluation is performed, and some initial conclusions are made.

#index 742049
#* Evaluating lock-based protocols for cooperation on XML documents
#@ Sven Helmer;Carl-Christian Kanne;Guido Moerkotte
#t 2004
#c 5
#% 459
#% 3645
#% 9241
#% 114583
#% 122915
#% 273922
#% 289399
#% 381812
#% 403195
#% 413565
#% 465006
#% 479956
#% 482657
#% 504574
#% 567011
#% 570875
#% 570876
#% 571098
#% 665535
#% 721245
#% 1394435
#! We discuss four different core protocols for synchronizing access to and modifications of XML document collections. These core protocols synchronize structure traversals and modifications. They are meant to be integrated into a native XML base management System (XBMS) and are based on two phase locking. We also demonstrate the different degrees of cooperation that are possible with these protocols by various experimental results. Furthermore, we also discuss extensions of these core protocols to full-fledged protocols. Further, we show how to achieve a higher degree of concurrency by exploiting the semantics expressed in Document Type Definitions (DTDs).

#index 742050
#* Land below a DBMS
#@ Kaladhar Voruganti;Jai Menon;Sandeep Gopisetty
#t 2004
#c 5
#% 283761
#% 612166
#% 770362
#% 770366
#% 770372
#% 830711

#index 742051
#* A context-aware methodology for very small data base design
#@ C. Bolchini;F. A. Schreiber;L. Tanca
#t 2004
#c 5
#% 401371
#% 433364
#% 438656
#% 438657
#% 449851
#% 566138
#% 641769
#! The design of a Data Base to be resident on portable devices and embedded processors for professional systems requires considering both the device memory peculiarities and the mobility aspects, which are an essential feature of the embedded applications. Moreover, these devices are often part of a larger Information System, comprising fixed and mobile resources. We propose a complete methodology for designing Very Small Data Bases, from the identification of the device resident portions down to the choice of the physical data structure, optimizing the cost and power consumption of the Flash memory, which - in the greatest generality - constitutes the permanent storage of the device.

#index 742052
#* Entity-Relationship modeling revisited
#@ Antonio Badia
#t 2004
#c 5
#% 319
#% 11284
#% 55294
#% 98454
#% 100745
#% 126330
#% 158200
#% 158908
#% 163046
#% 166203
#% 207596
#% 248800
#% 264771
#% 287631
#% 335500
#% 389068
#% 397598
#% 442861
#% 481280
#% 534064
#% 534359
#% 534875
#% 571297
#! In this position paper, we argue the modern applications require databases to capture and enforce more domain semantics than traditional applications. We also argue that the best way to incorporate additional semantics into database systems is by capturing the added information in conceptual models and then using it for database design. In this light, we revisit Entity-Relationship models and investigate ways in which such models could be extended to play a role in the process. Inspired by a paper by Rafael Camps Pare ([2]), we suggest avenues of research in the issue.

#index 742053
#* Reconsidering Multi-Dimensional schemas
#@ Tim Martyn
#t 2004
#c 5
#% 264952
#% 282431
#% 308509
#% 392740
#% 480122
#! This paper challenges the currently popular "Data Warehouse is a Special Animal" philosophy and advocates that practitioners adopt a more conservative "Data Warehouse=Database" philosophy. The primary focus is the relevancy of Multi-Dimensional logical schemas. After enumerating the advantages of such schemas, a number of caveats to the presumed advantages are identified. The paper concludes with guidelines and commentary on implications for data warehouse design methodologies.

#index 742054
#* Simulation data as data streams
#@ Ghaleb Abdulla;Terence Critchlow;William Arrighi
#t 2004
#c 5
#% 308500
#% 337432
#% 378388
#% 482096
#% 577265
#% 730012
#% 733253
#! Computational or scientific simulations are increasingly being applied to solve a variety of scientific problems. Domains such as astrophysics, engineering, chemistry, biology, and environmental studies are benefiting from this important capability. Simulations, however, produce enormous amounts of data that need to be analyzed and understood. In this overview paper, we describe scientific simulation data, its characteristics, and the way scientists generate and use the data. We then compare and contrast simulation data to data streams. Finally, we describe our approach to analyzing simulation data, present the AQSim (Ad-hoc Queries for Simulation data) system, and discuss some of the challenges that result from handling this kind of data.

#index 742055
#* Toward an ontology-enhanced information filtering agent
#@ Kwang Mong Sim
#t 2004
#c 5
#% 55490
#% 240957
#% 445309
#% 732627
#! Whereas search engines assist users in locating initial information sources, often an overwhelmingly large number of ULRs is returned, and the task of browsing websites rests heavily on users. The contribution of this work is developing an information filtering agent (IFA) that assists users in identifying out-of-context web pages and rating the relevance of web pages. An IFA determines the relevance of web pages by adopting three heuristics: (i) detecting evidence phrases (EP) constructed from WORDNET's ontology, (ii) counting the frequencies of EP and (iii) considering the nearness among keywords. Favorable experimental results show that the IFA's ratings of web pages are generally close to human ratings in many instances. The strength and weaknesses of the IFA are also discussed.

#index 742056
#* XPath query containment
#@ Thomas Schwentick
#t 2004
#c 5
#% 287339
#% 378391
#% 378393
#% 384978
#% 427027
#% 465043
#% 465051
#% 465065
#% 487257
#% 564264
#% 570877
#% 576102
#% 576108
#% 578562
#% 599549
#% 717141
#% 733593
#% 993939
#% 1015267

#index 742057
#* SQL:2003 has been published
#@ Andrew Eisenberg;Jim Melton;Krishna Kulkarni;Jan-Eike Michels;Fred Zemke
#t 2004
#c 5
#% 397607

#index 742058
#* Report on the Dagstuhl Seminar
#@ Michael Gertz;M. Tamer Özsu;Gunter Saake;Kai-Uwe Sattler
#t 2004
#c 5
#% 228356
#% 344895
#% 912094

#index 742059
#* Science of design for information systems: report of the NSF workshop, Seattle, 2003
#@  ACM SIGMOD Record staff
#t 2004
#c 5
#! The Workshop on Science of Design for Information Systems (SDIS2003) was held in Seattle, September 16 and 17, 2003. It was funded through a grant from the National Science Foundation2, with the goal of assessing the state-of-the-art in information systems design and suggesting promising directions for future research and development in this critical area.This short report is intended to provide an overview of the workshop report for the SIGMOD audience. In summary, we believe that there is a need to develop a new set of methodologies for information system design that cover advanced aspects of such systems. In particular, we are interested in techniques that offer guidance in the design of information systems that integrate data from multiple sources, handle dynamic aspects of the system (e.g., rapidly changing data, tracking provenance, version management), include aspects influenced by data location (e.g., cached objects and queries, peer-to-peer data sharing), model process-oriented issues (e.g., workflows, web-services), and account for the security and privacy of the data. AB@The interested reader can find a full version of this report at the workshop website, www.cs.wisc.edu/sdis03.

#index 742060
#* Report from the first workshop on geo sensor networks
#@ S. Nittel;A. Stefanidis;I. Cruz;M. Egenhofer;D. Goldin;A. Howard;A. Labrinidis;S. Madden;A. Voisard;M. Worboys
#t 2004
#c 5

#index 765398
#* Proceedings of the 2004 ACM SIGMOD international conference on Management of data
#@ Arnd Christian König;Stefan Dessloch;Patrick Valduriez;Gerhard Weikum
#t 2004
#c 5
#! The 2004 ACM SIGMOD International Conference on Management of Data, held in Paris in the week of June 13-18, is the first SIGMOD ever held outside of North America, and it has chosen a place that is rich in tradition but also rich in new departures, one of the focal points of the age of enlightenment and the place of the French revolution in 1789. We hope that this is a symbol of SIGMOD embracing new challenges in the broader area of business and scientific information management and the wider spectrum of network-enabled data-rich applications.The SIGMOD conference is sponsored by the Association for Computing Machinery (ACM) and its Special Interest Group on Management of Data (SIGMOD). Two days of the conference overlap with the Symposium on Principles of Database System (PODS), and several workshops and other events are co-located with SIGMOD.

#index 765399
#* The next database revolution
#@ Jim Gray
#t 2004
#c 5
#! Database system architectures are undergoing revolutionary changes. Most importantly, algorithms and data are being unified by integrating programming languages with the database system. This gives an extensible object-relational system where non-procedural relational operators manipulate object sets. Coupled with this, each DBMS is now a web service. This has huge implications for how we structure applications. DBMSs are now object containers. Queues are the first objects to be added. These queues are the basis for transaction processing and workflow applications. Future workflow systems are likely to be built on this core. Data cubes and online analytic processing are now baked into most DBMSs. Beyond that, DBMSs have a framework for data mining and machine learning algorithms. Decision trees, Bayes nets, clustering, and time series analysis are built in; new algorithms can be added. There is a rebirth of column stores for sparse tables and to optimize bandwidth. Text, temporal, and spatial data access methods, along with their probabilistic reasoning have been added to database systems. Allowing approximate and probabilistic answers is essential for many applications. Many believe that XML and xQuery will be the main data structure and access pattern. Database systems must accommodate that perspective. External data increasingly arrives as streams to be compared to historical data; so stream-processing operators are being added to the DBMS. Publish-subscribe systems invert the data-query ratios; incoming data is compared against millions of queries rather than queries searching millions of records. Meanwhile, disk and memory capacities are growing much faster than their bandwidth and latency, so the database systems increasingly use huge main memories and sequential disk access. These changes mandate a much more dynamic query optimization strategy - one that adapts to current conditions and selectivities rather than having a static plan. Intelligence is moving to the periphery of the network. Each disk and each sensor will be a competent database machine. Relational algebra is a convenient way to program these systems. Database systems are now expected to be self-managing, self-healing, and always-up. We researchers and developers have our work cut out for us in delivering all these features.

#index 765400
#* The role of cryptography in database security
#@ Ueli Maurer
#t 2004
#c 5
#% 23638
#% 54179
#% 54180
#% 164560
#% 381870
#% 593711
#% 1395005
#! In traditional database security research, the database is usually assumed to be trustworthy. Under this assumption, the goal is to achieve security against external attacks (e.g. from hackers) and possibly also against users trying to obtain information beyond their privileges, for instance by some type of statistical inference. However, for many database applications such as health information systems there exist conflicting interests of the database owner and the users or organizations interacting with the database, and also between the users. Therefore the database cannot necessarily be assumed to be fully trusted.In this extended abstract we address the problem of defining and achieving security in a context where the database is not fully trusted, i.e., when the users must be protected against a potentially malicious database. Moreover, we address the problem of the secure aggregation of databases owned by mutually mistrusting organisations, for example by competing companies.

#index 765401
#* Proceedings of the 2004 ACM SIGMOD international conference on Management of data
#@ Arnd Christian König;Stefan Dessloch;Patrick Valduriez;Gerhard Weikum
#t 2004
#c 5

#index 765402
#* Adaptive stream resource management using Kalman Filters
#@ Ankur Jain;Edward Y. Chang;Yuan-Fang Wang
#t 2004
#c 5
#% 273911
#% 308000
#% 333969
#% 378388
#% 378408
#% 578560
#% 613388
#% 632090
#% 654487
#% 654488
#% 654508
#% 729437
#% 730216
#% 745534
#% 1015280
#! To answer user queries efficiently, a stream management system must handle continuous, high-volume, possibly noisy, and time-varying data streams. One major research area in stream management seeks to allocate resources (such as network bandwidth and memory) to query plans, either to minimize resource usage under a precision requirement, or to maximize precision of results under resource constraints. To date, many solutions have been proposed; however, most solutions are ad hoc with hard-coded heuristics to generate query plans. In contrast, we perceive stream resource management as fundamentally a filtering problem, in which the objective is to filter out as much data as possible to conserve resources, provided that the precision standards can be met. We select the Kalman Filter as a general and adaptive filtering solution for conserving resources. The Kalman Filter has the ability to adapt to various stream characteristics, sensor noise, and time variance. Furthermore, we realize a significant performance boost by switching from traditional methods of caching static data (which can soon become stale) to our method of caching dynamic procedures that can predict data reliably at the server without the clients' involvement. In this work we focus on minimization of communication overhead for both synthetic and real-world streams. Through examples and empirical studies, we demonstrate the flexibility and effectiveness of using the Kalman Filter as a solution for managing trade-offs between precision of results and resources in satisfying stream queries.

#index 765403
#* Online event-driven subsequence matching over financial data streams
#@ Huanmei Wu;Betty Salzberg;Donghui Zhang
#t 2004
#c 5
#% 86950
#% 172949
#% 227857
#% 227924
#% 227937
#% 281750
#% 310537
#% 330932
#% 333926
#% 333941
#% 379445
#% 397380
#% 397381
#% 413606
#% 428155
#% 460862
#% 462231
#% 466506
#% 477479
#% 479462
#% 480628
#% 480632
#% 481279
#% 502142
#% 578560
#% 594012
#% 993961
#% 1015261
#! Subsequence similarity matching in time series databases is an important research area for many applications. This paper presents a new approximate approach for automatic online subsequence similarity matching over massive data streams. With a simultaneous on-line segmentation and pruning algorithm over the incoming stream, the resulting piecewise linear representation of the data stream features high sensitivity and accuracy. The similarity definition is based on a permutation followed by a metric distance function, which provides the similarity search with flexibility, sensitivity and scalability. Also, the metric-based indexing methods can be applied for speed-up. To reduce the system burden, the event-driven similarity search is performed only when there is a potential event. The query sequence is the most recent subsequence of piecewise data representation of the incoming stream which is automatically generated by the system. The retrieved results can be analyzed in different ways according to the requirements of specific applications. This paper discusses an application for future data movement prediction based on statistical information. Experiments on real stock data are performed. The correctness of trend predictions is used to evaluate the performance of subsequence similarity matching.

#index 765404
#* Holistic UDAFs at streaming speeds
#@ Graham Cormode;Theodore Johnson;Flip Korn;S. Muthukrishnan;Oliver Spatscheck;Divesh Srivastava
#t 2004
#c 5
#% 2833
#% 214073
#% 248820
#% 273682
#% 310500
#% 333926
#% 333931
#% 338425
#% 378388
#% 397354
#% 397385
#% 397414
#% 397426
#% 453512
#% 464215
#% 480628
#% 480805
#% 482123
#% 492912
#% 569754
#% 594012
#% 654497
#% 660004
#% 816392
#% 979303
#% 993949
#% 993959
#% 993960
#% 993969
#% 1015373
#! Many algorithms have been proposed to approximate holistic aggregates, such as quantiles and heavy hitters, over data streams. However, little work has been done to explore what techniques are required to incorporate these algorithms in a data stream query processor, and to make them useful in practice.In this paper, we study the performance implications of using user-defined aggregate functions (UDAFs) to incorporate selection-based and sketch-based algorithms for holistic aggregates into a data stream management system's query processing architecture. We identify key performance bottlenecks and tradeoffs, and propose novel techniques to make these holistic UDAFs fast and space-efficient for use in high-speed data stream applications. We evaluate performance using generated and actual IP packet data, focusing on approximating quantiles and heavy hitters. The best of our current implementations can process streaming queries at OC48 speeds (2x 2.4Gbps).

#index 765405
#* BLAS: an efficient XPath processing system
#@ Yi Chen;Susan B. Davidson;Yifeng Zheng
#t 2004
#c 5
#% 273922
#% 325384
#% 333981
#% 379483
#% 397358
#% 397360
#% 397375
#% 479956
#% 480489
#% 480656
#% 565265
#% 598374
#% 654451
#% 654493
#% 659924
#% 659999
#% 993953
#% 1015270
#! We present BLAS, a Bi-LAbeling based System, for efficiently processing complex XPath queries over XML data. BLAS uses P-labeling to process queries involving consecutive child axes, and D-labeling to process queries involving descendant axes traversal. The XML data is stored in labeled form, and indexed to optimize descendent axis traversals. Three algorithms are presented for translating complex XPath queries to SQL expressions, and two alternate query engines are provided. Experimental results demonstrate that the BLAS system has a substantial performance improvement compared to traditional XPath processing using D-labeling.

#index 765406
#* Efficient processing of XML twig queries with OR-predicates
#@ Haifeng Jiang;Hongjun Lu;Wei Wang
#t 2004
#c 5
#% 236416
#% 252366
#% 333981
#% 333989
#% 397360
#% 397375
#% 479465
#% 479806
#% 480489
#% 480656
#% 570875
#% 570876
#% 654450
#% 659999
#% 993953
#% 1015273
#% 1015277
#! An XML twig query, represented as a labeled tree, is essentially a complex selection predicate on both structure and content of an XML document. Twig query matching has been identified as a core operation in querying tree-structured XML data. A number of algorithms have been proposed recently to process a twig query holistically. Those algorithms, however, only deal with twig queries without OR-predicates. A straightforward approach that first decomposes a twig query with OR-predicates into multiple twig queries without OR-predicates and then combines their results is obviously not optimal in most cases. In this paper, we study novel holistic-processing algorithms for twig queries with OR-predicates without decomposition. In particular, we present a merge-based algorithm for sorted XML data and an index-based algorithm for indexed XML data. We show that holistic processing is much more efficient than the decomposition approach. Furthermore, we show that using indexes can significantly improve the performance for matching twig queries with OR-predicates, especially when the queries have large inputs but relatively small outputs.

#index 765407
#* Tree logical classes for efficient evaluation of XQuery
#@ Stelios Paparizos;Yuqing Wu;Laks V. S. Lakshmanan;H. V. Jagadish
#t 2004
#c 5
#% 333981
#% 397366
#% 397375
#% 413650
#% 465006
#% 479956
#% 562456
#% 570875
#% 654493
#% 659999
#% 994015
#% 1015274
#! XML is widely praised for its flexibility in allowing repeated and missing sub-elements. However, this flexibility makes it challenging to develop a bulk algebra, which typically manipulates sets of objects with identical structure. A set of XML elements, say of type book, may have members that vary greatly in structure, e.g. in the number of author sub-elements. This kind of heterogeneity may permeate the entire document in a recursive fashion: e.g., different authors of the same or different book may in turn greatly vary in structure. Even when the document conforms to a schema, the flexible nature of schemas for XML still allows such significant variations in structure among elements in a collection. Bulk processing of such heterogeneous sets is problematic.In this paper, we introduce the notion of logical classes (LC) of pattern tree nodes, and generalize the notion of pattern tree matching to handle node logical classes. This abstraction pays off significantly in allowing us to reason with an inherently heterogeneous collection of elements in a uniform, homogeneous way. Based on this, we define a Tree Logical Class (TLC) algebra that is capable of handling the heterogeneity arising in XML query processing, while avoiding redundant work. We present an algorithm to obtain a TLC algebra expression from an XQuery statement (for a large fragment of XQuery). We show how to implement the TLC algebra efficiently, introducing the nest-join as an important physical operator for XML query processing. We show that evaluation plans generated using the TLC algebra not only are simpler but also perform better than those generated by competing approaches. TLC is the algebra used in the Timber [8] system developed at the University of Michigan.

#index 765408
#* FleXPath: flexible structure and full-text querying for XML
#@ Sihem Amer-Yahia;Laks V. S. Lakshmanan;Shashank Pandit
#t 2004
#c 5
#% 194247
#% 227894
#% 262069
#% 309726
#% 333951
#% 378393
#% 399762
#% 406493
#% 458744
#% 458854
#% 458861
#% 465155
#% 504581
#% 571098
#% 642993
#% 654442
#% 745463
#% 754116
#% 978382
#% 1015258
#% 1015267
#! Querying XML data is a well-explored topic with powerful database-style query languages such as XPath and XQuery set to become W3C standards. An equally compelling paradigm for querying XML documents is full-text search on textual content. In this paper, we study fundamental challenges that arise when we try to integrate these two querying paradigms.While keyword search is based on approximate matching, XPath has exact match semantics. We address this mismatch by considering queries on structure as a "template", and looking for answers that best match this template and the full-text search. To achieve this, we provide an elegant definition of relaxation on structure and define primitive operators to span the space of relaxations. Query answering is now based on ranking potential answers on structural and full-text search conditions. We set out certain desirable principles for ranking schemes and propose natural ranking schemes that adhere to these principles. We develop efficient algorithms for answering top-K queries and discuss results from a comprehensive set of experiments that demonstrate the utility and scalability of the proposed framework and algorithms.

#index 765409
#* An interactive clustering-based approach to integrating source query interfaces on the deep Web
#@ Wensheng Wu;Clement Yu;AnHai Doan;Weiyi Meng
#t 2004
#c 5
#% 55294
#% 85086
#% 307632
#% 314740
#% 333990
#% 350103
#% 375017
#% 376266
#% 406493
#% 466580
#% 480134
#% 480479
#% 480499
#% 480645
#% 572314
#% 577238
#% 654459
#% 660001
#% 835741
#% 993982
#% 1015284
#% 1015326
#% 1279488
#! An increasing number of data sources now become available on the Web, but often their contents are only accessible through query interfaces. For a domain of interest, there often exist many such sources with varied coverage or querying capabilities. As an important step to the integration of these sources, we consider the integration of their query interfaces. More specifically, we focus on the crucial step of the integration: accurately matching the interfaces. While the integration of query interfaces has received more attentions recently, current approaches are not sufficiently general: (a) they all model interfaces with flat schemas; (b) most of them only consider 1:1 mappings of fields over the interfaces; (c) they all perform the integration in a blackbox-like fashion and the whole process has to be restarted from scratch if anything goes wrong; and (d) they often require laborious parameter tuning. In this paper, we propose an interactive, clustering-based approach to matching query interfaces. The hierarchical nature of interfaces is captured with ordered trees. Varied types of complex mappings of fields are examined and several approaches are proposed to effectively identify these mappings. We put the human integrator back in the loop and propose several novel approaches to the interactive learning of parameters and the resolution of uncertain mappings. Extensive experiments are conducted and results show that our approach is highly effective.

#index 765410
#* Understanding Web query interfaces: best-effort parsing with hidden syntax
#@ Zhen Zhang;Bin He;Kevin Chen-Chuan Chang
#t 2004
#c 5
#% 3888
#% 96275
#% 240955
#% 244252
#% 257680
#% 445255
#% 480479
#% 480824
#% 534096
#% 619792
#% 654459
#% 654469
#% 732669
#% 1015284
#% 1712590
#! Recently, the Web has been rapidly "deepened" by many searchable databases online, where data are hidden behind query forms. For modelling and integrating Web databases, the very first challenge is to understand what a query interface says- or what query capabilities a source supports. Such automatic extraction of interface semantics is challenging, as query forms are created autonomously. Our approach builds on the observation that, across myriad sources, query forms seem to reveal some "concerted structure," by sharing common building blocks. Toward this insight, we hypothesize the existence of a hidden syntax that guides the creation of query interfaces, albeit from different sources. This hypothesis effectively transforms query interfaces into a visual language with a non-prescribed grammar- and, thus, their semantic understanding a parsing problem. Such a paradigm enables principled solutions for both declaratively representing common patterns, by a derived grammar, and systematically interpreting query forms, by a global parsing mechanism. To realize this paradigm, we must address the challenges of a hypothetical syntax- that it is to be derived, and that it is secondary to the input. At the heart of our form extractor, we thus develop a 2P grammar and a best-effort parser, which together realize a parsing mechanism for a hypothetical syntax. Our experiments show the promise of this approach-it achieves above 85% accuracy for extracting query conditions across random sources.

#index 765411
#* Using the structure of Web sites for automatic segmentation of tables
#@ Kristina Lerman;Lise Getoor;Steven Minton;Craig Knoblock
#t 2004
#c 5
#% 95730
#% 237328
#% 330784
#% 333943
#% 348146
#% 348147
#% 464434
#% 464466
#% 480479
#% 480824
#% 493170
#% 529661
#% 534086
#% 544484
#% 544652
#% 643004
#% 654469
#% 716892
#% 755816
#% 786564
#% 1271981
#% 1414253
#% 1428371
#! Many Web sites, especially those that dynamically generate HTML pages to display the results of a user's query, present information in the form of list or tables. Current tools that allow applications to programmatically extract this information rely heavily on user input, often in the form of labeled extracted records. The sheer size and rate of growth of the Web make any solution that relies primarily on user input is infeasible in the long term. Fortunately, many Web sites contain much explicit and implicit structure, both in layout and content, that we can exploit for the purpose of information extraction. This paper describes an approach to automatic extraction and segmentation of records from Web tables. Automatic methods do not require any user input, but rely solely on the layout and content of the Web source. Our approach relies on the common structure of many Web sites, which present information as a list or a table, with a link in each entry leading to a detail page containing additional information about that item. We describe two algorithms that use redundancies in the content of table and detail pages to aid in information extraction. The first algorithm encodes additional information provided by detail pages as constraints and finds the segmentation by solving a constraint satisfaction problem. The second algorithm uses probabilistic inference to find the record segmentation. We show how each approach can exploit the web site structure in a general, domain-independent manner, and we demonstrate the effectiveness of each algorithm on a set of twelve Web sites.

#index 765412
#* Identifying similarities, periodicities and bursts for online search queries
#@ Michail Vlachos;Christopher Meek;Zografoula Vagena;Dimitrios Gunopulos
#t 2004
#c 5
#% 86950
#% 214595
#% 223859
#% 227937
#% 237204
#% 333941
#% 460862
#% 479462
#% 481279
#% 481460
#% 571043
#% 577220
#% 729943
#% 993965
#! We present several methods for mining knowledge from the query logs of the MSN search engine. Using the query logs, we build a time series for each query word or phrase (e.g., 'Thanksgiving' or 'Christmas gifts') where the elements of the time series are the number of times that a query is issued on a day. All of the methods we describe use sequences of this form and can be applied to time series data generally. Our primary goal is the discovery of semantically similar queries and we do so by identifying queries with similar demand patterns. Utilizing the best Fourier coefficients and the energy of the omitted components, we improve upon the state-of-the-art in time-series similarity matching. The extracted sequence features are then organized in an efficient metric tree index structure. We also demonstrate how to efficiently and accurately discover the important periods in a time-series. Finally we propose a simple but effective method for identification of bursts (long or short-term). Using the burst information extracted from a sequence, we are able to efficiently perform 'query-by-burst' on the database of time-series. We conclude the presentation with the description of a tool that uses the described methods, and serves as an interactive exploratory data discovery tool for the MSN query database.

#index 765413
#* FARMER: finding interesting rule groups in microarray datasets
#@ Gao Cong;Anthony K. H. Tung;Xin Xu;Feng Pan;Jiong Yang
#t 2004
#c 5
#% 248785
#% 273916
#% 280436
#% 299985
#% 310494
#% 338580
#% 469422
#% 479787
#% 481290
#% 631970
#% 729933
#% 729984
#! Microarray datasets typically contain large number of columns but small number of rows. Association rules have been proved to be useful in analyzing such datasets. However, most existing association rule mining algorithms are unable to efficiently handle datasets with large number of columns. Moreover, the number of association rules generated from such datasets is enormous due to the large number of possible column combinations.In this paper, we describe a new algorithm called FARMER that is specially designed to discover association rules from microarray datasets. Instead of finding individual association rules, FARMER finds interesting rule groups which are essentially a set of rules that are generated from the same set of rows. Unlike conventional rule mining algorithms, FARMER searches for interesting rules in the row enumeration space and exploits all user-specified constraints including minimum support, confidence and chi-square to support efficient pruning. Several experiments on real bioinformatics datasets show that FARMER is orders of magnitude faster than previous association rule mining algorithms.

#index 765414
#* Diamond in the rough: finding Hierarchical Heavy Hitters in multi-dimensional data
#@ Graham Cormode;Flip Korn;S. Muthukrishnan;Divesh Srivastava
#t 2004
#c 5
#% 259995
#% 273903
#% 273916
#% 338425
#% 397385
#% 453512
#% 576119
#% 646218
#% 801696
#% 993960
#% 993995
#% 1015293
#! Data items archived in data warehouses or those that arrive online as streams typically have attributes which take values from multiple hierarchies (e.g., time and geographic location; source and destination IP addresses). Providing an aggregate view of such data is important to summarize, visualize, and analyze. We develop the aggregate view based on certain hierarchically organized sets of large-valued regions ("heavy hitters"). Such Hierarchical Heavy Hitters (HHHs) were previously introduced as a crucial aggregation technique in one dimension. In order to analyze the wider range of data warehousing applications and realistic IP data streams, we generalize this problem to multiple dimensions.We identify and study two variants of HHHs for multi-dimensional data, namely the "overlap" and "split" cases, depending on how an aggregate computed for a child node in the multi-dimensional hierarchy is propagated to its parent element(s). For data warehousing applications, we present offline algorithms that take multiple passes over the data and produce the exact HHHs. For data stream applications, we present online algorithms that find approximate HHHs in one pass, with provable accuracy guarantees.We show experimentally, using real and synthetic data, that our proposed online algorithms yield outputs which are very similar (virtually identical, in many cases) to their offline counterparts. The lattice property of the product of hierarchical dimensions ("diamond") is crucially exploited in our online algorithms to track approximate HHHs using only a small, fixed number of statistics per candidate node, regardless of the number of dimensions.

#index 765415
#* Cost-based labeling of groups of mass spectra
#@ Lei Chen;Zheng Huang;Raghu Ramakrishnan
#t 2004
#c 5
#% 152934
#% 210160
#% 210173
#% 216508
#% 280417
#% 300120
#% 308753
#% 320113
#% 344153
#% 346527
#% 351088
#% 376266
#% 464824
#% 480318
#% 481779
#% 594012
#% 739742
#! We make two main contributions in this paper. First, we motivate and introduce a novel class of data mining problems that arise in labeling a group of mass spectra, specifically for analysis of atmospheric aerosols, but with natural applications to market-basket datasets. This builds upon other recent work in which we introduced the problem of labeling a single spectrum, and is motivated by the advent of a new generation of Aerosol Time-of-Flight Spectrometers, which are capable of generating mass spectra for hundreds of aerosol particles per minute. We also describe two algorithms for group labeling, which differ greatly in how they utilize a linear programming (LP) solver, and also differ greatly from algorithms for labeling a single spectrum.Our second contribution is to show how to automatically select between these two algorithms in a cost-based manner, analogous to how a query optimizer selects from a space of query plans. While the details are specific to the labeling problem, we believe that is a promising first step towards a general framework for cost-based data mining, and opens up an important direction for future search.

#index 765416
#* Optimization of query streams using semantic prefetching
#@ Ivan T. Bowman;Kenneth Salem
#t 2004
#c 5
#% 36117
#% 273915
#% 333935
#% 335725
#% 461897
#% 479950
#% 479955
#% 572305
#! Streams of relational queries submitted by client applications to database servers contain patterns that can be used to predict future requests. We present the Scalpel system, which detects these patterns and optimizes request streams using context-based predictions of future requests. Scalpel uses its predictions to provide a form of semantic prefetching, which involves combining a predicted series of requests into a single request that can be issued immediately. Scalpel's semantic prefetching reduces not only the latency experienced by the application but also the total cost of query evaluation. We describe how Scalpel learns to predict optimizable request patterns by observing the application's request stream during a training phase. We also describe the types of query pattern rewrites that Scalpel's cost-based optimizer considers. Finally, we present empirical results that show the costs and benefits of Scalpel's optimizations.

#index 765417
#* Buffering databse operations for enhanced instruction cache performance
#@ Jingren Zhou;Kenneth A. Ross
#t 2004
#c 5
#% 82291
#% 132611
#% 176360
#% 214978
#% 233033
#% 251473
#% 251474
#% 262068
#% 295704
#% 300194
#% 333940
#% 333949
#% 337067
#% 397362
#% 411554
#% 442705
#% 442706
#% 465169
#% 479819
#% 479821
#% 480119
#% 480821
#% 566122
#% 636906
#% 657620
#% 1015288
#! As more and more query processing work can be done in main memory access is becoming a significant cost component of database operations. Recent database research has shown that most of the memory stalls are due to second-level cache data misses and first-level instruction cache misses. While a lot of research has focused on reducing the data cache misses, relatively little research has been done on improving the instruction cache performance of database systems.We first answer the question "Why does a database system incur so many instruction cache misses?" We demonstrate that current demand-pull pipelined query execution engines suffer from significant instruction cache thrashing between different operators. We propose techniques to buffer database operations during query execution to avoid instruction cache thrashing. We implement a new light-weight "buffer" operator and study various factors which may affect the cache performance. We also introduce a plan refinement algorithm that considers the query plan and decides whether it is beneficial to add additional "buffer" operators and where to put them. The benefit is mainly from better instruction locality and better hardware branch prediction. Our techniques can be easily integrated into current database systems without significant changes. Our experiments in a memory-resident PostgreSQL database system show that buffering techniques can reduce the number of instruction cache misses by up to 80% and improve query performance by up to 15%.

#index 765418
#* Rank-aware query optimization
#@ Ihab F. Ilyas;Rahul Shah;Walid G. Aref;Jeffrey Scott Vitter;Ahmed K. Elmagarmid
#t 2004
#c 5
#% 32889
#% 43162
#% 159337
#% 159341
#% 227894
#% 273910
#% 278831
#% 300180
#% 330769
#% 333854
#% 333951
#% 397378
#% 399762
#% 411554
#% 458861
#% 479623
#% 479967
#% 480330
#% 480819
#% 565457
#% 659255
#% 766671
#% 777931
#% 994013
#% 1015325
#! Ranking is an important property that needs to be fully supported by current relational query engines. Recently, several rank-join query operators have been proposed based on rank aggregation algorithms. Rank-join operators progressively rank the join results while performing the join operation. The new operators have a direct impact on traditional query processing and optimization.We introduce a rank-aware query optimization framework that fully integrates rank-join operators into relational query engines. The framework is based on extending the System R dynamic programming algorithm in both enumeration and pruning. We define ranking as an interesting property that triggers the generation of rank-aware query plans. Unlike traditional join operators, optimizing for rank-join operators depends on estimating the input cardinality of these operators. We introduce a probabilistic model for estimating the input cardinality, and hence the cost of a rank-join operator. To our knowledge, this paper is the first effort in estimating the needed input size for optimal rank aggregation algorithms. Costing ranking plans, although challenging, is key to the full integration of rank-join operators in real-world query processing engines. We experimentally evaluate our framework by modifying the query optimizer of an open-source database management system. The experiments show the validity of our framework and the accuracy of the proposed estimation model.

#index 765419
#* Fast computation of database operations using graphics processors
#@ Naga K. Govindaraju;Brandon Lloyd;Wei Wang;Ming Lin;Dinesh Manocha
#t 2004
#c 5
#% 119597
#% 163438
#% 279852
#% 299984
#% 326337
#% 333986
#% 378397
#% 379435
#% 397361
#% 398455
#% 400133
#% 411545
#% 479819
#% 479821
#% 480119
#% 480464
#% 480821
#% 566122
#% 577065
#% 629126
#% 629133
#% 654479
#% 662819
#% 662870
#% 662871
#% 662872
#% 993947
#% 1112720
#! We present new algorithms for performing fast computation of several common database operations on commodity graphics processors. Specifically, we consider operations such as conjunctive selections, aggregations, and semi-linear queries, which are essential computational components of typical database, data warehousing, and data mining applications. While graphics processing units (GPUs) have been designed for fast display of geometric primitives, we utilize the inherent pipelining and parallelism, single instruction and multiple data (SIMD) capabilities, and vector processing functionality of GPUs, for evaluating boolean predicate combinations and semi-linear queries on attributes and executing database operations efficiently. Our algorithms take into account some of the limitations of the programming model of current GPUs and perform no data rearrangements. Our algorithms have been implemented on a programmable GPU (e.g. NVIDIA's GeForce FX 5900) and applied to databases consisting of up to a million records. We have compared their performance with an optimized implementation of CPU-based algorithms. Our experiments indicate that the graphics processor available on commodity computer systems is an effective co-processor for performing database operations.

#index 765420
#* Lazy query evaluation for Active XML
#@ Serge Abiteboul;Omar Benjelloun;Bogdan Cautis;Ioana Manolescu;Tova Milo;Nicoleta Preda
#t 2004
#c 5
#% 101946
#% 273702
#% 273924
#% 378393
#% 397359
#% 397407
#% 404772
#% 464706
#% 464724
#% 479465
#% 480822
#% 570875
#% 572311
#% 577359
#% 654465
#% 654485
#% 801671
#% 993939
#% 994034
#! In this paper, we study query evaluation on Active XML documents (AXML for short), a new generation of XML documents that has recently gained popularity. AXML documents are XML documents whose content is given partly extensionally, by explicit data elements, and partly intensionally, by embedded calls to Web services, which can be invoked to generate data.A major challenge in the efficient evaluation of queries over such documents is to detect which calls may bring data that is relevant for the query execution, and to avoid the materialization of irrelevant information. The problem is intricate, as service calls may be embedded anywhere in the document, and service invocations possibly return data containing calls to new services. Hence, the detection of relevant calls becomes a continuous process. Also, a good analysis must take the service signatures into consideration.We formalize the problem, and provide algorithms to solve it. We also present an implementation that is compliant with XML and Web services standards, and is used as part of the ActiveXML system. Finally, we experimentally measure the performance gains obtained by a careful filtering of the service calls to be triggered.

#index 765421
#* Data stream management for historical XML data
#@ Sujoe Bose;Leonidas Fegaras
#t 2004
#c 5
#% 378388
#% 397349
#% 413563
#% 428155
#% 578560
#% 654455
#% 979303
#% 993949
#% 994015
#% 1015283
#% 1015307
#% 1015324
#! We are presenting a framework for continuous querying of time-varying streamed XML data. A continuous stream in our framework consists of a finite XML document followed by a continuous stream of updates. The unit of update is an XML fragment, which can relate to other fragments through system-generated unique IDs. The reconstruction of temporal data from continuous updates at a current time is never materialized and historical queries operate directly on the fragmented streams. We are incorporating temporal constructs to XQuery with minimal changes to the existing language structure to support continuous querying of time-varying streams of XML data. Our extensions use time projections to capture time-sliding windows, version control for tuple-based windows, and coincidence queries to synchronize events between streams. These XQuery extensions are compiled away to standard XQuery code and the resulting queries operate continuously over the existing fragmented streams.

#index 765422
#* Colorful XML: one hierarchy isn't enough
#@ H. V. Jagadish;Laks V. S. Lakshmanan;Monica Scannapieco;Divesh Srivastava;Nuwee Wiwatwattana
#t 2004
#c 5
#% 154334
#% 210214
#% 236409
#% 268797
#% 333979
#% 340144
#% 378064
#% 378395
#% 378403
#% 397366
#% 397375
#% 413125
#% 465006
#% 479956
#% 480123
#% 480129
#% 488602
#% 570875
#% 570876
#% 659999
#! XML has a tree-structured data model, which is used to uniformly represent structured as well as semi-structured data, and also enable concise query specification in XQuery, via the use of its XPath (twig) patterns. This in turn can leverage the recently developed technology of structural join algorithms to evaluate the query efficiently. In this paper, we identify a fundamental tension in XML data modeling: (i) data represented as deep trees (which can make effective use of twig patterns) are often un-normalized, leading to update anomalies, while (ii) normalized data tends to be shallow, resulting in heavy use of expensive value-based joins in queries.Our solution to this data modeling problem is a novel multi-colored trees (MCT) logical data model, which is an evolutionary extension of the XML data model, and permits trees with multi-colored nodes to signify their participation in multiple hierarchies. This adds significant semantic structure to individual data nodes. We extend XQuery expressions to navigate between structurally related nodes, taking color into account, and also to create new colored trees as restructurings of an MCT database. While MCT serves as a significant evolutionary extension to XML as a logical data model, one of the key roles of XML is for information exchange. To enable exchange of MCT information, we develop algorithms for optimally serializing an MCT database as XML. We discuss alternative physical representations for MCT databases, using relational and native XML databases, and describe an implementation on top of the Timber native XML database. Experimental evaluation, using our prototype implementation, shows that not only are MCT queries/updates more succinct and easier to express than equivalent shallow tree XML queries, but they can also be significantly more efficient to evaluate than equivalent deep and shallow tree XML queries/updates.

#index 765423
#* Approximate XML query answers
#@ Neoklis Polyzotis;Minos Garofalakis;Yannis Ioannidis
#t 2004
#c 5
#% 84549
#% 210173
#% 397364
#% 397379
#% 458836
#% 465018
#% 479806
#% 479984
#% 480306
#% 480488
#% 654453
#% 660000
#% 715568
#% 745463
#% 993968
#% 993970
#% 1015266
#% 1015274
#! The rapid adoption of XML as the standard for data representation and exchange foreshadows a massive increase in the amounts of XML data collected, maintained, and queried over the Internet or in large corporate data-stores. Inevitably, this will result in the development of on-line decision support systems, where users and analysts interactively explore large XML data sets through a declarative query interface (e.g., XQuery or XSLT). Given the importance of remaining interactive, such on-line systems can employ approximate query answers as an effective mechanism for reducing response time and providing users with early feedback. This approach has been successfully used in relational systems and it becomes even more compelling in the XML world, where the evaluation of complex queries over massive tree-structured data is inherently more expensive.In this paper, we initiate a study of approximate query answering techniques for large XML databases. Our approach is based on a novel, conceptually simple, yet very effective XML-summarization mechanism: TREESKETCH synopses. We demonstrate that, unlike earlier techniques focusing solely on selectivity estimation, our TREESKETCH synopses are much more effective in capturing the complete tree structure of the underlying XML database. We propose novel construction algorithms for building TREESKETCH summaries of limited size, and describe schemes for processing general XML twig queries over a concise TREESKETCH in order to produce very fast, approximate tree-structured query answers. To quantify the quality of such approximate answers, we propose a novel, intuitive error metric that captures the quality of the approximation in terms of both the overall structure of the XML tree and the distribution of document edges. Experimental results on real-life and synthetic data sets verify the effectiveness of our TREESKETCH synopses in producing fast, accurate approximate answers and demonstrate their benefits over previously proposed techniques that focus solely on selectivity estimation. In particular, TREESKETCHes yield faster, more accurate approximate answers and selectivity estimates, and are more efficient to construct. To the best of our knowledge, ours is the first work to address the timely problem of producing fast, approximate tree-structured answers for complex XML queries.

#index 765424
#* A bi-level Bernoulli scheme for database sampling
#@ Peter J. Haas;Christian König
#t 2004
#c 5
#% 210188
#% 227883
#% 273908
#% 273909
#% 277347
#% 465162
#% 479931
#! Current database sampling methods give the user insufficient control when processing ISO-style sampling queries. To address this problem, we provide a bi-level Bernoulli sampling scheme that combines the row-level and page-level sampling methods currently used in most commercial systems. By adjusting the parameters of the method, the user can systematically trade off processing speed and statistical precision---the appropriate choice of parameter settings becomes a query optimization problem. We indicate the SQL extensions needed to support bi-level sampling and determine the optimal parameter settings for an important class of sampling queries with explicit time or accuracy constraints. As might be expected, row-level sampling is preferable when data values on each page are homogeneous, whereas page-level sampling should be used when data values on a page vary widely. Perhaps surprisingly, we show that in many cases the optimal sampling policy is of the "bang-bang" type: we identify a "page-heterogeneity index" (PHI) such that optimal sampling is as "row-like" as possible if the PHI is less than 1 and as "page-like" as possible otherwise. The PHI depends upon both the query and the data, and can be estimated by means of a pilot sample. Because pilot sampling can be nontrivial to implement in commercial database systems, we also give a heuristic method for setting the sampling parameters; the method avoids pilot sampling by using a small number of summary statistics that are maintained in the system catalog. Results from over 1100 experiments on 372 real and synthetic data sets show that the heuristic method performs optimally about half of the time, and yields sampling errors within a factor of 2.2 of optimal about 93% of the time. The heuristic method is stable over a wide range of sampling rates and performs best in the most critical cases, where the data is highly clustered or skewed.

#index 765425
#* Effective use of block-level sampling in statistics estimation
#@ Surajit Chaudhuri;Gautam Das;Utkarsh Srivastava
#t 2004
#c 5
#% 82346
#% 102786
#% 116084
#% 210190
#% 248821
#% 277347
#% 299989
#% 427219
#% 481749
#% 482123
#! Block-level sampling is far more efficient than true uniform-random sampling over a large database, but prone to significant errors if used to create database statistics. In this paper, we develop principled approaches to overcome this limitation of block-level sampling for histograms as well as distinct-value estimations. For histogram construction, we give a novel two-phase adaptive method in which the sample size required to reach a desired accuracy is decided based on a first phase sample. This method is significantly faster than previous iterative methods proposed for the same problem. For distinct-value estimation, we show that existing estimators designed for uniform-random samples may perform very poorly if used directly on block-level samples. We present a key technique that computes an appropriate subset of a block-level sample that is suitable for use with most existing estimators. This, to the best of our knowledge, is the first principled method for distinct-value estimation with block-level samples. We provide extensive experimental results validating our methods.

#index 765426
#* Online maintenance of very large random samples
#@ Christopher Jermaine;Abhijit Pol;Subramanian Arumugam
#t 2004
#c 5
#% 1331
#% 18658
#% 77967
#% 86955
#% 210188
#% 227883
#% 273902
#% 273909
#% 273910
#% 300193
#% 300195
#% 333926
#% 333955
#% 379444
#% 397354
#% 411355
#% 420114
#% 465162
#% 480471
#% 481779
#% 503535
#% 654444
#% 654486
#% 1015328
#! Random sampling is one of the most fundamental data management tools available. However, most current research involving sampling considers the problem of how to use a sample, and not how to compute one. The implicit assumption is that a "sample" is a small data structure that is easily maintained as new data are encountered, even though simple statistical arguments demonstrate that very large samples of gigabytes or terabytes in size can be necessary to provide high accuracy. No existing work tackles the problem of maintaining very large, disk-based samples from a data management perspective, and no techniques now exist for maintaining very large samples in an online manner from streaming data. In this paper, we present online algorithms for maintaining on-disk samples that are gigabytes or terabytes in size. The algorithms are designed for streaming data, or for any environment where a large sample must be maintained online in a single pass through a data set. The algorithms meet the strict requirement that the sample always be a true, statistically random sample (without replacement) of all of the data processed thus far. Our algorithms are also suitable for biased or unequal probability sampling.

#index 765427
#* Conditional selectivity for statistics on query expressions
#@ Nicolas Bruno;Surajit Chaudhuri
#t 2004
#c 5
#% 43163
#% 58377
#% 210190
#% 248010
#% 273901
#% 273909
#% 300193
#% 333854
#% 333946
#% 333947
#% 333965
#% 333986
#% 397371
#% 411554
#% 427219
#% 464056
#% 479648
#% 480125
#% 480471
#% 480803
#% 482092
#% 482123
#% 659993
#% 731865
#% 1015334
#! Cardinality estimation during query optimization relies on simplifying assumptions that usually do not hold in practice. To diminish the impact of inaccurate estimates during optimization, statistics on query expressions (SITs) have been previously proposed. These statistics help directly model the distribution of tuples on query sub-plans. Past work in statistics on query expressions has exploited view matching technology to harness their benefits. In this paper we argue against such an approach as it overlooks significant opportunities for improvement in cardinality estimations. We then introduce a framework to reason with SITs based on the notion of conditional selectivity. We present a dynamic programming algorithm to efficiently find the most accurate selectivity estimation for given queries, and discuss how such an approach can be incorporated into existing optimizers with a small number of changes. Finally, we demonstrate experimentally that our technique results in superior cardinality estimations than previous approaches with very little overhead.

#index 765428
#* Transaction support for indexed summary views
#@ Goetz Graefe;Michael Zwilling
#t 2004
#c 5
#% 117
#% 4618
#% 9241
#% 64430
#% 114582
#% 116063
#% 116086
#% 201869
#% 268755
#% 289399
#% 411601
#% 464705
#% 479749
#% 479770
#% 480288
#% 481256
#% 1015304
#! Materialized views have become a standard technique for performance improvement in decision support databases and for a variety of monitoring purposes. In order to avoid inconsistencies and thus unpredictable query results, materialized views and their indexes should be maintained immediately within user transaction just like indexes on ordinary tables. Unfortunately, the smaller a materialized view is, the higher the concurrency contention between queries and updates as well as among concurrent updates. Therefore, we have investigated methods that reduce contention without forcing users to sacrifice serializability and thus predictable application semantics. These methods extend escrow locking with multi-granularity (hierarchical) locking, snapshot transactions, multi-version concurrency control, key range locking, and system transactions, i.e., multiple proven database implementation techniques. The complete design eliminates all contention between pure read transactions and pure update transactions as well as contention among pure update transactions as well as contention among pure update transactions; it enables maximal concurrency of mixed read-write transactions with other transactions; it supports bulk operations such as data import and online index creation; and it provides recovery for transaction, media, and system failures.

#index 765429
#* Graph indexing: a frequent structure-based approach
#@ Xifeng Yan;Philip S. Yu;Jiawei Han
#t 2004
#c 5
#% 344549
#% 378391
#% 397359
#% 435373
#% 443133
#% 466644
#% 479465
#% 480656
#% 629603
#% 629646
#% 629708
#% 654452
#% 660000
#% 729938
#% 729942
#% 731608
#% 1015336
#! Graph has become increasingly important in modelling complicated structures and schemaless data such as proteins, chemical compounds, and XML documents. Given a graph query, it is desirable to retrieve graphs quickly from a large database via graph-based indices. In this paper, we investigate the issues of indexing graphs and propose a novel solution by applying a graph mining technique. Different from the existing path-based methods, our approach, called gIndex, makes use of frequent substructure as the basic indexing feature. Frequent substructures are ideal candidates since they explore the intrinsic characteristics of the data and are relatively stable to database updates. To reduce the size of index structure, two techniques, size-increasing support constraint and discriminative fragments, are introduced. Our performance study shows that gIndex has 10 times smaller index size, but achieves 3--10 times better performance in comparison with a typical path-based method, GraphGrep. The gIndex approach not only provides and elegant solution to the graph indexing problem, but also demonstrates how database indexing and query processing can benefit form data mining, especially frequent pattern mining. Furthermore, the concepts developed here can be applied to indexing sequences, trees, and other complicated structures as well.

#index 765430
#* The Priority R-tree: a practically efficient and worst-case optimal R-tree
#@ Lars Arge;Mark de Berg;Herman J. Haverkort;Ke Yi
#t 2004
#c 5
#% 86950
#% 153260
#% 252304
#% 260073
#% 286237
#% 317933
#% 345611
#% 349452
#% 411694
#% 427199
#% 458741
#% 462059
#% 480093
#% 481428
#% 481455
#% 548489
#% 783997
#! We present the Priority R-tree, or PR-tree, which is the first R-tree variant that always answers a window query using O((N/B)1 1/d + T/B) I/Os, where N is the number of d-dimensional (hyper-) rectangles stored in the R-tree, B is the disk block size, and T is the output size. This is provably asymptotically optimal and significantly better than other R-tree variants, where a query may visit all N/B leaves in the tree even when T = 0. We also present an extensive experimental study of the practical performance of the PR-tree using both real-life and synthetic data. This study shows that the PR-tree performs similar to the best known R-tree variants on real-life and relatively nicely distributed data, but outperforms them significantly on more extreme data.

#index 765431
#* Integrating vertical and horizontal partitioning into automated physical database design
#@ Sanjay Agrawal;Vivek Narasayya;Beverly Yang
#t 2004
#c 5
#% 1327
#% 42276
#% 58381
#% 69094
#% 248815
#% 397397
#% 462204
#% 480158
#% 480298
#% 480821
#% 481290
#% 482100
#% 631950
#% 632100
#% 708321
#% 993967
#% 994007
#! In addition to indexes and materialized views, horizontal and vertical partitioning are important aspects of physical design in a relational database system that significantly impact performance. Horizontal partitioning also provides manageability; database administrators often require indexes and their underlying tables partitioned identically so as to make common operations such as backup/restore easier. While partitioning is important, incorporating partitioning makes the problem of automating physical design much harder since: (a) The choices of partitioning can strongly interact with choices of indexes and materialized views. (b) A large new space of physical design alternatives must be considered. (c) Manageability requirements impose a new constraint on the problem. In this paper, we present novel techniques for designing a scalable solution to this integrated physical design problem that takes both performance and manageability into account. We have implemented our techniques and evaluated it on Microsoft SQL Server. Our experiments highlight: (a) the importance of taking an integrated approach to automated physical design and (b) the scalability of our techniques.

#index 765432
#* Constraint-based XML query rewriting for data integration
#@ Cong Yu;Lucian Popa
#t 2004
#c 5
#% 583
#% 6259
#% 198465
#% 237189
#% 237190
#% 264858
#% 273924
#% 283052
#% 299967
#% 378409
#% 397393
#% 411759
#% 465053
#% 465057
#% 480134
#% 480657
#% 480822
#% 509696
#% 564416
#% 572311
#% 572314
#% 654464
#% 770338
#% 993981
#% 1015271
#! We study the problem of answering queries through a target schema, given a set of mappings between one or more source schemas and this target schema, and given that the data is at the sources. The schemas can be any combination of relational or XML schemas, and can be independently designed. In addition to the source-to-target mappings, we consider as part of the mapping scenario a set of target constraints specifying additional properties on the target schema. This becomes particularly important when integrating data from multiple data sources with overlapping data and when such constraints can express data merging rules at the target. We define the semantics of query answering in such an integration scenario, and design two novel algorithms, basic query rewrite and query resolution, to implement the semantics. The basic query rewrite algorithm reformulates target queries in terms of the source schemas, based on the mappings. The query resolution algorithm generates additional rewritings that merge related information from multiple sources and assemble a coherent view of the data, by incorporating target constraints. The algorithms are implemented and then evaluated using a comprehensive set of experiments based on both synthetic and real-life data integration scenarios.

#index 765433
#* iMAP: discovering complex semantic matches between database schemas
#@ Robin Dhamankar;Yoonkyong Lee;AnHai Doan;Alon Halevy;Pedro Domingos
#t 2004
#c 5
#% 115608
#% 174161
#% 248800
#% 279755
#% 307632
#% 333988
#% 333990
#% 378409
#% 397369
#% 465896
#% 479783
#% 480645
#% 488766
#% 551850
#% 572314
#% 637829
#% 654458
#% 654459
#% 660001
#% 993982
#% 1275347
#! Creating semantic matches between disparate data sources is fundamental to numerous data sharing efforts. Manually creating matches is extremely tedious and error-prone. Hence many recent works have focused on automating the matching process. To date, however, virtually all of these works deal only with one-to-one (1-1) matches, such as address = location. They do not consider the important class of more complex matches, such as address = concat (city, state) and room-pric = room-rate* (1 + tax-rate).We describe the iMAP system which semi-automatically discovers both 1-1 and complex matches. iMAP reformulates schema matching as a search in an often very large or infinite match space. To search effectively, it employs a set of searchers, each discovering specific types of complex matches. To further improve matching accuracy, iMAP exploits a variety of domain knowledge, including past complex matches, domain integrity constraints, and overlap data. Finally, iMAP introduces a novel feature that generates explanation of predicted matches, to provide insights into the matching process and suggest actions to converge on correct matches quickly. We apply iMAP to several real-world domains to match relational tables, and show that it discovers both 1-1 and complex matches with high accuracy.

#index 765434
#* Adapting to source properties in processing data integration queries
#@ Zachary G. Ives;Alon Y. Halevy;Daniel S. Weld
#t 2004
#c 5
#% 172900
#% 172902
#% 227883
#% 248793
#% 248795
#% 273910
#% 273911
#% 273912
#% 300167
#% 397353
#% 397371
#% 411355
#% 479452
#% 479617
#% 479928
#% 480642
#% 480803
#% 481288
#% 481749
#% 570880
#% 571294
#% 572311
#% 715955
#% 1015282
#! An effective query optimizer finds a query plan that exploits the characteristics of the source data. In data integration, little is known in advance about sources' properties, which necessitates the use of adaptive query processing techniques to adjust query processing on-the-fly. Prior work in adaptive query processing has focused on compensating for delays and adjusting for mis-estimated cardinality or selectivity values. In this paper, we present a generalized architecture for adaptive query processing and introduce a new technique, called adaptive data partitioning (ADP), which is based on the idea of dividing the source data into regions, each executed by different, complementary plans. We show how this model can be applied in novel ways to not only correct for underestimated selectivity and cardinality values, but also to discover and exploit order in the source data, and to detect and exploit source data that can be effectively pre-aggregated. We experimentally compare a number of alternative strategies and show that our approach is effective.

#index 765435
#* Adaptive ordering of pipelined stream filters
#@ Shivnath Babu;Rajeev Motwani;Kamesh Munagala;Itaru Nishizawa;Jennifer Widom
#t 2004
#c 5
#% 248793
#% 248795
#% 249985
#% 273911
#% 285924
#% 287461
#% 300167
#% 300179
#% 310900
#% 333938
#% 378397
#% 397353
#% 397371
#% 453556
#% 479938
#% 480803
#% 480944
#% 511303
#% 654497
#% 715955
#% 742047
#% 853011
#% 993949
#% 1015278
#% 1015296
#! We consider the problem of pipelined filters, where a continuous stream of tuples is processed by a set of commutative filters. Pipelined filters are common in stream applications and capture a large class of multiway stream joins. We focus on the problem of ordering the filters adaptively to minimize processing cost in an environment where stream and filter characteristics vary unpredictably over time. Our core algorithm, A-Greedy (for Adaptive Greedy), has strong theoretical guarantees: If stream and filter characteristics were to stabilize, A-Greedy would converge to an ordering within a small constant factor of optimal. (In experiments A-Greedy usually converges to the optimal ordering.) One very important feature of A-Greedy is that it monitors and responds to selectivities that are correlated across filters (i.e., that are nonindependent), which provides the strong quality guarantee but incurs run-time overhead. We identify a three-way tradeoff among provable convergence to good orderings, run-time overhead, and speed of adaptivity. We develop a suite of variants of A-Greedy that lie at different points on this tradeoff spectrum. We have implemented all our algorithms in the STREAM prototype Data Stream Management System and a thorough performance evaluation is presented.

#index 765436
#* Static optimization of conjunctive queries with sliding windows over infinite streams
#@ Ahmed M. Ayad;Jeffrey F. Naughton
#t 2004
#c 5
#% 554
#% 1252
#% 86949
#% 248793
#% 248795
#% 249985
#% 273910
#% 273911
#% 300167
#% 300179
#% 340635
#% 378388
#% 378408
#% 397352
#% 397353
#% 411554
#% 654444
#% 654462
#% 654482
#% 726621
#% 745534
#% 993948
#% 1015278
#% 1015279
#% 1015280
#% 1015282
#! We define a framework for static optimization of sliding window conjunctive queries over infinite streams. When computational resources are sufficient, we propose that the goal of optimization should be to find an execution plan that minimizes resource usage within the available resource constraints. When resources are insufficient, on the other hand, we propose that the goal should be to find an execution plan that sheds some of the input load (by randomly dropping tuples) to keep resource usage within bounds while maximizing the output rate. An intuitive approach to load shedding suggests starting with the plan that would be optimal if resources were sufficient and adding "drop boxes" to this plan. We find this to be often times suboptimal - in many instances the optimal partial answer plan results from adding drop boxes to plans that are not optimal in the unlimited resource case. In view of this, we use our framework to investigate an approach to optimization that unifies the placement of drop boxes and the choice of the query plan from which to drop tuples. The effectiveness of our optimizer is experimentally validated and the results show the promise of this approach.

#index 765437
#* Dynamic plan migration for continuous queries over data streams
#@ Yali Zhu;Elke A. Rundensteiner;George T. Heineman
#t 2004
#c 5
#% 159341
#% 172900
#% 248793
#% 273910
#% 378388
#% 397352
#% 397353
#% 464233
#% 480955
#% 570880
#% 578391
#% 617870
#% 632755
#% 654462
#% 659996
#% 993948
#% 993949
#% 1015279
#! Dynamic plan migration is concerned with the on-the-fly transition from one continuous query plan to a semantically equivalent yet more efficient plan. Migration is important for stream monitoring systems where long-running queries may have to withstand fluctuations in stream workloads and data characteristics. Existing migration methods generally adopt a pause-drain-resume strategy that pauses the processing of new data, purges all old data in the existing plan, until finally the new plan can be plugged into the system. However, these existing strategies do not address the problem of migrating query plans that contain stateful operators, such as joins. We now develop solutions for online plan migration for continuous stateful plans. In particular, in this paper, we propose two alternative strategies, called the moving state strategy and the parallel track strategy, one exploiting reusability and the second employs parallelism to seamlessly migrate between continuous join plans without affecting the results of the query. We develop cost models for both migration strategies to analytically compare them. We embed these migration strategies into the CAPE [7], a prototype system of a stream query engine, and conduct a comparative experimental study to evaluate these two strategies for window-based join plans. Our experimental results illustrate that the two strategies can vary significantly in terms of output rates and intermediate storage spaces given distinct system configurations and stream workloads.

#index 765438
#* Clustering objects on a spatial network
#@ Man Lung Yiu;Nikos Mamoulis
#t 2004
#c 5
#% 36672
#% 210173
#% 248790
#% 273890
#% 274612
#% 316709
#% 410276
#% 421124
#% 438137
#% 442858
#% 443105
#% 443208
#% 464223
#% 480812
#% 481281
#! Clustering is one of the most important analysis tasks in spatial databases. We study the problem of clustering objects, which lie on edges of a large weighted spatial network. The distance between two objects is defined by their shortest path distance over the network. Past algorithms are based on the Euclidean distance and cannot be applied for this setting. We propose variants of partitioning, density-based, and hierarchical methods. Their effectiveness and efficiency is evaluated for collections of objects which appear on real road networks. The results show that our methods can correctly identify clusters and they are scalable for large problems.

#index 765439
#* Computing Clusters of Correlation Connected objects
#@ Christian Böhm;Karin Kailing;Peer Kröger;Arthur Zimek
#t 2004
#c 5
#% 248792
#% 273890
#% 273891
#% 280417
#% 300131
#% 310537
#% 397382
#% 397384
#% 420078
#% 462243
#% 480307
#% 481956
#% 632035
#% 659967
#% 727882
#% 727908
#! The detection of correlations between different features in a set of feature vectors is a very important data mining task because correlation indicates a dependency between the features or some association of cause and effect between them. This association can be arbitrarily complex, i.e. one or more features might be dependent from a combination of several other features. Well-known methods like the principal components analysis (PCA) can perfectly find correlations which are global, linear, not hidden in a set of noise vectors, and uniform, i.e. the same type of correlation is exhibited in all feature vectors. In many applications such as medical diagnosis, molecular biology, time sequences, or electronic commerce, however, correlations are not global since the dependency between features can be different in different subgroups of the set. In this paper, we propose a method called 4C (Computing Correlation Connected Clusters) to identify local subgroups of the data objects sharing a uniform but arbitrarily complex correlation. Our algorithm is based on a combination of PCA and density-based clustering (DBSCAN). Our method has a determinate result and is robust against noise. A broad comparative evaluation demonstrates the superior performance of 4C over competing methods such as DBSCAN, CLIQUE and ORCLUS.

#index 765440
#* Incremental and effective data summarization for dynamic hierarchical clustering
#@ Samer Nassar;Jörg Sander;Corrine Cheng
#t 2004
#c 5
#% 210173
#% 232768
#% 273890
#% 280404
#% 333933
#% 345857
#% 345859
#% 465160
#% 479658
#% 566870
#% 629674
#% 632009
#% 659972
#% 925064
#% 1015261
#% 1390149
#! Mining informative patterns from very large, dynamically changing databases poses numerous interesting challenges. Data summarizations (e.g., data bubbles) have been proposed to compress very large static databases into representative points suitable for subsequent effective hierarchical cluster analysis. In many real world applications, however, the databases dynamically change due to frequent insertions and deletions, possibly changing the data distribution and clustering structure over time. Completely reapplying both the data summarization and the clustering algorithm to detect the changes in the clustering structure and update the uncovered data patterns following such deletions and insertions is prohibitively expensive for large fast changing databases. In this paper, we propose a new scheme to maintain data bubbles incrementally. By using incremental data bubbles, a high-quality hierarchical clustering is quickly available at any point in time. In our scheme, a quality measure for incremental data bubbles is used to identify data bubbles that do not compress well their underlying data points after certain insertions and deletions. Only these data bubbles are re-built using efficient split and merge operations. An extensive experimental evaluation shows that the incremental data bubbles provide significantly faster data summarization than completely re-building the data bubbles after a certain number of insertions and deletions, and are effective in preserving (and in some cases even improving) the quality of the data summarization.

#index 765441
#* Implementing a scalable XML publish/subscribe system using relational database systems
#@ Feng Tian;Berthold Reinwald;Hamid Pirahesh;Tobias Mayr;Jussi Myllymaki
#t 2004
#c 5
#% 36117
#% 146399
#% 151529
#% 204908
#% 271199
#% 300179
#% 463117
#% 465061
#% 480296
#% 631962
#% 654476
#% 654477
#% 659987
#% 659995
#% 659996
#! An XML publish/subscribe system needs to match many XPath queries (subscriptions) over published XML documents. The performance and scalability of the matching algorithm is essential for the system when the number of XPath subscriptions is large. Earlier solutions to this problem usually built large finite state automata for all the XPath subscriptions in memory. The scalability of this approach is limited by the amount of available physical memory. In this paper, we propose an implementation that uses a relational database as the matching engine. The heavy lifting part of evaluating a large number of subscriptions is done inside a relational database using indices and joins. We described several different implementation strategies and presented a performance evaluation. The system shows very good performance and scalability in our experiments, handling millions of subscriptions with moderate amount of physical memory.

#index 765442
#* Incremental maintenance of XML structural indexes
#@ Ke Yi;Hao He;Ioana Stanoi;Jun Yang
#t 2004
#c 5
#% 31484
#% 397379
#% 462235
#% 479465
#% 479806
#% 480488
#% 488720
#% 511653
#% 528124
#% 654452
#% 745468
#! Increasing popularity of XML in recent years has generated much interest in query processing over graph-structured data. To support efficient evaluation of path expressions, many structural indexes have been proposed. The most popular ones are the 1-index, based on the notion of graph bisimilarity, and the recently proposed A(k)-index, based on the notion of local similarity to provide a trade-off between index size and query answering power. For these indexes to be practical, we need effective and efficient incremental maintenance algorithms to keep them consistent with the underlying data. However, existing update algorithms for structural indexes essentially provide no guarantees on the quality of the index; the updated index is usually larger size than necessary, degrading the performance for subsequent queries.In this paper, we propose update algorithms for the 1-index and the A(k)-index with provable guarantees on the resulting index quality. Our algorithms always maintain a minimal index, i.e., merging any two index nodes would result in an incorrect index. For the 1-index, if the data graph is acyclic, our algorithm further ensures that the index is minimum, i.e., it has the least number of index nodes possible. For the A(k)-index, we show that the minimal index our algorithm maintains is also the unique minimum A(k)-index, for both acyclic and cyclic data graphs. Finally, through experimental evaluation, we demonstrate that our algorithms bring significant improvement over previous methods, in terms of both index size and update time.

#index 765443
#* Incremental evaluation of schema-directed XML publishing
#@ Philip Bohannon;Byron Choi;Wenfei Fan
#t 2004
#c 5
#% 59350
#% 142231
#% 152928
#% 201928
#% 201930
#% 286916
#% 300177
#% 333935
#% 335500
#% 443232
#% 462051
#% 462213
#% 464988
#% 465059
#% 479629
#% 480317
#% 480816
#% 481128
#% 482083
#% 564207
#% 654464
#% 993941
#% 994001
#! When large XML documents published from a database are maintained externally, it is inefficient to repeatedly recompute them when the database is updated. Vastly preferable is incremental update, as common for views stored in a data warehouse. However, to support schema-directed publishing, there may be no simple query that defines the mapping from the database to the external document. To meet the need for efficient incremental update, this paper studies two approaches for incremental evaluation of ATGs [4], a formalism for schema-directed XML publishing. The reduction approach seeks to push as much work as possible to the underlying DBMS. It is based on a relational encoding of XML trees and a nontrivial translation of ATGs to SQL 99 queries with recursion. However, a weakness of this approach is that it relies on high-end DBMS features rather than the lowest common denominator. In contrast, the bud-cut approach pushes only simple queries to the DBNS and performs the bulk of the work in middleware. It capitalizes on the tree-structure of XML views to minimize unnecessary recomputations and leverages optimization techniques developed for XML publishing. While implementation of the reduction approach is not yet in the reach of commercial DBMS, we have implemented the bud-cut approach and experimentally evaluated its performance compared to recomputation.

#index 765444
#* The price of validity in dynamic networks
#@ Mayank Bawa;Aristides Gionis;Hector Garcia-Molina;Rajeev Motwani
#t 2004
#c 5
#% 35764
#% 214073
#% 227883
#% 237196
#% 281556
#% 281557
#% 297915
#% 300078
#% 309433
#% 340175
#% 401980
#% 433981
#% 480965
#% 505869
#% 523065
#% 545923
#% 569762
#% 577219
#% 622728
#% 656792
#% 723903
#% 745442
#% 805466
#% 805467
#! Massive-scale self-administered networks like Peer-to-Peer and Sensor Networks have data distributed across thousands of participant hosts. These networks are highly dynamic with short-lived hosts being the norm rather than an exception. In recent years, researchers have investigated best-effort algorithms to efficiently process aggregate queries (e.g., sum, count, average, minimum and maximum) [6, 13, 21, 34, 35, 37] on these networks. Unfortunately, query semantics for best-effort algorithms are ill-defined, making it hard to reason about guarantees associated with the result returned. In this paper, we specify a correctness condition, single-site validity, with respect to which the above algorithms are best-effort. We present a class of algorithms that guarantee validity in dynamic networks. Experiments on real-life and synthetic network topologies validate performance of our algorithms, revealing the hitherto unknown price of validity.

#index 765445
#* Compressing historical information in sensor networks
#@ Antonios Deligiannakis;Yannis Kotidis;Nick Roussopoulos
#t 2004
#c 5
#% 210190
#% 248822
#% 273902
#% 273903
#% 281556
#% 300179
#% 333863
#% 339227
#% 342371
#% 397352
#% 397389
#% 413604
#% 427022
#% 480306
#% 480332
#% 480634
#% 482092
#% 576978
#% 635986
#% 654482
#% 654487
#% 654488
#% 805466
#% 993958
#! We are inevitably moving into a realm where small and inexpensive wireless devices would be seamlessly embedded in the physical world and form a wireless sensor network in order to perform complex monitoring and computational tasks. Such networks pose new challenges in data processing and dissemination because of the limited resources (processing, bandwidth, energy) that such devices possess. In this paper we propose a new technique for compressing multiple streams containing historical data from each sensor. Our method exploits correlation and redundancy among multiple measurements on the same sensor and achieves high degree of data reduction while managing to capture even the smallest details of the recorded measurements. The key to our technique is the base signal, a series of values extracted from the real measurements, used for encoding piece-wise linear correlations among the collected data values. We provide efficient algorithms for extracting the base signal features from the data and for encoding the measurements using these features. Our experiments demonstrate that our method by far outperforms standard approximation techniques like Wavelets. Histograms and the Discrete Cosine Transform, on a variety of error metrics and for real datasets from different domains.

#index 765446
#* Efficient query reformulation in peer data management systems
#@ Igor Tatarinov;Alon Halevy
#t 2004
#c 5
#% 237181
#% 248038
#% 261741
#% 296931
#% 309851
#% 333989
#% 333990
#% 378393
#% 378409
#% 384978
#% 449218
#% 480646
#% 572311
#% 572314
#% 577359
#% 723449
#% 723450
#% 801676
#% 993982
#% 1015271
#% 1015302
#! Peer data management systems (PDMS) offer a flexible architecture for decentralized data sharing. In a PDMS, every peer is associated with a schema that represents the peer's domain of interest, and semantic relationships between peers are provided locally between pairs (or small sets) of peers. By traversing semantic paths of mappings, a query over one peer can obtain relevant data from any reachable peer in the network. Semantic paths are traversed by reformulating queries at a peer into queries on its neighbors.Naively following semantic paths is highly inefficient in practice. We describe several techniques for optimizing the reformulation process in a PDMS and validate their effectiveness using real-life data sets. In particular, we develop techniques for pruning paths in the reformulation process and for minimizing the reformulated queries as they are created. In addition, we consider the effect of the strategy we use to search through the space of reformulations. Finally, we show that pre-computing semantic paths in a PDMS can greatly improve the efficiency of the reformulation process. Together, all of these techniques form a basis for scalable query reformulation in PDMS.To enable our optimizations, we developed practical algorithms, of independent interest, for checking containment and minimization of XML queries, and for composing XML mappings.

#index 765447
#* Extending query rewriting techniques for fine-grained access control
#@ Shariq Rizvi;Alberto Mendelzon;S. Sudarshan;Prasan Roy
#t 2004
#c 5
#% 102749
#% 137867
#% 164560
#% 211987
#% 243322
#% 273696
#% 273923
#% 300138
#% 300166
#% 320870
#% 333965
#% 340030
#% 340827
#% 443382
#% 462501
#% 464056
#% 479792
#% 481604
#% 482081
#% 488476
#% 565457
#% 572311
#% 575966
#% 646027
#! Current day database applications, with large numbers of users, require fine-grained access control mechanisms, at the level of individual tuples, not just entire relations/views, to control which parts of the data can be accessed by each user. Fine-grained access control is often enforced in the application code, which has numerous drawbacks; these can be avoided by specifying/enforcing access control at the database level. We present a novel fine-grained access control model based on authorization views that allows "authorization-transparent" querying; that is, user queries can be phrased in terms of the database relations, and are valid if they can be answered using only the information contained in these authorization views. We extend earlier work on authorization-transparent querying by introducing a new notion of validity, conditional validity. We give a powerful set of inference rules to check for query validity. We demonstrate the practicality of our techniques by describing how an existing query optimizer can be extended to perform access control checks by incorporating these inference rules.

#index 765448
#* Order preserving encryption for numeric data
#@ Rakesh Agrawal;Jerry Kiernan;Ramakrishnan Srikant;Yirong Xu
#t 2004
#c 5
#% 25450
#% 111315
#% 129023
#% 132779
#% 232409
#% 350873
#% 369349
#% 374401
#% 397367
#% 459008
#% 480125
#% 664705
#% 725292
#% 993942
#% 993943
#% 994006
#% 1015256
#! Encryption is a well established technology for protecting sensitive data. However, once encrypted, data can no longer be easily queried aside from exact matches. We present an order-preserving encryption scheme for numeric data that allows any comparison operation to be directly applied on encrypted data. Query results produced are sound (no false hits) and complete (no false drops). Our scheme handles updates gracefully and new values can be added without requiring changes in the encryption of other values. It allows standard databse indexes to be built over encrypted tables and can easily be integrated with existing database systems. The proposed scheme has been designed to be deployed in application environments in which the intruder can get access to the encrypted database, but does not have prior domain information such as the distribution of values and annot encrypt or decrypt arbitrary values of his choice. The encryption is robust against estimation of the true value in such environments.

#index 765449
#* A formal analysis of information disclosure in data exchange
#@ Gerome Miklau;Dan Suciu
#t 2004
#c 5
#% 59350
#% 67453
#% 164364
#% 195456
#% 266230
#% 268764
#% 291870
#% 333986
#% 374401
#% 481128
#% 572311
#% 576761
#! We perform a theoretical study of the following query-view security problem: given a view V to be published, does V logically disclose information about a confidential query S? The problem is motivated by the need to manage the risk of unintended information disclosure in today's world of universal data exchange. We present a novel information-theoretic standard for query-view security. This criterion can be used to provide a precise analysis of information disclosure for a host of data exchange scenarios, including multi-party collusion and the use of outside knowledge by an adversary trying to learn privileged facts about the database. We prove a number of theoretical results for deciding security according to this standard. We also generalize our security criterion to account for prior knowledge a user or adversary may possess, and introduce techniques for measuring the magnitude of partical disclosures. We believe these results can be a foundation for practical efforts to secure data exchange frameworks, and also illuminate a nice interaction between logic and probability theory.

#index 765450
#* Secure XML querying with security views
#@ Wenfei Fan;Chee-Yong Chan;Minos Garofalakis
#t 2004
#c 5
#% 164560
#% 273924
#% 291299
#% 333989
#% 344639
#% 369768
#% 378393
#% 379248
#% 397374
#% 411759
#% 462235
#% 465053
#% 494906
#% 564264
#% 564416
#% 725290
#! The prevalent use of XML highlights the need for a generic, flexible access-control mechanism for XML documents that supports efficient and secure query access, without revealing sensitive information unauthorized users. This paper introduces a novel paradigm for specifying XML security constraints and investigates the enforcement of such constraints during XML query evaluation. Our approach is based on the novel concept of security views, which provide for each user group (a) an XML view consisting of all and only the information that the users are authorized to access, and (b) a view DTD that the XML view conforms to. Security views effectively protect sensitive data from access and potential inferences by unauthorized user, and provide authorized users with necessary schema information to facilitate effective query formulation and optimization. We propose an efficient algorithm for deriving security view definitions from security policies (defined on the original document DTD) for different user groups. We also develop novel algorithms for XPath query rewriting and optimization such that queries over security views can be efficiently answered without materializing the views. Our algorithms transform a query over a security view to an equivalent query over the original document, and effectively prune query nodes by exploiting the structural properties of the document DTD in conjunction with approximate XPath containment tests. Our work is the first to study a flexible, DTD-based access-control model for XML and its implications on the XML query-execution engine. Furthermore, it is among the first efforts for query rewriting and optimization in the presence of general DTDs for a rich a class of XPath queries. An empirical study based on real-life DTDs verifies the effectiveness of our approach.

#index 765451
#* Indexing spatio-temporal trajectories with Chebyshev polynomials
#@ Yuhan Cai;Raymond Ng
#t 2004
#c 5
#% 4178
#% 172949
#% 201876
#% 227924
#% 248798
#% 252304
#% 273706
#% 316560
#% 333941
#% 334059
#% 458857
#% 461885
#% 464851
#% 479649
#% 480146
#% 480307
#% 481609
#% 503869
#% 631923
#% 632089
#% 659936
#% 659961
#% 659971
#! In this paper, we attempt to approximate and index a d- dimensional (d ≥ 1) spatio-temporal trajectory with a low order continuous polynomial. There are many possible ways to choose the polynomial, including (continuous)Fourier transforms, splines, non-linear regressino, etc. Some of these possiblities have indeed been studied beofre. We hypothesize that one of the best possibilities is the polynomial that minimizes the maximum deviation from the true value, which is called the minimax polynomial. Minimax approximation is particularly meaningful for indexing because in a branch-and-bound search (i.e., for finding nearest neighbours), the smaller the maximum deviation, the more pruning opportunities there exist. However, in general, among all the polynomials of the same degree, the optimal minimax polynomial is very hard to compute. However, it has been shown thta the Chebyshev approximation is almost identical to the optimal minimax polynomial, and is easy to compute [16]. Thus, in this paper, we explore how to use the Chebyshev polynomials as a basis for approximating and indexing d-dimenstional trajectories.The key analytic result of this paper is the Lower Bounding Lemma. that is, we show that the Euclidean distance between two d-dimensional trajectories is lower bounded by the weighted Euclidean distance between the two vectors of Chebyshev coefficients. this lemma is not trivial to show, and it ensures that indexing with Chebyshev cofficients aedmits no false negatives. To complement that analystic result, we conducted comprehensive experimental evaluation with real and generated 1-dimensional to 4-dimensional data sets. We compared the proposed schem with the Adaptive Piecewise Constant Approximation (APCA) scheme. Our preliminary results indicate that in all situations we tested, Chebyshev indexing dominates APCA in pruning power, I/O and CPU costs.

#index 765452
#* Prediction and indexing of moving objects with unknown motion patterns
#@ Yufei Tao;Christos Faloutsos;Dimitris Papadias;Bin Liu
#t 2004
#c 5
#% 86950
#% 137887
#% 273706
#% 299979
#% 300174
#% 397377
#% 397386
#% 576115
#% 1015297
#! Existing methods for peediction spatio-temporal databases assume that objects move according to linear functions. This severely limits their applicability, since in practice movement is more complex, and individual objects may follow drastically diffferent motion patterns. In order to overcome these problems, we first introduce a general framework for monitoring and indexing moving objects, where (i) each boject computes individually the function that accurately captures its movement and (ii) a server indexes the object locations at a coarse level and processes queries using a filter-refinement mechanism. Our second contribution is a novel recursive motion function that supports a broad class of non-linear motion patterns. The function does not presume any a-priori movement but can postulate the particular motion of each object by examining its locations at recent timestamps. Finally. we propse an efficient indexing scheme that faciliates the processing of predicitive queries without false misses.

#index 765453
#* SINA: scalable incremental processing of continuous queries in spatio-temporal databases
#@ Mohamed F. Mokbel;Xiaopeing Xiong;Walid G. Aref
#t 2004
#c 5
#% 36117
#% 86950
#% 116082
#% 152937
#% 210187
#% 287466
#% 300174
#% 300179
#% 318051
#% 340635
#% 421124
#% 427199
#% 442615
#% 458853
#% 495433
#% 527187
#% 527191
#% 554884
#% 567868
#% 654478
#% 745458
#% 745488
#% 1015305
#% 1015320
#! This paper intoduces the Scalable INcremental hash-based Algorithm (SINA, for short); a new algorithm for evaluting a set of concurrent continuous spatio-temporal queries. SINA is designed with two goals in mind: (1) Scalability in terms of the number of concurrent continuous spatio-temporal queries, and (2) Incremental evaluation of continyous spatio-temporal queries. SINA achieves scalability by empolying a shared execution paradigm where the execution of continuous spatio-temporal queries is abstracted as a spatial join between a set of moving objects and a set of moving queries. Incremental evaluation is achived by computing only the updates of the previously reported answer. We introduce two types of updaes, namely positive and negative updates. Positive or negative updates indicate that a certain object should be added to or removed from the previously reported answer, respectively. SINA manages the computation of postive and negative updates via three phases: the hashing phase, the invalidation phase, and the joining phase. the hashing phase employs an in-memory hash-based join algorithm that results in a set a positive upldates. The invalidation phase is triggered every T seconds or when the memory is fully occupied to produce a set of negative updates. Finally, the joining phase is triggered by the end of the invalidation phase to produce a set of both positive and negative updates that result from joining in-memory data with in-disk data. Experimental results show that SINA is scalable and is more efficient than other index-based spatio-temporal algorithms.

#index 765454
#* STRIPES: an efficient index for predicted trajectories
#@ Jignesh M. Patel;Yun Chen;V. Prasad Chakka
#t 2004
#c 5
#% 86950
#% 172939
#% 273706
#% 296090
#% 299979
#% 300174
#% 315005
#% 318051
#% 319508
#% 427199
#% 442615
#% 443328
#% 443430
#% 480423
#% 480473
#% 480817
#% 481956
#% 503869
#% 510512
#% 527195
#% 554884
#% 555050
#% 572293
#% 660007
#% 1015305
#% 1015320
#% 1180064
#! Moving object databases are required to support queries on a large number of continuously moving objects. A key requirement for indexing methods in this domain is to efficiently support both update and query operations. Previous work on indexing such databases can be broadly divided into categories: indexing the past positions and indexing the future predicted positions. In this paper we focus on an efficient indexing method for indexing the future positions of moving objects.In this paper we propose an indexing method, called STRIPES, which indexes predicted trajectories in a dual transformed space. Trajectories for objects in d-dimensional space become points in a higher-dimensional 2d-space. This dual transformed space is then indexed using a regular hierarchical grid decomposition indexing structure. STRIPES can evaluate a range of queries including time-slice, window, and moving queries. We have carried out extensive experimental evaluation comparing the performance of STRIPES with the best known existing predicted trajectory index (the TPR*-tree), and show that our approach is significantly faster than TPR*-tree for both updates and search queries.

#index 765455
#* CORDS: automatic discovery of correlations and soft functional dependencies
#@ Ihab F. Ilyas;Volker Markl;Peter Haas;Paul Brown;Ashraf Aboulnaga
#t 2004
#c 5
#% 131546
#% 210160
#% 240222
#% 273901
#% 333946
#% 333947
#% 333986
#% 397371
#% 480803
#% 1015285
#% 1015310
#! The rich dependency structure found in the columns of real-world relational databases can be exploited to great advantage, but can also cause query optimizers---which usually assume that columns are statistically independent---to underestimate the selectivities of conjunctive predicates by orders of magnitude. We introduce CORDS, an efficient and scalable tool for automatic discovery of correlations and soft functional dependencies between columns. CORDS searches for column pairs that might have interesting and useful dependency relations by systematically enumerating candidate pairs and simultaneously pruning unpromising candidates using a flexible set of heuristics. A robust chi-squared analysis is applied to a sample of column values in order to identify correlations, and the number of distinct values in the sampled columns is analyzed to detect soft functional dependencies. CORDS can be used as a data mining tool, producing dependency graphs that are of intrinsic interest. We focus primarily on the use of CORDS in query optimization. Specifically, CORDS recommends groups of columns on which to maintain certain simple joint statistics. These "column-group" statistics are then used by the optimizer to avoid naive selectivity estimates based on inappropriate independence assumptions. This approach, because of its simplicity and judicious use of sampling, is relatively easy to implement in existing commercial systems, has very low overhead, and scales well to the large numbers of columns and large table sizes found in real-world databases. Experiments with a prototype implementation show that the use of CORDS in query optimization can speed up query execution times by an order of magnitude. CORDS can be used in tandem with query feedback systems such as the LEO learning optimizer, leveraging the infrastructure of such systems to correct bad selectivity estimates and ameliorating the poor performance of feedback systems during slow learning phases.

#index 765456
#* Robust query processing through progressive optimization
#@ Volker Markl;Vijayshankar Raman;David Simmen;Guy Lohman;Hamid Pirahesh;Miso Cilimdzic
#t 2004
#c 5
#% 54023
#% 58375
#% 102784
#% 137882
#% 169841
#% 172900
#% 210190
#% 248793
#% 248795
#% 286820
#% 287664
#% 300138
#% 300167
#% 397353
#% 397371
#% 411554
#% 480803
#% 482092
#% 565457
#% 715955
#! Virtually every commercial query optimizer chooses the best plan for a query using a cost model that relies heavily on accurate cardinality estimation. Cardinality estimation errors can occur due to the use of inaccurate statistics, invalid assumptions about attribute independence, parameter markers, and so on. Cardinality estimation errors may cause the optimizer to choose a sub-optimal plan. We present an approach to query processing that is extremely robust because it is able to detect and recover from cardinality estimation errors. We call this approach "progressive query optimization" (POP). POP validates cardinality estimates against actual values as measured during query execution. If there is significant disagreement between estimated and actual values, execution might be stopped and re-optimization might occur. Oscillation between optimization and execution steps can occur any number of times. A re-optimization step can exploit both the actual cardinality and partial results, computed during a previous execution step. Checkpoint operators (CHECK) validate the optimizer's cardinality estimates against actual cardinalities. Each CHECK has a condition that indicates the cardinality bounds within which a plan is valid. We compute this validity range through a novel sensitivity analysis of query plan operators. If the CHECK condition is violated, CHECK triggers re-optimization. POP has been prototyped in a leading commercial DBMS. An experimental evaluation of POP using TPC-H queries illustrates the robustness POP adds to query processing, while incurring only negligible overhead. A case-study applying POP to a real-world database and workload shows the potential of POP, accelerating complex OLAP queries by almost two orders of magnitude.

#index 765457
#* Canonical abstraction for outerjoin optimization
#@ Jun Rao;Hamid Pirahesh;Calisto Zuzarte
#t 2004
#c 5
#% 86947
#% 149845
#% 172933
#% 220425
#% 287005
#% 287333
#% 334025
#% 368248
#% 411554
#% 415987
#% 463276
#% 465165
#% 565457
#% 645385
#! Outerjoins are an important class of joins and are widely used in various kinds of applications. It is challenging to optimize queries that contain outerjoins because outerjoins do not always commute with inner joins. Previous work has studied this problem and provided techniques that allow certain reordering of the join sequences. However, the optimization of outerjoin queries is still not as powerful as that of inner joins.An inner join query can always be canonically represented as a sequence of Cartesian products of all relations, followed by a sequence of selection operations, each applying a conjunct in the join predicates. This canonical abstraction is very powerful because it enables the optimizer to use any join sequence for plan generation. Unfortunately, such a canonical abstraction for outerjoin queries has not been developed. As a result, existing techniques always exclude certain join sequences from planning, which can lead to a severe performance penalty.Given a query consisting of a sequence of inner and outer joins, we, for the first time, present a canonical abstraction based on three operations: outer Cartesian products, nullification, and best match. Like the inner join abstraction, our outerjoin abstraction permits all join sequences, and preserves the property of both commutativity and transitivity among predicates. This allows us to generate plans that are very desirable for performance reasons but that couldn't be done before. We present an algorithm that produces such a canonical abstraction, and a method that extends an inner-join optimizer to generate plans in an expanded search space. We also describe an efficient implementation of the best match operation using the OLAP functionalities in SQL:1999. Our experimental results show that our technique can significantly improve the performance of outerjoin queries.

#index 765458
#* Joining interval data in relational databases
#@ Jost Enderle;Matthias Hampel;Thomas Seidl
#t 2004
#c 5
#% 64431
#% 86950
#% 152937
#% 182700
#% 206915
#% 210186
#% 210187
#% 333981
#% 462792
#% 463595
#% 463749
#% 463751
#% 464007
#% 464856
#% 479453
#% 479797
#% 479815
#% 479953
#% 480299
#% 480422
#% 481286
#% 481934
#% 503697
#% 511005
#% 527181
#% 562809
#% 617838
#% 618613
#% 631949
#% 632099
#% 659975
#! The increasing use of temporal and spatial data in present-day relational systems necessitates an efficient support of joins on interval-valued attributes. Standard join algorithms do not support those data types adequately, whereas special approaches for interval joins usually require an augmentation of the internal access methods which is not supported by existing relational systems. To overcome these problems we introduce new join algorithms for interval data. Based on the Relational Interval Tree, these algorithms can easily be implemented on top of any relational database system while providing excellent performance on joining intervals. As experimental results on an Oracle9i server show, the new techniques outperform existing relational methods for joining intervals significantly.

#index 765459
#* Approximation techniques for spatial data
#@ Abhinandan Das;Johannes Gehrke;Mirek Riedewald
#t 2004
#c 5
#% 201880
#% 213975
#% 214073
#% 227883
#% 251459
#% 273682
#% 273887
#% 300160
#% 334053
#% 378388
#% 379443
#% 397354
#% 397385
#% 443327
#% 458863
#% 464205
#% 464850
#% 492912
#% 503853
#% 504156
#% 527190
#% 576119
#% 632072
#% 632104
#% 659976
#% 745486
#% 816392
#% 993969
#! Spatial Database Management Systems (SDBMS), e.g., Geographical Information Systems, that manage spatial objects such as points, lines, and hyper-rectangles, often have very high query processing costs. Accurate selectivity estimation during query optimization therefore is crucially important for finding good query plans, especially when spatial joins are involved. Selectivity estimation has been studied for relational database systems, but to date has only received little attention in SDBMS. In this paper, we introduce novel methods that permit high-quality selectivity estimation for spatial joins and range queries. Our techniques can be constructed in a single scan over the input, handle inserts and deletes to the database incrementally, and hence they can also be used for processing of streaming spatial data. In contrast to previous approaches, our techniques return approximate results that come with provable probabilistic quality guarantees. We present a detailed analysis and experimentally demonstrate the efficacy of the proposed techniques.

#index 765460
#* Spatially-decaying aggregation over a network: model and algorithms
#@ Edith Cohen;Haim Kaplan
#t 2004
#c 5
#% 45094
#% 63825
#% 152804
#% 243166
#% 248820
#% 278835
#% 300163
#% 333931
#% 338380
#% 378388
#% 397443
#% 401228
#% 414993
#% 460784
#% 576112
#% 593957
#% 594029
#% 636008
#% 654483
#% 731113
#% 745533
#% 749450
#% 805466
#% 993960
#% 993999
#! Data items are often associated with a location in which they are present or collected, and their relevance or influence decays with their distance. Aggregate values over such data thus depend on the observing location, where the weight given to each item depends on its distance from that location. We term such aggregation spatially-decaying.Spatially-decaying aggregation has numerous applications: Individual sensor nodes collect readings of an environmental parameter such as contamination level or parking spot availability; the nodes then communicate to integrate their readings so that each location obtains contamination level or parking availability in its neighborhood. Nodes in a p2p network could use a summary of content and properties of nodes in their neighborhood in order to guide search. In graphical databases such as Web hyperlink structure, properties such as subject of pages that can reach or be reached from a page using link traversals provide information on the page.We formalize the notion of spatially-decaying aggregation and develop efficient algorithms for fundamental aggregation functions, including sums and averages, random sampling, heavy hitters, quantiles, and Lp norms.

#index 765461
#* TOSS: an extension of TAX with ontologies and similarity queries
#@ Edward Hung;Yu Deng;V. S. Subrahmanian
#t 2004
#c 5
#% 116303
#% 152980
#% 224704
#% 315533
#% 497947
#% 562456
#% 654441
#! TAX is perhaps the best known extension of the relational algebra to handle queries to XML databases. One problem with TAX (as with many existing relational DBMSs) is that the semantics of terms in a TAX DB are not taken into account when answering queries. Thus, even though TAX answers queries with 100% precision, the recall of TAX is relatively low. Our TOSS system improves the recall of TAX via the concept of a similarity enhanced ontology (SEO). Intuitively, an ontology is a set of graphs describing relationships (such as isa, partof, etc.) between terms in a DB. An SEO also evaluates how similarities between terms (e.g. "J. Ullman", "Jeff Ullman", and "Jeffrey Ullman") affect ontologies. Finally, we show how the algebra proposed in TAX can be extended to take SEOs into account. The result is a system that provides a much higher answer quality than TAX does alone (quality is defined as the square root of the product of precision and recall). We experimentally evaluate the TOSS system on the DBLP and SIGMOD bibliographic databases and show that TOSS has acceptable performance.

#index 765462
#* Information-theoretic tools for mining database structure from large data sets
#@ Periklis Andritsos;Renée J. Miller;Panayiotis Tsaparas
#t 2004
#c 5
#% 872
#% 58381
#% 115608
#% 152934
#% 201889
#% 210173
#% 289268
#% 300711
#% 334026
#% 384978
#% 397369
#% 408396
#% 480499
#% 576092
#% 577238
#% 644182
#% 726619
#% 993980
#% 993981
#% 998770
#! Data design has been characterized as a process of arriving at a design that maximizes the information content of each piece of data (or equivalently, one that minimizes redundancy). Information content (or redundancy) is measured with respect to a prescribed model for the data, a model that is often expressed as a set of constraints. In this work, we consider the problem of doing data redesign in an environment where the prescribed model is unknown or incomplete. Specifically, we consider the problem of finding structural clues in an instance of data, an instance which may contain errors, missing values, and duplicate records. We propose a set of information-theoretic tools for finding structural summaries that are useful in characterizing the information content of the data, and ultimately useful in data design. We provide algorithms for creating these summaries over large, categorical data sets. We study the use of these summaries in one specific physical design task, that of ranking functional dependencies based on their data redundancy. We show how our ranking can be used by a physical data-design tool to find good vertical decompositions of a relation (decompositions that improve the information content of the design). We present an evaluation of the approach on real data sets.

#index 765463
#* Efficient set joins on similarity predicates
#@ Sunita Sarawagi;Alok Kirpal
#t 2004
#c 5
#% 255137
#% 278048
#% 290703
#% 299984
#% 314740
#% 420072
#% 443121
#% 443393
#% 480463
#% 480654
#% 480774
#% 481290
#% 482121
#% 577238
#% 654454
#% 654467
#% 655485
#% 729418
#% 993980
#! In this paper we present an efficient, scalable and general algorithm for performing set joins on predicates involving various similarity measures like intersect size, Jaccard-coefficient, cosine similarity, and edit-distance. This expands the existing suite of algorithms for set joins on simpler predicates such as, set containment, equality and non-zero overlap. We start with a basic inverted index based probing method and add a sequence of optimizations that result in one to two orders of magnitude improvement in running time. The algorithm folds in a data partitioning strategy that can work efficiently with an index compressed to fit in any available amount of main memory. The optimizations used in our algorithm generalize to several weighted and unweighted measures of partial word overlap between sets.

#index 765464
#* Automatic categorization of query results
#@ Kaushik Chakrabarti;Surajit Chaudhuri;Seung-won Hwang
#t 2004
#c 5
#% 210173
#% 210190
#% 260001
#% 270633
#% 273900
#% 280419
#% 333947
#% 344447
#% 387427
#% 399762
#% 420053
#% 479782
#% 659990
#% 854659
#% 993987
#! Exploratory ad-hoc queries could return too many answers - a phenomenon commonly referred to as "information overload". In this paper, we propose to automatically categorize the results of SQL queries to address this problem. We dynamically generate a labeled, hierarchical category structure - users can determine whether a category is relevant or not by examining simply its label; she can then explore just the relevant categories and ignore the remaining ones, thereby reducing information overload. We first develop analytical models to estimate information overload faced by a user for a given exploration. Based on those models, we formulate the categorization problem as a cost optimization problem and develop heuristic algorithms to compute the min-cost categorization.

#index 765465
#* When one sample is not enough: improving text database selection using shrinkage
#@ Panagiotis G. Ipeirotis;Luis Gravano
#t 2004
#c 5
#% 194246
#% 215227
#% 227891
#% 262063
#% 273926
#% 280853
#% 280856
#% 281354
#% 282422
#% 287237
#% 287463
#% 301225
#% 316534
#% 329244
#% 340146
#% 406493
#% 413594
#% 447946
#% 466078
#% 479642
#% 567255
#% 608625
#% 643012
#% 745472
#% 993964
#! Database selection is an important step when searching over large numbers of distributed text databases. The database selection task relies on statistical summaries of the database contents, which are not typically exported by databases. Previous research has developed algorithms for constructing an approximate content summary of a text database from a small document sample extracted via querying. Unfortunately, Zipf's law practically guarantees that content summaries built this way for any relatively large database will fail to cover many low-frequency words. Incomplete content summaries might negatively affect the database selection process, especially for short queries with infrequent words. To improve the coverage of approximate content summaries, we build on the observation that topically similar databases tend to have related vocabularies. Therefore, the approximate content summaries of topically related databases can complement each other and increase their coverage. Specifically, we exploit a (given or derived) hierarchical categorization of the databases and adapt the notion of "shrinkage" -a form of smoothing that has been used successfully for document classification-to the content summary construction task. A thorough evaluation over 315 real web databases as well as over TREC data suggests that the shrinkage-based content summaries are substantially more complete than their "unshrunk" counterparts. We also describe how to modify existing database selection algorithms to adaptively decide -at run-time-whether to apply shrinkage for a query. Our experiments, which rely on TREC data sets, queries, and the associated "relevance judgments," show that our shrinkage-based approach significantly improves state-of-the-art database selection algorithms, and also outperforms a recently proposed hierarchical strategy that exploits database classification as well.

#index 765466
#* On the integration of structure indexes and inverted lists
#@ Raghav Kaushik;Rajasekar Krishnamurthy;Jeffrey F. Naughton;Raghu Ramakrishnan
#t 2004
#c 5
#% 144012
#% 237053
#% 262069
#% 333854
#% 340144
#% 340914
#% 397360
#% 397375
#% 458829
#% 458861
#% 479465
#% 479782
#% 479803
#% 479806
#% 480489
#% 480656
#% 570875
#% 654441
#% 654442
#% 654450
#% 659990
#% 746508
#% 993953
#! Several methods have been proposed to evaluate queries over a native XML DBMS, where the queries specify both path and keyword constraints. These broadly consist of graph traversal approaches, optimized with auxiliary structures known as structure indexes; and approaches based on information-retrieval style inverted lists. We propose a strategy that combines the two forms of auxiliary indexes, and a query evaluation algorithm for branching path expressions based on this strategy. Our technique is general and applicable for a wide range of choices of structure indexes and inverted list join algorithms. Our experiments over the Niagara XML DBMS show the benefit of integrating the two forms of indexes. We also consider algorithmic issues in evaluating path expression queries when the notion of relevance ranking is incorporated. By integrating the above techniques with the Threshold Algorithm proposed by Fagin et al., we obtain instance optimal algorithms to push down top k computation.

#index 765467
#* Toward a progress indicator for database queries
#@ Gang Luo;Jeffrey F. Naughton;Curt J. Ellmann;Michael W. Watzke
#t 2004
#c 5
#% 152996
#% 172900
#% 191608
#% 227883
#% 248793
#% 273901
#% 273910
#% 284910
#% 333947
#% 393844
#% 463444
#% 480803
#% 571088
#% 617870
#% 654472
#! Many modern software systems provide progress indicators for long-running tasks. These progress indicators make systems more user-friendly by helping the user quickly estimate how much of the task has been completed and when the task will finish. However, none of the existing commercial RDBMSs provides a non-trival progress indicator for long-running queries. In this paper, we consider the problem of supporting such progress indicators. After discussing the goals and challenges inherent in this problem, we present a set of techniques sufficient for implementing a simple yet useful progress indicator for a large subset of RDBMS queries. We report an initial implementation of these techniques in PostgreSQL.

#index 765468
#* Estimating progress of execution for SQL queries
#@ Surajit Chaudhuri;Vivek Narasayya;Ravishankar Ramamurthy
#t 2004
#c 5
#% 102784
#% 136740
#% 201921
#% 227883
#% 248793
#% 273901
#% 273910
#% 284910
#% 463444
#% 480803
#! Today's database systems provide little feedback to the user/DBA on how much of a SQL query's execution has been completed. For long running queries, such feedback can be very useful, for example, to help decide whether the query should be terminated or allowed to run to completion. Although the above requirement is easy to express, developing a robust indicator of progress for query execution is challenging. In this paper, we study the above problem and present techniques that can form the basis for effective progress estimation. The results of experimentally validating our techniques in Microsoft SQL Server are promising.

#index 765469
#* Relaxed currency and consistency: how to say "good enough" in SQL
#@ Hongfei Guo;Per-Åke Larson;Raghu Ramakrishnan;Jonathan Goldstein
#t 2004
#c 5
#% 102804
#% 230401
#% 273694
#% 286967
#% 287260
#% 333965
#% 333969
#% 340607
#% 458535
#% 458601
#% 461847
#% 462645
#% 479967
#% 480332
#% 481584
#% 503869
#% 566135
#% 635858
#% 654504
#% 745536
#% 993994
#% 1015287
#! Despite the widespread and growing use of asynchronous copies to improve scalability, performance and availability, this practice still lacks a firm semantic foundation. Applications are written with some understanding of which queries can use data that is not entirely current and which copies are "good enough"; however, there are neither explicit requirements nor guarantees. We propose to make this knowledge available to the DBMS through explicit currency and consistency (C&C) constraints in queries and develop techniques so the DBMS can guarantee that the constraints are satisfied. In this paper we describe our model for expressing C&C constraints, define their semantics, and propose SQL syntax. We explain how C&C constraints are enforced in MTCache, our prototype mid-tier database cache, including how constraints and replica update policies are elegantly integrated into the cost-based query optimizer. Consistency constraints are enforced at compile time while currency constraints are enforced at run time by dynamic plans that check the currency of each local replica before use and select sub-plans accordingly. This approach makes optimal use of the cache DBMS while at the same time guaranteeing that applications always get data that is "good enough" for their purpose.

#index 765470
#* Highly available, fault-tolerant, parallel dataflows
#@ Mehul A. Shah;Joseph M. Hellerstein;Eric Brewer
#t 2004
#c 5
#% 86476
#% 86929
#% 86930
#% 115661
#% 136740
#% 300179
#% 302479
#% 318426
#% 340898
#% 397295
#% 397606
#% 403195
#% 439903
#% 462779
#% 481753
#% 594282
#% 616946
#% 648165
#% 649054
#% 729645
#! We present a technique that masks failures in a cluster to provide high availability and fault-tolerance for long-running, parallelized dataflows. We can use these dataflows to implement a variety of continuous query (CQ) applications that require high-throughput, 24x7 operation. Examples include network monitoring, phone call processing, click-stream processing, and online financial analysis. Our main contribution is a scheme that carefully integrates traditional query processing techniques for partitioned parallelism with the process-pairs approach for high availability. This delicate integration allows us to tolerate failures of portions of a parallel dataflow without sacrificing result quality. Upon failure, our technique provides quick fail-over, and automatically recovers the lost pieces on the fly. This piecemeal recovery provides minimal disruption to the ongoing dataflow computation and improved reliability as compared to the straight-forward application of the process-pairs technique on a per dataflow basis. Thus, our technique provides the high availability necessary for critical CQ applications. Our techniques are encapsulated in a reusable dataflow operator called Flux, an extension of the Exchange that is used to compose parallel dataflows. Encapsulating the fault-tolerance logic into Flux minimizes modifications to existing operator code and relieves the burden on the operator writer of repeatedly implementing and verifying this critical logic. We present experiments illustrating these features with an implementation of Flux in the TelegraphCQ code base [8].

#index 765471
#* Query sampling in DB2 Universal Database
#@ Jarek Gryz;Junjie Guo;Linqi Liu;Calisto Zuzarte
#t 2004
#c 5
#% 69272
#% 99463
#% 116043
#% 164361
#% 172889
#% 210188
#% 227883
#% 248821
#% 273908
#% 273909
#% 300195
#% 334053
#% 442706
#% 462058
#% 463761
#% 479814
#% 481288
#% 481293
#% 732905
#! Executing ad hoc queries against large databases can be prohibitively expensive. Exploratory analysis of data may not require exact answers to queries, however: results based on sampling the data are often satisfactory. Supporting sampling as a primitive SQL operator turns out to be difficult because sampling does not commute with many SQL operators.In this paper, we describe an implementation in IBM® DB2® Universal Database (UDB) of a sampling operator that commutes with some SQL operators. As a result, the query with the sampling operator always returns a random sample of the answers and in many cases runs faster than it would have without such an operator.

#index 765472
#* Query processing for SQL updates
#@ César A. Galindo-Legaria;Stefano Stefani;Florian Waas
#t 2004
#c 5
#% 9241
#% 461895
#% 465149
#! A rich set of concepts and techniques has been developed in the context of query processing for the efficient and robust execution of queries. So far, this work has mostly focused on issues related to data-retrieval queries, with a strong backing on relational algebra. However, update operations can also exhibit a number of query processing issues, depending on the complexity of the operations and the volume of data to process. Such issues include lookup and matching of values, navigational vs. set-oriented algorithms and trade-offs between plans that do serial or random I/Os.In this paper we present an overview of the basic techniques used to support SQL DML (Data Manipulation Language) in Microsoft SQL Server. Our focus is on the integration of update operations into the query processor, the query execution primitives required to support updates, and the update-specific considerations to analyze and execute update plans. Full integration of update processing in the query processor provides a robust and flexible framework and leverages existing query processing techniques.

#index 765473
#* Parallel SQL execution in Oracle 10g
#@ Thierry Cruanes;Benoit Dageville;Bhaskar Ghosh
#t 2004
#c 5
#% 136740
#% 188719
#% 334006
#% 442850
#% 461897
#% 462046
#% 479794
#% 654445
#% 994014
#! This paper describes the new architecture and optimizations for parallel SQL execution in the Oracle 10g database. Based on the fundamental shared-disk architecture underpinning Oracle's parallel SQL execution engine since Oracle7, we show in this paper how Oracle's engine responds to the challenges of performing in new grid-computing environments. This is made possible by using advanced optimization techniques, which enable Oracle to exploit data and system architecture dynamically without being constrained by them. We show how we have evolved and re-architected our engine in Oracle 10g to make it more efficient and manageable by using a single global parallel plan model.

#index 765474
#* Data densification in a relational database system
#@ Abhinav Gupta;Sankar Subramanian;Srikanth Bellamkonda;Tolga Bozkaya;Nathan Folkert;Lei Sheng;Andrew Witkowski
#t 2004
#c 5
#% 152940
#% 481293
#% 654445
#! Data in a relational data warehouse is usually sparse. That is, if no value exists for a given combination of dimension values, no row exists in the fact table. Densities of 0.1-2% are very common. However, users may want to view the data in a dense form, with rows for all combination of dimension values displayed even when no fact data exists for them. For example, if a product did not sell during a particular time period, users may still want to see the product for that time period with zero sales value next to it. Moreover, analytic window functions [1] and the SQL model clause [2] can more easily express time series calculations if data is dense along the time dimension because dense data will fill a consistent number of rows for each period.Data densification is the process of converting spare data into dense form. The current SQL technique for densification (using the combination of DISTINCT, CROSS JOIN and OUTER JOIN operations) is extremely unintuitive, difficult to express and inefficient to compute. Hence, we propose an extension to the ANSI SQL join operator, referred to as "PARTITIONED OUTER JOIN", which allows for a succinct expression of densification along the dimensions of interest. We also present various algorithms to evaluate the new join operator efficiently and compare it with existing methods of doing the equivalent operation. We also define a new window function "LAST_VALUE (IGNORE NULLS)" which is very useful with partitioned outer join.

#index 765475
#* Hosting the .NET Runtime in Microsoft SQL server
#@ Alazel Acheson;Mason Bendixen;José A. Blakeley;Peter Carlin;Ebru Ersan;Jun Fang;Xiaowei Jiang;Christian Kleinerman;Balaji Rathakrishnan;Gideon Schaller;Beysim Sezgin;Ramachandran Venkatesh;Honggang Zhang
#t 2004
#c 5
#% 248817
#% 316870
#% 428410
#! The integration of the .NET Common Language Runtime (CLR) inside the SQL Server DBMS enables database programmers to write business logic in the form of functions, stored procedures, triggers, data types, and aggregates using modern programming languages such as C#, Visual Basic, C++, COBOL, and J++. This paper presents three main aspects of this work. First, it describes the architecture of the integration of the CLR inside the SQL Server database process to provide a safe, scalable, secure, and efficient environment to run user code. Second, it describes our approach to defining and enforcing extensibility contracts to allow a tight integration of types, aggregates, functions, triggers, and procedures written in modern languages with the DBMS. Finally, it presents initial performance results showing the efficiency of user-defined types and functions relative to equivalent native DBMS features.

#index 765476
#* Vertical and horizontal percentage aggregations
#@ Carlos Ordonez
#t 2004
#c 5
#% 199537
#% 201928
#% 223781
#% 280521
#% 300138
#% 333925
#% 464215
#% 654445
#! Existing SQL aggregate functions present important limitations to compute percentages. This article proposes two SQL aggregate functions to compute percentages addressing such limitations. The first function returns one row for each percentage in vertical form like standard SQL aggregations. The second function returns each set of percentages adding 100% on the same row in horizontal form. These novel aggregate functions are used as a framework to introduce the concept of percentage queries and to generate efficient SQL code. Experiments study different percentage query optimization strategies and compare evaluation time of percentage queries taking advantage of our proposed aggregations against queries using available OLAP extensions. The proposed percentage aggregations are easy to use, have wide applicability and can be efficiently evaluated.

#index 765477
#* Models for Web Services tansactions
#@ Mark Little
#t 2004
#c 5

#index 765478
#* Enabling sovereign information sharing using Web Services
#@ Rakesh Agrawal;Dmitri Asonov;Ramakrishnan Srikant
#t 2004
#c 5
#% 286916
#% 318418
#% 654448
#! Sovereign information sharing allows autonomous entities to compute queries across their databases in such a way that nothing apart from the result is revealed. We describe an implementation of this model using web services infrastructure. Each site participating in sovereign sharing offers a data service that allows database operations to be applied on the tables they own. Of particular interest is the provision for binary operations such as relational joins. Applications are developed by combining these data services. We present performance measurements that show the promise of a new breed of practical applications based on the paradigm of sovereign information integration.

#index 765479
#* Building dynamic application networks with Web Services
#@ Matthew Mihic
#t 2004
#c 5
#! Looking at the state of the industry today, it is clear that we are in the early stages of Web Services development. Companies are still evaluating what the technology and considering how to apply it to their business. But over the past year, we seem to have reached an inflection point of companies building real systems based on Web Services. Partly this reflects an acceptance that the basic Web Services technologies - XML Schema [1][2], SOAP [3], WSDL [4] - have matured to the point where they can be used for mission critical applications. But it also reflects a growing understanding that Web Services enable a large class of systems that were previously very difficult to build. These systems are characterized by several critical properties:1. Rapid rates of change. The time is long past when companies could afford a year-long-effort to build out a new application. Businesses move at a faster pace today then ever before, and they are increasingly under pressure to do more work with fewer resources. This places a premium on the ability to build applications by quickly composing pre-existing services. The result is that systems are being connected in ways that were never imagined during development. This is reuse in the large - not just small services, but entire applications being linked together to solve a complex business function.2. Significant availability and scalability requirements. Many of these systems are "bet-your-business" types of applications. They have heavy scalability and availability requirements. Often then need to connect multiple partners and service hundreds of thousands of updates in a day, without ever suffering an interruption in service.3. Heterogeneous development tools and software platforms. Each of these applications typically involves components built using a wildly diverse set of tools, operating systems, and software platforms. Partly this is a result of building systems out of existing components - many of these components are locked into certain environments, and there are no resources to rewrite or migrate to a single homogenous platform. But it is also recognition that different problems are best solved by different toolsets. Some problems are best solved by writing code on an application server, others are best suited for scripting, and still others are solved by customizing an existing enterprise application. Heterogeneity is not going away. It is only increasing.4. Multiple domains of administrative control. An aspect of heterogeneity that is often overlooked is distributed ownership. As businesses merge, acquire, and partner with other companies, there is an increasing need to build applications that span organizational boundaries.These characteristics present a unique set of challenges to the way we think about developing, describing, connecting, and configuring applications. The challenges require us to develop new ways of looking at what it takes to build an application, and what makes up a network.In this session, we examine the nature of this next generation of application, and discuss the way in which Web Services are evolving to meet their needs. The session focuses on the development techniques that allow services to be easily and dynamically composed into rich applications, and considers the capabilities required of the underlying network fabric. The session concludes with an in-depth look at some of the critical Web Services specifications actively under development by industry leaders.

#index 765480
#* Secure, reliable, transacted: innovation in Web Services architecture
#@ Martin Gudgin
#t 2004
#c 5
#! This paper discusses the design of Web Services Protocols paying special attention to composition of such protocols. The transaction related protocols are discussed as exemplars.

#index 765481
#* SoundCompass: a practical query-by-humming system; normalization of scalable and shiftable time-series data and effective subsequence generation
#@ Naoko Kosugi;Yasushi Sakurai;Masashi Morimoto
#t 2004
#c 5
#% 137711
#% 172949
#% 194192
#% 286746
#% 316259
#% 341300
#% 654456
#% 993965
#! This paper describes our practical query-by-humming system, SoundCompass, which is being used as a karaoke song selection system in Japan. First, we describe the fundamental techniques employed by SoundCompass such as normalization in a time-wise sense of music data, time-scalable and tone-shiftable time-series data, and making subsequences for efficient matching. Second, we describe techniques to make effective feature vectors based on real music data and do matching with them to develop accurate query-by-humming. Third, we share valuable knowledge that has been obtained through month's of practical use of Sound Compass. Fourth, we describe the latest version of the SoundCompass system that incorporates these new techniques and knowledge, as well as describe quantitative evaluations that prove the practicality of SoundCompass. The new system provides flexible and accurate similarity retrieval based on k-nearest neighbor searches with multi-dimensional spatial indices structured with multi-dimensional features vectors.

#index 765482
#* Model-driven business UI based on maps
#@ Per Bendsen
#t 2004
#c 5
#! Future business applications will often have more than 2,000 forms and need to target several user interface (UI) technologies including: Web Browsers, Windows® Applications, PDA's, and cell phones. The applications will need state-of-the-art layout combined with excellent usability with specially built forms that handle specific tasks based on user roles. How can the trade-off between developer productivity and user experience be handled?The technologies being implemented in Microsoft® Business Framework include a model-driven business UI platform that exploits flexible maps and a layered form definition. The framework generates forms based on a model of the business logic, which is an integrated part of the business framework. The generation process uses declarative and changeable maps so that the process can be controlled and modified by the business developer.

#index 765483
#* dbSwitch™: towards a database utility
#@ Shaul Dar;Gil Hecht;Eden Shochat
#t 2004
#c 5
#% 390892
#% 644136
#! Savantis Systems' dbSwitch™ is an innovative commercial product providing database server virtualization and advancing a database utility model. The dbSwitch enables a new architecture, called a Database Area Network (DAN), which pools database server resources and shares them among multiple database applications. Specific benefits of the DAN architecture for enterprise data centers include server consolidation, improved utilization, high availability and capacity management. We describe the major components of the dbSwitch, namely routing of application requests to database instances, optimization of database server resources and capacity visualization and manipulation. We also relate dbSwitch to recent work on utility and grid computing.

#index 765484
#* Requirements and policy challenges in highly secure environments
#@ Dean E. Hall
#t 2004
#c 5

#index 765485
#* Information assurance technical challenges
#@ Nicholas J. Multari
#t 2004
#c 5

#index 765486
#* Service-oriented BI: towards tight integration of business intelligence into operational applications
#@ Marcus Dill;Achim Kraiss;Stefan Sigg;Thomas Zurek
#t 2004
#c 5

#index 765487
#* XML in the middle: XQuery in the WebLogic Platform
#@ Michael J. Carey
#t 2004
#c 5
#% 745541
#! The BEA WebLogic Platform product suite consists of WebLogic Server, WebLogic Workshop, WebLogic Integration, WebLogic Portal, and Liquid Data for WebLogic. W3C standards including XML, XML Schema, and the emerging XML query language XQuery play important roles in several of these products. This industrial presentation will discuss the increasingly central role of XML in the middle tier of enterprise IT architectures and cover some of the key XML technologies that the BEA WebLogic Platform provides for creating enterprise applications in today's IT world. We focus in particular on how XQuery fits into this picture, both for today's WebLogic Platform 8.1 and going forward in terms of the Platform roadmap.

#index 765488
#* ORDPATHs: insert-friendly XML node labels
#@ Patrick O'Neil;Elizabeth O'Neil;Shankar Pal;Istvan Cseri;Gideon Schaller;Nigel Westbury
#t 2004
#c 5
#% 333981
#% 397358
#% 397366
#% 407822
#% 428146
#% 577353
#% 994015
#! We introduce a hierarchical labeling scheme called ORDPATH that is implemented in the upcoming version of Microsoft® SQL Server™. ORDPATH labels nodes of an XML tree without requiring a schema (the most general case---a schema simplifies the problem). An example of an ORDPATH value display format is "1.5.3.9.1". A compressed binary representation of ORDPATH provides document order by simple byte-by-byte comparison and ancestry relationship equally simply. In addition, the ORDPATH scheme supports insertion of new nodes at arbitrary positions in the XML tree, their ORDPATH values "careted in" between ORDPATHs of sibling nodes, without relabeling any old nodes.

#index 765489
#* Declarative specification of Web applications exploiting Web services and workflows
#@ Marco Brambilla;Stefano Ceri;Sara Comai;Marco Dario;Piero Fraternali;Ioana Manolescu
#t 2004
#c 5
#% 309729
#% 425200
#% 1599270
#! This demo presents an extension of a declarative language for specifying data-intensive Web applications. We demonstrate a scenario extracted from a real-life application, the Web portal of a computer manufacturer, including interactions with third-party service providers and enabling distributors to participate in well-defined business processes. The crucial advantage of our framework is the high-level modeling of a complex Web application, extended with Web service and workflow capabilities. The application is automatically verified for correctness and the code is automatically generated and deployed.

#index 765490
#* Yoo-Hoo!: building a presence service with XQuery and WSDL
#@ Mary Fernández;Nicola Onose;Jérôme Siméon
#t 2004
#c 5
#% 1015354

#index 765491
#* Knocking the door to the deep Web: integrating Web query interfaces
#@ Bin He;Zhen Zhang;Kevin Chen-Chuan Chang
#t 2004
#c 5
#% 273926
#% 333932
#% 479642
#% 654459
#% 765410
#% 1015284

#index 765492
#* Efficient development of data migration transformations
#@ Paulo Carreira;Helena Galhardas
#t 2004
#c 5
#! In this paper, we present a data migration tool named DATA FUSION. Its main features are: A domain specific language designed to conveniently model complex data transformations; an integrated development environment that assists users on managing complex data transformation projects and an auditing facility that provides relevant information to project managers and external auditors.

#index 765493
#* Liquid data for WebLogic: integrating enterprise data and services
#@ Vinayak Borkar
#t 2004
#c 5
#! Information in today's enterprises commonly resides in a variety of heterogeneous data sources, including relational databases, web services, files, packaged applications, and custom data repositories. BEA's enterprise information integration product, Liquid Data for WebLogic, takes an XML-based approach to providing integrated access to such heterogeneous information. This demonstration highlights the XML technologies involved - including web services, XQuery, and XML Schema - and shows how they can be brought to bear on the enterprise information integration problem. The demonstration uses a simple end-to-end example, one that involves integrating data from relational databases and web services, to walk the audience through the overall architecture, XML-based data modeling approach, programming model, declarative query and view facilities, and distributed processing features of Liquid Data.

#index 765494
#* MAIDS: mining alarming incidents from data streams
#@ Y. Dora Cai;David Clutter;Greg Pape;Jiawei Han;Michael Welge;Loretta Auvil
#t 2004
#c 5
#% 210173
#% 248790
#% 300120
#% 333925
#% 376266
#% 378388
#% 993958
#% 1015261

#index 765495
#* FAÇADE: a fast and effective approach to the discovery of dense clusters in noisy spatial data
#@ Yu Qian;Gang Zhang;Kang Zhang
#t 2004
#c 5
#% 438137
#% 737335
#! FAÇADE (Fast and Automatic Clustering Approach to Data Engineering) is a spatial clustering tool that can discover clusters of different sizes, shapes, and densities in noisy spatial data. Compared with the existing clustering methods, FAÇADE has several advantages: first, it separates true data and noise more effectively. Second, most steps of FAÇADE are automatic. Third, it requires only O(nlogn) time. 2D and 3D visualizations are used in FAÇADE to assist parameter selection and result evaluation. More information on FAÇADE is available at: http://viscomp.utdallas.edu/FACADE.

#index 765496
#* DataMIME™
#@ Masum Serazi;Vasily Malakhov;Dongmei Ren;Amal Perera;Imad Rahal;Weihua Wu;Qiang Ding;Fei Pan;William Perrizo
#t 2004
#c 5
#% 300120
#% 346513
#% 502134
#% 728304

#index 765497
#* PIPES: a public infrastructure for processing and exploring streams
#@ Jürgen Krämer;Bernhard Seeger
#t 2004
#c 5
#% 480825
#! PIPES is a flexible and extensible infrastructure providing fundamental building blocks to implement a data stream management system (DSMS). It is seamlessly integrated into the Java library XXL [1, 2, 3] for advanced query processing and extends XXL's scope towards continuous data-driven query processing over autonomous data sources.

#index 765498
#* Web-CAM: monitoring the dynamic Web to respond to continual queries
#@ Shaveen Garg;Krithi Ramamritham;Soumen Chakrabarti
#t 2004
#c 5
#% 577369

#index 765499
#* Load management and high availability in the Medusa distributed stream processing system
#@ Magdalena Balazinska;Hari Balakrishnan;Michael Stonebraker
#t 2004
#c 5
#% 240016
#% 300179
#% 378388
#% 397295
#% 571217
#% 715028
#% 726621
#% 765470
#% 963595
#! Medusa [3, 6] is a distributed stream processing system based on the Aurora single-site stream processing engine [1]. We demonstrate how Medusa handles time-varying load spikes and provides high availability in the face of network partitions. We demonstrate Medusa in the context of Borealis, a second generation stream processing engine based on Aurora and Medusa.

#index 765500
#* StreaMon: an adaptive engine for stream query processing
#@ Shivnath Babu;Jennifer Widom
#t 2004
#c 5
#% 765435
#% 1015278
#! StreaMon is the adaptive query processing engine of the STREAM prototype Data Stream Management System (DSMS) [4]. A fundamental challenge in many DSMS applications (e.g., network monitoring, financial monitoring over stock tickers, sensor processing) is that conditions may vary significantly over time. Since queries in these systems are usually long-running, or continuous [4], it is important to consider adaptive approaches to query processing. Without adaptivity, performance may drop drastically as stream data and arrival characteristics, query loads, and system conditions change over time.StreaMon uses several techniques to support adaptive query processing [1, 2, 3]; we demonstrate three of them:•Reducing run-time memory requirements for continuous queries by exploiting stream data and arrival patterns.•Adaptive join ordering for pipelined multiway stream joins, with strong quality guarantees.•Placing subresult caches adaptively in pipelined multiway stream joins to avoid recomputation of intermediate results.

#index 765501
#* P2P-DIET: an extensible P2P service that unifies ad-hoc and continuous querying in super-peer networks
#@ Stratos Idreos;Manolis Koubarakis;Christos Tryfonopoulos
#t 2004
#c 5
#% 267451
#% 338354
#% 508414
#% 736382
#% 736383

#index 765502
#* Querying at Internet scale
#@ Brent Chun;Joseph M. Hellerstein;Ryan Huebsch;Shawn R. Jeffery;Boon Thau Loo;Sam Mardanbeigi;Timothy Roscoe;Sean Rhea;Scott Shenker;Ion Stoica
#t 2004
#c 5
#% 340175
#% 340176
#% 960186
#% 1015281
#% 1722230
#! We are developing a distributed query processor called PIER, which is designed to run on the scale of the entire Internet. PIER utilizes a Distributed Hash Table (DHT) as its communication substrate in order to achieve scalability, reliability, decentralized control, and load balancing. PIER enhances DHTs with declarative and algebraic query interfaces, and underneath those interfaces implements multihop, in-network versions of joins, aggregation, recursion, and query/result dissemination. PIER is currently being used for diverse applications, including network monitoring, keyword-based filesharing search, and network topology mapping. We will demonstrate PIER's functionality by showing system monitoring queries running on PlanetLab, a testbed of over 300 machines distributed across the globe.

#index 765503
#* Support for relaxed currency and consistency constraints in MTCache
#@ Hongfei Guo;Per-Åke Larson;Raghu Ramakrishnan;Jonathan Goldstein
#t 2004
#c 5
#% 654504
#% 745536
#% 765469

#index 765504
#* An indexing framework for peer-to-peer systems
#@ Adina Crainiceanu;Prakash Linga;Ashwin Machanavajjhala;Johannes Gehrke;Jayavel Shanmugasundaram
#t 2004
#c 5
#% 340175
#% 342375
#% 453509

#index 765505
#* XSeq: an indexing infrastructure for tree pattern queries
#@ Xiaofeng Meng;Yu Jiang;Yan Chen;Haixun Wang
#t 2004
#c 5
#% 291299
#% 397359
#% 397360
#% 479465
#% 480489
#% 480656
#% 654450
#! Given a tree-pattern query, most XML indexing approaches decompose it into multiple sub-queries, and then join their results to provide the answer to the original query. Join operations have been identified as the most time-consuming component in XML query processing. XSeq is a powerful XML indexing infrastructure which makes tree patterns a first class citizen in XML query processing. Unlike most indexing methods that directly manipulate tree structures, XSeq builds its indexing infrastructure on a much simpler data model: sequences. That is, we represent both XML data and XML queries by structure-encoded sequences. We have shown that this new data representation preserves query equivalence, and more importantly, through subsequence matching, structured queries can be answered directly without resorting to expensive join operations. Moreover, the XSeq infrastructure unifies indices on both the content and the structure of XML documents, hence it achieves an additional performance advantage over methods indexing either just content or structure, or indexing them separately.

#index 765506
#* A TeXQuery-based XML full-text search engine
#@ Chavdar Botev;Sihem Amer-Yahia;Jayavel Shanmugasundaram
#t 2004
#c 5
#% 754116
#! We demonstrate an XML full-text search engine that implements the TeXQuery language. TeXQuery is a powerful full-text search extension to XQuery that provides a rich set of fully composable full-text primitives, such as phrase matching, proximity distance, stemming and thesauri. TeXQuery enables users to seamlessly query over both structure data and text, by embedding full-text primitives in XQuery and vice versa. TeXQuery also supports a flexible scoring construct that scores query results based on full-text predicates and permits top-k queries. TeXQuery is the precursor of the full-text language extension to XPath 2.0 and XQuery 1.0 currently being developed by W3C.

#index 765507
#* "Share your data, keep your secrets."
#@ Irini Fundulaki;Arnaud Sahuguet
#t 2004
#c 5

#index 765508
#* Managing healthcare data hippocratically
#@ Rakesh Agrawal;Ameet Kini;Kristen LeFevre;Amy Wang;Yirong Xu;Diana Zhou
#t 2004
#c 5
#% 577366
#% 993943

#index 765509
#* LexEQUAL: multilexical matching operator in SQL
#@ A. Kumaran;Jayant R. Haritsa
#t 2004
#c 5
#% 219033
#% 252608
#% 480654
#% 1015263

#index 765510
#* ITQS: an integrated transport query system
#@ B. Huang;Z. Huang;H. Li;D. Lin;H. Lu;Y. Song
#t 2004
#c 5
#% 86950
#% 300174

#index 765511
#* BODHI: a database habitat for bio-diversity information
#@ Srikanta J. Bedathur;Abhijit Kadlag;Jayant R. Haritsa
#t 2004
#c 5
#% 581926
#% 745465
#% 745510

#index 765512
#* CAMAS: a citizen awareness system for crisis mitigation
#@ Sharad Mehrotra;Carter Butts;Dmitri V. Kalashnikov;Nalini Venkatasubramanian;Kemal Altintas;Ram Hariharan;Haimin Lee;Yiming Ma;Amnon Myers;Jehan Wickramasuriya;Ron Eguchi;Charles Huyck
#t 2004
#c 5

#index 765513
#* Rethinking the conference reviewing process
#@ Michael J. Franklin;Jennifer Widom;Anastassia Ailamaki;Philip A. Bernstein;David DeWitt;Alon Halevy;Zachary Ives;Gerhard Weikum
#t 2004
#c 5

#index 765514
#* Tools for design of composite Web services
#@ Richard Hull;Jianwen Su
#t 2004
#c 5
#% 101955
#% 248013
#% 248029
#% 295410
#% 311858
#% 342119
#% 348130
#% 348131
#% 404772
#% 445446
#% 489794
#% 562155
#% 576091
#% 577343
#% 653685
#% 654485
#% 716293
#% 754118
#% 754120
#% 765490
#% 801675
#% 1389602

#index 765515
#* Security of shared data in large systems: state of the art and research directions
#@ Arnon Rosenthal;Marianne Winslett
#t 2004
#c 5
#! The target audience for this tutorial is the entire SIGMOD research community. The goals of the tutorial are to enlighten the SIGMOD research community about the state of the art in data security, especially for enterprise or larger systems, and to engage the community's interest in improving the state of the art.

#index 765516
#* Fast algorithms for time series with applications to finance, physics, music, biology, and other suspects
#@ Alberto Lerner;Dennis Shasha;Zhihua Wang;Xiaojian Zhao;Yunyue Zhu
#t 2004
#c 5
#% 172949
#% 334038
#% 482088
#% 645385
#% 765537
#% 993965
#! Financial time series streams are watched closely by millions of traders. What exactly do they look for and how can we help them do it faster? Physicists study the time series emerging from their sensors. The same question holds for them. Musicians produce time series. Consumers may want to compare them. This tutorial presents techniques and case studies for four problems:1. Finding sliding window correlations in financial, physical, and other applications.2. Discovering bursts in large sensor data of gamma rays.3. Matching hums to recorded music, even when people don't hum well.4. Maintaining and manipulating time-ordered data in a database setting.This tutorial draws mostly from the book High Performance Discovery in Time Series: techniques and case studies, Springer-Verlag 2004. You can find the power point slides for this tutorial at http://cs.nyu.edu/cs/faculty/shasha/papers/sigmod04.ppt.The tutorial is aimed at researchers in streams, data mining, and scientific computing. Its applications should interest anyone who works with scientists or financial "quants." The emphasis will be on recent results and open problems. This is a ripe area for further advance.

#index 765517
#* Indexing and mining streams
#@ Christos Faloutsos
#t 2004
#c 5
#% 359751
#% 427199
#% 479462

#index 766197
#* Proceedings of the 9th ACM SIGMOD workshop on Research issues in data mining and knowledge discovery
#@ Gautam Das;Bing Liu;Philip S. Yu
#t 2004
#c 5
#! This volume contains all the papers presented in the SIGMOD Workshop on Research Issues in Data Mining and Knowledge Discovery (DMKD), June 13, 2004, Maison de la Chimie, Paris, France. This was the 9th workshop on DMKD held annually in conjunction with ACM SIGMOD conference. The workshop was sponsored by ACM SIGMOD with corporate support from Microsoft Research. With the help of program committee, we selected 7 full papers and 4 short papers out of 34 submissions. The theme of this year's workshop was on data mining and information integration.

#index 766198
#* Mining complex matchings across Web query interfaces
#@ Bin He;Kevin Chen-Chuan Chang;Jiawei Han
#t 2004
#c 5
#% 22948
#% 152934
#% 227919
#% 333932
#% 333988
#% 333990
#% 452846
#% 480645
#% 572314
#% 577214
#% 654459
#% 654467
#% 727869
#% 765410
#% 1712590
#! To enable information integration, schema matching is a critical step for discovering semantic correspondences of attributes across heterogeneous sourcess. As a new attempt, this paper studies such matching as a data mining problem. Specifically, while complex matchings are common, because of their far more complex search space, most existing techniques focus on simple 1:1 matchings. To tackle this challenge, this paper takes a conceptually novel approach by viewing schema matching as correlation mining, for our task of matching Web query interfaces to integrate the myriad databases on the Internet. On this "deep Web," query interfaces generally form complex matchings between attribute groups (e.g., {author} corresponds to {first name, last name} in the Books domain). We observe that the co-occurrences patterns across query interfaces often reveal such complex semantic relationships: grouping attributes (e.g., {first name, last name}) tend to be co-present in query interfaces and thus positively correlated. In contrast, synonym attributes are negatively correlated because they rarely co-occur. This insight enables us to discover complex matchings by a correlation mining approach, which consists of dual mining of positive and negative correlations. We evaluate our approach on deep Web sources in several object domains (e.g., Books and Airfares) and the results show that the correlation mining approach does discover semantically meaningful matchings among attributes.

#index 766199
#* Iterative record linkage for cleaning and integration
#@ Indrajit Bhattacharya;Lise Getoor
#t 2004
#c 5
#% 201889
#% 249143
#% 251405
#% 310516
#% 314740
#% 333679
#% 350103
#% 503213
#% 577238
#% 577263
#% 654467
#% 659991
#% 729913
#% 993980
#! Record linkage, the problem of determining when two records refer to the same entity, has applications for both data cleaning (deduplication) and for integrating data from multiple sources. Traditional approaches use a similarity measure that compares tuples' attribute values; tuples with similarity scores above a certain threshold are declared to be matches. While this method can perform quite well in many domains, particularly domains where there is not a large amount of noise in the data, in some domains looking only at tuple values is not enough. By also examining the context of the tuple, i.e. the other tuples to which it is linked, we can come up with a more accurate linkage decision. But this additional accuracy comes at a price. In order to correctly find all duplicates, we may need to make multiple passes over the data; as linkages are discovered, they may in turn allow us to discover additional linkages. We present results that illustrate the power and feasibility of making use of join information when comparing records.

#index 766200
#* Privacy-preserving data integration and sharing
#@ Chris Clifton;Murat Kantarcioǧlu;AnHai Doan;Gunther Schadow;Jaideep Vaidya;Ahmed Elmagarmid;Dan Suciu
#t 2004
#c 5
#% 67453
#% 287297
#% 287794
#% 307632
#% 420072
#% 431103
#% 443010
#% 572314
#% 575969
#% 577522
#% 595719
#% 635221
#% 637829
#% 654448
#% 659991
#% 662761
#% 993943
#% 1015329
#! Integrating data from multiple sources has been a longstanding challenge in the database community. Techniques such as privacy-preserving data mining promises privacy, but assume data has integration has been accomplished. Data integration methods are seriously hampered by inability to share the data to be integrated. This paper lays out a privacy framework for data integration. Challenges for data integration in the context of this framework are discussed, in the context of existing accomplishments in data integration. Many of these challenges are opportunities for the data mining community.

#index 766201
#* Mining association rules with non-uniform privacy concerns
#@ Yi Xia;Yirong Yang;Yun Chi
#t 2004
#c 5
#% 300184
#% 481290
#% 576111
#% 577233
#% 577289
#% 727866
#% 729962
#% 993988
#! Privacy concerns have become an important issue in data mining. A popular way to preserve privacy is to randomize the dataset to be mined in a systematic way and mine the randomized dataset instead. On the other hand, people usually have different privacy concerns for different attributes in data. E.g., in survey data, the sensitivity of questions varies. Appropriate use of this information can lead to more accurate data mining results. However, this information has not been fully utilized by many privacy preserving association rule mining algorithms.In this paper, we generalize the privacy preserving association rule mining problem by allowing different attributes to have different levels of privacy, that is, using different randomization factors for values of different attributes in the randomization process. We also propose an efficient algorithm RE (Recursive Estimation) to estimate the support of itemsets under this framework. Both theoretical and empirical results show that the use of non-uniform randomization factors improves the accuracy of the support estimates, compared to the use of one conservative randomization factor.

#index 766202
#* Horizontal aggregations for building tabular data sets
#@ Carlos Ordonez
#t 2004
#c 5
#% 152934
#% 199537
#% 201927
#% 220425
#% 248813
#% 278011
#% 280520
#% 280521
#% 287333
#% 300213
#% 322880
#% 342704
#% 413619
#% 464215
#% 654445
#% 662751
#% 662752
#% 662754
#% 765476
#% 766665
#% 1015290
#% 1015363
#! In a data mining project, a significant portion of time is devoted to building a data set suitable for analysis. In a relational database environment, building such data set usually requires joining tables and aggregating columns with SQL queries. Existing SQL aggregations are limited since they return a single number per aggregated group, producing one row for each computed number. These aggregations help, but a significant effort is still required to build data sets suitable for data mining purposes, where a tabular format is generally required. This work proposes very simple, yet powerful, extensions to SQL aggregate functions to produce aggregations in tabular form, returning a set of numbers instead of one number per row. We call this new class of functions horizontal aggregations. Horizontal aggregations help building answer sets in tabular form (e.g. point-dimension, observation-variable, instance-feature), which is the standard form needed by most data mining algorithms. Two common data preparation tasks are explained, including transposition/aggregation and transforming categorical attributes into binary dimensions. We propose two strategies to evaluate horizontal aggregations using standard SQL. The first strategy is based only on relational operators and the second one uses the "case" construct. Experiments with large data sets study the proposed query optimization strategies.

#index 766203
#* Discovering spatial patterns accurately with effective noise removal
#@ Yu Qian;Kang Zhang
#t 2004
#c 5
#% 36672
#% 273890
#% 342623
#% 438137
#% 737335
#% 765495
#! Cluster analysis is a common approach to pattern discovery in spatial databases. While many clustering techniques have been developed, it is still challenging to discover implicit patterns accurately when the data set contains two kinds of noise or outliers: 1) domain-specific noise; 2) noise similar to true data on size, shape, or density. This paper presents a two-step strategy to solve the problem effectively: firstly, groups of data points are separated into different layers according to their sizes and densities; then a layered visualization is provided to the user to separate noise and true data intuitively. Such a strategy not only produces user-desired results but also separates noise and true data accurately. After noise removal, a hierarchical clustering is performed on remaining data to discover natural clusters. The experimental studies on both benchmark data sets and real images show very encouraging results.

#index 766204
#* Diagonally Subgraphs Pattern Mining
#@ Moti Cohen;Ehud Gudes
#t 2004
#c 5
#% 316709
#% 410276
#% 465003
#% 479465
#% 481290
#% 577218
#% 629603
#% 629646
#% 629708
#% 727845
#% 745514
#! In this paper we present an efficient algorithm, called DSPM, for mining all frequent subgraphs in large set of graphs. The algorithm explores the search space in a DFS fashion, while generating candidates in advance to each mining phase just like the Apriori algorithm does. It combines the candidate generation and anti monotone pruning into one efficient operation thanks to the unique mode of exploration. DSPM efficiently enumerates all frequent patterns by using diagonal search, which is a general scheme for designing effective algorithms for hard enumeration problems. Our experiments show that DSPM has better performance, from several aspects, than the current state of the art - gSpan algorithm.

#index 766205
#* FP-tax: tree structure based generalized association rule mining
#@ Iko Pramudiono;Masaru Kitsuregawa
#t 2004
#c 5
#% 152934
#% 248786
#% 300120
#% 477656
#% 481290
#% 481758
#% 502147
#! Data mining has been widely recognized as a powerful tool to explore added value from large-scale databases. One of data mining techniques, generalized association rule mining with taxonomy, is potential to discover more useful knowledge than ordinary flat association rule mining by taking application specific information into account. We propose pattern growth mining paradigm based FP-tax algorithm, which employs a tree structure to compress the database. Two methods to traverse the tree structure are examined: Bottom-Up and Top-Down. Experimental results show that both methods significantly outperform classic Cumulate algorithm, in particular Top-Down FP-tax can achieve two order of magnitudes better performance than Cumulate.

#index 766206
#* An associative classifier based on positive and negative rules
#@ Maria-Luiza Antonie;Osmar R. Zaïane
#t 2004
#c 5
#% 90661
#% 92148
#% 136350
#% 152934
#% 190581
#% 227919
#% 464603
#% 464822
#% 466483
#% 628207
#% 629618
#% 629642
#% 629658
#! Associative classifiers use association rules to associate attribute values with observed class labels. This model has been recently introduced in the literature and shows good promise. The proposals so far have only concentrated on, and differ only in the way rules are ranked and selected in the model. We propose a new framework that uses different types of association rules, positive and negative. Negative association rules of interest are rules that either associate negations of attribute values to classes or negatively associate attribute values to classes. In this paper we propose a new algorithm to discover at the same time positive and negative association rules. We introduce a new associative classifier that takes advantage of these two types of rules. Moreover, we present a new way to prune irrelevant classification rules using a correlation coefficient without jeopardizing the accuracy of our associative classifier model. Our preliminary results with UCI datasets are very encouraging.

#index 766207
#* COFI approach for mining frequent itemsets revisited
#@ Mohammad El-Hajj;Osmar R. Zaïane
#t 2004
#c 5
#% 152934
#% 244336
#% 248791
#% 300120
#% 443350
#% 465003
#% 466664
#% 481290
#% 577257
#% 729920
#! The COFI approach for mining frequent itemsets, introduced recently, is an efficient algorithm that was demonstrated to outperform state-of-the-art algorithms on synthetic data. For instance, COFI is not only one order of magnitude faster and requires significantly less memory than the popular FP-Growth, it is also very effective with extremely large datasets, better than any reported algorithm. However, COFI has a significant drawback when mining dense transactional databases which is the case with some real datasets. The algorithm performs poorly in these cases because it ends up generating too many local candidates that are doomed to be infrequent. In this paper, we present a new algorithm COFI* for mining frequent itemsets. This novel algorithm uses the same data structure COFI-tree as its predecessor, but partitions the patterns in such a way to avoid the drawbacks of COFI. Moreover, its approach uses a pseudo-Oracle to pinpoint the maximal itemsets, from which all frequent itemsets are derived and counted, avoiding the generation of candidates fated infrequent. Our implementation tested on real and synthetic data shows that COFI* algorithm outperforms state-of-the-art algorithms, among them COFI itself.

#index 766208
#* Discovery of ads web hosts through traffic data analysis
#@ V. Bacarella;F. Giannotti;M. Nanni;D. Pedreschi
#t 2004
#c 5
#% 268087
#% 281251
#% 348180
#% 413655
#% 565488
#% 642981
#% 679843
#% 807657
#! One of the most actual problems on web crawling -- the most expensive task of any search engine, in terms of time and bandwidth consumption -- is the detection of useless segments of Internet. In some cases such segments are purposely created to deceive the crawling engine while, in others, they simply do not contain any useful information. Currently, the typical approach to the problem consists in using a human-compiled blacklist of sites to avoid (e.g., advertising sites and web counters), but, due to the strongly dynamical nature of Internet, keeping them manually up-to-date is quite unfeasible. In this work we present a web usage statistics-based solution to the problem, aimed at automatically -- and, therefore, dynamically -- building blacklists of sites that the users of a monitored web-community consider (or appear to consider) useless or uninteresting. Our method performs a linear time complexity analysis on the traffic information which yields an abstraction of the linked web which can be incrementally up-dated, therefore allowing a streaming computation. The crawler can use the list produced in this way to prune out such sites or to give them a low priority before the (re-)spidering activity starts and, therefore, without analysing the content of crawled documents.

#index 772018
#* Spam, damn spam, and statistics: using statistical analysis to locate spam web pages
#@ Dennis Fetterly;Mark Manasse;Marc Najork
#t 2004
#c 5
#% 255137
#% 309749
#% 480136
#% 565488
#% 577370
#% 577371
#% 590524
#% 679843
#% 728115
#! The increasing importance of search engines to commercial web sites has given rise to a phenomenon we call "web spam", that is, web pages that exist only to mislead search engines into (mis)leading users to certain web sites. Web spam is a nuisance to users as well as search engines: users have a harder time finding the information they need, and search engines have to cope with an inflated corpus, which in turn causes their cost per query to increase. Therefore, search engines have a strong incentive to weed out spam web pages from their index.We propose that some spam web pages can be identified through statistical analysis: Certain classes of spam pages, in particular those that are machine-generated, diverge in some of their properties from the properties of web pages at large. We have examined a variety of such properties, including linkage structure, page content, and page evolution, and have found that outliers in the statistical distribution of these properties are highly likely to be caused by web spam.This paper describes the properties we have examined, gives the statistical distributions we have observed, and shows which kinds of outliers are highly correlated with web spam.

#index 772019
#* Querying bi-level information
#@ Sudarshan Murthy;David Maier;Lois Delcambre
#t 2004
#c 5
#% 116303
#% 157704
#% 274160
#% 481923
#% 533911
#% 533998
#% 743927
#! In our research on superimposed information management, we have developed applications where information elements in the superimposed layer serve to annotate, comment, restructure, and combine selections from one or more existing documents in the base layer. Base documents tend to be unstructured or semi-structured (HTML pages, Excel spreadsheets, and so on) with marks delimiting selections. Selections in the base layer can be programmatically accessed via marks to retrieve content and context. The applications we have built to date allow creation of new marks and new superimposed elements (that use marks), but they have been browse-oriented and tend to expose the line between superimposed and base layers. Here, we present a new access capability, called bi-level queries, that allows an application or user to query over both layers as a whole. Bi-level queries provide an alternative style of data integration where only relevant portions of a base document are mediated (not the whole document) and the superimposed layer can add information not present in the base layer. We discuss our framework for superimposed information management, an initial implementation of a bi-level query system with an XML Query interface, and suggest mechanisms to improve scalability and performance.

#index 772020
#* Visualizing and discovering web navigational patterns
#@ Jiyang Chen;Lisheng Sun;Osmar R. Zaïane;Randy Goebel
#t 2004
#c 5
#% 195970
#% 247316
#% 310517
#% 434000
#% 446241
#% 502133
#% 724554
#! Web site structures are complex to analyze. Cross-referencing the web structure with navigational behaviour adds to the complexity of the analysis. However, this convoluted analysis is necessary to discover useful patterns and understand the navigational behaviour of web site visitors, whether to improve web site structures, provide intelligent on-line tools or offer support to human decision makers. Moreover, interactive investigation of web access logs is often desired since it allows ad hoc discovery and examination of patterns not a priori known. Various visualization tools have been provided for this task but they often lack the functionality to conveniently generate new patterns. In this paper we propose a visualization tool to visualize web graphs, representations of web structure overlaid with information and pattern tiers. We also propose a web graph algebra to manipulate and combine web graphs and their layers in order to discover new patterns in an ad hoc manner.

#index 772021
#* One torus to rule them all: multi-dimensional queries in P2P systems
#@ Prasanna Ganesan;Beverly Yang;Hector Garcia-Molina
#t 2004
#c 5
#% 43171
#% 86951
#% 116042
#% 339622
#% 340175
#% 340176
#% 384872
#% 415957
#% 453509
#% 505869
#% 527181
#% 578904
#% 580728
#% 963874
#% 1722229
#! Peer-to-peer systems enable access to data spread over an extremely large number of machines. Most P2P systems support only simple lookup queries. However, many new applications, such as P2P photo sharing and massively multi-player games, would benefit greatly from support for multidimensional range queries. We show how such queries may be supported in a P2P system by adapting traditional spatial-database technologies with novel P2P routing networks and load-balancing algorithms. We show how to adapt two popular spatial-database solutions - kd-trees and space-filling curves - and experimentally compare their effectiveness.

#index 772022
#* Querying peer-to-peer networks using P-trees
#@ Adina Crainiceanu;Prakash Linga;Johannes Gehrke;Jayavel Shanmugasundaram
#t 2004
#c 5
#% 317933
#% 340175
#% 340176
#% 340297
#% 453509
#% 505869
#! We propose a new distributed, fault-tolerant peer-to-peer index structure called the P-tree. P-trees efficiently evaluate range queries in addition to equality queries.

#index 772023
#* Scalable dissemination: what's hot and what's not
#@ Jonathan Beaver;Nicholas Morsillo;Kirk Pruhs;Panos K. Chrysanthis;Vincenzo Liberatore
#t 2004
#c 5
#% 227885
#% 248838
#% 274142
#% 280466
#% 290747
#% 300177
#% 303746
#% 310775
#% 334036
#% 379138
#% 397357
#% 397402
#% 397403
#% 410276
#% 413193
#% 482107
#% 555275
#% 576119
#% 740851
#% 963648
#% 1015287
#% 1392876
#! A major problem in web database applications and on the Internet in general is the scalable delivery of data. One proposed solution for this problem is a hybrid system that uses multicast push to scalably deliver the most popular data, and reserves traditional unicast pull for delivery of less popular data. However, such a hybrid scheme introduces a variety of data management problems at the server. In this paper we examine three of these problems: the push popularity problem, the document classification problem, and the bandwidth division problem. The push popularity problem is to estimate the popularity of the documents in the web site. The document classification problem is to determine which documents should be pushed and which documents must be pulled. The band-width division problem is to determine how much of the server bandwidth to devote to pushed documents and how much of the server bandwidth should be reserved for pulled documents. We propose simple and elegant solutions for these problems. We report on experiments with our system that validate our algorithms.

#index 772024
#* Semantic multicast for content-based stream dissemination
#@ Olga Papaemmanouil;Uĝur Çetintemel
#t 2004
#c 5
#% 290142
#% 302816
#% 338354
#% 342372
#% 465051
#% 480296
#% 555272
#% 570879
#% 646220
#% 654508
#% 661478
#% 662367
#% 723296
#% 723297
#% 1502090
#% 1849768
#! We consider the problem of content-based routing and dissemination of highly-distributed, fast data streams from multiple sources to multiple receivers. Our target application domain includes real-time, stream-based monitoring applications and large-scale event dissemination. We introduce SemCast, a new semantic multicast approach that, unlike previous approaches, eliminates the need for content-based forwarding at interior brokers and facilitates fine-grained control over the construction of dissemination overlays. We present the initial design of SemCast and provide an outline of the architectural and algorithmic challenges as well as our initial solutions. Preliminary experimental results show that SemCast can significantly reduce overall bandwidth requirements compared to traditional event-dissemination approaches.

#index 772025
#* Twig query processing over graph-structured XML data
#@ Zografoula Vagena;Mirella M. Moro;Vassilis J. Tsotras
#t 2004
#c 5
#% 31484
#% 172924
#% 300157
#% 333981
#% 333989
#% 379482
#% 397359
#% 397360
#% 397375
#% 479465
#% 480489
#% 654450
#% 654452
#% 659999
#% 660000
#% 731408
#% 745461
#% 765442
#% 993951
#% 993953
#% 1015273
#% 1015275
#% 1015277
#! XML and semi-structured data is usually modeled using graph structures. Structural summaries, which have been proposed to speedup XML query processing have graph forms as well. The existent approaches for evaluating queries over tree structured data (i.e. data whose underlying structure is a tree) are not directly applicable when the data is modeled as a random graph. Moreover, they cannot be applied when structural summaries are employed and, to the best of our knowledge, no analogous techniques have been reported for this case either. As a result, the potential of structural summaries is not fully exploited.In this paper, we investigate query evaluation techniques applicable to graph-structured data. We propose efficient algorithms for the case of directed acyclic graphs, which appear in many real world situations. We then tailor our approaches to handle other directed graphs as well. Our experimental evaluation reveals the advantages of our solutions over existing methods for graph-structured data.

#index 772026
#* Unraveling the duplicate-elimination problem in XML-to-SQL query translation
#@ Rajasekar Krishnamurthy;Raghav Kaushik;Jeffrey F Naughton
#t 2004
#c 5
#% 333935
#% 348183
#% 411759
#% 428146
#% 443173
#% 479956
#% 480657
#% 480822
#% 487250
#% 489174
#% 745478
#% 993941
#% 994001
#% 1015271
#% 1393700
#! We consider the scenario where existing relational data is exported as XML. In this context, we look at the problem of translating XML queries into SQL. XML query languages have two different notions of duplicates: node-identity based and value-based. Path expression queries have an implicit node-identity based duplicate elimination built into them. On the other hand, SQL only supports value-based duplicate elimination. In this paper, using a simple path expression query we illustrate the problems that arise when we attempt to simulate the node-identity based duplicate elimination using value-based duplicate elimination in the SQL queries. We show how a general solution for this problem covering the class of views considered in published literature requires a fairly complex mechanism.

#index 772027
#* Best-match querying from document-centric XML
#@ Jaap Kamps;Maarten Marx;Maarten de Rijke;Börkur Sigurbjörnsson
#t 2004
#c 5
#% 184486
#% 269563
#% 333841
#% 338753
#% 397358
#% 465065
#! On the Web, there is a pervasive use of XML to give lightweight semantics to textual collections. Such document-centric XML collections require a query language that can gracefully handle structural constraints as well as constraints on the free text of the documents. Our main contributions are three-fold. First, we outline two fragments of XPath tailored to users that have varying degrees of understanding of the XML structure used, and give both syntactic and semantic characterizations of these fragments. Second, we extend XPath with an about function having a best-match semantics based on the relevance of the document component for the expressed information need. Third, we evaluate the resulting query language using the INEX 2003 test suite, and show that best-match approaches outperform exact-match approaches for evaluating content-and-structure queries.

#index 772028
#* Challenges in selecting paths for navigational queries: trade-off of benefit of path versus cost of plan
#@ María-Esther Vidal;Louiqa Raschid;Julian Mestre
#t 2004
#c 5
#% 77943
#% 273912
#% 333847
#% 397356
#% 571217
#% 659968
#% 1015319
#% 1414317
#! Life sciences sources are characterized by a complex graph of overlapping sources, and multiple alternate links between sources. A (navigational) query may be answered by traversing multiple alternate paths between a start source and a target source. Each of these paths may have dissimilar benefit, e.g., the cardinality of result objects that are reached in the target source. Paths may also have dissimilar costs of evaluation, i.e., the execution cost of a query evaluation plan for a path. In prior research, we developed ESearch, an algorithm based on a Deterministic Finite Automaton (DFA), which exhaustively enumerates all paths to answer a navigational query. The challenge is to develop heuristics that improve on the exhaustive ESearch solution and identify good utility functions that can rank the sources, the links between sources, and the sub-paths that are already visited, in order to quickly produce paths that have the highest benefit and the least cost. In this paper, we present a heuristic that uses local utility functions to rank sources, using either the benefit attributed to the source, the cost of a plan using the source, or both. The heuristic will limit its search to some Top XX% of the ranked sources. To compare ESearch and the heuristic, we construct a Pareto surface of all dominant solutions produced by ESearch, with respect to benefit and cost. We choose the Top 25% of the ESearch solutions that are in the Pareto surface. We compare the paths produced by the heuristic to this Top 25% of ESearch solutions with respect to precision and recall. This motivates the need for further research on developing a more efficient algorithm and better utility functions.

#index 772029
#* Content and structure in indexing and ranking XML
#@ Felix Weigel;Holger Meuss;Klaus U. Schulz;François Bry
#t 2004
#c 5
#% 249160
#% 340914
#% 345712
#% 406493
#% 458829
#% 479465
#% 480656
#% 584940
#% 765466
#! Rooted in electronic publishing, XML is now widely used for modelling and storing structured text documents. Especially in the WWW, retrieval of XML documents is most useful in combination with a relevance-based ranking of the query result. Index structures with ranking support are therefore needed for fast access to relevant parts of large document collections. This paper proposes a classification scheme for both XML ranking models and index structures, allowing to determine which index suits which ranking model. An analysis reveals that ranking parameters related to both the content and structure of the data are poorly supported by most known XML indices. The IR-CADG index, owing to its tight integration of content and structure, supports various XML ranking models in a very efficient retrieval process. Experiments show that it outperforms separate content/structure indexing by more than two orders of magnitude for large corpora of several hundred MB.

#index 772030
#* Mining approximate functional dependencies and concept similarities to answer imprecise queries
#@ Ullas Nambiar;Subbarao Kambhampati
#t 2004
#c 5
#% 30077
#% 41230
#% 84656
#% 189872
#% 248801
#% 300711
#% 348165
#% 387427
#% 442712
#% 464837
#% 479803
#% 716425
#% 729894
#% 769421
#% 1015359
#! Current approaches for answering queries with imprecise constraints require users to provide distance metrics and importance measures for attributes of interest. In this paper we focus on providing a domain and end-user independent solution for supporting imprecise queries over Web databases without affecting the underlying database. We propose a query processing framework that integrates techniques from IR and database research to efficiently determine answers for imprecise queries. We mine and use approximate functional dependencies between attributes to create precise queries having tuples relevant to the given imprecise query. An approach to automatically estimate the semantic distances between values of categorical attributes is also proposed. We provide preliminary results showing the utility of our approach.

#index 772031
#* DTDs versus XML schema: a practical study
#@ Geert Jan Bex;Frank Neven;Jan Van den Bussche
#t 2004
#c 5
#% 257873
#% 299944
#% 504573
#% 848763
#! Among the various proposals answering the shortcomings of Document Type Definitions (DTDs), XML Schema is the most widely used. Although DTDs and XML Schema Definitions (XSDs) differ syntactically, they are still quite related on an abstract level. Indeed, freed from all syntactic sugar, XML Schemas can be seen as an extension of DTDs with a restricted form of specialization. In the present paper, we inspect a number of DTDs and XSDs harvested from the web and try to answer the following questions: (1) which of the extra features/expressiveness of XML Schema not allowed by DTDs are effectively used in practice; and, (2) how sophisticated are the structural properties (i.e. the nature of regular expressions) of the two formalisms. It turns out that at present real-world XSDs only sparingly use the new features introduced by XML Schema: on a structural level the vast majority of them can already be defined by DTDs. Further, we introduce a class of simple regular expressions and obtain that a surprisingly high fraction of the content models belong to this class. The latter result sheds light on the justification of simplifying assumptions that sometimes have to be made in XML research.

#index 772032
#* On validation of XML streams using finite state machines
#@ Cristiana Chitic;Daniela Rosu
#t 2004
#c 5
#% 1976
#% 101941
#% 257873
#% 288599
#% 378392
#% 564568
#! We study validation of streamed XML documents by means of finite state machines. Previous work has shown that validation is in principle possible by finite state automata, but the construction was prohibitively expensive, giving an exponential-size nondeterministic automaton. Instead, we want to find deterministic automata for validating streamed documents: for them, the complexity of validation is constant per tag. We show that for a reading window of size one and nonrecursive DTDs with one-unambiguous content (i.e. conforming to the current XML standard) there is an algorithm producing a deterministic automaton that validates documents with respect to that DTD. The size of the automaton is at most exponential and we give matching lower bounds. To capture the possible advantages offered by reading windows of size k, we introduce k-unambiguity as a generalization of one-unambiguity, and study the validation against DTDs with k-unambiguous content. We also consider recursive DTDs and give conditions under which they can be validated against by using one-counter automata.

#index 772033
#* Checking potential validity of XML documents
#@ Ionut E. Iacob;Alex Dekhtyar;Michael I. Dekhtyar
#t 2004
#c 5
#% 3888
#% 155296
#% 322318
#% 465059
#% 745467
#% 840584
#! The process of creation of document-centric XML documents often starts with a prepared textual content, into which the editor introduces markup. In such situations, intermediate XML is almost never valid with respect to the DTD/Schema used for the encoding. At the same time, it is important to ensure that at each moment of time, the editor is working with an XML document that can enriched with further markup to become valid. In this paper we introduce the notion of potential validity of XML documents, which allows us to distinguish between XML documents that are invalid because the encoding is simply incomplete and XML documents that are invalid because some of the DTD rules guiding the structure of the encoding were violated during the markup process. We give a linear-time algorithm for checking potential validity for documents.

#index 776770
#* Text Centric Structure Extraction and Exploitation (abstract only)
#@ Prabhakar Raghavan
#t 2004
#c 5
#! In this talk we look at the convergence of three text-centricareas: entity extraction, semi-structured querying and theintegration of search results; we view these in the context oftext-centric XML applications. The main focus of the talk will beon text in XML querying and some recent work in approaching it fromthe perspective of information retrieval.

#index 778465
#* Life science research and data management—what can they give each other?
#@ Amarnath Gupta
#t 2004
#c 5
#% 342360
#% 565353
#% 1778607
#! "Databases for the life sciences" is not really a newly emerging area in databases. The Nucleotide Sequence Database from the European Molecular Biology Laboratory (EMBL) has been operational at the European Bioinformatics Institute (EBI) since 1980. SWISS-PROT, the classical database containing protein information, was established in 1986. Today, there are over a thousand different life science databases with contents ranging from gene-property data for different organisms to brain image data for patients with neurological disorders.

#index 778466
#* Database management for life sciences research
#@ H. V. Jagadish;Frank Olken
#t 2004
#c 5
#! The life sciences provide a rich application domain for data management research, with a broad diversity of problems that can make a significant difference to progress in life sciences research. This article is an extract from the Report of the NSF Workshop on Data Management for Molecular and Cell Biology, edited by H. V. Jagadish and Frank Olken. The workshop was held at the National Library of Medicine, Bethesda, MD, Feb. 2-3, 2003.

#index 778467
#* An optimal algorithm for querying tree structures and its applications in bioinformatics
#@ Hsiao-Fei Liu;Ya-Hui Chang;Kun-Mao Chao
#t 2004
#c 5
#% 235941
#% 378391
#% 379448
#% 410276
#% 451548
#% 480656
#% 654450
#% 1015274
#! Trees and graphs are widely used to model biological databases. Providing efficient algorithms to support tree-based or graph-based querying is therefore an important issue. In this paper, we propose an optimal algorithm which can answer the following question: "Where do the root-to-leaf paths of a rooted labeled tree Q occur in another rooted labeled tree T?" in time O(m + Occ), where m is the size of Q and Occ is the output size. We also show the problem of querying a general graph is NP-complete and not approximable within nk for any k n is the number of nodes in the queried graph, unless P = NP.

#index 778468
#* Using reasoning to guide annotation with gene ontology terms in GOAT
#@ Michael Bada;Daniele Turi;Robin McEntire;Robert Stevens
#t 2004
#c 5
#% 458868
#! High-quality annotation of biological data is central to bioinformatics. Annotation using terms from ontologies provides reliable computational access to data. The Gene Ontology (GO), a structured controlled vocabulary of nearly 17,000 terms, is becoming the de facto standard for describing the functionality of gene products. Many prominent biomedical databases use GO as a source of terms for functional annotation of their gene-product entries to promote consistent querying and interoperability. However, current annotation editors do not constrain the choice of GO terms users may enter for a given gene product, potentially resulting in an inconsistent or even nonsensical description. Furthermore, the process of annotation is largely an unguided one in which the user must wade through large GO subtrees in search of terms. Relying upon a reasoner loaded with a DAML+OIL version of GO and an instance store of mined GO-term-to-GO-term associations, GOAT aims to aid the user in the annotation of gene products with GO terms by displaying those field values that are most likely to be appropriate based on previously entered terms. This can result in a reduction in biologically inconsistent combinations of GO terms and a less tedious annotation process on the part of the user.

#index 778469
#* Managing and analyzing carbohydrate data
#@ Kiyoko F. Aoki;Nobuhisa Ueda;Atsuko Yamaguchi;Tatsuya Akutsu;Minoru Kanehisa;Hiroshi Mamitsuka
#t 2004
#c 5
#% 137711
#% 289193
#% 292235
#% 328362
#% 464640
#% 571903
#% 629617
#% 729941
#! One of the most vital molecules in multicellular organisms is the carbohydrate, as it is structurally important in the construction of such organisms. In fact, all cells in nature carry carbohydrate sugar chains, or glycans, that help modulate various cell-cell events for the development of the organism. Unfortunately, informatics research on glycans has been slow in comparison to DNA and proteins, largely due to difficulties in the biological analysis of glycan structures. Our work consists of data engineering approaches in order to glean some understanding of the current glycan data that is publicly available. In particular, by modeling glycans as labeled unordered trees, we have implemented a tree-matching algorithm for measuring tree similarity. Our algorithm utilizes proven efficient methodologies in computer science that has been extended and developed for glycan data. Moreover, since glycans are recognized by various agents in multicellular organisms, in order to capture the patterns that might be recognized, we needed to somehow capture the dependencies that seem to range beyond the directly connected nodes in a tree. Therefore, by defining glycans as labeled ordered trees, we were able to develop a new probabilistic tree model such that sibling patterns across a tree could be mined. We provide promising results from our methodologies that could prove useful for the future of glycome informatics.

#index 778470
#* Piers: an efficient model for similarity search in DNA sequence databases
#@ Xia Cao;Shuai Cheng Li;Beng Chin Ooi;Anthony K. H. Tung
#t 2004
#c 5
#% 143306
#% 172949
#% 235941
#% 269546
#% 300105
#% 443469
#% 480482
#% 480484
#% 589459
#% 853018
#% 1015330
#! Growing interest in genomic research has resulted in the creation of huge biological sequence databases. In this paper, we present a hash-based pier model for efficient homology search in large DNA sequence databases. In our model, only certain segments in the databases called 'piers' need to be accessed during searches as opposite to other approaches which require a full scan on the biological sequence database. To further improve the search efficiency, the piers are stored in a specially designed hash table which helps to avoid expensive alignment operation. The has table is small enough to reside in main memory, hence avoiding I/O in the search steps. We show theoretically and empirically that the proposed approach can efficiently detect biological sequences that are similar to a query sequence with very high sensitivity.

#index 778471
#* The GenAlg project: developing a new integrating data model, language, and tool for managing and querying genomic information
#@ Joachim Hammer;Markus Schneider
#t 2004
#c 5
#! Life scientists are faced with a rapidly increasing accumulation of data stored in large genomic repositories like EMBL, GenBank, SwissProt, and DDBJ, to name a few. The flood of genomic data, their high variety and heterogeneity in semantics, formats and access methods, their semi-structured nature, and the increasing complexity of biological applications and methods mean that many and very important challenges in biology are now challenges in computing.

#index 778472
#* BIO-AJAX: an extensible framework for biological data cleaning
#@ Katherine G. Herbert;Narain H. Gehani;William H. Piel;Jason T. L. Wang;Cathy H. Wu
#t 2004
#c 5
#% 350102
#% 480496
#% 644182
#% 745511
#% 772128
#% 1796827
#! As databases become more pervasive through the biological sciences, various data quality issues regarding data legacy, data uniformity and data duplication arise. Due to the nature of this data, each of these problems is non-trivial. For biological data to be corrected and standardized, new methods and frameworks must be developed. This paper proposes one such framework, called BIO-AJAX, which uses principles from data cleaning to improve data quality in biological information systems, specifically in TreeBASE.

#index 778473
#* Automatic composite wrapper generation for semi-structured biological data based on table structure identification
#@ Liangyou Chen;Hasan M. Jamil;Nan Wang
#t 2004
#c 5
#% 273925
#% 300288
#% 301180
#% 334041
#% 342663
#% 397605
#% 589390
#% 589446
#% 745439
#! Biological data analyses usually require complex manipulations involving tool applications, multiple web site navigation, result selection and filtering, and iteration over the internet. Most biological data are generated from structured databases and by applications and presented to the users embedded within repeated structures, or tables, in HTML documents. In this paper we outline a novel technique for the identification of table structures in HTML documents. This identification technique is then used to automatically generate composite wrappers for applications requiring distributed resources. We demonstrate that our method is robust enough to discover standard as well as non-standard table structures in HTML documents. Thus our technique outperforms contemporary techniques used in systems such as XWrap and AutoWrapper. We discuss our technique in the context of our PickUp system that exploits the theoretical developments presented in this paper and emerges as an elegant automatic wrapper generation system.

#index 778474
#* A distributed database for bio-molecular images
#@ Ambuj K. Singh;B. S. Manjunath;Robert F. Murphy
#t 2004
#c 5
#% 201893
#% 248798
#% 333975
#% 359751
#% 479788
#% 577293
#% 645687
#% 717359
#% 729437
#% 768039
#% 1854745
#! Information technology research has played a significant role in the genomics revolution over the past decade, from aiding with large-scale sequence assembly to automating gene identification to efficiently searching databases by sequence similarity. The tremendous amount of information gathered from genomics will be dwarfed in the next decade by the knowledge to be gained from comprehensive, systematic studies of the properties and behaviors of all proteins and other biomolecules. High-resolution imaging of molecules and cells will be critical for understanding complex systems such as the nervous system, whether it be for the localization of specific neuron types within a region of the central nervous system, the branching pattern of dendritic trees, or the localization of molecules at the subcellular level. Furthermore, knowing how these distribution patterns and subcellular locations change as a function of time is critical to understanding how cells respond to stress, injury, aging, and disease. We are developing sophisticated information technologies for collecting and interpreting the enormous volume of biological image data. A major outcome of the research will be a unique, fully operational, distributed digital library of biomolecular image data accessible to researchers around the world. Such searchable databases will make it possible to optimally understand and interpret the data, leading to a more complete and integrated understanding of cellular structure, function and regulation.

#index 778475
#* BioFast: challenges in exploring linked life sciences sources
#@ Jens Bleiholder;Felix Naumann;Zoé Lacroix;Louiqa Raschid;Hyma Murthy;Maria-Esther Vidal
#t 2004
#c 5
#% 273911
#% 397356
#% 589356
#% 589376
#! An abundance of life sciences data sources contain data about scientific entities such as genes and sequences. Scientists are interested in exploring relationships between scientific objects, e.g., between genes and bibliographic citations. A scientist may choose the OMIM source, which contains information related to human genetic diseases, as a starting point for her exploration, and wish to eventually retrieve all related citations from the PUBMED source. Starting with a keyword search on a certain disease, she can explore all possible relationships between genes in OMIM and citations in PUBMED. This corresponds to the following query: "Return all citations of PUBMED that are linked to an OMIM entry that is related to some disease or condition."

#index 778476
#* Mobile databases: a selection of open issues and research directions
#@ Guy Bernard;Jalel Ben-othman;Luc Bouganim;Gérôme Canals;Sophie Chabridon;Bruno Defude;Jean Ferrié;Stéphane Gançarski;Rachid Guerraoui;Pascal Molli;Philippe Pucheral;Claudia Roncancio;Patricia Serrano-Alvarado;Patrick Valduriez
#t 2004
#c 5
#% 35764
#% 224892
#% 240013
#% 259646
#% 260739
#% 268609
#% 280424
#% 319698
#% 397367
#% 443318
#% 464840
#% 470593
#% 480634
#% 487534
#% 503869
#% 546223
#% 569762
#% 572304
#% 660004
#% 723306
#% 805466
#% 955956
#% 993932
#% 993942
#% 993943
#% 1015312
#! This paper reports on the main results of a specific action on mobile databases conducted by CNRS in France from October 2001 to December 2002. The objective of this action was to review the state of progress in mobile databases and identify major research directions for the French database community. Rather than provide a survey of all important issues in mobile databases, this paper gives an outline of the directions in which the action participants are now engaged, namely: copy synchronization in disconnected computing, mobile transactions, database embedded in ultra-light devices, data confidentiality, P2P dissemination models and middleware adaptability.

#index 778477
#* Introducing an annotated bibliography on temporal and evolution aspects in the World Wide Web
#@ Fabio Grandi
#t 2004
#c 5
#! Time is a pervasive dimension of reality as everything evolves as time elapses. Information systems and applications at least mirror, and often have to capture, the time-varying and evolutionary nature of the phenomena they model and the activities they support. This aspect has been acknowledged and long studied in the field of temporal databases but it truly applies also to the World Wide Web, although it has not seemingly considered as a primary issue yet. However, several papers addressing, in an explicit or implicit way, the representation and management of time and change in the World Wide Web appeared recently and, on some aspects, showed a clear upward trend in last months, witnessing a sustained and/or growing interest.

#index 778478
#* Logic-based web information extraction
#@ Georg Gottlob;Christoph Koch
#t 2004
#c 5
#% 6787
#% 23898
#% 36683
#% 49315
#% 54225
#% 101944
#% 159110
#% 241166
#% 271065
#% 275922
#% 331772
#% 342829
#% 344425
#% 378389
#% 384978
#% 388009
#% 401124
#% 424931
#% 427027
#% 427161
#% 459241
#% 473117
#% 480648
#% 576106
#% 587580
#% 598376
#% 632051
#% 733595
#% 801686
#% 993939
#% 1015275
#% 1279271
#! The Web wrapping proble, i.e., the problem of extracting structured information from HTML documents, is one of great practical importance. The often observed information overload that users of the Web experience witnesses the lack of intelligent and encompassing Web services that provide high-quality collected and value-added inforamtion. The Web wrapping problem has been addressed by a significant amount of research work. Previous work can be classified into two categories, depending on whether the HTML input is regarded as a sequential character string (e.g., [34, 27, 24, 30, 23]) or a pre-parsed document tree (for instance, [35, 25, 22, 29, 3, 2, 26]). The latter category of work thus assumes that systems may make use of an existing HTML parser as a front and.

#index 778479
#* Jeffrey naughton speaks out on database systems as control freaks, how to choose students, how to choose problems, how to get attention, the importance of being true to yourself, and more
#@ Marianne Winslett
#t 2004
#c 5
#! Welcome to this installment of ACM SIGMOD Record's series of interviews with distinguished members of the database community. I'm Marianne Winslett, and today we're in the Department of Computer Science at the University of Illinois at Urbana-Champaign. I have here with me Jeffrey Naughton, who is a professor of computer science at the University of Wisconsin at Madison. Jeff's research has focused on improving the performance and functionality of database management systems. He received the National Science Foundation's Presidential Young Investigator Award, and he is an ACM Fellow. Jeff's PhD is from Stanford University. So, Jeff, welcome!

#index 778480
#* An early look at XQuery API for Java™ (XQJ)
#@ Andrew Eisenberg;Jim Melton
#t 2004
#c 5
#% 449227
#! In Feb. 2004, the period for submitting Last Call Working Draft comments for most parts of the XQuery specification came to a close. While there is still a great deal of work to be done to make XQuery a W3C Recommendation, the documents have become more stable with each public release. In this column we'd like to provide an initial look at the XQuery API for Java™ (XQJ), a project that is taking place within the Java Community Process (JCP).

#index 778481
#* Review of "Managing gigabytes: compressing and indexing documents and images, by Ian H. Witten, Alistair Moffat, and Timothy C. Bell" Morgan Kauffman, 1999, 519 pp. ISBN 1558605703.
#@ S. V. Nagaraj
#t 2004
#c 5
#! This book published in 1999 is a revised second edition of the version that first appeared in 1994. The authors have incorporated many changes in the second edition by updating various chapters to include the latest developments. The book is concerned with the task of managing large volumes of text and image data amounting to several gigabytes. The fundamental problems addressed in the book include compressing such data and indexing it, in order to enable easy search. Many books look at compressing and indexing as if they are unrelated techniques; however, this book brings out the advantages of combining them in a beneficial way.

#index 778482
#* Database tuning principles, experiments, and troubleshooting techniques
#@ Dennis Shasha;Philippe Bonnet;Nancy Hartline Bercich
#t 2004
#c 5
#! As our reliance on computers and computerized data has increased, we have come to expect more from our computers. We no longer expect our computers to act as large expensive calculators that merely spit out bills and paychecks. We now, additionally, expect our systems to rapidly access and interactively present us with large volumes of accurate data. In fact, our expectations have changed so much, in the past decade, that we no longer focus on what our systems are but rather on what they do. We no longer refer to our systems as computer systems but rather information systems. With these new expectations have come new responsibilities for the information systems professional. We can no longer concern ourselves merely with keeping our systems up and running. We now need to concern ourselves with subjective concepts such as response time and throughput. With current expectations what they are, performance tuning has become vitally important.

#index 778483
#* Branding yourself
#@ Richard Snodgrass;Merrie Brucks
#t 2004
#c 5
#% 345761
#! Our here in the western US a "brand" is distinctive symbol placed on livestock (cattle, horses) to indicate their ranch of origin. Brands originated in the 1800's to deter thieves and to place lost animals. The brand is applied by heating an iron template to red-hot and then burning the brand into the hide of the animal. In that context, the title of this essay may be quite disturbing.

#index 778484
#* Spatial, temporal and spatio-temporal databases - hot issues and directions for phd research
#@ John F. Roddick;Erik Hoel;Max J. Egenhofer;Dimitris Papadias;Betty Salzberg
#t 2004
#c 5
#% 451553
#% 729850
#% 742060
#% 993955
#% 1015321
#! Spatial and temporal database systems, both in theory and in practice, have developed dramatically over the past two decades to the point where usable commercial systems, underpinned by a robust theoretical foundation, are now starting to appear. While much remains to be done, topics for research must be chosen carefully to avoid embarking on impractical or unprofitable areas. This is particularly true for doctoral research where the candidate must build a tangible contribution in a relatively short time.The panel session at the Eighth International Symposium on Spatial and Temporal Databases (SSTD 2003) held on Santorini Island, Greece [7] in July 2003 thus took as its focus the question What to focus on (and what to avoid) in Spatial and Temporal Databases: recommendations for doctoral research. This short paper, authored by the panel members, summarizes these discussions.

#index 778485
#* REPORT on the EDBT'04 workshop on database technologies for handling XML information on the web
#@ Marco Mesiti;Barbara Catania;Giovanna Guerrini;Akmal Chaudhri
#t 2004
#c 5
#% 454834
#! The EDBT'04 Workshop on Database Technologies for Handling XML Information on the Web (DataX'04) was held in Heraklion, Crete, on Sunday 14 March, 2004, and attracted approximately 30 participants from different countries.

#index 778486
#* Issues in mechanical engineering design management
#@ Dave Hislop;Zoé Lacroix;Gerald Moeller
#t 2004
#c 5
#% 157103
#% 339246
#% 346656
#% 379942
#! The Virtual Parts Engineering Research Center (VPERC), funded by the Army Research Office, focuses on building frameworks, tools, and technologies for making engineered systems sustainable and maintainable thanks to a virtual engineering environment intended to transform the engineering process, thus supporting extremely fast turnaround times for urgent part supply needs. One of its key research thrusts is data management targeted for the design process. The invitational VPERC workshop held at Arizona State University in June 2003, involved 38 participants from academia, governmental institutions and industry who discussed legacy systems engineering. This paper presents the data management needs to support mechanical engineering design as they were discussed at the meeting.

#index 783783
#* A denotational semantics for continuous queries over streams and relations
#@ Arvind Arasu;Jennifer Widom
#t 2004
#c 5
#% 125951
#% 375245
#% 378388
#% 578560
#% 801694
#% 993949
#% 1015363
#! Continuous queries over data streams are an important new class of queries motivated by a number of applications [BBD+02, Geh03, GO03], and several languages for continuous queries have been proposed recently [ABW03, CC+02, CC+03, WZL03]. To date the semantics of these languages have been specified fairly informally, sometimes solely through illustrative examples.

#index 783784
#* A read-only transaction anomaly under snapshot isolation
#@ Alan Fekete;Elizabeth O'Neil;Patrick O'Neil
#t 2004
#c 5
#% 3645
#% 9241
#% 201869
#% 403195
#% 632092
#! Snapshot Isolation (SI), is a multi-version concurrency control algorithm introduced in [BBGMOO95] and later implemented by Oracle. SI avoids many concurrency errors, and it never delays read-only transactions. However it does not guarantee serializability. It has been widely assumed that, under SI, read-only transactions always execute serializably provided the concurrent update transactions are serializable. The reason for this is that all SI reads return values from a single instant of time when all committed transactions have completed their writes and no writes of non-committed transactions are visible. This seems to imply that read-only transactions will not read anomalous results so long as the update transactions with which they execute do not write such results. In the current note, however, we exhibit an example contradicting these assumptions: it is possible for an SI history to be non-serializable while the sub-history containing all update transactions is serializable.

#index 783785
#* CiVeDi: a customized virtual environment for database interaction
#@ Pietro Mazzoleni;Elisa Bertino;Elena Ferrari;Stefano Valtolina
#t 2004
#c 5
#% 85706
#% 310211
#% 413717
#% 528039
#% 620005
#% 632375
#% 1855592
#! This paper presents CiVedi, a scalable system providing a flexible and customizable virtual environment for displaying multimedia contents. Using CiVeDi, both the final users and the exhibition curators can personalize the content of the visit as well as the visit appearance and its duration. The proposed solution aims to be used transparently over different media objects either stored into a database or dynamically collected from online digital libraries.

#index 783786
#* Semantically enriched web services for the travel industry
#@ A. Dogac;Y. Kabak;G. Laleci;S. Sinir;A. Yildiz;S. Kirbas;Y. Gurcan
#t 2004
#c 5
#% 459496
#% 519428
#% 753534
#! Today, the travel information services are dominantly provided by Global Distribution Systems (GDS). The Global Distribution Systems provide access to real time availability and price information for flights, hotels and car rental companies. However GDSs have legacy architectures with private networks, specialized hardware, limited speed and search capabilities. Furthermore, being legacy systems, it is very difficult to interoperate them with other systems and data sources. For these reasons, Web service technology is an ideal fit for travel information systems. However to be able to exploit Web services to their full potential, it is necessary to introduce semantics. Without describing the semantics of Web services we are looking for, it is difficult to find them in an automated way and if we cannot describe the service we have, the probability that people will find it in an automated way is low. Furthermore, to make the semantics machine processable and interoperable, we need to describe domain knowledge through standard ontology languages. In this paper, we describe how to deploy semantically enriched travel Web services and how to exploit semantics through Web service registries. We also address the need to use the semantics in discovering both Web services and Web service registries through peer-to-peer technology.

#index 783787
#* An evaluation of XML indexes for structural join
#@ Hanyu Li;Mong Li Lee;Wynne Hsu;Chao Chen
#t 2004
#c 5
#% 333981
#% 397375
#% 480489
#% 993953
#! XML queries differ from relational queries in that the former are expressed as path expressions. The efficient handling of structural relationships has become a key factor in XML query processing. Many index-based solutions have been proposed for efficient structural join in XML queries. This work explores the state-of-the-art indexes, namely, B+-tree, XB-tree and XR-tree, and analyzes how well they support XML structural joins. Experiment results indicate that all three indexes yield comparable performances for non-recursive XML data, while the XB-tree outperforms the rest for highly recursive XML data.

#index 783788
#* Optimization of data stream processing
#@ Janusz R. Getta;Ehsan Vossough
#t 2004
#c 5
#% 116082
#% 227883
#% 300167
#% 336201
#% 340635
#% 378388
#% 397352
#% 397353
#% 480120
#% 654462
#% 654507
#% 654508
#% 654510
#% 660004
#% 993949
#% 1015282
#! Efficient processing of unlimited and continuously expanding sequences of data items is one of the key factors in the implementations of Data Stream Management Systems (DSMS). Analysis of stream processing at the dataflow level reveals execution plans which are not visible at a logical level. This work introduces a new model of data stream processing and discusses a number of optimization techniques applicable to this model and its implementation. The optimization techniques include applications of containers with intermediate results, analysis of data processing rates, and efficient synchronization of elementary operations on data streams. The paper also describes the translation of logical level expressions on data streams into the sets of dataflow level expressions, syntax based optimization of dataflow expression, and scheduling of concurrent computations of the dataflow expressions.

#index 783789
#* Phil Bernstein speaks out on trends at industrial research labs: what metadata management products need, peculiarities of the tenure sytem, how to fix the problems with database research conferences, and more
#@ Marianne Winslett
#t 2004
#c 5
#! This interview with Phil Bernstein was videotaped at SIGMOD 2002 in Madison, Wisconsin, and revised slightly in July 2004 to bring it up to date.

#index 783790
#* Integration of biological sources: current systems and challenges ahead
#@ Thomas Hernandez;Subbarao Kambhampati
#t 2004
#c 5
#% 85086
#% 261741
#% 283052
#% 342684
#% 378409
#% 397951
#% 449220
#% 449226
#% 465159
#% 471746
#% 480822
#% 481614
#% 565184
#% 568191
#% 589356
#% 589376
#% 617862
#% 617885
#% 745490
#% 772131
#% 772132
#! This paper surveys the area of biological and genomic sources integration, which has recently become a major focus of the data integration research field. The challenges that an integration system for biological sources must face are due to several factors such as the variety and amount of data available, the representational heterogeneity of the data in the different sources, and the autonomy and differing capabilities of the sources. This survey describes the main integration approaches that have been adopted. They include warehouse integration, mediator-based integration, and navigational integration. Then we look at the four major existing integration systems that have been developed for the biological domain: SRS, BioKleisli, TAMBIS, and DiscoveryLink. After analyzing these systems and mentioning a few others, we identify the pros and cons of the current approaches and systems and discuss what an integration system for biologists ought to be.

#index 783791
#* Structured databases on the web: observations and implications
#@ Kevin Chen-Chuan Chang;Bin He;Chengkai Li;Mitesh Patel;Zhen Zhang
#t 2004
#c 5
#% 261741
#% 267454
#% 273925
#% 273926
#% 309783
#% 332166
#% 333932
#% 445255
#% 464222
#% 464717
#% 479451
#% 479642
#% 480479
#% 480824
#% 481923
#% 654459
#% 745535
#% 765410
#% 769890
#! The Web has been rapidly "deepened" by the prevalence of databases online. With the potentially unlimited information hidden behind their query interfaces, this "deep Web" of searchable databses is clearly an important frontier for data access. This paper surveys this relatively unexplored frontier, measuring characteristics pertinent to both exploring and integrating structured Web sources. On one hand, our "macro" study surveys the deep Web at large, in April 2004, adopting the random IP-sampling approach, with one million samples. (How large is the deep Web? How is it covered by current directory services?) On the other hand, our "micro" study surveys source-specific characteristics over 441 sources in eight representative domains, in December 2002. (How "hidden" are deep-Web sources? How do search engines cover their data? How complex and expressive are query forms?) We report our observations and publish the resulting datasets to the research community. We conclude with several implications (of our own) which, while necessarily subjective, might help shape research directions and solutions.

#index 783792
#* NESTREAM: querying nested streams
#@ Damianos Chatziantoniou;Achilleas Anagnostopoulos
#t 2004
#c 5
#% 262249
#% 333926
#% 378388
#% 459024
#% 482082
#% 482088
#% 578391
#% 632070
#% 654497
#% 993949
#% 1015283
#! This article identifies an interesting class of applications where stream sessions may be organized in a hierarchical fashion - i.e. sessions may contain sub-sessions. For example, log streams from call centers belong to different call sessions and call sessions consist of services' sub-sessions. We may want to monitor statistics and perform accounting at any level on this hierarchy, relative to any other higher level (e.g. monitoring the average service session per call vs. the average service session for the entire system.) We argue that data streams of this kind have rich procedural semantics - i.e. behavior - and therefore a semantically rich model should be used: a session may be defined by opening and closing conditions, may have data and methods and may consist of sub-sessions. We propose a simple conceptual model based on the notion of "session" - similar to a class in an object-oriented environment -- having lifetime semantics. Queries on top of this schema can be formulated via HSA (hierarchical stream aggregate) expressions. We describe an algorithm dictating how stream data flow down session hierarchies and discuss potential evaluation and optimization techniques for HSAs. Finally we introduce NESTREAM, a prototype implementation for these ideas and give some preliminary experimental results.

#index 783793
#* Advancements in SQL/XML
#@ Andrew Eisenberg;Jim Melton
#t 2004
#c 5
#% 397607
#! Since we last wrote about SQL/XML in [2], the first edition of that new part of the SQL standard has been officially published as an international standard [1], commonly called SQL/XML:2003. At the time of that earlier column, SQL/XML was just entering its first official ballot, meaning that (possibly significant) changes to the text were expected in response to ballot comments submitted by the various participants in the SQL standardization process.

#index 783794
#* Report on the 9th ACM symposium on access control models and technologies (SACMAT'04)
#@ Elena Ferrari
#t 2004
#c 5
#! SACMAT'04 was held on June 2--4, 2004, at Yorktown Heights, New York, USA and was hosted by IBM T.J. Watson Research Center. The symposium, which was colocated with the IEEE International Workshop on Policies for Distributed Systems and Networks (POLICY 2004), continues its tradition of being the premier forum for presentation of research results and experience reports on leading edge issues of access control and related technologies, including models, systems, applications, and theory. SACMAT gives researchers and practitioners a unique opportunity to share their perspectives with others interested in the various aspects of access control. The call for papers attracted 65 submissions, from all over the world. The program committee selected 18 papers for presentation. The 18 papers, organized in seven sessions, were presented over two and an half days. The selected papers cover a wide range of topics, ranging from next generation access control models, to security analysis, role administration, policy specification and implementation, and access control for distributed environments and XML data. All the accepted papers were included in a volume published by ACM, whereas best papers from the symposium have been invited for possible publication in ACM Transactions on Information and System Security (TISSEC). Besides the technical sessions, this year program included a keynote speech on Access Control for Databases, and a panel on Security for Grid-based Computing Systems.

#index 783795
#* Report on the 3rd web dynamics workshop, at WWW'2004
#@ Mark Levene;Alexandra Poulovassilis
#t 2004
#c 5
#% 750711
#! The web is highly dynamic in both the content and quantity of the information that it encompasses. In order to fully exploit its enormous potential as a global repository of information, we need to understand how its size, topology, and content are evolving. This then allows the development of new techniques for locating and retrieving information that are better able to adapt and scale to its change and growth. The web's users are highly diverse and can access the it from a variety of devices and interfaces, at different places and times, and for varying purposes. Thus, new techniques are being developed for personalising the presentation and content of web-based information depending on how it is being accessed and on the individual user's requirements and preferences. New applications in areas such as e-business, sensor networks, and mobile and ubiquitous computing need to be able to detect and react quickly to events and changes in web-based information. Traditional approaches using query-based `pull' of information to find out if events or changes of interest have occurred may not be able to scale to the quantity and frequency of events and changes being generated, and new `push'-based techniques are being deployed in which information producers automatically notify consumers when events or changes of interest to them occur. Semantic Web and Web Service technologies are being developed and adopted, with the aim of providing standard ways for web-based applications to share and personalise information.

#index 783796
#* Report on the workshop on metadata management in grid and peer-to-peer systems, London, December 16 2003
#@ Kevin Keenoy;Alexandra Poulovassilis;Vassilis Christophides;George Loizou;Giorgos Kokkinidis;George Samaras;Nicolas Spyratos
#t 2004
#c 5
#! A workshop on Metadata Management in Grid and Peer-to-Peer Systems was held in the Senate House of the University of London on December 16 2003.1 The workshop was organised by the SeLeNe (Self e-Learning Networks) IST project as part of its dissemination activities.2 The goal of the workshop was to identify recent technological achievements and open challenges regarding metadata management in novel applications requiring peer-to-peer information management in a distributed or Grid setting. The target audience for this event were researchers from the Grid, peer-to-peer and e-learning communities, as well as other application areas requiring Grid and/or peer-to-peer support. The event attracted 43 participants from 8 different European countries, and we believe that it was an important step in coordinating research activities in these inter-related areas. The presentations at the workshop fell into one of four sessions, each of which we report on below.

#index 790843
#* Introduction to the special issue on semantic integration
#@ AnHai Doan;Natalya F. Noy;Alon Y. Halevy
#t 2004
#c 5
#% 22948
#% 278397
#% 333988
#% 333990
#% 378409
#% 488766
#% 572311
#% 654458
#% 654459
#% 765409
#% 800497
#% 830529
#% 993982
#! Semantic heterogeneity is one of the key challenges in integrating and sharing data across disparate sources, data exchange and migration, data warehousing, model management, the Semantic Web and peer-to-peer databases. Semantic heterogeneity can arise at the schema level and at the data level. At the schema level, sources can differ in relations, attribute and tag names, data normalization, levels of detail, and the coverage of a particular domain. The problem of reconciling schema-level heterogeneity is often referred to as schema matching or schema mapping. At the data level, we find different representations of the same real-world entities (e.g., people, companies, publications, etc.). Reconciling data-level heterogeneity is referred to as data deduplication, record linkage, and entity/object matching. To exacerbate the heterogeneity challenges, schema elements of one source can be represented as data in another. This special issue presents a set of articles that describe recent work on semantic heterogeneity at the schema level.

#index 790844
#* Automatic direct and indirect schema mapping: experiences and lessons learned
#@ David W. Embley;Li Xu;Yihong Ding
#t 2004
#c 5
#% 300288
#% 307632
#% 333990
#% 443408
#% 449212
#% 480645
#% 533935
#% 572314
#% 572457
#% 587740
#% 660001
#% 728755
#% 732239
#% 765433
#% 993981
#! Schema mapping produces a semantic correspondence between two schemas. Automating schema mapping is challenging. The existence of 1:n (or n:1) and n:m mapping cardinalities makes the problem even harder. Recently, we have studied automated schema mapping techniques (using data frames and domain ontology snippets) that not only address the traditional 1:1 mapping problem, but also the harder 1:n and n:m mapping problems. Experimental results show that the approach can achieve excellent precision and recall. In this paper, we share our experiences and lessons we have learned during our schema mapping studies.

#index 790845
#* A holistic paradigm for large scale schema matching
#@ Bin He;Kevin Chen-Chuan Chang
#t 2004
#c 5
#% 22948
#% 333988
#% 480645
#% 481911
#% 572314
#% 654459
#% 765409
#% 765433
#% 769890
#% 783791
#% 1015326
#! Schema matching is a critical problem for integrating heterogeneous information sources. Traditionally, the problem of matching multiple schemas has essentially relied on finding pairwise-attribute correspondences in isolation. In contrast, we propose a new matching paradigm, holistic schema matching, to match many schemas at the same time and find all matchings at once. By handling a set of schemas together, we can explore their context information that reflects the semantic correspondences among attributes. Such information is not available when schemas are matched only in pairs. As the realizations of holistic schema matching, we develop two alternative approaches: global evaluation and local evaluation. Global evaluation exhaustively assesses all possible "models," where a model expresses all attribute matchings. In particular, we propose the MGS framework for such global evaluation, building upon the hypothesis of the existence of a hidden schema model that probabilistically generates the schemas we observed. On the other hand, local evaluation independently assesses every single matching to incrementally construct such a model. In particular, we develop the DCM framework for local evaluation, building upon the observation that co-occurrence patterns across schemas often reveal the complex relationships of attributes. We apply our approaches to match query interfaces on the deep Web. The result shows the effectiveness of both the MGS and DCM approaches, which together demonstrate the promise of holistic schema matching.

#index 790846
#* Matching large XML schemas
#@ Erhard Rahm;Hong-Hai Do;Sabine Maßmann
#t 2004
#c 5
#% 176887
#% 278445
#% 307632
#% 333990
#% 452859
#% 480645
#% 551850
#% 572314
#% 660001
#% 745476
#% 790848
#% 993982
#! Current schema matching approaches still have to improve for very large and complex schemas. Such schemas are increasingly written in the standard language W3C XML schema, especially in E-business applications. The high expressive power and versatility of this schema language, in particular its type system and support for distributed schemas and name-spaces, introduce new issues. In this paper, we study some of the important problems in matching such large XML schemas. We propose a fragment-oriented match approach to decompose a large match problem into several smaller ones and to reuse previous match results at the level of schema fragments.

#index 790847
#* Kanata: adaptation and evolution in data sharing systems
#@ Periklis Andritsos;Ariel Fuxman;Anastasios Kementsietsidis;Renée J. Miller;Yannis Velegrakis
#t 2004
#c 5
#% 199537
#% 201898
#% 273687
#% 283052
#% 328429
#% 378409
#% 384978
#% 443527
#% 465057
#% 480134
#% 572311
#% 572314
#% 576116
#% 654468
#% 723449
#% 765462
#% 777935
#% 801676
#% 993981
#% 1015302
#% 1015303
#% 1016168
#! In Toronto's Kanata project, we are investigating the integration and exchange of data and metadata in dynamic, autonomous environments. Our focus is on the development and maintenance of semantic mappings that permit runtime sharing of information.

#index 790848
#* Industrial-strength schema matching
#@ Philip A. Bernstein;Sergey Melnik;Michalis Petropoulos;Christoph Quix
#t 2004
#c 5
#% 463919
#% 480134
#% 480645
#% 572314
#% 660001
#% 745476
#% 993982
#! Schema matching identifies elements of two given schemas that correspond to each other. Although there are many algorithms for schema matching, little has been written about building a system that can be used in practice. We describe our initial experience building such a system, a customizable schema matcher called Protoplasm.

#index 790849
#* From semantic integration to semantics management: case studies and a way forward
#@ Arnon Rosenthal;Len Seligman;Scott Renner
#t 2004
#c 5
#% 198055
#% 263981
#% 481427
#! For meaningful information exchange or integration, providers and consumers need compatible semantics between source and target systems. It is widely recognized that achieving this semantic integration is very costly. Nearly all the published research concerns how system integrators can discover and exploit semantic knowledge in order to better share data among the systems they already have. This research is very important, but to make the greatest impact, we must go beyond after-the-fact semantic integration among existing systems, to actively guiding semantic choices in new ontologies and systems - e.g., what concepts should be used as descriptive vocabularies for existing data, or as definitions for newly built systems. The goal is to ease data sharing for both new and old systems, to ensure that needed data is actually collected, and to maximize over time the business value of an enterprise's information systems.

#index 790850
#* Multiplex, Fusionplex and Autoplex: three generations of information integration
#@ Amihai Motro;Jacob Berlin;Philipp Anokhin
#t 2004
#c 5
#% 24408
#% 227992
#% 229827
#% 301084
#% 333990
#% 445255
#% 480134
#% 480645
#% 481923
#% 488766
#% 572311
#% 735938
#% 768941
#! We describe three generations of information integration systems developed at George Mason University. All three systems adopt a virtual database design: a global integration schema, a mapping between this schema and the schemas of the participating information sources, and automatic interpretation of global queries. The focus of Multiplex is rapid integration of very large, evolving, and heterogeneous collections of information sources. Fusionplex strengthens these capabilities with powerful tools for resolving data inconsistencies. Finally, Autoplex takes a more proactive approach to integration, by "recruiting" contributions to the global integration schema from available information sources. Using machine learning techniques it confronts a major cost of integration, that of mapping new sources into the global schema.

#index 790851
#* Ontologies and semantics for seamless connectivity
#@ Michael Uschold;Michael Gruninger
#t 2004
#c 5
#% 156337
#% 348187
#% 643964
#% 731212
#% 759723
#% 778138
#! The goal of having networks of seamlessly connected people, software agents and IT systems remains elusive. Early integration efforts focused on connectivity at the physical and syntactic layers. Great strides were made; there are many commercial tools available, for example to assist with enterprise application integration. It is now recognized that physical and syntactic connectivity is not adequate. A variety of research systems have been developed addressing some of the semantic issues. In this paper, we argue that ontologies in particular and semantics-based technologies in general will play a key role in achieving seamless connectivity. We give a detailed introduction to ontologies, summarize the current state of the art for applying ontologies to achieve semantic connectivity and highlight some key challenges.

#index 790852
#* Semantic integration: a survey of ontology-based approaches
#@ Natalya F. Noy
#t 2004
#c 5
#% 237519
#% 344361
#% 348187
#% 384416
#% 445288
#% 459496
#% 480645
#% 572314
#% 731207
#% 731208
#% 735938
#% 742769
#% 790851
#% 1289178
#! Semantic integration is an active area of research in several disciplines, such as databases, information-integration, and ontologies. This paper provides a brief survey of the approaches to semantic integration developed by researchers in the ontology community. We focus on the approaches that differentiate the ontology research from other related areas. The goal of the paper is to provide a reader who may not be very familiar with ontology research with introduction to major themes in this research and with pointers to different research projects. We discuss techniques for finding correspondences between ontologies, declarative ways of representing these correspondences, and use of these correspondences in various semantic-integration tasks

#index 790853
#* UbiData: requirements and architecture for ubiquitous data access
#@ Abdelsalam Helal;Joachim Hammer
#t 2004
#c 5
#% 70068
#% 227859
#% 236416
#% 240013
#% 245017
#% 346767
#% 414101
#% 499883
#% 622753
#% 653985
#% 657595
#% 728368
#% 979694
#! Mobile users today demand ubiquitous access to their data from any mobile device and under variable connection quality. We refer to this requirement as any-time, any-where data access whose realization requires much more support for asynchronous and disconnected operation than is currently available from existing research prototypes or commercial products. Furthermore, the proliferation of mobile devices and applications, forges the additional requirement of device- and application-transparent data access. Support for such any-device, any-application computing paradigm requires the ability to store and manage data in a generic representation and to transform it for usage in different applications, which may also be running on different platforms. In this article, we give an overview of the UbiData architecture and prototype system and show how it addresses these challenging requirements. We also summarize our ongoing and future efforts.

#index 790854
#* C. Mohan speaks out: on R*, message queues, computer science in India, how ARIES came about, life as an IBM fellow, and more
#@ Marianne Winslett
#t 2004
#c 5

#index 790855
#* Review of "Web Services by G. Alonso, F. Casati, H. Kuno, and V. Machiraju", Springer-Verlag, 2004, ISBN 30540-44008-9
#@ Dirk Wodtke
#t 2004
#c 5

#index 790856
#* Review of "Emergence and Convergence: Qualitative Novelty and the Unity of Knowledge by Mario Bunge", University of Toronto Press, 2004, ISBN 0-8020-8860-0.
#@ Haim Kilov
#t 2004
#c 5
#% 57961
#% 141551
#% 287333
#% 287730
#% 739909

#index 790857
#* Report on the seventh EDBT summer school: XML & databases
#@ Riccardo Torlone;Paolo Atzeni
#t 2004
#c 5
#! The Seventh EDBT Summer School 2004 was held in Santa Margherita di Pula, Sardinia, Italy, September 6-10, 2004. It continued a successful series of international schools promoted by the EDBT Association and dedicated to the dissemination of latest advances in database management. Indeed, the EDBT Association (a Europe-based non-profit organization also known as the EDBT Endowment) has the purpose of promoting and supporting the progress and the study of the database technology and application. Its two major initiatives are the EDBT Conference (International Conference on Extending Data Base Technology), held every two years since 1988, and the EDBT Summer School, held almost with the same regularity, since 1991. Previous schools took place in Alghero (Italy) in 1991, Leysin (Switzerland) in 1993, Gubbio (Italy) in 1995, Capri (Italy) in 1997, La Baule (France) in 1999, and Cargèese (France) in 2002. Some information on the previous editions is available at the EDBT Association site (www.edbt.org), which also illustrates the goals and structure of the association.

#index 790858
#* Report on the first "XQuery Implementation, Experience and Perspectives" workshop (XIME-P)
#@ Ioana Manolescu;Yannis Papakonstantinou
#t 2004
#c 5
#! The XQuery Implementation, Experience and Perspectives (XIME-P) workshop was organized by Ioana Manolescu and Yannis Papakonstantinou in cooperation with the ACM SIGMOD Conference, and was held in Maison de la Chimie, in Paris, France, on June 17 and 18, 2004. This report summarizes the goals and topics of the workshop, presents the major workshop highlights and the main issues discussed during the workshop.

#index 790859
#* Report on the first twente data management workshop on XML databases and information retrieval
#@ Djoerd Hiemstra;Vojkan Mihajlović
#t 2004
#c 5
#% 752308
#% 766416
#% 778485
#% 801669
#% 1016150
#% 1279272
#! The Database Group of the University of Twente initiated a new series of workshops called Twente Data Management workshops (TDM), starting with one on XML Databases and Information Retrieval which took place on 21 June 2004 at the University of Twente. We have set ourselves two goals for the workshop series: i) To provide a forum to share original ideas as well as research results on data management problems; ii) To bring together researchers from the database community and researchers from related research fields

#index 801667
#* Proceedings of the twenty-third ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems
#@ Catriel Beeri
#t 2004
#c 5
#! This volume contains the proceedings of the Twenty-third ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems (PODS 2004), held at Le Maison de la Chimie, Paris, France, on June 14-16, 2004 in conjunction with the 2004 ACM SIGMOD International Conference on Management of Data. It consists of a paper based on the invited talk by Georg Gottlob, and 30 contributed papers that were selected by the program committee for presentation at the symposium.The program committee has selected the contributed papers from 183 submissions. Some of the papers are \extended abstracts" and are preliminary reports on work in progress. While the program committee has read them carefully and discussed them intensively, they have not been formally refereed. It is expected that much of the research described in these papers will be published in detail in computer science journals, where they will go through a formal review.The program committee selected Conditional XPath, the First Order Complete XPath Dialect by Maarten Marx for both the PODS 2004 Best Paper Award and the Best Newcomer Award. Warmest congratulations to the author.The year 2004 marks the first time when the SIGMOD/PODS event was located outside of North America. This recognizes the international status of the event, and will hopefully start a beautiful new tradition.

#index 801668
#* The Lixto data extraction project: back and forth between theory and practice
#@ Georg Gottlob;Christoph Koch;Robert Baumgartner;Marcus Herzog;Sergio Flesca
#t 2004
#c 5
#% 49315
#% 54225
#% 101944
#% 183411
#% 241166
#% 271065
#% 275922
#% 331772
#% 342829
#% 344425
#% 384978
#% 401124
#% 424931
#% 459241
#% 480648
#% 576106
#% 576108
#% 587580
#% 632051
#% 733595
#% 801686
#% 993939
#% 1015275
#% 1279271
#! We present the Lixto project, which is both a research project in database theory and a commercial enterprise that develops Web data extraction (wrapping) and Web service definition software. We discuss the project's main motivations and ideas, in particular the use of a logic-based framework for wrapping. Then we present theoretical results on monadic datalog over trees and on Elog, its close relative which is used as the internal wrapper language in the Lixto system. These results include both a characterization of the expressive power and the complexity of these languages. We describe the visual wrapper specification process in Lixto and various practical aspects of wrapping. We discuss work on the complexity of query languages for trees that was inseminated by our theoretical study of logic-based languages for wrapping. Then we return to the practice of wrapping and the Lixto Transformation Server, which allows for streaming integration of data extracted from Web pages. This is a natural requirement in complex services based on Web wrapping. Finally, we discuss industrial applications of Lixto and point to open problems for future study.

#index 801669
#* Conditional XPath, the first order complete XPath dialect
#@ Maarten Marx
#t 2004
#c 5
#% 168262
#% 191611
#% 299976
#% 357150
#% 378393
#% 399031
#% 465065
#% 473117
#% 473125
#% 473422
#% 483548
#% 510128
#% 576108
#% 599549
#% 993939
#% 1015275
#! XPath is the W3C -- standard node addressing language for XML documents. XPath is still under development and its technical aspects are intensively studied. What is missing at present is a clear characterization of the expressive power of XPath, be it either semantical or with reference to some well established existing (logical) formalism. Core XPath (the logical core of XPath 1.0 defined by Gottlob et al.) cannot express queries with conditional paths as exemplified by "do a child step, while test is true at the resulting node." In a first-order complete extension of Core XPath, such queries are expressible, We add conditional axis relations to Core XPath and show that the resulting language, called conditional XPath, is equally expressive as first-order logic when interpreted on ordered trees. Both the result, the extended XPath language, and the proof are closely related to temporal logic. Specifically, while Core XPath may be viewed as a simple temporal logic, conditional XPath extends this with (counterparts of) the since and until operators.

#index 801670
#* Frontiers of tractability for typechecking simple XML transformations
#@ Wim Martens;Frank Neven
#t 2004
#c 5
#% 6242
#% 241160
#% 273702
#% 299944
#% 342438
#% 345757
#% 427027
#% 427874
#% 465051
#% 465054
#% 465059
#% 465061
#% 562461
#% 564264
#% 571040
#% 572328
#% 575379
#% 630965
#% 643569
#% 722731
#! Typechecking consists of statically verifying whether the output of an XML transformation is always conform to an output type for documents satisfying a given input type. We focus on complete algorithms which always produce the correct answer. We consider top-down XML transformations incorporating XPath expressions and abstract document types by grammars and tree automata. By restricting schema languages and transformations, we identify several practical settings for which typechecking is in polynomial time. Moreover, the resulting framework provides a rather complete picture as we show that most scenarios can not be enlarged without rendering the typechecking problem intractable. So, the present research sheds light on when to use fast complete algorithms and when to reside to sound but incomplete ones.

#index 801671
#* Positive active XML
#@ Serge Abiteboul;Omar Benjelloun;Tova Milo
#t 2004
#c 5
#% 58356
#% 91072
#% 188086
#% 189739
#% 237193
#% 286973
#% 333858
#% 378388
#% 378396
#% 384978
#% 394417
#% 593696
#% 654465
#% 654485
#% 765420
#% 994034
#! The increasing popularity of XML and Web services have given rise to a new generation of documents, called Active XML documents (AXML), where some of the data is given explicitly while other parts are given intensionally, by means of embedded calls to Web services. Web services in this context can exchange intensional information, using AXML documents as parameters and results.The goal of this paper is to provide a formal foundation for this new generation of AXML documents and services, and to study fundamental issues they raise. We focus on Web services that are (1) monotone and (2) defined declaratively as conjunctive queries over AXML documents. We study the semantics of documents and queries, the confluence of computations, termination and lazy query evaluation.

#index 801672
#* The past, present and future of web information retrieval
#@ Monika Henzinger
#t 2004
#c 5
#! Web search engines have emerged as one of the central applications on the Internet. In fact, search has become one of the most important activities that people engage in on the the Internet. Even beyond becoming the number one source of information, a growing number of businesses are depending on web search engines for customer acquisition.The first generation of web search engines used text-only retrieval techniques. Google revolutionized the field by deploying the PageRank technology - an eigenvector-based analysis of the hyperlink structure - to analyze the web in order to produce relevant results. Moving forward, our goal is to achieve a better understanding of a page with a view towards producing even more relevant results.An exciting new form of search for the future is query-free search: While a user performs her daily tasks, searches are automatically performed to supply her with information that is relevant to her activity. We present one type of query-free search, namely query-free news search: While a user watches TV news the system finds in real-time web pages that are relevant to the news stories.

#index 801673
#* Comparing and aggregating rankings with ties
#@ Ronald Fagin;Ravi Kumar;Mohammad Mahdian;D. Sivakumar;Erik Vee
#t 2004
#c 5
#% 330769
#% 333854
#% 340936
#% 348165
#% 413613
#% 453464
#% 464451
#% 577339
#% 654466
#% 728360
#% 1272396
#! Rank aggregation has recently been proposed as a useful abstraction that has several applications, including meta-search, synthesizing rank functions from multiple indices, similarity search, and classification. In database applications (catalog searches, fielded searches, parametric searches, etc.), the rankings are produced by sorting an underlying database according to various fields. Typically, there are a number of fields that each have very few distinct values, and hence the corresponding rankings have many ties in them. Known methods for rank aggregation are poorly suited to this context, and the difficulties can be traced back to the fact that we do not have sound mathematical principles to compare two partial rankings, that is, rankings that allow ties.In this work, we provide a comprehensive picture of how to compare partial rankings, We propose several metrics to compare partial rankings, present algorithms that efficiently compute them, and prove that they are within constant multiples of each other. Based on these concepts, we formulate aggregation problems for partial rankings, and develop a highly efficient algorithm to compute the top few elements of a near-optimal aggregation of multiple partial rankings. In a model of access that is suitable for databases, our algorithm reads essentially as few elements of each partial ranking as are necessary to determine the winner(s).

#index 801674
#* Using non-linear dynamical systems for web searching and ranking
#@ Panayiotis Tsaparas
#t 2004
#c 5
#% 83976
#% 249110
#% 262061
#% 268079
#% 281209
#% 282481
#% 282905
#% 309779
#% 309868
#% 330707
#% 338443
#% 340932
#% 466574
#% 479659
#% 788584
#% 1650298
#! In the recent years there has been a surge of research activity in the area of information retrieval on the World Wide Web, using link analysis of the underlying hypertext graph topology. Most of the algorithms in the literature can be described as dynamical systems, that is, the repetitive application of a function on a set of weights. Algorithms that rely on eigenvector computations, such as HITS and PAGERANK, correspond to linear dynamical systems. In this work we consider two families of link analysis ranking algorithms that no longer enjoy the linearity property of the previous approaches. We study in depth an interesting special case of these two families. We prove that the corresponding non-linear dynamical system converges for any initialization, and we provide a rigorous characterization of the combinatorial properties of the stationary weights. The study of the weights provides a clear and insightful view of the mechanics of the algorithm. We also present extensive experimental results that demonstrate that our algorithm performs well in practice.

#index 801675
#* Specification and verification of data-driven web services
#@ Alin Deutsch;Liying Sui;Victor Vianu
#t 2004
#c 5
#% 101955
#% 114677
#% 169697
#% 185412
#% 190683
#% 213957
#% 248013
#% 298915
#% 321054
#% 342119
#% 348131
#% 384978
#% 425200
#% 519432
#% 565496
#% 571038
#% 576091
#% 630964
#% 1599270
#! We study data-driven Web services provided by Web sites interacting with users or applications. The Web site can access an underlying database, as well as state information updated as the interaction progresses, and receives user input. The structure and contents of Web pages, as well as the actions to be taken, are determined dynamically by querying the underlying database as well as the state and inputs. The properties to be verified concern the sequences of events (inputs, states, and actions) resulting from the interaction, and are expressed in linear or branching-time temporal logics. The results establish under what conditions automatic verification of such properties is possible and provide the complexity of verification. This brings into play a mix of techniques from logic and automatic verification.

#index 801676
#* Composing schema mappings: second-order dependencies to the rescue
#@ Ronald Fagin;Phokion G. Kolaitis;Wang-Chiew Tan;Lucian Popa
#t 2004
#c 5
#% 583
#% 33376
#% 150197
#% 248038
#% 262718
#% 289384
#% 378409
#% 384978
#% 465057
#% 480134
#% 488616
#% 576100
#% 577359
#% 993981
#% 1015302
#! A schema mapping is a specification that describes how data structured under one schema (the source schema) is to be transformed into data structured under a different schema (the target schema). Schema mappings play a key role in numerous areas of database systems, including database design, information integration, and model management. A fundamental problem in this context is composing schema mappings: given two successive schema mappings, derive a schema mapping between the source schema of the first and the target schema of the second that has the same effect as applying successively the two schema mappings.In this paper, we give a rigorous semantics to the composition of schema mappings and investigate the definability and computational complexity of the composition of two schema mappings. We first study the important case of schema mappings in which the specification is given by a finite set of source-to-target tuple-generating dependencies (source-to-target tgds). We show that the composition of a finite set of full source-to-target tgds with a finite set of tgds is always definable by a finite set of source-to-target tgds, but the composition of a finite set of source-to-target tgds with a finite set of full source-to-target tgds may not be definable by any set (finite or infinite) of source-to-target tgds; furthermore, it may not be definable by any formula of least fixed-point logic, and the associated composition query may be NP-complete. After this, we introduce a class of existential second-order formulas with function symbols, which we call second-order tgds, and make a case that they are the "right" language for composing schema mappings. To this effect, we show that the composition of finite sets of source-to-target tgds is always definable by a second-order tgd. Moreover, the composition of second-order tgds is also definable by a second-order tgd. Our second-order tgds allow equalities, even though the "obvious" way to define them does not require equalities. Allowing equalities in second-order tgds turns out to be of the essence, because we show that second-order tgds without equalities are not sufficiently expressive to define even the composition of finite sets of source-to-target tgds. Finally, we show that second-order tgds possess good properties for data exchange. In particular. the chase procedure can be extended to second-order tgds so that it produces polynomial-time computable universal solutions in data exchange settings specified by second-order tgds.

#index 801677
#* Foundations of semantic web databases
#@ Claudio Gutierrez;Carlos Hurtado;Alberto O. Mendelzon
#t 2004
#c 5
#% 47963
#% 129217
#% 252464
#% 273703
#% 348181
#% 384978
#% 509845
#% 519415
#% 519557
#% 576100
#! The Semantic Web is based on the idea of adding more machine-readable semantics to web information via annotations written in a language called the Resource Description Framework (RDF). RDF resembles a subset of binary first-order logic including the ability to refer to anonymous objects. Its extended version, RDFS, supports reification, typing and inheritance. These features introduce new challenges into the formal study of sets of RDF/RDFS statements and languages for querying them. Although several such query languages have been proposed, there has been little work on foundational aspects. We investigate these, including computational aspects of testing entailment and redundancy. We propose a query language with well-defined semantics and study the complexity of query processing, query containment, and simplification of answers.

#index 801678
#* A characterization of first-order topological properties of planar spatial data
#@ Michael Benedikt;Jan Van den Bussche;Christof Löding;Thomas Wilke
#t 2004
#c 5
#% 164406
#% 190332
#% 219805
#% 224744
#% 241166
#% 246560
#% 261690
#% 278828
#% 321057
#% 345772
#% 384978
#% 836138
#% 1562065
#! Closed semi-algebraic sets in the plane form a powerful model of planar spatial datasets. We establish a characterization of the topological properties of such datasets expressible in the relational calculus with real polynomial constraints. The characterization is in the form of a query language that can only talk about points in the set and the "cones" around these points.

#index 801679
#* Roads, codes, and spatiotemporal queries
#@ Sandeep Gupta;Swastik Kopparty;Chinya Ravishankar
#t 2004
#c 5
#% 31486
#% 70370
#% 243015
#% 273706
#% 299979
#% 300174
#% 315005
#% 325314
#% 413797
#% 494027
#% 527198
#% 836124
#% 1015320
#! We present a novel coding-based technique for answering spatial and spatiotemporal queries on objects moving along a system of curves on the plane such as many road networks. We handle join, range, intercept, and other spatial and spatiotemporal queries under these assumptions, with distances being measured along the trajectories. Most work to date has studied the significantly simpler case of objects moving in straight lines on the plane. Our work is an advance toward solving the problem in its more general form.Central to our approach is an efficient coding technique, based on hypercube embedding, for assigning labels to nodes in the network. The Hamming distance between codes corresponds to the physical distance between nodes, so that we can determine shortest distances in the network extremely quickly. The coding method also efficiently captures many properties of the network relevant to spatial and spatiotemporal queries. Our approach also yields a very effective spatial hashing method for this domain. Our analytical results demonstrate that our methods are space- and time-efficient.We have studied the performance of our method for large planar graphs designed to represent road networks. Experiments show that our methods are efficient and practical.

#index 801680
#* Replicated declustering of spatial data
#@ Hakan Ferhatosmanoǧlu;Ali Şaman Tosun;Aravind Ramachandran
#t 2004
#c 5
#% 43179
#% 64432
#% 68091
#% 83129
#% 83235
#% 86950
#% 116042
#% 164363
#% 194176
#% 227856
#% 250026
#% 252304
#% 286962
#% 299983
#% 302731
#% 303086
#% 339622
#% 378390
#% 427199
#% 462233
#% 464718
#% 469603
#% 480794
#% 531039
#% 562329
#% 631955
#% 632069
#% 632996
#% 737367
#! The problem of disk declustering is to distribute data among multiple disks to reduce query response times through parallel I/O. A strictly optimal declustering technique is one that achieves optimal parallel I/O for all possible queries. In this paper, we focus on techniques that are optimized for spatial range queries. Current declustering techniques, which have single copies of the data, have been shown to be suboptimal for range queries. The lower bound on extra disk accesses is proved to be Ω(log N) for N disks even in the restricted case of an N-by-N grid, and all current approaches have been trying to achieve this bound. Replication is a well-known and effective solution for several problems in databases, especially for availability and load balancing. In this paper, we explore the idea of replication in the context of declustering and propose a framework where strictly optimal parallel I/O is achievable using a small amount of replication. We provide some theoretical foundations for replicated declustering, e.g., a bound for number of copies for strict optimality on any number of disks, and propose a class of replicated declustering schemes, periodic allocations, which are shown to be strictly optimal. The results for optimal disk allocation are extended for larger number of disks by increasing replication. Our techniques and results are valid for any arbitrary a-by-b grids, and any declustering scheme can be further improved using our replication framework. Using the framework, we perform experiments to identify a strictly optimal disk access schedule for any given arbitrary range query. In addition to the theoretical bounds, we compare the proposed replication based scheme to other existing techniques by performing experiments on real datasets.

#index 801681
#* Clustering via matrix powering
#@ Hanson Zhou;David Woodruff
#t 2004
#c 5
#% 189842
#% 248027
#% 249176
#% 249305
#% 271130
#% 282481
#% 331989
#% 338442
#% 593913
#% 593937
#% 594009
#% 594011
#% 656762
#% 728120
#% 729918
#! Given a set of n points with a matrix of pairwise similarity measures, one would like to partition the points into clusters so that similar points are together and different ones apart. We present an algorithm requiring only matrix powering that performs well in practice and bears an elegant interpretation in terms of random walks on a graph. Under a certain mixture model involving planting a partition via randomized rounding of tailored matrix entries, the algorithm can be proven effective for only a single squaring. It is shown that the clustering performance of the algorithm degrades with larger values of the exponent, thus revealing that a single squaring is optimal.

#index 801682
#* Computational complexity of itemset frequency satisfiability
#@ Toon Calders
#t 2004
#c 5
#% 3034
#% 42485
#% 152934
#% 248791
#% 300184
#% 320944
#% 336009
#% 338594
#% 408396
#% 478770
#! Computing frequent itemsets is one of the most prominent problems in data mining. We introduce a new, related problem, called FREQSAT: given some itemset-interval pairs, does there exist a database such that for every pair the frequency of the itemset falls in the interval? It is shown in this paper that FREQSAT is not finitely axiomatizable and that it is NP-complete. We also study cases in which other characteristics of the database are given as well. These characteristics can complicate FREQSAT even more. For example, when the maximal number of duplicates of a transaction is known, FREQSAT becomes PP-hard. We describe applications of FREQSAT in frequent itemset mining algorithms and privacy in data mining.

#index 801683
#* k-means projective clustering
#@ Pankaj K. Agarwal;Nabil H. Mustafa
#t 2004
#c 5
#% 36672
#% 210173
#% 248790
#% 248792
#% 273891
#% 300131
#% 304590
#% 316709
#% 325300
#% 397384
#% 479962
#% 480132
#% 480307
#% 481281
#% 548649
#% 656802
#! In many applications it is desirable to cluster high dimensional data along various subspaces, which we refer to as projective clustering. We propose a new objective function for projective clustering, taking into account the inherent trade-off between the dimension of a subspace and the induced clustering error. We then present an extension of the k-means clustering algorithm for projective clustering in arbitrary subspaces, and also propose techniques to avoid local minima. Unlike previous algorithms, ours can choose the dimension of each cluster independently and automatically. Furthermore, experimental results show that our algorithm is significantly more accurate than the previous approaches.

#index 801684
#* Deterministic wavelet thresholding for maximum-error metrics
#@ Minos Garofalakis;Amit Kumar
#t 2004
#c 5
#% 116084
#% 168862
#% 190611
#% 227883
#% 248822
#% 257637
#% 273902
#% 273909
#% 273919
#% 300193
#% 333946
#% 397389
#% 480306
#% 480465
#% 480628
#% 654460
#% 742562
#! Several studies have demonstrated the effectiveness of the wavelet, decomposition as a tool for reducing large amounts of data down to compact, wavelet synopses that can be used to obtain fast, accurate approximate answers to user queries. While conventional wavelet synopses are based on greedily minimizing the overall root-mean-squared (i.e., L2-norm) error in the data approximation, recent work has demonstrated that such synopses can suffer from important problems, including severe bias and wide variance in the quality of the data reconstruction, and lack of non-trivial guarantees for individual approximate answers. As a result, probabilistic thresholding schemes have been recently proposed as a means of building wavelet synopses that try to probabilistically control other approximation-error metrics, such as the maximum relative error in data-value reconstruction, which is arguably the most important for approximate query answers and meaningful error guarantees.One of the main open problems posed by this earlier work is whether it is possible to design efficient deterministic wavelet-thresholding algorithms for minimizing non-L2 error metrics that are relevant to approximate query processing systems, such as maximum relative or maximum absolute error. Obviously, such algorithms can guarantee better wavelet synopses and avoid the pitfalls of probabilistic techniques (e.g., "bad" coin-flip sequences) leading to poor solutions. In this paper, we address this problem and propose novel, computationally efficient schemes for deterministic wavelet thresholding with the objective of optimizing maximum-error metrics. We introduce an optimal low polynomial-time algorithm for one-dimensional wavelet thresholding--our algorithm is based on a new Dynamic-Programming (DP) formulation, and can be employed to minimize the maximum relative or absolute error in the data reconstruction. Unfortunately, directly extending our one-dimensional DP algorithm to multi-dimensional wavelets results in a super-exponential increase in time complexity with the data dimensionality. Thus, we also introduce novel, polynomial-time approximation schemes (with tunable approximation guarantees for the target maximum-error metric) for deterministic wavelet thresholding in multiple dimensions.

#index 801685
#* On the memory requirements of XPath evaluation over XML streams
#@ Ziv Bar-Yossef;Marcus Fontoura;Vanja Josifovski
#t 2004
#c 5
#% 238182
#% 378408
#% 465061
#% 480296
#% 576107
#% 576108
#% 598376
#% 600560
#% 643566
#% 654476
#% 654477
#% 659987
#% 659995
#% 803121
#! The important challenge of evaluating XPath queries over XML streams has sparked much interest in the past two years, A number of algorithms have been proposed, supporting wider fragments of the query language, and exhibiting better performance and memory utilization. Nevertheless, all the algorithms known to date use a prohibitively large amount of memory for certain types of queries. A natural question then is whether this memory bottleneck is inherent or just an artifact of the proposed algorithms.In this paper we initiate the first systematic and theoretical study of lower bounds on the amount of memory required to evaluate XPath queries over XML streams. We present a general lower bound technique, which given a query, specifies the minimum amount of memory that any algorithm evaluating the query on a stream would need to incur. The lower bounds are stated in terms of new graph-theoretic properties of queries. The proof is based on tools from communication complexity.We then exploit insights learned from the lower bounds to obtain a new algorithm for XPath evaluation on streams. The algorithm uses space close to the optimum. Our algorithm deviates from the standard paradigm of using automata or transducers, thereby avoiding the need to store large transition tables.

#index 801686
#* Conjunctive queries over trees
#@ Georg Gottlob;Christoph Koch;Klaus U. Schulz
#t 2004
#c 5
#% 49315
#% 340148
#% 384978
#% 464727
#% 465053
#% 465065
#% 473117
#% 480648
#% 526619
#% 576108
#% 599549
#% 600496
#% 644201
#% 733595
#% 748314
#% 749438
#% 836134
#% 993939
#% 1015271
#! We study the complexity and expressive power of conjunctive queries over unranked labeled trees, where the tree structure are represented using "axis relations" such as "child", "descendant", and "following" (we consider a superset of the XPath axes) as well as unary relations for node labels. (Cyclic) conjunctive queries over trees occur in a wide range of data management scenarios related to XML, the Web, and computational linguistics. We establish a framework for characterizing structures representing trees for which conjunctive queries can be evaluated efficiently. Then we completely chart the tractability frontier of the problem for our axis relations, i.e., we find all subset maximal sets of axes for which query evaluation is in polynomial time. All polynomial-time results are obtained immediately using the proof techniques from our framework. Finally, we study the expressiveness of conjunctive queries over trees and compare it to the expressive power of fragments of XPath. We show that for each conjunctive query, there is an equivalent acyclic positive query (i.e., a set of acyclic conjunctive queries), but that in general this query is not of polynomial size.

#index 801687
#* Synopses for query optimization: a space-complexity perspective
#@ Raghav Kaushik;Raghu Ramakrishnan;Venkatesan T. Chakaravarthy
#t 2004
#c 5
#% 554
#% 43163
#% 82346
#% 102784
#% 116084
#% 132779
#% 152585
#% 201921
#% 210190
#% 214073
#% 238182
#% 273682
#% 273902
#% 273908
#% 273909
#% 299982
#% 300193
#% 333947
#% 378413
#% 397354
#% 397371
#% 408237
#% 411554
#% 427219
#% 479648
#% 479931
#% 480306
#% 481266
#% 482123
#% 600560
#% 859116
#% 1015256
#! Database systems use precomputed synopses of data to estimate the cost of alternative plans during query optimization. A number of alternative synopsis structures have been proposed, but histograms are by far the most commonly used. While histograms have proved to be very effective in (cost estimation for) single-table selections, queries with joins have long been seen as a challenge; under a model where histograms are maintained for individual tables, a celebrated result of Ioannidis and Christodoulakis observes that errors propagate exponentially with the number of joins in a query.In this paper, we make two main contributions. First, we study the space complexity of using synopses for query optimization from a novel information-theoretic perspective. In particular, we offer evidence in support of histograms for single-table selections, and illustrate their limitations for join queries. Second, for a broad class of common queries involving joins (specifically, all queries involving only key-foreign key joins) we show that the strategy of storing a small pre-computed sample of the database yields probabilistic guarantees that are almost space-optimal, in the sense that in order to provide the same guarantee as sampling, any strategy requires almost the same amount of space. This is an important property if these samples are to be used as database statistics. This is the first such optimality result, to our knowledge, and suggests that pre-computed samples might be an effective way to circumvent the error propagation problem for queries with key-foreign key joins. We support this result empirically through an experimental study that demonstrates the effectiveness of pre-computed samples, and also shows the increasing difference in the effectiveness of samples versus multi-dimensional histograms as the number of joins in the query grows.

#index 801688
#* Weighted hypertree decompositions and optimal query plans
#@ Francesco Scarcello;Gianluigi Greco;Nicola Leone
#t 2004
#c 5
#% 998
#% 1324
#% 2028
#% 2657
#% 36683
#% 101922
#% 159244
#% 237180
#% 248038
#% 286995
#% 287031
#% 289287
#% 289424
#% 299969
#% 303886
#% 321058
#% 331899
#% 333866
#% 338450
#% 339937
#% 344155
#% 384978
#% 387508
#% 398173
#% 427161
#% 481223
#% 598376
#% 599549
#% 644201
#% 723931
#% 727988
#% 836134
#% 1015256
#! Hypertree width [22, 25] is a measure of the degree of cyclicity of hypergraphs. A number of relevant problems from different areas, e.g., the evaluation of conjunctive queries in database theory or the constraint satisfaction in AI, are tractable when their underlying hypergraphs have bounded hypertree width. However, in practical contexts like the evaluation of database queries, we have more information besides the structure of queries. For instance, we know the number of tuples in relations, the selectivity of attributes and so on. In fact, all commercial query-optimizers are based on quantitative methods and do not care about structural properties.In this paper, we define the notion of weighted hypertree decomposition, in order to combine structural decomposition methods with quantitative approaches. Weighted hypertree decompositions are equipped with cost functions, that can be used for modelling many situations where we have further information on the given problem, besides its hypergraph representation. We analyze the complexity of computing the hypertree decompositions having the smallest weights, called minimal hypertree decompositions. We show that, in many cases, adding weights we loose tractability. However, we prove that, under some - not very severe - restrictions on the allowed cost functions and on the target hypertrees, optimal weighted hypertree decompositions can be computed in polynomial time. For some easier hypertree weighting functions, this problem is also highly parallelizable. Then, we provide a cost function that models query evaluation costs and show how to exploit weighted hypertree decompositions for determining (logical) query plans for answering conjunctive queries. Finally, we present the results of an experimental comparison of this query optimization technique with the query optimization of a commercial DBMS. These preliminary results are very promising, as for some large queries (with many joins) our hybrid technique clearly outperforms the commercial optimizer.

#index 801689
#* Trees, automata and XML
#@ Thomas Schwentick
#t 2004
#c 5
#! Formal languages play an important role for many aspects of XML processing. This is obvious for type specifications (as DTD) which use context-free grammars and for navigation in documents (as in XPath) which is based on regular expressions. But the investigation of query, typing, navigation and transformation languages for XML has used many more concepts from Formal Language Theory, in particular many different kinds of string and tree automata.The close connection between automata and logics helps to allow a declarative specification of queries and transformations that can be evaluated or performed by tree automata. This connection also facilitates the investigation of the expressive power of query and transformation languages. Furthermore, in many cases automata characterizations enable static analysis like containment and satisfiability tests for queries or type checking for transformations.The tutorial will give a gentle introduction into the connections between XML languages and various kinds of automata and it will survey some classical and recent results in this area.

#index 801690
#* On the complexity of optimal K-anonymity
#@ Adam Meyerson;Ryan Williams
#t 2004
#c 5
#% 248030
#% 299970
#% 300184
#% 333876
#% 576110
#% 576111
#% 576761
#% 993943
#! The technique of k-anonymization has been proposed in the literature as an alternative way to release public information, while ensuring both data privacy and data integrity. We prove that two general versions of optimal k-anonymization of relations are NP-hard, including the suppression version which amounts to choosing a minimum number of entries to delete from the relation. We also present a polynomial time algorithm for optimal k-anonymity that achieves an approximation ratio independent of the size of the database, when k is constant. In particular, it is a O(k log k)-approximation where the constant in the big-O is no more than 4, However, the runtime of the algorithm is exponential in k. A slightly more clever algorithm removes this condition, but is a O(k log m)-approximation, where m is the degree of the relation. We believe this algorithm could potentially be quite fast in practice.

#index 801691
#* Locally consistent transformations and query answering in data exchange
#@ Marcelo Arenas;Pablo Barceló;Ronald Fagin;Leonid Libkin
#t 2004
#c 5
#% 583
#% 663
#% 129217
#% 188350
#% 198465
#% 217086
#% 230556
#% 248038
#% 257866
#% 261370
#% 287339
#% 287733
#% 289370
#% 289384
#% 307249
#% 328424
#% 342387
#% 378409
#% 384978
#% 465057
#% 480429
#% 576100
#! Data exchange is the problem of taking data structured under a source schema and creating an instance of a target schema. Given a source instance, there may be many solutions - target instances that satisfy the constraints of the data exchange problem. Previous work has identified two classes of desirable solutions: canonical universal solutions, and their cores. Query answering in data exchange amounts to rewriting a query over the target schema to another query that, over a materialized target instance, gives the result that is semantically consistent with the source. A basic question is then whether there exists a transformation sending a source instance into a solution over which target queries can be answered.We show that the answer is negative for many data exchange transformations that have structural properties similar to canonical universal solutions and cores. Namely, we prove that many such transformations preserve the local structure of the data. Using this notion, we further show that every target query rewritable over such a transformation cannot distinguish tuples whose neighborhoods in the source are similar. This gives us a first tool that helps check whether a query is rewritable, We also show that these results are robust: they hold for an extension of relational calculus with grouping and aggregates, and for two different semantics of query answering.

#index 801692
#* Logical foundations of peer-to-peer data integration
#@ Diego Calvanese;Giuseppe De Giacomo;Maurizio Lenzerini;Riccardo Rosati
#t 2004
#c 5
#% 16
#% 36683
#% 167194
#% 283052
#% 326595
#% 378409
#% 433982
#% 445447
#% 465057
#% 490489
#% 568191
#% 576091
#% 576116
#% 749088
#% 801676
#% 1015302
#% 1279214
#% 1388086
#! In peer-to-peer data integration, each peer exports data in terms of its own schema, and data interoperation is achieved by means of mappings among the peer schemas. Peers are autonomous systems and mappings are dynamically created and changed. One of the challenges in these systems is answering queries posed to one peer taking into account the mappings. Obviously, query answering strongly depends on the semantics of the overall system. In this paper, we compare the commonly adopted approach of interpreting peer-to-peer systems using a first-order semantics, with an alternative approach based on epistemic logic. We consider several central properties of peer-to-peer systems: modularity, generality, and decidability. We argue that the approach based on epistemic logic is superior with respect to all the above properties. In particular, we show that, in systems in which peers have decidable schemas and conjunctive mappings, but are arbitrarily interconnected, the first-order approach may lead to undecidability of query answering, while the epistemic approach always preserves decidability. This is a fundamental property, since the actual interconnections among peers are not under the control of any actor in the system.

#index 801693
#* Adaptive sampling for geometric problems over data streams
#@ John Hershberger;Subhash Suri
#t 2004
#c 5
#% 2115
#% 69316
#% 70370
#% 235114
#% 273907
#% 278835
#% 283750
#% 333931
#% 378388
#% 593957
#% 766228
#! Geometric coordinates are an integral part of many data streams. Examples include sensor locations in environmental monitoring, vehicle locations in traffic monitoring or battlefield simulations, scientific measurements of earth or atmospheric phenomena, etc. How can one summarize such data streams using limited storage so that many natural geometric queries can be answered faithfully? Some examples of such queries are: report the smallest convex region in which a chemical leak has been sensed, or track the diameter of the dataset. One can also pose queries over multiple streams: track the minimum distance between the convex hulls of two data streams; or report when datasets A and B are no longer linearly separable.In this paper, we propose an adaptive sampling scheme that gives provably optimal error bounds for extremal problems of this nature. All our results follow from a single technique for computing the approximate convex hull of a point stream in a single pass. Our main result is this: given a stream of two-dimensional points and an integer r, we can maintain an adaptive sample of at most 2r + 1 points such that the distance between the true convex hull and the convex hull of the sample points is O(D/r2), where D is the diameter of the sample set. With our sample convex hull, all the queries mentioned above can be answered in either O(log r) or O(r) time.

#index 801694
#* Flexible time management in data stream systems
#@ Utkarsh Srivastava;Jennifer Widom
#t 2004
#c 5
#% 149247
#% 158051
#% 172950
#% 286256
#% 300179
#% 320187
#% 428155
#% 578391
#% 578560
#% 654462
#! Continuous queries in a Data Stream Management System (DSMS) rely on time as a basis for windows on streams and for defining a consistent semantics for multiple streams and updatable relations. The system clock in a centralized DSMS provides a convenient and well-behaved notion of time, but often it is more appropriate for a DSMS application to define its own notion of time---its own clock(s), sequence numbers, or other forms of ordering and times-tamping. Flexible application-defined time poses challenges to the DSMS, since streams may be out of order and uncoordinated with each other, they may incur latency reaching the DSMS, and they may pause or stop. We formalize these challenges and specify how to generate heartbeats so that queries can be evaluated correctly and continuously in an application-defined time domain. Our heartbeat generation algorithm is based on parameters capturing skew between streams, unordering within streams, and latency in streams reaching the DSMS. We also describe how to estimate these parameters at run-time, and we discuss how heartbeats can be used for processing continuous queries.

#index 801695
#* Power-conserving computation of order-statistics over sensor networks
#@ Michael B. Greenwald;Sanjeev Khanna
#t 2004
#c 5
#% 248820
#% 297915
#% 309433
#% 333931
#% 397354
#% 397389
#% 464215
#% 622760
#% 654482
#% 660004
#% 805466
#% 1394366
#! We study the problem of power-conserving computation of order statistics in sensor networks. Significant power-reducing optimizations have been devised for computing simple aggregate queries such as COUNT, AVERAGE, or MAX over sensor networks. In contrast, aggregate queries such as MEDIAN have seen little progress over the brute force approach of forwarding all data to a central server. Moreover, battery life of current sensors seems largely determined by communication costs --- therefore we aim to minimize the number of bytes transmitted. Unoptimized aggregate queries typically impose extremely high power consumption on a subset of sensors located near the server. Metrics such as total communication cost underestimate the penalty of such imbalance: network lifetime may be dominated by the worst-case replacement time for depleted batteries.In this paper, we design the first algorithms for computing order-statistics such that power consumption is balanced across the entire network. Our first main result is a distributed algorithm to compute an ε-approximate quantile summary of the sensor data such that each sensor transmits only O(log2 n/ε) data values, irrespective of the network topology, an improvement over the current worst-case behavior of Ω(n). Second, we show an improved result when the height, h, of the network is significantly smaller than n. Our third result is that we can exactly compute any order statistic (e.g., median) in a distributed manner such that each sensor needs to transmit O(log3 n) values.Further, we design the aggregates used by our algorithms to be decomposable. An aggregate Q over a set S is decomposable if there exists a function, f, such that for all S = S1 ∪ S2, Q(S) = f(Q(S1), Q(S2)). We can thus directly apply existing optimizations to decomposable aggregates that increase error-resilience and reduce communication cost.Finally, we validate our results empirically, through simulation. When we compute the median exactly, we show that, even for moderate size networks, the worst communication cost for any single node is several times smaller than the corresponding cost in prior median algorithms. We show similar cost reductions when computing approximate order-statistic summaries with guaranteed precision. In all cases, our total communication cost over the entire network is smaller than or equal to the total cost of prior algorithms.

#index 801696
#* Approximate counts and quantiles over sliding windows
#@ Arvind Arasu;Gurmeet Singh Manku
#t 2004
#c 5
#% 1331
#% 152934
#% 248812
#% 248820
#% 273907
#% 321186
#% 333931
#% 340670
#% 378388
#% 379444
#% 379445
#% 397443
#% 480727
#% 481779
#% 492912
#% 548479
#% 569754
#% 576113
#% 576119
#% 745533
#% 993960
#! We consider the problem of maintaining ε-approximate counts and quantiles over a stream sliding window using limited space. We consider two types of sliding windows depending on whether the number of elements N in the window is fixed (fixed-size sliding window) or variable (variable-size sliding window). In a fixed-size sliding window, both the ends of the window slide synchronously over the stream. In a variable-size sliding window, an adversary slides the window ends independently, and therefore has the ability to vary the number of elements N in the window.We present various deterministic and randomized algorithms for approximate counts and quantiles. All of our algorithms require O(1/ε polylog(1/ε, N)) space. For quantiles, this space requirement is an improvement over the previous best bound of O(1/ε2 polylog(1/ε, N)). We believe that no previous work on space-efficient approximate counts over sliding windows exists.

#index 801697
#* On the decidability of containment of recursive datalog queries - preliminary report
#@ Piero A. Bonatti
#t 2004
#c 5
#% 54225
#% 101944
#% 140410
#% 248026
#% 464717
#% 465043
#% 495988
#% 496108
#% 561701
#% 599549
#% 1279260
#! The problem of deciding query containment has important applications in classical query optimization and heterogeneous database systems. Query containment is undecidable for unrestricted recursive queries, and decidable for recursive monadic queries and conjunctive queries over regular path expressions. In this paper, we identify a new class of recursive queries with decidable containment. Our framework extends the aforementioned query classes by supporting recursive predicates with more than two arguments and nonlinear recursion.

#index 801698
#* Processing first-order queries under limited access patterns
#@ Alan Nash;Bertram Ludäscher
#t 2004
#c 5
#% 91617
#% 198466
#% 273912
#% 277327
#% 289266
#% 299968
#% 342359
#% 384978
#% 481923
#% 576091
#% 599549
#% 654520
#% 726626
#% 853032
#! We study the problem of answering queries over sources with limited access patterns. Given a first-order query Q, the problem is to decide whether there is an equivalent query which can be executed observing the access patterns restrictions. If so, we say that Q is feasible. We define feasible for first-order queries---previous definitions handled only some existential cases---and characterize the complexity of many first-order query classes. For each of them, we show that deciding feasibility is as hard as deciding containment. Since feasibility is undecidable in many cases and hard to decide in some others, we also define an approximation to it which can be computed in NP for any first-order query and in P for unions of conjunctive queries with negation. Finally, we outline a practical overall strategy for processing first-order queries under limited access patterns.

#index 801699
#* On preservation under homomorphisms and unions of conjunctive queries
#@ Albert Atserias;Anuj Dawar;Phokion G. Kolaitis
#t 2004
#c 5
#% 5804
#% 26735
#% 55926
#% 64421
#% 129217
#% 154290
#% 175735
#% 190340
#% 289266
#% 321058
#% 338450
#% 344155
#% 384978
#% 427161
#% 483541
#% 535150
#% 544735
#% 576100
#% 587326
#% 587602
#% 599549
#% 723931
#% 1972413
#! Unions of conjunctive queries, also known as select-project-join-union queries, are the most frequently asked queries in relational database systems. These queries are definable by existential positive first-order formulas and are preserved under homomorphisms. A classical result of mathematical logic asserts that existential positive formulas are the only first-order formulas (up to logical equivalence) that are preserved under homomorphisms on all structures, finite and infinite. It is long-standing open problem in finite model theory, however, to determine whether the same homomorphism-preservation result holds in the finite, that is, whether every first-order formula preserved under homomorphisms on finite structures is logically equivalent to an existential positive formula on finite structures. In this paper, we show that the homomorphism-preservation theorem holds for several large classes of finite structures of interest in graph theory and database theory. Specifically, we show that this result holds for all classes of finite structures of bounded degree, all classes of finite structures of bounded treewidth, and, more generally, all classes of finite structures whose cores exclude at least one minor.

#index 801700
#* Multi-valued dependencies in the presence of lists
#@ Sven Hartmann;Sebastian Link
#t 2004
#c 5
#% 10245
#% 38696
#% 48752
#% 55408
#% 55899
#% 77929
#% 84990
#% 99437
#% 106916
#% 177516
#% 193287
#% 205246
#% 250473
#% 274437
#% 286860
#% 287295
#% 287478
#% 287631
#% 287754
#% 287792
#% 289164
#% 289237
#% 289336
#% 291299
#% 299943
#% 330627
#% 333855
#% 346917
#% 378395
#% 384978
#% 389068
#% 397347
#% 431018
#% 480946
#% 482064
#% 482088
#% 527113
#% 535024
#% 687518
#% 1388499
#! Multi-valued depdendencies (MVDs) are an important class of relational constraints. We axiomatise MVDs in data models that support nested list types. In order to capture different data models at a time, an abstract approach based on nested attributes is taken. The set of subattributes of some fixed nested attribute carries the structure of a co-Heyting algebra. This enables us to generalise significant features of MVDs from the relational data model to the presence of lists. It is shown that an MVD is satisfied by some instance exactly when this instance can be decomposed without loss of information. The full power of the algebraic framework allows to provide a sound and complete set of inference rules for the finite implication of MVDs in the context of lists. The presence of the list operator calls for a new inference rule which is not required in the relational data model. Further differences become apparant when the minimality of the inference rules is investigated. The extension of the relational theory of MVDs to the presence of lists allows to specify more real-world constraints and increases therefore the number of application domains.

#index 803597
#* In memory of Seymour Ginsburg 1928 - 2004
#@ S. Abiteboul;R. Hull;V. Vianu
#t 2005
#c 5
#! Seymour Ginsburg, a pioneer of formal language theory and database theory, passed away on December 5 after a long battle with Alzheimer's disease.

#index 803598
#* Stonebraker receives IEEE John von Neumann Medal
#@ David DeWitt;Michael Carey;Joseph M. Hellerstein
#t 2005
#c 5
#! In December 2004, Michael Stonebraker was selected to receive the 2005 IEEE John von Neumann Medal for his "contributions to the design, implementation, and commercialization of relational and object-relational database systems." Mike is the first person from the database field selected to receive this award. He joins an illustrious group of former recipients, including Barbara Liskov (2004), Alfred Aho (2003), Ole-Johan Dahl and Kristen Nygaard (2002), Butler Lampson (2001), John Hennessy and David Patterson (2000), Douglas Engelbart (1999), Ivan Sutherland (1998), Maurice Wilkes (1997), Carver Mead (1996), Donald Knuth (1995), John Cocke (1994), Fred Brooks (1993), and Gordon Bell (1992).

#index 803599
#* Candidates for the upcoming ACM SIGMOD elections
#@ Mike Franklin
#t 2005
#c 5
#! A clear sign of a healthy organization is the willingness of its members to volunteer their time to support and guide it. SIGMOD is particularly fortunate in this regard. As evidence is the slate of candidates listed on the following pages. These six people have agreed to run for positions as SIGMOD officers, and if elected, are committed to overseeing and improving the wide-ranging activities of SIGMOD and continuing the progress that has made us one of the leading SIGs in ACM.

#index 803600
#* A snapshot of public web services
#@ Jianchun Fan;Subbarao Kambhampati
#t 2005
#c 5
#% 115478
#% 577343
#% 765399
#! Web Service Technology has been developing rapidly as it provides a flexible application-to-application interaction mechanism. Several ongoing research efforts focus on various aspects of web service technology, including the modeling, specification, discovery, composition and verification of web services. The approaches advocated are often conflicting---based as they are on differing expectations on the current status of web services as well as differing models of their future evolution. One way of deciding the relative relevance of the various research directions is to look at their applicability to the currently available web services. To this end, we took a snapshot of the currently publicly available web services. Our aim is to get an idea of the number, type, complexity and composability of these web services and see if this analysis provides useful information about the near-term fruitful research directions.

#index 803601
#* Research issues in automatic database clustering
#@ Sylvain Guinepain;Le Gruenwald
#t 2005
#c 5
#% 872
#% 1748
#% 58374
#% 58381
#% 59348
#% 88054
#% 172948
#% 210173
#% 248815
#% 275367
#% 279120
#% 287257
#% 324431
#% 348611
#% 384872
#% 393812
#% 442704
#% 445775
#% 562325
#% 765431
#% 773935
#% 773942
#% 773951
#% 797999
#! While a lot of work has been published on clustering of data on storage medium, little has been done about automating this process. This is an important area because with data proliferation, human attention has become a precious and expensive resource. Our goal is to develop an automatic and dynamic database clustering technique that will dynamically re-cluster a database with little intervention of a database administrator (DBA) and maintain an acceptable query response time at all times. In this paper we describe the issues that need to be solved when developing such a technique.

#index 803602
#* No pane, no gain: efficient evaluation of sliding-window aggregates over data streams
#@ Jin Li;David Maier;Kristin Tufte;Vassilis Papadimos;Peter A. Tucker
#t 2005
#c 5
#% 378388
#% 420053
#% 654497
#% 765404
#% 993949
#% 1016157
#! Windows queries are proving essential to data-stream processing. In this paper, we present an approach for evaluating sliding-window aggregate queries that reduces both space and computation time for query execution. Our approach divides overlapping windows into disjoint panes, computes sub-aggregates over each pane, and "rolls up" the pane-aggregates to computer window-aggregates. Our experimental study shows that using panes has significant performance benefits.

#index 803603
#* A unified spatiotemporal schema for representing and querying moving features
#@ Rong Xie;Ryosuke Shibasaki
#t 2005
#c 5
#% 260065
#% 260066
#% 300174
#% 315005
#% 319244
#% 421073
#% 427199
#% 461923
#! A conceptual schema is essentially required to effectively and efficiently manage and manipulate dynamically and continuously changing data and information of moving features. In the paper, spatiotemporal schema (STS) is proposed to describe characteristics of moving features and to efficiently manage moving features data, including the necessity aspects: abstract data types, dynamic attributes, spatiotemporal topological relationships and a minimum set of spatiotemporal operations. On the basis of the proposal of schema, spatiotemporal object-based class library (STOCL) is further developed for the implementation of STS, which allows development of various spatiotemporal queries and simulations. The conceptual schema and implemented object library are then applied to the development of passengers' movement simulation and pattern analysis in railway stations in Tokyo.

#index 803604
#* RedBD: the database research community in Spain
#@ Arantza Illarramendi;Esperanza Marcos;Carmen Costilla
#t 2005
#c 5
#! During the last decade, the Database research community in Spain has grown significantly in the quantity of groups interested in the area, and especially in the quality of those groups. Those database research groups are not in general very large; usually five or six full-time researchers compose them. The economic resources for supporting the researching activity came from public organisms like the Spanish government, the European Union or local governments, and in a less rate from the industry. Research is carried out mainly at the Informatics Departments of the Universities. As the different groups usually share research topics and also fields of applications, in the last years there has been an important movement to join research efforts.

#index 803605
#* Report from the first international workshop on computer vision meets databases (CVDB 2004)
#@ Laurent Amsaleg;Björn Pór Jónsson;Vincent Oria
#t 2005
#c 5
#% 789786
#% 789787
#% 789788
#% 789789
#% 789790
#% 789791
#% 789792
#! This report summarizes the presentations and discussions of the First International Workshop on Computer Vision meets Databases, or CVDB 2004, which was held in Paris, France, on June 13, 2004. The workshop was co-located with the 2004 ACM SIGMOD/PODS conferences and was attended by forty-two participants from all over the world.

#index 803606
#* Report on the 19th Brazilian symposium on databases (SBBD 2004)
#@ Sérgio Lifschitz;Alberto H. F. Laender
#t 2005
#c 5
#! The Brazilian Symposium on Databases (SBBD) is an annual event promoted by the Brazilian Computer Society (SBC) through its Database Special Committee. In 2004, the 19th edition of SBBD was held in Brasília, Brazil's capital, on 18-20 October, organized by the Computer Science Department of the University of Brasília (UnB). As in the previous years, SBBD 2004 received the in-cooperation status from ACM SIGMOD and was partially supported by the VLDB Endowment, thus confirming the recognition of the international community of SBBD as the most important database event in Latin America.

#index 803607
#* The atomic manifesto: a story in four quarks
#@ Cliff Jones;David Lomet;Alexander Romanovsky;Gerhard Weikum;Alan Fekete;Marie-Claude Gaudel;Henry F. Korth;Rogerio de Lemos;Eliot Moss;Ravi Rajwar;Krithi Ramamritham;Brian Randell;Luis Rodrigues
#t 2005
#c 5
#% 1486
#% 55348
#% 82786
#% 148195
#% 193149
#% 202505
#% 213073
#% 235084
#% 336201
#% 383646
#% 391421
#% 403195
#% 429187
#% 656868
#% 725884
#% 726549
#% 780020
#% 1112242
#! This paper is based on a five-day workshop on "Atomicity in System Design and Execution" that took place in Schloss Dagstuhl in Germany [5] in April 2004 and was attended by 32 people from different scientific communities. The participants included researchers from the four areas of• database and transaction processing systems,• fault tolerance and dependable systems,• formal methods for system design and correctness reasoning, and• to a smaller extent, hardware architecture and programming languages.

#index 803608
#* Report on MobiDE 2003: the 3rd international ACM Workshop on Data Engineering for Wireless and Mobile Access
#@ Sujata Banerjee;Mitch Cherniack;Panos K. Chrysanthis;Vijay Kumar;Alexandros Labrinidis
#t 2005
#c 5
#% 720025
#% 720026
#% 720027
#% 720028
#% 720029
#% 720030
#% 720031
#% 720032
#% 720033
#% 720034
#% 720035
#% 720036
#% 720037
#% 720038
#! The 3rd International ACM Workshop on Data Engineering for Wireless and Mobile Access (MobiDE 2003 for short) took place on September 19, 2003 at the Westin Horton Plaza Hotel in San Diego, California in conjunction with MobiCom 2003. The MobiDE workshops serve as a bridge between the data management and network research communities, and have a tradition of presenting innovations on mobile as well as wireless data engineering issues (such as those found in sensor networks). This workshop was the third in the MobiDE series, MobiDE 1999 having taken place in Seattle in conjunction with MobiCom 1999, and MobiDE 2001 having taken place in Santa Barbara in conjunction with SIGMOD 2001.

#index 803609
#* Containment of aggregate queries
#@ Sara Cohen
#t 2005
#c 5
#% 36683
#% 123118
#% 137867
#% 172874
#% 188853
#% 190638
#% 198473
#% 223781
#% 248034
#% 273691
#% 273696
#% 279164
#% 342387
#% 397354
#% 464056
#% 465190
#% 477212
#% 481604
#% 482081
#% 599549
#% 622760
#% 630967
#% 769869
#% 801769
#! It is now common for databases to contain many gigabytes, or even many terabytes, of data. Scientific experiments in areas such as high energy physics produce data sets of enormous size, while in the business sector the emergence of decision-support systems and data warehouses has led organizations to build up gigantic collections of data. Aggregate queries allow one to retrieve concise information from such a database, since they can cover many data items while returning a small result. OLAP queries, used extensively in data warehousing, are based almost entirely on aggregation [4, 16]. Aggregate queries have also been studied in a variety of settings beyond relational databases, such as mobile computing [1], global information systems [21], stream data analysis [12], sensor networks [22] and constraint databases [2].

#index 803610
#* Databases in Virtual Organizations: a collective interview and call for researchers
#@ Marianne Winslett
#t 2005
#c 5
#! When the Databases in Virtual Organizations (DIVO) workshop convened after SIGMOD 2004 in Paris, many of us attending weren't sure what a virtual organization was, much less what relevance it could have to database research. Five hours later, as the lights snapped off in the rest of the building and the maintenance crew hovered patiently outside our meeting room, we had become a group with a mission: to let the database research community know what an incredible idea generator and testbed virtual organizations could be for research on information integration and data security.

#index 803611
#* Developments at ACM TODS
#@ Richard Snodgrass
#t 2005
#c 5
#! The March 2005 issue of TODS has eight papers invited from the SIGMOD and PODS'2003 conferences. These papers are significantly extended versions of the conference papers, allowing the authors to refine and elaborate without the strictures of a twelve-page limit.

#index 806211
#* Exchanging intensional XML data
#@ Tova Milo;Serge Abiteboul;Bernd Amann;Omar Benjelloun;Fred Dang Ngoc
#t 2005
#c 5
#% 101947
#% 201928
#% 229827
#% 248799
#% 264263
#% 299942
#% 333844
#% 333990
#% 390964
#% 393907
#% 395735
#% 461901
#% 464706
#% 482111
#% 504575
#% 654485
#% 994034
#% 1015358
#! XML is becoming the universal format for data exchange between applications. Recently, the emergence of Web services as standard means of publishing and accessing data on the Web introduced a new class of XML documents, which we call intensional documents. These are XML documents where some of the data is given explicitly while other parts are defined only intensionally by means of embedded calls to Web services.When such documents are exchanged between applications, one has the choice of whether or not to materialize the intensional data (i.e., to invoke the embedded calls) before the document is sent. This choice may be influenced by various parameters, such as performance and security considerations. This article addresses the problem of guiding this materialization process.We argue that---like for regular XML data---schemas (à la DTD and XML Schema) can be used to control the exchange of intensional data and, in particular, to determine which data should be materialized before sending a document, and which should not. We formalize the problem and provide algorithms to solve it. We also present an implementation that complies with real-life standards for XML data, schemas, and Web services, and is used in the Active XML system. We illustrate the usefulness of this approach through a real-life application for peer-to-peer news exchange.

#index 806212
#* Progressive skyline computation in database systems
#@ Dimitris Papadias;Yufei Tao;Greg Fu;Bernhard Seeger
#t 2005
#c 5
#% 2115
#% 43163
#% 62323
#% 86950
#% 100803
#% 201876
#% 273887
#% 287070
#% 287466
#% 288976
#% 300180
#% 333854
#% 333951
#% 427199
#% 438135
#% 443327
#% 465167
#% 480093
#% 480133
#% 480671
#% 480819
#% 481956
#% 527189
#% 527328
#% 654480
#% 993954
#! The skyline of a d-dimensional dataset contains the points that are not dominated by any other point on all dimensions. Skyline computation has recently received considerable attention in the database community, especially for progressive methods that can quickly return the initial results without reading the entire database. All the existing algorithms, however, have some serious shortcomings which limit their applicability in practice. In this article we develop branch-and-bound skyline (BBS), an algorithm based on nearest-neighbor search, which is I/O optimal, that is, it performs a single access only to those nodes that may contain skyline points. BBS is simple to implement and supports all types of progressive processing (e.g., user preferences, arbitrary dimensionality, etc). Furthermore, we propose several interesting variations of skyline computation, and show how BBS can be applied for their efficient processing.

#index 806213
#* Advanced SQL modeling in RDBMS
#@ Andrew Witkowski;Srikanth Bellamkonda;Tolga Bozkaya;Nathan Folkert;Abhinav Gupta;John Haydu;Lei Sheng;Sankar Subramanian
#t 2005
#c 5
#% 13016
#% 86943
#% 86950
#% 123113
#% 152928
#% 284140
#% 353359
#% 354739
#% 388706
#% 397388
#% 427199
#% 464215
#% 479792
#% 480470
#% 481293
#% 654445
#% 654446
#% 1247511
#! Commercial relational database systems lack support for complex business modeling. ANSI SQL cannot treat relations as multidimensional arrays and define multiple, interrelated formulas over them, operations which are needed for business modeling. Relational OLAP (ROLAP) applications have to perform such tasks using joins, SQL Window Functions, complex CASE expressions, and the GROUP BY operator simulating the pivot operation. The designated place in SQL for calculations is the SELECT clause, which is extremely limiting and forces the user to generate queries with nested views, subqueries and complex joins. Furthermore, SQL query optimizers are preoccupied with determining efficient join orders and choosing optimal access methods and largely disregard optimization of multiple, interrelated formulas. Research into execution methods has thus far concentrated on efficient computation of data cubes and cube compression rather than on access structures for random, interrow calculations. This has created a gap that has been filled by spreadsheets and specialized MOLAP engines, which are good at specification of formulas for modeling but lack the formalism of the relational model, are difficult to coordinate across large user groups, exhibit scalability problems, and require replication of data between the tool and RDBMS. This article presents an SQL extension called SQL Spreadsheet, to provide array calculations over relations for complex modeling. We present optimizations, access structures, and execution models for processing them efficiently. Special attention is paid to compile time optimization for expensive operations like aggregation. Furthermore, ANSI SQL does not provide a good separation between data and computation and hence cannot support parameterization for SQL Spreadsheets models. We propose two parameterization methods for SQL. One parameterizes ANSI SQL view using subqueries and scalars, which allows passing data to SQL Spreadsheet. Another method presents parameterization of the SQL Spreadsheet formulas. This supports building stand-alone SQL Spreadsheet libraries. These models are then subject to the SQL Spreadsheet optimizations during model invocation time.

#index 806214
#* TinyDB: an acquisitional query processing system for sensor networks
#@ Samuel R. Madden;Michael J. Franklin;Joseph M. Hellerstein;Wei Hong
#t 2005
#c 5
#% 554
#% 111368
#% 116043
#% 152954
#% 201883
#% 248795
#% 249985
#% 273911
#% 274143
#% 297915
#% 300167
#% 300179
#% 309430
#% 309433
#% 330305
#% 330413
#% 333926
#% 333953
#% 336865
#% 339223
#% 397353
#% 397355
#% 401228
#% 427022
#% 442700
#% 443019
#% 443298
#% 479938
#% 480113
#% 480602
#% 480810
#% 480965
#% 481448
#% 571048
#% 572308
#% 581040
#% 636008
#% 660004
#% 731094
#% 731097
#% 761322
#% 805466
#% 858997
#% 993949
#% 1394366
#! We discuss the design of an acquisitional query processor for data collection in sensor networks. Acquisitional issues are those that pertain to where, when, and how often data is physically acquired (sampled) and delivered to query processing operators. By focusing on the locations and costs of acquiring data, we are able to significantly reduce power consumption over traditional passive systems that assume the a priori existence of data. We discuss simple extensions to SQL for controlling data acquisition, and show how acquisitional issues influence query optimization, dissemination, and execution. We evaluate these issues in the context of TinyDB, a distributed query processor for smart sensor devices, and show how acquisitional techniques can provide significant reductions in power consumption on our sensor devices.

#index 806215
#* Data exchange: getting to the core
#@ Ronald Fagin;Phokion G. Kolaitis;Lucian Popa
#t 2005
#c 5
#% 391
#% 583
#% 101956
#% 129217
#% 136362
#% 248038
#% 264858
#% 283052
#% 287339
#% 287733
#% 289384
#% 378409
#% 384978
#% 465053
#% 465057
#% 480134
#% 564416
#% 572311
#% 598389
#% 599549
#% 809247
#% 993981
#! Data exchange is the problem of taking data structured under a source schema and creating an instance of a target schema that reflects the source data as accurately as possible. Given a source instance, there may be many solutions to the data exchange problem, that is, many target instances that satisfy the constraints of the data exchange problem. In an earlier article, we identified a special class of solutions that we call universal. A universal solution has homomorphisms into every possible solution, and hence is a “most general possible” solution. Nonetheless, given a source instance, there may be many universal solutions. This naturally raises the question of whether there is a “best” universal solution, and hence a best solution for data exchange. We answer this question by considering the well-known notion of the core of a structure, a notion that was first studied in graph theory, and has also played a role in conjunctive-query processing. The core of a structure is the smallest substructure that is also a homomorphic image of the structure. All universal solutions have the same core (up to isomorphism); we show that this core is also a universal solution, and hence the smallest universal solution. The uniqueness of the core of a universal solution together with its minimality make the core an ideal solution for data exchange. We investigate the computational complexity of producing the core. Well-known results by Chandra and Merlin imply that, unless P &equals; NP, there is no polynomial-time algorithm that, given a structure as input, returns the core of that structure as output. In contrast, in the context of data exchange, we identify natural and fairly broad conditions under which there are polynomial-time algorithms for computing the core of a universal solution. We also analyze the computational complexity of the following decision problem that underlies the computation of cores: given two graphs G and H, is H the core of G? Earlier results imply that this problem is both NP-hard and coNP-hard. Here, we pinpoint its exact complexity by establishing that it is a DP-complete problem. Finally, we show that the core is the best among all universal solutions for answering existential queries, and we propose an alternative semantics for answering queries in data exchange settings.

#index 806216
#* Concise descriptions of subsets of structured sets
#@ Ken Q. Pu;Alberto O. Mendelzon
#t 2005
#c 5
#% 248792
#% 248807
#% 271239
#% 318386
#% 378403
#% 408396
#% 459028
#% 461921
#% 464853
#% 482093
#% 519534
#% 562299
#% 570891
#! We study the problem of economical representation of subsets of structured sets, which are sets equipped with a set cover or a family of preorders. Given a structured set U, and a language L whose expressions define subsets of U, the problem of minimum description length in L (L-MDL) is: “given a subset V of U, find a shortest string in L that defines V.” Depending on the structure and the language, the MDL-problem is in general intractable. We study the complexity of the MDL-problem for various structures and show that certain specializations are tractable. The families of focus are hierarchy, linear order, and their multidimensional extensions; these are found in the context of statistical and OLAP databases. In the case of general OLAP databases, data organization is a mixture of multidimensionality, hierarchy, and ordering, which can also be viewed naturally as a cover-structured ordered set. Efficient algorithms are provided for the MDL-problem for hierarchical and linearly ordered structures, and we prove that the multidimensional extensions are NP-complete. Finally, we illustrate the application of the theory to summarization of large result sets and (multi) query optimization for ROLAP queries.

#index 806217
#* What's hot and what's not: tracking most frequent items dynamically
#@ Graham Cormode;S. Muthukrishnan
#t 2005
#c 5
#% 152585
#% 190611
#% 201921
#% 238182
#% 248812
#% 273682
#% 278835
#% 293714
#% 303036
#% 347226
#% 397426
#% 407822
#% 446438
#% 453512
#% 479795
#% 482123
#% 492912
#% 548479
#% 569754
#% 576119
#% 632090
#% 654443
#% 816392
#% 993960
#% 993969
#! Most database management systems maintain statistics on the underlying relation. One of the important statistics is that of the “hot items” in the relation: those that appear many times (most frequently, or more than some threshold). For example, end-biased histograms keep the hot items as part of the histogram and are used in selectivity estimation. Hot items are used as simple outliers in data mining, and in anomaly detection in many applications.We present new methods for dynamically determining the hot items at any time in a relation which is undergoing deletion operations as well as inserts. Our methods maintain small space data structures that monitor the transactions on the relation, and, when required, quickly output all hot items without rescanning the relation in the database. With user-specified probability, all hot items are correctly reported. Our methods rely on ideas from “group testing.” They are simple to implement, and have provable quality, space, and time guarantees. Previously known algorithms for this problem that make similar quality and performance guarantees cannot handle deletions, and those that handle deletions cannot make similar guarantees without rescanning the database. Our experiments with real and synthetic data show that our algorithms are accurate in dynamically tracking the hot items independent of the rate of insertions and deletions.

#index 806218
#* XML stream processing using tree-edit distance embeddings
#@ Minos Garofalakis;Amit Kumar
#t 2005
#c 5
#% 66654
#% 121278
#% 190611
#% 214073
#% 244328
#% 273682
#% 273902
#% 273909
#% 333931
#% 347226
#% 378388
#% 378408
#% 379449
#% 397354
#% 397373
#% 397379
#% 397385
#% 458847
#% 479984
#% 480156
#% 480296
#% 480306
#% 480628
#% 480654
#% 480810
#% 482108
#% 492912
#% 519953
#% 544195
#% 565342
#% 576105
#% 593957
#% 594029
#% 644182
#% 654476
#% 656804
#% 659979
#% 659995
#% 731408
#% 765423
#% 768815
#% 993959
#% 993960
#% 993969
#% 994015
#% 1015276
#! We propose the first known solution to the problem of correlating, in small space, continuous streams of XML data through approximate (structure and content) matching, as defined by a general tree-edit distance metric. The key element of our solution is a novel algorithm for obliviously embedding tree-edit distance metrics into an L1 vector space while guaranteeing a (worst-case) upper bound of O(log2n log&ast;n) on the distance distortion between any data trees with at most n nodes. We demonstrate how our embedding algorithm can be applied in conjunction with known random sketching techniques to (1) build a compact synopsis of a massive, streaming XML data tree that can be used as a concise surrogate for the full tree in approximate tree-edit distance computations; and (2) approximate the result of tree-edit-distance similarity joins over continuous XML document streams. Experimental results from an empirical study with both synthetic and real-life XML data trees validate our approach, demonstrating that the average-case behavior of our embedding techniques is much better than what would be predicted from our theoretical worst-case distortion bounds. To the best of our knowledge, these are the first algorithmic results on low-distortion embeddings for tree-edit distance metrics, and on correlating (e.g., through similarity joins) XML data in the streaming model.

#index 809233
#* Proceedings of the twenty-fourth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems
#@ Georg Gottlob;Foto Afrati
#t 2005
#c 5
#! This volume contains the proceedings of the Twenty-fourth ACMSIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems(PODS 2005), held in Baltimore, USA, on June 13-15, 2005 inconjunction with the 2005 ACM SIGMOD International Conference onManagement of Data. It consists of a paper based on the invitedtalk by Phokion Kolaitis, a paper based on the invited tutorial byMonica Lam, and 32 contributed papers that were selected by theprogram committee for presentation at the symposium.The program committee has selected the contributed papers from157 submissions. Some of the papers are "extended abstracts" andare preliminary reports on work in progress. While the programcommittee has read them carefully and discussed them intensively,they have not been formally refereed. It is expected that much ofthe research described in these papers will be published in detailin computer science journals, where they will go through a formalreview.The program committee selected two papers for the PODS 2005 BestPaper Award: XML Data Exchange: Consistency and QueryAnswering by Marcelo Arenas and Leonid Libkin, andOn the complexity of division and set joins in therelational algebra by Dirk Leinders and Jan Van denBussche, and also selected Security Analysis ofCryptographically Controlled Access to XML Documents byMartin Abadi and Bogdan Warinschi for the PODS 2005 Best NewcomerAward. Warmest congratulations to the authors.This volume is dedicated to the memory of Seymour Ginsburg, oneof the great pioneers of theoretical computer science and databasetheory who has influenced the career of many distinguishedresearchers and educators.

#index 809234
#* Context-sensitive program analysis as database queries
#@ Monica S. Lam;John Whaley;V. Benjamin Livshits;Michael C. Martin;Dzintars Avots;Michael Carbin;Christopher Unkel
#t 2005
#c 5
#% 3873
#% 11797
#% 29253
#% 64421
#% 73005
#% 139017
#% 152929
#% 160429
#% 186829
#% 205234
#% 213989
#% 287596
#% 311540
#% 312242
#% 336771
#% 348839
#% 368248
#% 402638
#% 447606
#% 463885
#% 464589
#% 480450
#% 480531
#% 500949
#% 524218
#% 526419
#% 526424
#% 535427
#% 576800
#% 581048
#% 668334
#% 727293
#% 761220
#% 761221
#% 761222
#% 788974
#% 796176
#% 807093
#% 963748
#% 963789
#% 1271979
#% 1389620
#% 1394334
#! Program analysis has been increasingly used in softwareengineering tasks such as auditing programs for securityvulnerabilities and finding errors in general. Such tools oftenrequire analyses much more sophisticated than those traditionallyused in compiler optimizations. In particular, context-sensitivepointer alias information is a prerequisite for any sound andprecise analysis that reasons about uses of heap objects in aprogram. Context-sensitive analysis is challenging because thereare over 1014 contexts in a typical large program, evenafter recursive cycles are collapsed. Moreover, pointers cannot beresolved in general without analyzing the entire program.This paper presents a new framework, based on the concept ofdeductive databases, for context-sensitive program analysis. Inthis framework, all program information is stored as relations;data access and analyses are written as Datalog queries. To handlethe large number of contexts in a program, the database representsrelations with binary decision diagrams (BDDs). The system we havedeveloped, called bddbddb, automatically translates databasequeries into highly optimized BDD programs.Our preliminary experiences suggest that a large class ofanalyses involving heap objects can be described succinctly inDatalog and implemented efficiently with BDDs. To make developingapplication-specific analyses easy for programmers, we have alsocreated a language called PQL that makes a subset of Datalogqueries more intuitive to define. We have used the language to findmany security holes in Web applications.

#index 809235
#* XML data exchange: consistency and query answering
#@ Marcelo Arenas;Leonid Libkin
#t 2005
#c 5
#% 663
#% 70235
#% 94459
#% 248038
#% 287733
#% 289328
#% 332166
#% 333841
#% 333858
#% 378409
#% 465051
#% 465057
#% 465065
#% 545382
#% 564264
#% 570877
#% 576100
#% 745473
#% 765432
#% 801676
#% 801686
#% 801691
#% 993981
#% 1016139
#! Data exchange is the problem of finding an instance of a target schema, given an instance of a source schema and a specification of the relationship between the source and the target. Theoretical foundations of data exchange have recently been investigated for relational data.In this paper, we start looking into the basic properties of XML data exchange, that is, restructuring of XML documents that conform to a source DTD under a target DTD, and answering queries written over the target schema. We define XML data exchange settings in which source-to-target dependencies refer to the hierarchical structure of the data. Combining DTDs and dependencies makes some XML data exchange settings inconsistent. We investigate the consistency problem and determine its exact complexity.We then move to query answering, and prove a dichotomy theorem that classifies data exchange settings into those over which query answering is tractable, and those over which it is coNP-complete, depending on classes of regular expressions used in DTDs. Furthermore, for all tractable cases we give polynomial-time algorithms that compute target XML documents over which queries can be answered.

#index 809236
#* XPath satisfiability in the presence of DTDs
#@ Michael Benedikt;Wenfei Fan;Floris Geerts
#t 2005
#c 5
#% 246333
#% 299942
#% 299976
#% 333856
#% 333989
#% 404772
#% 465051
#% 465065
#% 487257
#% 562456
#% 564264
#% 733593
#% 765407
#% 765450
#% 801670
#% 993939
#% 1016139
#! We study the satisfiability problem associated with XPath in the presence of DTDs. This is the problem of determining, given a query p in an XPath fragment and a DTD D, whether or not there exists an XML document T such that T conforms to D and the answer of p on T is nonempty. We consider a variety of XPath fragments widely used in practice, and investigate the impact of different XPath operators on satisfiability analysis. We first study the problem for negation-free XPath fragments with and without upward axes, recursion and data-value joins, identifying which factors lead to tractability and which to NP-completeness. We then turn to fragments with negation but without data values, establishing lower and upper bounds in the absence and in the presence of upward modalities and recursion. We show that with negation the complexity ranges from PSPACE to EXPTIME. Moreover, when both data values and negation are in place, we find that the complexity ranges from NEXPTIME to undecidable. Finally, we give a finer analysis of the problem for particular classes of DTDs, exploring the impact of various DTD constructs, identifying tractable cases, as well as providing the complexity in the query size alone.

#index 809237
#* Deciding well-definedness of XQuery fragments
#@ Stijn Vansummeren
#t 2005
#c 5
#% 11802
#% 147801
#% 189868
#% 190638
#% 384978
#% 571038
#% 571040
#% 575379
#% 643569
#% 700905
#% 1016139
#% 1016140
#% 1700124
#! Unlike in traditional query languages, expressions in XQuery can have an undefined meaning (i.e., these expressions produce a run-time error). It is hence natural to ask whether we can solve the well-definedness problem for XQuery: given an expression and an input type, check whether the semantics of the expression is defined for all inputs adhering to the input type. In this paper we investigate the well-definedness problem for non-recursive fragments of XQuery under a bounded-depth type system. We identify properties of base operations which can make the problem undecidable and give conditions which are sufficient to ensure decidability.

#index 809238
#* Views and queries: determinacy and rewriting
#@ Luc Segoufin;Victor Vianu
#t 2005
#c 5
#% 198465
#% 198466
#% 237190
#% 248038
#% 273698
#% 273700
#% 299945
#% 299967
#% 378407
#% 378410
#% 384978
#% 464717
#% 464727
#% 480158
#% 481916
#% 482116
#% 544559
#% 564416
#% 564419
#% 599549
#% 693170
#! We investigate the question of whether a query Q can be answered using a set V of views. We first define the problem in information-theoretic terms: we say that V determines Q if V provides enough information to uniquely determine the answer to Q. Next, we look at the problem of rewriting Q in terms of V using a specific language. Given a view language V and query language Q, we say that a rewriting language R is complete for Vto-Q rewritings if every Q ε Q can be rewritten in terms of V ε v using a query in R, whenever V determines Q. While query rewriting using views has been extensively investigated for some specific languages, the connection to the information-theoretic notion of determinacy, and the question of completeness of a rewriting language, have received little attention. In this paper we investigate systematically the notion of determinacy and its connection to rewriting. The results concern decidability of determinacy for various view and query languages, as well as the power required of complete rewriting languages. We consider languages ranging from first-order to conjunctive queries.

#index 809239
#* Schema mappings, data exchange, and metadata management
#@ Phokion G. Kolaitis
#t 2005
#c 5
#% 583
#% 101949
#% 101956
#% 129217
#% 150197
#% 248038
#% 262718
#% 264858
#% 287339
#% 287733
#% 289384
#% 378409
#% 465057
#% 480134
#% 509558
#% 576100
#% 598376
#% 598389
#% 599549
#% 765540
#% 801676
#% 809235
#% 809247
#% 809248
#% 809249
#% 810078
#% 823106
#% 993981
#% 1015302
#! Schema mappings are high-level specifications that describe the relationship between database schemas. Schema mappings are prominent in several different areas of database management, including database design, information integration, data exchange, metadata management, and peer-to-peer data management systems. Our main aim in this paper is to present an overview of recent advances in data exchange and metadata management, where the schema mappings are between relational schemas. In addition, we highlight some research issues and directions for future work.

#index 809240
#* On the complexity of division and set joins in the relational algebra
#@ Dirk Leinders;Jan Van den Bussche
#t 2005
#c 5
#% 189638
#% 210183
#% 289282
#% 289424
#% 343623
#% 345776
#% 384978
#% 407822
#% 427161
#% 462499
#% 480463
#% 482121
#% 576031
#% 654454
#% 765463
#% 771325
#! We show that any expression of the relational division operator in the relational algebra with union, difference, projection, selection, and equijoins, must produce intermediate results of quadratic size. To prove this result, we show a dichotomy theorem about intermediate sizes of relational algebra expressions (they are either all linear, or at least one is quadratic); we link linear relational algebra expressions to expressions using only semijoins instead of joins; and we link these semijoin algebra expressions to the guarded fragment of first-order logic.

#index 809241
#* On the complexity of nonrecursive XQuery and functional query languages on complex values
#@ Christoph Koch
#t 2005
#c 5
#% 58355
#% 84275
#% 100616
#% 101922
#% 137866
#% 145169
#% 154314
#% 164377
#% 183411
#% 189868
#% 210349
#% 212253
#% 245655
#% 248037
#% 277323
#% 289287
#% 299977
#% 342829
#% 384978
#% 415948
#% 416030
#% 435157
#% 464540
#% 464691
#% 472987
#% 576107
#% 576108
#% 598376
#% 993939
#% 993950
#% 1015272
#% 1015338
#% 1016148
#! This paper studies the complexity of evaluating functional query languages for complex values such as monad algebra and the recursion-free fragment of XQuery.We show that monad algebra with equality restricted to atomic values is complete for the class TA[2o(n), O(n)] of problems solvable in linear exponential time with a linear number of alternations. The monotone fragment of monad algebra with atomic value equality but without negation is complete for nondeterministic exponential time. For monad algebra with deep equality, we establish TA[2o(n), O(n)] lower and exponential-space upper bounds.Then we study a fragment of XQuery, Core XQuery, that seems to incorporate all the features of a query language on complex values that are traditionally deemed essential. A close connection between monad algebra on lists and Core XQuery (with "child" as the only axis) is exhibited, and it is shown that these languages are expressively equivalent up to representation issues. We show that Core XQuery is just as hard as monad algebra w.r.t. combined complexity, and that it is in TC0 if the query is assumed fixed.

#index 809242
#* An incremental algorithm for computing ranked full disjunctions
#@ Sara Cohen;Yehoshua Sagiv
#t 2005
#c 5
#% 39702
#% 172933
#% 191611
#% 213983
#% 289321
#% 333854
#% 480819
#% 576099
#% 763882
#% 765418
#% 1015317
#% 1700126
#! The full disjunction is a variation of the join operator that maximally combines tuples from connected relations, while preserving all information in the relations. The full disjunction can be seen as a natural extension of the binary outerjoin operator to an arbitrary number of relations and is a useful operator for information integration. This paper presents the algorithm INCREMENTALFD for computing the full disjunction of a set of relations. INCREMENTALFD improves upon previous algorithms for computing the full disjunction in three ways. First, it has a lower total run-time when computing the full result and a lower runtime when computing only k tuples of the result, for any constant k. Second, for a natural class of ranking functions, INCREMENTALFD returns tuples in ranking order. Third, INCREMENTALFD can be adapted to have a block-based execution, instead of a tuple-based execution.

#index 809243
#* Security analysis of cryptographically controlled access to XML documents
#@ Martín Abadi;Bogdan Warinschi
#t 2005
#c 5
#% 164560
#% 314755
#% 319295
#% 319849
#% 344639
#% 374001
#% 414366
#% 415036
#% 433922
#% 725304
#% 828021
#% 848534
#% 991986
#% 1015329
#% 1016137
#% 1707122
#! Some promising recent schemes for XML access control employ encryption for implementing security policies on published data, avoiding data duplication. In this paper we study one such scheme, due to Miklau and Suciu. That scheme was introduced with some intuitive explanations and goals, but without precise definitions and guarantees for the use of cryptography (specifically, symmetric encryption and secret sharing). We bridge this gap in the present work. We analyze the scheme in the context of the rigorous models of modern cryptography. We obtain formal results in simple, symbolic terms close to the vocabulary of Miklau and Suciu. We also obtain more detailed computational results that establish security against probabilistic polynomial-time adversaries. Our approach, which relates these two layers of the analysis, continues a recent thrust in security research and may be applicable to a broad class of systems that rely on cryptographic data protection.

#index 809244
#* Simulatable auditing
#@ Krishnaram Kenthapadi;Nina Mishra;Kobbi Nissim
#t 2005
#c 5
#% 3421
#% 67453
#% 90740
#% 230158
#% 235439
#% 287297
#% 287711
#% 287795
#% 289165
#% 347210
#% 488743
#% 576110
#% 576111
#% 598402
#% 605785
#% 630970
#% 723920
#! Given a data set consisting of private information about individuals, we consider the online query auditing problem: given a sequence of queries that have already been posed about the data, their corresponding answers -- where each answer is either the true answer or "denied" (in the event that revealing the answer compromises privacy) -- and given a new query, deny the answer if privacy may be breached or give the true answer otherwise. A related problem is the offline auditing problem where one is given a sequence of queries and all of their true answers and the goal is to determine if a privacy breach has already occurred.We uncover the fundamental issue that solutions to the offline auditing problem cannot be directly used to solve the online auditing problem since query denials may leak information. Consequently, we introduce a new model called simulatable auditing where query denials provably do not leak information. We demonstrate that max queries may be audited in this simulatable paradigm under the classical definition of privacy where a breach occurs if a sensitive value is fully compromised. We also introduce a probabilistic notion of (partial) compromise. Our privacy definition requires that the a-priori probability that a sensitive value lies within some small interval is not that different from the posterior probability (given the query answers). We demonstrate that sum queries can be audited in a simulatable fashion under this privacy definition.

#index 809245
#* Practical privacy: the SuLQ framework
#@ Avrim Blum;Cynthia Dwork;Frank McSherry;Kobbi Nissim
#t 2005
#c 5
#% 67453
#% 264164
#% 287794
#% 300184
#% 333876
#% 338434
#% 449588
#% 482040
#% 576110
#% 576111
#% 739899
#% 1707132
#! We consider a statistical database in which a trusted administrator introduces noise to the query responses with the goal of maintaining privacy of individual database entries. In such a database, a query consists of a pair (S, f) where S is a set of rows in the database and f is a function mapping database rows to {0, 1}. The true answer is ΣiεS f(di), and a noisy version is released as the response to the query. Results of Dinur, Dwork, and Nissim show that a strong form of privacy can be maintained using a surprisingly small amount of noise -- much less than the sampling error -- provided the total number of queries is sublinear in the number of database rows. We call this query and (slightly) noisy reply the SuLQ (Sub-Linear Queries) primitive. The assumption of sublinearity becomes reasonable as databases grow increasingly large.We extend this work in two ways. First, we modify the privacy analysis to real-valued functions f and arbitrary row types, as a consequence greatly improving the bounds on noise required for privacy. Second, we examine the computational power of the SuLQ primitive. We show that it is very powerful indeed, in that slightly noisy versions of the following computations can be carried out with very few invocations of the primitive: principal component analysis, k means clustering, the Perceptron Algorithm, the ID3 algorithm, and (apparently!) all algorithms that operate in the in the statistical query learning model [11].

#index 809246
#* Privacy-enhancing k-anonymization of customer data
#@ Sheng Zhong;Zhiqiang Yang;Rebecca N. Wright
#t 2005
#c 5
#% 149
#% 15374
#% 67453
#% 248030
#% 287795
#% 299970
#% 300184
#% 319849
#% 333876
#% 340475
#% 443463
#% 482049
#% 513373
#% 557515
#% 576110
#% 576111
#% 576761
#% 576762
#% 577233
#% 577289
#% 727904
#% 729930
#% 743280
#% 800515
#% 801690
#% 1395173
#! In order to protect individuals' privacy, the technique of k-anonymization has been proposed to de-associate sensitive attributes from the corresponding identifiers. In this paper, we provide privacy-enhancing methods for creating k-anonymous tables in a distributed scenario. Specifically, we consider a setting in which there is a set of customers, each of whom has a row of a table, and a miner, who wants to mine the entire table. Our objective is to design protocols that allow the miner to obtain a k-anonymous table representing the customer data, in such a way that does not reveal any extra information that can be used to link sensitive attributes to corresponding identifiers, and without requiring a central authority who has access to all the original data. We give two different formulations of this problem, with provably private solutions. Our solutions enhance the privacy of k-anonymization in the distributed scenario by maintaining end-to-end privacy from the original customer data to the final k-anonymous results.

#index 809247
#* Computing cores for data exchange: new algorithms and practical solutions
#@ Georg Gottlob
#t 2005
#c 5
#% 583
#% 36683
#% 103700
#% 109863
#% 125557
#% 193146
#% 198465
#% 287336
#% 287339
#% 292675
#% 303886
#% 321058
#% 331899
#% 332931
#% 339937
#% 384978
#% 407822
#% 427161
#% 449224
#% 464203
#% 465057
#% 480134
#% 576100
#% 599549
#% 643572
#% 741321
#% 801677
#% 801688
#% 801691
#% 809239
#% 993981
#% 1972413
#! Data Exchange is the problem of inserting data structured under a source schema into a target schema of different structure (possibly with integrity constraints), while reflecting the source data as accurately as possible. We study computational issues related to data exchange in the setting of Fagin, Kolaitis, and Popa(PODS'03). We use the technique of hypertree decompositions to derive improved algorithms for computing the core of a relational instance with labeled nulls, a problem we show to be fixed-parameter intractable with respect to the block size of the input instances. We show that computing the core of a data exchange problem is tractable for two large and useful classes of target constraints. The first class includes functional dependencies and weakly acyclic inclusion dependencies. The second class consists of full tuple generating dependencies and arbitrary equation generating dependencies. Finally, we show that computing cores is NP-hard in presence of a system-predicate NULL(x), which is true iff x is a null value.

#index 809248
#* Peer data exchange
#@ Ariel Fuxman;Phokion G. Kolaitis;Renée J. Miller;Wang-Chiew Tan
#t 2005
#c 5
#% 583
#% 248038
#% 264858
#% 273687
#% 366807
#% 378409
#% 465057
#% 576100
#% 745436
#% 765446
#% 795553
#% 801692
#% 826032
#% 993981
#% 1712584
#! In this paper, we introduce and study a framework, calledpeer data exchange, for sharing and exchanging data betweenpeers. This framework is a special case of a full-fledged peer datamanagement system and a generalization of data exchange between asource schema and a target schema. The motivation behind peer dataexchange is to model authority relationships between peers, where asource peer may contribute data to a target peer, specified usingsource-to-target constraints, and a target peer may usetarget-to-source constraints to restrict the data it is willing toreceive, but cannot modify the data of the source peer.A fundamental algorithmic problem in this framework is that ofdeciding the existence of a solution: given a source instance and atarget instance for a fixed peer data exchange setting, can thetarget instance be augmented in such a way that the source instanceand the augmented target instance satisfy all constraints of thesetting? We investigate the computational complexity of the problemfor peer data exchange settings in which the constraints are givenby tuple generating dependencies. We show that this problem isalways in NP, and that it can be NP-complete even for "acyclic"peer data exchange settings. We also show that the data complexityof the certain answers of target conjunctive queries is in coNP,and that it can be coNP-complete even for "acyclic" peer dataexchange settings.After this, we explore the boundary between tractability andintractability for the problem of deciding the existence of asolution. To this effect, we identify broad syntactic conditions onthe constraints between the peers under which testing for solutionsis solvable in polynomial time. These syntactic conditions includethe important special case of peer data exchange in which thesource-to-target constraints are arbitrary tuple generatingdependencies, but the target-to-source constraints arelocal-as-view dependencies. Finally, we show that the syntacticconditions we identified are tight, in the sense that minimalrelaxations of them lead to intractability.

#index 809249
#* Composition of mappings given by embedded dependencies
#@ Alan Nash;Philip A. Bernstein;Sergey Melnik
#t 2005
#c 5
#% 289266
#% 328429
#% 359443
#% 378409
#% 384978
#% 415979
#% 465053
#% 465057
#% 490489
#% 765432
#% 765540
#% 801676
#% 1015302
#! Composition of mappings between schemas is essential to support schema evolution, data exchange, data integration, and other data management tasks. In many applications, mappings are given by embedded dependencies. In this paper, we study the issues involved in composing such mappings.

#index 809250
#* Multi-structural databases
#@ Ronald Fagin;R. Guha;Ravi Kumar;Jasmine Novak;D. Sivakumar;Andrew Tomkins
#t 2005
#c 5
#% 210182
#% 246016
#% 256685
#% 275929
#% 280419
#% 301165
#% 308509
#% 408602
#% 413618
#% 459025
#% 461921
#% 464215
#% 482093
#% 485251
#% 494333
#% 631985
#% 770307
#% 770327
#% 783496
#% 993996
#% 1227358
#! We introduce the Multi-Structural Database, a new dataframework to support efficient analysis of large, complex datasets. An instance of the model consists of a set of data objects,together with a schema that specifies segmentations of the set ofdata objects according to multiple distinct criteria (e.g., into ataxonomy based on a hierarchical attribute). Within this model, wedevelop a rich set of analytical operations and design highlyefficient algorithms for these operations. Our operations areformulated as optimization problems, and allow the user to analyzethe underlying data in terms of the allowed segmentations.Our algorithms and results extend those of Fagin et al. [8] whostudied composition of mappings given by several kinds ofconstraints. In particular, they proved that full source-to-targettuple-generating dependencies (tgds) are closed under composition,but embedded source-to-target tgds are not. They introduced a classof second-order constraints, SO tgds, that isclosed under composition and has desirable properties for dataexchange.We study constraints that need not be source-to-target and weconcentrate on obtaining (first-order) embedded dependencies. Aspart of this study, we also consider full dependencies andsecond-order constraints that arise from Skolemizing embeddeddependencies. For each of the three classes of mappings that westudy, we provide (a) an algorithm that attempts to compute thecomposition and (b) sufficient conditions on the input mappingsthat guarantee that the algorithm will succeed.In addition, we give several negative results. In particular, weshow that full dependencies are not closed under composition, andthat second-order dependencies that are not limited to besource-to-target are not closed under restricted composition.Furthermore, we show that determining whether the composition canbe given by these kinds of dependencies is undecidable.

#index 809251
#* A divide-and-merge methodology for clustering
#@ David Cheng;Santosh Vempala;Ravi Kannan;Grant Wang
#t 2005
#c 5
#% 36672
#% 118771
#% 232768
#% 280404
#% 289037
#% 296738
#% 309128
#% 342621
#% 375017
#% 413610
#% 413618
#% 420083
#% 460812
#% 495795
#% 577257
#% 580670
#% 723892
#% 749492
#% 755402
#% 785121
#! We present a divide-and-merge methodology for clustering a set of objects that combines a top-down "divide" phase with a bottom-up "merge" phase. In contrast, previous algorithms either use top-down or bottom-up methods to construct a hierarchical clustering or produce a flat clustering using local search (e.g., k-means). Our divide phase produces a tree whose leaves are the elements of the set. For this phase, we use an efficient spectral algorithm. The merge phase quickly finds an optimal tree-respecting partition for many natural objective functions, e.g., k-means, min-diameter, min-sum, correlation clustering, etc., We present a meta-search engine that uses this methodology to cluster results from web searches. We also give empirical results on text-based data where the algorithm performs better than or competitively with existing clustering algorithms.

#index 809252
#* Allocating isolation levels to transactions
#@ Alan Fekete
#t 2005
#c 5
#% 3645
#% 9241
#% 55415
#% 201869
#% 336201
#% 403195
#% 509531
#% 632092
#% 668738
#! Serializability is a key property for executions of OLTP systems; without this, integrity constraints on the data can be violated due to concurrent activity. Serializability can be guaranteed regardless of application logic, by using a serializable concurrency control mechanism such as strict two-phase locking (S2PL); however the reduction in concurrency from this is often too great, and so a DBMS offers the DBA the opportunity to use different concurrency control mechanisms for some transactions, if it is safe to do so. However, little theory has existed to decide when it is safe! In this paper, we discuss the problem of taking a collection of transactions, and allocating each to run at an appropriate isolation level (and thus use a particular concurrency control mechanism), while still ensuring that every execution will be conflict serializable. When each transaction can use either S2PL, or snapshot isolation, we characterize exactly the acceptable allocations, and provide a simple graph-based algorithm which determines the weakest acceptable allocation.

#index 809253
#* Buffering in query evaluation over XML streams
#@ Ziv Bar-Yossef;Marcus Fontoura;Vanja Josifovski
#t 2005
#c 5
#% 238182
#% 278835
#% 293720
#% 347223
#% 378388
#% 378408
#% 453512
#% 460787
#% 465061
#% 480296
#% 576107
#% 576108
#% 600560
#% 643566
#% 654476
#% 654477
#% 659987
#% 659995
#% 723898
#% 749451
#% 801685
#% 803121
#% 1015272
#% 1016148
#! All known algorithms for evaluating advanced XPath queries (e.g., ones with predicates or with closure axes) on XML streams employ buffers to temporarily store fragments of the document stream. In many cases, these buffers grow very large and constitute a major memory bottleneck. In this paper, we identify two broad classes of evaluation problems that independently necessitate the use of large memory buffers in evaluation of queries over XML streams: (1) full-fledged evaluation (as opposed to just filtering) of queries with predicates; (2) evaluation (whether full-fledged or filtering) of queries with "multi-variate" predicates.We prove quantitative lower bounds on the amount of memory required in each of these scenarios. The bounds are stated in terms of novel document properties that we define. We show that these scenarios, in combination with query evaluation over recursive documents, cover the cases in which large buffers are required. Finally, we present algorithms that match the lower bounds for an important fragment of XPath.

#index 809254
#* Histograms revisited: when are histograms the best approximation method for aggregates over joins?
#@ Alin Dobra
#t 2005
#c 5
#% 102784
#% 152585
#% 201921
#% 214073
#% 273682
#% 273908
#% 397354
#% 427219
#% 481266
#% 861968
#! The traditional statistical assumption for interpreting histograms and justifying approximate query processing methods based on them is that all elements in a bucket have the same frequency -- the so called uniform distribution assumption. In this paper we show that a significantly less restrictive statistical assumption - the elements within a bucket are randomly arranged even though they might have different frequencies -- leads to identical formulae for approximating aggregate queries using histograms. This observation allows us to identify scenarios in which histograms are well suited as approximation methods -- in fact we show that in these situations sampling and sketching are significantly worse -- and provide tight error guarantees for the quality of approximations. At the same time we show that, on average, histograms are rather poor approximators outside these scenarios.

#index 809255
#* Lower bounds for sorting with few random accesses to external memory
#@ Martin Grohe;Nicole Schweikardt
#t 2005
#c 5
#% 101797
#% 278835
#% 293720
#% 341100
#% 378388
#% 453512
#% 785130
#% 801685
#% 1015275
#% 1376720
#! We consider a scenario where we want to query a large dataset that is stored in external memory and does not fit into main memory. The most constrained resources in such a situation are the size of the main memory and the number of random accesses to external memory. We note that sequentially streaming data from external memory through main memory is much less prohibitive.We propose an abstract model of this scenario in which we restrict the size of the main memory and the number of random accesses to external memory, but do not restrict sequential reads. A distinguishing feature of our model is that it admits the usage of unlimited external memory for storing intermediate results, such as several hard disks that can be accessed in parallel. In practice, such auxiliary external memory can be crucial. For example, in a first sequential pass the data can be annotated, and in a second pass this annotation can be used to answer the query. Koch's [9] ARB system for answering XPath queries is based on such a strategy.In this model, we prove lower bounds for sorting the input data. As opposed to related results for models without auxiliary external memory for intermediate results, we cannot rely on communication complexity to establish these lower bounds. Instead, we simulate. our model by a non-uniform computation model for which we can establish the lower bounds by combinatorial means.

#index 809256
#* Operator placement for in-network stream query processing
#@ Utkarsh Srivastava;Kamesh Munagala;Jennifer Widom
#t 2005
#c 5
#% 152940
#% 287461
#% 300179
#% 346845
#% 378388
#% 397414
#% 408396
#% 654482
#% 654483
#% 781735
#% 805466
#% 1015278
#% 1016167
#% 1700123
#! In sensor networks, data acquisition frequently takes place at low-capability devices. The acquired data is then transmitted through a hierarchy of nodes having progressively increasing network band-width and computational power. We consider the problem of executing queries over these data streams, posed at the root of the hierarchy. To minimize data transmission, it is desirable to perform "in-network" query processing: do some part of the work at intermediate nodes as the data travels to the root. Most previous work on in-network query processing has focused on aggregation and inexpensive filters. In this paper, we address in-network processing for queries involving possibly expensive conjunctive filters, and joins. We consider the problem of placing operators along the nodes of the hierarchy so that the overall cost of computation and data transmission is minimized. We show that the problem is tractable, give an optimal algorithm, and demonstrate that a simpler greedy operator placement algorithm can fail to find the optimal solution. Finally we define a number of interesting variations of the basic operator placement problem and demonstrate their hardness.

#index 809257
#* Join-distinct aggregate estimation over update streams
#@ Sumit Ganguly;Minos Garofalakis;Amit Kumar;Rajeev Rastogi
#t 2005
#c 5
#% 2833
#% 4439
#% 70370
#% 123589
#% 125830
#% 190611
#% 214073
#% 238182
#% 273682
#% 299989
#% 336610
#% 347226
#% 397354
#% 480628
#% 480805
#% 481749
#% 519953
#% 594029
#% 788218
#% 993969
#! There is growing interest in algorithms for processing andquerying continuous data streams (i.e., data that is seenonly once in a fixed order) with limited memory resources.Providing (perhaps approximate) answers to queries over suchstreams is a crucial requirement for many application environments;examples include large IP network installations where performancedata from different parts of the network needs to be continuouslycollected and analyzed.The ability to estimate the number of distinct (sub)tuples inthe result of a join operation correlating two data streams (i.e.,the cardinality of a projection with duplicate elimination over ajoin) is an important requirement for several data-analysisscenarios. For instance, to enable real-time traffic analysis andload balancing, a network-monitoring application may need toestimate the number of distinct (source,destination) IP-address pairs occurring in the stream of IP packetsobserved by router R1,where the source address is also seen in packets routed through adifferent router R2.Earlier work has presented solutions to the individual problems ofdistinct counting and join-size estimation (without duplicateelimination) over streams. These solutions, however, arefundamentally different and extending or combining them to handleour more complex "Join-Distinct" estimation problem is far fromobvious. In this paper, we propose the firstspace-efficient algorithmic solution to the general Join-Distinctestimation problem over continuous data streams (our techniques canactually handle general update streamscomprising tuple deletions as well as insertions). Our estimatorsare probabilistic in nature and rely on novel algorithms forbuilding and combining a new class of hash-based synopses (termed"JD sketches") for individual update streams. Wedemonstrate that our algorithms can provide low error,high-confidence Join-Distinct estimates using only small space andsmall processing time per update. In fact, we present lower boundsshowing that the space usage of our estimators is within smallfactors of the best possible for the Join-Distinct problem.Preliminary experimental results verify the effectiveness of ourapproach.

#index 809258
#* Space efficient mining of multigraph streams
#@ Graham Cormode;S. Muthukrishnan
#t 2005
#c 5
#% 2833
#% 170682
#% 190611
#% 214073
#% 273682
#% 329790
#% 336610
#% 378388
#% 379443
#% 397426
#% 446438
#% 449100
#% 453512
#% 480805
#% 492912
#% 519953
#% 548654
#% 578389
#% 654463
#% 654497
#% 654507
#% 783741
#% 813786
#% 816392
#% 993960
#% 1015373
#! The challenge of monitoring massive amounts of data generated by communication networks has led to the interest in data stream processing. We study streams of edges in massive communication multigraphs, defined by (source, destination) pairs. The goal is to compute properties of the underlying graph while using small space (much smaller than the number of communicants), and to avoid bias introduced because some edges may appear many times, while others are seen only once. We give results for three fundamental problems on multigraph degree sequences: estimating frequency moments of degrees, finding the heavy hitter degrees, and computing range sums of degree values. In all cases we are able to show space bounds for our summarizing algorithms that are significantly smaller than storing complete information. We use a variety of data stream methods: sketches, sampling, hashing and distinct counting, but a common feature is that we use cascaded summaries: nesting multiple estimation techniques within one another. In our experimental study, we see that such summaries are highly effective, enabling massive multigraph streams to be effectively summarized to answer queries of interest with high accuracy using only a small amount of space.

#index 809259
#* XML type checking with macro tree transducers
#@ S. Maneth;A. Berlea;T. Perst;H. Seidl
#t 2005
#c 5
#% 17980
#% 287482
#% 342438
#% 386282
#% 427027
#% 483543
#% 572328
#% 630965
#% 722731
#% 743615
#% 801670
#% 1700119
#! MSO logic on unranked trees has been identified as a convenient theoretical framework for reasoning about expressiveness and implementations of practical XML query languages. As a corresponding theoretical foundation of XML transformation languages, the "transformation language" TL is proposed. This language is based on the "document transformation language" DTL of Maneth and Neven which incorporates full MSO pattern matching, arbitrary navigation in the input tree using also MSO patterns, and named procedures. The new language generalizes DTL by additionally allowing procedures to accumulate intermediate results in parameters. It is proved that TL -- and thus in particular DTL - despite their expressiveness still allow for effective inverse type inference. This result is obtained by means of a translation of TL programs into compositions of top-down finite state tree transductions with parameters, also called (stay) macro tree transducers.

#index 809260
#* Regular rewriting of active XML and unambiguity
#@ Serge Abiteboul;Tova Milo;Omar Benjelloun
#t 2005
#c 5
#% 262724
#% 299942
#% 404772
#% 654465
#% 765420
#% 801689
#% 848763
#% 1700122
#! We consider here the exchange of Active XML (AXML) data, i.e., XML documents where some of the data is given explicitly while other parts are given only intensionally as calls to Web services. Peers exchanging AXML data agree on a data exchange schema that specifies in particular which parts of the data are allowed to be intensional. Before sending a document, a peer may need to rewrite it to match the agreed data exchange schema, by calling some of the services and materializing their data. Previous works showed that the rewriting problem is undecidable in the general case and of high complexity in some restricted cases. We argue here that this difficulty is somewhat artificial. Indeed, we study what we believe to be a more adequate, from a practical view point, rewriting problem that is (1) in the spirit of standard 1-unambiguity constraints imposed on XML schema and (2) can be solved by a single pass over the document with a computational device not stronger than a finite state automation. Following previous works, we focus on the core of the problem, i.e., on the problem on words. The results may be extended to (A)XML trees in a straightforward manner.

#index 809261
#* Determining source contribution in integration systems
#@ Alin Deutsch;Yannis Katsis;Yannis Papakonstantinou
#t 2005
#c 5
#% 248038
#% 283052
#% 332166
#% 378409
#% 384978
#% 398240
#% 464717
#% 465053
#% 465057
#% 488620
#% 490489
#% 564416
#% 576100
#% 576116
#% 630963
#% 762652
#% 765432
#% 826031
#% 1700141
#! Owners of sources registered in an information integration system, which provides answers to a (potentially evolving) set of client queries, need to know their contribution to the query results. We study the problem of deciding, given a client query Q and a source registration R, whether R is (i) "self-sufficient" (can contribute to the result of Q even if it is the only source in the system) or (ii) "now complementary" (can contribute, but only in cooperation with other specific existing sources), or (iii)"later complementary" (can contribute if in the future appropriate new sources join the system). We consider open-world integration systems in which registrations are expressed using source-to-target constraints, and queries are answered under "certain answer" semantics.

#index 809262
#* Models and methods for privacy-preserving data publishing and analysis: invited tutorial
#@ Johannes Gehrke
#t 2005
#c 5
#! The digitization of our daily lives has led to an explosion in the collection of digital data by governments, corporations, and individuals. Protection of confidentiality of this data is of utmost importance. However, knowledge of statistical properties of this private data can have significant societal benefit, for example, in decisions about the allocation of public funds based on Census data, or in the analysis of medical data from different hospitals to understand the interaction of drugs.This tutorial will survey recent research that builds bridges between the two seemingly conflicting goals of sharing data while preserving data privacy and confidentiality. The tutorial will cover definitions of privacy and disclosure, and associated methods how to enforce them.More information, including a list of references to related work can be found at the following website: http://www.cs.cornell.edu/database/privacy.

#index 809263
#* Estimating arbitrary subset sums with few probes
#@ Noga Alon;Nick Duffield;Carsten Lund;Mikkel Thorup
#t 2005
#c 5
#% 1331
#% 227883
#% 243166
#% 248812
#% 273714
#% 273908
#% 274152
#% 580214
#% 636486
#% 763999
#% 1815547
#! Suppose we have a large table T of items i, each with a weight wi, e.g., people and their salary. In a general preprocessing step for estimating arbitrary subset sums, we assign each item a random priority depending on its weight. Suppose we want to estimate the sum of an arbitrary subset I ⊆ T. For any q 2, considering only the q highest priority items from I, we obtain an unbiased estimator of the sum whose relative standard deviation is O(1/√q). Thus to get an expected approximation factor of 1 ± ε, it suffices to consider O(1/±ε2) items from I. Our estimator needs no knowledge of the number of items in the subset I, but we can also estimate that number if we want to estimate averages.The above scheme performs the same role as the on-line aggregation of Hellerstein et al. (SIGMOD'97) but it has the advantage of having expected good performance for any possible sequence of weights. In particular, the performance does not deteriorate in the common case of heavy-tailed weight distributions. This point is illustrated experimentally both with real and synthetic data.We will also show that our approach can be used to improve Cohen's size estimation framework (FOCS'94).

#index 809264
#* FTW: fast similarity search under the time warping distance
#@ Yasushi Sakurai;Masatoshi Yoshikawa;Christos Faloutsos
#t 2005
#c 5
#% 80995
#% 86950
#% 137711
#% 168260
#% 172949
#% 232122
#% 248796
#% 286739
#% 308497
#% 333941
#% 341300
#% 397381
#% 460862
#% 462231
#% 479649
#% 480146
#% 481609
#% 529663
#% 564263
#% 616530
#% 654456
#% 993965
#! Time-series data naturally arise in countless domains, such as meteorology, astrophysics, geology, multimedia, and economics. Similarity search is very popular, and DTW (Dynamic Time Warping) is one of the two prevailing distance measures. Although DTW incurs a heavy computation cost, it provides scaling along the time axis. In this paper, we propose FTW (Fast search method for dynamic Time Warping), which guarantees no false dismissals in similarity query processing. FTW efficiently prunes a significant number of the search cost. Experiments on real and synthetic sequence data sets reveals that FTW is significantly faster than the best existing method, up to 222 times.

#index 809265
#* Space complexity of hierarchical heavy hitters in multi-dimensional data streams
#@ John Hershberger;Nisheeth Shrivastava;Subhash Suri;Csaba D. Tóth
#t 2005
#c 5
#% 278835
#% 344400
#% 378388
#% 492912
#% 548479
#% 569754
#% 646218
#% 765414
#% 801696
#% 993960
#% 1015293
#! Heavy hitters, which are items occurring with frequency above a given threshold, are an important aggregation and summary tool when processing data streams or data warehouses. Hierarchical heavy hitters (HHHs) have been introduced as a natural generalization for hierarchical data domains, including multi-dimensional data. An item x in a hierarchy is called a φ-HHH if its frequency after discounting the frequencies of all its descendant hierarchical heavy hitters exceeds φn, where φ is a user-specified parameter and n is the size of the data set. Recently, single-pass schemes have been proposed for computing φ-HHHs using space roughly O(1/φ log(φn)). The frequency estimates of these algorithms, however, hold only for the total frequencies of items, and not the discounted frequencies; this leads to false positives because the discounted frequency can be significantly smaller than the total frequency. This paper attempts to explain the difficulty of finding hierarchical heavy hitters with better accuracy. We show that a single-pass deterministic scheme that computes φ-HHHs in a d-dimensional hierarchy with any approximation guarantee must use Ω(1/φd+1) space. This bound is tight: in fact, we present a data stream algorithm that can report the φ-HHHs without false positives in O(1/φd+1) space.

#index 809266
#* Differential constraints
#@ Bassem Sayrafi;Dirk Van Gucht
#t 2005
#c 5
#% 5385
#% 26736
#% 30077
#% 101956
#% 129564
#% 289305
#% 300711
#% 333877
#% 378395
#% 478770
#% 481290
#% 502141
#% 729449
#% 801682
#% 801700
#% 1738871
#! Differential constraints are a class of finite difference equations specified over functions from the powerset of a finite set into the reals. We characterize the implication problem for such constraints in terms of lattice decompositions, and give a sound and complete set of inference rules. We relate differential constraints to a subclass of propositional logic formulas, allowing us to show that the implication problem is coNP-complete. Furthermore, we apply the theory of differential constraints to the problem of concise representations in the frequent itemset problem by linking differential constraints to disjunctive rules. We also establish a connection to relational databases by associating differential constraints to positive boolean dependencies.

#index 809267
#* Diagnosis of asynchronous discrete event systems: datalog to the rescue!
#@ Serge Abiteboul;Zoë Abrams;Stefan Haar;Tova Milo
#t 2005
#c 5
#% 2991
#% 11797
#% 11819
#% 13025
#% 54570
#% 54571
#% 77932
#% 83933
#% 106829
#% 183006
#% 333844
#% 384978
#% 420238
#% 429749
#% 452753
#% 480417
#% 496291
#% 543169
#% 572300
#% 654485
#% 1388056
#% 1389466
#% 1700138
#! We consider query optimization techniques for data intensive P2P applications. We show how to adapt an old technique from deductive databases, namely Query-Sub-Query (QSQ), to a setting where autonomous and distributed peers share large volumes of interelated data.We illustrate the technique with an important telecommunication problem, the diagnosis of distributed telecom systems. We show that (i) the problem can be modeled using Datalog programs, and (ii) it can benefit from the large battery of optimization techniques developed for Datalog. In particular, we show that a simple generic use of the extension of QSQ achieves an optimization as good as that previously provided by dedicated diagnosis algorithms. Furthermore, we show that it allows solving efficiently a much larger class of system analysis problems.

#index 809268
#* Relative risk and odds ratio: a data mining perspective
#@ Haiquan Li;Jinyan Li;Limsoon Wong;Mengling Feng;Yap-Peng Tan
#t 2005
#c 5
#% 152934
#% 248791
#% 279120
#% 280409
#% 300120
#% 326878
#% 338594
#% 466426
#% 487998
#% 577214
#% 729933
#% 729984
#% 751575
#! We are often interested to test whether a given cause has a given effect. If we cannot specify the nature of the factors involved, such tests are called model-free studies. There are two major strategies to demonstrate associations between risk factors (ie. patterns) and outcome phenotypes (ie. class labels). The first is that of prospective study designs, and the analysis is based on the concept of "relative risk": What fraction of the exposed (ie. has the pattern) or unexposed (ie. lacks the pattern) individuals have the phenotype (ie. the class label)? The second is that of retrospective designs, and the analysis is based on the concept of "odds ratio": The odds that a case has been exposed to a risk factor is compared to the odds for a case that has not been exposed. The efficient extraction of patterns that have good relative risk and/or odds ratio has not been previously studied in the data mining context. In this paper, we investigate such patterns. We show that this pattern space can be systematically stratified into plateaus of convex spaces based on their support levels. Exploiting convexity, we formulate a number of sound and complete algorithms to extract the most general and the most specific of such patterns at each support level. We compare these algorithms. We further demonstrate that the most efficient among these algorithms is able to mine these sophisticated patterns at a speed comparable to that of mining frequent closed patterns, which are patterns that satisfy considerably simpler conditions.

#index 809788
#* Proceedings of the 4th ACM international workshop on Data engineering for wireless and mobile access
#@ Vijay Kumar;Arkady Zaslavsky;Ugur Cetintemel;Alexandros Labrinidis
#t 2005
#c 5
#! It is our great pleasure to welcome you all to the biennial ACM International Workshop on Data Engineering for Wireless and Mobile Access (MobiDE'05), held in conjunction with SIGMOD 2005. MobiDE continues its tradition of bringing together researchers and practitioners in databases, mobile computing, and networking, and providing a full day of exciting presentations and discussions. As in previous years, the workshop will serve as a forum to present latest research and engineering results and contributions, and set future directions in wireless and mobile data management.MobiDE'05 is the fourth of a series of workshops that strives to bridge the data management and mobile computing communities. The first MobiDE workshop (MobiDE'99) took place in Seattle in August 1999, in conjunction with MobiCom 1999. The second MobiDE workshop (MobiDE'01) was held in conjunction with SIGMOD/PODS 2001 in Santa Barbara in May 2001. The third MobiDE workshop was held in conjunction with MobiCom 2003 in San Diego in September 2003.The call for papers for MobiDE'05 attracted 33 high-quality submissions, making the selection process very competitive. All papers were reviewed by three members of the Program Committee. Eventually, 12 papers were selected, resulting in an acceptance rate of 36%. The final program covers a broad variety of topics, including mobile systems and applications, location-based data management, wireless sensor networks, and QoS-driven wireless data delivery. We believe that these proceedings will thus serve as a valuable reference point for the latest results on mobile and wireless data engineering. In addition, the workshop program includes a keynote speech by Prof. Michael Franklin of the University of California, Berkeley on "Space, Time, and Other Tricky Issues in Bridging the Physical-Virtual Divide".

#index 810006
#* Proceedings of the 2005 ACM SIGMOD international conference on Management of data
#@ Fatma Ozcan
#t 2005
#c 5
#! 2005 marks the 31st SIGMOD conference, preceded by 5 workshops sponsored by SIGMOD's predecessor organization, SIGFIDET. The field has grown and evolved significantly over these 36 years, but the SIGMOD conference has remained throughout as one of the foremost annual events for exchanging the latest ideas and advances in data management.The SIGMOD conference is sponsored by the Association for Computing Machinery (ACM) and its Special Interest Group on Management of Data (SIGMOD). Two days of the conference overlap with the Symposium on Principles of Database System (PODS), and several workshops and other events are co-located with SIGMOD.This year 455 research papers were submitted to SIGMOD for consideration, of which 431 were reviewed. (The remaining papers were eliminated due to double-blind or length violations, overlap with other papers, or author withdrawal.) There has been much discussion in recent years of improving the overall conference reviewing process, as our field grows and evolves, and demands on researchers increase. Two innovations were added to this year's reviewing process on an experimental basis for SIGMOD:•There was no limit on the number of accepted papers -- decisions were based on suitable quality alone. Nevertheless, the tally of 65 accepted papers constitutes an acceptance ratio (15%) similar to recent years.•An "author feedback" phase was added, in which authors could read their reviews and provide feedback to be used in the final decision process. Reactions to this new feature ran the spectrum from frustration to joy, but most felt it to be a worthwhile experiment.

#index 810007
#* Sampling algorithms in a stream operator
#@ Theodore Johnson;S. Muthukrishnan;Irina Rozenbaum
#t 2005
#c 5
#% 1331
#% 213969
#% 300179
#% 333931
#% 378388
#% 480805
#% 481749
#% 548654
#% 616528
#% 654497
#% 762042
#% 765404
#% 801693
#% 993949
#% 993960
#% 1015363
#% 1016170
#! Complex queries over high speed data streams often need to rely on approximations to keep up with their input. The research community has developed a rich literature on approximate streaming algorithms for this application. Many of these algorithms produce samples of the input stream, providing better properties than conventional random sampling. In this paper, we abstract the stream sampling process and design a new stream sample operator. We show how it can be used to implement a wide variety of algorithms that perform sampling and sampling-based aggregations. Also, we show how to implement the operator in Gigascope - a high speed stream database specialized for IP network monitoring applications. As an example study, we apply the operator within such an enhanced Gigascope to perform subset-sum sampling which is of great interest for IP network management. We evaluate this implemention on a live, high speed internet traffic data stream and find that (a) the operator is a flexible, versatile addition to Gigascope suitable for tuning and algorithm engineering, and (b) the operator imposes only a small evaluation overhead. This is the first operational implementation we know of, for a wide variety of stream sampling algorithms at line speed within a data stream management system.

#index 810008
#* Fault-tolerance in the Borealis distributed stream processing system
#@ Magdalena Balazinska;Hari Balakrishnan;Samuel Madden;Michael Stonebraker
#t 2005
#c 5
#% 2027
#% 86930
#% 202146
#% 210179
#% 300167
#% 378388
#% 397372
#% 399766
#% 403195
#% 433941
#% 459005
#% 580964
#% 602675
#% 654444
#% 654462
#% 654474
#% 654488
#% 654497
#% 726621
#% 732000
#% 765470
#% 778539
#% 800583
#% 801694
#% 1015280
#% 1015324
#% 1016158
#% 1016170
#! We present a replication-based approach to fault-tolerant distributed stream processing in the face of node failures, network failures, and network partitions. Our approach aims to reduce the degree of inconsistency in the system while guaranteeing that available inputs capable of being processed are processed within a specified time threshold. This threshold allows a user to trade availability for consistency: a larger time threshold decreases availability but limits inconsistency, while a smaller threshold increases availability but produces more inconsistent results based on partial data. In addition, when failures heal, our scheme corrects previously produced results, ensuring eventual consistency.Our scheme uses a data-serializing operator to ensure that all replicas process data in the same order, and thus remain consistent in the absence of failures. To regain consistency after a failure heals, we experimentally compare approaches based on checkpoint/redo and undo/redo techniques and illustrate the performance trade-offs between these schemes.

#index 810009
#* Holistic aggregates in a networked world: distributed tracking of approximate quantiles
#@ Graham Cormode;Minos Garofalakis;S. Muthukrishnan;Rajeev Rastogi
#t 2005
#c 5
#% 214073
#% 248820
#% 273682
#% 333931
#% 480628
#% 480805
#% 576119
#% 654443
#% 654463
#% 654482
#% 654488
#% 654497
#% 783740
#% 800582
#% 801695
#% 993960
#% 993969
#% 1016155
#% 1016178
#! While traditional database systems optimize for performance on one-shot queries, emerging large-scale monitoring applications require continuous tracking of complex aggregates and data-distribution summaries over collections of physically-distributed streams. Thus, effective solutions have to be simultaneously space efficient (at each remote site), communication efficient (across the underlying communication network), and provide continuous, guaranteed-quality estimates. In this paper, we propose novel algorithmic solutions for the problem of continuously tracking complex holistic aggregates in such a distributed-streams setting --- our primary focus is on approximate quantile summaries, but our approach is more broadly applicable and can handle other holistic-aggregate functions (e.g., "heavy-hitters" queries). We present the first known distributed-tracking schemes for maintaining accurate quantile estimates with provable approximation guarantees, while simultaneously optimizing the storage space at each remote site as well as the communication cost across the network. In a nutshell, our algorithms employ a combination of local tracking at remote sites and simple prediction models for local site behavior in order to produce highly communication- and space-efficient solutions. We perform extensive experiments with real and synthetic data to explore the various tradeoffs and understand the role of prediction models in our schemes. The results clearly validate our approach, revealing significant savings over naive solutions as well as our analytical worst-case guarantees.

#index 810010
#* Deriving private information from randomized data
#@ Zhengli Huang;Wenliang Du;Biao Chen
#t 2005
#c 5
#% 15268
#% 157045
#% 233480
#% 300184
#% 333876
#% 575967
#% 575969
#% 576111
#% 577233
#% 577289
#% 727904
#% 729930
#% 729962
#% 769937
#% 769943
#% 769956
#% 769962
#% 785363
#% 785414
#% 993988
#! Randomization has emerged as a useful technique for data disguising in privacy-preserving data mining. Its privacy properties have been studied in a number of papers. Kargupta et al. challenged the randomization schemes, and they pointed out that randomization might not be able to preserve privacy. However, it is still unclear what factors cause such a security breach, how they affect the privacy preserving property of the randomization, and what kinds of data have higher risk of disclosing their private contents even though they are randomized.We believe that the key factor is the correlations among attributes. We propose two data reconstruction methods that are based on data correlations. One method uses the Principal Component Analysis (PCA) technique, and the other method uses the Bayes Estimate (BE) technique. We have conducted theoretical and experimental analysis on the relationship between data correlations and the amount of private information that can be disclosed based our proposed data reconstructions schemes. Our studies have shown that when the correlations are high, the original data can be reconstructed more accurately, i.e., more private information can be disclosed.To improve privacy, we propose a modified randomization scheme, in which we let the correlation of random noises "similar" to the original data. Our results have shown that the reconstruction accuracy of both PCA-based and BE-based schemes become worse as the similarity increases.

#index 810011
#* Incognito: efficient full-domain K-anonymity
#@ Kristen LeFevre;David J. DeWitt;Raghu Ramakrishnan
#t 2005
#c 5
#% 210182
#% 223781
#% 420053
#% 443463
#% 481290
#% 481758
#% 576761
#% 576762
#% 577239
#% 644560
#% 785363
#% 800514
#% 800515
#% 801690
#% 1700134
#! A number of organizations publish microdata for purposes such as public health and demographic research. Although attributes that clearly identify individuals, such as Name and Social Security Number, are generally removed, these databases can sometimes be joined with other public databases on attributes such as Zipcode, Sex, and Birthdate to re-identify individuals who were supposed to remain anonymous. "Joining" attacks are made easier by the availability of other, complementary, databases over the Internet.K-anonymization is a technique that prevents joining attacks by generalizing and/or suppressing portions of the released microdata so that no individual can be uniquely distinguished from a group of size k. In this paper, we provide a practical framework for implementing one model of k-anonymization, called full-domain generalization. We introduce a set of algorithms for producing minimal full-domain generalizations, and show that these algorithms perform up to an order of magnitude faster than previous algorithms on two real-life databases.Besides full-domain generalization, numerous other models have also been proposed for k-anonymization. The second contribution in this paper is a single taxonomy that categorizes previous models and introduces some promising new alternatives.

#index 810012
#* To do or not to do: the dilemma of disclosing anonymized data
#@ Laks V. S. Lakshmanan;Raymond T. Ng;Ganesh Ramesh
#t 2005
#c 5
#% 67453
#% 152934
#% 297186
#% 300184
#% 329858
#% 333876
#% 338460
#% 340475
#% 504167
#% 575967
#% 576110
#% 576761
#% 577239
#% 740764
#% 751578
#% 772829
#% 801690
#% 1016137
#% 1700134
#! Decision makers of companies often face the dilemma of whether to release data for knowledge discovery, vis a vis the risk of disclosing proprietary or sensitive information. While there are various "sanitization" methods, in this paper we focus on anonymization, given its widespread use in practice. We give due diligence to the question of "just how safe the anonymized data is", in terms of protecting the true identities of the data objects. We consider both the scenarios when the hacker has no information, and more realistically, when the hacker may have partial information about items in the domain. We conduct our analyses in the context of frequent set mining. We propose to capture the prior knowledge of the hacker by means of a belief function, where an educated guess of the frequency of each item is assumed. For various classes of belief functions, which correspond to different degrees of prior knowledge, we derive formulas for computing the expected number of "cracks". While obtaining the exact values for the more general situations is computationally hard, we propose a heuristic called the O-estimate. It is easy to compute, and is shown to be accurate empirically with real benchmark datasets. Finally, based on the O-estimates, we propose a recipe for the decision makers to resolve their dilemma.

#index 810013
#* Constrained optimalities in query personalization
#@ Georgia Koutrika;Yannis Ioannidis
#t 2005
#c 5
#% 116040
#% 315024
#% 342687
#% 369236
#% 399057
#% 399762
#% 411554
#% 413615
#% 479816
#% 745519
#% 777931
#! Personalization is a powerful mechanism that helps users to cope with the abundance of information on the Web. Database query personalization achieves this by dynamically constructing queries that return results of high interest to the user. This, however, may conflict with other constraints on the query execution time and/or result size that may be imposed by the search context, such as the device used, the network connection, etc. For example, if the user is accessing information using a mobile phone, then it is desirable to construct a personalized query that executes quickly and returns a handful of answers. Constrained Query Personalization (CQP) is an integrated approach to database query answering that dynamically takes into account the queries issued, the user's interest in the results, response time, and result size in order to build personalized queries. In this paper, we introduce CQP as a family of constrained optimization problems, where each time one of the parameters of concern is optimized while the others remain within the bounds of range constraints. Taking into account some key (exact or approximate) properties of these parameters, we map CQP to a state search problem and provide several algorithms for the discovery of optimal solutions. Experimental results demonstrate the effectiveness of the proposed techniques and the appropriateness of the overall approach.

#index 810014
#* Reference reconciliation in complex information spaces
#@ Xin Dong;Alon Halevy;Jayant Madhavan
#t 2005
#c 5
#% 201889
#% 310516
#% 310533
#% 310546
#% 420495
#% 480496
#% 577238
#% 577247
#% 587758
#% 642983
#% 654467
#% 729913
#% 766199
#% 870896
#% 993980
#! Reference reconciliation is the problem of identifying when different references (i.e., sets of attribute values) in a dataset correspond to the same real-world entity. Most previous literature assumed references to a single class that had a fair number of attributes (e.g., research publications). We consider complex information spaces: our references belong to multiple related classes and each reference may have very few attribute values. A prime example of such a space is Personal Information Management, where the goal is to provide a coherent view of all the information on one's desktop.Our reconciliation algorithm has three principal features. First, we exploit the associations between references to design new methods for reference comparison. Second, we propagate information between reconciliation decisions to accumulate positive and negative evidences. Third, we gradually enrich references by merging attribute values. Our experiments show that (1) we considerably improve precision and recall over standard methods on a diverse set of personal information datasets, and (2) there are advantages to using our algorithm even on a standard citation dataset benchmark.

#index 810015
#* Magnet: supporting navigation in semistructured data environments
#@ Vineet Sinha;David R. Karger
#t 2005
#c 5
#% 10019
#% 85443
#% 115473
#% 115476
#% 118771
#% 201991
#% 232644
#% 236416
#% 244016
#% 249142
#% 318975
#% 452641
#% 460924
#% 504571
#% 642993
#% 643044
#% 751830
#! With the growing importance of systems containing arbitrary semi-structured relationships, the need for supporting users searching in such repositories has grown. Currently support for users' search needs either has required domain-specific user interfaces or has required users to be schema experts. We have developed a general-purpose tool that offers users helpful navigation and refinement options for seeking information in these semistructured repositories. We show how a tool can be built without requiring domain-specific assumptions about the information being explored. In addition to describing a general approach to the problem, we provide a set of natural, general-purpose refinement tactics, many generalized from past work on textual information retrieval.

#index 810016
#* Proactive re-optimization
#@ Shivnath Babu;Pedro Bizarro;David DeWitt
#t 2005
#c 5
#% 58375
#% 77963
#% 102784
#% 210190
#% 245996
#% 248793
#% 248795
#% 273694
#% 273908
#% 273909
#% 273911
#% 285924
#% 300167
#% 411355
#% 411554
#% 420114
#% 480955
#% 578391
#% 728308
#% 729915
#% 746499
#% 765434
#% 765456
#% 765468
#% 810017
#! Traditional query optimizers rely on the accuracy of estimated statistics to choose good execution plans. This design often leads to suboptimal plan choices for complex queries, since errors in estimates for intermediate subexpressions grow exponentially in the presence of skewed and correlated data distributions. Reoptimization is a promising technique to cope with such mistakes. Current re-optimizers first use a traditional optimizer to pick a plan, and then react to estimation errors and resulting suboptimalities detected in the plan during execution. The effectiveness of this approach is limited because traditional optimizers choose plans unaware of issues affecting reoptimization. We address this problem using proactive reoptimization, a new approach that incorporates three techniques: i) the uncertainty in estimates of statistics is computed in the form of bounding boxes around these estimates, ii) these bounding boxes are used to pick plans that are robust to deviations of actual values from their estimates, and iii) accurate measurements of statistics are collected quickly and efficiently during query execution. We present an extensive evaluation of these techniques using a prototype proactive re-optimizer named Rio. In our experiments Rio outperforms current re-optimizers by up to a factor of three.

#index 810017
#* Towards a robust query optimizer: a principled and practical approach
#@ Brian Babcock;Surajit Chaudhuri
#t 2005
#c 5
#% 43163
#% 54047
#% 82346
#% 102784
#% 116084
#% 172902
#% 210190
#% 210353
#% 245654
#% 248822
#% 260026
#% 273694
#% 273903
#% 273909
#% 333946
#% 333986
#% 378414
#% 411554
#% 479648
#% 479967
#% 481749
#% 482092
#% 632048
#% 1015256
#! Research on query optimization has focused almost exclusively on reducing query execution time, while important qualities such as consistency and predictability have largely been ignored, even though most database users consider these qualities to be at least as important as raw performance. In this paper, we explore how the query optimization process can be made more robust, focusing on the important subproblem of cardinality estimation. The robust cardinality estimation technique that we propose allows for a user- or application-specified trade-off between performance and predictability, and it captures multi-dimensional correlations while remaining space- and time-efficient.

#index 810018
#* RankSQL: query algebra and optimization for relational top-k queries
#@ Chengkai Li;Kevin Chen-Chuan Chang;Ihab F. Ilyas;Sumin Song
#t 2005
#c 5
#% 77944
#% 152940
#% 213981
#% 227894
#% 273908
#% 273910
#% 300180
#% 333854
#% 333951
#% 340635
#% 340663
#% 397378
#% 411554
#% 479816
#% 480330
#% 480819
#% 481915
#% 565457
#% 631988
#% 765418
#% 765435
#% 993957
#% 994013
#% 1015257
#% 1015317
#% 1015319
#% 1016182
#! This paper introduces RankSQL, a system that provides a systematic and principled framework to support efficient evaluations of ranking (top-k) queries in relational database systems (RDBMS), by extending relational algebra and query optimization. Previously, top-k query processing is studied in the middleware scenario or in RDBMS in a "piecemeal" fashion, i.e., focusing on specific operator or sitting outside the core of query engines. In contrast, we aim to support ranking as a first-class database construct. As a key insight, the new ranking relationship can be viewed as another logical property of data, parallel to the "membership" property of relational data model. While membership is essentially supported in RDBMS, the same support for ranking is clearly lacking. We address the fundamental integration of ranking in RDBMS in a way similar to how membership, i.e., Boolean filtering, is supported. We extend relational algebra by proposing a rank-relational model to capture the ranking property, and introducing new and extended operators to support ranking as a first-class construct. Enabled by the extended algebra, we present a pipelined and incremental execution model of ranking query plans (that cannot be expressed traditionally) based on a fundamental ranking principle. To optimize top-k queries, we propose a dimensional enumeration algorithm to explore the extended plan space by enumerating plans along two dual dimensions: ranking and membership. We also propose a sampling-based method to estimate the cardinality of rank-aware operators, for costing plans. Our experiments show the validity of our framework and the accuracy of the proposed estimation model.

#index 810019
#* A cost-based model and effective heuristic for repairing constraints by value modification
#@ Philip Bohannon;Wenfei Fan;Michael Flaster;Rajeev Rastogi
#t 2005
#c 5
#% 273687
#% 301169
#% 378401
#% 420072
#% 465052
#% 480496
#% 480499
#% 572314
#% 576116
#% 727668
#% 814475
#% 1279213
#% 1279214
#% 1705010
#! Data integrated from multiple sources may contain inconsistencies that violate integrity constraints. The constraint repair problem attempts to find "low cost" changes that, when applied, will cause the constraints to be satisfied. While in most previous work repair cost is stated in terms of tuple insertions and deletions, we follow recent work to define a database repair as a set of value modifications. In this context, we introduce a novel cost framework that allows for the application of techniques from record-linkage to the search for good repairs. We prove that finding minimal-cost repairs in this model is NP-complete in the size of the database, and introduce an approach to heuristic repair-construction based on equivalence classes of attribute values. Following this approach, we define two greedy algorithms. While these simple algorithms take time cubic in the size of the database, we develop optimizations inspired by algorithms for duplicate-record detection that greatly improve scalability. We evaluate our framework and algorithms on synthetic and real data, and show that our proposed optimizations greatly improve performance at little or no cost in repair quality.

#index 810020
#* ConQuer: efficient management of inconsistent databases
#@ Ariel Fuxman;Elham Fazli;Renée J. Miller
#t 2005
#c 5
#% 248038
#% 273687
#% 576116
#% 644182
#% 783532
#% 814475
#% 1016201
#% 1700140
#! Although integrity constraints have long been used to maintain data consistency, there are situations in which they may not be enforced or satisfied. In this paper, we present ConQuer, a system for efficient and scalable answering of SQL queries on databases that may violate a set of constraints. ConQuer permits users to postulate a set of key constraints together with their queries. The system rewrites the queries to retrieve all (and only) data that is consistent with respect to the constraints. The rewriting is into SQL, so the rewritten queries can be efficiently optimized and executed by commercial database systems.We study the overhead of resolving inconsistencies dynamically (at query time). In particular, we present a set of performance experiments that compare the efficiency of the rewriting strategies used by ConQuer. The experiments use queries taken from the TPC-H workload. We show that the overhead is not onerous, and the consistent query answers can often be computed within twice the time required to obtain the answers to the original (non-rewritten) query.

#index 810021
#* Supporting executable mappings in model management
#@ Sergey Melnik;Philip A. Bernstein;Alon Halevy;Erhard Rahm
#t 2005
#c 5
#% 13048
#% 158906
#% 224186
#% 286901
#% 328429
#% 342389
#% 378409
#% 384978
#% 397432
#% 415979
#% 442861
#% 458607
#% 465057
#% 480158
#% 480657
#% 480670
#% 562447
#% 572311
#% 572314
#% 578668
#% 579315
#% 654457
#% 801676
#% 809238
#% 809249
#% 993981
#% 1015302
#% 1015326
#% 1393677
#% 1700139
#! Model management is an approach to simplify the programming of metadata-intensive applications. It offers developers powerful operators, such as Compose, Diff, and Merge, that are applied to models, such as database schemas or interface specifications, and to mappings between models. Prior model management solutions focused on a simple class of mappings that do not have executable semantics. Yet many metadata applications require that mappings be executable, expressed in SQL, XSLT, or other data transformation languages.In this paper, we develop a semantics for model-management operators that allows applying the operators to executable mappings. Our semantics captures previously-proposed desiderata and is language-independent: the effect of the operators is expressed in terms of what they do to the instances of models and mappings. We describe an implemented prototype in which mappings are represented as dependencies between relational schemas, and discuss algebraic optimization of model-management scripts.

#index 810022
#* Stacked indexed views in microsoft SQL server
#@ David DeHaan;Per-Ake Larson;Jingren Zhou
#t 2005
#c 5
#% 13016
#% 152928
#% 169844
#% 210182
#% 210210
#% 227869
#% 300138
#% 300141
#% 333965
#% 463735
#% 464056
#% 479792
#% 480158
#% 481608
#% 482115
#% 565457
#% 571062
#% 571169
#% 1700143
#! Appropriately selected materialized views (also called indexed views) can speed up query execution by orders of magnitude. Most database systems limit support for materialized views to select-project-join expressions, possibly with a group-by, over base tables because this class of views can be efficiently maintained incrementally and thus kept up to date with the underlying source tables. However, limiting views to reference only base tables restricts the class of queries that can be supported by materialized views. View stacking (also called views on views) relaxes one restriction by allowing a materialized view to reference both base tables and other materialized views. This extends materialized view support to additional types of queries. This paper describes a prototype implementation of stacked views within Microsoft SQL Server and explains which classes of queries can be supported. To support view matching for stacked views, a signature mechanism was added to the optimizer. This mechanism turned out to be beneficial also for regular views by significantly speeding up view matching.

#index 810023
#* A nested relational approach to processing SQL subqueries
#@ Bin Cao;Antonio Badia
#t 2005
#c 5
#% 32878
#% 55899
#% 77931
#% 84615
#% 139178
#% 189634
#% 210207
#% 220425
#% 287005
#% 334006
#% 461897
#% 465165
#% 480091
#% 481604
#% 564426
#% 654454
#! One of the most powerful features of SQL is the use of nested queries. Most research work on the optimization of nested queries focuses on aggregate subqueries. However, the solutions proposed for non-aggregate subqueries are still limited, especially for queries having multiple subqueries and null values. In this paper, we show that existing approaches to queries containing non-aggregate subqueries proposed in the literature (including rewrites) are not adequate. We then propose a new efficient approach, the nested relational approach, based on the nested relational algebra. Our approach directly unnests non-aggregate subqueries using hash joins, and treats all subqueries in a uniform manner, being able to deal with nested queries of any type and any level. We report on experimental work that confirms that existing approaches have difficulties dealing with non-aggregate subqueries, and that our approach offers better performance. We also discuss some possibilities for algebraic optimization and the issue of integrating our approach in a relational database system.

#index 810024
#* Stratified computation of skylines with partially-ordered domains
#@ Chee-Yong Chan;Pin-Kwang Eng;Kian-Lee Tan
#t 2005
#c 5
#% 2115
#% 58365
#% 100803
#% 227894
#% 288976
#% 300170
#% 333847
#% 333951
#% 343807
#% 458873
#% 480671
#% 654480
#% 731407
#% 993954
#% 993957
#% 994017
#! In this paper, we study the evaluation of skyline queries with partially-ordered attributes. Because such attributes lack a total ordering, traditional index-based evaluation algorithms (e.g., NN and BBS) that are designed for totally-ordered attributes can no longer prune the space as effectively. Our solution is to transform each partially-ordered attribute into a two-integer domain that allows us to exploit index-based algorithms to compute skyline queries on the transformed space. Based on this framework, we propose three novel algorithms: BBS+ is a straightforward adaptation of BBS using the framework, and SDC (Stratification by Dominance Classification) and SDC+ are optimized to handle false positives and support progressive evaluation. Both SDC and SDC+ exploit a dominance relationship to organize the data into strata. While SDC generates its strata at run time, SDC+ partitions the data into strata offline. We also design two dominance classification strategies (MinPC and MaxPC) to further optimize the performance of SDC and SDC+. We implemented the proposed schemes and evaluated their efficiency. Our results show that our proposed techniques outperform existing approaches by a wide margin, with SDC+-MinPC giving the best performance in terms of both response time as well as progressiveness. To the best of our knowledge, this is the first paper to address the problem of skyline query evaluation involving partially-ordered attribute domains.

#index 810025
#* AGILE: adaptive indexing for context-aware information filters
#@ Jens-Peter Dittrich;Peter M. Fischer;Donald Kossmann
#t 2005
#c 5
#% 64791
#% 86950
#% 143040
#% 206915
#% 248183
#% 297191
#% 300174
#% 333938
#% 420175
#% 427199
#% 463917
#% 464720
#% 622692
#% 640098
#% 640616
#% 731408
#% 800593
#% 993966
#% 1015305
#% 1016270
#! Information filtering has become a key technology for modern information systems. The goal of an information filter is to route messages to the right recipients (possibly none) according to declarative rules called profiles. In order to deal with high volumes of messages, several index structures have been proposed in the past. The challenge addressed in this paper is to carry out stateful information filtering in which profiles refer to values in a database or to previous messages. The difficulty is that database update streams need to be processed in addition to messages. This paper presents AGILE, a way to extend existing index structures so that the indexes adapt to the message/update workload and show good performance in all situations. Performance experiments show that AGILE is overall the clear winner as compared to the best existing approaches. In extreme situations in which it is not the winner, the overheads are small.

#index 810026
#* Automatic physical database tuning: a relaxation-based approach
#@ Nicolas Bruno;Surajit Chaudhuri
#t 2005
#c 5
#% 248815
#% 273908
#% 411554
#% 480158
#% 482100
#% 631950
#% 632100
#% 820356
#% 1016220
#% 1016221
#! In recent years there has been considerable research on automated selection of physical design in database systems. In current solutions, candidate access paths are heuristically chosen based on the structure of each input query, and a subsequent bottom-up search is performed to identify the best overall configuration. To handle large workloads and multiple kinds of physical structures, recent techniques have become increasingly complex: they exhibit many special cases, shortcuts, and heuristics that make it very difficult to analyze and extract properties. In this paper we critically examine the architecture of current solutions. We then design a new framework for the physical design problem that significantly reduces the assumptions and heuristics used in previous approaches. While simplicity and uniformity are important contributions in themselves, we report extensive experimental results showing that our approach could result in comparable (and, in many cases, considerably better) recommendations than state-of-the-art commercial alternatives.

#index 810027
#* Goals and benchmarks for autonomic configuration recommenders
#@ Mariano P. Consens;Denilson Barbosa;Adrian Teisanu;Laurent Mignet
#t 2005
#c 5
#% 1925
#% 97491
#% 102784
#% 248815
#% 248855
#% 365700
#% 397390
#% 397399
#% 427029
#% 479795
#% 480158
#% 482100
#% 765176
#% 770354
#% 820356
#% 1016216
#% 1016220
#% 1016221
#! We are witnessing an explosive increase in the complexity of the information systems we rely upon, Autonomic systems address this challenge by continuously configuring and tuning themselves. Recently, a number of autonomic features have been incorporated into commercial RDBMS; tools for recommending database configurations (i.e., indexes, materialized views, partitions) for a given workload are prominent examples of this promising trend.In this paper, we introduce a flexible characterization of the performance goals of configuration recommenders and develop an experimental evaluation approach to benchmark the effectiveness of these autonomic tools. We focus on exploratory queries and present extensive experimental results using both real and synthetic data that demonstrate the validity of the approach introduced. Our results identify a specific index configuration based on single-column indexes as a very useful baseline for comparisons in the exploratory setting. Furthermore, the experimental results demonstrate the unfulfilled potential for achieving improvements of several orders of magnitude.

#index 810028
#* Privacy preserving OLAP
#@ Rakesh Agrawal;Ramakrishnan Srikant;Dilys Thomas
#t 2005
#c 5
#% 1868
#% 23638
#% 67453
#% 300184
#% 301569
#% 333876
#% 575969
#% 576111
#% 577233
#% 654448
#% 720449
#% 810252
#% 993988
#% 1707132
#! We present techniques for privacy-preserving computation of multidimensional aggregates on data partitioned across multiple clients. Data from different clients is perturbed (randomized) in order to preserve privacy before it is integrated at the server. We develop formal notions of privacy obtained from data perturbation and show that our perturbation provides guarantees against privacy breaches. We develop and analyze algorithms for reconstructing counts of subcubes over perturbed data. We also evaluate the tradeoff between privacy guarantees and reconstruction accuracy and show the practicality of our approach.

#index 810029
#* Efficient computation of multiple group by queries
#@ Zhimin Chen;Vivek Narasayya
#t 2005
#c 5
#% 36117
#% 210182
#% 210208
#% 237203
#% 248014
#% 248815
#% 300166
#% 333848
#% 397371
#% 451543
#% 464706
#% 465007
#% 479450
#% 480158
#% 481288
#% 481290
#% 481749
#% 481951
#% 820356
#% 1015290
#! Data analysts need to understand the quality of data in the warehouse. This is often done by issuing many Group By queries on the sets of columns of interest. Since the volume of data in these warehouses can be large, and tables in a data warehouse often contain many columns, this analysis typically requires executing a large number of Group By queries, which can be expensive. We show that the performance of today's database systems for such data analysis is inadequate. We also show that the problem is computationally hard, and develop efficient techniques for solving it. We demonstrate significant speedup over existing approaches on today's commercial database systems.

#index 810030
#* SHIFT-SPLIT: I/O efficient maintenance of wavelet-transformed multidimensional data
#@ Mehrdad Jahangiri;Dimitris Sacharidis;Cyrus Shahabi
#t 2005
#c 5
#% 257637
#% 259995
#% 273902
#% 316551
#% 397389
#% 458875
#% 480306
#% 480628
#% 581675
#% 654460
#% 810099
#% 1015301
#! The Discrete Wavelet Transform is a proven tool for a wide range of database applications. However, despite broad acceptance, some of its properties have not been fully explored and thus not exploited, particularly for two common forms of multidimensional decomposition. We introduce two novel operations for wavelet transformed data, termed SHIFT and SPLIT, based on the properties of wavelet trees, which work directly in the wavelet domain. We demonstrate their significance and usefulness by analytically proving six important results in four common data maintenance scenarios, i.e., transformation of massive datasets, appending data, approximation of data streams and partial data reconstruction, leading to significant I/O cost reduction in all cases. Furthermore, we show how these operations can be further improved in combination with the optimal coefficient-to-disk-block allocation strategy. Our exhaustive set of empirical experiments with real-world datasets verifies our claims.

#index 810031
#* Tributaries and deltas: efficient and robust aggregation in sensor network streams
#@ Amit Manjhi;Suman Nath;Phillip B. Gibbons
#t 2005
#c 5
#% 2833
#% 378388
#% 519953
#% 577219
#% 654482
#% 731086
#% 745442
#% 745486
#% 765444
#% 783721
#% 783736
#% 783740
#% 783741
#% 800582
#% 801695
#% 805466
#% 993960
#% 1016178
#! Existing energy-efficient approaches to in-network aggregation in sensor networks can be classified into two categories, tree-based and multi-path-based, with each having unique strengths and weaknesses. In this paper, we introduce Tributary-Delta, a novel approach that combines the advantages of the tree and multi-path approaches by running them simultaneously in different regions of the network. We present schemes for adjusting the regions in response to changes in network conditions, and show how many useful aggregates can be readily computed within this new framework. We then show how a difficult aggregate for this context---finding frequent items---can be efficiently computed within the framework. To this end, we devise the first algorithm for frequent items (and for quantiles) that provably minimizes the worst case total communication for non-regular trees. In addition, we give a multi-path algorithm for frequent items that is considerably more accurate than previous approaches. These algorithms form the basis for our efficient Tributary-Delta frequent items algorithm. Through extensive simulation with real-world and synthetic data, we show the significant advantages of our techniques. For example, in computing Count under realistic loss rates, our techniques reduce answer error by up to a factor of 3 compared to any previous technique.

#index 810032
#* Multiple aggregations over data streams
#@ Rui Zhang;Nick Koudas;Beng Chin Ooi;Divesh Srivastava
#t 2005
#c 5
#% 36117
#% 210182
#% 210208
#% 300179
#% 333938
#% 378388
#% 397353
#% 654497
#% 979303
#% 993948
#% 993949
#% 1015373
#% 1016157
#! Monitoring aggregates on IP traffic data streams is a compelling application for data stream management systems. The need for exploratory IP traffic data analysis naturally leads to posing related aggregation queries on data streams, that differ only in the choice of grouping attributes. In this paper, we address this problem of efficiently computing multiple aggregations over high speed data streams, based on a two-level LFTA/HFTA DSMS architecture, inspired by Gigascope.Our first contribution is the insight that in such a scenario, additionally computing and maintaining fine-granularity aggregation queries (phantoms) at the LFTA has the benefit of supporting shared computation. Our second contribution is an investigation into the problem of identifying beneficial LFTA configurations of phantoms and user-queries. We formulate this problem as a cost optimization problem, which consists of two sub-optimization problems: how to choose phantoms and how to allocate space for them in the LFTA. We formally show the hardness of determining the optimal configuration, and propose cost greedy heuristics for these independent sub-problems based on detailed analyses. Our final contribution is a thorough experimental study, based on real IP traffic data, as well as synthetic data, to demonstrate the effectiveness of our techniques for identifying beneficial configurations.

#index 810033
#* Semantics and evaluation techniques for window aggregates in data streams
#@ Jin Li;David Maier;Kristin Tufte;Vassilis Papadimos;Peter A. Tucker
#t 2005
#c 5
#% 378388
#% 420053
#% 480120
#% 480642
#% 578391
#% 654497
#% 726621
#% 803602
#% 1015279
#! A windowed query operator breaks a data stream into possibly overlapping subsets of data and computes a result over each. Many stream systems can evaluate window aggregate queries. However, current stream systems suffer from a lack of an explicit definition of window semantics. As a result, their implementations unnecessarily confuse window definition with physical stream properties. This confusion complicates the stream system, and even worse, can hurt performance both in terms of memory usage and execution time. To address this problem, we propose a framework for defining window semantics, which can be used to express almost all types of windows of which we are aware, and which is easily extensible to other types of windows that may occur in the future. Based on this definition, we explore a one-pass query evaluation strategy, the Window-ID (WID) approach, for various types of window aggregate queries. WID significantly reduces both required memory space and execution time for a large class of window definitions. In addition, WID can leverage punctuations to gracefully handle disorder. Our experimental study shows that WID has better execution-time performance than existing window aggregate query evaluation options that retain and reprocess tuples, and has better latency-accuracy tradeoffs for disordered input streams compared to using a fixed delay for handling disorder.

#index 810034
#* Guaranteeing correctness and availability in P2P range indices
#@ Prakash Linga;Adina Crainiceanu;Johannes Gehrke;Jayavel Shanmugasudaram
#t 2005
#c 5
#% 9241
#% 152947
#% 172918
#% 330305
#% 340175
#% 340176
#% 340297
#% 342374
#% 342375
#% 360802
#% 453509
#% 470844
#% 481296
#% 505869
#% 636008
#% 745498
#% 765444
#% 765446
#% 769458
#% 770901
#% 1015281
#% 1015327
#% 1016166
#! New and emerging P2P applications require sophisticated range query capability and also have strict requirements on query correctness, system availability and item availability. While there has been recent work on developing new P2P range indices, none of these indices guarantee correctness and availability. In this paper, we develop new techniques that can provably guarantee the correctness and availability of P2P range indices. We develop our techniques in the context of a general P2P indexing framework that can be instantiated with most P2P index structures from the literature. As a specific instantiation, we implement P-Ring, an existing P2P range index, and show how it can be extended to guarantee correctness and availability. We quantitatively evaluate our techniques using a real distributed implementation.

#index 810035
#* Online B-tree merging
#@ Xiaowei Sun;Rui Wang;Betty Salzberg;Chendong Zou
#t 2005
#c 5
#% 83183
#% 116085
#% 116086
#% 116087
#% 210174
#% 210175
#% 268786
#% 286929
#% 300164
#% 300207
#% 317933
#% 396925
#% 403195
#% 435119
#% 465149
#% 479470
#% 479483
#% 479639
#% 571093
#% 745497
#! Many scenarios involve merging of two B-tree indexes, both covering the same key range. Increasing demand for continuous availability and high performance requires that such merging be done online, with minimal interference to normal user transactions. In this paper we present an online B-tree merging method, in which the merging of leaf pages in two B-trees are piggybacked lazily with normal user transactions, thus making the merging I/O efficient and allowing user transactions to access only one index instead of both. The concurrency control mechanism is designed to interfere as little as possible with ongoing user transactions. Merging is made forward recoverable by following a conventional logging protocol, with a few extensions. Should a system failure occur, both indexes being merged can be recovered to a consistent state and no merging work is lost. Experiments and analysis show the I/O savings and the performance, and compare variations on the basic algorithm.

#index 810036
#* System RX: one part relational, one part XML
#@ Kevin Beyer;Roberta J. Cochrane;Vanja Josifovski;Jim Kleewein;George Lapis;Guy Lohman;Bob Lyle;Fatma Özcan;Hamid Pirahesh;Normen Seemann;Tuong Truong;Bert Van der Linden;Brian Vickery;Chun Zhang
#t 2005
#c 5
#% 43162
#% 116043
#% 333981
#% 345742
#% 397360
#% 397366
#% 397375
#% 411554
#% 411759
#% 442706
#% 462235
#% 479452
#% 479956
#% 480657
#% 480822
#% 562456
#% 570875
#% 570876
#% 654493
#% 745477
#% 745478
#% 803121
#% 994035
#% 1015273
#% 1015274
#% 1015338
#% 1016134
#! This paper describes the overall architecture and design aspects of a hybrid relational and XML database system called System RX. We believe that such a system is fundamental in the evolution of enterprise data management solutions: XML and relational data will co-exist and complement each other in enterprise solutions. Furthermore, a successful XML repository requires much of the same infrastructure that already exists in a relational database management system. Finally, XML query languages have considerable conceptual and functional overlap with relational dataflow engines. System RX is the first truly hybrid system that comingles XML and relational data, giving them equal footing. The new support for XML includes native support for storage and indexing as well as query compilation and evaluation support for the latest industry-standard query languages, SQL/XML and XQuery. By building a hybrid system, we leverage more than 20 years of data management research to advance XML technology to the same standards expected from mature relational systems.

#index 810037
#* On joining and caching stochastic streams
#@ Junyi Xie;Jun Yang;Yuguo Chen
#t 2005
#c 5
#% 243299
#% 270503
#% 273908
#% 288718
#% 338425
#% 531958
#% 576113
#% 654444
#% 771230
#% 993958
#% 1015280
#% 1016156
#! We consider the problem of joining data streams using limited cache memory, with the goal of producing as many result tuples as possible from the cache. Many cache replacement heuristics have been proposed in the past. Their performance often relies on implicit assumptions about the input streams, e.g., that the join attribute values follow a relatively stationary distribution. However, in general and in practice, streams often exhibit more complex behaviors, such as increasing trends and random walks, rendering these "hardwired" heuristics inadequate.In this paper, we propose a framework that is able to exploit known or observed statistical properties of input streams to make cache replacement decisions aimed at maximizing the expected number of result tuples. To illustrate the complexity of the solution space, we show that even an algorithm that considers, at every time step, all possible sequences of future replacement decisions may not be optimal. We then identify a condition between two candidate tuples under which an optimal algorithm would always choose one tuple over the other to replace. We develop a heuristic that behaves consistently with an optimal algorithm whenever this condition is satisfied. We show through experiments that our heuristic outperforms previous ones.As another evidence of the generality of our framework, we show that the classic caching/paging problem for static objects can be reduced to a stream join problem and analyzed under our framework, yielding results that agree with or extend classic ones.

#index 810038
#* RPJ: producing fast join results on streams through rate-based optimization
#@ Yufei Tao;Man Lung Yiu;Dimitris Papadias;Marios Hadjieleftheriou;Nikos Mamoulis
#t 2005
#c 5
#% 159337
#% 273909
#% 378388
#% 397352
#% 576104
#% 654444
#% 659919
#% 745488
#% 745534
#% 993956
#% 1015278
#% 1015280
#% 1015296
#% 1016153
#% 1016156
#! We consider the problem of "progressively" joining relations whose records are continuously retrieved from remote sources through an unstable network that may incur temporary failures. The objectives are to (i) start reporting the first output tuples as soon as possible (before the participating relations are completely received), and (ii) produce the remaining results at a fast rate. We develop a new algorithm RPJ (Rate-based Progressive Join) based on solid theoretical analysis. RPJ maximizes the output rate by optimizing its execution according to the characteristics of the join relations (e.g., data distribution, tuple arrival pattern, etc.). Extensive experiments prove that our technique delivers results significantly faster than the previous methods.

#index 810039
#* QPipe: a simultaneously pipelined relational query engine
#@ Stavros Harizopoulos;Vladislav Shkapenyuk;Anastassia Ailamaki
#t 2005
#c 5
#% 4683
#% 13016
#% 36117
#% 152943
#% 172939
#% 172968
#% 204182
#% 248793
#% 286991
#% 300166
#% 300179
#% 333848
#% 397353
#% 411750
#% 442700
#% 442850
#% 479467
#% 480158
#% 481450
#% 481916
#% 617869
#% 765399
#% 765417
#% 830700
#% 993948
#% 1016264
#! Relational DBMS typically execute concurrent queries independently by invoking a set of operator instances for each query. To exploit common data retrievals and computation in concurrent queries, researchers have proposed a wealth of techniques, ranging from buffering disk pages to constructing materialized views and optimizing multiple queries. The ideas proposed, however, are inherently limited by the query-centric philosophy of modern engine designs. Ideally, the query engine should proactively coordinate same-operator execution among concurrent queries, thereby exploiting common accesses to memory and disks as well as common intermediate result computation.This paper introduces on-demand simultaneous pipelining (OSP), a novel query evaluation paradigm for maximizing data and work sharing across concurrent queries at execution time. OSP enables proactive, dynamic operator sharing by pipelining the operator's output simultaneously to multiple parent nodes. This paper also introduces QPipe, a new operator-centric relational engine that effortlessly supports OSP. Each relational operator is encapsulated in a micro-engine serving query tasks from a queue, naturally exploiting all data and work sharing opportunities. Evaluation of QPipe built on top of BerkeleyDB shows that QPipe achieves a 2x speedup over a commercial DBMS when running a workload consisting of TPC-H queries.

#index 810040
#* Proceedings of the 2005 ACM SIGMOD international conference on Management of data
#@ Fatma Ozcan
#t 2005
#c 5

#index 810041
#* Fossilized index: the linchpin of trustworthy non-alterable electronic records
#@ Qingbo Zhu;Windsor W. Hsu
#t 2005
#c 5
#% 3623
#% 10392
#% 164157
#% 287400
#% 295947
#% 318305
#% 397151
#% 463553
#% 480096
#% 571296
#% 596715
#% 978658
#! As critical records are increasingly stored in electronic form, which tends to make for easy destruction and clandestine modification, it is imperative that they be properly managed to preserve their trustworthiness, i.e., their ability to provide irrefutable proof and accurate details of events that have occurred. The need for proper record keeping is further underscored by the recent corporate misconduct and ensuing attempts to destroy incriminating records. Currently, the industry practice and regulatory requirements (e.g., SEC Rule 17a-4) rely on storing records in WORM storage to immutably preserve the records. In this paper, we contend that simply storing records in WORM storage is increasingly inadequate to ensure that they are trustworthy. Specifically, with the large volume of records that are typical today, meeting the ever more stringent query response time requires the use of direct access mechanisms such as indexes. Relying on indexes for accessing records could, however, provide a means for effectively altering or deleting records, even those stored in WORM storage.In this paper, we establish the key requirements for a fossilized index that protects the records from such logical modification. We also analyze current indexing methods to determine how they fall short of these requirements. Based on our insights, we propose the Generalized Hash Tree (GHT). Using both theoretical analysis and simulations with real system data, we demonstrate that the GHT can satisfy the requirements of a fossilized index with performance and cost that are comparable to regular indexing techniques such as the B-tree. We further note that as records are indexed on multiple fields to facilitate search and retrieval, the records can be reconstructed from the corresponding index entries even after the records expire and are disposed of, Therefore, we also present a novel method to eliminate this disclosure risk by allowing an index entry to be effectively disposed of when its record expires.

#index 810042
#* Verifying completeness of relational query results in data publishing
#@ HweeHwa Pang;Arpit Jain;Krithi Ramamritham;Kian-Lee Tan
#t 2005
#c 5
#% 115673
#% 319994
#% 397402
#% 513367
#% 528442
#% 566391
#% 745532
#% 805478
#% 1015281
#% 1015329
#% 1394513
#! In data publishing, the owner delegates the role of satisfying user queries to a third-party publisher. As the publisher may be untrusted or susceptible to attacks, it could produce incorrect query results. In this paper, we introduce a scheme for users to verify that their query results are complete (i.e., no qualifying tuples are omitted) and authentic (i.e., all the result values originated from the owner). The scheme supports range selection on key and non-key attributes, project as well as join queries on relational databases. Moreover, the proposed scheme complies with access control policies, is computationally secure, and can be implemented efficiently.

#index 810043
#* Middleware based data replication providing snapshot isolation
#@ Yi Lin;Bettina Kemme;Marta Patiño-Martínez;Ricardo Jiménez-Peris
#t 2005
#c 5
#% 201869
#% 210179
#% 240016
#% 248825
#% 273894
#% 323980
#% 335454
#% 359934
#% 458546
#% 461887
#% 508197
#% 509531
#% 570890
#% 594328
#% 636006
#% 721142
#% 745516
#% 793894
#% 800544
#% 960202
#% 993994
#% 1015337
#% 1180884
#! Many cluster based replication solutions have been proposed providing scalability and fault-tolerance. Many of these solutions perform replica control in a middleware on top of the database replicas. In such a setting concurrency control is a challenge and is often performed on a table basis. Additionally, some systems put severe requirements on transaction programs (e.g., to declare all objects to be accessed in advance). This paper addresses these issues and presents a middleware-based replication scheme which provides the popular snapshot isolation level at the same tuple-level granularity as database systems like PostgreSQL and Oracle, without any need to declare transaction properties in advance. Both read-only and update transactions can be executed at any replica while providing data consistency at all times. Our approach provides what we call "1-copy-snapshot-isolation" as long as the underlying database replicas provide snapshot isolation. We have implemented our approach as a replicated middleware on top of PostgreSQL replicas. By providing a standard JDBC interface, the middleware is completely transparent to the client program. Fault-tolerance is provided by automatically reconnecting clients in case of crashes. Our middleware shows good performance in terms of response times and scalability.

#index 810044
#* DogmatiX tracks down duplicates in XML
#@ Melanie Weis;Felix Naumann
#t 2005
#c 5
#% 201889
#% 387427
#% 397373
#% 463445
#% 480496
#% 480499
#% 577238
#% 587758
#% 729887
#% 729913
#% 993980
#! Duplicate detection is the problem of detecting different entries in a data source representing the same real-world entity. While research abounds in the realm of duplicate detection in relational data, there is yet little work for duplicates in other, more complex data models, such as XML. In this paper, we present a generalized framework for duplicate detection, dividing the problem into three components: candidate definition defining which objects are to be compared, duplicate definition defining when two duplicate candidates are in fact duplicates, and duplicate detection specifying how to efficiently find those duplicates.Using this framework, we propose an XML duplicate detection method, DogmatiX, which compares XML elements based not only on their direct data values, but also on the similarity of their parents, children, structure, etc. We propose heuristics to determine which of these to choose, as well as a similarity measure specifically geared towards the XML data model. An evaluation of our algorithm using several heuristics validates our approach.

#index 810045
#* Incremental maintenance of path-expression views
#@ Arsany Sawires;Junichi Tatemura;Oliver Po;Divyakant Agrawal;K. SelÇuk Candan
#t 2005
#c 5
#% 13016
#% 152928
#% 201929
#% 378412
#% 462213
#% 479629
#% 504578
#% 504585
#% 654450
#% 729896
#% 765488
#% 994015
#% 1016134
#! Caching data by maintaining materialized views typically requires updating the cache appropriately to reflect dynamic source updates. Extensive research has addressed the problem of incremental view maintenance for relational data but only few works have addressed it for semi-structured data. In this paper we address the problem of incremental maintenance of views defined over XML documents using path-expressions. The approach described in this paper has the following main features that distinguish it from the previous works: (1) The view specification language is powerful and standardized enough to be used in realistic applications. (2) The size of the auxiliary data maintained with the views depends on the expression size and the answer size regardless of the source data size.(3) No source schema is assumed to exist; the source data can be any general well-formed XML document. Experimental evaluation is conducted to assess the performance benefits of the proposed approach.

#index 810046
#* On boosting holism in XML twig pattern matching using structural indexing techniques
#@ Ting Chen;Jiaheng Lu;Tok Wang Ling
#t 2005
#c 5
#% 333981
#% 397360
#% 397375
#% 479465
#% 654452
#% 659999
#% 660000
#% 745468
#% 765405
#% 765406
#% 765466
#% 783547
#% 1015277
#! Searching for all occurrences of a twig pattern in an XML document is an important operation in XML query processing. Recently a holistic method TwigStack. [2] has been proposed. The method avoids generating large intermediate results which do not contribute to the final answer and is CPU and I/O optimal when twig patterns only have ancestor-descendant relationships. Another important direction of XML query processing is to build structural indexes [3][8][13][15] over XML documents to avoid unnecessary scanning of source documents. We regard XML structural indexing as a technique to partition XML documents and call it streaming scheme in our paper. In this paper we develop a method to perform holistic twig pattern matching on XML documents partitioned using various streaming schemes. Our method avoids unnecessary scanning of irrelevant portion of XML documents. More importantly, depending on different streaming schemes used, it can process a large class of twig patterns consisting of both ancestor-descendant and parent-child relationships and avoid generating redundant intermediate results. Our experiments demonstrate the applicability and the performance advantages of our approach.

#index 810047
#* CURLER: finding and visualizing nonlinear correlation clusters
#@ Anthony K. H. Tung;Xin Xu;Beng Chin Ooi
#t 2005
#c 5
#% 248792
#% 271870
#% 273890
#% 273891
#% 280417
#% 300131
#% 316709
#% 397384
#% 462243
#% 465004
#% 479962
#% 765439
#! While much work has been done in finding linear correlation among subsets of features in high-dimensional data, work on detecting nonlinear correlation has been left largely untouched. In this paper, we present an algorithm for finding and visualizing nonlinear correlation clusters in the subspace of high-dimensional databases.Unlike the detection of linear correlation in which clusters are of unique orientations, finding nonlinear correlation clusters of varying orientations requires merging clusters of possibly very different orientations. Combined with the fact that spatial proximity must be judged based on a subset of features that are not originally known, deciding which clusters to be merged during the clustering process becomes a challenge. To avoid this problem, we propose a novel concept called co-sharing level which captures both spatial proximity and cluster orientation when judging similarity between clusters. Based on this concept, we develop an algorithm which not only detects nonlinear correlation clusters but also provides a way to visualize them. Experiments on both synthetic and real-life datasets are done to show the effectiveness of our method.

#index 810048
#* A generic framework for monitoring continuous spatial queries over moving objects
#@ Haibo Hu;Jianliang Xu;Dik Lun Lee
#t 2005
#c 5
#% 86950
#% 201876
#% 259642
#% 287466
#% 300174
#% 300179
#% 427199
#% 428155
#% 442615
#% 452871
#% 495433
#% 574283
#% 654478
#% 720832
#% 736290
#% 765452
#% 765453
#% 765454
#% 800571
#% 800572
#% 805466
#% 993955
#% 1015297
#% 1015305
#% 1015320
#% 1016193
#% 1016198
#! This paper proposes a generic framework for monitoring continuous spatial queries over moving objects. The framework distinguishes itself from existing work by being the first to address the location update issue and to provide a common interface for monitoring mixed types of queries. Based on the notion of safe region, the client location update strategy is developed based on the queries being monitored. Thus, it significantly reduces the wireless communication and query reevaluation costs required to maintain the up-to-date query results. We propose algorithms for query evaluation/reevaluation and for safe region computation in this framework. Enhancements are also proposed to take advantage of two practical mobility assumptions: maximum speed and steady movement. The experimental results show that our framework substantially outperforms the traditional periodic monitoring scheme in terms of monitoring accuracy and CPU time while achieving a close-to-optimal wireless communication cost. The framework also can scale up to a large monitoring system and is robust under various object mobility patterns.

#index 810049
#* Robust and fast similarity search for moving object trajectories
#@ Lei Chen;M. Tamer Özsu;Vincent Oria
#t 2005
#c 5
#% 36672
#% 227924
#% 240182
#% 310545
#% 316780
#% 333941
#% 379449
#% 443432
#% 443698
#% 451645
#% 453529
#% 460862
#% 462231
#% 464851
#% 479649
#% 480146
#% 480473
#% 480654
#% 534183
#% 546257
#% 555056
#% 577221
#% 631923
#% 632042
#% 659971
#% 729931
#% 765451
#% 769961
#% 842688
#% 993965
#% 1016195
#! An important consideration in similarity-based retrieval of moving object trajectories is the definition of a distance function. The existing distance functions are usually sensitive to noise, shifts and scaling of data that commonly occur due to sensor failures, errors in detection techniques, disturbance signals, and different sampling rates. Cleaning data to eliminate these is not always possible. In this paper, we introduce a novel distance function, Edit Distance on Real sequence (EDR) which is robust against these data imperfections. Analysis and comparison of EDR with other popular distance functions, such as Euclidean distance, Dynamic Time Warping (DTW), Edit distance with Real Penalty (ERP), and Longest Common Subsequences (LCSS), indicate that EDR is more robust than Euclidean distance, DTW and ERP, and it is on average 50% more accurate than LCSS. We also develop three pruning techniques to improve the retrieval efficiency of EDR and show that these techniques can be combined effectively in a search, increasing the pruning power significantly. The experimental results confirm the superior efficiency of the combined methods.

#index 810050
#* Extending XQuery for analytics
#@ Kevin Beyer;Don Chambérlin;Latha S. Colby;Fatma Özcan;Hamid Pirahesh;Yu Xu
#t 2005
#c 5
#% 273916
#% 287005
#% 413563
#% 461897
#% 464215
#% 487267
#% 562135
#% 665635
#% 810036
#% 1016143
#! XQuery is a query language under development by the W3C XML Query Working Group. The language contains constructs for navigating, searching, and restructuring XML data. With XML gaining importance as the standard for representing business data, XQuery must support the types of queries that are common in business analytics. One such class of queries is OLAP-style aggregation queries. Although these queries are expressible in XQuery Version 1, the lack of explicit grouping constructs makes the construction of these queries non-intuitive and places a burden on the XQuery engine to recognize and optimize the implicit grouping constructs. Furthermore, although the flexibility of the XML data model provides an opportunity for advanced forms of grouping that are not easily represented in relational systems, these queries are difficult to express using the current XQuery syntax. In this paper, we provide a proposal for extending the XQuery FLWOR expression with explicit syntax for grouping and for numbering of results. We show that these new XQuery constructs not only simplify the construction and evaluation of queries requiring grouping and ranking but also enable complex analytic queries such as moving-window aggregation and rollups along dynamic hierarchies to be expressed without additional language extensions.

#index 810051
#* Lazy XML updates: laziness as a virtue, of update and structural join efficiency
#@ Barbara Catania;Beng Chin Ooi;Wenqiang Wang;Xiaoling Wang
#t 2005
#c 5
#% 333979
#% 333981
#% 378412
#% 397366
#% 397375
#% 480489
#% 659999
#% 745479
#% 765442
#% 765488
#% 800523
#% 993951
#% 993953
#! XML documents are normally stored as plain text files. Hence, the natural and most convenient way to update XML documents is to simply edit the text files. But efficient query evaluation algorithms require XML documents to be indexed. Every element is given a unique identifier based on its location in the document or its preorder-traversal order, and this identifier is later used as (part of) the key in the index. Reassigning orders of possibly a large number of elements is therefore necessary when the original XML documents are updated. Immutable dynamic labeling schemes have been proposed to solve this problem, that, however, require very long labels and may decrease query performance. If we consider a real-world scenario, we note that many relatively small ad-hoc XML segments are inserted/deleted into/from an existing XML database. In this paper, we start from this consideration and we propose a new lazy approach to handle XML updates that also improves query performance. The lazy approach: (i) completely avoids reassigning existing element orders after updates; (ii) improves query processing by taking advantages from segments. Experimental results show that our approach is much more efficient in handling updates than using immutable labeling and, at the same time, it also improves the performance of recently defined structural join algorithms.

#index 810052
#* Efficient keyword search for smallest LCAs in XML databases
#@ Yu Xu;Yannis Papakonstantinou
#t 2005
#c 5
#% 47710
#% 163084
#% 268079
#% 309726
#% 340914
#% 387508
#% 397366
#% 458829
#% 458861
#% 465018
#% 465155
#% 479803
#% 480489
#% 504581
#% 654442
#% 660011
#% 993987
#% 1015258
#% 1016135
#! Keyword search is a proven, user-friendly way to query HTML documents in the World Wide Web. We propose keyword search in XML documents, modeled as labeled trees, and describe corresponding efficient algorithms. The proposed keyword search returns the set of smallest trees containing all keywords, where a tree is designated as "smallest" if it contains no tree that also contains all keywords. Our core contribution, the Indexed Lookup Eager algorithm, exploits key properties of smallest trees in order to outperform prior algorithms by orders of magnitude when the query contains keywords with significantly different frequencies. The Scan Eager variant is tuned for the case where the keywords have similar frequencies. We analytically and experimentally evaluate two variants of the Eager algorithm, along with the Stack algorithm [13]. We also present the XKSearch system, which utilizes the Indexed Lookup Eager, Scan Eager and Stack algorithms and a demo of which on DBLP data is available at http://www.db.ucsd.edu/projects/xksearch. Finally, we extend the Indexed Lookup Eager algorithm to answer Lowest Common Ancestor (LCA) queries.

#index 810053
#* A verifier for interactive, data-driven web applications
#@ Alin Deutsch;Monica Marcus;Liying Sui;Victor Vianu;Dayou Zhou
#t 2005
#c 5
#% 114677
#% 169697
#% 185412
#% 190683
#% 248013
#% 321054
#% 343051
#% 348131
#% 384978
#% 425200
#% 479031
#% 519432
#% 542846
#% 543675
#% 565496
#% 571038
#% 576091
#% 630964
#% 754120
#% 765514
#% 801675
#% 1086662
#% 1599270
#! We present WAVE, a verifier for interactive, database-driven Web applications specified using high-level modeling tools such as WebML. WAVE is complete for a broad class of applications and temporal properties. For other applications, WAVE can be used as an incomplete verifier, as commonly done in software verification. Our experiments on four representative data-driven applications and a battery of common properties yielded surprisingly good verification times, on the order of seconds. This suggests that interactive applications controlled by database queries may be unusually well suited to automatic verification. They also show that the coupling of model checking with database optimization techniques used in the implementation of WAVE can be extremely effective. This is significant both to the database area and to automatic verification in general.

#index 810054
#* Page quality: in search of an unbiased web ranking
#@ Junghoo Cho;Sourashis Roy;Robert E. Adams
#t 2005
#c 5
#% 69328
#% 115474
#% 120104
#% 208931
#% 290830
#% 309749
#% 348173
#% 406493
#% 479803
#% 503228
#% 577328
#% 577330
#% 577337
#% 577338
#% 656794
#% 754060
#% 840583
#% 1016164
#% 1016175
#% 1016176
#! In a number of recent studies [4, 8] researchers have found that because search engines repeatedly return currently popular pages at the top of search results, popular pages tend to get even more popular, while unpopular pages get ignored by an average user. This "rich-get-richer" phenomenon is particularly problematic for new and high-quality pages because they may never get a chance to get users' attention, decreasing the overall quality of search results in the long run. In this paper, we propose a new ranking function, called page quality that can alleviate the problem of popularity-based ranking. We first present a formal framework to study the search engine bias by discussing what is an "ideal" way to measure the intrinsic quality of a page. We then compare how PageRank, the current ranking metric used by major search engines, differs from this ideal quality metric. This framework will help us investigate the search engine bias in more concrete terms and provide clear understanding why PageRank is effective in many cases and exactly when it is problematic. We then propose a practical way to estimate the intrinsic page quality to avoid the inherent bias of PageRank. We derive our proposed quality estimator through a careful analysis of a reasonable web user model, and we present experimental results that show the potential of our proposed estimator. We believe that our quality estimator has the potential to alleviate the rich-get-richer phenomenon and help new and high-quality pages get the attention that they deserve.

#index 810055
#* A disk-based join with probabilistic guarantees
#@ Christopher Jermaine;Alin Dobra;Subramanian Arumugam;Shantanu Joshi;Abhijit Pol
#t 2005
#c 5
#% 3771
#% 58348
#% 77967
#% 86955
#% 210188
#% 210353
#% 227883
#% 273908
#% 273910
#% 277347
#% 397370
#% 438135
#% 503719
#% 576104
#% 659919
#% 993956
#! One of the most common operations in analytic query processing is the application of an aggregate function to the result of a relational join. We describe an algorithm for computing the answer to such a query over large, disk-based input tables. The key innovation of our algorithm is that at all times, it provides an online, statistical estimator for the eventual answer to the query, as well as probabilistic confidence bounds. Thus, a user can monitor the progress of the join throughout its execution and stop the join when satisfied with the estimate's accuracy, or run the algorithm to completion with a total time requirement that is not much longer than other common join algorithms. This contrasts with other online join algorithms, which either do not offer such statistical guarantees or can only offer guarantees so long as the input data can fit into core memory.

#index 810056
#* When can we trust progress estimators for SQL queries?
#@ Surajit Chaudhuri;Raghav Kaushik;Ravishankar Ramamurthy
#t 2005
#c 5
#% 102784
#% 201921
#% 210353
#% 211087
#% 227883
#% 248793
#% 273908
#% 273909
#% 273910
#% 503719
#% 765456
#% 765467
#% 765468
#% 800589
#! The problem of estimating progress for long-running queries has recently been introduced. We analyze the characteristics of the progress estimation problem, from the perspective of providing robust, worst-case guarantees. Our first result is that in the worst case, no progress estimation algorithm can yield anything even moderately better than the trivial guarantee that identifies the progress as lying between 0% and 100%. In such cases, we introduce an estimator that can optimally bound the error. However, we show that in many "good" scenarios, it is possible to design effective progress estimators with small error bounds. We then demonstrate empirically that these "good" scenarios are common in practice and discuss possible ways of combining the estimators.

#index 810057
#* Relational confidence bounds are easy with the bootstrap
#@ Abhijit Pol;Christopher Jermaine
#t 2005
#c 5
#% 82346
#% 102316
#% 137885
#% 210353
#% 227883
#% 243299
#% 273910
#% 274152
#% 299989
#% 481749
#% 496116
#% 503719
#% 527167
#! Statistical estimation and approximate query processing have become increasingly prevalent applications for database systems. However, approximation is usually of little use without some sort of guarantee on estimation accuracy, or "confidence bound." Analytically deriving probabilistic guarantees for database queries over sampled data is a daunting task, not suitable for the faint of heart, and certainly beyond the expertise of the typical database system end-user. This paper considers the problem of incorporating into a database system a powerful "plug-in" method for computing confidence bounds on the answer to relational database queries over sampled or incomplete data. This statistical tool, called the bootstrap, is simple enough that it can be used by a data-base programmer with a rudimentary mathematical background, but general enough that it can be applied to almost any statistical inference problem. Given the power and ease-of-use of the bootstrap, we argue that the algorithms presented for supporting the bootstrap should be incorporated into any database system which is intended to support analytic processing.

#index 810058
#* BRAID: stream mining through group lag correlations
#@ Yasushi Sakurai;Spiros Papadimitriou;Christos Faloutsos
#t 2005
#c 5
#% 172949
#% 310500
#% 316709
#% 333941
#% 342600
#% 345857
#% 347200
#% 394984
#% 397353
#% 397354
#% 408522
#% 460862
#% 480628
#% 578388
#% 654444
#% 654462
#% 654463
#% 654497
#% 726621
#% 729943
#% 993961
#% 993965
#% 1015280
#% 1015301
#% 1015324
#% 1016153
#% 1016158
#% 1016196
#! The goal is to monitor multiple numerical streams, and determine which pairs are correlated with lags, as well as the value of each such lag. Lag correlations (and anti-correlations) are frequent, and very interesting in practice: For example, a decrease in interest rates typically precedes an increase in house sales by a few months; higher amounts of fluoride in the drinking water may lead to fewer dental cavities, some years later. Additional settings include network analysis, sensor monitoring, financial data analysis, and moving object tracking. Such data streams are often correlated (or anti-correlated), but with an unknown lag.We propose BRAID, a method to detect lag correlations between data streams. BRAID can handle data streams of semi-infinite length, incrementally, quickly, and with small resource consumption. We also provide a theoretical analysis, which, based on Nyquist's sampling theorem, shows that BRAID can estimate lag correlations with little, and often with no error at all. Our experiments on real and realistic data show that BRAID detects the correct lag perfectly most of the time (the largest relative error was about 1%); while it is up to 40,000 times faster than the naive implementation.

#index 810059
#* Fast and approximate stream mining of quantiles and frequencies using graphics processors
#@ Naga K. Govindaraju;Nikunj Raghuvanshi;Dinesh Manocha
#t 2005
#c 5
#% 36354
#% 66928
#% 210185
#% 248820
#% 248821
#% 273907
#% 282235
#% 333931
#% 356652
#% 379445
#% 397361
#% 397385
#% 453512
#% 492912
#% 548479
#% 569754
#% 576119
#% 578392
#% 629126
#% 654479
#% 660003
#% 730046
#% 745533
#% 749495
#% 765291
#% 765419
#% 771088
#% 785130
#% 799972
#% 801695
#% 801696
#% 803594
#% 993960
#% 1016214
#! We present algorithms for fast quantile and frequency estimation in large data streams using graphics processors (GPUs). We exploit the high computation power and memory bandwidth of graphics processors and present a new sorting algorithm that performs rasterization operations on the GPUs. We use sorting as the main computational component for histogram approximation and construction of ε-approximate quantile and frequency summaries. Our algorithms for numerical statistics computation on data streams are deterministic, applicable to fixed or variable-sized sliding windows and use a limited memory footprint. We use GPU as a co-processor and minimize the data transmission between the CPU and GPU by taking into account the low bus bandwidth. We implemented our algorithms on a PC with a NVIDIA GeForce FX 6800 Ultra GPU and a 3.4 GHz Pentium IV CPU and applied them to large data streams consisting of more than 100 million values. We also compared the performance of our GPU-based algorithms with optimized implementations of prior CPU-based algorithms. Overall, our results demonstrate that the graphics processors available on a commodity computer system are efficient stream-processor and useful co-processors for mining data streams.

#index 810060
#* Mining periodic patterns with gap requirement from sequences
#@ Minghua Zhang;Ben Kao;David W. Cheung;Kevin Y. Yip
#t 2005
#c 5
#% 152934
#% 259993
#% 310542
#% 420063
#% 459006
#% 464996
#% 469570
#% 631926
#! We study a problem of mining frequently occurring periodic patterns with a gap requirement from sequences. Given a character sequence S of length L and a pattern P of length l, we consider P a frequently occurring pattern in S if the probability of observing P given a randomly picked length-l subsequence of S exceeds a certain threshold. In many applications, particularly those related to bioinformatics, interesting patterns are periodic with a gap requirement. That is to say, the characters in P should match subsequences of S in such a way that the matching characters in S are separated by gaps of more or less the same size. We show the complexity of the mining problem and discuss why traditional mining algorithms are computationally infeasible. We propose practical algorithms for solving the problem, and study their characteristics. We also present a case study in which we apply our algorithms on some DNA sequences. We discuss some interesting patterns obtained from the case study.

#index 810061
#* Conceptual partitioning: an efficient method for continuous nearest neighbor monitoring
#@ Kyriakos Mouratidis;Dimitris Papadias;Marios Hadjieleftheriou
#t 2005
#c 5
#% 201876
#% 287466
#% 421124
#% 442615
#% 480661
#% 495433
#% 579313
#% 654478
#% 765453
#% 993999
#% 1016196
#! Given a set of objects P and a query point q, a k nearest neighbor (k-NN) query retrieves the k objects in P that lie closest to q. Even though the problem is well-studied for static datasets, the traditional methods do not extend to highly dynamic environments where multiple continuous queries require real-time results, and both objects and queries receive frequent location updates. In this paper we propose conceptual partitioning (CPM), a comprehensive technique for the efficient monitoring of continuous NN queries. CPM achieves low running time by handling location updates only from objects that fall in the vicinity of some query (and ignoring the rest). It can be used with multiple, static or moving queries, and it does not make any assumptions about the object moving patterns. We analyze the performance of CPM and show that it outperforms the current state-of-the-art algorithms for all problem settings. Finally, we extend our framework to aggregate NN (ANN) queries, which monitor the data objects that minimize the aggregate distance with respect to a set of query points (e.g., the objects with the minimum sum of distances to all query points).

#index 810062
#* Predicate result range caching for continuous queries
#@ Matthew Denny;Michael J. Franklin
#t 2005
#c 5
#% 136740
#% 152940
#% 172931
#% 210206
#% 381563
#% 479636
#% 481915
#% 654487
#% 654488
#% 836002
#% 993949
#! Many analysis and monitoring applications require the repeated execution of expensive modeling functions over streams of rapidly changing data. These applications can often be expressed declaratively, but the continuous query processors developed to date are not designed to optimize queries with expensive functions. To speed up such queries, we present CASPER: the CAching System for PrEdicate Result ranges. CASPER computes and caches predicate result ranges, which are ranges of stream input values where the system knows the results of expensive predicate evaluations. Over time, CASPER expands ranges so that they are more likely to contain future stream values. This paper presents the CASPER architecture, as well as algorithms for computing and expanding ranges for a large class of predicates. We demonstrate the effectiveness of CASPER using a prototype implementation and a financial application using real bond market data.

#index 810063
#* Update-pattern-aware modeling and processing of continuous queries
#@ Lukasz Golab;M. Tamer Özsu
#t 2005
#c 5
#% 54971
#% 116082
#% 188026
#% 578391
#% 726621
#% 742565
#% 745434
#% 765435
#% 765436
#% 771230
#% 788215
#% 801694
#% 878299
#% 1015278
#% 1015296
#% 1015324
#% 1016170
#! A defining characteristic of continuous queries over on-line data streams, possibly bounded by sliding windows, is the potentially infinite and time-evolving nature of their inputs and outputs. New items continually arrive on the input streams and new results are continually produced. Additionally, inputs expire by falling out of range of their sliding windows and results expire when they cease to satisfy the query. This impacts continuous query processing in two ways. First, data stream systems allow tables to be queried alongside data streams, but in terms of query semantics, it is not clear how updates of tables are different from insertions and deletions caused by the movement of the sliding windows. Second, many interesting queries need to store state, which must be kept up-to-date as time goes on. Therefore, query processing efficiency depends highly on the amount of overhead involved in state maintenance.In this paper, we show that the above issues can be solved by understanding the update patterns of continuous queries and exploiting them during query processing. We propose a classification that defines four types of update characteristics. Using our classification, we present a definition of continuous query semantics that clearly states the role of relations. We then propose the notion of update-pattern-aware query processing, where physical implementations of query operators, including the data structures used for storing intermediate state, vary depending on the update patterns of their inputs and outputs. When tested on IP traffic logs, our update-pattern-aware query plans routinely outperform the existing techniques by an order of magnitude.

#index 810064
#* Mining top-K covering rule groups for gene expression data
#@ Gao Cong;Kian-Lee Tan;Anthony K. H. Tung;Xin Xu
#t 2005
#c 5
#% 248785
#% 269217
#% 280433
#% 280436
#% 300120
#% 338580
#% 341682
#% 462234
#% 481290
#% 662759
#% 727908
#% 729933
#% 729984
#% 765413
#% 769919
#% 1499573
#% 1716939
#! In this paper, we propose a novel algorithm to discover the top-k covering rule groups for each row of gene expression profiles. Several experiments on real bioinformatics datasets show that the new top-k covering rule mining algorithm is orders of magnitude faster than previous association rule mining algorithms.Furthermore, we propose a new classification method RCBT. RCBT classifier is constructed from the top-k covering rule groups. The rule groups generated for building RCBT are bounded in number. This is in contrast to existing rule-based classification methods like CBA [19] which despite generating excessive number of redundant rules, is still unable to cover some training data with the discovered rules. Experiments show that the RCBT classifier can match or outperform other state-of-the-art classifiers on several benchmark gene expression datasets. In addition, the top-k covering rule groups themselves provide insights into the mechanisms responsible for diseases directly.

#index 810065
#* Subsequence matching on structured time series data
#@ Huanmei Wu;Betty Salzberg;Gregory C Sharp;Steve B Jiang;Hiroki Shirato;David Kaeli
#t 2005
#c 5
#% 172949
#% 227857
#% 227924
#% 330932
#% 333926
#% 333941
#% 379445
#% 397380
#% 397381
#% 413606
#% 428155
#% 460862
#% 462231
#% 477479
#% 480628
#% 578560
#% 765403
#% 993961
#! Subsequence matching in time series databases is a useful technique, with applications in pattern matching, prediction, and rule discovery. Internal structure within the time series data can be used to improve these tasks, and provide important insight into the problem domain. This paper introduces our research effort in using the internal structure of a time series directly in the matching process. This idea is applied to the problem domain of respiratory motion data in cancer radiation treatment. We propose a comprehensive solution for analysis, clustering, and online prediction of respiratory motion using subsequence similarity matching. In this system, a motion signal is captured in real time as a data stream, and is analyzed immediately for treatment and also saved in a database for future study. A piecewise linear representation of the signal is generated from a finite state model, and is used as a query for subsequence matching. To ensure that the query subsequence is representative, we introduce the concept of subsequence stability, which can be used to dynamically adjust the query subsequence length. To satisfy the special needs of similarity matching over breathing patterns, a new subsequence similarity measure is introduced. This new measure uses a weighted L1 distance function to capture the relative importance of each source stream, amplitude, frequency, and proximity in time. From the subsequence similarity measure, stream and patient similarity can be defined, which are then used for offline and online applications. The matching results are analyzed and applied for motion prediction and correlation discovery. While our system has been customized for use in radiation therapy, our approach to time series modeling is general enough for application domains with structured time series data.

#index 810066
#* TRICLUSTER: an effective algorithm for mining coherent clusters in 3D microarray data
#@ Lizhuang Zhao;Mohammed J. Zaki
#t 2005
#c 5
#% 248792
#% 269534
#% 269568
#% 273891
#% 300131
#% 328332
#% 397382
#% 397384
#% 397632
#% 469422
#% 469425
#% 589373
#% 659967
#% 727882
#% 737250
#% 762824
#% 769919
#% 778215
#% 832892
#! In this paper we introduce a novel algorithm called TRICLUSTER, for mining coherent clusters in three-dimensional (3D) gene expression datasets. TRICLUSTER can mine arbitrarily positioned and overlapping clusters, and depending on different parameter values, it can mine different types of clusters, including those with constant or similar values along each dimension, as well as scaling and shifting expression patterns. TRICLUSTER relies on graph-based approach to mine all valid clusters. For each time slice, i.e., a gene×sample matrix, it constructs the range multigraph, a compact representation of all similar value ranges between any two sample columns. It then searches for constrained maximal cliques in this multigraph to yield the set of bi-clusters for this time slice. Then TRICLUSTER constructs another graph using the biclusters (as vertices) from each time slice; mining cliques from this graph yields the final set of triclusters. Optionally, TRICLUSTER merges/deletes some clusters having large overlaps. We present a useful set of metrics to evaluate the clustering quality, and we show that TRICLUSTER can find significant triclusters in the real microarray datasets.

#index 810067
#* Query-sensitive embeddings
#@ Vassilis Athitsos;Marios Hadjieleftheriou;George Kollios;Stan Sclaroff
#t 2005
#c 5
#% 201893
#% 209623
#% 248798
#% 281750
#% 294634
#% 302391
#% 316526
#% 332094
#% 342828
#% 443517
#% 443975
#% 444007
#% 451653
#% 462231
#% 479649
#% 479973
#% 480133
#% 480307
#% 571079
#% 578407
#% 650248
#% 712783
#% 729931
#% 745496
#% 993965
#% 1502433
#! A common problem in many types of databases is retrieving the most similar matches to a query object. Finding those matches in a large database can be too slow to be practical, especially in domains where objects are compared using computationally expensive similarity (or distance) measures. This paper proposes a novel method for approximate nearest neighbor retrieval in such spaces. Our method is embedding-based, meaning that it constructs a function that maps objects into a real vector space. The mapping preserves a large amount of the proximity structure of the original space, and it can be used to rapidly obtain a short list of likely matches to the query. The main novelty of our method is that it constructs, together with the embedding, a query-sensitive distance measure that should be used when measuring distances in the vector space. The term "query-sensitive" means that the distance measure changes depending on the current query object. We report experiments with an image database of handwritten digits, and a time-series database. In both cases, the proposed method outperforms existing state-of-the-art embedding methods, meaning that it provides significantly better trade-offs between efficiency and retrieval accuracy.

#index 810068
#* STRG-Index: spatio-temporal region graph indexing for large video databases
#@ JeongKyu Lee;JungHwan Oh;Sae Hwang
#t 2005
#c 5
#% 70370
#% 327927
#% 334561
#% 342827
#% 349208
#% 413620
#% 466425
#% 479462
#% 487887
#% 529841
#% 769952
#% 780150
#% 899497
#% 1016195
#% 1180064
#% 1857581
#! In this paper, we propose new graph-based data structure and indexing to organize and retrieve video data. Several researches have shown that a graph can be a better candidate for modeling semantically rich and complicated multimedia data. However, there are few methods that consider the temporal feature of video data, which is a distinguishable and representative characteristic when compared with other multimedia (i.e., images). In order to consider the temporal feature effectively and efficiently, we propose a new graph-based data structure called Spatio-Temporal Region Graph (STRG). Unlike existing graph-based data structures which provide only spatial features, the proposed STRG further provides temporal features, which represent temporal relationships among spatial objects. The STRG is decomposed into its subgraphs in which redundant subgraphs are eliminated to reduce the index size and search time, because the computational complexity of graph matching (subgraph isomorphism) is NP-complete. In addition, a new distance measure, called Extended Graph Edit Distance (EGED), is introduced in both non-metric and metric spaces for matching and indexing respectively. Based on STRG and EGED, we propose a new indexing method STRG-Index, which is faster and more accurate since it uses tree structure and clustering algorithm. We compare the STRG-Index with the M-tree, which is a popular tree-based indexing method for multimedia data. The STRG-Index outperforms the M-tree for various query loads in terms of cost and speed.

#index 810069
#* Towards effective indexing for very large video sequence database
#@ Heng Tao Shen;Beng Chin Ooi;Xiaofang Zhou
#t 2005
#c 5
#% 248796
#% 342657
#% 342828
#% 480307
#% 480632
#% 1016193
#% 1857639
#% 1858015
#! With rapid advances in video processing technologies and ever fast increments in network bandwidth, the popularity of video content publishing and sharing has made similarity search an indispensable operation to retrieve videos of user interests. The video similarity is usually measured by the percentage of similar frames shared by two video sequences, and each frame is typically represented as a high-dimensional feature vector. Unfortunately, high complexity of video content has posed the following major challenges for fast retrieval: (a) effective and compact video representations, (b) efficient similarity measurements, and (c) efficient indexing on the compact representations. In this paper, we propose a number of methods to achieve fast similarity search for very large video database. First, each video sequence is summarized into a small number of clusters, each of which contains similar frames and is represented by a novel compact model called Video Triplet (ViTri). ViTri models a cluster as a tightly bounded hypersphere described by its position, radius, and density. The ViTri similarity is measured by the volume of intersection between two hyperspheres multiplying the minimal density, i.e., the estimated number of similar frames shared by two clusters. The total number of similar frames is then estimated to derive the overall similarity between two video sequences. Hence the time complexity of video similarity measure can be reduced greatly. To further reduce the number of similarity computations on ViTris, we introduce a new one dimensional transformation technique which rotates and shifts the original axis system using PCA in such a way that the original inter-distance between two high-dimensional vectors can be maximally retained after mapping. An efficient B+-tree is then built on the transformed one dimensional values of ViTris' positions. Such a transformation enables B+-tree to achieve its optimal performance by quickly filtering a large portion of non-similar ViTris. Our extensive experiments on real large video datasets prove the effectiveness of our proposals that outperform existing methods significantly.

#index 810070
#* Cost-sensitive reordering of navigational primitives
#@ Carl-Christian Kanne;Matthias Brantner;Guido Moerkotte
#t 2005
#c 5
#% 83148
#% 102761
#% 121873
#% 136740
#% 273922
#% 397358
#% 397375
#% 479956
#% 481417
#% 481431
#% 487257
#% 570876
#% 659999
#% 765488
#% 800521
#% 800577
#% 993939
#% 994015
#% 1015275
#! We present a method to evaluate path queries based on the novel concept of partial path instances. Our method (1) maximizes performance by means of sequential scans or asynchronous I/O, (2) does not require a special storage format, (3) relies on simple navigational primitives on trees, and (4) can be complemented by existing logical and physical optimizations such as duplicate elimination, duplicate prevention and path rewriting.We use a physical algebra which separates those navigation operations that require I/O from those that do not. All I/O operations necessary for the evaluation of a path are isolated in a single operator, which may employ efficient I/O scheduling strategies such as sequential scans or asynchronous I/O.Performance results for queries from the XMark benchmark show that reordering the navigation operations can increase performance up to a factor of four.

#index 810071
#* Similarity evaluation on tree-structured data
#@ Rui Yang;Panos Kalnis;Anthony K. H. Tung
#t 2005
#c 5
#% 66654
#% 121278
#% 234905
#% 248797
#% 273920
#% 342827
#% 397373
#% 427199
#% 479649
#% 480133
#% 480654
#% 481216
#% 546123
#% 547438
#% 572294
#% 576105
#% 577218
#! Tree-structured data are becoming ubiquitous nowadays and manipulating them based on similarity is essential for many applications. The generally accepted similarity measure for trees is the edit distance. Although similarity search has been extensively studied, searching for similar trees is still an open problem due to the high complexity of computing the tree edit distance. In this paper, we propose to transform tree-structured data into an approximate numerical multidimensional vector which encodes the original structure information. We prove that the L1 distance of the corresponding vectors, whose computational complexity is O(|T1| + |T2|), forms a lower bound for the edit distance between trees. Based on the theoretical analysis, we describe a novel algorithm which embeds the proposed distance into a filter-and-refine framework to process similarity search on tree-structured data. The experimental results show that our algorithm reduces dramatically the distance computation cost. Our method is especially suitable for accelerating similarity query processing on large trees in massive datasets.

#index 810072
#* Substructure similarity search in graph databases
#@ Xifeng Yan;Philip S. Yu;Jiawei Han
#t 2005
#c 5
#% 25470
#% 121278
#% 217812
#% 251403
#% 256685
#% 260974
#% 333679
#% 344549
#% 378391
#% 408396
#% 442886
#% 443133
#% 466644
#% 765429
#% 1015336
#! Advanced database systems face a great challenge raised by the emergence of massive, complex structural data in bioinformatics, chem-informatics, and many other applications. The most fundamental support needed in these applications is the efficient search of complex structured data. Since exact matching is often too restrictive, similarity search of complex structures becomes a vital operation that must be supported efficiently.In this paper, we investigate the issues of substructure similarity search using indexed features in graph databases. By transforming the edge relaxation ratio of a query graph into the maximum allowed missing features, our structural filtering algorithm, called Grafil, can filter many graphs without performing pairwise similarity computations. It is further shown that using either too few or too many features can result in poor filtering performance. Thus the challenge is to design an effective feature set selection strategy for filtering. By examining the effect of different feature selection mechanisms, we develop a multi-filter composition strategy, where each filter uses a distinct and complementary subset of the features. We identify the criteria to form effective feature sets for filtering, and demonstrate that combining features with similar size and selectivity can improve the filtering and search performance significantly. Moreover, the concept presented in Grafil can be applied to searching approximate non-consecutive sequences, trees, and other complicated structures as well.

#index 810073
#* Enterprise information integration: successes, challenges and controversies
#@ Alon Y. Halevy;Naveen Ashish;Dina Bitton;Michael Carey;Denise Draper;Jeff Pollock;Arnon Rosenthal;Vishal Sikka
#t 2005
#c 5
#% 286916
#% 345758
#% 479452
#% 790849
#! The goal of EII systems is to provide uniform access to multiple data sources without having to first load them into a data warehouse. Since the late 1990's, several EII products have appeared in the marketplace and significant experience has been accumulated from fielding such systems. This collection of articles, by individuals who were involved in this industry in various ways, describes some of these experiences and points to the challenges ahead.

#index 810074
#* Lean middleware
#@ David A. Maluf;David G. Bell;Naveen Ashish
#t 2005
#c 5
#% 314020
#% 428146
#% 464852
#% 570875
#% 810073
#! This paper describes an approach to achieving data integration across multiple sources in an enterprise, in a manner that does not require heavy investment in database and middleware maintenance. This "lean" approach to integration leads to cost-effectiveness and scalability of data integration in the enterprise.

#index 810075
#* The many roles of meta data in data integration
#@ Philip A. Bernstein
#t 2005
#c 5
#% 810076
#% 810077
#% 810078
#! This paper is a short introduction to an industrial session on the use of meta data to address data integration problems in large enterprises. The main topics are data discovery, version and configuration management, and mapping development.

#index 810076
#* Information intelligence: metadata for information discovery, access, and integration
#@ Randall Hauch;Alex Miller;Rob Cardwell
#t 2005
#c 5
#% 264773
#% 790848
#! Integrating enterprise information requires an accurate, precise and complete understanding of the disparate data sources, the needs of the information consumers, and how these map to the semantic business concepts of the enterprise. We describe how MetaMatrix captures and manages this metadata through the use of the OMG's MOF architecture and multiple domain-specific modeling languages, and how this semantic and syntactic metadata is then used for a variety of purposes, including accessing data in real-time from the underlying enterprise systems, integrating it, and returning it as information expected by consumers.

#index 810077
#* Meta-data version and configuration management in multi-vendor environments
#@ John R Friedrich
#t 2005
#c 5
#% 391151
#% 411887
#! Nearly all components that comprise modern information technology, such as Computer Aided Software Engineering (CASE) tools, Enterprise Application Integration (EAI) environments, Extract/Transform/Load (ETL) engines. Warehouses, EII, and Business Intelligence (BI), contain a great deal of meta-data, which often drive much of the tool's functionality. These metadata are distributed and duplicated, are often times actively interacting with the tools as they process data, and are generally represented in a variety of methodologies. Meta-data exchange and reuse is now becoming commonplace. This article is based upon the real challenges found in these complicated meta-data environments, and identifies the often overlooked distinctions and importance of meta-data version and configuration management (CM), including the extensive use of automated meta-data comparison, mapping comparison, mapping generation and mapping update functions, which comprise a complete meta-data CM environment. Also addressed is the reality that most repositories are not up to the task of true version and configuration management, and thus true impact and lineage analysis, as their emphasis has been on the development a single enterprise architecture and the concept of "a single version of the truth."

#index 810078
#* Clio grows up: from research prototype to industrial tool
#@ Laura M. Haas;Mauricio A. Hernández;Howard Ho;Lucian Popa;Mary Roth
#t 2005
#c 5
#% 287733
#% 378409
#% 465057
#% 480134
#% 572314
#% 801676
#% 810021
#% 993981
#! Clio, the IBM Research system for expressing declarative schema mappings, has progressed in the past few years from a research prototype into a technology that is behind some of IBM's mapping technology. Clio provides a declarative way of specifying schema mappings between either XML or relational schemas. Mappings are compiled into an abstract query graph representation that captures the transformation semantics of the mappings. The query graph can then be serialized into different query languages, depending on the kind of schemas and systems involved in the mapping. Clio currently produces XQuery, XSLT, SQL, and SQL/XML queries. In this paper, we revisit the architecture and algorithms behind Clio. We then discuss some implementation issues, optimizations needed for scalability, and general lessons learned in the road towards creating an industrial-strength tool.

#index 810079
#* Integration of structured and unstructured data in IBM content manager
#@ David M. Choy
#t 2005
#c 5
#! Integration of structured and unstructured data goes much deeper than supporting large objects in a database. Through an architecture overview of the IBM Content Manager, this paper examines some of the requirements, challenges, and solutions in managing a large volume of content and in support of a wide range of content applications. The discussion touches upon system architecture, data model, and access control.

#index 810080
#* Database issues for the 21st century
#@ Adam Bosworth
#t 2005
#c 5
#! The Web has democratized and revolutionized computing in the last 10 years. Acting as a universal communications mechanism, it has let anyone talk to anyone (for example via email, IM and voip), anyone talk to any application (the Web), anyone talk to some very limited forms of information (Blogs/RSS), any application talk to anyone (Spam), and any application talk to any other application (Web Services).

#index 810081
#* Managing structure in bits & pieces: the killer use case for XML
#@ Eric Sedlar
#t 2005
#c 5
#! This paper asserts that for databases to manage a significantly greater percentage of the world's data, managing structural information must get significantly easier. XML technologies provide a widely accepted basis for significant advances in managing data structure. Topics include schema design, evolution, and versioning; managing related applications; and application architecture.

#index 810082
#* Modeling and querying multidimensional data sources in Siebel Analytics: a federated relational system
#@ Kazi A. Zaman;Donovan A. Schneider
#t 2005
#c 5
#% 393641
#% 397393
#% 480822
#% 504578
#! Large organizations have a multitude of data sources across the enterprise and want to obtain business value from all of them. While the majority of these data sources may be consolidated in an enterprise data warehouse, many business units have their own data marts where analysis is carried out against data stored in multidimensional data structures. It is often critical to pose queries which span both these sources. This is a challenge since these sources have differing models and query languages (SQL vs MDX). The Siebel Analytics Server enables this requirement to be fulfilled. In this paper, we describe how the multidimensional metadata is modeled relationally within Siebel Analytics, efficient SQL to MDX translation algorithms and the conversion protocols required to convert a multidimensional result into a relational rowset.

#index 810083
#* Native Xquery processing in oracle XMLDB
#@ Zhen Hua Liu;Muralidhar Krishnaprasad;Vikas Arora
#t 2005
#c 5
#% 116043
#% 397607
#% 413650
#% 480152
#% 480657
#% 480822
#% 654493
#% 781453
#% 800601
#% 994015
#% 1015339
#% 1016150
#% 1016223
#! With XQuery becoming the standard language for querying XML, and the relational SQL platform being recognized as an important platform to store and process XML, the SQL/XML standard is integrating XML query capability into the SQL system by introducing new SQL functions and constructs such as XMLQuery() and XMLTable. This paper discusses the Oracle XMLDB XQuery architecture for supporting XQuery in the Oracle ORDBMS kernel which has the XQuery processing tightly integrated with the SQL/XML engine using native XQuery compilation, optimization and execution techniques.

#index 810084
#* Optimizing recursive queries in SQL
#@ Carlos Ordonez
#t 2005
#c 5
#% 32882
#% 140753
#% 273695
#% 277355
#% 308452
#% 461881
#% 462945
#% 607454
#! Recursion represents an important addition to the SQL language. This work focuses on the optimization of linear recursive queries in SQL. To provide an abstract framework for discussion, we focus on computing the transitive closure of a graph. Three optimizations are studied: (1) Early evaluation of row selection conditions. (2) Eliminating duplicate rows in intermediate tables. (3) Defining an enhanced index to accelerate join computation. Optimizations are evaluated on two types of graphs: binary trees and sparse graphs. Binary trees represent an ideal graph with no cycles and a linear number of edges. Sparse graphs represent an average case with some cycles and a linear number of edges. In general, the proposed optimizations produce a significant reduction in the evaluation time of recursive queries.

#index 810085
#* IBM SOA "on the edge"
#@ Gennaro (Jerry) Cuomo
#t 2005
#c 5
#! This paper introduces the concept of an SOA edge server and a set of complementary design patterns designed to optimize performance, improve manageability and enable customers to cost effectively deploy SOA applications into complex, mission-critical, high-volume distributed environments.

#index 810086
#* Impact of SOA on enterprise information architectures
#@ Paul Patrick
#t 2005
#c 5
#% 952999
#! Enterprises are looking to find new and cost effective means to leverage existing investments in IT infrastructure and incorporate new capabilities in order to improve business productivity. As a means to improve the integration of applications hosted both internal and external to the enterprise, enterprises are turning to Service Oriented Architectures.In this paper, we describe some of the major aspects associated with the introduction of a Service Oriented Architecture and the impact that it can have on an enterprise's information architecture. We outline the concept of exposing data sources as services and discuss the critical integration aspects that need to be addressed including data access, data transformation, and integration into an over arching enterprise security scheme. The paper suggests alternatives, utilizing a Service Oriented Architecture approach, to promote flexible, extensible, and evolvable information architectures.

#index 810087
#* Data and metadata management in service-oriented architectures: some open challenges
#@ Vishal Sikka
#t 2005
#c 5
#% 810073
#! Over the last decade, the role of information technology in enterprises has been transforming from one of providing automation services to one of enabling business innovation. IT's charter is now closely aligned with the business goals and processes in a company and to support this charter, enterprise application architecture is shifting towards what's commonly referred to as a services-oriented architecture (SOA), or an enterprise-services architecture [1, 2]. In this talk, I want to discuss the shift to this new architecture and some ramifications of this, in particular some challenges posed by this shift for our research community to pursue.

#index 810088
#* Model-driven design of service-enabled web applications
#@ Marco Brambilla;Stefano Ceri;Piero Fraternali;Roberto Acerbis;Aldo Bongio
#t 2005
#c 5
#% 248819
#% 266552
#% 292634
#% 309729
#% 425200
#% 435009
#% 825632
#% 1599270
#! Significant efforts are currently invested in application integration to enable the interaction and composition of business processes of different companies, yielding complex; multi-party processes. Web service standards, based on WSDL, have been adopted as a process-to-process communication paradigm. This paper presents an industrial experience in integrating data-intensive and process-intensive Web applications through Web services. Design of sites and of Web services interaction exploits modern Web engineering methods, including conceptual modeling, model verification, visual data marshalling and automatic code generation. In particular, the applied method is based on a declarative model for specifying data-intensive Web applications that enact complex interactions, driven by the user, with remote processes implemented as services. We describe the internal architecture of the CASE tool that has been used, and give an overview of three industrial applications developed with the described approach.

#index 810089
#* Service Oriented Database Architecture: APP server-lite?
#@ David Campbell
#t 2005
#c 5
#% 765475
#! As the capabilities and service levels of enterprise database systems have evolved, they have collided with incumbent technologies such as TP-Monitors or Message Oriented Middleware (MOM). We believe this trend will continue and have architected the upcoming release of SQL Server to advance this technology trend. This paper describes the Service Oriented Database Architecture (SODA) developed for the Microsoft SQL Server DBMS. First, it motivates the need for building Service Oriented Architecture (SOA) features directly into a database engine. Second, it describes a set of features in SQL Server that have been designed for SOA use. Finally, it concludes with some thoughts on how SODA can enable multiple service deployment topologies.

#index 810090
#* Event processing with an oracle database
#@ Bob Thome;Dieter Gawlick;Maria Pratt
#t 2005
#c 5
#% 351041
#% 480331
#% 481448
#% 501937
#% 745520
#! In this paper, we examine how active database technology developed over the past few years has been put to use to solve real world problems. We note how the technology had to be extended beyond the feature set originally identified in early research to meet these real-world needs, and discuss why this technology was best suited to solving these problems.

#index 810091
#* A high-performance, transactional filestore for application servers
#@ Bill Gallagher;Dean Jacobs;Anno Langen
#t 2005
#c 5
#% 107692
#% 239972
#% 267190
#% 403195
#! There is a class of data, including messages and business workflow state, for which conventional monolithic databases are less than ideal. Performance and scalability of Application Server systems can be dramatically increased by distributing such data across transactional filestores, each of which is bound to a server instance in a cluster. This paper describes a high-performance, transactional filestore that has been developed for the BEA WebLogic Application ServerTM and benchmarks it against a database. The filestore uses a novel, platform-independent disk scheduling algorithm to minimize the latency of small, synchronous writes to disk.

#index 810092
#* A native extension of SQL for mining data streams
#@ Chang Luo;Hetal Thakkar;Haixun Wang;Carlo Zaniolo
#t 2005
#c 5
#% 248813
#% 300179
#% 310500
#% 378388
#% 379445
#% 397353
#% 480144
#% 578560
#% 729932
#% 785339
#% 993949
#% 1016170
#! ESL1 enables users to develop stream applications in an SQL-like high level language that provides the ease-of-use of a declarative language, which is Turing complete in terms of expressive power [11].

#index 810093
#* SPIDER: flexible matching in databases
#@ Nick Koudas;Amit Marathe;Divesh Srivastava
#t 2005
#c 5
#% 480654
#% 577309
#% 644182
#% 654467
#% 1016219
#! We present a prototype system, SPIDER, developed at AT&T Labs-Research, which supports flexible string attribute value matching in large databases. We discuss the design principles on which SPIDER is based, describe the basic techniques encompassed by the tool and provide a description of the demo.

#index 810094
#* GraphMiner: a structural pattern-mining system for large disk-based graph databases and its applications
#@ Wei Wang;Chen Wang;Yongtai Zhu;Baile Shi;Jian Pei;Xifeng Yan;Jiawei Han
#t 2005
#c 5
#% 466644
#% 629603
#% 629646
#% 629708
#% 729938
#% 769907
#! Mining frequent structural patterns from graph databases is an important research problem with broad applications. Recently, we developed an effective index structure, ADI, and efficient algorithms for mining frequent patterns from large, disk-based graph databases [5], as well as constraint-based mining techniques. The techniques have been integrated into a research prototype system--- GraphMiner. In this paper, we describe a demo of GraphMiner which showcases the technical details of the index structure and the mining algorithms including their efficient implementation, the mining performance and the comparison with some state-of-the-art methods, the constraint-based graph-pattern mining techniques and the procedure of constrained graph mining, as well as mining real data sets in novel applications.

#index 810095
#* Distributed operation in the Borealis stream processing engine
#@ Yanif Ahmad;Bradley Berg;Uǧur Cetintemel;Mark Humphrey;Jeong-Hyon Hwang;Anjali Jhingran;Anurag Maskey;Olga Papaemmanouil;Alexander Rasin;Nesime Tatbul;Wenjuan Xing;Ying Xing;Stan Zdonik
#t 2005
#c 5
#% 654507
#% 654508
#% 654510
#% 800583
#% 800584
#% 1015280
#% 1015324
#% 1016169
#! Borealis is a distributed stream processing engine that is being developed at Brandeis University, Brown University, and MIT. Borealis inherits core stream processing functionality from Aurora and inter-node communication functionality from Medusa.We propose to demonstrate some of the key aspects of distributed operation in Borealis, using a multi-player network game as the underlying application. The demonstration will illustrate the dynamic resource management, query optimization and high availability mechanisms employed by Borealis, using visual performance-monitoring tools as well as the gaming experience.

#index 810096
#* Events on the edge
#@ Shariq Rizvi;Shawn R. Jeffery;Sailesh Krishnamurthy;Michael J. Franklin;Nathan Burkhart;Anil Edakkunni;Linus Liang
#t 2005
#c 5
#% 333850
#% 480938
#% 503878
#% 569762
#% 1016270
#! The emergence of large-scale receptor-based systems has enabled applications to execute complex business logic over data generated from monitoring the physical world. An important functionality required by these applications is the detection and response to complex events, often in real-time. Bridging the gap between low-level receptor technology and such high-level needs of applications remains a significant challenge.We demonstrate our solution to this problem in the context of HiFi, a system we are building to solve the data management problems of large-scale receptor-based systems. Specifically, we show how HiFi generates simple events out of receptor data at its edges and provides high-functionality complex event processing mechanisms for sophisticated event detection using a real-world library scenario.

#index 810097
#* Safe data sharing and data dissemination on smart devices
#@ Luc Bouganim;Cosmin Cremarenco;François Dang Ngoc;Nicolas Dieu;Philippe Pucheral
#t 2005
#c 5
#% 344639
#% 378393
#% 397367
#% 433922
#% 613813
#% 654477
#% 994006
#% 1016136
#! The erosion of trust put in traditional database servers and in Database Service Providers (DSP), the growing interest for different forms of data dissemination and the concern for protecting children from suspicious Internet content are different factors that lead to move the access control from servers to clients. Due to the intrinsic untrustworthiness of client devices, client-based access control solutions rely on data encryption. The data are kept encrypted at the server and a client is granted access to subparts of them according to the decryption keys in its possession. Several variations of this basic model have been proposed (e.g., [1, 6]) but they have in common to minimize the trust required on the client at the cost of a static way of sharing data. Indeed, whatever the granularity of sharing, the dataset is split in subsets reflecting a current sharing situation, each encrypted with a different key. Once the dataset is encrypted, changes in the access control rules definition may impact the subset boundaries, hence incurring a partial re-encryption of the dataset and a potential redistribution of keys.

#index 810098
#* MYSTIQ: a system for finding more answers by using probabilities
#@ Jihad Boulos;Nilesh Dalvi;Bhushan Mandhani;Shobhit Mathur;Chris Re;Dan Suciu
#t 2005
#c 5
#% 41230
#% 215225
#% 235023
#% 397406
#% 460928
#% 754068
#% 765455
#% 793254
#% 1016201
#! MystiQ is a system that uses probabilistic query semantics [3] to find answers in large numbers of data sources of less than perfect quality. There are many reasons why the data originating from many different sources may be of poor quality, and therefore difficult to query: the same data item may have different representation in different sources; the schema alignments needed by a query system are imperfect and noisy; different sources may contain contradictory information, and, in particular, their combined data may violate some global integrity constraints; fuzzy matches between objects from different sources may return false positives or negatives. Even in such environment, users some-times want to ask complex, structurally rich queries, using query constructs typically found in SQL queries: joins, subqueries, existential/universal quantifiers, aggregate and group-by queries: for example scientists may use such queries to query multiple scientific data sources, or a law enforcement agency may use it in order to find rare associations from multiple data sources. If standard query semantics were applied to such queries, all but the most trivial queries will return an empty answer.

#index 810099
#* ProDA: a suite of web-services for progressive data analysis
#@ Mehrdad Jahangiri;Cyrus Shahabi
#t 2005
#c 5
#% 378399
#% 458875
#% 810030
#! Online Scientific Applications (OSA) require statistical analysis of large multidimensional datasets. Towards this end, we have designed and developed a data storage and retrieval system, called ProDA, which deploys wavelet transform and provides fast approximate answers with progressively increasing accuracy in support of the OSA queries. ProDA employs a standard web-service infrastructure to enable remote users to interact with their data. These web-services enable wavelet transformation of large multidimensional datasets as well as inserting, updating, and exact, approximate and progressive querying of these datasets in the wavelet domain. We demonstrate the features of ProDA on a massive atmospheric dataset provided to us by NASA/JPL.

#index 810100
#* A framework for processing complex document-centric XML with overlapping structures
#@ Ionut E. Iacob;Alex Dekhtyar
#t 2005
#c 5
#% 765422
#% 772033
#% 783695
#% 806624
#% 809409
#! Management of multihierarchical XML encodings has attracted attention of a number of researchers both in databases [8] and in humanities[10]. Encoding documents using multiple hierarchies can yield overlapping markup. Previously proposed solutions to management of document-centric XML with overlapping markup rely on the XML expertise of humans and their ability to maintain correct schemas for complex markup languages.We demonstrate a unified solution for management of complex, multihierarchical document-centric XML. Our framework includes software for storing, parsing, in-memory access, editing and querying, multihierarchical XML documents with conflicting structures.

#index 810101
#* NaLIX: an interactive natural language interface for querying XML
#@ Yunyao Li;Huahai Yang;H. V. Jagadish
#t 2005
#c 5
#% 284891
#% 1016135
#! Database query languages can be intimidating to the non-expert, leading to the immense recent popularity for keyword based search in spite of its significant limitations. The holy grail has been the development of a natural language query interface. We present NaLIX, a generic interactive natural language query interface to an XML database. Our system can accept an arbitrary English language sentence as query input, which can include aggregation, nesting, and value joins, among other things. This query is translated, potentially after reformulation, into an XQuery expression that can be evaluated against an XML database. The translation is done through mapping grammatical proximity of natural language parsed tokens to proximity of corresponding elements in the result XML. In this demonstration, we show that NaLIX, while far from being able to pass the Turing test, is perfectly usable in practice, and able to handle even quite complex queries in a variety of application domains. In addition, we also demonstrate how carefully designed features in NaLIX facilitate the interactive query process and improve the usability of the interface.

#index 810102
#* XQBE: a visual environment for learning XML query languages
#@ Daniele Braga;Alessandro Campi;Stefano Ceri;Alessandro Raffio
#t 2005
#c 5
#% 814647
#! XQBE (XQuery By Example) is a visual XML query language which, coherently with the hierarchical XML data model, uses tree-shaped structures to express queries and transformations over XML documents. These structures are annotated to express selection predicates; explicit bindings between the nodes of such structures visualize the input/output mappings.XQuery and XSLT, the standard query and transformation languages for XML, happen to be too complex for most occasional or unskilled users who might need to specify queries, schema mappings, or document transformations, if they are only aware of the basics of the XML data model. The implementation of XQBE allows to generate the XQuery and XSLT translations of the visual queries, assisting the user in several aspects of the interaction (e.g. providing interactive access to schema information); therefore, XQBE provides an integrated environment where users can edit the visual queries and their textual counterparts, executing them on several engines. Alternating among different representations of the same query is valuable for training beginners, as we have experienced in our database courses.

#index 810103
#* Schema and ontology matching with COMA++
#@ David Aumueller;Hong-Hai Do;Sabine Massmann;Erhard Rahm
#t 2005
#c 5
#% 348187
#% 551850
#% 572314
#% 742769
#% 790846
#% 800497
#% 939794
#% 993982
#! We demonstrate the schema and ontology matching tool COMA++. It extends our previous prototype COMA utilizing a composite approach to combine different match algorithms [3]. COMA++ implements significant improvements and offers a comprehensive infrastructure to solve large real-world match problems. It comes with a graphical interface enabling a variety of user interactions. Using a generic data representation, COMA++ uniformly supports schemas and ontologies, e.g. the powerful standard languages W3C XML Schema and OWL. COMA++ includes new approaches for ontology matching, in particular the utilization of shared taxonomies. Furthermore, different match strategies can be applied including various forms of reusing previously determined match results and a so-called fragment-based match approach which decomposes a large match problem into smaller problems. Finally, COMA++ cannot only be used to solve match problems but also to comparatively evaluate the effectiveness of different match algorithms and strategies.

#index 810104
#* SMART: a tool for semantic-driven creation of complex XML mappings
#@ Atsuyuki Morishima;Toshiaki Okawara;Jun'ichi Tanaka;Ken'ichi Ishikawa
#t 2005
#c 5
#% 333990
#% 479783
#% 480134
#% 572314
#% 665856
#% 745462
#% 993981
#! We focus on the problem of data transformations, i.e., how to transform data to another structure to adapt it to new application requirements or given environments. Here, we define data transformation as the process of taking as input two schemas A and B and an instance of A, and producing an instance of B. Today, data transformations are required in many situations: to integrate multiple information sources, to construct and receive data for Web services, and to migrate data from legacy systems to new systems, from local databases to data warehouses. This demonstration focuses on XML transformations, since XML is the de facto standard for data exchange.

#index 810105
#* Relational data mapping in MIQIS
#@ George H. L. Fletcher;Catharine M. Wyss
#t 2005
#c 5
#% 102748
#% 126335
#% 248800
#% 378409
#% 431103
#% 480134
#% 572314
#% 654458
#% 654468
#% 665627
#% 742769
#% 743126
#% 765540
#% 783791
#% 800498
#% 801668
#% 814652
#! We demonstrate a prototype of the relational data mapping module of MIQIS, a formal framework for investigating information flow in peer-to-peer database management systems. Data maps constitute effective mappings between structured data sources. These mappings are the `glue' for facilitating large scale ad-hoc information sharing between autonomous peers, and automating their discovery is one of the fundamental unsolved challenges for information interoperability and sharing. Our approach to automating data map discovery utilizes heuristic search within a space delineated by basic relational transformation operators. A novelty of our approach is that these operators include data to metadata transformations (and vice versa). This approach leverages new perspectives on the data mapping problem, and generalizes previous approaches such as token-based schema matching.

#index 810106
#* The INFOMIX system for advanced integration of incomplete and inconsistent data
#@ Nicola Leone;Gianluigi Greco;Giovambattista Ianni;Vincenzino Lio;Giorgio Terracina;Thomas Eiter;Wolfgang Faber;Michael Fink;Georg Gottlob;Riccardo Rosati;Domenico Lembo;Maurizio Lenzerini;Marco Ruzzi;Edyta Kalka;Bartosz Nowicki;Witold Staniszkis
#t 2005
#c 5
#% 378409
#% 480648
#% 576116
#% 727668
#% 801668
#% 880394
#% 1015254
#% 1279213
#% 1279214
#% 1700138
#! The task of an information integration system is to combine data residing at different sources, providing the user with a unified view of them, called global schema. Users formulate queries over the global schema, and the system suitably queries the sources, providing an answer to the user, who is not obliged to have any information about the sources. Recent developments in IT such as the expansion of the Internet and the World Wide Web, have made available to users a huge number of information sources, generally autonomous, heterogeneous and widely distributed: as a consequence, information integration has emerged as a crucial issue in many application domains, e.g., distributed databases, cooperative information systems, data warehousing, or on-demand computing. Recent estimates view information integration to be a $10 Billion market by 2006 [14].

#index 810107
#* Data cleaning in microsoft SQL server 2005
#@ Surajit Chaudhuri;Kris Ganjam;Venky Ganti;Rahul Kapoor;Vivek Narasayya;Theo Vassilakis
#t 2005
#c 5
#% 201889
#% 654467
#! When collecting and combining data from various sources into a data warehouse, ensuring high data quality and consistency becomes a significant, often expensive, challenge. Common data quality problems include inconsistent data conventions amongst sources such as different abbreviations or synonyms; data entry errors such as spelling mistakes; missing, incomplete, outdated or otherwise incorrect attribute values. These data defects generally manifest themselves as foreign-key mismatches and approximately duplicate records, both of which make further data mining and decision support analyses either impossible or suspect. We demonstrate two new data cleansing operators, Fuzzy Lookup and Fuzzy Grouping, which address these problems in a scalable and domain-independent manner. These operators are implemented within Microsoft SQL Server 2005 Integration Services. Our demo will explain their functionality and highlight multiple real-world scenarios in which they can be used to achieve high data quality.

#index 810108
#* Personal information management with SEMEX
#@ Yuhan Cai;Xin Luna Dong;Alon Halevy;Jing Michelle Liu;Jayant Madhavan
#t 2005
#c 5
#% 268079
#% 480634
#% 810014
#% 840583
#% 1015345
#! The explosion of information available in digital form has made search a hot research topic for the Information Management Community. While most of the research on search is focused on the WWW, individual computer users have developed their own vast collections of data on their desktops, and these collections are in critical need for good search and query tools. The problem is exacerbated by the proliferation of varied electronic devices (laptops, PDAs, cellphones) that are at our disposal, which often hold subsets or variations of our data. In fact, several recent venues have noted Personal Information Management (PIM) as an area of growing interest to the data management community [1, 8, 6]

#index 810109
#* A system for analyzing and indexing human-motion databases
#@ Guodong Liu;Jingdan Zhang;Wei Wang;Leonard McMillan
#t 2005
#c 5
#% 229940
#% 308505
#% 398425
#% 431104
#% 466428
#% 492652
#% 593620
#% 959896
#% 1016194
#! We demonstrate a data-driven approach for representing, compressing, and indexing human-motion databases. Our modeling approach is based on piecewise-linear components that are determined via a divisive clustering method. Selection of the appropriate linear model is determined automatically via a classifier using a subspace of the most significant, or principle features (markers). We show that, after offline training, our model can accurately estimate and classify human motions. We can also construct indexing structures for motion sequences according to their transition trajectories through these linear components. Our method not only provides indices for whole and/or partial motion sequences, but also serves as a compressed representation for the entire motion database. Our method also tends to be immune to temporal variations, and thus avoids the expense of time-warping.

#index 810110
#* MetaQuerier: querying structured web sources on-the-fly
#@ Bin He;Zhen Zhang;Kevin Chen-Chuan Chang
#t 2005
#c 5
#% 273914
#% 273923
#% 654459
#% 765410
#% 765491
#% 769890
#% 783791
#! Recently, we witness the rapid growth and thus the prevalence of databases on the Web. Our recent survey [2] in April 2004 estimated 450,000 online databases. On this deep Web, myriad online databases provide dynamic query-based data access through their query interfaces, instead of static URL links. As the door to the deep Web, it is essential to integrate these query interfaces for integrating the deep Web.

#index 810111
#* Database tuning advisor for microsoft SQL server 2005: demo
#@ Sanjay Agrawal;Surajit Chaudhuri;Lubor Kollar;Arun Marathe;Vivek Narasayya;Manoj Syamala
#t 2005
#c 5
#% 248815
#% 480158
#% 482100
#% 631950
#% 765431
#! Database Tuning Advisor (DTA) is a physical database design tool that is part of Microsoft's SQL Server 2005 relational database management system. Previously known as "Index Tuning Wizard" in SQL Server 7.0 and SQL Server 2000, DTA adds new functionality that is not available in other contemporary physical design tuning tools. Novel aspects of DTA that will be demonstrated include: (a) Ability to take into account both performance and manageability requirements of DBAs (b) Fully integrated recommendations for indexes, materialized views and horizontal partitioning (c) Transparently leverage a test server to offload tuning load from production server and (d) Easy programmability and scriptability.

#index 810112
#* Automated statistics collection in action
#@ P. Haas;M. Kandil;A. Lerner;V. Markl;I. Popivanov;V. Raman;D. Zilio
#t 2005
#c 5
#% 480803
#% 1016225
#! If presented with inaccurate statistics, even the most sophisticated query optimizers make mistakes. They may wrongly estimate the output cardinality of a certain operation and thus make sub-optimal plan choices based on that cardinality. Maintaining accurate statistics is hard, both because each table may need a specifically parameterized set of statistics and because statistics get outdated as the database changes. Automated Statistic Collection (ASC) is a new component in IBM DB2 UDB that, without any DBA intervention, observes and analyzes the effects of faulty statistics and, in response, it triggers actions that continuously repair the latter. In this demonstration, we will show how ASC works to alleviate the DBA from the task of maintaining fresh, accurate statistics in several challenging scenarios. ASC is able to reconfigure the statistics collection parameters (e.g, number of frequent values for a column, or correlations between certain column pairs) on a per-table basis. ASC can also detect and guard against outdated statistics caused by high updates/inserts/deletes rates in volatile, dynamic databases. We will also show how ASC works from the inside: from how cardinality mis-estimations are introduced in different kind of operators, to how this error is propagated to later operations in the plan, to how this influences plan choices inside the optimizer.

#index 810113
#* Proactive re-optimization with Rio
#@ Shivnath Babu;Pedro Bizarro;David DeWitt
#t 2005
#c 5
#% 245996
#% 248793
#% 765456
#% 765500
#% 810016
#! Traditional query optimizers rely on the accuracy of estimated statistics of intermediate subexpressions to choose good query execution plans. This design often leads to suboptimal plan choices for complex queries since errors in estimates grow exponentially in the presence of skewed and correlated data distributions. We propose to demonstrate the Rio prototype database system that uses proactive re-optimization to address the problems with traditional optimizers. Rio supports three new techniques:1. Intervals of uncertainty are considered around estimates of statistics during plan enumeration and costing2. These intervals are used to pick execution plans that are robust to deviations of actual values of statistics from estimated values, or to defer the choice of execution plan until the uncertainty in estimates can be resolved3. Statistics of intermediate subexpressions are collected quickly, accurately, and efficiently during query executionThese three features are fully functional in the current Rio prototype which is built using the Predator open-source DBMS [5]. In this proposal, we first describe the novel features of Rio, then we use an example query to illustrate the main aspects of our demonstration.

#index 810114
#* Immortal DB: transaction time support for SQL server
#@ David Lomet;Roger Barga;Mohamed F. Mokbel;German Shegalov;Rui Wang;Yunyue Zhu
#t 2005
#c 5
#% 9241
#% 58371
#% 259487
#% 421124
#! Immortal DB builds transaction time database support into the SQL Server engine, not in middleware. Transaction time databases retain and provide access to prior states of a database. An update "inserts" a new record while preserving the old version. The system supports as of queries returning records current at the specified time. It also supports snapshot isolation concurrency control. Versions are stamped with the times of their updating transactions. The timestamp order agrees with transaction serialization order. Lazy timestamping propagates timestamps to all updates of a transaction after commit. All versions are kept in an integrated storage structure, with historical versions initially stored with current data. Time-splits of pages permit large histories to be maintained, and enable time based indexing. We demonstrate Immortal DB with a moving objects application that tracks cars in the Seattle area.

#index 810115
#* DBNotes: a post-it system for relational databases based on provenance
#@ Laura Chiticariu;Wang-Chiew Tan;Gaurav Vijayvargiya
#t 2005
#c 5
#% 1016204
#! We demonstrate DBNotes, a Post-It note system for relational databases where every piece of data may be associated with zero or more notes (or annotations). These annotations are transparently propagated along as data is being transformed. The method by which annotations are propagated is based on provenance (aka lineage): the annotations associated with a piece of data d in the result of a transformation consist of the annotations associated with each piece of data in the source where d is copied from. One immediate application of this system is to use annotations to systematically trace the provenance and flow of data. If every piece of source data is attached with an annotation that describes its address (i.e., origins), then the annotations of a piece of data in the result of a transformation describe its provenance. Hence, one can easily determine the provenance of data through a sequence of transformation steps simply by examining the annotations. Annotations can also be used to store additional information about data. Since a database schema is often proprietary, the ability to insert new information about data without having to change the underlying schema is a useful feature. For example, an error report could be attached to an erroneous piece of data, and this error report will be propagated to other databases along transformations, thus notifying other users of the error. Overall, the annotations on the result of a transformation can also provide an estimate on the quality of the resulting database.

#index 810116
#* XML and relational database management systems: the inside story
#@ Michael Rys;Don Chamberlin;Daniela Florescu
#t 2005
#c 5
#! As XML has evolved from a document markup language to a widely-used format for exchange of structured and semistructured data, managing large amounts of XML data has become increasingly important. A number of companies, including both established database vendors and startups, have recently announced new XML database systems or new XML functionality integrated into existing database systems. This tutorial will provide an insight into how XML functionality fits into relational database management systems as seen by three major relational vendors: IBM, Microsoft and Oracle.

#index 810117
#* DB2/XML: designing for evolution
#@ Kevin Beyer;Fatma Özcan;Sundar Saiprasad;Bert Van der Linden
#t 2005
#c 5
#% 783793
#% 810036
#! DB2 provides native XML storage, indexing, navigation and query processing through both SQL/XML and XQuery using the XML data type introduced by SQL/XML. In this tutorial we focus on DB2's XML support for schema evolution, especially DB2's schema repository and document-level validation.

#index 810118
#* Towards an enterprise XML architecture
#@ Ravi Murthy;Zhen Hua Liu;Muralidhar Krishnaprasad;Sivasankaran Chandrasekar;Anh-Tuan Tran;Eric Sedlar;Daniela Florescu;Susan Kotsovolos;Nipun Agarwal;Vikas Arora;Viswanathan Krishnamurthy
#t 2005
#c 5
#% 800601
#% 810083
#% 1015339
#% 1016223
#! XML is being increasingly used in diverse domains ranging from data and application integration to content management. Oracle provides an enterprise wide platform for managing all types of XML content. Within the Oracle database and the application server, the XML content can be efficiently stored using a variety of storage and indexing methods and it can be processed using multiple standard languages within different programmatic environments.

#index 810119
#* XML and relational database management systems: inside Microsoft® SQL Server™ 2005
#@ Michael Rys
#t 2005
#c 5
#% 464988
#% 765488
#% 1016224
#! Microsoft® SQL Server™ has a long history of XML support dating back to 1999. While first concentrating on enabling the transport of relational data via XML with the SQL Server 2000 release, SQL Server 2005 now additionally provides native XML storage and query support. This part of the tutorial will provide an insight into how SQL Server 2005 fits XML functionality into its core relational database management framework.

#index 810120
#* Foundations of probabilistic answers to queries
#@ Dan Suciu;Nilesh Dalvi
#t 2005
#c 5
#! Overview: Probabilistic query answering is a fundamental set of techniques that underlies several, very recent database applications: exploratory queries in databases, novel IR-style approaches to data integration, querying information extracted from the Web, queries over sensor networks, data acquisition, querying data sources that violate integrity constraints, controlling information disclosure in data exchange, and reasoning about privacy breaches in data mining. This is a surprisingly diverse range of applications, most of which have either emerged recently, or have seen a recent increased interest, and which all share a common fundamental abstraction: that an item being in the answer to a query is no longer a boolean value, but a probabilistic event. It this authors belief that this is a new paradigm in query answering, whose foundations lie in random graphs, and 0/1-laws in finite model theory. The results from these fields, and their relevance to the probabilistic query answering method, are very little known in the database research community, and the theoretical research papers or books that describe them are not very popular in the systems database research community.

#index 810121
#* Foundations of automated database tuning
#@ Surajit Chaudhuri;Gerhard Weikum
#t 2005
#c 5
#% 1016240
#! 1. The Challenge of Total Cost of-Ownership Our society is more dependent on information systems than ever before. However, managing the information systems infrastructure in a cost-effective manner is a growing challenge. The total cost of ownership (TCO) of information technology is increasingly dominated by people costs. In fact, mistakes in operations and administration of information systems are the single most reasons for system outage and unacceptable performance. For information systems to provide value to their customers, we must reduce the complexity associated with their deployment and usage.

#index 810122
#* Research issues in protein location image databases
#@ Robert F. Murphy;Christos Faloutsos
#t 2005
#c 5
#! Which proteins have similar locations within cells? How many distinct location patters do cells display? How do we answer these questions quickly, from a large collection of microscope images such as in on-line journals?

#index 810123
#* Computing for biologists: lessons from some successful case studies
#@ Dennis Shasha
#t 2005
#c 5
#% 765429
#! My presentation will be online at the address http://cs.nyu.edu/cs/faculty/shasha/papers/sigmodtut05.ppt in addition to at the SIGMOD site. The presentation discusses computational techniques that have helped biologists, including combinatorial design to support a disciplined experimental design, visualization techniques to display the interaction among multiple inputs, and the discovery of gene function through the search through related species, and others.In this writeup, I confine myself to informal remarks describing both social and technical lessons I have learned while working with biologists. I intersperse these comments with references to relevant papers when appropriate.The tutorial is meant to appeal to researchers and practitioners in databases, data mining, and combinatorial algorithms as well as to natural scientists, especially biologists.

#index 819543
#* Proceedings of the 2nd international workshop on Information quality in information systems
#@ Laure Berti-Equille;Carlo Batini;Divesh Srivastava
#t 2005
#c 5
#! The problem of poor data quality stored in database-backed information systems is widespread in the governmental, commercial and industrial environments. Alarming situations with various information quality problems can not be ignored anymore and theoretical as well as pragmatic approaches are urgently needed to be proposed and validated. As a consequence, information quality is now becoming one of the hot topics of emerging interest in the academic and industrial communities.Many processes and applications (such as information system integration, information retrieval, and knowledge discovery from databases) require various forms of data preparation or repair with several data processing techniques, because the data input to the application-dedicated algorithms is assumed to conform to nice data distributions, containing no missing, inconsistent or incorrect values. This leaves a large gap between the available "dirty" data and the available machinery to process the data for application purposes.The Second Edition of the International Workshop IQIS 2005 (Information Quality in Information Systems) is held in Baltimore, MD, USA, on June 17, 2005. The workshop is sponsored by ACM and in conjunction with the Symposium on Principles of Database System (PODS) and the ACM SIGMOD International Conference on Management of Data. IQIS workshop focuses on database-centric issues in data quality (scalability, quality-aware query processing, applications like data integration). It intends to address methods, techniques of massive data processing and analysis, methodologies, new algorithmic approaches or frameworks for designing data quality metrics in order to understand and to explore data quality, to end data glitches (as data quality problems such as duplicates, errors, outliers, contradictions, inconsistencies, etc.) and to ensure both data and information quality of database-backed information systems.The 11 papers collected in this volume, out of 26 papers that were submitted (with 10 short papers and 16 research papers), are a significant sample of recent achievements in the various areas of information and data quality, ranging from quality models to record linkage and statistical techniques.

#index 824793
#* A tribute to Professor Hongjun Lu
#@ Michael J. Carey;Jiawei Han
#t 2005
#c 5
#! Dr. Hongjun Lu, Professor of Computer Science, Hong Kong University of Science and Technology, lost his brave fight against cancer and left us in the evening of March 3, 2005. The world lost a dedicated and brilliant computer scientist. The database research community lost a respected and prolific researcher, an effortless organizer and promoter of database research in the world, a friend cherished by many colleagues and researchers, and a wonderful teacher who deeply affected and was revered by his students.

#index 824794
#* Peer-to-peer management of XML data: issues and research challenges
#@ Georgia Koloniari;Evaggelia Pitoura
#t 2005
#c 5
#% 281495
#% 340175
#% 340176
#% 345693
#% 349973
#% 413582
#% 446428
#% 465063
#% 479465
#% 577357
#% 610849
#% 612645
#% 636008
#% 643013
#% 646221
#% 654468
#% 654485
#% 723445
#% 729627
#% 729941
#% 745427
#% 754121
#% 754123
#% 765446
#% 770338
#% 783698
#% 993970
#% 1015258
#% 1015327
#% 1016133
#% 1388072
#% 1388073
#% 1712582
#! Peer-to-peer (p2p) systems are attracting increasing attention as an efficient means of sharing data among large, diverse and dynamic sets of users. The widespread use of XML as a standard for representing and exchanging data in the Internet suggests using XML for describing data shared in a p2p system. However, sharing XML data imposes new challenges in p2p systems related to supporting advanced querying beyond simple keyword-based retrieval. In this paper, we focus on data management issues for processing XML data in a p2p setting, namely indexing, replication, clustering and query routing and processing. For each of these topics, we present the issues that arise, survey related research and highlight open research problems.

#index 824795
#* Mining data streams: a review
#@ Mohamed Medhat Gaber;Arkady Zaslavsky;Shonali Krishnaswamy
#t 2005
#c 5
#% 280408
#% 310500
#% 341700
#% 342600
#% 345857
#% 345861
#% 346511
#% 378388
#% 397426
#% 453512
#% 456630
#% 466908
#% 480156
#% 564281
#% 576113
#% 576119
#% 578388
#% 578390
#% 578560
#% 580668
#% 594012
#% 630972
#% 654507
#% 659972
#% 662750
#% 662751
#% 727900
#% 729932
#% 765494
#% 769927
#% 993958
#% 993960
#% 993961
#% 998631
#% 1015261
#% 1015301
#% 1016200
#! The recent advances in hardware and software have enabled the capture of different measurements of data in a wide range of fields. These measurements are generated continuously and in a very high fluctuating data rates. Examples include sensor networks, web logs, and computer network traffic. The storage, querying and mining of such data sets are highly computationally challenging tasks. Mining data streams is concerned with extracting knowledge structures represented in models and patterns in non stopping streams of information. The research in data stream mining has gained a high attraction due to the importance of its applications and the increasing generation of streaming information. Applications of data stream analysis can vary from critical scientific and astronomical applications to important business and financial ones. Algorithms, systems and frameworks that address streaming challenges have been developed over the past three years. In this review paper, we present the state-of-the-art in this growing vital field.

#index 824796
#* Analytical processing of XML documents: opportunities and challenges
#@ Rajesh R. Bordawekar;Christian A. Lang
#t 2005
#c 5
#% 223781
#% 235941
#% 236410
#% 237053
#% 308444
#% 308509
#% 333955
#% 413125
#% 420053
#% 428154
#% 480123
#% 487267
#% 642993
#% 654442
#% 654486
#% 664843
#% 1015272
#% 1015274
#% 1015283
#% 1275285
#! Online Analytical Processing (OLAP) has been a valuable tool for analyzing trends in business information. While the multi-dimensional cube model used by OLAP is ideal for analyzing structured business data, it is not suitable for representing and analyzing complex semi-structured data, such as, XML documents. Need for analyzing XML documents is gaining urgency as XML has become the language of choice for data representation across a wide range of application domains. This paper describes a proposal for analyzing XML documents using the abstract XML tree model. We argue that OLAP's multi-dimensional aggregation operators can not express structurally complex analytical operations on XML documents. Hence, we outline new extensions to XQuery for supporting such complex analytical operations. Finally, we discuss various challenges in implementing XML analysis in a real system.

#index 824797
#* On six degrees of separation in DBLP-DB and more
#@ Ergin Elmacioglu;Dongwon Lee
#t 2005
#c 5
#% 287267
#% 723439
#% 723440
#! An extensive bibliometric study on the db community using the collaboration network constructed from DBLP data is presented. Among many, we have found that (1) the average distance of all db scholars in the network has been stabilized to about 6 for the last 15 years, coinciding with the so-called six degrees of separation phenomenon; (2) In sync with Lotka's law on the frequency of publications, the db community also shows that a few number of scholars publish a large number of papers, while the majority of authors publish a small number of papers (i.e., following the power-law with exponent about -2); and (3) with the increasing demand to publish more, scholars collaborate more often than before (i.e., 3.93 collaborators per scholar and with steadily increasing clustering coefficients).

#index 824798
#* Semantic characterizations of navigational XPath
#@ Maarten Marx;Maarten de Rijke
#t 2005
#c 5
#% 299942
#% 299976
#% 333841
#% 333856
#% 338753
#% 378393
#% 427874
#% 465065
#% 473117
#% 544561
#% 576108
#% 776459
#% 801669
#% 801686
#% 1700125
#! We give semantic characterizations of the expressive power of navigational XPath (a.k.a. Core XPath) in terms of first order logic. XPath can be used to specify sets of nodes and sets of paths in an XML document tree. We consider both uses. For sets of nodes, XPath is equally expressive as first order logic in two variables. For paths, XPath can be defined using four simple connectives, which together yield the class of first order definable relations which are safe for bisimulation. Furthermore, we give a characterization of the XPath expressible paths in terms of conjunctive queries.

#index 824799
#* Nested intervals tree encoding in SQL
#@ Vadim Tropashko
#t 2005
#c 5
#% 59536
#% 749906
#! Nested Intervals generalize Nested Sets. They are immune to hierarchy reorganization problem. They allow answering ancestor path hierarchical queries algorithmically - without accessing the stored hierarchy relation.

#index 824800
#* The Indiana Center for Database Systems at Purdue University
#@ Mourad Ouzzani;Walid G. Aref;Elisa Bertino;Ann Christine Catlin;Christopher W. Clifton;Wing-Kai Hon;Ahmed K. Elmagarmid;Arif Ghafoor;Susanne E. Hambrusch;Sunil Prabhakar;Jeffrey S. Vitter;Xiang Zhang
#t 2005
#c 5
#% 430750
#% 452559
#% 453572
#% 570628
#% 654449
#% 664822
#% 729930
#% 745434
#% 745495
#% 765418
#% 765453
#% 766200
#% 769943
#% 772829
#% 777931
#% 797819
#% 800182
#% 800539
#% 800572
#% 808349
#% 812771
#% 994013
#% 1015279
#% 1016202
#% 1775237
#% 1855648
#! The Indiana Center for Database Systems (ICDS) at Purdue University has embarked in an ambitious endeavor to become a premiere world-class database research center. This goal is substantiated by the diversity of its research topics, the large and diverse funding base, and the steady publication trend in top conferences and journals. ICDS was founded with an initial grant from the State of Indiana Corporation of Science and Technology in 1990. Since then it has grown to now have 9 faculty members and about 30 total researchers. This report describes the major research projects underway at ICDS as well as efforts to move research toward practice.

#index 824801
#* Report on the ninth conference on Software Engineering and Databases (JISBD 2004)
#@ Juan Hernández;Ernesto Pimentel;Ambrosio Toval
#t 2005
#c 5
#! The ninth edition of the Conference on Software Engineering and Databases (named JISBD after its initials in Spanish) was held in Málaga (Spain) November 10-12, 2004. This conference had its origins in two previous, separate events comprising two scientific communities closely related to the subjects of the conferences: on the one hand the Spanish Software Engineering Conference, held for the first time in Sevilla (Spain) in 1996, and, on the other, the Spanish Conference on Research and Education in Databases, held in A Coruña (Spain), also for the first time in the same year. These two events were joined in 1999, giving rise to the Fourth Conference on Software Engineering and Databases, for the first time with this name, which has been preserved to date.

#index 824802
#* Report on the 1st International Symposium on the Applications of Constraint Databases (CDB'04)
#@ Bart Kuijpers;Peter Revesz
#t 2005
#c 5
#! The 1st International Symposium on the Applications of Constraint Databases (CDB'04) was held on June 12-13, 2004, just before the ACM SIGMOD and PODS conferences, in the Amphithéatre de Chimie of the Université Pierre et Marie Curie in Paris, France. We acted as program committee chairs and Irène Guessarian and Patrick Cégielski as local organization chairs.

#index 824803
#* Report on the International Workshop on Pattern Representation and Management (PaRMa'04)
#@ Yannis Theodoridis;Panos Vassiliadis
#t 2005
#c 5
#! The increasing ability to quickly collect and cheaply store large volumes of data, and the need for extracting concise information to be efficiently manipulated and intuitively analyzed, are posing new requirements for Database Management Systems (DBMS) in both industrial and scientific applications. A common approach to deal with huge data volumes is to reduce the available information to knowledge artifacts (i.e., clusters, rules, etc.), hereafter called patterns, through data processing methods (pattern recognition, data mining, knowledge extraction). Patterns reduce the number and size of the original information to manageable size while preserving as much as possible its hidden / interesting content. In order to efficiently and effectively deal with patterns, academic groups and industrial consortiums have recently devoted efforts towards modeling, storage, retrieval, analysis and manipulation of patterns with results mainly in the areas of Inductive Databases and Pattern Base Management Systems (PBMS).

#index 824804
#* Bruce Lindsay speaks out: on System R, benchmarking, life as an IBM fellow, the power of DBAs in the old days, why performance still matters, Heisenbugs, why he still writes code, singing pigs, and more
#@ Marianne Winslett
#t 2005
#c 5
#! Welcome to ACM SIGMOD Record's series of interviews with distinguished members of the database community. I'm Marianne Winslett, and today we're in San Diego at the 2003 SIGMOD and PODS conferences. I have here with me Bruce Lindsay, a member of the research staff at IBM Almaden Research Center. Bruce is well-known for his work on relational databases, which has been very influential both inside and outside of IBM, starting with his work on the System R project. Bruce is an IBM Fellow and his PhD is from Berkeley. So Bruce, welcome!

#index 824805
#* Building data mining solutions with OLE DB for DM and XML for analysis
#@ Zhaohui Tang;Jamie Maclennan;Peter Pyungchul Kim
#t 2005
#c 5
#% 385563
#% 631966
#! A data mining component is included in Microsoft SQL Server 2000 and SQL Server 2005, one of the most popular DBMSs. This gives a push for data mining technologies to move from a niche towards the mainstream. Apart from a few algorithms, the main contribution of SQL Server Data Mining is the implementation of OLE DB for Data Mining. OLE DB for Data mining is an industrial standard led by Microsoft and supported by a number of ISVs. It leverages two existing relational technologies: SQL and OLE DB. It defines a SQL language for data mining based on a relational concept. More recently, Microsoft, Hyperion, SAS and a few other BI vendors formed the XML for Analysis Council. XML for Analysis covers both OLAP and Data Mining. The goal is to allow consumer applications to query various BI packages from different platforms. This paper gives an overview of OLE DB for Data Mining and XML for Analysis. It also shows how to build data mining application using these APIs.

#index 824806
#* Tools for composite web services: a short overview
#@ Richard Hull;Jianwen Su
#t 2005
#c 5
#% 101953
#% 174161
#% 248013
#% 248029
#% 289415
#% 295410
#% 311858
#% 320204
#% 342119
#% 348131
#% 445446
#% 458807
#% 526776
#% 562155
#% 572366
#% 576091
#% 577343
#% 592504
#% 630964
#% 701158
#% 731212
#% 754120
#% 765393
#% 786859
#% 786874
#% 801675
#% 1086662
#% 1389602
#% 1561977
#% 1656081
#! Web services technologies enable flexible and dynamic interoperation of autonomous software and information systems. A central challenge is the development of modeling techniques and tools for eanbling the (semi-)automatic composition and analysis of these services, taking into account their semantic and behavioral properties. This paper presents an overview of the fundamental assumptions and concepts underlying current work on service composition, and provides a sampling of key results in the area. It also provides a brief tour of several composition models including semantic web services, the "Roman" model, and the Mealy / conversation model.

#index 825656
#* Guest editors' introduction to the special section on scientific workflows
#@ Bertram Ludäscher;Carole Goble
#t 2005
#c 5
#% 679607
#! Business-oriented workflows have been studied since the 70's under various names (office automation, workflow management, business process management) and by different communities, including the database community. Much basic and applied research has been conducted over the years, e.g. theoretical studies of workflow languages and models (based on Petri-nets or process calculi), their properties, transactional behavior, etc.

#index 825657
#* Integrating databases and workflow systems
#@ Srinath Shankar;Ameet Kini;David J. DeWitt;Jeffrey Naughton
#t 2005
#c 5
#% 172900
#% 228007
#% 248029
#% 273709
#% 279905
#% 287461
#% 442700
#% 504161
#% 569916
#% 745408
#% 765129
#% 963607
#% 1016179
#% 1655938
#! There has been an information explosion in fields of science such as high energy physics, astronomy, environmental sciences and biology. There is a critical need for automated systems to manage scientific applications and data. Database technology is well-suited to handle several aspects of workflow management. Contemporary workflow systems are built from multiple, separately developed components and do not exploit the full power of DBMSs in handling data of large magnitudes. We advocate a holistic view of a WFMS that includes not only workflow modeling but planning, scheduling, data management and cluster management. Thus, it is worthwhile to explore the ways in which databases can be augmented to manage workflows in addition to data. We present a language for modeling workflows that is tightly integrated with SQL. Each scientific program in a workflow is associated with an active table or view. The definition of data products is in relational format, and invocation of programs and querying is done in SQL. The tight coupling between workflow management and data-manipulation is an advantage for data-intensive scientific programs.

#index 825658
#* An approach for pipelining nested collections in scientific workflows
#@ Timothy M. McPhillips;Shawn Bowers
#t 2005
#c 5
#% 189868
#% 503870
#% 578560
#% 736877
#% 832825
#% 853046
#% 871425
#% 879810
#% 1720925
#! We describe an approach for pipelining nested data collections in scientific workflows. Our approach logically delimits arbitrarily nested collections of data tokens using special, paired control tokens inserted into token streams, and provides workflow components with high-level operations for managing these collections. Our framework provides new capabilities for: (1) concurrent operation on collections; (2) on-the-fly customization of workflow component behavior; (3) improved handling of exceptions and faults; and (4) transparent passing of provenance and metadata within token streams. We demonstrate our approach using a workflow for inferring phylogenetic trees. We also describe future extensions to support richer typing mechanisms for facilitating sharing and reuse of workflow components between disciplines. This work represents a step towards our larger goal of exploiting collection-oriented dataflow programming as a new paradigm for scientific workflow systems, an approach we believe will significantly reduce the complexity of creating and reusing workflows and workflow components.

#index 825659
#* WOODSS and the Web: annotating and reusing scientific workflows
#@ C. B. Medeiros;J. Perez-Alcazar;L. Digiampietri;G. Z. Pastorello, Jr.;A. Santanche;R. S. Torres;E. Madeira;E. Bacarin
#t 2005
#c 5
#% 63875
#% 303396
#% 314738
#% 728758
#% 743353
#% 806635
#% 833049
#% 1374383
#% 1712544
#% 1729883
#! This paper discusses ongoing research on scientific workflows at the Institute of Computing, University of Campinas (IC - UNICAMP) Brazil. Our projects with bio-scientists have led us to develop a scientific workflow infrastructure named WOODSS. This framework has two main objectives in mind: to help scientists to specify and annotate their models and experiments; and to document collaborative efforts in scientific activities. In both contexts, workflows are annotated and stored in a database. This "annotated scientific workflow" database is treated as a repository of (sometimes incomplete) approaches to solving scientific problems. Thus, it serves two purposes: allows comparison of distinct solutions to a problem, and their designs; and provides reusable and executable building blocks to construct new scientific workflows, to meet specific needs. Annotations, moreover, allow further insight into methodology, success rates, underlying hypotheses and other issues in experimental activities.The many research challenges faced by us at the moment include: the extension of this framework to the Web, following Semantic Web standards; providing means of discovering workflow components on the Web for reuse; and taking advantage of planning in Artificial Intelligence to support composition mechanisms. This paper describes our efforts in these directions, tested over two domains - agro-environmental planning and bioinformatics.

#index 825660
#* Simplifying construction of complex workflows for non-expert users of the Southern California Earthquake Center Community Modeling Environment
#@ Philip Maechling;Hans Chalupsky;Maureen Dougherty;Ewa Deelman;Yolanda Gil;Sridhar Gullapalli;Vipin Gupta;Carl Kesselman;Jihic Kim;Gaurang Mehta;Brian Mendenhall;Thomas Russ;Gurmeet Singh;Marc Spraragen;Garrick Staples;Karan Vahi
#t 2005
#c 5
#% 419721
#% 504161
#% 590497
#% 610683
#% 733998
#% 734965
#% 822359
#% 879810
#% 1112384
#! Workflow systems often present the user with rich interfaces that express all the capabilities and complexities of the application programs and the computing environments that they support. However, non-expert users are better served with simple interfaces that abstract away system complexities and still enable them to construct and execute complex workflows. To explore this idea, we have created a set of tools and interfaces that simplify the construction of workflows. Implemented as part of the Community Modeling Environment developed by the Southern California Earthquake Center, these tools, are integrated into a comprehensive workflow system that supports both domain experts as well as non expert users.

#index 825661
#* A survey of data provenance in e-science
#@ Yogesh L. Simmhan;Beth Plale;Dennis Gannon
#t 2005
#c 5
#% 462072
#% 504161
#% 577523
#% 632040
#% 664819
#% 765161
#% 778466
#% 803468
#% 1016204
#% 1069035
#% 1717976
#! Data management is growing in complexity as large-scale applications take advantage of the loosely coupled resources brought together by grid middleware and by abundant storage capacity. Metadata describing the data products used in and generated by these applications is essential to disambiguate the data and enable reuse. Data provenance, one kind of metadata, pertains to the derivation history of a data product starting from its original sources.In this paper we create a taxonomy of data provenance characteristics and apply it to current research efforts in e-science, focusing primarily on scientific workflow approaches. The main aspect of our taxonomy categorizes provenance systems based on why they record provenance, what they describe, how they represent and store provenance, and ways to disseminate it. The survey culminates with an identification of open research problems in the field.

#index 825662
#* A notation and system for expressing and executing cleanly typed workflows on messy scientific data
#@ Yong Zhao;Jed Dobson;Ian Foster;Luc Moreau;Michael Wilde
#t 2005
#c 5
#% 504161
#% 765129
#% 832825
#% 879810
#% 1719169
#! The description, composition, and execution of even logically simple scientific workflows are often complicated by the need to deal with "messy" issues like heterogeneous storage formats and ad-hoc file system structures. We show how these difficulties can be overcome via a typed, compositional workflow notation within which issues of physical representation are cleanly separated from logical typing, and by the implementation of this notation within the context of a powerful runtime system that supports distributed execution. The resulting notation and system are capable both of expressing complex workflows in a simple, compact form, and of enacting those workflows in distributed environments. We apply our technique to cognitive neuroscience workflows that analyze functional MRI image data, and demonstrate significant reductions in code size relative to other approaches.

#index 825663
#* A taxonomy of scientific workflow systems for grid computing
#@ Jia Yu;Rajkumar Buyya
#t 2005
#c 5
#% 66718
#% 261139
#% 347174
#% 590497
#% 610848
#% 785154
#% 800867
#% 808590
#% 822361
#% 832825
#% 1719023
#! With the advent of Grid and application technologies, scientists and engineers are building more and more complex applications to manage and process large data sets, and execute scientific experiments on distributed resources. Such application scenarios require means for composing and executing complex workflows. Therefore, many efforts have been made towards the development of workflow management systems for Grid computing. In this paper, we propose a taxonomy that characterizes and classifies various approaches for building and executing workflows on Grids. The taxonomy not only highlights the design and engineering similarities and differences of state-of-the-art in Grid workflow systems, but also identifies the areas that need further research.

#index 825664
#* XML database support for distributed execution of data-intensive scientific workflows
#@ Shannon Hastings;Matheus Ribeiro;Stephen Langella;Scott Oster;Umit Catalyurek;Tony Pan;Kun Huang;Renato Ferreira;Joel Saltz;Tahsin Kurc
#t 2005
#c 5
#% 378384
#% 504161
#% 610845
#% 633902
#% 657654
#% 848842
#% 1719169
#! In this paper we look at the application of XML data management support in scientific data analysis workflows. We describe a software infrastructure that aims to address issues associated with metadata management, data storage and management, and execution of data analysis workflows on distributed storage and compute platforms. This system couples a distributed, filter-stream based dataflow engine with a distributed XML-based data and metadata management system. We present experimental results from a biomedical image analysis use case that involves processing of digitized microscopy images for feature segmentation.

#index 825665
#* Scheduling of scientific workflows in the ASKALON grid environment
#@ Marek Wieczorek;Radu Prodan;Thomas Fahringer
#t 2005
#c 5
#% 369236
#% 743756
#% 810563
#% 897416
#% 1719165
#! Scheduling is a key concern for the execution of performance-driven Grid applications. In this paper we comparatively examine different existing approaches for scheduling of scientific workflow applications in a Grid environment. We evaluate three algorithms namely genetic, HEFT, and simple "myopic" and compare incremental workflow partitioning against the full-graph scheduling strategy. We demonstrate experiments using real-world scientific applications covering both balanced (symmetric) and unbalanced (asymmetric) workflows. Our results demonstrate that full-graph scheduling with the HEFT algorithm performs best compared to the other strategies examined in this paper.

#index 825666
#* Efficient calendar based temporal association rule
#@ Keshri Verma;O. P. Vyas
#t 2005
#c 5
#% 390187
#% 450539
#% 464839
#% 466490
#% 477983
#% 479627
#% 481290
#! Associationship is an important component of data mining. In real world data the knowledge used for mining rule is almost time varying. The item have the dynamic characteristic in terms of transaction, which have seasonal selling rate and it hold time-based associationship with another item. It is also important that in database, some items which are infrequent in whole dataset but those may be frequent in a particular time period. If these items are ignored then associationship WVW200R3100221-398 result will no longer be accurate. To restrict the time based associationship calendar based pattern can be used [YPXS03]. A calendar unit such as months and days, clock units, such as hours and seconds & specialized units, such as business days and academic years, play a major role in a wide range of information system applications[BX00].Most of the popular associationship rule mining methods are having performance bottleneck for database with different characteristics. Some of the methods are efficient for sparse dataset where as some are good for a dense dataset. Our focus is to find effective time sensitive algorithm using H-struct called temporal H-mine, which takes the advantage of this data structure and dynamically adjusts links in the mining process [PHNTY01]. It is faster in traversing & advantage of precisely predictable spaces overhead. It can be scaled up to large database by database partitioning, end when dataset becomes dense, conditionally temporal FP-tree. can be constructed dynamically as part of mining.

#index 825667
#* Artemis message exchange framework: semantic interoperability of exchanged messages in the healthcare domain
#@ Veli Bicer;Gokce B. Laleci;Asuman Dogac;Yildiray Kabak
#t 2005
#c 5
#% 459496
#! One of the most challenging problems in the healthcare domain is providing interoperability among healthcare information systems. In order to address this problem, we propose the semantic mediation of exchanged messages. Given that most of the messages exchanged in the healthcare domain are in EDI (Electronic Data Interchange) or XML format, we describe how to transform these messages into OWL (Web Ontology Language) ontology instances. The OWL message instances are then mediated through an ontology mapping tool that we developed, namely, OWLmt. OWLmt uses OWL-QL engine which enables the mapping tool to reason over the source ontology instances while generating the target ontology instances according to the mapping patterns defined through a GUI.Through a prototype implementation, we demonstrate how to mediate between HL7 Version 2 and HL7 Version 3 messages. However, the framework proposed is generic enough to mediate between any incompatible healthcare standards that are currently in use.

#index 825668
#* Database research at Bilkent University
#@ Özgür Ulusoy
#t 2005
#c 5
#% 311370
#% 443559
#% 452675
#% 511668
#% 575697
#% 614658
#% 733375
#% 740595
#% 776574
#% 788942
#% 791178
#% 823280
#% 827132
#% 993936
#% 1016274
#% 1855594
#! This report provides a brief description of the research activities of the Database Research Group of Bilkent University. The current research of the group is mainly focused on the topics of Multimedia Databases (in particular video database management and content-based retrieval of document images), Web Databases (in particular the modeling and querying of Web resources and focused crawling), and Mobile Computing (in particular moving object processing and mobile data management).

#index 825669
#* Data management research at the Middle East Technical University
#@ Nihan K. Cicekli;Ahmet Cosar;Asuman Dogac;Faruk Polat;Pinar Senkul;I. Hakki Toroslu;Adnan Yazici
#t 2005
#c 5
#% 329613
#% 330924
#% 398009
#% 460917
#% 622415
#% 721133
#% 737252
#% 752453
#% 762819
#% 766312
#% 783786
#% 790688
#% 803120
#% 808740
#% 814210
#% 819792
#% 825667
#% 833684
#% 873466
#% 967411
#% 993989
#% 1049664
#% 1345684
#% 1731121
#% 1788477
#! The Middle East Technical University (METU) (http://www.metu.edu.tr) is the leading technical university in Turkey. The department of Computer Engineering (http://www.ceng.metu.edu.tr) has twenty seven faculty members with PhDs, 550 undergraduate students and 165 graduate-students. The major research funding sources include the Scientific and Technical Research Council of Turkey (TÜBÍTAK), the European Commission, and the internal research funds of METU. Data management research conducted in the department is summarized in this article.

#index 825670
#* Report on the workshop on wrapper techniques for legacy data systems
#@ Ph. Thiran;T. Risch;C. Costilla;J. Henrard;Th. Kabisch;J. Petrini;W-J. van den Heuvel;J-L. Hainaut
#t 2005
#c 5
#! This report summarizes the presentations and discussions of the first workshop on Wrapper Techniques for Legacy Data Systems which was held in Delft on November 12 2004. This workshop was co-located with the 2004 WCRE conference. This workshop entails to our best knowledge the first in its kind, concentrating on challenging research issues regarding the development of wrappers for legacy data systems.

#index 825671
#* Exchange, integration, and consistency of data: report on the ARISE/NISR workshop
#@ Leopoldo Bertossi;Jan Chomicki;Parke Godfrey;Phokion G. Kolaitis;Alex Thomo;Calisto Zuzarte
#t 2005
#c 5
#% 2655
#% 273687
#% 273700
#% 350105
#% 465052
#% 465057
#% 519568
#% 576097
#% 576100
#% 576116
#% 579612
#% 632039
#% 727668
#% 752741
#% 801676
#% 814475
#% 993981
#% 1015302
#% 1015326
#% 1705008
#! The "ARISE/NISR Workshop on Exchange and Integration of Data" was held at the IBM Center for Advanced Studies, Toronto Lab., between October 7-9, 2004.

#index 825672
#* Query answering exploiting structural properties
#@ Francesco Scarcello
#t 2005
#c 5
#% 451
#% 2028
#% 2657
#% 36683
#% 101922
#% 142111
#% 159244
#% 201885
#% 237180
#% 248038
#% 286995
#% 287031
#% 289424
#% 299969
#% 303886
#% 321058
#% 331899
#% 338450
#% 339937
#% 384978
#% 387508
#% 398173
#% 427161
#% 598376
#% 599549
#% 643572
#% 644201
#% 723931
#% 801688
#% 836134
#% 1709984
#! We review the notion of hypertree width, a measure of the degree of cyclicity of hypergraphs that is useful for identifying and solving efficiently easy instances of hard problems, by exploiting their structural properties. Indeed, a number of relevant problems from different areas, such as database theory, artificial intelligence, and game theory, are tractable when their underlying hypergraphs have small (i.e., bounded by some fixed constant) hypertree width. In particular, we describe how this notion may be used for identifying tractable classes of database queries and answering such queries in an efficient way.

#index 825673
#* John Wilkes speaks out: on what the DB community needs to know about storage, how the DB and storage communities can join forces and change the world, and more
#@ Marianne Winslett
#t 2005
#c 5
#! Welcome to this installment of ACM SIGMOD Record's series of interviews with distinguished members of the database community. I'm Marianne Winslett, and today [February 2004] we are at the Department of Computer Science at the University of Illinois at Urbana-Champaign. I have here with me John Wilkes, who is an HP Fellow in the Internet Systems and Storage Laboratory at Hewlett Packard Laboratories in Palo Alto, California, where his research focuses on the design and management of storage systems. John is a member of the editorial board of ACM Transactions on Computer Systems, and until recently he was a member of the Technical Council of the Storage Network Industry Association. John is an ACM Fellow, and his PhD is from the University of Cambridge. So, John, welcome!

#index 825674
#* CMM and TODS
#@ Richard Snodgrass
#t 2005
#c 5
#% 187119
#% 342985
#% 361209
#% 723440
#! The Capability Maturity Model [4] is an orderly way for organizations to determine the capabilities of their current processes for developing software and to establish priorities for improvement [2]. It defines five levels of progressively more mature process capability [3].

#index 845348
#* Information source selection for resource constrained environments
#@ Demet Aksoy
#t 2005
#c 5
#% 194246
#% 194275
#% 227891
#% 262096
#% 267454
#% 273926
#% 280856
#% 287463
#% 301225
#% 309133
#% 340146
#% 413594
#% 447946
#% 567255
#% 660301
#% 665561
#% 665642
#% 751050
#% 993964
#! Distributed information retrieval has pressing scalability concerns due to the growing number of independent sources of on-line data and the emerging applications. A promising solution to distributed retrieval is metasearching, which dispatches a user's query to multiple sources and gathers the results into a single result set. An important component of metasearching is selecting the set of information sources most likely to provide relevant documents. Recent research has focused on how to obtain statistics for the selection task. In this paper we discuss different information source selection approaches and their applicability for resource-constrained sensor network applications.

#index 845349
#* LiXQuery: a formal foundation for XQuery research
#@ Jan Hidders;Philippe Michiels;Jan Paredaens;Roel Vercammen
#t 2005
#c 5
#% 333979
#% 427874
#% 465065
#% 576108
#% 643790
#% 770338
#% 801669
#% 801670
#% 814647
#! XQuery is considered to become the standard query language for XML documents. However, the complete XQuery syntax and semantics seem too complicated for research and educational purposes. By defining a concise backwards compatible subset of XQuery with a complete formal description, we provide a practical foundation for XQuery research. We pay special attention to usability by supporting the most typical XQuery expressions.

#index 845350
#* From databases to dataspaces: a new abstraction for information management
#@ Michael Franklin;Alon Halevy;David Maier
#t 2005
#c 5
#% 275367
#% 805821
#! The development of relational database management systems served to focus the data management community for decades, with spectacular results. In recent years, however, the rapidly-expanding demands of "data everywhere" have led to a field comprised of interesting and productive efforts, but without a central focus or coordinated agenda. The most acute information management challenges today stem from organizations (e.g., enterprises, government agencies, libraries, "smart" homes) relying on a large number of diverse, interrelated data sources, but having no way to manage their dataspaces in a convenient, integrated, or principled fashion. This paper proposes dataspaces and their support systems as a new agenda for data management. This agenda encompasses much of the work going on in data management today, while posing additional research objectives.

#index 845351
#* Scientific data management in the coming decade
#@ Jim Gray;David T. Liu;Maria Nieto-Santisteban;Alex Szalay;David J. DeWitt;Gerd Heber
#t 2005
#c 5
#% 115661
#! Scientific instruments and computer simulations are creating vast data stores that require new scientific methods to analyze and organize the data. Data volumes are approximately doubling each year. Since these new instruments have extraordinary precision, the data quality is also rapidly improving. Analyzing this data to find the subtle effects missed by previous studies requires algorithms that can simultaneously deal with huge datasets and that can find very subtle effects --- finding both needles in the haystack and finding very small haystacks that were undetected in previous measurements.

#index 845352
#* The 8 requirements of real-time stream processing
#@ Michael Stonebraker;Uǧur Çetintemel;Stan Zdonik
#t 2005
#c 5
#% 1797
#% 654507
#% 726621
#% 993949
#% 1016169
#! Applications that require real-time processing of high-volume data steams are pushing the limits of traditional data processing infrastructures. These stream-based applications include market feed processing and electronic trading on Wall Street, network and infrastructure monitoring, fraud detection, and command and control in military environments. Furthermore, as the "sea change" caused by cheap micro-sensor technology takes hold, we expect to see everything of material significance on the planet get "sensor-tagged" and report its state or location in real time. This sensorization of the real world will lead to a "green field" of novel monitoring and control applications with high-volume and low-latency processing requirements.Recently, several technologies have emerged---including off-the-shelf stream processing engines---specifically to address the challenges of processing high-volume, real-time data without requiring the use of custom code. At the same time, some existing software technologies, such as main memory DBMSs and rule engines, are also being "repurposed" by marketing departments to address these applications.In this paper, we outline eight requirements that a system software should meet to excel at a variety of real-time stream processing applications. Our goal is to provide high-level guidance to information technologists so that they will know what to look for when evaluation alternative stream processing solutions. As such, this paper serves a purpose comparable to the requirements papers in relational DBMSs and on-line analytical processing. We also briefly review alternative system software technologies in the context of our requirements.The paper attempts to be vendor neutral, so no specific commercial products are mentioned.

#index 845353
#* Citation analysis of database publications
#@ Erhard Rahm;Andreas Thor
#t 2005
#c 5
#% 723439
#% 723440
#% 824757
#! We analyze citation frequencies for two main database conferences (SIGMOD, VLDB) and three database journals (TODS, VLDB Journal, Sigmod Record) over 10 years. The citation data is obtained by integrating and cleaning data from DBLP and Google Scholar. Our analysis considers different comparative metrics per publication venue, in particular the total and average number of citations as well as the impact factor which has so far only been considered for journals. We also determine the most cited papers, authors, author institutions and their countries.

#index 845354
#* A citation-based system to assist prize awarding
#@ Antonis Sidiropoulos;Yannis Manolopoulos
#t 2005
#c 5
#% 268079
#% 290830
#% 311868
#% 340674
#% 451536
#% 791197
#% 801834
#% 1394202
#! Citation analysis is performed to evaluate the impact of scientific collections (journals and conferences), publications and scholar authors. In this paper we investigate alternative methods to provide a generalized approach to rank scientific publications. We use the SCEAS system [12] as a base platform to introduce new methods that can be used for ranking scientific publications. Moreover, we tune our approach along the reasoning of the prizes 'VLDB 10 Year Award' and 'SIGMOD Test of Time Award', which have been awarded in the course of the top two database conferences. Our approach can be used to objectively suggest the publications and the respective authors the are more likely to be awarded in the near future at these conferences.

#index 845355
#* An apples-to-apples comparison of two database journals
#@ Philip A. Bernstein;Elisa Bertino;Andreas Heuer;Christian S. Jensen;Holger Meyer;M. Tamer Özsu;Richard T. Snodgrass;Kyu-Young Whang
#t 2005
#c 5
#% 723440
#% 824757
#! This paper defines a collection of metrics on manuscript reviewing and presents historical data for ACM Transactions on Database Systems and The VLDB Journal.

#index 845356
#* In memoriam Alberto Oscar Mendelzon
#@ Renèe J. Miller
#t 2005
#c 5
#! Alberto Oscar Mendelzon passed away on June 16, 2005 after a two-year battle with cancer. This tribute to Alberto and his achievements is written in recognition of his great intellect and his generous friendship. Both have influenced and inspired many in the database research community.

#index 845357
#* Tips on giving a good demo
#@ Mary Fernández
#t 2005
#c 5
#% 810095
#% 810101
#% 810108
#! For the first time this year, a "Best Demonstrations" session was included in the SIGMOD program. The first two days of the program included 24 demonstrations, each of which was presented during two of six interactive demo sessions. During the first two days, panels of three judges visited each demo group, each of whom was allotted 15 minutes to present their system to the judges. The friendly competition made for very exciting and noisy demo sessions!

#index 845358
#* Data management research at Technische Universität Darmstadt
#@ A. Buchmann;M. Cilia
#t 2005
#c 5
#% 338354
#% 488406
#% 544951
#% 559129
#% 591579
#% 656500
#% 663054
#% 665399
#% 781482
#% 831839
#% 841860
#% 875205
#% 884946
#% 994030
#% 1180865
#% 1180875
#% 1712583
#! The Databases and Distributed Systems Group at Technische Universität Darmstadt is devoted to research in the areas of data management middleware and reactive, event-based systems. Special emphasis is placed on handling the flow of data and events in a variety of environments: publish/subscribe mechanisms, information dissemination and integration, ubiquitous computing, peer-to-peer infrastructures, and a variety of sensor-based systems ranging from passive RFID infrastructures to active wireless sensor networks. A special concern is placed on non-functional aspects of the middleware, such as performance, scalability and security, where members of our group are involved in the definition of the SPEC family of benchmarks for J2EE (SPECjAppServer200x) and JMS.

#index 845359
#* Report on the DB/IR panel at SIGMOD 2005
#@ Sihem Amer-Yahia;Pat Case;Thomas Rölleke;Jayavel Shanmugasundaram;Gerhard Weikum
#t 2005
#c 5
#% 35949
#% 86371
#% 194247
#% 215225
#% 228817
#% 248801
#% 249160
#% 309726
#% 321635
#% 322880
#% 333981
#% 340914
#% 342708
#% 387427
#% 406493
#% 428409
#% 458828
#% 458829
#% 504581
#% 642993
#% 654441
#% 654442
#% 660011
#% 719598
#% 750867
#% 754116
#% 765408
#% 765423
#% 765466
#% 772029
#% 799737
#% 800508
#% 824681
#% 824703
#% 1015258
#% 1015325
#% 1016176
#% 1016203
#% 1016242
#% 1715599
#! This paper summarizes the salient aspects of the SIGMOD 2005 panel on "Databases and Information Retrieval: Rethinking the Great Divide". The goal of the panel was to discuss whether we should rethink data management systems architectures to truly merge Database (DB) and Information Retrieval (IR) technologies. The panel had very high attendance and generated lively discussions.

#index 845360
#* Report on the first IEEE international workshop on networking meets databases (NetDB'05)
#@ Cyrus Shahabi;Ramesh Govindan;Karl Aberer
#t 2005
#c 5
#! In this report, to the best of our ability, we try to summarize the presentations and discussions occurred within the First IEEE International Workshop on Networking Meets Databases (NetDB) which was held in Tokyo Japan on April 8th and 9th, 2005. NetDB was one of the many (11 to be exact) satellite workshops of the IEEE ICDE (International Conference on Data Engineering) 2005 conference. This workshop is part of the very few initiatives in bringing the networking and database communities together. The focus research areas of NetDB 2005 were sensor and peer-to-peer networks.

#index 845361
#* XQuery 1.0 is nearing completion
#@ Andrew Eisenberg;Jim Melton
#t 2005
#c 5
#% 449227
#! XQuery is a query language designed for querying real and virtual XML documents and collections of these documents. Its development began in the second half of 1999. We provided an early look at XQuery in Dec. 2002 [1]. XQuery 1.0 is now approaching its publication as a W3C Recommendation, and we would like to update you on its progress. We can speak to this area with even more authority than we did last time, as we both became co-chairs of the W3C XML Query Working Group [2] in summer 2004.

#index 845362
#* Christos Faloutsos speaks out: on power laws, fractals, the future of data mining, sabbaticals, and more
#@ Marianne Winslett
#t 2005
#c 5
#! Welcome to this installment of ACM SIGMOD Record's series of interviews with distinguished members of the database community. I'm Marianne Winslett, and today I have here with me Christos Faloutsos, who is a professor of computer science at Carnegie Mellon University. Christos recieved the Presidential Young Investigator Award from the National Science Foundation in 1989. He received the 1997 VLDB Ten Year Paper Award for his paper on R+ trees, and the SIGMOD 1994 Best Paper Award for a paper on fast subsequence matching in time series databases. Christos is a member of the SIGKDD Executive Committee, and he has wide-ranging interests in data mining, database performance, and spatial and multimedia databases. His PhD is from the University of Toronto. So, Christos, welcome!

#index 850727
#* Wavelet synopses for general error metrics
#@ Minos Garofalakis;Amit Kumar
#t 2005
#c 5
#% 116084
#% 168862
#% 190611
#% 227883
#% 248822
#% 257637
#% 273902
#% 273909
#% 273919
#% 300193
#% 333946
#% 397389
#% 465064
#% 480306
#% 480465
#% 480628
#% 480810
#% 572308
#% 654460
#% 742562
#% 765451
#% 801684
#% 1015256
#! Several studies have demonstrated the effectiveness of the wavelet decomposition as a tool for reducing large amounts of data down to compact wavelet synopses that can be used to obtain fast, accurate approximate query answers. Conventional wavelet synopses that greedily minimize the overall root-mean-squared (i.e., L2-norm) error in the data approximation can suffer from important problems, including severe bias and wide variance in the quality of the data reconstruction, and lack of nontrivial guarantees for individual approximate answers. Thus, probabilistic thresholding schemes have been recently proposed as a means of building wavelet synopses that try to probabilistically control maximum approximation-error metrics (e.g., maximum relative error).A key open problem is whether it is possible to design efficient deterministic wavelet-thresholding algorithms for minimizing general, non-L2 error metrics that are relevant to approximate query processing systems, such as maximum relative or maximum absolute error. Obviously, such algorithms can guarantee better maximum-error wavelet synopses and avoid the pitfalls of probabilistic techniques (e.g., “bad” coin-flip sequences) leading to poor solutions; in addition, they can be used to directly optimize the synopsis construction process for other useful error metrics, such as the mean relative error in data-value reconstruction. In this article, we propose novel, computationally efficient schemes for deterministic wavelet thresholding with the objective of optimizing general approximation-error metrics. We first consider the problem of constructing wavelet synopses optimized for maximum error, and introduce an optimal low polynomial-time algorithm for one-dimensional wavelet thresholding---our algorithm is based on a new Dynamic-Programming (DP) formulation, and can be employed to minimize the maximum relative or absolute error in the data reconstruction. Unfortunately, directly extending our one-dimensional DP algorithm to multidimensional wavelets results in a super-exponential increase in time complexity with the data dimensionality. Thus, we also introduce novel, polynomial-time approximation schemes (with tunable approximation guarantees) for deterministic wavelet thresholding in multiple dimensions. We then demonstrate how our optimal and approximate thresholding algorithms for maximum error can be extended to handle a broad, natural class of distributive error metrics, which includes several important error measures, such as mean weighted relative error and weighted Lp-norm error. Experimental results on real-world and synthetic data sets evaluate our novel optimization algorithms, and demonstrate their effectiveness against earlier wavelet-thresholding schemes.

#index 850728
#* Conditional XPath
#@ Maarten Marx
#t 2005
#c 5
#% 168262
#% 191611
#% 230455
#% 299976
#% 338753
#% 357150
#% 378393
#% 390685
#% 399031
#% 465065
#% 473117
#% 473125
#% 473422
#% 483548
#% 487257
#% 510128
#% 544561
#% 576108
#% 587436
#% 599549
#% 717512
#% 733595
#% 801669
#% 801686
#% 821610
#% 824798
#% 993939
#% 1700125
#! XPath 1.0 is a variable free language designed to specify paths between nodes in XML documents. Such paths can alternatively be specified in first-order logic. The logical abstraction of XPath 1.0, usually called Navigational or Core XPath, is not powerful enough to express every first-order definable path. In this article, we show that there exists a natural expansion of Core XPath in which every first-order definable path in XML document trees is expressible. This expansion is called Conditional XPath. It contains additional axis relations of the form (child::n[F])&plus;, denoting the transitive closure of the path expressed by child::n[F]. The difference with XPath's descendant::n[F] is that the path (child::n[F])&plus; is conditional on the fact that all nodes in between the start and end node of the path should also be labeled by n and should make the predicate F true. This result can be viewed as the XPath analogue of the expressive completeness of the relational algebra with respect to first-order logic.

#index 850729
#* Graph indexing based on discriminative frequent structure analysis
#@ Xifeng Yan;Philip S. Yu;Jiawei Han
#t 2005
#c 5
#% 344549
#% 378391
#% 397359
#% 435374
#% 443133
#% 466644
#% 479465
#% 480656
#% 481290
#% 481754
#% 481779
#% 601159
#% 629603
#% 629646
#% 629708
#% 654452
#% 729938
#% 729942
#% 731608
#% 1015336
#! Graphs have become increasingly important in modelling complicated structures and schemaless data such as chemical compounds, proteins, and XML documents. Given a graph query, it is desirable to retrieve graphs quickly from a large database via indices. In this article, we investigate the issues of indexing graphs and propose a novel indexing model based on discriminative frequent structures that are identified through a graph mining process. We show that the compact index built under this model can achieve better performance in processing graph queries. Since discriminative frequent structures capture the intrinsic characteristics of the data, they are relatively stable to database updates, thus facilitating sampling-based feature extraction and incremental index maintenance. Our approach not only provides an elegant solution to the graph indexing problem, but also demonstrates how database indexing and query processing can benefit from data mining, especially frequent pattern mining. Furthermore, the concepts developed here can be generalized and applied to indexing sequences, trees, and other complicated structures as well.

#index 850730
#* Composing schema mappings: Second-order dependencies to the rescue
#@ Ronald Fagin;Phokion G. Kolaitis;Lucian Popa;Wang-Chiew Tan
#t 2005
#c 5
#% 583
#% 150197
#% 248038
#% 262718
#% 289384
#% 378409
#% 384978
#% 465057
#% 480134
#% 488616
#% 576100
#% 577359
#% 801676
#% 993981
#% 1015302
#! A schema mapping is a specification that describes how data structured under one schema (the source schema) is to be transformed into data structured under a different schema (the target schema). A fundamental problem is composing schema mappings: given two successive schema mappings, derive a schema mapping between the source schema of the first and the target schema of the second that has the same effect as applying successively the two schema mappings.In this article, we give a rigorous semantics to the composition of schema mappings and investigate the definability and computational complexity of the composition of two schema mappings. We first study the important case of schema mappings in which the specification is given by a finite set of source-to-target tuple-generating dependencies (source-to-target tgds). We show that the composition of a finite set of full source-to-target tgds with a finite set of tgds is always definable by a finite set of source-to-target tgds, but the composition of a finite set of source-to-target tgds with a finite set of full source-to-target tgds may not be definable by any set (finite or infinite) of source-to-target tgds; furthermore, it may not be definable by any formula of least fixed-point logic, and the associated composition query may be NP-complete. After this, we introduce a class of existential second-order formulas with function symbols and equalities, which we call second-order tgds, and make a case that they are the “right” language for composing schema mappings. Specifically, we show that second-order tgds form the smallest class (up to logical equivalence) that contains every source-to-target tgd and is closed under conjunction and composition. Allowing equalities in second-order tgds turns out to be of the essence, even though the “obvious” way to define second-order tgds does not require equalities. We show that second-order tgds without equalities are not sufficiently expressive to define the composition of finite sets of source-to-target tgds. Finally, we show that second-order tgds possess good properties for data exchange and query answering: the chase procedure can be extended to second-order tgds so that it produces polynomial-time computable universal solutions in data exchange settings specified by second-order tgds.

#index 850731
#* Optimization of query streams using semantic prefetching
#@ Ivan T. Bowman;Kenneth Salem
#t 2005
#c 5
#% 36117
#% 215316
#% 273915
#% 333935
#% 334006
#% 335725
#% 461897
#% 479950
#% 479955
#% 572305
#% 753710
#% 842312
#! Streams of relational queries submitted by client applications to database servers contain patterns that can be used to predict future requests. We present the Scalpel system, which detects these patterns and optimizes request streams using context-based predictions of future requests. Scalpel uses its predictions to provide a form of semantic prefetching, which involves combining a predicted series of requests into a single request that can be issued immediately. Scalpel's semantic prefetching reduces not only the latency experienced by the application but also the total cost of query evaluation. We describe how Scalpel learns to predict optimizable request patterns by observing the application's request stream during a training phase. We also describe the types of query pattern rewrites that Scalpels cost-based optimizer considers. Finally, we present empirical results that show the costs and benefits of Scalpel's optimizations.

#index 850732
#* Synopses for query optimization: A space-complexity perspective
#@ Raghav Kaushik;Jeffrey F. Naughton;Raghu Ramakrishnan;Venkatesan T. Chakravarthy
#t 2005
#c 5
#% 554
#% 43163
#% 82346
#% 102784
#% 116084
#% 152585
#% 201921
#% 210190
#% 214073
#% 238182
#% 248014
#% 248821
#% 273682
#% 273902
#% 273908
#% 273909
#% 285924
#% 299982
#% 300193
#% 333947
#% 378413
#% 396104
#% 397354
#% 397371
#% 408237
#% 411554
#% 427219
#% 479648
#% 479931
#% 480306
#% 480948
#% 481266
#% 482123
#% 600560
#% 859116
#% 1015256
#% 1810782
#! Database systems use precomputed synopses of data to estimate the cost of alternative plans during query optimization. A number of alternative synopsis structures have been proposed, but histograms are by far the most commonly used. While histograms have proved to be very effective in (cost estimation for) single-table selections, queries with joins have long been seen as a challenge; under a model where histograms are maintained for individual tables, a celebrated result of Ioannidis and Christodoulakis [1991] observes that errors propagate exponentially with the number of joins in a query.In this article, we make two main contributions. First, we study the space complexity of using synopses for query optimization from a novel information-theoretic perspective. In particular, we offer evidence in support of histograms for single-table selections, including an analysis over data distributions known to be common in practice, and illustrate their limitations for join queries. Second, for a broad class of common queries involving joins (specifically, all queries involving only key-foreign key joins) we show that the strategy of storing a small precomputed sample of the database yields probabilistic guarantees that are almost space-optimal, which is an important property if these samples are to be used as database statistics. This is the first such optimality result, to our knowledge, and suggests that precomputed samples might be an effective way to circumvent the error propagation problem for queries with key-foreign key joins. We support this result empirically through an experimental study that demonstrates the effectiveness of precomputed samples, and also shows the increasing difference in the effectiveness of samples versus multidimensional histograms as the number of joins in the query grows.

#index 857492
#* Exploiting predicate-window semantics over data streams
#@ Thanaa M. Ghanem;Walid G. Aref;Ahmed K. Elmagarmid
#t 2006
#c 5
#% 116082
#% 578391
#% 810033
#% 853011
#% 1015296
#! The continuous sliding-window query model is used widely in data stream management systems where the focus of a continuous query is limited to a set of the most recent tuples. In this paper, we show that an interesting and important class of queries over data streams cannot be answered using the sliding-window query model. Thus, we introduce a new model for continuous window queries, termed the predicate-window query model that limits the focus of a continuous query to the stream tuples that qualify a certain predicate. Predicate-window queries have some distinguishing characteristics, e.g., (1) The window predicate can be defined over any attribute in the stream tuple (ordered or unordered). (2) Stream tuples qualify and disqualify the window predicate in an out-of-order manner. In this paper, we discuss the applicability of the predicate-window query model. We will show how the existing sliding-window query models fail to answer some of the predicate-window queries. Finally, we discuss the challenges in supporting the predicate-window query model in data stream management systems.

#index 857493
#* Micro-views, or on how to protect privacy while enhancing data usability: concepts and challenges
#@ Ji-Won Byun;Elisa Bertino
#t 2006
#c 5
#% 67453
#% 91075
#% 102749
#% 261360
#% 496009
#% 576761
#% 576762
#% 577238
#% 662424
#% 808349
#% 810014
#% 993943
#% 1016138
#% 1548044
#! The large availability of repositories storing various types of information about individuals has raised serious privacy concerns over the past decade. Nonetheless, database technology is far from providing adequate solutions to this problem that requires a delicate balance between an individual's privacy and convenience and data usability by enterprises and organizations - a database which is rigid and over-protective may render data of little value. Though these goals may seem odd, we argue that the development of solutions able to reconcile them will be an important challenge to be addressed in the next few years. We believe that the next wave of database technology will be represented by a DBMS that provides high-assurance privacy and security. In this paper, we elaborate on such challenges. In particular, we argue that we need to provide different views of data at a very fine level of granularity; conventional view technology is able to select only up to a single attribute value for a single tuple. We need to go even beyond this level. That is, we need a mechanism by which even a single value inside a tuple's attribute may have different views; we refer them as micro-views. We believe that such a mechanism can be an important building block, together with other mechanisms and tools, of the next wave of database technology.

#index 857494
#* Research issues in data stream association rule mining
#@ Nan Jiang;Le Gruenwald
#t 2006
#c 5
#% 152934
#% 287242
#% 300120
#% 310525
#% 338425
#% 342666
#% 342715
#% 464204
#% 481290
#% 511333
#% 548479
#% 569754
#% 629681
#% 727846
#% 727891
#% 729932
#% 729959
#% 730046
#% 751684
#% 755090
#% 765494
#% 785339
#% 789007
#% 806217
#% 824795
#% 993960
#% 993961
#% 1016146
#% 1390195
#% 1781523
#% 1781524
#! There exist emerging applications of data streams that require association rule mining, such as network traffic monitoring and web click streams analysis. Different from data in traditional static databases, data streams typically arrive continuously in high speed with huge amount and changing data distribution. This raises new issues that need to be considered when developing association rule mining techniques for stream data. This paper discusses those issues and how they are addressed in the existing literature.

#index 857495
#* Join minimization in XML-to-SQL translation: an algebraic approach
#@ Murali Mani;Song Wang;Dan Dougherty;Elke A. Rundensteiner
#t 2006
#c 5
#% 32891
#% 116043
#% 137867
#% 190638
#% 287336
#% 411759
#% 413581
#% 416034
#% 428149
#% 461897
#% 462058
#% 480657
#% 564416
#% 599549
#% 1016141
#! Consider an XML view defined over a relational database, and a user query specified over this view. This user XML query is typically processed using the following steps: (a) our translator maps the XML query to one or more SQL queries, (b) the relational engine translates an SQL query to a relational algebra plan, (c) the relational engine executes the algebra plan and returns SQL results, and (d) our translator translates the SQL results back to XML. However, a straightforward approach produces a relational algebra plan after step (b) that is inefficient and has redundant joins. In this paper, we report on our preliminary observations with respect to how joins in such a relational algebra plan can be minimized. Our approach works on the relational algebra plan and optimizes it using novel rewrite rules that consider pairs of joins in the plan and determine whether one of them is redundant and hence can be removed. Our study shows that algebraic techniques achieve effective join minimization, and such techniques are useful and can be integrated into mainstream SQL engines.

#index 857496
#* Dynamic count filters
#@ J. Aguilar-Saborit;P. Trancoso;V. Muntes-Mulero;J. L. Larriba-Pey
#t 2006
#c 5
#% 59286
#% 307424
#% 319037
#% 322884
#% 479795
#% 646223
#% 654461
#% 993960
#% 1016149
#! Bloom filters are not able to handle deletes and inserts on multisets over time. This is important in many situations when streamed data evolve rapidly and change patterns frequently. Counting Bloom Filters (CBF) have been proposed to overcome this limitation and allow for the dynamic evolution of Bloom filters. The only dynamic approach to a compact and efficient representation of CBF are the Spectral Bloom Filters (SBF).In this paper we propose the Dynamic Count Filters (DCF) as a new dynamic and space-time efficient representation of CBF. Although DCF does not make a compact use of memory, it shows to be faster and more space efficient than any previous proposal. Results show that the proposed data structure is more efficient independently of the incoming data characteristics.

#index 857497
#* Towards a dynamic multi-policy dissemination control model: (DMDCON)
#@ Zude Li;Xiaojun Ye
#t 2006
#c 5
#% 204453
#% 342328
#% 345972
#% 583821
#% 663870
#% 664539
#% 750874
#% 764977
#% 808349
#% 1016138
#! Dissemination control (DCON) is a security policy of controlling digital resource access before and after distribution. It is an extension of traditional access control within client-side domain, digital rights management by payment-free applications, and originator control on recipients' re-dissemination rights allowance. Different application domains may adopt dynamically different resource dissemination policies, but current DCON models cannot solve the multi-policy coexistence and compatibility problems. A dynamic multi-policy dissemination control model (DMDCON) is proposed to express the dynamic and multi-policy nature existing in reality, which are indispensable for well formed resource dissemination control application. The goal of this paper is to define and extend formally some basic concepts related with resource dissemination (such as dissemination policy, chain, tree, etc.) and further, propose a comprehensive DMDCON model to describe universal resource dissemination applications through specifying temporal dissemination features, restrictions, and policy revocation (cascade or non-cascade). Finally, we briefly discuss the importance of DCON within the usage control domain.

#index 857498
#* B-tree indexes for high update rates
#@ Goetz Graefe
#t 2006
#c 5
#% 43172
#% 57333
#% 208047
#% 287672
#% 322412
#% 465149
#% 479470
#% 479473
#% 481771
#% 570884
#% 604239
#% 830711
#% 1016185
#! In some applications, data capture dominates query processing. For example, monitoring moving objects often requires more insertions and updates than queries. Data gathering using automated sensors often exhibits this imbalance. More generally, indexing streams is considered an unsolved problem.For those applications, B-tree indexes are good choices if some trade-off decisions are tilted towards optimization of updates rather than towards optimization of queries. This paper surveys some techniques that let B-trees sustain very high update rates, up to multiple orders of magnitude higher than traditional B-trees, at the expense of query processing performance. Not surprisingly, some of these techniques are reminiscent of those employed during index creation, index rebuild, etc., while other techniques are derived from well known technologies such as differential files and log-structured file systems.

#index 857499
#* Report on the 10th International Symposium on Database Programming Languages: (DBPL 2005)
#@ Gavin Bierman;Christoph Koch
#t 2006
#c 5
#! DBPL 2005 was held on August 28-29, 2005, in the charming surroundings of Trondheim, Norway, and was one of the eleven meetings that were co-located with VLDB. DBPL meets every two years and presents the very best work at the intersection of database and programming language research. DBPL 2005 is the tenth symposium in the series.

#index 857500
#* The WS-DAI family of specifications for web service data access and integration
#@ Mario Antonioletti;Amy Krause;Norman W. Paton;Andrew Eisenberg;Simon Laws;Susan Malaika;Jim Melton;Dave Pearson
#t 2006
#c 5
#! This month, we are pleased to provide to our readers a column that addresses an important aspect of grid computing: data access.

#index 857501
#* Moshe Vardi speaks out on the proof, the whole proof, and nothing but the proof
#@ Marianne Winslett
#t 2006
#c 5
#! Welcome to ACM SIGMOD Record's series of interviews with distinguished members of the database community. I'm Marianne Winslett, and I have here with me Moshe Vardi, who holds an endowed professorship at Rice University and is a former chair of their Computer Science Department. Before joining Rice, Moshe was a manager at IBM Almaden Research Center. Moshe is an ACM Fellow, a AAAI Fellow, a co-winner of the Goedel Prize, and a member of the U.S. National Academy of Engineering and the European Academy of Sciences. His research interests include databases, verification, complexity theory, and multi-agent systems, and his PhD is from the Hebrew University of Jerusalem. So Moshe, welcome!

#index 857502
#* Query reformulation with constraints
#@ Alin Deutsch;Lucian Popa;Val Tannen
#t 2006
#c 5
#% 583
#% 145178
#% 189868
#% 198465
#% 210176
#% 237190
#% 248038
#% 287339
#% 289384
#% 294600
#% 300168
#% 378409
#% 384978
#% 465053
#% 465057
#% 480657
#% 481923
#% 488620
#% 490489
#% 563512
#% 564416
#% 571169
#% 572311
#% 599549
#% 712339
#% 716433
#% 806215
#% 809247
#% 826031
#% 826032
#% 993981
#% 1015271
#! Let Σ1, Σ2 be two schemas, which may overlap, C be a set of constraints on the joint schema Σ1 ∪ Σ2, and q1 be a Σ1-query. An (equivalent) reformulation of q1 in the presence of C is a Σ2-query, q2, such that q2 gives the same answers as q1 on any Σ1 ∪ Σ2-database instance that satisfies C. In general, there may exist multiple such reformulations and choosing among them may require, for example, a cost model.

#index 873097
#* Proceedings of the 5th ACM international workshop on Data engineering for wireless and mobile access
#@ Vijay Kumar;Alexandros Labrinidis;Panos K. Chrysanthis;Christian S. Jensen
#t 2006
#c 5
#! It is our great pleasure to welcome you all to the Fifth ACM International Workshop on Data Engineering for Wireless and Mobile Access (MobiDE 2006), which is held in conjunction with ACM SIGMOD/PODS 2006. This workshop brings together researchers in databases, networking, and mobile computing with the aim of connecting the data management and mobile computing communities. It provides opportunities for a full day of exciting discussions on the topic of data management in mobile and wireless environments. The workshop serves as a forum for researchers and technologists to discuss the state-of-the-art, present their contributions, generate novel ideas, and set new directions for data management for mobile and wireless access.MobiDE 2006 is the fifth in a series of workshops. The previous workshops took place in Seattle, WA, in conjunction with MobiCom 1999; in Santa Barbara, CA, together with SIGMOD/PODS 2001; in San Diego, CA, in conjunction with MobiCom 2003; and in Baltimore, MD, in conjunction with SIGMOD/PODS 2005.The workshop program has been put together with the aim of presenting new and controversial research ideas so as to foster interaction among researchers from around the world. The call for papers attracted 31 submissions from 14 countries: Brazil, Canada, China, Cyprus, France, Germany, Greece, Japan, Korea, Norway, Portugal, Tunis, U.K., and U.S.A. Due in parts to MobiDE now being an established outlet for research results and to the co-location with ACM SIGMOD/PODS, many submissions were of very high quality, making the selection process quite competitive. All submissions were reviewed by at least 3 members of the Program Committee. This was followed by a discussion phase, where the reviewers of each submission had the opportunity to discuss the submission and its reviews. As a result, 10 submissions were accepted as full papers, and 2 submissions that reported on promising works in progress were accepted as short papers. In addition, a very interesting invited talk, "A Data Architecture for Consumer RFID Applications," by Gaetano Borriello from the University of Washington is included in the program. This talk probes into the consumer side of the mobile environment.The program reflects the depth and breadth of the field, with sessions covering important aspects of mobility, location-based processing, and the introduction of sensor technology into the real world. Thus, these proceedings provide an excellent point of reference on the increasingly important area of data engineering for mobile and wireless computing.

#index 874875
#* Proceedings of the twenty-fifth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems
#@ Georg Gottlob;Jan Van den Bussche
#t 2006
#c 5

#index 874876
#* Principles of dataspace systems
#@ Alon Halevy;Michael Franklin;David Maier
#t 2006
#c 5
#% 663
#% 235023
#% 243720
#% 266230
#% 273687
#% 318704
#% 333990
#% 378409
#% 384978
#% 442830
#% 465008
#% 479754
#% 479782
#% 481923
#% 533911
#% 572311
#% 577238
#% 577523
#% 599549
#% 642983
#% 654442
#% 654459
#% 659990
#% 660011
#% 743927
#% 751818
#% 772019
#% 800497
#% 809239
#% 810020
#% 824681
#% 824718
#% 845350
#% 850428
#% 864394
#% 874971
#% 1015325
#% 1016201
#% 1016204
#! The most acute information management challenges today stem from organizations relying on a large number of diverse, interrelated data sources, but having no means of managing them in a convenient, integrated, or principled fashion. These challenges arise in enterprise and government data management, digital libraries, "smart" homes and personal information management. We have proposed dataspaces as a data management abstraction for these diverse applications and DataSpace Support Platforms (DSSPs) as systems that should be built to provide the required services over dataspaces. Unlike data integration systems, DSSPs do not require full semantic integration of the sources in order to provide useful services. This paper lays out specific technical challenges to realizing DSSPs and ties them to existing work in our field. We focus on query answering in DSSPs, the DSSP's ability to introspect on its content, and the use of human attention to enhance the semantic relationships in a dataspace.

#index 874877
#* Two-variable logic on data trees and XML reasoning
#@ Mikolaj Bojańczyk;Claire David;Anca Muscholl;Thomas Schwentick;Luc Segoufin
#t 2006
#c 5
#% 175464
#% 283011
#% 564264
#% 578964
#% 581901
#% 643569
#% 733268
#% 769518
#% 776157
#% 809236
#% 812921
#% 821604
#% 993939
#% 1673664
#% 1673671
#% 1700125
#% 1705007
#% 1718404
#% 1718501
#! Motivated by reasoning tasks in the context of XML languages, the satisfiability problem of logics on data trees is investigated. The nodes of a data tree have a label from a finite set and a data value from a possibly infinite set. It is shown that satisfiability for two-variable first-order logic is decidable if the tree structure can be accessed only through the child and the next sibling predicates and the access to data values is restricted to equality tests. From this main result decidability of satisfiability and containment for a data-aware fragment of XPath and of the implication problem for unary key and inclusion constraints is concluded.

#index 874878
#* An adaptive packed-memory array
#@ Michael A. Bender;Haodong Hu
#t 2006
#c 5
#% 280
#% 13034
#% 23651
#% 145742
#% 273713
#% 379364
#% 379365
#% 480380
#% 480575
#% 491046
#% 548622
#% 593968
#% 598373
#% 598374
#% 793007
#% 816651
#% 833367
#% 874900
#! The packed-memory array (PMA) is a data structure that maintains a dynamic set of N elements in sorted order in a Θ(N)-sized array. The idea is to intersperse Θ(N) empty spaces or gaps among the elements so that only a small number of elements need to be shifted around on an insert or delete. Because the elements are stored physically in sorted order in memory or on disk, the PMA can be used to support extremely efficient range queries. Specifically, the cost to scan L consecutive elements is O(1+L/B) memory transfers.This paper gives the first adaptive packed-memory array (APMA), which automatically adjusts to the input pattern. Like the original PMA, any pattern of updates costs only O(log2 N) amortized element moves and O(1+(log2 N)/B) amortized memory transfers per update. However, the APMA performs even better on many common input distributions achieving only O(logN) amortized element moves and O(1+(logN)/B) amortized memory transfers. The paper analyzes sequential inserts, where the insertions are to the front of the APMA, hammer inserts, where the insertions "hammer" on one part of the APMA, random inserts, where the insertions are after random elements in the APMA, and bulk inserts, where for constant α∈[0,1], Nα elements are inserted after random elements in the APMA. The paper then gives simulation results that are consistent with the asymptotic bounds. For sequential insertions of roughly 1.4 million elements, the APMA has four times fewer element moves per insertion than the traditional PMA and running times that are more than seven times faster.

#index 874879
#* The complexity of data exchange
#@ Phokion G. Kolaitis;Jonathan Panttaja;Wang-Chiew Tan
#t 2006
#c 5
#% 583
#% 183411
#% 289384
#% 378409
#% 416042
#% 465053
#% 490909
#% 591778
#% 598376
#% 749088
#% 801676
#% 806215
#% 809235
#% 809247
#% 809249
#% 826032
#% 993981
#! Data exchange is the problem of transforming data structured under a source schema into data structured under a target schema in such a way that all constraints of a schema mapping are satisfied. At the heart of data exchange, lies a basic decision problem, called the existence-of-solutions problem: given a source instance, is there a target instance that satisfies the constraints of the schema mapping at hand? Earlier work showed that for schema mappings specified by embedded implicational dependencies, this problem is solvable in polynomial time, assuming that (1) the schema mapping is kept fixed and (2) the constraints of the schema mapping satisfy a certain structural condition, called weak acyclicity.We investigate the effect of these assumptions on the complexity of the existence-of-solutions problem, and show that each one is indispensable in deriving polynomial-time algorithms for this problem. Specifically, using machinery from universal algebra, we show that if the weak acyclicity assumption is relaxed even in a minimal way, then the existence-of-solutions problem becomes undecidable. We also show that if, in addition to the source instance, the schema mapping is part of the input, then the existence-of-solutions problem becomes EXPTIME-complete. Thus, there is a provable exponential gap between the data complexity and the combined complexity of data exchange. Finally, we study restricted classes of schema mappings and develop a comprehensive picture for the combined complexity of the existence-of-solutions problem for these restrictions. In particular, depending on the restriction considered, the combined complexity of this problem turns out to be either EXPTIME-complete or coNP-complete.

#index 874880
#* Data exchange: computing cores in polynomial time
#@ Georg Gottlob;Alan Nash
#t 2006
#c 5
#% 583
#% 129217
#% 136362
#% 287316
#% 287339
#% 289384
#% 384978
#% 465053
#% 576100
#% 599549
#% 809239
#% 809247
#% 826032
#! Data exchange deals with inserting data from one database into another database having a different schema. We study and solve a central computational problem of data exchange, namely, computing the core of a universal solution to a data exchange problem. Fagin, Kolaitis, and Popa [9], have shown that among the universal solutions of a solvable data exchange problem, there exists a most compact one (up to isomorphism), "the core" (of any universal solution), and have convincingly argued that this core should be the solution of choice. They stated as an important open problem whether the core of a universal solution can be computed in polynomial time in the general setting where the source-to-target constraints are arbitrary tuple generating dependencies (TGDs) and the target constraints consist of equation generating dependencies (EGDs) and weakly-acyclic TGDs. In this paper we solve this problem by developing new efficient methods for computing the core of a universal solution. This positive result shows that the core approach of Fagin, Kolaitis, and Popa is feasible and applicable in a very general setting and thus provides a further momentum to the use of cores in data exchange.

#index 874881
#* Inverting schema mappings
#@ Ronald Fagin
#t 2006
#c 5
#% 583
#% 11284
#% 129217
#% 287733
#% 287754
#% 378409
#% 765540
#% 801676
#% 806215
#% 810021
#% 824736
#% 826032
#! A schema mapping is a specification that describes how data structured under one schema (the source schema) is to be transformed into data structured under a different schema (the target schema). Although the notion of an inverse of a schema mapping is important, the exact definition of an inverse mapping is somewhat elusive. This is because a schema mapping may associate many target instances with each source instance, and many source instances with each target instance. Based on the notion that the composition of a mapping and its inverse is the identity, we give a formal definition for what it means for a schema mapping M′ to be an inverse of a schema mapping M for a class S of source instances. We call such an inverse an S-inverse. A particular case of interest arises when S is the class of all instances, in which case an S-inverse is a global inverse. We focus on the important and practical case of schema mappings defined by source-to-target tuple-generating dependencies, and uncover a rich theory. When S is defined by a set of dependencies with a finite chase, we show how to construct an S-inverse when one exists. In particular, we show how to construct a global inverse when one exists. Given M and M′, we show how to define the largest class S such that M′ is an S-inverse of M.

#index 874882
#* Data exchange and incomplete information
#@ Leonid Libkin
#t 2006
#c 5
#% 663
#% 11817
#% 94459
#% 101649
#% 109995
#% 121628
#% 129217
#% 233695
#% 247419
#% 248038
#% 267604
#% 273687
#% 287313
#% 287733
#% 332166
#% 384978
#% 465057
#% 480249
#% 576100
#% 801676
#% 801691
#% 809235
#% 809239
#% 809247
#% 823106
#% 825671
#% 993981
#! Data exchange is the problem of finding an instance of a target schema, given an instance of a source schema and a specification of the relationship between the source and the target, and answering queries over target instances in a way that is semantically consistent with the information in the source. Theoretical foundations of data exchange have been actively explored recently. It was also noticed that the standard certain answers semantics may behave in very odd ways.In this paper I explain that this behavior is due to the fact that the presence of incomplete information in target instances has been ignored; in particular, proper query evaluation techniques for databases with nulls have not been used, and the distinction between closed and open world semantics has not been made. I present a concept of target solutions based on the closed world assumption, and show that the space of all solutions has two extreme points: the canonical universal solution and the core, well studied in data exchange. I show how to define semantics of query answering taking into account incomplete information, and show that the well-known anomalies go away with the new semantics. The paper also contains results on the complexity of query answering, upper approximations to queries (maybe-answers), and various extensions.

#index 874883
#* Equivalence of queries combining set and bag-set semantics
#@ Sara Cohen
#t 2006
#c 5
#% 36181
#% 123118
#% 129572
#% 137867
#% 137871
#% 190638
#% 198473
#% 248034
#% 273696
#% 287336
#% 289266
#% 303886
#% 342389
#% 368248
#% 415941
#% 415980
#% 416034
#% 443173
#% 465043
#% 465190
#% 599549
#% 801769
#% 874884
#! The query equivalence problem has been studied extensively for set-semantics and, more recently, for bag-set semantics. However, SQL queries often combine set and bag-set semantics. For example, an SQL query that returns a multiset of elements may call a subquery or view that returns a set of elements. As another example, in SQL one can compute a multiset-union of queries, each of which returns a set of answers. This paper presents combined semantics, which formally models query evaluation combining set and bag-set semantics. The equivalence problem for queries evaluated under combined semantics is studied. A sufficient condition for equivalence is presented. For several important common classes of queries necessary and sufficient conditions for equivalence are presented.

#index 874884
#* The containment problem for Real conjunctive queries with inequalities
#@ T. S. Jayram;Phokion G. Kolaitis;Erik Vee
#t 2006
#c 5
#% 36181
#% 137867
#% 147801
#% 190638
#% 230142
#% 248032
#% 248034
#% 273696
#% 289266
#% 566304
#% 599549
#! Query containment is a fundamental algorithmic problem in database query processing and optimization. Under set semantics, the query-containment problem for conjunctive queries has long been known to be NP-complete. In real database systems, however, queries are usually evaluated under bag semantics, not set semantics. In particular, SQL queries are evaluated under bag semantics and return multisets as answers, since duplicates are not eliminated unless explicitly requested. The exact complexity of the query-containment problem for conjunctive queries under bag semantics has been an open problem for more than a decade; in fact, it is not even known whether this problem is decidable.Here, we investigate, under bag semantics, the query-containment problem for conjunctive queries with inequalities. It has been previously shown that, under set semantics, this problem is complete for the second level of the polynomial hierarchy. Our main result asserts that, under bag semantics, the query-containment problem for conjunctive queries with inequalities is undecidable. Actually, we establish the stronger result that this problem is undecidable even if the following two restrictions hold at the same time: (1) the queries use just a single binary relation; and (2) the total number of inequalities is bounded by a certain fixed value. Moreover, the same undecidability results hold under bag-set semantics.

#index 874885
#* Verification of communicating data-driven web services
#@ Alin Deutsch;Liying Sui;Victor Vianu;Dayou Zhou
#t 2006
#c 5
#% 101955
#% 213957
#% 220272
#% 289415
#% 348131
#% 425200
#% 458807
#% 519432
#% 576091
#% 592504
#% 630964
#% 754120
#% 765514
#% 767388
#% 799801
#% 801675
#% 810053
#% 824702
#% 824806
#% 837365
#! We study the verification of compositions of Web Service peers which interact asynchronously by exchanging messages. Each peer has access to a local database and reacts to user input and incoming messages by performing various actions and sending messages. The reaction is described by queries over the database, internal state, user input and received messages. We consider two formalisms for specification of correctness properties of compositions, namely Linear Temporal First-Order Logic and Conversation Protocols. For both formalisms, we map the boundaries of verification decidability, showing that they include expressive classes of compositions and properties. We also address modular verification, in which the correctness of a composition is predicated on the properties of its environment.

#index 874886
#* Analyzing workflows implied by instance-dependent access rules
#@ Toon Calders;Stijn Dekeyser;Jan Hidders;Jan Paredaens
#t 2006
#c 5
#% 5379
#% 488623
#% 755175
#% 801692
#% 823674
#% 882476
#% 995939
#! Recently proposed form-based web information systems liberate the capture and reuse of data in organizations by substituting the development of technical implementations of electronic forms for the conceptual modelling of forms' tree-structured schemas and their data access rules. Significantly, these instance-dependent rules also imply a workflow process associated to a form, eliminating the need for a costly workflow design phase. Instead, the workflows thus created in an ad hoc manner by unsophisticated end-users can be automatically analyzed, and incorrect forms rejected.This paper examines fundamental correctness properties of workflows that are implied by instance-dependent access rules. Specifically, we study the decidability of the form completability property and the semi-soundness of a form's workflow. These problems are affected by a choice of constraints on the path language used to express access rules and completion formulas, and on the depth of the form's schema tree. Hence, we study these problems by examining them in the context of several different fragments determined by such constraints.

#index 874887
#* From statistical knowledge bases to degrees of belief: an overview
#@ Joseph Y. Halpern
#t 2006
#c 5
#% 61176
#% 61219
#% 89958
#% 90371
#% 100178
#% 205808
#% 216970
#% 480102
#% 824733
#% 864412
#% 1016201
#% 1700137
#! An intelligent agent will often be uncertain about various properties of its environment, and when acting in that environment it will frequently need to quantify its uncertainty. For example, if the agent wishes to employ the expected-utility paradigm of decision theory to guide its actions, she will need to assign degrees of belief (subjective probabilities) to various assertions. Of course, these degrees of belief should not be arbitrary, but rather should be based on the information available to the agent. This paper provides a brief overview of one approach for inducing degrees of belief from very rich knowledge bases that can include information about particular individuals, statistical correlations, physical laws, and default rules. The approach is called the random-worlds method. The method is based on the principle of indifference: it treats all of the worlds the agent considers possible as being equally likely. It is able to integrate qualitative default reasoning with quantitative probabilistic reasoning by providing a language in which both types of information can be easily expressed. A number of desiderata that arise in direct inference (reasoning from statistical information to conclusions about individuals) and default reasoning follow directly from the semantics of random worlds. For example, random worlds captures important patterns of reasoning such as specificity, inheritance, indifference to irrelevant information, and default assumptions of independence. Furthermore, the expressive power of the language used and the intuitive semantics of random worlds allow the method to deal with problems that are beyond the scope of many other non-deductive reasoning systems. The relevance of the random-worlds method to database systems is also discussed.

#index 874888
#* On redundancy vs dependency preservation in normalization: an information-theoretic study of 3NF
#@ Solmaz Kolahi;Leonid Libkin
#t 2006
#c 5
#% 11284
#% 30077
#% 101956
#% 115608
#% 286852
#% 286860
#% 287008
#% 287677
#% 292680
#% 300711
#% 333841
#% 384978
#% 387089
#% 411570
#% 443343
#% 480102
#% 527113
#% 533902
#% 572459
#% 742566
#% 798970
#% 804840
#% 993272
#% 1673672
#! A recently introduced information-theoretic approach to analyzing redundancies in database design was used to justify normal forms like BCNF that completely eliminate redundancies. The main notion is that of an information content of each datum in an instance (which is a number in [0,1]): the closer to 1, the less redundancy it carries. In practice, however, one usually settles for 3NF which, unlike BCNF, may not eliminate all redundancies but always guarantees dependency preservation.In this paper we use the information-theoretic approach to prove that 3NF is the best normal form if one needs to achieve dependency preservation. For each dependency-preserving normal form, we define the price of dependency preservation as an information-theoretic measure of redundancy that gets introduced to compensate for dependency preservation. This is a number in the [0,1] range: the smaller it is, the less redundancy a normal form guarantees. We prove that for every dependency-preserving normal form, the price of dependency preservation is at least 1/2, and it is precisely 1/2 for 3NF. Hence, 3NF has the least amount of redundancy among all dependency-preserving normal forms. We also show that, information-theoretically, unnormalized schemas have at least twice the amount of redundancy than schemas in 3NF.

#index 874889
#* Tractable database design through bounded treewidth
#@ Georg Gottlob;Reinhard Pichler;Fang Wei
#t 2006
#c 5
#% 59787
#% 101944
#% 125557
#% 181220
#% 197754
#% 219474
#% 221328
#% 287295
#% 287677
#% 347184
#% 398173
#% 427161
#! Given that most elementary problems in database design are NP-hard, the currently used database design algorithms produce suboptimal results. For example, the current 3NF decomposition algorithms may continue further decomposing a relation even though it is already in 3NF. In this paper we study database design problems whose sets of functional dependencies have bounded treewidth. For such sets, which frequently occur in practice, we develop polynomial-time and highly parallelizable algorithms for a number of central database design problems such as: • primality of an attribute • 3NF-test for a relational schema or subschema • BCNF-test for a subschema.For establishing these results, we propose a new characterization for keys and for the primality of a single attribute.In order to define the treewidth of a relational schema, we shall associate a hypergraph with it. Note that there are two main possibilities of defining the treewidth of a hypergraph H: One is via the primal graph of H and one is via the incidence graph of H. Our algorithms apply to the case where the primal graph is considered. However, we also show that the tractability results still hold when the incidence graph is considered instead.

#index 874890
#* Evolution of page popularity under random web graph models
#@ Rajeev Motwani;Ying Xu
#t 2006
#c 5
#% 190611
#% 379401
#% 453479
#% 510723
#% 581160
#% 593994
#% 754060
#% 810054
#% 813734
#% 824716
#% 1394202
#! The link structure of the Web can be viewed as a massive graph. The preferential attachment model and its variants are well-known random graph models that help explain the evolution of the web graph. However, those models assign more links to older pages without reference to the quality of web pages, which does not capture the real-world evolution of the web graph and renders the models inappropriate for studying the popularity evolution of new pages.We extend the preferential attachment model with page quality, where the probability of a page getting new links depends not only on its current degree but also on its quality. We study the distribution of degrees among different quality values, and prove that under discrete quality distributions, the degree sequence still follows a power law distribution. Then we use the model to study the evolution of page popularity. We show that for pages with the same quality, the older pages are more popular; if a younger page is better than an older page, then eventually the younger-and-better page will become more popular. We also use the model to study a randomized ranking scheme proposed earlier [18] and show that it accelerates popularity evolution of new pages.

#index 874891
#* Privacy via pseudorandom sketches
#@ Nina Mishra;Mark Sandler
#t 2006
#c 5
#% 287297
#% 576110
#% 576111
#% 576761
#% 630970
#% 743280
#% 751578
#% 809244
#% 809245
#% 810028
#% 864412
#% 1700134
#% 1707132
#! Imagine a collection of individuals who each possess private data that they do not wish to share with a third party. This paper considers how individuals may represent and publish their own data so as to simultaneously preserve their privacy and to ensure that it is possible to extract large-scale statistical behavior from the original unperturbed data. Existing techniques for perturbing data are limited by the number of users required to obtain approximate answers to queries, the richness of preserved statistical behavior, the privacy guarantees given and/or the amount of data that each individual must publish.This paper introduces a new technique to describe parts of an individual's data that is based on pseudorandom sketches. The sketches guarantee that each individual's privacy is provably maintained assuming one of the strongest definitions of privacy that we are aware of: given unlimited computational power and arbitrary partial knowledge, the attacker can not learn any additional private information from the published sketches. However, sketches from multiple users that describe a subset of attributes can be used to estimate the fraction of users that satisfy any conjunction over the full set of negated or unnegated attributes provided that there are enough users. We show that the error of approximation is independent of the number of attributes involved and only depends on the number of users available. An additional benefit is that the size of the sketch is minuscule: [log log O(M)] bits, where M is the number of users. Finally, we show how sketches can be combined to answer more complex queries. An interesting property of our approach is that despite using cryptographic primitives, our privacy guarantees do not rely on any unproven cryptographic conjectures.

#index 874892
#* Achieving anonymity via clustering
#@ Gagan Aggarwal;Tomás Feder;Krishnaram Kenthapadi;Samir Khuller;Rina Panigrahy;Dilys Thomas;An Zhu
#t 2006
#c 5
#% 146132
#% 313050
#% 325409
#% 404719
#% 576761
#% 593937
#% 594002
#% 594003
#% 800515
#% 801690
#% 810011
#% 864412
#% 1707132
#! Publishing data for analysis from a table containing personal records, while maintaining individual privacy, is a problem of increasing importance today. The traditional approach of de-identifying records is to remove identifying fields such as social security number, name etc. However, recent research has shown that a large fraction of the US population can be identified using non-key attributes (called quasi-identifiers) such as date of birth, gender, and zip code [15]. Sweeney [16] proposed the k-anonymity model for privacy where non-key attributes that leak information are suppressed or generalized so that, for every record in the modified table, there are at least k−1 other records having exactly the same values for quasi-identifiers. We propose a new method for anonymizing data records, where quasi-identifiers of data records are first clustered and then cluster centers are published. To ensure privacy of the data records, we impose the constraint that each cluster must contain no fewer than a pre-specified number of data records. This technique is more general since we have a much larger choice for cluster centers than k-Anonymity. In many cases, it lets us release a lot more information without compromising privacy. We also provide constant-factor approximation algorithms to come up with such a clustering. This is the first set of algorithms for the anonymization problem where the performance is independent of the anonymity parameter k. We further observe that a few outlier points can significantly increase the cost of anonymization. Hence, we extend our algorithms to allow an ε fraction of points to remain unclustered, i.e., deleted from the anonymized publication. Thus, by not releasing a small fraction of the database records, we can ensure that the data published for analysis has less distortion and hence is more useful. Our approximation algorithms for new clustering objectives are of independent interest and could be applicable in other clustering scenarios as well.

#index 874893
#* On the efficiency of checking perfect privacy
#@ Ashwin Machanavajjhala;Johannes Gehrke
#t 2006
#c 5
#% 36181
#% 67453
#% 164560
#% 230142
#% 248032
#% 248033
#% 268758
#% 275836
#% 287670
#% 322115
#% 331899
#% 464203
#% 464727
#% 576761
#% 599549
#% 765449
#% 809244
#% 864412
#% 1698887
#% 1700133
#% 1700137
#% 1707132
#! Privacy-preserving query-answering systems answer queries while provably guaranteeing that sensitive information is kept secret. One very attractive notion of privacy is perfect privacy—a secret is expressed through a query QS, and a query QV is answered only if it discloses no information about the secret query QS. However, if QS and QV are arbitrary conjunctive queries, the problem of checking whether QV discloses any information about QS is known to be Πp2-complete.In this paper, we show that for large interesting subclasses of conjunctive queries enforcing perfect privacy is tractable. Instead of giving different arguments for query classes of varying complexity, we make a connection between perfect privacy and the problem of checking query containment. We then use this connection to relate the complexity of enforcing perfect privacy to the complexity of query containment.

#index 874894
#* Finding and approximating top-k answers in keyword proximity search
#@ Benny Kimelfeld;Yehoshua Sagiv
#t 2006
#c 5
#% 39702
#% 268079
#% 282600
#% 303076
#% 321577
#% 330678
#% 333854
#% 397418
#% 593930
#% 598376
#% 660011
#% 824693
#% 1673660
#! Various approaches for keyword proximity search have been implemented in relational databases, XML and the Web. Yet, in all of them, an answer is a Q-fragment, namely, a subtree T of the given data graph G, such that T contains all the keywords of the query Q and has no proper subtree with this property. The rank of an answer is inversely proportional to its weight. Three problems are of interest: finding an optimal (i.e., top-ranked) answer, computing the top-k answers and enumerating all the answers in ranked order. It is shown that, under data complexity, an efficient algorithm for solving the first problem is sufficient for solving the other two problems with polynomial delay. Similarly, an efficient algorithm for finding a θ-approximation of the optimal answer suffices for carrying out the following two tasks with polynomial delay, under query-and-data complexity. First, enumerating in a (θ+1)-approximate order. Second, computing a (θ+1)-approximation of the top-k answers. As a corollary, this paper gives the first efficient algorithms, under data complexity, for enumerating all the answers in ranked order and for computing the top-k answers. It also gives the first efficient algorithms, under query-and-data complexity, for enumerating in a provably approximate order and for computing an approximation of the top-k answers.

#index 874895
#* Efficiently ordering subgoals with access constraints
#@ Guizhen Yang;Michael Kifer;Vinay K. Chaudhri
#t 2006
#c 5
#% 2852
#% 91617
#% 198466
#% 213437
#% 248859
#% 273923
#% 277327
#% 277328
#% 368248
#% 481923
#% 494028
#% 801698
#% 1671667
#% 1700141
#! In this paper, we study the problem of ordering subgoals under binding pattern restrictions for queries posed as nonrecursive Datalog programs. We prove that despite their limited expressive power, the problem is computationally hard—PSPACE-complete in the size of the nonrecursive Datalog program even for fairly restricted cases. As a practical solution to this problem, we develop an asymptotically optimal algorithm that runs in time linear in the size of the query plan. We also study extensions of our algorithm that efficiently solve other query planning problems under binding pattern restrictions. These problems include conjunctive queries with nested grouping constraints, distributed conjunctive queries, and first-order queries.

#index 874896
#* Flow algorithms for two pipelined filter ordering problems
#@ Anne Condon;Amol Deshpande;Lisa Hellerstein;Ning Wu
#t 2006
#c 5
#% 201936
#% 257871
#% 300167
#% 300169
#% 453556
#% 479938
#% 656701
#% 765435
#% 781735
#% 800505
#% 805762
#% 1700123
#! Pipelined filter ordering is a central problem in database query optimization, and has received renewed attention recently in the context of environments such as the web, continuous high-speed data streams and sensor networks. We present algorithms for two natural extensions of the classical pipelined filter ordering problem: (1) a distributional type problem where the filters run in parallel and the goal is to maximize throughput, and (2) an adversarial type problem where the goal is to minimize the expected value of multiplicative regret. We show that both problems can be solved using similar flow algorithms, which find an optimal ordering scheme in time O(n2), where n is the number of filters. Our algorithm for (1) improves on an earlier O(n3 log n) algorithm of Kodialam.

#index 874897
#* Asking the right questions: model-driven optimization using probes
#@ Ashish Goel;Sudipto Guha;Kamesh Munagala
#t 2006
#c 5
#% 273694
#% 300130
#% 300167
#% 302717
#% 333863
#% 378414
#% 415062
#% 448478
#% 593903
#% 646235
#% 654443
#% 656763
#% 732000
#% 749517
#% 765296
#% 785096
#% 785097
#% 785098
#% 810016
#% 810017
#% 864455
#% 1015280
#% 1016178
#! In several database applications, parameters like selectivities and load are known only with some associated uncertainty, which is specified, or modeled, as a distribution over values. The performance of query optimizers and monitoring schemes can be improved by spending resources like time or bandwidth in observing or resolving these parameters, so that better query plans can be generated. In a resource-constrained situation, deciding which parameters to observe in order to best optimize the expected quality of the plan generated (or in general, optimize the expected value of a certain objective function) itself becomes an interesting optimization problem.We present a framework for studying such problems, and present several scenarios arising in anomaly detection in complex systems, monitoring extreme values in sensor networks, load shedding in data stream systems, and estimating rates in wireless channels and minimum latency routes in networks, which can be modeled in this framework with the appropriate objective functions.Even for several simple objective functions, we show the problems are Np-Hard. We present greedy algorithms with good performance bounds. The proof of the performance bounds are via novel sub-modularity arguments.

#index 874898
#* Processing queries on tree-structured data efficiently
#@ Christoph Koch
#t 2006
#c 5
#% 49315
#% 54225
#% 101922
#% 101944
#% 108287
#% 183411
#% 248819
#% 321058
#% 331772
#% 339937
#% 342829
#% 378392
#% 384978
#% 397375
#% 427161
#% 464727
#% 465061
#% 465065
#% 473117
#% 480296
#% 480648
#% 487257
#% 504580
#% 516520
#% 574818
#% 587580
#% 598376
#% 644201
#% 654450
#% 654476
#% 654477
#% 659995
#% 659999
#% 717512
#% 733595
#% 742561
#% 742563
#% 745461
#% 765488
#% 778122
#% 800521
#% 800577
#% 801685
#% 803121
#% 804841
#% 809235
#% 814648
#% 821568
#% 865766
#% 976789
#% 993939
#% 1015271
#% 1015275
#% 1015277
#% 1016148
#% 1016150
#% 1676013
#% 1700125
#% 1721248
#! This is a survey of algorithms, complexity results, and general solution techniques for efficiently processing queries on tree-structured data. I focus on query languages that compute nodes or tuples of nodes—conjunctive queries, first-order queries, datalog, and XPath. I also point out a number of connections among previous results that have not been observed before.

#index 874899
#* Scalable computation of acyclic joins
#@ Anna Pagh;Rasmus Pagh
#t 2006
#c 5
#% 554
#% 31002
#% 41684
#% 102765
#% 102784
#% 289321
#% 322880
#% 368248
#% 397354
#% 411554
#% 780655
#% 818380
#% 1711504
#! The join operation of relational algebra is a cornerstone of relational database systems. Computing the join of several relations is NP-hard in general, whereas special (and typical) cases are tractable. This paper considers joins having an acyclic join graph, for which current methods initially apply a full reducer to efficiently eliminate tuples that will not contribute to the result of the join. From a worst-case perspective, previous algorithms for computing an acyclic join of k fully reduced relations, occupying a total of n≥k blocks on disk, use Ω((n+z)k) I/Os, where z is the size of the join result in blocks.In this paper we show how to compute the join in a time bound that is within a constant factor of the cost of running a full reducer plus sorting the output. For a broad class of acyclic join graphs this is O(sort(n+z)) I/Os, removing the dependence on k from previous bounds. Traditional methods decompose the join into a number of binary joins, which are then carried out one by one. Departing from this approach, our technique is based on computing the size of certain subsets of the result, and using these sizes to compute the location(s) of each data item in the result.Finally, as an initial study of cyclic joins in the I/O model, we show how to compute a join whose join graph is a 3-cycle, in O(n2/m+sort(n+z)) I/Os, where m is the number of blocks in internal memory.

#index 874900
#* Cache-oblivious string B-trees
#@ Michael A. Bender;Martin Farach-Colton;Bradley C. Kuszmaul
#t 2006
#c 5
#% 23651
#% 41684
#% 158790
#% 232800
#% 271801
#% 287715
#% 296253
#% 317933
#% 379364
#% 379365
#% 491046
#% 492884
#% 545963
#% 548622
#% 548643
#% 593909
#% 593968
#% 768815
#% 793007
#% 833367
#% 847099
#! B-trees are the data structure of choice for maintaining searchable data on disk. However, B-trees perform suboptimally when keys are long or of variable length,when keys are compressed, even when using front compression, the standard B-tree compression scheme,for range queries, andwith respect to memory effects such as disk prefetching.This paper presents a cache-oblivious string B-tree (COSB-tree) data structure that is efficient in all these ways: The COSB-tree searches asymptotically optimally and inserts and deletes nearly optimally.It maintains an index whose size is proportional to the front-compressed size of the dictionary. Furthermore, unlike standard front-compressed strings, keys can be decompressed in a memory-efficient manner.It performs range queries with no extra disk seeks; in contrast, B-trees incur disk seeks when skipping from leaf block to leaf block.It utilizes all levels of a memory hierarchy efficiently and makes good use of disk locality by using cache-oblivious layout strategies.

#index 874901
#* Randomized computations on large data sets: tight lower bounds
#@ Martin Grohe;André Hernich;Nicole Schweikardt
#t 2006
#c 5
#% 101797
#% 190611
#% 278835
#% 293720
#% 293726
#% 341100
#% 378388
#% 453512
#% 785130
#% 801685
#% 809253
#% 809255
#% 829161
#% 1676013
#% 1718398
#! We study the randomized version of a computation model (introduced in [9, 10]) that restricts random access to external memory and internal memory space. Essentially, this model can be viewed as a powerful version of a data stream model that puts no cost on sequential scans of external memory (as other models for data streams) and, in addition, (like other external memory models, but unlike streaming models), admits several large external memory devices that can be read and written to in parallel.We obtain tight lower bounds for the decision problems set equality, multiset equality, and checksort. More precisely, we show that any randomized one-sided-error bounded Monte Carlo algorithm for these problems must perform Ω(logN) random accesses to external memory devices, provided that the internal memory size is at most O(4√N/logN), where N denotes the size of the input data.From the lower bound on the set equality problem we can infer lower bounds on the worst case data complexity of query evaluation for the languages XQuery, XPath, and relational algebra on streaming data. More precisely, we show that there exist queries in XQuery, XPath, and relational algebra, such that any (randomized) Las Vegas algorithm that evaluates these queries must perform Ω(logN) random accesses to external memory devices, provided that the internal memory size is at most O(4√N/logN).

#index 874902
#* Counting triangles in data streams
#@ Luciana S. Buriol;Gereon Frahling;Stefano Leonardi;Alberto Marchetti-Spaccamela;Christian Sohler
#t 2006
#c 5
#% 1331
#% 68247
#% 278835
#% 281214
#% 338425
#% 378391
#% 379443
#% 480628
#% 580733
#% 593994
#% 765429
#% 1682599
#% 1719564
#! We present two space bounded random sampling algorithms that compute an approximation of the number of triangles in an undirected graph given as a stream of edges. Our first algorithm does not make any assumptions on the order of edges in the stream. It uses space that is inversely related to the ratio between the number of triangles and the number of triples with at least one edge in the induced subgraph, and constant expected update time per edge. Our second algorithm is designed for incidence streams (all edges incident to the same vertex appear consecutively). It uses space that is inversely related to the ratio between the number of triangles and length 2 paths in the graph and expected update time O(log|V|⋅(1+s⋅|V|/|E|)), where s is the space requirement of the algorithm. These results significantly improve over previous work [20, 8]. Since the space complexity depends only on the structure of the input graph and not on the number of nodes, our algorithms scale very well with increasing graph size and so they provide a basic tool to analyze the structure of large graphs. They have many applications, for example, in the discovery of Web communities, the computation of clustering and transitivity coefficient, and discovery of frequent patterns in large graphs.We have implemented both algorithms and evaluated their performance on networks from different application domains. The sizes of the considered graphs varied from about 8,000 nodes and 40,000 edges to 135 million nodes and more than 1 billion edges. For both algorithms we run experiments with parameter s=1,000, 10,000, 100,000, 1,000,000 to evaluate running time and approximation guarantee. Both algorithms appear to be time efficient for these sample sizes. The approximation quality of the first algorithm was varying significantly and even for s=1,000,000 we had more than 10% deviation for more than half of the instances. The second algorithm performed much better and even for s=10,000 we had an average deviation of less than 6% (taken over all but the largest instance for which we could not compute the number of triangles exactly).

#index 874903
#* Space- and time-efficient deterministic algorithms for biased quantiles over data streams
#@ Graham Cormode;Flip Korn;S. Muthukrishnan;Divesh Srivastava
#t 2006
#c 5
#% 70370
#% 273907
#% 333931
#% 378388
#% 453493
#% 453512
#% 783740
#% 800494
#% 864438
#! Skew is prevalent in data streams, and should be taken into account by algorithms that analyze the data. The problem of finding "biased quantiles"—that is, approximate quantiles which must be more accurate for more extreme values—is a framework for summarizing such skewed data on data streams. We present the first deterministic algorithms for answering biased quantiles queries accurately with small—sublinear in the input size—space and time bounds in one pass. The space bound is near-optimal, and the amortized update cost is close to constant, making it practical for handling high speed network data streams. We not only demonstrate theoretical properties of the algorithm, but also show it uses less space than existing methods in many practical settings, and is fast to maintain.

#index 874904
#* Approximate quantiles and the order of the stream
#@ Sudipto Guha;Andrew McGregor
#t 2006
#c 5
#% 2833
#% 116084
#% 248820
#% 248821
#% 273907
#% 293714
#% 300167
#% 320240
#% 333931
#% 411355
#% 453493
#% 481749
#% 519953
#% 548479
#% 726621
#% 765434
#% 847115
#% 993969
#% 1016178
#! Recently, there has been an increased focus on modeling uncertainty by distributions. Suppose we wish to compute a function of a stream whose elements are samples drawn independently from some distribution. The distribution is unknown, but the order in which the samples are presented to us will not be completely adversarial. In this paper, we investigate the importance of the ordering of a data stream, without making any assumptions about the actual distribution of the data. Using quantiles as an example application, we show that we can design provably better algorithms, and settle several open questions on the impact of order on streams. With the recent impetus in the investigation of models for sensor networks, we believe that our approach will allow the construction of novel and significantly improved algorithms.

#index 874905
#* Deterministic k-set structure
#@ Sumit Ganguly;Anirban Majumder
#t 2006
#c 5
#% 35764
#% 69724
#% 190611
#% 492912
#% 571888
#% 576119
#% 894646
#% 1698257
#% 1814872
#! A k-set structure over data streams is a bounded-space data structure that supports stream insertion and deletion operations and returns the set of (item, frequency) pairs in the stream, provided, the number of distinct items in the stream does not exceed k; and returns nil otherwise. This is a fundamental problem with applications in data streaming [14], data reconciliation in distributed systems [12] and mobile computing [16], etc. In this paper, we present a deterministic algorithm for the k-set problem that matches the space lower bound to within a logarithmic factor.

#index 874906
#* A simpler and more efficient deterministic scheme for finding frequent items over sliding windows
#@ L. K. Lee;H. F. Ting
#t 2006
#c 5
#% 248812
#% 278835
#% 338425
#% 414993
#% 480628
#% 492912
#% 548479
#% 569754
#% 576119
#% 594012
#% 642409
#% 725366
#% 730046
#% 765181
#% 800582
#% 801696
#% 847114
#! In this paper, we give a simple scheme for identifying ε-approximate frequent items over a sliding window of size n. Our scheme is deterministic and does not make any assumption on the distribution of the item frequencies. It supports O(1/ε) update and query time, and uses O(1/ε) space. It is very simple; its main data structures are just a few short queues whose entries store the position of some items in the sliding window. We also extend our scheme for variable-size window. This extended scheme uses O(1/ε log(εn)) space.

#index 874907
#* Finding global icebergs over distributed data sets
#@ Qi (George) Zhao;Mitsunori Ogihara;Haixun Wang;Jun (Jim) Xu
#t 2006
#c 5
#% 307424
#% 322884
#% 479795
#% 492912
#% 548479
#% 569754
#% 576119
#% 654443
#% 654461
#% 654462
#% 654488
#% 659921
#% 725365
#% 764007
#% 800582
#% 801696
#% 816392
#% 993960
#% 1016155
#! Finding icebergs–items whose frequency of occurrence is above a certain threshold–is an important problem with a wide range of applications. Most of the existing work focuses on iceberg queries at a single node. However, in many real-life applications, data sets are distributed across a large number of nodes. Two naïve approaches might be considered. In the first, each node ships its entire data set to a central server, and the central server uses single-node algorithms to find icebergs. But it may incur prohibitive communication overhead. In the second, each node submits local icebergs, and the central server combines local icebergs to find global icebergs. But it may fail because in many important applications, globally frequent items may not be frequent at any node. In this work, we propose two novel schemes that provide accurate and efficient solutions to this problem: a sampling-based scheme and a counting-sketch-based scheme. In particular, the latter scheme incurs a communication cost at least an order of magnitude smaller than the naïve scheme of shipping all data, yet is able to achieve very high accuracy. Through rigorous theoretical and experimental analysis we establish the statistical properties of our proposed algorithms, including their accuracy bounds.

#index 874908
#* Efficient gossip-based aggregate computation
#@ Srinivas Kashyap;Supratim Deb;K. V. M. Naidu;Rajeev Rastogi;Anand Srinivasan
#t 2006
#c 5
#% 31681
#% 35764
#% 190611
#% 203139
#% 213849
#% 297915
#% 340175
#% 505869
#% 545923
#% 569762
#% 593985
#% 654443
#% 654482
#% 723903
#% 745401
#% 783741
#% 805466
#% 810009
#% 879260
#! Recently, there has been a growing interest in gossip-based protocols that employ randomized communication to ensure robust information dissemination. In this paper, we present a novel gossip-based scheme using which all the nodes in an n-node overlay network can compute the common aggregates of MIN, MAX, SUM, AVERAGE, and RANK of their values using O(n log log n) messages within O(log n log log n) rounds of communication. To the best of our knowledge, ours is the first result that shows how to compute these aggregates with high probability using only O(n log log n) messages. In contrast, the best known gossip-based algorithm for computing these aggregates requires O(nlog n) messages and O(log n) rounds. Thus, our algorithm allows system designers to trade off a small increase in round complexity with a significant reduction in message complexity. This can lead to dramatically lower network congestion and longer node lifetimes in wireless and sensor networks, where channel bandwidth and battery life are severely constrained.

#index 874909
#* Structural characterizations of the semantics of XPath as navigation tool on a document
#@ Marc Gyssens;Jan Paredaens;Dirk Van Gucht;George H. L. Fletcher
#t 2006
#c 5
#% 338753
#% 465065
#% 473117
#% 479465
#% 660000
#% 733593
#% 755175
#% 801669
#% 809236
#% 814648
#% 824798
#% 1015266
#% 1015268
#% 1675930
#! Given a document D in the form of an unordered labeled tree, we study the expressibility on D of various fragments of XPath, the core navigational language on XML documents. We give characterizations, in terms of the structure of D, for when a binary relation on its nodes is definable by an XPath expression in these fragments. Since each pair of nodes in such a relation represents a unique path in D, our results therefore capture the sets of paths in D definable in XPath. We refer to this perspective on the semantics of XPath as the "global view." In contrast with this global view, there is also a "local view" where one is interested in the nodes to which one can navigate starting from a particular node in the document. In this view, we characterize when a set of nodes in D can be defined as the result of applying an XPath expression to a given node of D. All these definability results, both in the global and the local view, are obtained by using a robust two-step methodology, which consists of first characterizing when two nodes cannot be distinguished by an expression in the respective fragments of XPath, and then bootstrapping these characterizations to the desired results.

#index 874910
#* The expressivity of XPath with transitive closure
#@ Balder ten Cate
#t 2006
#c 5
#% 45255
#% 77979
#% 210214
#% 384978
#% 452816
#% 543184
#% 821610
#% 826029
#! We extend Core XPath, the navigational fragment of XPath 1.0, with transitive closure and path equalities. The resulting language, Regular XPATH≈, is expressively complete for FO* (first-order logic extended with a transitive closure operator that can be applied to formulas with exactly two free variables). As a corollary, we obtain that Regular XPATH≈ is closed under path intersection and complementation. We also provide characterizations for the *-positive fragment of Regular XPATH≈, and for μRegular XPATH (the extension of Regular XPATH≈ with least fixed points).

#index 874911
#* Relational lenses: a language for updatable views
#@ Aaron Bohannon;Benjamin C. Pierce;Jeffrey A. Vaughan
#t 2006
#c 5
#% 43031
#% 193296
#% 286901
#% 287000
#% 291869
#% 576096
#% 723773
#% 790328
#% 1673659
#! We propose a novel approach to the classical view update problem. The view update problem arises from the fact that modifications to a database view may not correspond uniquely to modifications on the underlying database; we need a means of determining an "update policy" that guides how view updates are reflected in the database. Our approach is to define a bi-directional query language, in which every expression can be read bot(from left to right) as a view definition and (from right to left) as an update policy. The primitives of this language are based on standard relational operators. Its type system, which includes record-level predicates and functional dependencies, plays a crucial role in guaranteeing that update policies are well-behaved, in a precise sense, and that they are total—i.e., able to handle arbitrary changes to the view.

#index 874912
#* Programmable clustering
#@ Sreenivas Gollapudi;Ravi Kumar;D. Sivakumar
#t 2006
#c 5
#% 729437
#% 800530
#% 805798
#! We initiate a novel study of clustering problems. Rather than specifying an explicit objective function to optimize, our framework allows the user of clustering algorithm to specify, via a first-order formula, what constitutes an acceptable clustering to them. While the resulting genre of problems includes, in general, NP-complete problems, we highlight three specific first-order formulae, and provide efficient algorithms for the resulting clustering problems.

#index 874913
#* The logic of RDF and SPARQL: a tutorial
#@ Enrico Franconi;Sergio Tessaris
#t 2006
#c 5
#% 140407
#% 801677
#% 1655420
#% 1721025
#! The Resource Description Framework (RDF [Hayes, 2004]) is a W3C standard language for representing information about resources in the World Wide Web; RDF provides a common framework for expressing this information so it can be exchanged between applications without loss of meaning. In this tutorial, RDF will be presented as a data model in the database sense. Its motivations will be analysed, and its current formal status revised. The data model can be understood both from a graph theoretical perspective and from a logical perspective. While the former has been the focus of most theoretical (see, e.g., [Gutierrez et al., 2004]) and practical approaches to RDF, the logical view of RDF has been mostly neglected by the community so far. Two provably correct (w.r.t. the normative W3C definitions of RDF [Hayes, 2004]) logical reconstructions of RDF will be presented, by reducing (a fragment of) it to a classical first-order framework suitable for knowledge representation (first developed in [de Bruijn et al., 2005]), and by encoding the full RDF data model in the HiLog logic introduced by Kifer et al. several years ago [Chen et al., 1993]. An emphasis will be given to three main characteristics of RDF: the presence of anonymous bnodes, the non-well-foundedness of the basic rdf:type relation, and the presence of the RDF vocabulary in the mode itself.In the second part of the tutorial, the relation of the logical reconstructions of RDF with a database perspective will be introduced. An RDF database is seen as a model of a suitable theory in first order logic or in HiLog. While in the pure RDF sense the two approaches are equivalent, it will be shown how the difference becomes relevant whenever additional constraints (e.g., in the form of ontologies or database dependencies) are introduced in the framework. In order to allow for additional constraints (e.g., in the standard W3C OWL-DL ontology language [Patel-Schneider et al., 2004]) while keeping the framework first order, only a fragment of RDF can be considered; this restriction is not needed if the framework is in HiLog (see, e.g., [Motik, 2005]). Various complexity and decidability results will be summarised. In the last part of the tutorial, the W3C standard query language for RDF (SPARQL [Prud'hommeaux and Seaborne, 2006]) will be presented. SPARQL is currently a candidate recommendation. The core of SPARQL is a conjunctive query language, with the added complication that the data model includes existential information in the form of bnodes, and that bnodes may be returned by the query. The formal semantics of the core query language will be given. The problem of the canonical representation of the answer set will be introduced, since bnodes introduce a behaviour similar to the null values in SQL. Complexity results for query answering will be given for different cases. Finally, the possible extensions of SPARQL with various classes of constraints will be discussed.

#index 874914
#* On the decidability and finite controllability of query processing in databases with incomplete information
#@ Riccardo Rosati
#t 2006
#c 5
#% 16
#% 583
#% 2463
#% 11817
#% 36181
#% 69283
#% 154067
#% 205851
#% 230142
#% 248038
#% 248039
#% 264858
#% 273687
#% 299943
#% 342389
#% 378409
#% 384978
#% 398752
#% 416043
#% 443173
#% 465057
#% 490909
#% 572311
#% 576100
#% 576116
#% 801676
#% 1289425
#! In this paper we study queries over relational databases with integrity constraints (ICs). The main problem we analyze is OWA query answering, i.e., query answering over a database with ICs under open-world assumption. The kinds of ICs that we consider are functional dependencies (in particular key dependencies) and inclusion dependencies; the query languages we consider are conjunctive queries (CQs), union of conjunctive queries (UCQs), CQs and UCQs with negation and/or inequality. We present a set of results about the decidability and finite controllability of OWA query answering under ICs. In particular: (i) we identify the decidability/undecidability frontier for OWA query answering under different combinations of the ICs allowed and the query language allowed; (ii) we study OWA query answering both over finite databases and over unrestricted databases, and identify the cases in which such a problem is finitely controllable, i.e., when OWA query answering over finite databases coincides with OWA query answering over unrestricted databases. Moreover, we are able to easily turn the above results into new results about implication of ICs and query containment under ICs, due to the deep relationship between OWA query answering and these two classical problems in database theory. In particular, we close two long-standing open problems in query containment, since we prove finite controllability of containment of conjunctive queries both under arbitrary inclusion dependencies and under key and foreign key dependencies. Besides their theoretical interest, we believe that the results of our investigation are very relevant in many research areas which have recently dealt with databases under an incomplete information assumption: e.g., view-based information access, ontology-based information systems, data integration, data exchange, and peer-to-peer information systems.

#index 874969
#* Proceedings of the 2006 ACM SIGMOD international conference on Management of data
#@ Clement Yu;Peter Scheuermann;Surajit Chaudhuri
#t 2006
#c 5

#index 874970
#* Speeding up search in peer-to-peer networks with a multi-way tree structure
#@ H. V. Jagadish;Beng Chin Ooi;Kian-Lee Tan;Quang Hieu Vu;Rong Zhang
#t 2006
#c 5
#% 213080
#% 340175
#% 340176
#% 453509
#% 505869
#% 610849
#% 612643
#% 659957
#% 727799
#% 730102
#% 731091
#% 745498
#% 770901
#% 772022
#% 824706
#% 864421
#% 963874
#% 1711098
#! Peer-to-Peer systems have recently become a popular means to share resources. Effective search is a critical requirement in such systems, and a number of distributed search structures have been proposed in the literature. Most of these structures provide "log time search" capability, where the logarithm is taken base 2. That is, in a system with N nodes, the cost of the search is O(log2N).In database systems, the importance of large fanout index structures has been well recognized. In P2P search too, the cost could be reduced considerably if this logarithm were taken to a larger base. In this paper, we propose a multi-way tree search structure, which reduces the cost of search to O(logmN), where m is the fanout. The penalty paid is a larger update cost, but we show how to keep this penalty to be no worse than linear in m. We experimentally explore this tradeoff between search and update cost as a function of m, and suggest how to find a good trade-off point.The multi-way tree structure we propose, BATON*, is derived from the BATON structure that has recently been suggested. In addition to multi-way fanout, BATON* also adds support for multi-attribute queries to BATON.

#index 874971
#* Reconciling while tolerating disagreement in collaborative data sharing
#@ Nicholas E. Taylor;Zachary G. Ives
#t 2006
#c 5
#% 35764
#% 70068
#% 114554
#% 169706
#% 184206
#% 201939
#% 209729
#% 229827
#% 237073
#% 273687
#% 286836
#% 340175
#% 340176
#% 340949
#% 481923
#% 505869
#% 654468
#% 715288
#% 801692
#% 805459
#! In many data sharing settings, such as within the biological and biomedical communities, global data consistency is not always attainable: different sites' data may be dirty, uncertain, or even controversial. Collaborators are willing to share their data, and in many cases they also want to selectively import data from others --- but must occasionally diverge when they disagree about uncertain or controversial facts or values. For this reason, traditional data sharing and data integration approaches are not applicable, since they require a globally consistent data instance. Additionally, many of these approaches do not allow participants to make updates; if they do, concurrency control algorithms or inconsistency repair techniques must be used to ensure a consistent view of the data for all users.In this paper, we develop and present a fully decentralized model of collaborative data sharing, in which participants publish their data on an ad hoc basis and simultaneously reconcile updates with those published by others. Individual updates are associated with provenance information, and each participant accepts only updates with a sufficient authority ranking, meaning that each participant may have a different (though conceptually overlapping) data instance. We define a consistency semantics for database instances under this model of disagreement, present algorithms that perform reconciliation for distributed clusters of participants, and demonstrate their ability to handle typical update and conflict loads in settings involving the sharing of curated data.

#index 874972
#* Approximately detecting duplicates for streaming data using stable bloom filters
#@ Fan Deng;Davood Rafiei
#t 2006
#c 5
#% 2833
#% 69273
#% 300179
#% 307424
#% 322884
#% 378388
#% 387508
#% 411437
#% 424292
#% 576112
#% 577371
#% 578389
#% 654461
#% 654497
#% 729913
#% 745513
#% 745534
#% 800590
#% 805840
#% 810044
#% 993949
#% 993960
#% 993980
#% 1015280
#! Traditional duplicate elimination techniques are not applicable to many data stream applications. In general, precisely eliminating duplicates in an unbounded data stream is not feasible in many streaming scenarios. Therefore, we target at approximately eliminating duplicates in streaming environments given a limited space. Based on a well-known bitmap sketch, we introduce a data structure, Stable Bloom Filter, and a novel and simple algorithm. The basic idea is as follows: since there is no way to store the whole history of the stream, SBF continuously evicts the stale information so that SBF has room for those more recent elements. After finding some properties of SBF analytically, we show that a tight upper bound of false positive rates is guaranteed. In our empirical study, we compare SBF to alternative methods. The results show that our method is superior in terms of both accuracy and time effciency when a fixed small space and an acceptable false positive rate are given.

#index 874973
#* Query evaluation using overlapping views: completeness and efficiency
#@ Gang Gou;Maxim Kormilitsin;Rada Chirkova
#t 2006
#c 5
#% 137867
#% 198465
#% 223781
#% 273696
#% 300138
#% 333964
#% 333965
#% 393907
#% 411554
#% 462204
#% 464056
#% 479792
#% 481604
#% 482081
#% 482110
#% 564419
#% 571169
#% 572307
#% 572311
#% 599549
#% 810022
#% 1700143
#! We study the problem of finding efficient equivalent view-based rewritings of relational queries, focusing on query optimization using materialized views under the assumption that base relations cannot contain duplicate tuples. A lot of work in the literature addresses the problems of answering queries using views and query optimization. However, most of it proposes solutions for special cases, such as for conjunctive queries (CQs) or for aggregate queries only. In addition, most of it addresses the problems separately under set or bag-set semantics for query evaluation, and some of it proposes heuristics without formal proofs for completeness or soundness. In this paper we look at the two problems by considering CQ/A queries - that is, both pure conjunctive and aggregate queries, with aggregation functions SUM, COUNT, MIN, and MAX; the DISTINCT keyword in (SQL versions of) our queries is also allowed. We build on past work to provide algorithms that handle this general setting. This is possible because recent results on rewritings of CQ/A queries [1, 8] show that there are sound and complete algorithms based on containment tests of CQs.Our focus is that our algorithms are efficient as well as sound and complete. Besides the contribution we make in putting and addressing the problems in this general setting, we make two additional contributions for bag-set and set semantics. First, we propose efficient sound and complete tests for equivalence of CQ/A queries to rewritings that use overlapping views (the algorithms are complete with respect to the language of rewritings). These results apply not only to query optimization, but to all areas where the goal is to obtain efficient equivalent view-based query rewritings. Second, based on these results we propose two sound algorithms, BDPV and CDPV, that find efficient execution plans for CQ/A queries in terms of materialized views. Both algorithms extend the cost-based query-optimization approach of System R [19]. The efficient sound algorithm BDPV is also complete in some cases, whereas CDPV is sound and complete for all CQ/A queries we consider. We present a study of the completeness-efficiency tradeoff in the algorithms, and provide experimental results that show the viability of our approach and test the limits of query optimization using overlapping views.

#index 874974
#* User-defined aggregate functions: bridging theory and practice
#@ Sara Cohen
#t 2006
#c 5
#% 36683
#% 59350
#% 83144
#% 152928
#% 181035
#% 201929
#% 210182
#% 210208
#% 273696
#% 300138
#% 305947
#% 333965
#% 334006
#% 458550
#% 481128
#% 481293
#% 481604
#% 481608
#% 481923
#% 482081
#% 572307
#% 617880
#% 630967
#% 871765
#! The ability to create user-defined aggregate functions (UDAs) is rapidly becoming a standard feature in relational database systems. Therefore, problems such as query optimization, query rewriting and view maintenance must take into account queries (or views) with UDAs. There is a wealth of research on these problems for queries with general aggregate functions. Unfortunately, there is a mismatch between the manner in which UDAs are created, and the information that the database system requires in order to apply previous research.The purpose of this paper is to explore this mismatch and to bridge the gap between theory and practice, thereby enabling UDAs to become first-class citizens within the database. Specifically, we consider query optimization, query rewriting and view maintenance for queries with UDAs. For each of these problems we first survey previous results and explore the mismatch between theory and practice. We then present theoretical and practical insights that can be combined to derive a coherent framework for defining UDAs within a database system.

#index 874975
#* Supporting ad-hoc ranking aggregates
#@ Chengkai Li;Kevin Chen-Chuan Chang;Ihab F. Ilyas
#t 2006
#c 5
#% 210169
#% 210182
#% 213981
#% 227880
#% 227883
#% 227894
#% 273696
#% 273910
#% 273916
#% 333854
#% 333925
#% 397378
#% 420053
#% 462204
#% 463735
#% 479450
#% 479795
#% 479816
#% 479967
#% 481288
#% 481604
#% 481608
#% 481951
#% 482081
#% 571045
#% 765418
#% 810018
#% 864388
#% 1015308
#% 1015317
#% 1700143
#! This paper presents a principled framework for efficient processing of ad-hoc top-k (ranking) aggregate queries, which provide the k groups with the highest aggregates as results. Essential support of such queries is lacking in current systems, which process the queries in a naïve materialize-group-sort scheme that can be prohibitively inefficient. Our framework is based on three fundamental principles. The Upper-Bound Principle dictates the requirements of early pruning, and the Group-Ranking and Tuple-Ranking Principles dictate group-ordering and tuple-ordering requirements. They together guide the query processor toward a provably optimal tuple schedule for aggregate query processing. We propose a new execution framework to apply the principles and requirements. We address the challenges in realizing the framework and implementing new query operators, enabling efficient group-aware and rank-aware query plans. The experimental study validates our framework by demonstrating orders of magnitude performance improvement in the new query plans, compared with the traditional plans.

#index 874976
#* MauveDB: supporting model-based user views in database systems
#@ Amol Deshpande;Samuel Madden
#t 2006
#c 5
#% 663
#% 18521
#% 64413
#% 102756
#% 215225
#% 235023
#% 248813
#% 279164
#% 309433
#% 330413
#% 341700
#% 401228
#% 414174
#% 442830
#% 480459
#% 480789
#% 654487
#% 659966
#% 751027
#% 761322
#% 765402
#% 810645
#% 864417
#% 1289474
#! Real-world data --- especially when generated by distributed measurement infrastructures such as sensor networks --- tends to be incomplete, imprecise, and erroneous, making it impossible to present it to users or feed it directly into applications. The traditional approach to dealing with this problem is to first process the data using statistical or probabilistic models that can provide more robust interpretations of the data. Current database systems, however, do not provide adequate support for applying models to such data, especially when those models need to be frequently updated as new data arrives in the system. Hence, most scientists and engineers who depend on models for managing their data do not use database systems for archival or querying at all; at best, databases serve as a persistent raw data store.In this paper we define a new abstraction called model-based views and present the architecture of MauveDB, the system we are building to support such views. Just as traditional database views provide logical data independence, model-based views provide independence from the details of the underlying data generating mechanism and hide the irregularities of the data by using models to present a consistent view to the users. MauveDB supports a declarative language for defining model-based views, allows declarative querying over such views using SQL, and supports several different materialization strategies and techniques to efficiently maintain them in the face of frequent updates. We have implemented a prototype system that currently supports views based on regression and interpolation, using the Apache Derby open source DBMS, and we present results that show the utility and performance benefits that can be obtained by supporting several different types of model-based views in a database system.

#index 874977
#* Database support for matching: limitations and opportunities
#@ Ameet Kini;Srinath Shankar;Jeffrey F. Naughton;David J. Dewitt
#t 2006
#c 5
#% 53085
#% 88364
#% 122671
#% 300170
#% 300180
#% 347174
#% 479816
#% 565445
#% 610668
#% 777931
#% 836518
#% 1015319
#% 1016182
#! We define a match join of R and S with predicate θ to be a subset of the θ-join of R and S such that each tuple of R and S contributes to at most one result tuple. Match joins and their generalizations belong to a broad class of matching problems that have attracted a great deal of attention in disciplines including operations research and theoretical computer science. Instances of these problems arise in practice in resource allocation scenarios. To the best of our knowledge no one uses an RDBMS as a tool to help solve these problems; our goal in this paper is to explore whether or not this needs to be the case. We show that the simple approach of computing the full θ-join and then applying standard graph-matching algorithms to the result is ineffective for all but the smallest of problem instances. By contrast, a closer study shows that the DBMS primitives of grouping, sorting, and joining can be exploited to yield efficient match join operations. This suggests that RDBMSs can play a role in matching related problems beyond merely serving as expensive file systems exporting data sets to external user programs.

#index 874978
#* Declarative networking: language, execution and optimization
#@ Boon Thau Loo;Tyson Condie;Minos Garofalakis;David E. Gay;Joseph M. Hellerstein;Petros Maniatis;Raghu Ramakrishnan;Timothy Roscoe;Ion Stoica
#t 2006
#c 5
#% 5965
#% 11797
#% 17084
#% 29253
#% 86929
#% 152928
#% 159733
#% 205153
#% 286378
#% 333844
#% 340175
#% 424907
#% 465043
#% 480602
#% 577219
#% 579687
#% 761220
#% 764971
#% 780670
#% 809267
#% 821939
#% 835186
#! The networking and distributed systems communities have recently explored a variety of new network architectures, both for application-level overlay networks, and as prototypes for a next-generation Internet architecture. In this context, we have investigated declarative networking: the use of a distributed recursive query engine as a powerful vehicle for accelerating innovation in network architectures [23, 24, 33]. Declarative networking represents a significant new application area for database research on recursive query processing. In this paper, we address fundamental database issues in this domain. First, we motivate and formally define the Network Datalog (NDlog) language for declarative network specifications. Second, we introduce and prove correct relaxed versions of the traditional semi-naïve query evaluation technique, to overcome fundamental problems of the traditional technique in an asynchronous distributed setting. Third, we consider the dynamics of network state, and formalize the iheventual consistencyl. of our programs even when bursts of updates can arrive in the midst of query execution. Fourth, we present a number of query optimization opportunities that arise in the declarative networking context, including applications of traditional techniques as well as new optimizations. Last, we present evaluation results of the above ideas implemented in our P2 declarative networking system, running on 100 machines over the Emulab network testbed.

#index 874979
#* Forensic analysis of database tampering
#@ Kyriacos Pavlou;Richard T. Snodgrass
#t 2006
#c 5
#% 54711
#% 284594
#% 286473
#% 442920
#% 805459
#% 810041
#% 810114
#% 963647
#% 1016171
#! Mechanisms now exist that detect tampering of a database, through the use of cryptographically-strong hash functions. This paper addresses the next problem, that of determining who, when, and what, by providing a systematic means of performing forensic analysis after such tampering has been uncovered. We introduce a schematic representation termed a "corruption diagram" that aids in intrusion investigation. We use these diagrams to fully analyze the original proposal, that of a linked sequence of hash values. We examine the various kinds of intrusions that are possible, including retroactive, introactive, backdating, and postdating intrusions. We then introduce successively more sophisticated forensic analysis algorithms: the monochromatic, RGB, and polychromatic algorithms, and characterize the "forensic strength" of these algorithms. We show how forensic analysis can efficiently extract a good deal of information concerning a corruption event.

#index 874980
#* Dynamic authenticated index structures for outsourced databases
#@ Feifei Li;Marios Hadjieleftheriou;George Kollios;Leonid Reyzin
#t 2006
#c 5
#% 39029
#% 300184
#% 317933
#% 319994
#% 397367
#% 513367
#% 566391
#% 576111
#% 657774
#% 659992
#% 669725
#% 745532
#% 761411
#% 765447
#% 772846
#% 810042
#% 810097
#% 824701
#% 838424
#% 1015329
#% 1015368
#% 1016189
#! In outsourced database (ODB)systems the database owner publishes its data through a number of remote servers, with the goal of enabling clients at the edge of the network to access and query the data more efficiently. As servers might be untrusted or can be compromised, query authentication becomes an essential component of ODB systems. Existing solutions for this problem concentrate mostly on static scenarios and are based on idealistic properties for certain cryptographic primitives. In this work, first we define a variety of essential and practical cost metrics associated with ODB systems. Then, we analytically evaluate a number of different approaches, in search for a solution that best leverages all metrics. Most importantly, we look at solutions that can handle dynamic scenarios, where owners periodically update the data residing at the servers. Finally, we discuss query freshness, a new dimension in data authentication that has not been explored before. A comprehensive experimental evaluation of the proposed and existing approaches is used to validate the analytical models and verify our claims. Our findings exhibit that the proposed solutions improve performance substantially over existing approaches, both for static and dynamic environments.

#index 874981
#* Redundancy and information leakage in fine-grained access control
#@ Govind Kabra;Ravishankar Ramamurthy;S. Sudarshan
#t 2006
#c 5
#% 443382
#% 462501
#% 565457
#% 599549
#% 606353
#% 765447
#% 800603
#% 1016138
#! The current SQL standard for access control is coarse grained, in that it grants access to all rows of a table or none. Fine-grained access control, which allows control of access at the granularity of individual rows, and to specific columns within those rows, is required in practically all database applications. There are several models for fine grained access control, but the majority of them follow a view replacement strategy. There are two significant problems with most implementations of the view replacement model, namely (a) the unnecessary overhead of the access control predicates when they are redundant and (b) the potential of information leakage through channels such as user-defined functions, and operations that cause exceptions and error messages. We first propose techniques for redundancy removal. We then define when a query plan is safe with respect to UDFs and other unsafe functions, and propose techniques to generate safe query plans. We have prototyped redundancy removal and safe UDF pushdown on the Microsoft SQL Server query optimizer, and present a preliminary performance study.

#index 874982
#* Contour map matching for event detection in sensor networks
#@ Wenwei Xue;Qiong Luo;Lei Chen;Yunhao Liu
#t 2006
#c 5
#% 280408
#% 309433
#% 356568
#% 481611
#% 654482
#% 745442
#% 765403
#% 765445
#% 787175
#% 800503
#% 805466
#% 810031
#% 824715
#% 837279
#% 879284
#% 1016178
#! Many sensor network applications, such as object tracking and disaster monitoring, require effective techniques for event detection. In this paper, we propose a novel event detection mechanism based on matching the contour maps of in-network sensory data distribution. Our key observation is that events in sensor networks can be abstracted into spatio-temporal patterns of sensory data and that pattern matching can be done efficiently through contour map matching. Therefore, we propose simple SQL extensions to allow users to specify common types of events as patterns in contour maps and study energy-efficient techniques of contour map construction and maintenance for our pattern-based event detection. Our experiments with synthetic workloads derived from a real-world coal mine surveillance application validate the effectiveness and efficiency of our approach.

#index 874983
#* Constraint chaining: on energy-efficient continuous monitoring in sensor networks
#@ Adam Silberstein;Rebecca Braynard;Jun Yang
#t 2006
#c 5
#% 297915
#% 410276
#% 427022
#% 449869
#% 720035
#% 731087
#% 751030
#% 765402
#% 765445
#% 800503
#% 805466
#% 810031
#% 837279
#% 864435
#! Wireless sensor networks have created new opportunities for data collection in a variety of scenarios, such as environmental and industrial, where we expect data to be temporally and spatially correlated. Researchers may want to continuously collect all sensor data from the network for later analysis. Suppression, both temporal and spatial, provides opportunities for reducing the energy cost of sensor data collection. We demonstrate how both types can be combined for maximal benefit. We frame the problem as one of monitoring node and edge constraints. A monitored node triggers a report if its value changes. A monitored edge triggers a report if the difference between its nodes' values changes. The set of reports collected at the base station is used to derive all node values. We fully exploit the potential of this global inference in our algorithm, CONCH, short for constraint chaining. Constraint chaining builds a network of constraints that are maintained locally, but allow a global view of values to be maintained with minimal cost. Network failure complicates the use of suppression, since either causes an absence of reports. We add enhancements to CONCH to build in redundant constraints and provide a method to interpret the resulting reports in case of uncertainty. Using simulation we experimentally evaluate CONCH's effectiveness against competing schemes in a number of interesting scenarios.

#index 874984
#* Energy-efficient monitoring of extreme values in sensor networks
#@ Adam Silberstein;Kamesh Munagala;Jun Yang
#t 2006
#c 5
#% 333969
#% 576113
#% 622760
#% 745442
#% 751030
#% 783740
#% 805466
#% 810553
#% 824654
#% 864455
#! Monitoring extreme values (MAX or MIN) is a fundamental problem in wireless sensor networks (and in general, complex dynamic systems). This problem presents very different algorithmic challenges from aggregate and selection queries, in the sense that an individual node cannot by itself determine its inclusion in the query result. We present novel query processing algorithms for this problem, with the goal of minimizing message traffic in the network. These algorithms employ a hierarchy of local constraints, or thresholds, to leverage network topology such that message-passing is localized. We evaluate all algorithms using simulated and real-world data to study various trade-offs.

#index 874985
#* Modeling skew in data streams
#@ Flip Korn;S. Muthukrishnan;Yihua Wu
#t 2006
#c 5
#% 164360
#% 263480
#% 333926
#% 342592
#% 378388
#% 379445
#% 424281
#% 449100
#% 453512
#% 481620
#% 481782
#% 576112
#% 654497
#% 765404
#% 781691
#% 801696
#% 805889
#% 806686
#% 864468
#% 1809763
#% 1828078
#! Data stream applications have made use of statistical summaries to reason about the data using nonparametric tools such as histograms, heavy hitters, and join sizes. However, relatively little attention has been paid to modeling stream data parametrically, despite the potential this approach has for mining the data. The challenges to do model fitting at streaming speeds are both technical -- how to continually find fast and reliable parameter estimates on high speed streams of skewed data using small space -- and conceptual -- how to validate the goodness-of-fit and stability of the model online.In this paper, we show how to fit hierarchical (binomial multifractal) and non-hierarchical (Pareto) power-law models on a data stream. We address the technical challenges using an approach that maintains a sketch of the data stream and fits least-squares straight lines; it yields algorithms that are fast, space-efficient, and provide approximations of parameter value estimates with a priori quality guarantees relative to those obtained offline. We address the conceptual challenge by designing fast methods for online goodness-of-fit measurements on a data stream; we adapt the statistical testing technique of examining the quantile-quantile (q-q) plot, to perform online model validation at streaming speeds.As a concrete application of our techniques, we focus on network traffic data which has been shown to exhibit skewed distributions. We complement our analytic and algorithmic results with experiments on IP traffic streams in AT&T's Gigascope® data stream management system, to demonstrate practicality of our methods at line speeds. We measured the stability and robustness of these models over weeks of operational packet data in an IP network. In addition, we study an intrusion detection application, and demonstrate the potential of online parametric modeling.

#index 874986
#* Fast range-summable random variables for efficient aggregate estimation
#@ Florin Rusu;Alin Dobra
#t 2006
#c 5
#% 5351
#% 176258
#% 214073
#% 379443
#% 397354
#% 397385
#% 419377
#% 578390
#% 654463
#% 674702
#% 675515
#% 675516
#% 723903
#% 765459
#% 800495
#% 813797
#% 813981
#! Exact computation for aggregate queries usually requires large amounts of memory - constrained in data-streaming - or communication - constrained in distributed computation - and large processing times. In this situation, approximation techniques with provable guarantees, like sketches, are the only viable solution. The performance of sketches crucially depends on the ability to efficiently generate particular pseudo-random numbers. In this paper we investigate both theoretically and empirically the problem of generating k-wise independent pseudo-random numbers and, in particular, that of generating 3 and 4-wise independent pseudo-random numbers that are fast range-summable (i.e., they can be summed up in sub-linear time). Our specific contributions are: (a) we provide an empirical comparison of the various pseudo-random number generating schemes, (b) we study both theoretically and empirically the fast range-summation practicality for the 3 and 4-wise independent generating schemes and we provide efficient implementations for the 3-wise independent schemes, (c) we show convincing theoretical and empirical evidence that the extended Hamming scheme performs as well as any 4-wise independent scheme for estimating the size of join using AMS-sketches, even though it is only 3-wise independent. We use this generating scheme to produce estimators that significantly out-perform the state-of-the-art solutions for two problems - size of spatial joins and selectivity estimation.

#index 874987
#* Graph-based synopses for relational selectivity estimation
#@ Joshua Spiegel;Neoklis Polyzotis
#t 2006
#c 5
#% 145196
#% 210173
#% 210188
#% 210190
#% 214073
#% 248792
#% 248822
#% 273901
#% 273908
#% 273909
#% 333946
#% 333986
#% 397354
#% 480306
#% 765423
#% 816392
#! This paper introduces the Tuple Graph (TUG) synopses, a new class of data summaries that enable accurate selectivity estimates for complex relational queries. The proposed summarization framework adopts a "semi-structured" view of the relational database, modeling a relational data set as a graph of tuples and join queries as graph traversals respectively. The key idea is to approximate the structure of the induced data graph in a concise synopsis, and to estimate the selectivity of a query by performing the corresponding traversal over the summarized graph. We detail the TUG synopsis model that is based on this novel approach, and we describe an efficient and scalable construction algorithm for building accurate TUGs within a specific storage budget. We validate the performance of TUGs with an extensive experimental study on real-life and synthetic data sets. Our results verify the effectiveness of TUGs in generating accurate selectivity estimates for complex join queries, and demonstrate their benefits over existing summarization techniques.

#index 874988
#* Injecting utility into anonymized datasets
#@ Daniel Kifer;Johannes Gehrke
#t 2006
#c 5
#% 44876
#% 67453
#% 211044
#% 300184
#% 333876
#% 443463
#% 528180
#% 576110
#% 576111
#% 576761
#% 577239
#% 756489
#% 765449
#% 800515
#% 801690
#% 809244
#% 810011
#% 824726
#% 844340
#% 864412
#% 1707132
#! Limiting disclosure in data publishing requires a careful balance between privacy and utility. Information about individuals must not be revealed, but a dataset should still be useful for studying the characteristics of a population. Privacy requirements such as k-anonymity and l-diversity are designed to thwart attacks that attempt to identify individuals in the data and to discover their sensitive information. On the other hand, the utility of such data has not been well-studied.In this paper we will discuss the shortcomings of current heuristic approaches to measuring utility and we will introduce a formal approach to measuring utility. Armed with this utility metric, we will show how to inject additional information into k-anonymous and l-diverse tables. This information has an intuitive semantic meaning, it increases the utility beyond what is possible in the original k-anonymity and l-diversity frameworks, and it maintains the privacy guarantees of k-anonymity and l-diversity.

#index 874989
#* Personalized privacy preservation
#@ Xiaokui Xiao;Yufei Tao
#t 2006
#c 5
#% 67453
#% 210160
#% 443463
#% 576110
#% 576762
#% 577239
#% 785363
#% 800514
#% 800515
#% 801690
#% 809246
#% 810011
#% 810028
#% 824726
#% 824727
#% 864406
#% 864412
#% 951837
#% 1700134
#! We study generalization for preserving privacy in publication of sensitive data. The existing methods focus on a universal approach that exerts the same amount of preservation for all persons, with-out catering for their concrete needs. The consequence is that we may be offering insufficient protection to a subset of people, while applying excessive privacy control to another subset. Motivated by this, we present a new generalization framework based on the concept of personalized anonymity. Our technique performs the minimum generalization for satisfying everybody's requirements, and thus, retains the largest amount of information from the microdata. We carry out a careful theoretical study that leads to valuable insight into the behavior of alternative solutions. In particular, our analysis mathematically reveals the circumstances where the previous work fails to protect privacy, and establishes the superiority of the proposed solutions. The theoretical findings are verified with extensive experiments.

#index 874990
#* Simultaneous scalability and security for data-intensive web applications
#@ Amit Manjhi;Anastassia Ailamaki;Bruce M. Maggs;Todd C. Mowry;Christopher Olston;Anthony Tomasic
#t 2006
#c 5
#% 59350
#% 200966
#% 340301
#% 397367
#% 397402
#% 434043
#% 481128
#% 659992
#% 765448
#% 805474
#% 993978
#% 1015314
#% 1015362
#! For Web applications in which the database component is the bottleneck, scalability can be provided by a third-party Database Scalability Service Provider (DSSP) that caches application data and supplies query answers on behalf of the application. Cost-effective DSSPs will need to cache data from many applications, inevitably raising concerns about security. However, if all data passing through a DSSP is encrypted to enhance security, then data updates trigger invalidation of large regions of cache. Consequently, achieving good scalability becomes virtually impossible. There is a tradeoff between security and scalability, which requires careful consideration.In this paper we study the security-scalability tradeoff, both formally and empirically. We begin by providing a method for statically identifying segments of the database that can be encrypted without impacting scalability. Experiments over a prototype DSSP system show the effectiveness of our static analysis method--for all three realistic bench-mark applications that we study, our method enables a significant fraction of the database to be encrypted without impacting scalability. Moreover, most of the data that can be encrypted without impacting scalability is of the type that application designers will want to encrypt, all other things being equal. Based on our static analysis method, we propose a new scalability-conscious security design methodology that features: (a) compulsory encryption of highly sensitive data like credit card information, and (b) encryption of data for which encryption does not impair scalability. As a result, the security-scalability tradeoff needs to be considered only over data for which encryption impacts scalability, thus greatly simplifying the task of managing the tradeoff.

#index 874991
#* Interactive query formulation over web service-accessed sources
#@ Michalis Petropoulos;Alin Deutsch;Yannis Papakonstantinou
#t 2006
#c 5
#% 198466
#% 213982
#% 223647
#% 273923
#% 342359
#% 384978
#% 393907
#% 479449
#% 482116
#% 572307
#% 572311
#% 572314
#% 765447
#% 1016138
#! Integration systems typically support only a restricted set of queries over the schema they export. The reason is that the participating information sources contribute limited content and limited access methods. In prior work, these limited access methods have often been specified using a set of parameterized views, with the understanding that the integration system accepts only queries which have an equivalent rewriting using the views. These queries are called feasible. Infeasible queries are rejected without an explanatory feedback. To help a developer, who is building an integration application, avoid a frustrating trial-and-error cycle, we introduce the CLIDE query formulation interface, which extends the QBE-like query builder of Microsoft's SQL Server with a coloring scheme that guides the user toward formulating feasible queries. We provide guarantees that the suggested query edit actions are complete (i.e. each feasible query can be built by following only suggestions), rapidly convergent (the suggestions are tuned to lead to the closest feasible completions of the query) and suitably summarized (at each interaction step, only a minimal number of actions needed to preserve completeness are suggested). We present the algorithms, implementation and performance evaluation showing that CLIDE is a viable on-line tool.

#index 874992
#* To search or to crawl?: towards a query optimizer for text-centric tasks
#@ Panagiotis G. Ipeirotis;Eugene Agichtein;Pranay Jain;Luis Gravano
#t 2006
#c 5
#% 68611
#% 194246
#% 240955
#% 248821
#% 273926
#% 281251
#% 287461
#% 287463
#% 301241
#% 340146
#% 344447
#% 348138
#% 423979
#% 447946
#% 464044
#% 478258
#% 480309
#% 504443
#% 613977
#% 654467
#% 754068
#% 783439
#% 795959
#% 805883
#% 809418
#! Text is ubiquitous and, not surprisingly, many important applications rely on textual data for a variety of tasks. As a notable example, information extraction applications derive structured relations from unstructured text; as another example, focused crawlers explore the web to locate pages about specific topics. Execution plans for text-centric tasks follow two general paradigms for processing a text database: either we can scan, or 'crawl," the text database or, alternatively, we can exploit search engine indexes and retrieve the documents of interest via carefully crafted queries constructed in task-specific ways. The choice between crawl- and query-based execution plans can have a substantial impact on both execution time and output "completeness" (e.g., in terms of recall). Nevertheless, this choice is typically ad-hoc and based on heuristics or plain intuition. In this paper, we present fundamental building blocks to make the choice of execution plans for text-centric tasks in an informed, cost-based way. Towards this goal, we show how to analyze query- and crawl-based plans in terms of both execution time and output completeness. We adapt results from random-graph theory and statistics to develop a rigorous cost model for the execution plans. Our cost model reflects the fact that the performance of the plans depends on fundamental task-specific properties of the underlying text databases. We identify these properties and present efficient techniques for estimating the associated cost-model parameters. Overall, our approach helps predict the most appropriate execution plans for a task, resulting in significant efficiency and output completeness benefits. We complement our results with a large-scale experimental evaluation for three important text-centric tasks and over multiple real-life data sets.

#index 874993
#* Efficient query processing in geographic web search engines
#@ Yen-Yu Chen;Torsten Suel;Alexander Markowetz
#t 2006
#c 5
#% 86950
#% 210172
#% 212665
#% 213786
#% 213981
#% 248794
#% 252304
#% 268079
#% 281251
#% 285932
#% 290703
#% 292684
#% 330677
#% 333854
#% 340141
#% 340886
#% 340888
#% 387427
#% 397151
#% 413782
#% 420492
#% 435137
#% 480467
#% 527181
#% 577302
#% 730051
#% 766441
#% 805864
#% 836096
#% 838407
#% 993962
#% 1015265
#% 1720754
#! Geographic web search engines allow users to constrain and order search results in an intuitive manner by focusing a query on a particular geographic region. Geographic search technology, also called local search, has recently received significant interest from major search engine companies. Academic research in this area has focused primarily on techniques for extracting geographic knowledge from the web. In this paper, we study the problem of efficient query processing in scalable geographic search engines. Query processing is a major bottleneck in standard web search engines, and the main reason for the thousands of machines used by the major engines. Geographic search engine query processing is different in that it requires a combination of text and spatial data processing techniques. We propose several algorithms for efficient query processing in geographic search engines, integrate them into an existing web search query processor, and evaluate them on large sets of real data and query traces.

#index 874994
#* Communication-efficient distributed monitoring of thresholded counts
#@ Ram Keralapura;Graham Cormode;Jeyashankher Ramamirtham
#t 2006
#c 5
#% 446438
#% 654443
#% 654488
#% 654497
#% 654507
#% 654508
#% 654510
#% 745442
#% 783741
#% 810009
#% 810031
#% 810095
#% 816392
#% 824652
#% 854553
#% 864444
#% 874995
#! Monitoring is an issue of primary concern in current and next generation networked systems. For ex, the objective of sensor networks is to monitor their surroundings for a variety of different applications like atmospheric conditions, wildlife behavior, and troop movements among others. Similarly, monitoring in data networks is critical not only for accounting and management, but also for detecting anomalies and attacks. Such monitoring applications are inherently continuous and distributed, and must be designed to minimize the communication overhead that they introduce. In this context we introduce and study a fundamental class of problems called "thresholded counts" where we must return the aggregate frequency count of an event that is continuously monitored by distributed nodes with a user-specified accuracy whenever the actual count exceeds a given threshold value.In this paper we propose to address the problem of thresholded counts by setting local thresholds at each monitoring node and initiating communication only when the locally observed data exceeds these local thresholds. We explore algorithms in two categories: static and adaptive thresholds. In the static case, we consider thresholds based on a linear combination of two alternate strategies, and show that there exists an optimal blend of the two strategies that results in minimum communication overhead. We further show that this optimal blend can be found using a steepest descent search. In the adaptive case, we propose algorithms that adjust the local thresholds based on the observed distributions of updated information. We use extensive simulations not only to verify the accuracy of our algorithms and validate our theoretical results, but also to evaluate the performance of our algorithms. We find that both approaches yield significant savings over the naive approach of centralized processing.

#index 874995
#* A geometric approach to monitoring threshold functions over distributed data streams
#@ Izchak Sharfman;Assaf Schuster;Daniel Keren
#t 2006
#c 5
#% 116082
#% 214073
#% 336610
#% 378388
#% 397353
#% 397443
#% 428155
#% 443298
#% 492912
#% 632090
#% 654443
#% 654488
#% 660004
#% 763708
#% 800582
#% 800900
#% 801696
#% 810009
#% 874994
#% 993960
#% 993961
#! Monitoring data streams in a distributed system is the focus of much research in recent years. Most of the proposed schemes, however, deal with monitoring simple aggregated values, such as the frequency of appearance of items in the streams. More involved challenges, such as the important task of feature selection (e.g., by monitoring the information gain of various features), still require very high communication overhead using naive, centralized algorithms. We present a novel geometric approach by which an arbitrary global monitoring task can be split into a set of constraints applied locally on each of the streams. The constraints are used to locally filter out data increments that do not affect the monitoring outcome, thus avoiding unnecessary communication. As a result, our approach enables monitoring of arbitrary threshold functions over distributed data streams in an efficient manner. We present experimental results on real-world data which demonstrate that our algorithms are highly scalable, and considerably reduce communication load in comparison to centralized algorithms.

#index 874996
#* Continuous query processing in data streams using duality of data and queries
#@ Hyo-Sang Lim;Jae-Gil Lee;Min-Jae Lee;Kyu-Young Whang;Il-Yeol Song
#t 2006
#c 5
#% 64431
#% 86945
#% 111315
#% 116082
#% 152937
#% 248796
#% 300179
#% 378388
#% 415957
#% 443303
#% 479453
#% 479649
#% 510675
#% 566113
#% 578560
#% 587731
#% 765453
#% 993948
#! Recent data stream systems such as TelegraphCQ have employed the well-known property of duality between data and queries. In these systems, query processing methods are classified into two dual categories -- data-initiative and query-initiative -- depending on whether query processing is initiated by selecting a data element or a query. Although the duality property has been widely recognized, previous data stream systems do not fully take advantages of this property since they use the two dual methods independently: data-initiative methods only for continuous queries and query-initiative methods only for ad-hoc queries. We contend that continuous query processing can be better optimized by adopting an approach that integrates the two dual methods. Our primary contribution is based on the observation that spatial join is a powerful tool for achieving this objective. In this paper, we first present a new viewpoint of transforming the continuous query processing problem to a multi-dimensional spatial join problem. We then present a continuous query processing algorithm based on spatial join, which we name Spatial Join CQ. This algorithm processes continuous queries by finding the pairs of overlapping regions from a set of data elements and a set of queries, both defined as regions in the multi-dimensional space. The algorithm achieves the advantages of the two dual methods simultaneously. Experimental results show that the proposed algorithm outperforms earlier algorithms by up to 36 times for simple selection continuous queries and by up to 7 times for sliding window join queries.

#index 874997
#* GPUTeraSort: high performance graphics co-processor sorting for large database management
#@ Naga Govindaraju;Jim Gray;Ritesh Kumar;Dinesh Manocha
#t 2006
#c 5
#% 14204
#% 41684
#% 86928
#% 112212
#% 152688
#% 172911
#% 189744
#% 210185
#% 227914
#% 282235
#% 287981
#% 340670
#% 341100
#% 341704
#% 378397
#% 397361
#% 435159
#% 453512
#% 479819
#% 479821
#% 480119
#% 480464
#% 480821
#% 510628
#% 566122
#% 629126
#% 654479
#% 765419
#% 803594
#% 810059
#% 850734
#% 850735
#% 993947
#! We present a novel external sorting algorithm using graphics processors (GPUs) on large databases composed of billions of records and wide keys. Our algorithm uses the data parallelism within a GPU along with task parallelism by scheduling some of the memory-intensive and compute-intensive threads on the GPU. Our new sorting architecture provides multiple memory interfaces on the same PC -- a fast and dedicated memory interface on the GPU along with the main memory interface for CPU computations. As a result, we achieve higher memory bandwidth as compared to CPU-based algorithms running on commodity PCs. Our approach takes into account the limited communication bandwidth between the CPU and the GPU, and reduces the data communication between the two processors. Our algorithm also improves the performance of disk transfers and achieves close to peak I/O performance. We have tested the performance of our algorithm on the SortBenchmark and applied it to large databases composed of a few hundred Gigabytes of data. Our results on a 3 GHz Pentium IV PC with $300 NVIDIA 7800 GT GPU indicate a significant performance improvement over optimized CPU-based algorithms on high-end PCs with 3.6 GHz Dual Xeon processors. Our implementation is able to outperform the current high-end PennySort benchmark and results in a higher performance to price ratio. Overall, our results indicate that using a GPU as a co-processor can significantly improve the performance of sorting algorithms on large databases.

#index 874998
#* Recovery from "bad" user transactions
#@ David Lomet;Zografoula Vagena;Roger Barga
#t 2006
#c 5
#% 9241
#% 58371
#% 114582
#% 135384
#% 442705
#% 480096
#% 481126
#% 631961
#% 864422
#! User written transaction code is responsible for the "C" in ACID transactions, i.e., taking the database from one consistent state to the next. However, user transactions can be flawed and lead to inconsistent (or invalid) states. Database systems usually correct invalid data using "point in time" recovery, a costly process that installs a backup and rolls it forward. The result is long outages and the "de-commit" of many valid transactions, which must then be re-submitted, frequently manually. We have implemented in our transaction-time database system a technique in which only data tainted by a flawed transaction and transactions dependent upon its updates are "removed". This process identifies and quarantines tainted data despite the complication of determining transactions dependent on data written by the flawed transaction. A further property of our implementation is that no backup needs to be installed for this because the prior transaction-time states provide an online backup.

#index 874999
#* Run-time operator state spilling for memory intensive long-running queries
#@ Bin Liu;Yali Zhu;Elke Rundensteiner
#t 2006
#c 5
#% 86929
#% 159341
#% 273918
#% 300141
#% 300179
#% 378388
#% 397353
#% 480595
#% 480943
#% 480966
#% 654462
#% 726621
#% 745488
#% 824720
#% 824781
#% 993948
#% 1015278
#% 1015280
#! Main memory is a critical resource when processing long-running queries over data streams with state intensive operators. In this work, we investigate state spill strategies that handle run-time memory shortage when processing such complex queries by selectively pushing operator states into disks. Unlike previous solutions which all focus on one single operator only, we instead target queries with multiple state intensive operators. We observe an interdependency among multiple operators in the query plan when spilling operator states. We illustrate that existing strategies, which do not take account of this interdependency, become largely ineffective in this query context. Clearly, a consolidated plan level spill strategy must be devised to address this problem. Several data spill strategies are proposed in this paper to maximize the run-time query throughput in memory constrained environments. The bottom-up state spill strategy is an operator-level strategy that treats all data in one operator state equally. More sophisticated partition-level data spill strategies are then proposed to take different characteristics of the input data into account, including the local output, the global output and the global output with penalty strategies. All proposed state spill strategies have been implemented in the D-CAPE continuous query system. The experimental results confirm the effectiveness of our proposed strategies. In particular, the global output strategy and the global output with penalty strategy have shown favorable results as compared to the other two more localized strategies.

#index 875000
#* Boolean + ranking: querying a database by k-constrained optimization
#@ Zhen Zhang;Seung-won Hwang;Kevin Chen-Chuan Chang;Min Wang;Christian A. Lang;Yuan-chi Chang
#t 2006
#c 5
#% 132779
#% 210172
#% 213981
#% 248804
#% 301184
#% 333854
#% 396736
#% 396739
#% 397378
#% 480330
#% 480819
#% 576214
#% 765418
#% 810018
#! The wide spread of databases for managing structured data, compounded with the expanded reach of the Internet, has brought forward interesting data retrieval and analysis scenarios to RDBMS. In such settings, queries often take the form of k-constrained optimization, with a Boolean constraint and a numeric optimization expression as the goal function, retrieving only the top-k tuples. This paper proposes the concept of supporting such queries, as their nature implies, by a functional optimization machinery over the search space of multiple indices. To realize this concept, we combine the dual perspectives of discrete state search (from the view of indices) and continuous function optimization (from the view of goal functions). We present, as the marriage of the two perspectives, the OPT* framework, which encodes k-constrained optimization as an A* search over the composite space of multiple indices, driven by functional optimization for providing tight heuristics. By processing queries as optimization, OPT* significantly outperforms baseline approaches, with up to 3 orders of magnitude margins.

#index 875001
#* Ranking objects based on relationships
#@ Kaushik Chakrabarti;Venkatesh Ganti;Jiawei Han;Dong Xin
#t 2006
#c 5
#% 169784
#% 278831
#% 387427
#% 479469
#% 479816
#% 480330
#% 631988
#% 643566
#% 659990
#% 660011
#% 805896
#% 993987
#% 1015317
#% 1016176
#! In many document collections, documents are related to objects such as document authors, products described in the document, or persons referred to in the document. In many applications, the goal is to find these objects that best match a set of keywords. However, the keywords may not necessarily occur in the target objects; they occur only in the documents. For example, in a product review database, a user might search for names of products (say, laptops) using keywords like "lightweight" and "business use" that occur only in the reviews but not in the names of laptops. In order to answer these queries, we need to exploit relationships between documents containing the keywords and the target objects related to those documents. Current keyword query paradigms do not exploit these relationships effectively and hence are inefficient for these queries.In this paper, we consider a class of queries called the "object finder" queries. Our main intuition is to exploit the relationships between searchable documents and related objects and further "aggregate" the document scores from these relationships in order to find the best ranking target objects. Building upon existing keyword search engines such as full text search, we design efficient algorithms that exploit the requirement of only the best k target objects to terminate early. The main challenge here is to push early termination through blocking operators such as group by and aggregation. Our experiments with real datasets and workloads demonstrate the effectiveness of our techniques. Although we present our techniques in the context of keyword search, our techniques apply to other types of ranked searches (e.g., multimedia search) as well.

#index 875002
#* Context-sensitive ranking
#@ Rakesh Agrawal;Ralf Rantzau;Evimaria Terzi
#t 2006
#c 5
#% 152934
#% 249305
#% 268079
#% 272510
#% 287421
#% 300170
#% 330769
#% 333854
#% 348173
#% 453464
#% 465167
#% 654442
#% 654466
#% 654480
#% 660011
#% 731407
#% 745519
#% 800588
#% 801673
#% 810013
#% 844387
#% 993957
#% 1016175
#% 1016176
#% 1016203
#% 1682593
#! Contextual preferences take the form that item i1 is preferred to item i2 in the context of X. For example, a preference might state the choice for Nicole Kidman over Penelope Cruz in drama movies, whereas another preference might choose Penelope Cruz over Nicole Kidman in the context of Spanish dramas. Various sources provide preferences independently and thus preferences may contain cycles and contradictions. We reconcile democratically the preferences accumulated from various sources and use them to create a priori orderings of tuples in an off-line preprocessing step. Only a few representative orders are saved, each corre-sponding to a set of contexts. These orders and associated contexts are used at query time to expeditiously provide ranked answers. We formally define contextual preferences, provide algorithms for creating orders and processing queries, and present experimental results that show their efficacy and practical utility.

#index 875003
#* Ordering the attributes of query results
#@ Gautam Das;Vagelis Hristidis;Nishant Kapoor;S. Sudarshan
#t 2006
#c 5
#% 201893
#% 330769
#% 333854
#% 333941
#% 333951
#% 408396
#% 434617
#% 436116
#% 453464
#% 464224
#% 465754
#% 722929
#% 765464
#% 772030
#% 801673
#! There has been a great deal of interest in the past few years on ranking of results of queries on structured databases, including work on probabilistic information retrieval, rank aggregation, and algorithms for merging of ordered lists. In many applications, for example sales of homes, used cars or electronic goods, data items have a very large number of attributes. When displaying a (ranked) list of items to users, only a few attributes can be shown. Traditionally, these are selected manually. We argue that automatic selection of attributes is required to deal with different requirements of different users. We formulate the problem as an optimization problem of choosing the most "useful" set of attributes, that is, the attributes that are most influential in the ranking of the items. We discuss different variants of our notion of attribute usefulness, and propose a hybrid Split-Pane approach that returns a composite of the top attributes of each variant. We conduct both a performance and a user study illustrating the benefits of our algorithms in terms of efficiency and quality of explanation.

#index 875004
#* High-performance complex event processing over streams
#@ Eugene Wu;Yanlei Diao;Shariq Rizvi
#t 2006
#c 5
#% 37972
#% 151529
#% 271199
#% 333938
#% 397375
#% 459001
#% 464221
#% 480938
#% 481448
#% 482088
#% 527756
#% 569257
#% 631974
#% 646220
#% 731408
#% 763881
#% 798419
#% 810096
#% 824747
#% 1015283
#% 1388513
#% 1688281

#index 875005
#* Quality-aware dstributed data delivery for continuous query services
#@ Bugra Gedik;Ling Liu
#t 2006
#c 5
#% 25998
#% 80185
#% 116082
#% 300179
#% 338354
#% 443298
#% 661478
#% 723297
#% 745534
#% 788216
#% 799147
#% 800517
#% 800583
#% 800584
#% 810008
#% 824761
#% 1015280
#% 1849768
#! We consider the problem of distributed continuous data delivery services in an overlay network of heterogeneous nodes. Each node in the system can be a source for any number of data streams and at the same time be a consumer node that is receiving streams sourced at other nodes. A consumer node may define a filter on a source stream such that only the desired portion of the stream is delivered, minimizing the amount of unnecessary bandwidth consumption. By heterogeneous, we mean that nodes not only may have varying network bandwidths and computing resources but also different interests in terms of the filters and the rates of the data streams they are interested in. Our objective is to construct an efficient stream delivery network in which nodes cooperate in forwarding data streams in the presence of constrained resources. We formalize this distributed stream delivery problem as an optimization one by starting with a simple setup where the network topology is fixed and node bandwidth characteristics are known. The goal of the optimization is to find valid delivery graphs with minimum bandwidth consumption. We extend this problem formulation to QoS-aware stream delivery, in order to handle the bandwidth constrained cases in which unwanted drops and delays are inevitable. We provide a classification of delivery graph construction schemes, and in light of this classification we develop pragmatic quality-aware stream delivery (QASD) algorithms. These algorithms aim at constructing efficient stream delivery graphs in a distributed setting, where global knowledge is not available and network characteristics are not known in advance. We introduce a set of evaluation metrics and provide experimental results to illustrate the effectiveness of our proposed algorithms under these metrics.

#index 875006
#* Design, implementation, and evaluation of the linear road bnchmark on the stream processing core
#@ Navendu Jain;Lisa Amini;Henrique Andrade;Richard King;Yoonho Park;Philippe Selo;Chitra Venkatramani
#t 2006
#c 5
#% 332135
#% 378384
#% 397353
#% 535933
#% 654507
#% 654510
#% 726621
#% 884486
#% 1016169
#! Stream processing applications have recently gained significant attention in the networking and database community. At the core of these applications is a stream processing engine that performs resource allocation and management to support continuous tracking of queries over collections of physically-distributed and rapidly-updating data streams. While numerous stream processing systems exist, there has been little work on understanding the performance characteristics of these applications in a distributed setup. In this paper, we examine the performance bottlenecks of streaming data applications, in particular the Linear Road stream data management benchmark, in achieving good performance in large-scale distributed environments, using the Stream Processing Core (SPC), a stream processing middleware we have developed. First, we present the design and implementation of the Linear Road benchmark on the SPC middleware. SPC has been designed to scale to tens of thousands of processing nodes, while supporting concurrent applications and multiple simultaneous queries. Second, we identify the main performance bottlenecks in the Linear Road application in achieving scalability and low query response latency. Our results show that data locality, buffer capacity, physical allocation of processing elements to infrastructure nodes, and packaging for transporting streamed data are important factors in achieving good application performance. Though we evaluate our system primarily for the Linear Road application, we believe it also provides useful insights into the overall system behavior for supporting other distributed and large-scale continuous streaming data applications. Finally, we examine how SPC can be used and tuned to enable a very efficient implementation of the Linear Road application in a distributed environment.

#index 875007
#* Rewriting nested XML queries using nested views
#@ Nicola Onose;Alin Deutsch;Yannis Papakonstantinou;Emiran Curtmola
#t 2006
#c 5
#% 18614
#% 137867
#% 198465
#% 237181
#% 273924
#% 333989
#% 378393
#% 384978
#% 397374
#% 463919
#% 464727
#% 465051
#% 465053
#% 480317
#% 480443
#% 480822
#% 564264
#% 572311
#% 576108
#% 599549
#% 765432
#% 765447
#% 809237
#% 810050
#% 824661
#! We present and analyze an algorithm for equivalent rewriting of XQuery queries using XQuery views, which is complete for a large class of XQueries featuring nested FLWR blocks, XML construction and join equalities by value and identity. These features pose significant challenges which lead to fundamental extension of prior work on the problems of rewriting conjunctive and tree pattern queries. Our solution exploits the Nested XML Tableaux (NEXT) notation which enables a logical foundation for specifying XQuery semantics. We present a tool which inputs XQuery queries and views and outputs an XQuery rewriting, thus being usable on top of any of the existing XQuery processing engines. Our experimental evaluation shows that the tool scales well for large numbers of views and complex queries.

#index 875008
#* Meta-data indexing for XPath location steps
#@ SungRan Cho;Nick Koudas;Divesh Srivastava
#t 2006
#c 5
#% 102749
#% 252304
#% 309716
#% 378401
#% 397358
#% 427199
#% 433922
#% 462212
#% 465006
#% 465008
#% 480489
#% 480656
#% 570875
#% 644182
#% 654450
#% 659999
#% 763883
#% 772019
#% 993972
#% 1016204
#! XML is the de facto standard for data representation and exchange over the Web. Given the diversity of the information available in XML, it is very useful to annotate XML data with a wide variety of meta-data, such as quality and sensitivity. When querying such XML data, say using XPath, it is important to efficiently identify the data that meet specified constraints on the meta-data. For example, different users may be satisfied with different levels of quality guarantees, or may only have access to different parts of the XML data based on specified security policies. In this paper, we address the problem of efficiently identifying the XML elements along a location step in an XPath query, that satisfy meta-data range constraints, when the meta-data levels are specifically drawn from an ordered domain (e.g., accuracy in [0,1], recency using timestamps, multi-level security, etc.). More specifically, we develop a family of index structures, which we refer to as meta-data indexes, to address this problem. A meta-data index is easily instantiated using a multi-dimensional index structure, such as an R-tree, incorporating novel query and update algorithms. We show that the full meta-data index (FMI), based on associating each XML element with its meta-data level, has a very high update cost for modifying an element's meta-data level. We resolve this problem by designing the inheritance meta-data index (IMI), in which (i) actual meta-data levels are associated only with elements for which this value is explicitly specified, and (ii) inherited meta-data levels and inheritance source nodes are associated with non-leaf nodes of the index structure. We design efficient query (for all XPath axes) and update (of meta-data levels) algorithms for the IMI, and experimentally demonstrate the superiority of the IMI over the FMI using benchmark data sets.

#index 875009
#* Locking-aware structural join operators for XML query processing
#@ Christian Mathis;Theo Härder;Michael Haustein
#t 2006
#c 5
#% 333981
#% 378412
#% 397366
#% 397375
#% 403195
#% 480589
#% 577358
#% 659999
#% 765406
#% 765488
#% 810036
#% 838542
#% 993939
#% 994015
#% 1015277
#! As observed in many publications so far, the matching of twig pattern queries (i.e., queries that contain only the child and the descendant axis) is a core operation in XML database management systems (XDBMSs) for which the structural join and the holistic twig join algorithms were proposed. In a single-user environment, especially the latter algorithm provides a good evaluation strategy. However, when it comes to multi-user access to a single XML document, it may lead to extensive blocking situations: The XDBMS has to ensure data consistency and, therefore, has to prevent concurrent modification operations from changing elements in the input sequences, a holistic twig algorithm accesses while operating. To circumvent this problem, we propose a set of new locking-aware operators for twig pattern query evaluation that rely on stable path labels (SPLIDs) as well as document and element set indexes. Furthermore, by running extensive tests on our own XDBMS, we show that their performance is comparable to existing approaches in a single-user environment, and leads to higher throughput rates in the case of multi-user access.

#index 875010
#* MonetDB/XQuery: a fast XQuery processor powered by a relational engine
#@ Peter Boncz;Torsten Grust;Maurice van Keulen;Stefan Manegold;Jan Rittinger;Jens Teubner
#t 2006
#c 5
#% 210169
#% 333981
#% 378412
#% 397366
#% 397375
#% 480489
#% 551863
#% 654493
#% 659999
#% 742563
#% 765488
#% 800521
#% 803121
#% 810036
#% 810083
#% 824673
#% 824680
#% 824751
#% 994015
#% 1015271
#% 1015298
#% 1015323
#% 1015338
#% 1016148
#% 1016150
#% 1016209
#! Relational XQuery systems try to re-use mature relational data management infrastructures to create fast and scalable XML database technology. This paper describes the main features, key contributions, and lessons learned while implementing such a system. Its architecture consists of (i) a range-based encoding of XML documents into relational tables, (ii) a compilation technique that translates XQuery into a basic relational algebra, (iii) a restricted (order) property-aware peephole relational query optimization strategy, and (iv) a mapping from XML update statements into relational updates. Thus, this system implements all essential XML database functionalities (rather than a single feature) such that we can learn from the full consequences of our architectural decisions. While implementing this system, we had to extend the state-of-the-art with a number of new technical contributions, such as loop-lifted staircase join and efficient relational query evaluation strategies for XQuery theta-joins with existential semantics. These contributions as well as the architectural lessons learned are also deemed valuable for other relational back-end engines. The performance and scalability of the resulting system is evaluated on the XMark benchmark up to data sizes of 11GB. The performance section also provides an extensive benchmark comparison of all major XMark results published previously, which confirm that the goal of purely relational XQuery processing, namely speed and scalability, was met.

#index 875011
#* Refreshing the sky: the compressed skycube with efficient support for frequent updates
#@ Tian Xia;Donghui Zhang
#t 2006
#c 5
#% 2115
#% 287414
#% 288976
#% 289148
#% 333951
#% 465167
#% 480671
#% 481951
#% 654480
#% 799759
#% 800555
#% 810024
#% 824670
#% 824671
#% 824672
#% 864452
#% 993954
#% 1016207
#! The skyline query is important in many applications such as multi-criteria decision making, data mining, and user-preference queries. Given a set of d-dimensional objects, the skyline query finds the objects that are not dominated by others. In practice, different users may be interested in different dimensions of the data, and issue queries on any subset of d dimensions. This paper focuses on supporting concurrent and unpredictable subspace skyline queries in frequently updated databases. Simply to compute and store the skyline objects of every subspace in a skycube will incur expensive update cost. In this paper, we investigate the important issue of updating the skycube in a dynamic environment. To balance the query cost and update cost, we propose a new structure, the compressed skycube, which concisely represents the complete skycube. We thoroughly explore the properties of the compressed skycube and provide an efficient object-aware update scheme. Experimental results show that the compressed skycube is both query and update efficient.

#index 875012
#* Finding k-dominant skylines in high dimensional space
#@ Chee-Yong Chan;H. V. Jagadish;Kian-Lee Tan;Anthony K. H. Tung;Zhenjie Zhang
#t 2006
#c 5
#% 2115
#% 287466
#% 288976
#% 300170
#% 333854
#% 480671
#% 654480
#% 806212
#% 824670
#% 824671
#% 875025
#% 993954
#% 1688273
#! Given a d-dimensional data set, a point p dominates another point q if it is better than or equal to q in all dimensions and better than q in at least one dimension. A point is a skyline point if there does not exists any point that can dominate it. Skyline queries, which return skyline points, are useful in many decision making applications.Unfortunately, as the number of dimensions increases, the chance of one point dominating another point is very low. As such, the number of skyline points become too numerous to offer any interesting insights. To find more important and meaningful skyline points in high dimensional space, we propose a new concept, called k-dominant skyline which relaxes the idea of dominance to k-dominance. A point p is said to k-dominate another point q if there are k ≤ d dimensions in which p is better than or equal to q and is better in at least one of these k dimensions. A point that is not k-dominated by any other points is in the k-dominant skyline.We prove various properties of k-dominant skyline. In particular, because k-dominant skyline points are not transitive, existing skyline algorithms cannot be adapted for k-dominant skyline. We then present several new algorithms for finding k-dominant skyline and its variants. Extensive experiments show that our methods can answer different queries on both synthetic and real data sets efficiently.

#index 875013
#* Efficient reverse k-nearest neighbor search in arbitrary metric spaces
#@ Elke Achtert;Christian Böhm;Peer Kröger;Peter Kunath;Alexey Pryakhin;Matthias Renz
#t 2006
#c 5
#% 86950
#% 300163
#% 427199
#% 465009
#% 479462
#% 481956
#% 717417
#% 730019
#% 1016191
#! The reverse k-nearest neighbor (RkNN) problem, i.e. finding all objects in a data set the k-nearest neighbors of which include a specified query object, is a generalization of the reverse 1-nearest neighbor problem which has received increasing attention recently. Many industrial and scientific applications call for solutions of the RkNN problem in arbitrary metric spaces where the data objects are not Euclidean and only a metric distance function is given for specifying object similarity. Usually, these applications need a solution for the generalized problem where the value of k is not known in advance and may change from query to query. However, existing approaches, except one, are designed for the specific R1NN problem. In addition - to the best of our knowledge - all previously proposed methods, especially the one for generalized RkNN search, are only applicable to Euclidean vector data but not for general metric objects. In this paper, we propose the first approach for efficient RkNN search in arbitrary metric spaces where the value of k is specified at query time. Our approach uses the advantages of existing metric index structures but proposes to use conservative and progressive distance approximations in order to filter out true drops and true hits. In particular, we approximate the k-nearest neighbor distance for each data object by upper and lower bounds using two functions of only two parameters each. Thus, our method does not generate any considerable storage overhead. We show in a broad experimental evaluation on real-world data the scalability and the usability of our novel approach.

#index 875014
#* A non-linear dimensionality-reduction technique for fast similarity search in large databases
#@ Khanh Vu;Kien A. Hua;Hao Cheng;Sheau-Dong Lang
#t 2006
#c 5
#% 86950
#% 172949
#% 248796
#% 248797
#% 248798
#% 252304
#% 272536
#% 273919
#% 286878
#% 310545
#% 316560
#% 333941
#% 443482
#% 460862
#% 464196
#% 464859
#% 465160
#% 479649
#% 480146
#% 481956
#% 534183
#% 769911
#% 818938
#% 1113095
#% 1775143
#! To enable efficient similarity search in large databases, many indexing techniques use a linear transformation scheme to reduce dimensions and allow fast approximation. In this reduction approach the approximation is unbounded, so that the approximation volume extends across the dataspace. This causes over-estimation of retrieval sets and impairs performance.This paper presents a non-linear transformation scheme that extracts two important parameters specifying the data. We prove that these parameters correspond to a bounded volume around the search sphere, irrespective of dimensionality. We use a special workspace-mapping mechanism to derive tight bounds for the parameters and to prove further results, as well as highlighting insights into the problems and our proposed solutions. We formulate a measure that lower-bounds the Euclidean distance, and discuss the implementation of the technique upon a popular index structure. Extensive experiments confirm the superiority of this technique over recent state-of-the-art schemes.

#index 875015
#* Provenance management in curated databases
#@ Peter Buneman;Adriane Chapman;James Cheney
#t 2006
#c 5
#% 462072
#% 480659
#% 504161
#% 570875
#% 577523
#% 742561
#% 790328
#% 803468
#% 825661
#% 1016204
#% 1017302
#! Curated databases in bioinformatics and other disciplines are the result of a great deal of manual annotation, correction and transfer of data from other sources. Provenance information concerning the creation, attribution, or version history of such data is crucial for assessing its integrity and scientific value. General purpose database systems provide little support for tracking provenance, especially when data moves among databases. This paper investigates general-purpose techniques for recording provenance for data that is copied among databases. We describe an approach in which we track the user's actions while browsing source databases and copying data into a curated database, in order to record the user's actions in a convenient, queryable form. We present an implementation of this technique and use it to evaluate the feasibility of database support for provenance management. Our experiments show that although the overhead of a naive approach is fairly high, it can be decreased to an acceptable level using simple optimizations.

#index 875016
#* Efficient query processing on unstructured tetrahedral meshes
#@ Stratos Papadomanolakis;Anastassia Ailamaki;Julio C. Lopez;Tiankai Tu;David R. O'Hallaron;Gerd Heber
#t 2006
#c 5
#% 58369
#% 86950
#% 153260
#% 196867
#% 249339
#% 252304
#% 258598
#% 286237
#% 336567
#% 404719
#% 415957
#% 427199
#% 434499
#% 443397
#% 453548
#% 462059
#% 462617
#% 465015
#% 479473
#% 480093
#% 565447
#% 765430
#% 783993
#% 797231
#% 797236
#% 797260
#% 797419
#% 797466
#! Modern scientific applications such as fluid dynamics and earthquake modeling heavily depend on massive volumes of data produced by computer simulations. Such applications require new data management capabilities in order to scale to terabyte-scale data volumes. The most common way to discretize the application domain is to decompose it into pyramids, forming an unstructured tetrahedral mesh. Modern simulations generate meshes of high resolution and precision, to be queried by a visualization or analysis tool. Tetrahedral meshes are extremely flexible and therefore vital to accurately model complex geometries, but also are difficult to index. To reduce query execution time, applications either use only subsets of the data or rely on different (less flexible) structures, thereby trading accuracy for speed.This paper presents efficient indexing techniques for common spatial (point and range) on tetrahedral meshes. Because the prevailing multidimensional indexing techniques attempt to approximate the tetrahedra using simpler shapes (primarily rectangles) the query performance deteriorates significantly as a function of the mesh's geometric complexity. We develop Directed Local Search (DLS), an efficient indexing algorithm based on mesh topology information that is practically insensitive to the geometric properties of meshes. We show how DLS can be easily and efficiently implemented within modern DBMS without requiring new exotic index structures and complex preprocessing. Finally, we present a new data layout approach for tetrahedral mesh datasets that provides better performance for scientific applications.compared to the traditional space filling curves. In our PostgreSQL implementation DLS reduces the number of disk page accesses by 26% to 4x, and improves the overall query execution time by 25% to 4.

#index 875017
#* Effective keyword search in relational databases
#@ Fang Liu;Clement Yu;Weiyi Meng;Abdur Chowdhury
#t 2006
#c 5
#% 198058
#% 218982
#% 268079
#% 309726
#% 385946
#% 406493
#% 479803
#% 654442
#% 660011
#% 765466
#% 766440
#% 800524
#% 993987
#% 1015258
#! With the amount of available text data in relational databases growing rapidly, the need for ordinary users to search such information is dramatically increasing. Even though the major RDBMSs have provided full-text search capabilities, they still require users to have knowledge of the database schemas and use a structured query language to search information. This search model is complicated for most ordinary users. Inspired by the big success of information retrieval (IR) style keyword search on the web, keyword search in relational databases has recently emerged as a new research topic. The differences between text databases and relational databases result in three new challenges: (1) Answers needed by users are not limited to individual tuples, but results assembled from joining tuples from multiple tables are used to form answers in the form of tuple trees. (2) A single score for each answer (i.e. a tuple tree) is needed to estimate its relevance to a given query. These scores are used to rank the most relevant answers as high as possible. (3) Relational databases have much richer structures than text databases. Existing IR strategies to rank relational outputs are not adequate. In this paper, we propose a novel IR ranking strategy for effective keyword search. We are the first that conducts comprehensive experiments on search effectiveness using a real world database and a set of keyword queries collected by a major search company. Experimental results show that our strategy is significantly better than existing strategies. Our approach can be used both at the application level and be incorporated into a RDBMS to support keyword-based search in relational databases.

#index 875018
#* Flexible and efficient XML search with complex full-text predicates
#@ Sihem Amer-Yahia;Emiran Curtmola;Alin Deutsch
#t 2006
#c 5
#% 194247
#% 215225
#% 275308
#% 309726
#% 333981
#% 387427
#% 458829
#% 465155
#% 642993
#% 654441
#% 654442
#% 765461
#% 810052
#% 818242
#% 824681
#% 1015258
#% 1016135
#% 1721858
#! Recently, there has been extensive research that generated a wealth of new XML full-text query languages, ranging from simple Boolean search to combining sophisticated proximity and order predicates on keywords. While computing least common ancestors of query terms was proposed for efficient evaluation of conjunctive keyword queries by exploiting the document structure, no such solution was developed to evaluate complex full-text queries. We present efficient evaluation algorithms based on a formalization of XML queries in terms of keyword patterns and an algebra which manipulates pattern matches. Our algebra captures most existing languages and their varying semantics and our algorithms combine relational query evaluation techniques with the exploitation of document structure to process queries with complex full-text predicates. We show how scoring can be incorporated into our framework without compromising the algorithms complexity. Our experiments show that considering element nesting dramatically improves the performance of queries with complex full-text predicates.

#index 875019
#* On the database/network interface in large-scale publish/subscribe systems
#@ Badrish Chandramouli;Junyi Xie;Jun Yang
#t 2006
#c 5
#% 271199
#% 297191
#% 300179
#% 302816
#% 333938
#% 338354
#% 340176
#% 348071
#% 397353
#% 443298
#% 556654
#% 598250
#% 612477
#% 631962
#% 661478
#% 723296
#% 736382
#% 793899
#% 800517
#% 806212
#% 1016180
#! The work performed by a publish/subscribe system can conceptually be divided into subscription processing and notification dissemination. Traditionally, research in the database and networking communities has focused on these aspects in isolation. The interface between the database server and the network is often overlooked by previous research. At one extreme, database servers are directly responsible for notifying individual subscribers; at the other extreme, updates are injected directly into the network, and the network is solely responsible for processing subscriptions and forwarding notifications. These extremes are unsuitable for complex and stateful subscription queries. A primary goal of this paper is to explore the design space between the two extremes, and to devise solutions that incorporate both database-side and network-side considerations in order to reduce the communication and server load and maintain system scalability. Our techniques apply to a broad range of stateful query types, and we present solutions for several of them. Our detailed experiments based on real and synthetic workloads with varying characteristics and link-level network simulation show that by exploiting the query semantics and building an appropriate interface between the database and the network, it is possible to achieve orders-of-magnitude savings in network traffic at low server-side processing cost.

#index 875020
#* Relaxed-currency serializability for middle-tier caching and replication
#@ Philip A. Bernstein;Alan Fekete;Hongfei Guo;Raghu Ramakrishnan;Pradeep Tamma
#t 2006
#c 5
#% 4618
#% 9241
#% 27057
#% 44636
#% 102804
#% 139179
#% 167282
#% 172351
#% 257859
#% 286967
#% 333969
#% 340607
#% 443004
#% 458535
#% 458601
#% 463261
#% 765469
#% 781787
#% 824689
#% 824698
#% 993994
#! Many applications, such as e-commerce, routinely use copies of data that are not in sync with the database due to heuristic caching strategies used to enhance performance. We study concurrency control for a transactional model that allows update transactions to read out-of-date copies. Each read operation carries a "freshness constraint" that specifies how fresh a copy must be in order to be read. We offer a definition of correctness for this model and present algorithms to ensure several of the most interesting freshness constraints. We outline a serializability-theoretic correctness proof and present the results of a detailed performance study.

#index 875021
#* Extensible optimization in overlay dissemination trees
#@ Olga Papaemmanouil;Yanif Ahmad;Uğur Çetintemel;John Jannotti;Yenel Yildirim
#t 2006
#c 5
#% 286181
#% 287647
#% 316575
#% 338354
#% 481599
#% 612477
#% 646220
#% 723296
#% 723297
#% 745348
#% 800517
#% 835186
#% 864442
#% 875054
#% 963600
#% 1015276
#% 1016180
#% 1706222
#! We introduce XPORT, a profile-driven distributed data dissemination system that supports an extensible set of data types, profile types, and optimization metrics. XPORT efficiently implements a generic tree-based overlay network, which can be customized per application using a small number of methods that encapsulate application-specific data filtering, profile aggregation, and optimization logic. The clean separation between the "plumbing" and "application" enables the system to uniformly support disparate dissemination-based applications.We first provide an overview of the basic XPORT model and architecture. We then describe in detail an extensible optimization framework, based on a two-level aggregation model, that facilitates easy specification of a wide range of commonly used performance goals. We discuss distributed tree transformation protocols that allow XPORT to iteratively optimize its operation to achieve these goals under changing network and application conditions. Finally, we demonstrate the flexibility and the effectiveness of XPORT using real-world data and experimental results obtained from both prototype-based LAN emulation and deployment on PlanetLab.

#index 875022
#* On-the-fly sharing for streamed aggregation
#@ Sailesh Krishnamurthy;Chung Wu;Michael Franklin
#t 2006
#c 5
#% 36117
#% 136740
#% 210182
#% 248806
#% 300166
#% 300179
#% 397353
#% 464215
#% 480113
#% 654497
#% 803602
#% 805466
#% 810032
#% 810062
#% 810063
#% 878299
#% 993948
#% 993949
#% 1015279
#% 1016157
#% 1016210
#! Data streaming systems are becoming essential for monitoring applications such as financial analysis and network intrusion detection. These systems often have to process many similar but different queries over common data. Since executing each query separately can lead to significant scalability and performance problems, it is vital to share resources by exploiting similarities in the queries. In this paper we present ways to efficiently share streaming aggregate queries with differing periodic windows and arbitrary selection predicates. A major contribution is our sharing technique that does not require any up-front multiple query optimization. This is a significant departure from existing techniques that rely on complex static analyses of fixed query workloads. Our approach is particularly vital in streaming systems where queries can join and leave the system at any point. We present a detailed performance study that evaluates our strategies with an implementation and real data. In these experiments, our approach gives us as much as an order of magnitude performance improvement over the state of the art.

#index 875023
#* Continuous monitoring of top-k queries over sliding windows
#@ Kyriakos Mouratidis;Spiridon Bakiras;Dimitris Papadias
#t 2006
#c 5
#% 300180
#% 333854
#% 378388
#% 397378
#% 399762
#% 442615
#% 465167
#% 479967
#% 654443
#% 659921
#% 733373
#% 736290
#% 763882
#% 765418
#% 765453
#% 766671
#% 777931
#% 800555
#% 800571
#% 800572
#% 806212
#% 810061
#% 849816
#% 994013
#% 1016183
#% 1016196
#! Given a dataset P and a preference function f, a top-k query retrieves the k tuples in P with the highest scores according to f. Even though the problem is well-studied in conventional databases, the existing methods are inapplicable to highly dynamic environments involving numerous long-running queries. This paper studies continuous monitoring of top-k queries over a fixed-size window W of the most recent data. The window size can be expressed either in terms of the number of active tuples or time units. We propose a general methodology for top-k monitoring that restricts processing to the sub-domains of the workspace that influence the result of some query. To cope with high stream rates and provide fast answers in an on-line fashion, the data in W reside in main memory. The valid records are indexed by a grid structure, which also maintains book-keeping information. We present two processing techniques: the first one computes the new answer of a query whenever some of the current top-k points expire; the second one partially pre-computes the future changes in the result, achieving better running time at the expense of slightly higher space requirements. We analyze the performance of both algorithms and evaluate their efficiency through extensive experiments. Finally, we extend the proposed framework to other query types and a different data stream model.

#index 875024
#* Optimal multi-scale patterns in time series streams
#@ Spiros Papadimitriou;Philip Yu
#t 2006
#c 5
#% 172949
#% 313228
#% 399763
#% 460862
#% 480156
#% 629607
#% 729960
#% 729966
#% 745513
#% 765412
#% 765445
#% 777933
#% 800574
#% 812931
#% 824709
#% 838410
#% 993958
#% 1756509
#% 1857116
#! We introduce a method to discover optimal local patterns, which concisely describe the main trends in a time series. Our approach examines the time series at multiple time scales (i.e., window sizes) and efficiently discovers the key patterns in each. We also introduce a criterion to select the best window sizes, which most concisely capture the key oscillatory as well as aperiodic trends. Our key insight lies in learning an optimal orthonormal transform from the data itself, as opposed to using a predetermined basis or approximating function (such as piecewise constant, short-window Fourier or wavelets), which essentially restricts us to a particular family of trends. We go one step further, lifting even that limitation. Furthermore, our method lends itself to fast, incremental estimation in a streaming setting. Experimental evaluation shows that our method can capture meaningful patterns in a variety of settings. Our streaming approach requires order of magnitude less time and space, while still producing concise and informative patterns.

#index 875025
#* DADA: a data cube for dominant relationship analysis
#@ Cuiping Li;Beng Chin Ooi;Anthony K. H. Tung;Shan Wang
#t 2006
#c 5
#% 210182
#% 227866
#% 227868
#% 249305
#% 273916
#% 280456
#% 397388
#% 420053
#% 420082
#% 458833
#% 464215
#% 479450
#% 479962
#% 480671
#% 481951
#% 632083
#% 654480
#% 727906
#% 769898
#% 769936
#% 824671
#% 838430
#% 875012
#% 993954
#% 1688273
#! The concept of dominance has recently attracted much interest in the context of skyline computation. Given an N-dimensional data set S, a point p is said to dominate q if p is better than q in at least one dimension and equal to or better than it in the remaining dimensions. In this paper, we propose extending the concept of dominance for business analysis from a microeconomic perspective. More specifically, we propose a new form of analysis, called Dominant Relationship Analysis (DRA), which aims to provide insight into the dominant relationships between products and potential buyers. By analyzing such relationships, companies can position their products more effectively while remaining profitable.To support DRA, we propose a novel data cube called DADA (Data Cube for Dominant Relationship Analysis), which captures the dominant relationships between products and customers. Three types of queries called Dominant Relationship Queries (DRQs) are consequently proposed for analysis purposes: 1)Linear Optimization Queries (LOQ), 2)Subspace Analysis Queries (SAQ), and 3)Comparative Dominant Queries (CDQ). Algorithms are designed for efficient computation of DADA and answering the DRQs using DADA. Results of our comprehensive experiments show the effectiveness and efficiency of DADA and its associated query processing strategies.

#index 875026
#* Integrating compression and execution in column-oriented database systems
#@ Daniel Abadi;Samuel Madden;Miguel Ferreira
#t 2006
#c 5
#% 146203
#% 179585
#% 193923
#% 227861
#% 322412
#% 333953
#% 461896
#% 464177
#% 464843
#% 479808
#% 479821
#% 480329
#% 480821
#% 481424
#% 504155
#% 571056
#% 726619
#% 824697
#% 864446
#% 1016235
#! Column-oriented database system architectures invite a re-evaluation of how and when data in databases is compressed. Storing data in a column-oriented fashion greatly increases the similarity of adjacent records on disk and thus opportunities for compression. The ability to compress many adjacent tuples at once lowers the per-tuple cost of compression, both in terms of CPU and space overheads.In this paper, we discuss how we extended C-Store (a column-oriented DBMS) with a compression sub-system. We show how compression schemes not traditionally used in row-oriented DBMSs can be applied to column-oriented systems. We then evaluate a set of compression schemes and show that the best scheme depends not only on the properties of the data but also on the nature of the query workload.

#index 875027
#* Automatic physical design tuning: workload as a sequence
#@ Sanjay Agrawal;Eric Chu;Vivek Narasayya
#t 2006
#c 5
#% 174515
#% 346894
#% 397397
#% 462204
#% 482100
#% 632100
#% 745441
#% 765431
#% 810026
#% 1015367
#% 1016220
#% 1016221
#! The area of automatic selection of physical database design to optimize the performance of a relational database system based on a workload of SQL queries and updates has gained prominence in recent years. Major database vendors have released automated physical database design tools with the goal of reducing the total cost of ownership. An important assumption underlying these tools is that the workload is a set of SQL statements. In this paper, we show that being able to treat the workload as a sequence, i.e., exploiting the ordering of statements can significantly broaden the usage of such tools. We present scenarios where exploiting sequence information in the workload is crucial for performance tuning. We also propose techniques for addressing the technical challenges arising from treating the workload as a sequence. We evaluate the effectiveness of our techniques through experiments on Microsoft SQL Server.

#index 875028
#* Data delivery in a service-oriented world: the BEA aquaLogic data services platform
#@ Michael Carey
#t 2006
#c 5
#% 32897
#% 123997
#% 286831
#% 781453
#% 800087
#% 810073
#% 864484
#% 875045
#! "Wow. I fell asleep listening to SOA music, and when I woke up, I couldn't remember where I'd put my data. Now what?" Has this happened to you? With the new push towards service-oriented architectures (SOA) and process orientation, data seems to have been lost in the shuffle. At the end of the day, however, applications are still about data, and SOA applications are no different. In this paper, we present BEA's approach to serving up data to SOA applications. BEA recently introduced a new middleware product called the AquaLogic Data Services Platform (ALDSP). The purpose of ALDSP is to make it easy to design, develop, deploy, and maintain a data services layer in the world of service-oriented architecture. ALDSP provides a new, declarative foundation for building SOA applications and services that need to access and compose information from a range of enterprise data sources. The paper covers both the foundation and the key features of ALDSP, including its underlying technologies, its overall system architecture, and its most interesting capabilities.

#index 875029
#* LINQ: reconciling object, relations and XML in the .NET framework
#@ Erik Meijer;Brian Beckman;Gavin Bierman
#t 2006
#c 5
#! Many software applications today need to handle data from different data models; typically objects from the host programming language along with the relational and XML data models. The ROX impedance mismatch makes programs awkward to write and hard to maintain.The .NET Language-Integrated Query (LINQ) framework, proposed for the next release of the .NET framework, approaches this problem by defining a pattern of general-purpose standard query operators for traversal, filter, and projection. Based on this pattern, any .NET language can define special query comprehension syntax that is subsequently compiled into these standard operators (our code examples are in VB).Besides the general query operators, the LINQ framework also defines two domain specific APIs that work over XML (XLinq) and relational data (DLinq) respectively. The operators over XML use a lightweight and easy to use in-memory XML representation to provide XQuery-style expressiveness in the host programming language. The operators over relational data provide a simple OR mapping by leveraging remotable queries that are executed directly in the back-end relational store.

#index 875030
#* PHP: supporting the new paradigm of situational and composite web applications
#@ Andi Gutmans
#t 2006
#c 5
#! In this paper, I describe what we see as a paradigm shift in software development and how PHP plays into this change.

#index 875031
#* Documentum ECI self-repairing wrappers: performance analysis
#@ Boris Chidlovskii;Bruno Roustant;Marc Brette
#t 2006
#c 5
#% 261741
#% 271065
#% 283053
#% 312860
#% 397605
#% 443654
#% 479807
#% 480648
#% 480824
#% 551870
#% 632051
#% 637713
#% 772300
#% 801668
#% 805847
#% 840878
#% 926881
#% 1271981
#! Documentum Enterprise Content Integration (ECI) services is a content integration middleware that provides one-query access to the Intranet and Internet content resources. The ECI Adapter technology offers an interface to any application for data and metadata extraction from unstructured Web pages. It offers a unique frame-work of wrapper production, automatic recovery and maintenance, developed at Xerox Research Centre Europe and based on state-of-art algorithms from machine learning and grammatical inference. In this presentation we analyze the performance of ECI adapters deployed in current commercial installations. We benefit from accessing reports on daily tests for all ECI commercially deployed adapters collected from June 2003 to September 2005. Using the daily reports, we analyze different aspects of the wrapper technology.

#index 875032
#* Identity resolution: 23 years of practical experience and observations at scale
#@ Jeff Jonas
#t 2006
#c 5
#! Identity Resolution is a semantic reconciliation activity as applied to people and organizations. Identity resolution is most frequently quantified in terms of accuracy (false positives and false negatives), however, there are additional metrics by which to evaluate identity resolution algorithms including: methodology, persistence, streaming versus batch, data survivorship, operationalizing historical data, transaction/window size, ingestion speed, end-to-end latency, sequence neutrality, handling of ambiguous conditions, reconcilability, scalability, sustainability, and operational characteristics at scale. As well, a technique for "analytics in the anonymized data space" will be presented that makes it possible to resolve identities in a more privacy-preserving manner.

#index 875033
#* Using SPIDER: an experience report
#@ Nick Koudas;Amit Marathe;Divesh Srivastava
#t 2006
#c 5
#% 577309
#% 824787
#% 1016219
#! At AT&T Labs-Research, we have been developing a prototype system called SPIDER to efficiently support flexible string matching of attribute values in large databases. SPIDER has been used in AT&T, both as a key component of an operational portal for matching customer names and addresses, and for a variety of ad hoc data quality analyses. In this talk, we report on experiences with SPIDER.

#index 875034
#* Trends in high performance analytics
#@ Yossi Matias
#t 2006
#c 5
#! With the proliferation of analytic and business intelligence applications, and with the persistent growth in data sizes, there is an ever increasing need to support high performance analytics. This talk will present recent technological trends in addressing this need, and will particularly highlight the approach of facilitating high performance analytics in a relational database via a novel dichotomous combination with a non-relational aggregation-server.

#index 875035
#* VizQL: a language for query, analysis and visualization
#@ Pat Hanrahan
#t 2006
#c 5
#% 434617
#! Conventional query languages such as SQL and MDX have limited formatting and visualization capabilities. Thus, although powerful queries can be composed, another layer of software is needed to report or present the results in a useful form to the analyst. VizQL™ is designed to fill that gap. VizQL evolved from the Polaris system at Stanford, which combined query, analysis and visualization into a single framework [1].VizQL is a formal language for describing tables, charts, graphs, maps, time series and tables of visualizations. These different types of visual representations are unified into one framework, making it easy to switch from one visual representation to another (e.g. from a list view to a cross-tab to a chart). Unlike current charting packages and like query languages, VizQL permits an unlimited number of picture expressions. Visualizations can thus be easily customized and controlled. VizQL is a declarative language. The desired picture is described; the low-level operations needed to retrieve the results, to perform analytical calculations, to map the results to a visual representation, and to render the image are generated automatically by the query analyzer. The query analyzer compiles VizQL expressions to SQL and MDX and thus VizQL can be used with relational databases and datacubes. The current implementation supports Hyperion Essbase, Microsoft SQL Server, Microsoft Analysis Services, MySQL, Oracle, as well as desktop data sources such as CSV and Excel files. This analysis phase includes many optimizations that allow large databases to be browsed interactively. VizQL enables a new generation of visual analysis tools that closely couple query, analysis and visualization.

#index 875036
#* Using the oracle database as a declarative RSS hub
#@ Dieter Gawlick;Muralidhar Krishnaprasad;Zhen Hua Liu
#t 2006
#c 5
#! The interaction with the Web has historically evolved from static bookmarks to dynamic searches to the current usage of active notification mechanisms based on popular protocols like RSS or Atom. In the same time a large volume of important source data is still contained in relational databases. The talk will analyze the way the Oracle database participates to the activation of the data and opening the state changes in a standard and secure way for easy integrating with the rest of the push based Web protocols. We will study the declarative specification of RSS feeds generated based on the state changes detected in the data stored in the Oracle database. On the opposite, external RSS feeds can be injected to the database and processed declaratively in conjunction with the rest of the data. Most of the technical pieces required for such a solution are already supported by the database engine (e.g. declarative XML processing, state change notifications, queues, crawlers, continuous queries), effectively turning the database into a declarative XML hub. The advantages of using database solutions for such problems in an enterprise context are security, scalability and reliability.

#index 875037
#* Windows and RSS: beyond blogging
#@ Sean Lyndersay
#t 2006
#c 5
#! RSS (and related technologies like Atom) are gaining significant traction as a means for allowing users to "subscribe" to content on the web and get notified when new content is available. More recently, "podcasting" -- a simple extension to RSS to enable references to audio files -- has taken off as a means to subscribe to episodal audio content. More generally, RSS feeds are being used in many arenas to communicate all sorts of different types of content, either using extensions to the RSS format, or simply by transmitting binary files.At its heart, RSS is a very simple XML-based format with very simple semantics, but the potential uses appear endless. This talk will examine many of the uses of RSS, and discuss why this simple format has become so important that Microsoft is building native support for RSS into its next generation operating system and browser platforms.It will also cover many of the technical challenges inherent in building scalable support for RSS into a client operating system.

#index 875038
#* Fast approximate computation of statistics on views
#@ Calisto Zuzarte;Xiaohui Yu
#t 2006
#c 5
#% 397371
#% 765427
#% 765471
#% 1015334
#! Accurate estimation of the sizes of intermediate query results (cardinality estimation) is of critical importance to plan costing in query optimization. The common practice in current commercial database systems such as IBM DB2 Universal Database (DB2 UDB) is to derive the cardinality estimates from base-table statistics. However, this approach often suffers from simplifying yet unrealistic assumptions that have to be made about the underlying data (for example, different attributes are independently distributed).Ways for exploiting statistics on query expressions (or, statistics on views, or SITs) have been proposed to improve the accuracy of cardinality estimation. We propose a novel method for efficient computation of SITs for joins. In particular, we are concerned with statistics on join queries involving large fact tables and relatively small dimension tables. Rather than materializing the views, we make use of the frequency statistics that are available on the fact tables to obtain an approximate estimate of the statistics on various attributes in the join results. The dimension tables are generally much smaller than the fact table, and therefore we can afford to closely examine the dimension table, while at the same time avoid accessing the fact table. By closely examining the dimension table, we are able to capture the correlations between the attributes in the dimension table as well as the skew and domain range of the fact table join column values. This leads to reasonably accurate statistics on the join result. We prototyped this idea as a module on top of DB2 UDB, and our experience shows that employment of this technique results in a very significant speed-up in the computation of SITs, at the expense of only slight degradation in accuracy compared with the full-materialization method.

#index 875039
#* Data management projects at Google
#@ Wilson Hsieh;Jayant Madhavan;Rob Pike
#t 2006
#c 5
#! This session describes three data management projects at Google. BigTable is a highly scalable system for distributed storage and querying of structured data. Sawzall is a system for large-scale analysis of data sets that have a flat but regular structure. Finally, GoogleBase is a system for storing and searching structured data contributed by external parties.

#index 875040
#* PADS: an end-to-end system for processing ad hoc data
#@ Mark Daly;Yitzhak Mandelbaum;David Walker;Mary Fernández;Kathleen Fisher;Robert Gruber;Xuan Zheng
#t 2006
#c 5
#% 347226
#% 397414
#% 492932
#% 809123
#% 848307
#% 993969
#! Enormous amounts of data exist in "well-behaved" formats such as relational tables and XML, which come equipped with extensive tool support. However, vast amounts of data also exist in non-standard or ad hoc data formats, which often lack standard or extensible tools. This deficiency forces data analysts to implement their own tools for parsing, querying, and analyzing their ad hoc data. The resulting tools typically interleave parsing, querying, and analysis, obscuring the semantics of the data format and making it nearly impossible for others to resuse the tools. This proposal describes PADS, an end-to-end system for processing ad hoc data sources. The core of PADS is a declarative language for describing ad hoc data sources and a data-description compiler that produces customizable libraries for parsing the ad hoc data. A suite of tools built around this core includes statistical data-profiling tools, a query engine that permits viewing ad hoc sources as XML and for querying them with XQuery, and an interactive front-end that helps users produce PADS descriptions quickly.

#index 875041
#* Data management in the CarTel mobile sensor computing system
#@ V. Bychkovsky;K. Chen;M. Goraczko;H. Hu;B. Hull;A. Miu;E. Shih;Y. Zhang;H. Balakrishnan;S. Madden
#t 2006
#c 5
#! We propose a reusable data management system, called CarTel, for querying and collecting data from intermittently connected devices. CarTel provides a simple, incrementally-deployable platform for developing automobile-based sensor applications. Our platform provides a dynamic query system that allows both continuous (standing) and one-shot geo-spatial queries over car position, speed, and sensory data as well as a both a low-cost/high-bandwidth substrate for communicating with a large network of mobile devices.

#index 875042
#* VGM: visual graph mining
#@ Karsten Borgwardt;Sebastian Böttger;Hans-Peter Kriegel
#t 2006
#c 5
#% 327432
#% 629708
#! As more and more graph data become available in various application domains, graph mining is of ever increasing importance in data management.Graph kernels are a novel and successful method for data mining in graphs. Unfortunately, implementing graph kernels is not trivial, and few applied researchers have therefore used graph kernels so far. In this demonstration, we present a Java software package called Visual Graph Mining (VGM). VGM allows the user to classify graphs using graph kernels and Support Vector Machines in a graphical user interface that is easy to learn and use. It is linked to LIBSVM for Support Vector Machine computations, yet can be easily transferred to other Support Vector Machine packages. Furthermore, VGM provides basic data mining features such as Nearest Neighbor search, graph algorithms such as Dijkstra, Floyd-Warshall, and computes and visualizes product graphs and topological indices of graphs.VGM 's homepage can be found at: http://www.cip.ifi.lmu.de/~boettger/sigmod.

#index 875043
#* Data integration through transform reuse in the Morpheus project
#@ Tiffany Dohzen;Mujde Pamuk;Seok-Won Seong;Joachim Hammer;Michael Stonebraker
#t 2006
#c 5
#% 644923
#% 765433
#% 800497
#% 1655384
#! We discuss Morpheus, a data transformation construction tool and associated repository. The architecture of Morpheus is motivated by the goal to reuse (pieces of) previously written transformations to solve data integration problems by finding relevant ones in the repository and then modifying them for repurposing. In addition, Morpheus is integrated with a DBMS so as to leverage existing capabilities including the runtime environment for transforms. We discuss the architecture of Morpheus and illustrate its usage with the help of a simple transform construction scenario.

#index 875044
#* Testing database applications
#@ Carsten Binnig;Donald Kossmann;Eric Lo
#t 2006
#c 5
#% 115661
#% 172913
#% 435112
#% 819354
#% 824700
#% 824744
#! Testing database application is challenging because most methods and tools developed for application testing do not consider the database state during the test. In this paper we demonstrate three different tools for testing database applications: HTDGen, HTTrace and HTPar. HTDGen generates meaningful test databases for database applications. HTTrace executes database applications testing efficiently and HTPar extends HTTrace to run tests in parallel.

#index 875045
#* The BEA AquaLogic data services platform (Demo)
#@ Vinayak Borkar;Michael Carey;Dmitry Lychagin;Till Westmann
#t 2006
#c 5
#% 875028
#! We showcase the BEA AquaLogic Data Services Platform (ALDSP), a middleware infrastructure product that enables the declarative development of data services for service-oriented architectures (SOA). ALDSP includes support for modeling networks of interrelated data services, for realizing data services using either graphical or source-based XQuery editors, for testing data services as they are developed, and for identifying and incorporating changes in the structure of the underlying sources of data. Physical data sources supported include relational tables and views, Web services, packaged applications, stored procedures, XML files, delimited files, and custom Java applications. Data service definitions can be layered; as with relational views, such layering is virtual, and is rewritten away at query compilation time. ALDSP supports both read and update data service functions, and the ALDSP XML query runtime includes a number of interesting query operators and distributed query optimizations. In addition, ALDSP supports function caching, fine-grained security, and SQL-based data access as well as providing service-based and XQuery access to SOA data. We plan to demonstrate as much of this as time permits.

#index 875046
#* VisTrails: visualization meets data management
#@ Steven P. Callahan;Juliana Freire;Emanuele Santos;Carlos E. Scheidegger;Cláudio T. Silva;Huy T. Vo
#t 2006
#c 5
#% 202409
#% 641100
#! Scientists are now faced with an incredible volume of data to analyze. To successfully analyze and validate various hypothesis, it is necessary to pose several queries, correlate disparate data, and create insightful visualizations of both the simulated processes and observed phenomena. Often, insight comes from comparing the results of multiple visualizations. Unfortunately, today this process is far from interactive and contains many error-prone and time-consuming tasks. As a result, the generation and maintenance of visualizations is a major bottleneck in the scientific process, hindering both the ability to mine scientific data and the actual use of the data. The VisTrails system represents our initial attempt to improve the scientific discovery process and reduce the time to insight. In VisTrails, we address the problem of visualization from a data management perspective: VisTrails manages the data and metadata of a visualization product. In this demonstration, we show the power and flexibility of our system by presenting actual scenarios in which scientific visualization is used and showing how our system improves usability, enables reproducibility, and greatly reduces the time required to create scientific visualizations.

#index 875047
#* OMCAT: optimal maintenance of continuous queries' answers for trajectories
#@ Hui Ding;Goce Trajcevski;Peter Scheuermann
#t 2006
#c 5
#% 458865
#% 461923
#% 749907
#% 765453
#% 1015297
#! We present our prototype system, OMCAT, which optimizes the reevaluation of a set of pending continuous spatio-temporal queries on trajectory data, when some of the trajectories are affected by traffic abnormalities reported. The key observation that motivates OMCAT is that an abnormality in a given geographical region may cause changes to the answers of queries pertaining to future portions of affected trajectories. We investigate the sources of context-switching costs at various levels and propose solutions that utilize the correlation of several context dimensions to orchestrate the reevaluation of the queries. OMCAT, fully implemented on top of an existing Object Relational Database Management System - Oracle 9i, demonstrates that our techniques can substantially reduce the response time during query answer update.

#index 875048
#* Generic similarity detection in ontologies with the SOQA-SimPack toolkit
#@ Patrick Ziegler;Christoph Kiefer;Christoph Sturm;Klaus R. Dittrich;Abraham Bernstein
#t 2006
#c 5
#% 198058
#% 230386
#% 387427
#% 1688250
#! Ontologies are increasingly used to represent the intended real-world semantics of data and services in information systems. Unfortunately, different data sources often do not relate to the same ontologies when describing their semantics. Consequently, it is desirable to have information about the similarity between ontology concepts for ontology alignment and integration. In this demo, we present the SOQA-SimPack Toolkit (SST), an ontology language independent Java API that enables generic similarity detection and visualization in ontologies. We demonstrate SST's usefulness with the SOQA-SimPack Toolkit Browser that allows users to graphically perform similarity calculations in ontologies.

#index 875049
#* Searching in time
#@ Christian Plattner;Andreas Wapf;Gustavo Alonso
#t 2006
#c 5
#% 793894
#% 810114
#! This demonstration shows how to use external databases to provide an efficient implementation of a timetravel service. The timetravel semantics are defined using snapshot isolation. The system presented not only allows to retrieve older snapshots but also to identify snapshots of interest.

#index 875050
#* Derby/S: a DBMS for sample-based query answering
#@ Anja Klein;Rainer Gemulla;Philipp Rösch;Wolfgang Lehner
#t 2006
#c 5
#% 300195
#% 465162
#% 479804
#% 503719
#% 654486
#% 1015328
#% 1688270
#! Although approximate query processing is a prominent way to cope with the requirements of data analysis applications, current database systems do not provide integrated and comprehensive support for these techniques. To improve this situation, we propose an SQL extension---called SQL/S---for approximate query answering using random samples, and present a prototypical implementation within the engine of the open-source database system Derby---called Derby/S. Our approach significantly reduces the required expert knowledge by enabling the definition of samples in a declarative way; the choice of the specific sampling scheme and its parametrization is left to the system. SQL/S introduces new DDL commands to easily define and administrate random samples subject to a given set of optimization criteria. Derby/S automatically takes care of sample maintenance if the underlying dataset changes. Finally, samples are transparently used during query processing, and error bounds are provided. Our extensions do not affect traditional queries and provide the means to integrate sampling as a first-class citizen into a DBMS.

#index 875051
#* Automatic client-server partitioning of data-driven web applications
#@ Nicholas Gerner;Fan Yang;Alan Demers;Johannes Gehrke;Mirek Riedewald;Jayavel Shanmugasundaram
#t 2006
#c 5
#% 795078
#% 829169
#% 864419
#! Current application development tools provide completely different programming models for the application server (e.g., Java and J2EE) and the client web browser (e.g., JavaScript and HTML). Consequently, the application developer is forced to partition the application code between the server and client at the time of writing the application. However, the partitioning of the code between the client and server may have to be changed during the evolution of the application for performance reasons (it may be better to push more functionality to the client), for correctness reasons (data that conflicts with multiple clients cannot always be pushed to clients), and for supporting clients with different computing power (browsers on desktops vs. PDAs). Since the client and server use different programming models, moving application code from client to server (and vice versa) reduces programmer productivity and also has the potential to introduce concurrency bugs. In this demonstration, we advocate an alternative solution to this problem: we propose developing applications using a unified declarative high-level language called Hilda, and show how a Hilda compiler can automatically (and correctly) partition Hilda code between the client and the server using a real Course Management System application. We illustrate our techniques using two clients: a powerful laptop machine and a less powerful PDA.

#index 875052
#* Paper-based mobile access to databases
#@ Beat Signer;Moira C. Norrie;Michael Grossniklaus;Rudi Belotti;Corsin Decurtins;Nadir Weibel
#t 2006
#c 5
#% 955959
#% 1599309
#! Our demonstration is a paper-based interactive guide for visitors to the world's largest international arts festival that was developed as part of a project investigating new forms of context-aware information delivery and interaction in mobile environments. Information stored in a database is accessed from a set of interactive paper documents, including a printed festival brochure, a city map and a bookmark. Active areas are defined within the documents and selection of these using a special digital pen causes the corresponding query request along with context data to be sent to a festival application database and the response is returned to the visitor in the form of generated speech output. In addition to paper-based information browsing and transactions such as ticket booking, the digital pen can also be applied for data capture of event ratings and handwritten comments on events. The system integrates three main database components - a cross-media information platform, a content management framework for multi-channel context-aware publishing of data and the festival application database.

#index 875053
#* Proactive identification of performance problems
#@ Songyun Duan;Shivnath Babu
#t 2006
#c 5
#% 297171
#% 963675
#! We propose to demonstrate Fa, an automated tool for timely and accurate prediction of Service-Level-Agreement (SLA) violations caused by performance problems in database systems. Fa periodically collects performance data at three levels: applications, database server, and operating system. This data is used to construct probabilistic models for predicting SLA violations. Fa currently uses graphical Bayesian network models because of their ability to support a wide range of inferences, including prediction and diagnosis, as well as their support for interactive visualization and presentation of complex system behavior in intuitive ways.

#index 875054
#* XPORT: extensible profile-driven overlay routing trees
#@ Olga Papaemmanouil;Yanif Ahmad;Uğur Çetintemel;John Jannotti;Yenel Yildirim
#t 2006
#c 5
#% 338354
#% 875021
#% 1016180
#! XPORT is a profile-driven distributed data collection and dissemination system that supports an extensible set of data types, profiles, and optimization metrics. XPORT efficiently builds a generic tree-based overlay network, which can be customized per application using a small number of methods that encapsulate application-specific data-profile matching, aggregation, and optimization logic. The clean separation between the "plumbing" and "application" enables XPORT to uniformly and easily support disparate dissemination-based applications such as content-based feed dissemination and application-level multicast. We propose to demonstrate the basic XPORT system, featuring its extensible optimization framework that facilitates easy specification of a wide range of useful performance goals and a continuous, adaptive optimization model to achieve these goals under changing network and application conditions. We will use two different underlying applications, an RSS feed dissemination application and a multiplayer network game, along with visual system-monitoring tools to illustrate the extensibility and the operational aspects of XPORT.

#index 875055
#* A system for specification and verification of interactive, data-driven web applications
#@ Alin Deutsch;Liying Sui;Victor Vianu;Dayou Zhou
#t 2006
#c 5
#% 425200
#% 801675
#% 810053
#% 1599270
#! When comparing alternative query execution plans (QEPs), a cost-based query optimizer in a relational database management system needs to estimate the selectivity of conjunctive predicates. To avoid inaccurate independence assumptions, modern optimizers try to exploit multivariate statistics (MVS) that provide knowledge about joint frequencies in a table of a relation. Because the complete joint distribution is almost always too large to store, optimizers are given only partial knowledge about this distribution. As a result, there exist multiple, non-equivalent ways to estimate the selectivity of a conjunctive predicate. To consistently combine the partial knowledge during the estimation process, existing optimizers employ cumbersome ad hoc heuristics. These methods unjustifiably ignore valuable information, and the optimizer tends to favor QEPs for which the least information is available. This bias problem yields poor QEP quality and performance. We demonstrate MAXENT, a novel approach based on the maximum entropy principle, prototyped in IBM DB2 LUW. We illustrate MAXENT's ability to consistently estimate the selectivity of conjunctive predicates on a per-table basis. In contrast to the DB2 optimizer's current ad hoc methods, we show how MAXENT exploits all available information about the joint column distribution and thus avoids the bias problem. For some complex queries against a real-world database, we show that MAXENT improves selectivity estimates by orders of magnitude relative to the current DB2 optimizer, and also show how these improved estimate influence plan choices as well as query execution times.

#index 875056
#* MAXENT: consistent cardinality estimation in action
#@ V. Markl;M. Kutsch;T. M. Tran;P. J. Haas;N. Megiddo
#t 2006
#c 5
#% 824682
#! When comparing alternative query execution plans (QEPs), a cost-based query optimizer in a relational database management system needs to estimate the selectivity of conjunctive predicates. To avoid inaccurate independence assumptions, modern optimizers try to exploit multivariate statistics (MVS) that provide knowledge about joint frequencies in a table of a relation. Because the complete joint distribution is almost always too large to store, optimizers are given only partial knowledge about this distribution. As a result, there exist multiple, non-equivalent ways to estimate the selectivity of a conjunctive predicate. To consistently combine the partial knowledge during the estimation process, existing optimizers employ cumbersome ad hoc heuristics. These methods unjustifiably ignore valuable information, and the optimizer tends to favor QEPs for which the least information is available. This bias problem yields poor QEP quality and performance. We demonstrate MAXENT, a novel approach based on the maximum entropy principle, prototyped in IBM DB2 LUW. We illustrate MAXENT's ability to consistently estimate the selectivity of conjunctive predicates on a per-table basis. In contrast to the DB2 optimizer's current ad hoc methods, we show how MAXENT exploits all available information about the joint column distribution and thus avoids the bias problem. For some complex queries against a real-world database, we show that MAXENT improves selectivity estimates by orders of magnitude relative to the current DB2 optimizer, and also show how these improved estimate influence plan choices as well as query execution times.

#index 875057
#* InMAF: indexing music databases via multiple acoustic features
#@ Jialie Shen;John Shepherd;Anne Ngu
#t 2006
#c 5
#% 376266
#% 631963
#% 643010
#% 1775543
#! Music information processing has become very important due to the ever-growing amount of music data from emerging applications. In this demonstration,we present a novel approach for generating small but comprehensive music descriptors to facilitate efficient content music management (accessing and retrieval, in particular). Unlike previous approaches that rely on low-level spectral features adapted from speech analysis technology, our approach integrates human music perception to enhance the accuracy of the retrieval and classification process via PCA and neural networks. The superiority of our method is demonstrated by comparing it with state-of-the-art approaches in the areas of music classification query effectiveness, and robustness against various audio distortion/alternatives.

#index 875058
#* Quark: an efficient XQuery full-text implementation
#@ Anand Bhaskar;Chavdar Botev;Muthiah M. Muthaia Chettiar;Lin Guo;Jayavel Shanmugasundaram;Feng Shao;Fan Yang
#t 2006
#c 5
#% 215225
#% 309851
#% 340914
#% 397366
#% 458829
#% 480657
#% 754092
#! The XQuery 1.0 and XPath 2.0 Full-text (XQFT) language has been developed by the W3C to extend XQuery and XPath with full-text search capabilities. XQFT allows users to specify a mix of structured and complex full-text predicates, and also allows users to score/rank such queries. The power and flexibility of XQFT gives rise to two interesting questions. First, is it possible to efficiently integrate a full-function XML query language with sophisticated full-text search? Second, is it possible to score and rank arbitrary XQuery and XQFT queries? In this demonstration, we present evidence that it is indeed possible to achieve the above goals. We demonstrate the Quark open-source data management system and show how we can seamlessly and efficiently integrate structured and unstructured search over XML data. In particular, we demonstrate (a) techniques for efficiently evaluating keyword search over virtual XML views, and (b) a framework for scoring both structured and full-text predicates.

#index 875059
#* aAqua: a database-backended multilingual, multimedia community forum
#@ Krithi Ramamritham;Anil Bahuman;Subhasri Duttagupta
#t 2006
#c 5
#! aAQUA is an online multilingual, multimedia Agricultural portal for disseminating information from and to rural communities. It answers farmers' queries based on the location, season, crop and other information provided by farmers. aAQUA makes use of novel database systems and information retrieval techniques like intelligent caching, offline access with intermittent synchronization, semantic-based search, etc. aAQUA's large scale deployment provides avenues for researchers to contribute in the areas of knowledge management, cross-lingual information retrieval, and providing accessible content for rural populations. Apart from agriculture, aAQUA can be configured and customized for expert advice in education, healthcare and other domains of interest to a developing population. This demonstration showcases the utility of various component DB/IR technologies built into aAQUA to enhance the QoS delivered to rural populations.

#index 875060
#* CS cache engine: data access accelerator for location-based service in mobile environments
#@ Ken C. K. Lee;Wang-Chien Lee;Julian Winter;Baihua Zheng;Jianliang Xu
#t 2006
#c 5
#% 86950
#% 309461
#% 481916
#% 1688302
#! Location-based services (LBS) have emerged as one of the killer applications for mobile and pervasive computing environments. Due to limited bandwidth and scarce client resources, client-side data caching plays an important role of enhancing the data availability and improving the response time. In this demonstration, we present CS Cache Engine suitable for LBS. The underlying caching model is Complementary Space Caching (CS caching) scheme that we have recently presented in [citation]. Different from conventional data caching schemes, CS caching preserves a global view of the database by maintaining physical objects and capturing those objects in the server but not in the cache as Complementary Regions (CRs) in the cache. As a result, with the CS Cache Engine implementing CS caching, client assertiveness on their own answered queries is enhanced so that unnecessary requests over the wireless channel can be avoided; various kinds of location-based queries are naturally supported; and the client's ability to prefetch objects is introduced such that the response time can be further improved. In this demonstration paper, we discuss the architecture and the functionality of the CS Caching Engine that adopts CS caching. Specifically, for this demonstration, a tourist information named TravelGuide is prototyped with the support of this cache engine.

#index 875061
#* Avatar semantic search: a database approach to information retrieval
#@ Eser Kandogan;Rajasekar Krishnamurthy;Sriram Raghavan;Shivakumar Vaithyanathan;Huaiyu Zhu
#t 2006
#c 5
#% 294600
#% 340914
#% 458829
#% 642993
#% 722926
#% 782759
#% 824695
#% 1015258
#! We present Avatar Semantic Search, a prototype search engine that exploits annotations in the context of classical keyword search. The process of annotations is accomplished offline by using high-precision information extraction techniques to extract facts, con-cepts, and relationships from text. These facts and concepts are represented and indexed in a structured data store. At runtime, keyword queries are interpreted in the context of these extracted facts and converted into one or more precise queries over the structured store. In this demonstration we describe the overall architecture of the Avatar Semantic Search engine. We also demonstrate the superiority of the AVATAR approach over traditional keyword search engines using Enron email data set and a blog corpus.

#index 875062
#* COLT: continuous on-line tuning
#@ Karl Schnaitter;Serge Abiteboul;Tova Milo;Neoklis Polyzotis
#t 2006
#c 5
#% 248815
#% 346894
#% 462204
#% 479476
#% 480158
#% 482100
#% 775880
#% 810026
#% 1016220
#! The physical schema of a database plays a critical role in performance. Self-tuning is a cost-effective and elegant solution to optimize the physical configuration for the characteristics of the query load. Existing techniques operate in an off-line fashion, by choosing a fixed configuration that is tailored to a subset of the query load. The generated configurations therefore ignore any temporal patterns that may exist in the actual load submitted to the system.This demonstration introduces COLT (Continuous On-Line Tuning), a novel self-tuning framework that continuously monitors the incoming queries and adjusts the system configuration in order to maximize query performance. The key idea behind COLT is to gather performance statistics at different levels of detail and to carefully allocate profiling resources to the most promising candidate configurations. Moreover, COLT uses effective heuristics to regulate its own performance, lowering its overhead when the system is well-tuned, and being more aggressive when the workload shifts and it becomes necessary to re-tune the system. We present a specialization of COLT to the important problem of selecting an effective set of relational indices for the current query load. Our demonstration will use an implementation of our proposed framework in the PostgreSQL database system, showing the internal operation of COLT and the adaptive selection of indices as we vary the query load of the server.

#index 875063
#* ObjectRank: a system for authority-based search on databases
#@ Heasoo Hwang;Vagelis Hristidis;Yannis Papakonstantinou
#t 2006
#c 5
#% 268079
#% 393844
#! We present ObjectRank demo system that performs authority-based keyword search on bibliographic databases. We also provide Inverse ObjectRank as a keyword-specific specificity metric and other calibration parameters such as Global ObjectRank. Users can specify various combinations of calibration values to control the behavior of the demo. Finally, we propose a methodology that enables us to extend query results using the ontology graph.

#index 875064
#* Managing information extraction: state of the art and research directions
#@ AnHai Doan;Raghu Ramakrishnan;Shivakumar Vaithyanathan
#t 2006
#c 5
#! This tutorial makes the case for developing a unified framework that manages information extraction from unstructured data (focusing in particular on text). We first survey research on information extraction in the database, AI, NLP, IR, and Web communities in recent years. Then we discuss why this is the right time for the database community to actively participate and address the problem of managing information extraction (including in particular the challenges of maintaining and querying the extracted information, and accounting for the imprecision and uncertainty inherent in the extraction process). Finally, we show how interested researchers can take the next step, by pointing to open problems, available datasets, applicable standards, and software tools. We do not assume prior knowledge of text management, NLP, extraction techniques, or machine learning.

#index 875065
#* Programming for XML
#@ Daniela Florescu;Donald Kossmann
#t 2006
#c 5
#! There are many emerging applications for XML. Although there are many tools availalbe, an open question is the right programming paradigm to process XML data. Today, the most popular solutions are based on extensions to existing programming languages (e.g., Java, Python or PHP) with XML-specific libraries and APIs. Such libraries either represent the XML data as a virtual tree, or they read the XML data in a streaming (push or pull) fashion. This approach has the obvious problems that arise from the impedance mismatch between the XML type system and the type system of the host language. Moreover, the code written in such programming languages cannot be (easily) optimized using traditional techniques; good performance, scalability, and service-level guarantees is difficult to achieve for such programs on large datasets. Recently, several proposals for new programming languages have been made in both industry and the research community. One prominent example is Microsoft's XLinQ language. Another prominent example of XML processing in Web-based applications is AJAX (Asynchronous Java Programming with XML). In academia, XL, XStatic, Links, and several other languages have been proposed. All these solutions follow different philosophies and address critical design questions in different ways. This tutorial gives an overview of the current generation of programming languages for data-intensive XML applications. Furthermore, this tutorial compares the possible solutions based on a few comparative practical criteria. The tutorial shows how each solution addresses the design questions in different ways and gives the tradeoffs in terms of capabilities and optimizability of these languages are.

#index 875066
#* Record linkage: similarity measures and algorithms
#@ Nick Koudas;Sunita Sarawagi;Divesh Srivastava
#t 2006
#c 5
#% 654523
#% 824787
#! This tutorial provides a comprehensive and cohesive overview of the key research results in the area of record linkage methodologies and algorithms for identifying approximate duplicate records, and available tools for this purpose. It encompasses techniques introduced in several communities including databases, information retrieval, statistics and machine learning. It aims to identify similarities and differences across the techniques as well as their merits and limitations.

#index 875067
#* Accessing the web: from search to integration
#@ Kevin Chen-Chuan Chang;Junghoo Cho
#t 2006
#c 5
#% 783791
#! We have witnessed the rapid growth of the Web-- It has not only "broadened" but also "deepened": While the "surface Web" has expanded from the 1999 estimate of 800 million to the recent 19.2 billion pages reported by Yahoo index, an equally or even more significant amount of information is hidden on the "deep Web," behind query forms, recently estimated at over 1.2 million, of online databases. Accessing the information on the Web thus requires not only search to locate pages of interests, from the surface Web, but also integration to aggregate data from alternative or complementary sources, from the deep Web. Although the opportunities are unprecedented, the challenges are also immense: On the one hand, for the surface Web, while search seems to have evolved into a standard technology, its maturity and pervasiveness have also invited the attack of spam and the demand of personalization. On the other hand, for the deep Web, while the proliferation of structured sources has promised unlimited possibilities for more precise and aggregated access, it has also presented new challenges for realizing large scale and dynamic information integration. These issues are in essence related to data management, in a large scale, and thus present novel problems and interesting opportunities for our research community. This tutorial will discuss the new access scenarios and research problems in Web information access: from search of the surface Web to integration of the deep Web.

#index 875068
#* Adaptive query processing: why, how, when, what next
#@ A. Deshpande;J. M. Hellerstein;V. Raman
#t 2006
#c 5

#index 879030
#* Extending object database interfaces with fuzziness through aspect-oriented design
#@ Miguel-Ángel Sicilia;Elena García-Barriocanal
#t 2006
#c 5
#% 119200
#% 221392
#% 265413
#% 265768
#% 296685
#% 346357
#% 488392
#% 856403
#! Fuzzy logic has been used yet for extending database models to deal with vagueness in the definitions of linguistic concepts as "tall" or "long". However, the extension of existing programming interfaces for fuzziness requires a proper modularization of the underlying concerns of numerical imprecision handling. Such modularization should not interfere with existing programming practices, and they should not obscure the original design. Aspect-oriented design (AOD) enables such form of non-intrusive extensions to be added to existing software libraries. In this paper, the main design and implementation issues of such AOD-based extensions on OJB database libraries are briefly sketched.

#index 879031
#* Scientific formats for object-relational database systems: a study of suitability and performance
#@ Shirley Cohen;Patrick Hurley;Karl W. Schulz;William L. Barth;Brad Benton
#t 2006
#c 5
#% 168676
#% 273937
#% 385828
#! Commercial database management systems (DBMSs) have historically seen very limited use within the scientific computing community. One reason for this absence is that previous database systems lacked support for the extensible data structures and performance features required within a high-performance computing context. However, database vendors have recently enhanced the functionality of their systems by adding object extensions to the relational engine. In principle, these extensions allow for the representation of a rich collection of scientific datatypes and common statistical operations. Utilizing these new extensions, this paper presents a study of the suitability of incorporating two popular scientific formats, NetCDF and HDF, into an object-relational system. To assess the performance of the database approach, a series of solution variables from a regional weather forecast model are used to build representative small, medium and large databases. Common statistical operations and array element queries are then performed using the object-relational database, and the execution timings are compared against native NetCDF and HDF operations.

#index 879032
#* Temporal aggregates and temporal universal quantification in standard SQL
#@ Esteban Zimányi
#t 2006
#c 5
#% 287268
#% 361445
#% 481928
#% 565462
#% 578404
#% 645159
#% 726629
#! Although it has been acknowledged for many years that querying and updating time-varying information using standard (i.e., non-temporal) SQL is a challenging task, the proposed temporal extensions of SQL have not reach acceptance in the standardization committees. Therefore, nowadays database practitioners must still use standard SQL for manipulating time-varying information. This paper shows how to realize temporal aggregates and temporal universal quantifiers using standard SQL.

#index 879033
#* Developing scientific workflows from heterogeneous services
#@ A. Tsalgatidou;G. Athanasopoulos;M. Pantazoglou;C. Pautasso;T. Heinis;R. Grønmo;Hjørdis Hoff;Arne-Jørgen Berre;M. Glittum;S. Topouzidou
#t 2006
#c 5
#% 590497
#% 767482
#% 825656
#% 845705
#! Scientific WorkFlows (SWFs) need to utilize components and applications in order to satisfy the requirements of specific workflow tasks. Technology trends in software development signify a move from component-based to service-oriented approach, therefore SWF will inevitably need appropriate tools to discover and integrate heterogeneous services. In this paper we present the SODIUM platform consisting of a set of languages and tools as well as related middleware, for the development and execution of scientific workflows composed of heterogeneous services.

#index 879034
#* Impact of double-blind reviewing on SIGMOD publication rates
#@ Samuel Madden;David DeWitt
#t 2006
#c 5
#! Starting with the 2001 SIGMOD conference, the SIGMOD Chair, in consultation with the SIGMOD Advisory Committee, imposed a double blind rule on all future SIGMOD conferences. While there are many reasons why double-blind reviewing might be a good idea, the one most frequently cited is that it is fairer to more junior researchers. It is not, however, without its problems, including anecdotal reports of papers being rejected because their authors failed to cite their own papers as related work in order to not violate the anonymity rules and a complication of the job of the program chair who must interpret and enforce the double-blind rules. One very qualified individual turned down an offer to be PC chair of an upcoming SIGMOD conference because he did not want to have to deal with the headaches of double-blind reviewing.

#index 879035
#* "A veritable bucket of facts" origins of the data base management system
#@ Thomas Haigh
#t 2006
#c 5
#% 292
#% 3126
#% 53706
#% 90698
#% 186289
#% 207569
#% 286957
#% 288093
#% 317843
#% 317844
#% 317847
#% 319216
#% 319987
#% 322752
#% 322880
#% 344949
#% 349251
#% 360471
#% 374306
#% 375203
#% 408117
#% 433185
#% 433270
#% 606218
#% 606639
#% 771343
#% 790408
#% 840577
#! The data base concept derives from early military on-line systems, and was not originally associated with the specific technologies of modern data base management systems. While the idea of an integrated data base, or "bucket of facts," spread into corporate data processing and management circles during the early 1960s, it was seldom realized in practice. File-processing packages were among the very first distributed as supported products, but only in the late 1960s were they first called "data base management systems," in large part through the actions of the Data Base Task Group of the Committee on Data Systems Languages (CODASYL). As the DBMS concept spread, the data base itself was effectively redefined as the informational content of a packaged DBMS. Throughout the process, managerial descriptions of the data base as a flexible and integrated repository for all corporate data stood in sharp contrast with the useful but limited nature of actual systems.

#index 879036
#* Report from the First and Second International Workshops on Information Quality in Information Systems: IQIS 2004 and IQIS 2005 in conjunction with ACM SIGMOD/PODS Conferences
#@ Monica Scannapieco;Laure Berti-Équille
#t 2006
#c 5
#% 768937
#% 768938
#% 768939
#% 768940
#% 768941
#% 768942
#% 768943
#% 768944
#% 768945
#% 819544
#% 819545
#% 819546
#% 819547
#% 819548
#% 819549
#% 819550
#% 819551
#% 819552
#% 819553
#% 819554
#% 819555
#% 819556
#! This report summarizes the constructive discussions of the first two editions of the International Workshop on Information Quality in Information Systems, IQIS 2004 and IQIS 2005, held respectively in Paris, France, on June 13, 2004 and in Baltimore, MD, USA, on June 17, 2005.

#index 879037
#* Report on the 7th Workshop on Distributed Data and Structures: (WDAS 2006)
#@ Thomas Schwarz;Mark Manasse
#t 2006
#c 5
#% 772018
#! The seventh Workshop on Distributed Data and Structures (WDAS) took place on the campus of Santa Clara University on January 4 and 5, 2006 and drew participants actively working in research on distributed data, structures, and their applications. WDAS aims to stimulate the exchange of ideas and to be a forum for work in progress. The electronic version of the Proceedings contains all papers accepted at the conference [WDAS]. In addition, the workshop presentations helped to select a subset that will appear revised in the Proceedings in Informatics Series at Carleton Scientific. Two papers could not be presented by authors because of the difficulties of obtaining an entry visa to the United States, unfortunately a more and more common phenomenon. We now give an overview of the topics discussed at the workshop and the related presentations.

#index 879038
#* Report on the 2nd International Workshop on Data Integration in the Life Sciences: (DILS'05)
#@ Amarnath Gupta;Bertram Ludäscher;Louiqa Raschid
#t 2006
#c 5
#! Following a successful first international workshop on Data Integration in the Life Sciences (DILS) in 2004 in Leipzig, Germany [2], the second DILS workshop was held in San Diego, California from July 20-22, 2005 [1]. This new workshop series reflects the strong interest in an annual event, bringing together biologists and computer scientists conducting research in life science data management and integration, in domains and applications such as molecular biology, biodiversity, drug discovery and personalized medical research. The program committee accepted 15 long papers and 5 short papers from 42 submissions. In addition, DILS also featured 7 posters, 2 keynotes, several reports on ongoing research activities in academia and industry, and a panel on The Electronic Health Record of the Future: Incorporating Molecular Information, organized by the AMIA Genomics Working Group.

#index 879039
#* Report on the 7th ACM International Workshop on Web Information and Data Management: (WIDM 2005)
#@ Angela Bonifati;Dongwon Lee
#t 2006
#c 5
#% 836142
#% 836143
#% 836144
#% 836145
#% 836146
#% 836147
#% 836148
#% 836149
#% 836150
#% 836151
#% 836152
#% 836153
#% 836154
#% 836155
#! In this report, to our best recollection, we provide a summary of the 7th ACM International Workshop on Web Information and Data Management (WIDM 2005), which took place at the Hilton Bremen Hotel, in Bremen, on November 5, 2005, in conjunction with the 14th ACM Int'l Conf. on Information and Knowledge Management (CIKM).

#index 879040
#* Data management research at the Knowledge and Database Systems Lab: (NTU Athens)
#@ Timos Sellis;Yannis Vassiliou
#t 2006
#c 5
#% 413119
#% 488602
#% 488616
#% 757717
#% 773462
#% 800563
#% 810030
#% 824935
#% 830388
#% 836111
#% 838533
#% 839148
#% 839586
#% 864456
#% 885377
#% 885390
#% 1388088
#% 1688247
#% 1698667
#% 1698953
#% 1702403
#% 1703055
#% 1705180
#% 1720926
#% 1728691
#% 1728718
#! The Knowledge and Database Systems Lab (KDBSL) of the Electrical and Computer Engineering Dept. in the National Technical University of Athens was founded in 1992 by Prof. Timos Sellis and Prof. Yannis Vassiliou. Its activities involve theoretical and applied research in the area of Databases and Information Systems. The lab employs three postdoc researchers (Dr Theodore Dalamagas, Dr Alkis Simitsis, Dr Yannis Stavrakas), several PhD students and many graduate students. It has been involved in many research projects supported by the EU, international institutions, Greek organizations, the Greek Government and industrial companies.

#index 879041
#* Consistent query answering in databases
#@ Leopoldo Bertossi
#t 2006
#c 5
#% 2080
#% 175440
#% 273687
#% 342829
#% 378409
#% 417542
#% 449224
#% 460928
#% 465052
#% 465057
#% 476576
#% 488620
#% 576116
#% 582130
#% 724872
#% 727668
#% 752741
#% 783532
#% 809239
#% 810020
#% 814475
#% 825671
#% 838543
#% 851702
#% 857282
#% 1279213
#% 1673662
#% 1673673
#% 1673674
#% 1700140
#% 1705008
#% 1707166
#% 1712584
#% 1728683
#% 1972413
#! For several reasons databases may become inconsistent with respect to a given set of integrity constraints (ICs): (a) The DBMS have no mechanism to maintain certain classes of ICs. (b) New constraints are imposed on preexisting, legacy data. (c) The ICs are soft, user, or informational constraints that are considered at query time, but without being necessarily enforced. (d) Data from different and autonomous sources are being integrated, in particular in mediator-based approaches.

#index 879042
#* Raghu Ramakrishnan speaks out on deductive databases, what lies beyond scalability, how he burned through $20M briskly, why we should reach out to policymakers, and more
#@ Marianne Winslett
#t 2006
#c 5
#! Welcome to this installment of ACM SIGMOD Record's series of interviews with distinguished members of the database community. I'm Marianne Winslett, and today we are at the Department of Computer Science at the University of Illinois at Urbana-Champaign. I have here with me Raghu Ramakrishnan, who is a Professor of Computer Sciences at the University of Wisconsin-Madison. Raghu's research currently focuses on data retrieval and integration, analysis, and mining. Raghu was a founder of QUIQ, a company that developed a collaborative customer support facility. Raghu has received the National Science Foundation's Presidential Young Investigator award, a Packard Fellowship, and a SIGMOD Contributions Award for founding and maintaining DBWorld. Raghu is an ACM Fellow; he is a coauthor of the popular Database Management Systems textbook; and he is also the current chair of ACM SIGMOD. Raghu's PhD is from the University of Texas at Austin. So, Raghu, welcome!

#index 890476
#* Proceedings of the 2nd international workshop on Computer vision meets databases
#@ Laurent Amsaleg;Björn þór Jónsson;Vincent Oria
#t 2005
#c 5
#! This second edition of the Computer Vision meets Databases (CVDB) workshop is held in Baltimore, MD, USA, on June 17, 2005. The workshop is sponsored by ACM and together with other innovative workshops CVDB is co-located with the Symposium on Principles of Database System (PODS) and the ACM SIGMOD International Conference on Management of Data. For decades, the computer vision community has been working on content-based multimedia retrieval. Researchers from that community aim at defining better content-based descriptors and extracting them from images. The descriptors obtained are often represented as points in multi-dimensional spaces and some metrics are used during similarity retrieval. Their focus is on increasing the recognition power of their schemes and they usually evaluate their strength using data sets that fit in main memory because they try to avoid the secondary storage management burden.Facilitating the management of very large amounts of data and removing this disk burden has long been a strong motivation for the database community. This is particularly crucial for multimedia databases whose sizes grow very fast. As such, researchers in databases have proposed many smart multidimensional indexing schemes with some elegant algorithms to compute Nearest-Neighbor and Top-N queries.Yet, it is surprising to see that only few works in the computer vision community have adopted any of these indexing schemes. A common reason evoked is that the description schemes that database researchers use are way too simplistic. Therefore, it is hard for computer vision researchers to foresee how indexes could behave when used with a modern and powerful description scheme. Additional reasons given include the assumptions on the distribution of data, the ability to only retrieve the single nearest neighbor of query points, and the use of approximate search schemes that give little clue as to the quality of the returned results.The goal of' this workshop is to bridge this gap between the two communities. The idea is to provide database researchers with a snapshot of what computer vision people are dealing with and vice-versa, with the aim of' defining some research directions that can benefit both communities. There is great expertise on both sides, and this workshop is aimed at sharing it by means of tutorials and presentations.

#index 896020
#* Impact of double blind reviewing on SIGMOD publication: a more detail analysis
#@ Anthony K. H. Tung
#t 2006
#c 5
#% 879034
#! In [1], a set of statistic had been provided with the conclusion that double blind reviewing make no impact on SIGMOD publication. Our studies here will show results contrary to that finding.

#index 896021
#* Single- versus double-blind reviewing: an analysis of the literature
#@ Richard Snodgrass
#t 2006
#c 5
#% 731473
#% 748026
#% 879034
#! The peer review process is generally acknowledged as central to the advancement of scholarly knowledge. It is also vital to the advancement of individual careers.

#index 896022
#* Model driven development of secure XML databases
#@ Belén Vela;Eduardo Fernández-Medina;Esperanza Marcos;Mario Piattini
#t 2006
#c 5
#% 301160
#% 305928
#% 379248
#% 424307
#% 441858
#% 534091
#% 646047
#% 665543
#% 725290
#% 729342
#% 803779
#% 948859
#% 1347475
#! In this paper, we propose a methodological approach for the model driven development of secure XML databases (DB). This proposal is within the framework of MIDAS, a model driven methodology for the development of Web Information Systems based on the Model Driven Architecture (MDA) proposed by the Object Management Group (OMG) [20]. The XML DB development process in MIDAS proposes using the data conceptual model as a Platform Independent Model (PIM) and the XML Schema model as a Platform Specific Model (PSM), with both of these represented in UML. In this work, such models will be modified, so as to be able to add security aspects if the stored information is considered as critical. On the one hand, the use of a UML extension to incorporate security aspects at the conceptual level of secure DB development (PIM) is proposed; on the other, the previously-defined XML schema profile will be modified, the purpose being to incorporate security aspects at the logical level of the secure XML DB development (PSM). In addition to all this, the semi-automatic mappings from PIM to PSM for secure XML DB will be defined.

#index 896023
#* An automatic construction and organization strategy for ensemble learning on data streams
#@ Yi Zhang;Xiaoming Jin
#t 2006
#c 5
#% 136350
#% 204531
#% 209021
#% 290482
#% 342600
#% 342639
#% 424997
#% 727880
#% 729932
#% 769888
#% 769927
#% 785371
#% 823408
#% 1650665
#! As data streams are gaining prominence in a growing number of emerging application domains, classification on data streams is becoming an active research area. Currently, the typical approach to this problem is based on ensemble learning, which learns basic classifiers from training data stream and forms the global predictor by organizing these basic ones. While this approach seems successful to some extent, its performance usually suffers from two contradictory elements existing naturally within many application scenarios: firstly, the need for gathering sufficient training data for basic classifiers and engaging enough basic learners in voting for bias-variance reduction; and secondly, the requirement for significant sensitivity to concept-drifts, which places emphasis on using recent training data and up-to-date individual classifiers. It results in such a dilemma that some algorithms are not sensitive enough to concept-drifts while others, although sensitive enough, suffer from unsatisfactory classification accuracy. In this paper, we propose an ensemble learning algorithm, which: (1) furnishes training data for basic classifiers, starting from the up-to-date data chunk and searching for complement from past chunks while ruling out the data inconsistent with current concept; (2) provides effective voting by adaptively distinguishing sensible classifiers from the else and engaging sensible ones as voters. Experimental results justify the superiority of this strategy in terms of both accuracy and sensitivity, especially in severe circumstances where training data is extremely insufficient or concepts are evolving frequently and significantly.

#index 896024
#* A survey on ontology mapping
#@ Namyoun Choi;Il-Yeol Song;Hyoil Han
#t 2006
#c 5
#% 22948
#% 261694
#% 341655
#% 384416
#% 431103
#% 529190
#% 531444
#% 571670
#% 578668
#% 643964
#% 721077
#% 728755
#% 742769
#% 769985
#% 800497
#% 830529
#% 1289178
#% 1655419
#! Ontology is increasingly seen as a key factor for enabling interoperability across heterogeneous systems and semantic web applications. Ontology mapping is required for combining distributed and heterogeneous ontologies. Developing such ontology mapping has been a core issue of recent ontology research. This paper presents ontology mapping categories, describes the characteristics of each category, compares these characteristics, and surveys tools, systems, and related work based on each category of ontology mapping. We believe this paper provides readers with a comprehensive understanding of ontology mapping and points to various research topics about the specific roles of ontology mapping.

#index 896025
#* The database research group at the Max-Planck Institute for Informatics
#@ Gerhard Weikum
#t 2006
#c 5
#% 344448
#% 504581
#% 643566
#% 766435
#% 783476
#% 800534
#% 818210
#% 818232
#% 824695
#% 824703
#% 824704
#% 824762
#% 845359
#% 855601
#% 857107
#% 869600
#% 879625
#% 881539
#% 893123
#% 893128
#% 1663635
#% 1688255
#% 1742091
#% 1742096
#! The Max-Planck Institute for Informatics (MPI-INF) is one of 80 institutes of the Max-Planck Society, Germany's premier scientific organization for foundational research with numerous Nobel prizes in natural sciences and medicine. MPI-INF hosts about 150 researchers (including graduate students) and comprises 5 research groups on algorithms and complexity, programming logics, computational biology and applied algorithmics, computer graphics, and databases and information systems (DBIS). This report gives an overview of the DBIS group's mission and ongoing research.

#index 896026
#* A report on the first international workshop on best practices of UML: (BP-UML'05)
#@ Juan Trujillo
#t 2006
#c 5
#% 1739332
#% 1739333
#% 1739334
#% 1739335
#% 1739336
#% 1739337
#% 1739338
#% 1739340
#! The Unified Modeling Language (UML) has been widely accepted as the standard object-oriented (OO) modeling language for modeling various aspects of software and information systems. The UML is an extensible language, in the sense that it provides mechanisms to introduce new elements for specific domains if necessary, such as web applications, database applications, business modeling, software development processes, data warehouses and so on. Furthermore, the latest work of the Object Management Group (OMG) on UML [1] resulted in a larger and more complicated specification, with even more diagrams for some good reasons. Although providing different diagrams for modeling specific parts of a software system, not all of them need to be applied in most cases. Therefore, heuristics, design guidelines, and lessons learned from experiences are extremely important for the effective use of UML and to avoid unnecessary complication.

#index 896027
#* Report on the International Provenance and Annotation Workshop: (IPAW'06) 3-5 May 2006, Chicago
#@ Rajendra Bose;Ian Foster;Luc Moreau
#t 2006
#c 5
#! The provenance of a data item refers to its origins and processing history, while annotation is a term that refers to the process of adding notes or data to an existing structure. Because these terms are broad, and are used in slightly different ways by different communities, confusion is rampant. For example, consider that (1) annotating a data set with its provenance information, and (2) finding the provenance of a specific data annotation are both perfectly reasonable concepts.

#index 896028
#* Report on SciFlow 2006: the IEEE international workshop on workflow and data flow for scientific applications
#@ Brian F. Cooper;Roger Barga
#t 2006
#c 5
#! Computation has been described as the "third leg" of science, along with theory and experimentation. Certainly, modern information systems are vital to managing and processing the huge amounts of data produced by simulations and experiments. However, existing tools are only now beginning to catch up with the needs of today's scientists, and much more needs to be done to support the computational needs of tomorrow's scientists. In particular, scientists still need effective tools to deal with massive data sets that may be geographically scattered, to apply multiple complex and interacting transformations to the data, and to ensure the quality and repeatability of their computations. The IEEE SciFlow workshop brought together computing researchers who are exploring how to build the next generation of information systems to address these needs. The workshop was held on April 8, 2006, in conjunction with the IEEE International Conference on Data Engineering in Atlanta, Georgia, USA.

#index 896029
#* Jennifer Widom speaks out: on luck, what constitutes success, when to get out of an area, the importance of choosing the right husband, outlandish vacations, how hard it is to be an assistant professor, and more
#@ Marianne Winslett
#t 2006
#c 5
#! Welcome to this installment of ACM SIGMOD Record's series of interviews with distinguished members of the database community. I'm Marianne Winslett, and today we are at the Department of Computer Science at the University of Illinois at Urbana-Champaign. I have here with me Jennifer Widom, who is a professor of Computer Science at Stanford University. Jennifer's research currently focuses on data provenance, management of uncertainty, queries over web services, and data streams. Jennifer is a member of the National Academy of Engineering, is an ACM Fellow, and is a former Guggenheim Fellow. Before joining Stanford, Jennifer worked at IBM Almaden Research Center. Jennifer's PhD is from Cornell. So, Jennifer, welcome!

#index 945786
#* Why is schema matching tough and what can we do about it?
#@ Avigdor Gal
#t 2006
#c 5
#% 12952
#% 237184
#% 480134
#% 551850
#% 572314
#% 578668
#% 660001
#% 745483
#% 765540
#% 800005
#% 800498
#% 823368
#% 824735
#% 993982
#% 1671768
#% 1688251
#% 1702409
#% 1705177
#% 1720904
#% 1730010
#! In this paper we analyze the problem of schema matching, explain why it is such a "tough" problem and suggest directions for handling it effectively. In particular, we present the monotonicity principle and see how it leads to the use of top-K mappings rather than a single mapping.

#index 945787
#* TPCC-UVa: an open-source TPC-C implementation for global performance measurement of computer systems
#@ Diego R. Llanos
#t 2006
#c 5
#% 181429
#% 261740
#% 438274
#% 438537
#% 787944
#! This paper presents TPCC-UVa, an open-source implementation of the TPC-C benchmark version 5 intended to be used to measure performance of computer systems. TPCC-UVa is written entirely in C language and it uses the PostgreSQL database engine. This implementation includes all the functionalities described by the TPC-C standard specification for the measurement of both uni- and multiprocessor systems performance. The major characteristics of the TPC-C specification are discussed, together with a description of the TPCC-UVa implementation, architecture, and performance metrics obtained. As working examples, TPCC-UVa is used in this paper to measure performance of different file systems under Linux, and to compare the relative performance of multi-core CPU technologies and their single-core counterparts.

#index 945788
#* XML search: languages, INEX and scoring
#@ Sihem Amer-Yahia;Mounia Lalmas
#t 2006
#c 5
#% 215225
#% 275308
#% 333981
#% 458829
#% 465155
#% 642993
#% 654441
#% 654442
#% 660011
#% 745450
#% 754116
#% 765408
#% 766415
#% 766416
#% 766417
#% 783526
#% 803545
#% 810052
#% 818242
#% 824681
#% 824703
#% 824792
#% 845359
#% 875018
#% 1674721
#% 1674722
#% 1674723
#% 1674726
#% 1674730
#% 1674732
#% 1674734
#% 1674741
#% 1688266
#! The development of approaches to access XML content has generated a wealth of issues in information retrieval (IR) and database (DB) (e.g., [2, 15, 17, 20, 19, 47, 26, 32, 24]). While the IR community has traditionally focused on searching unstructured content, and has developed various techniques for ranking query results and evaluating their effectiveness, the DB community has focused on developing query languages and efficient evaluation algorithms for highly structured content. Recent trends in DB and IR research demonstrate a growing interest in merging IR and DB techniques for accessing XML content. Support for a combination of "structured" and full-text search for effectively querying XML documents was unanimous in a recent panel at SIGMOD 2005 [3], and is being widely studied in the IR community [20].

#index 945789
#* Relaxed space bounding for moving objects: a case for the buddy tree
#@ Shuqiao Guo;Zhiyong Huang;H. V. Jagadish;Beng Chin Ooi;Zhenjie Zhang
#t 2006
#c 5
#% 86950
#% 273706
#% 285932
#% 286929
#% 300174
#% 317933
#% 335044
#% 427199
#% 461923
#% 480587
#% 481759
#% 622692
#% 1015320
#% 1716977
#! Rapid advancements in positioning systems and wireless communications enable accurate tracking of continuously moving objects. This development poses new challenges to database technology since maintaining up-to-date information regarding the location of moving objects incurs an enormous amount of updates. There have been many efforts to address these challenges, most of which depend on the use of a minimum bounding rectangle (MBR) in a multi-dimensional index structure such as R-tree. The maintenance of MBRs causes lock contention and association of moving speeds with the MBRs cause large overlap between them. This problem becomes more severe as the number of concurrent operations increases. In this paper, we propose a "new" simple variant of the Buddy-tree, in which we enlarge the query rectangle to account for object movement rather than use an enlarged MBR. The result is not only elegant, but also efficient, particularly in terms of lock contention. An extensive experimental study was conducted and the results show that our proposed structure outperforms existing structures by a wide margin.

#index 945790
#* An online bibliography on schema evolution
#@ Erhard Rahm;Philip A. Bernstein
#t 2006
#c 5
#% 32903
#% 126704
#% 348187
#% 443145
#% 445750
#% 480134
#% 535525
#% 572314
#% 726550
#% 810117
#% 824736
#! We briefly motivate and present a new online bibliography on schema evolution, an area which has recently gained much interest in both research and practice.

#index 945791
#* The ADO.NET entity framework: making the conceptual level real
#@ José A. Blakeley;David Campbell;S. Muralidhar;Anil Nori
#t 2006
#c 5
#% 287631
#! This paper describes the ADO.NET Entity Framework, a platform for programming against data that raises the level of abstraction from the logical (relational) level to the conceptual (entity) level, and thereby significantly reduces the impedance mismatch for applications and data services such as reporting, analysis, and replication. The conceptual data model is made real by a runtime that implements an extended relational model (the Entity Data Model aka the EDM), that embraces entities and relationships as first class concepts; a query language for the EDM; a comprehensive mapping engine that translates from the conceptual to the logical (relational) level, and a set of model-driven tools that help create entity-object, object-xml, and entity-xml transformers.

#index 945792
#* Data management for a smart earth: the Swiss NCCR-MICS initiative
#@ Karl Aberer;Gustavo Alonso;Donald Kossmann
#t 2006
#c 5
#% 893201
#! The Swiss National Competence Center for Research in mobile Information and Communication Systems (NCCR-MICS or MICS) is one of several research initiatives sponsored by the Swiss National Science Foundation to promote long term research projects in areas of vital strategic importance for the evolution of science in Switzerland, for the country's economy and for Swiss society. NCCR-MICS covers a wide spectrum of topics in the area of mobile information and communication systems ranging from information theory related to ad-hoc sensor networks to business models for pervasive computing, including network and routing issues, software and application development, and actual deployments of sensor networks (from architecture to geology). In this paper, we briefly present MICS as a whole and discuss in some detail two ambitious projects in the area of data management. The first project, XTream, addresses the whole life cycle of sensor based applications from the acquisition by sensors, aggregation and integration in gateways, storage in databases, generation of events that are relevant to users and applications, up to the subscription and consumption of events in a distributed architecture. The second project is Global Sensor Networks (GSN), which aims at enabling the rapid and efficient publication, sharing and interoperability of heterogeneous sensor data sources over large networks such as the Internet and P2P overlays.

#index 945793
#* Report on the Second International Workshop on Data Management on Modern Hardware (DaMoN'06)
#@ Anastassia Ailamaki;Peter Boncz;Stefan Manegold
#t 2006
#c 5
#! This report summarizes the presentations and discussions that occurred during the Second International Workshop on Data Management on Modern Hardware (DaMoN). DaMoN was held in Chicago on June 25th, 2006, and was collocated with ACM SIGMOD 2006. The aim of this one-day workshop is to bring together researchers interested in optimizing database performance on modern computing infrastructure by designing new data management techniques and tools.

#index 945794
#* A report on the Eighth ACM International Workshop on Data Warehousing and OLAP (DOLAP'05)
#@ Juan Trujillo;Il-Yeol Song
#t 2006
#c 5
#% 836104
#% 836105
#% 836107
#% 836108
#% 836109
#% 836111
#% 836112
#% 836113
#% 836114
#% 836115
#% 836116
#! Research in data warehousing and OLAP has produced important technologies for the design, management and use of information systems for decision support. Much of the interest and success in this area can be attributed to the need for software and tools to improve data management and analysis given the large amounts of information that are being accumulated in corporate as well as scientific databases. However, in spite of the maturity of these technologies, new data needs or applications currently run at companies not only demand more capacity, but also new methods, models, techniques or architectures to satisfy these new needs.

#index 945795
#* ICDE 2006 Ph.D. workshop report
#@ Wai Gen Yee;Shamkant B. Navathe
#t 2006
#c 5
#! This report summarizes the Ph.D. Workshop held in conjunction with the 2006 IEEE International Conference on Data Engineering. This report includes a summary of the technical presentations as well as the panelist discussion.

#index 945796
#* Report on the Models of Trust for the Web workshop (MTW'06)
#@ Tim Finin;Lalana Kagal;Daniel Olmedilla
#t 2006
#c 5
#% 893171
#! We live in a time when millions of people are adding information to the Web through a growing collection of tools and platforms. Ordinary citizens publish all kinds of content on Web pages, blogs, wikis, podcasts, vlogs, message boards, shared spreadsheets, and new publishing forums that seem to appear almost monthly. As it becomes easier for people to add information to the Web, it is more difficult, and also more important, to distinguish reliable information and sources from those that are not.

#index 945797
#* Normalization theory for XML
#@ Marcelo Arenas
#t 2006
#c 5
#% 115608
#% 286860
#% 287754
#% 299943
#% 332151
#% 333841
#% 333979
#% 336874
#% 411570
#% 428149
#% 443343
#% 458850
#% 533902
#% 655781
#% 742566
#% 771227
#% 772031
#% 804840
#% 866986
#% 874888
#% 877241
#% 1673672
#% 1705007
#! Since the beginnings of the relational model, it was clear for the database community that the process of designing a database is a nontrivial and time-consuming task. Even for simple application domains, there are many possible ways of storing the data of interest.

#index 945798
#* Gerome Miklau speaks out on his SIGMOD distinguished dissertation award, how great it is to be a professor, and more
#@ Marianne Winslett
#t 2006
#c 5
#! I would like to take a moment to thank the many people who have helped me devise interview questions over the years. Often people propose questions under a promise of anonymity, so I will not name the individuals who have suggested questions---but you know who you are! Without you, these columns would not be possible. Thank you for your many excellent suggestions over the years.

#index 945799
#* Yannis Ioannidis speaks out on database research funding in Europe, the importance of being uncertain, teaching as show business, the history of histograms, and more
#@ Marianne Winslett
#t 2006
#c 5
#! Welcome to this installment of ACM SIGMOD Record's series of interviews with distinguished members of the database community. I'm Marianne Winslett, and today we are at the SIGMOD 2006 conference in Chicago. I have here with me Yannis Ioannidis, who is a professor of informatics and telecommunication at the University of Athens. Before that, he was a professor for many years at the University of Wisconsin in Madison. Yannis's research focuses on query optimization, digital libraries, and management of scientific data. Yannis is an ACM Fellow, has won numerous teaching awards, and received the VLDB 10 Year Best Paper Award. He is the Vice-Chair of SIGMOD, and his PhD is from the University of California at Berkeley. So, Yannis, welcome!

#index 960232
#* Proceedings of the 2007 ACM SIGMOD international conference on Management of data
#@ Lizhu Zhou;Tok Wang Ling;Beng Chin Ooi
#t 2007
#c 5
#! This is the first time that SIGMOD is held in Asia and the second time that it is held outside North America. The amazing growth rates of China in both its technology and economy fronts do not alter the image of its capital city, Beijing, being an ancient city whose recorded history stretches back more than 3,000 years. With a landscape dotted with ancient palaces and temples in the midst of modern infrastructure and architecture, Beijing is indeed a good venue for a forum of serious academic and professional exchanges, and an ideal place for meaningful entertainment and cultural immersion on the side. As in previous two years, SIGMOD 2007 accepted all papers that the Program Committee considered appropriate to accept, with no predefined "quota" based on the duration of the conference or other non-technical factors. The 88 members of the Program Committee were very thorough and dedicated, and accepted only 70 papers out of the 480 submissions giving an acceptance ratio of 14.6% (which is slightly higher than SIGMOD 2006). We adopted the author feedback process started in SIGMOD 2005 and the two-phase review process initiated in SIGMOD 2006. Two reviews were requested for every paper in the first phase and only papers that were deemed interesting by at least one of the two reviewers were considered for an additional review and for follow-up discussions. However, for a small number of papers, the third review was initiated due to the uncertainty shown in the first two reviews. In total, around 60% of the submissions were reviewed in the second phase and the author feedback process was then initiated for papers in which the Program Committee had a divided opinion or explicitly sought for feedback. All submissions were then actively discussed during the 18 days of online discussion. The double-blind reviewing process was further strengthened in SIGMOD 2007 by the non posting requirement of the submitted papers on the web during the reviewing period. For the benefit of the authors, the review process and the author feedback dates were clearly stated in the submission guidelines. In SIGMOD 2007, we have invited three distinguished members from our own community as keynote speakers. As in SIGMOD 2006, we have a plenary session on SIGMOD Awards Technical talks, apart from the research papers, industrial talks, demos and tutorials. For the industrial talks, there is a session dedicated to database system development in the Far East industry. The quality and variety of the research papers, industrial talks, demos, tutorials, invited and keynote presentations have allowed us to assemble a diverse and vibrant technical program. However, with the increase in the number of research and industrial papers, as well as the plenary session, we now have five parallel tracks compared to the usual four.

#index 960233
#* Model management 2.0: manipulating richer mappings
#@ Philip A. Bernstein;Sergey Melnik
#t 2007
#c 5
#% 11284
#% 111378
#% 116303
#% 223781
#% 248038
#% 271710
#% 275367
#% 281976
#% 283052
#% 286901
#% 287733
#% 294600
#% 328429
#% 332166
#% 333988
#% 378409
#% 384978
#% 394149
#% 442861
#% 458607
#% 458995
#% 480134
#% 480322
#% 482067
#% 572311
#% 572314
#% 579315
#% 654457
#% 762652
#% 765540
#% 801413
#% 805821
#% 806215
#% 809239
#% 809249
#% 810021
#% 810073
#% 810078
#% 824658
#% 824736
#% 824767
#% 826032
#% 845350
#% 850730
#% 874876
#% 874881
#% 875028
#% 881740
#% 893093
#% 893094
#% 893095
#% 893193
#% 945790
#% 945791
#% 960272
#% 960309
#% 976996
#% 1015303
#% 1015326
#% 1277746
#% 1393677
#% 1661428
#% 1688267
#% 1705177
#% 1730010
#! Model management is a generic approach to solving problems of data programmability where precisely engineered mappings are required. Applications include data warehousing, e-commerce, object-to-relational wrappers, enterprise information integration, database portals, and report generators. The goal is to develop a model management engine that can support tools for all of these applications. The engine supports operations to match schemas, compose mappings, diff schemas, merge schemas, translate schemas into different data models, and generate data transformations from mappings. Much has been learned about model management since it was proposed seven years ago. This leads us to a revised vision that differs from the original in two main respects: the operations must handle more expressive mappings, and the runtime that executes mappings should be added as an important model management component. We review what has been learned from recent experience, explain the revised model management vision based on that experience, and identify the research problems that the revised vision opens up.

#index 960234
#* Making database systems usable
#@ H. V. Jagadish;Adriane Chapman;Aaron Elkiss;Magesh Jayapandian;Yunyao Li;Arnab Nandi;Cong Yu
#t 2007
#c 5
#% 55914
#% 115412
#% 224690
#% 232476
#% 237099
#% 268079
#% 287497
#% 296371
#% 308769
#% 308770
#% 318444
#% 333845
#% 345710
#% 397365
#% 428249
#% 462072
#% 464891
#% 479803
#% 479805
#% 480153
#% 480483
#% 523480
#% 570875
#% 577329
#% 598741
#% 620053
#% 641979
#% 642993
#% 654442
#% 660011
#% 664819
#% 725502
#% 745519
#% 765408
#% 767029
#% 770354
#% 793417
#% 810101
#% 814647
#% 825661
#% 855482
#% 859777
#% 874876
#% 875015
#% 875035
#% 884422
#% 893115
#% 893167
#% 931379
#% 960287
#% 960360
#% 960363
#% 993987
#% 1015258
#% 1016135
#% 1016204
#% 1046515
#% 1069035
#% 1688287
#% 1696299
#% 1733302
#% 1783099
#! Database researchers have striven to improve the capability of a database in terms of both performance and functionality. We assert that the usability of a database is as important as its capability. In this paper, we study why database systems today are so difficult to use. We identify a set of five pain points and propose a research agenda to address these. In particular, we introduce a presentation data model and recommend direct data manipulation with a schema later approach. We also stress the importance of provenance and of consistency across presentation models.

#index 960235
#* DB&IR: both sides now
#@ Gerhard Weikum
#t 2007
#c 5
#% 215225
#% 237053
#% 248801
#% 340914
#% 393844
#% 452991
#% 504581
#% 793374
#% 803540
#% 824693
#% 824695
#% 824703
#% 824792
#% 830520
#% 838492
#% 845359
#% 858036
#% 866985
#% 874894
#% 874992
#% 879568
#% 879620
#% 881539
#% 893119
#% 913793
#% 956564
#% 956574
#% 960259
#% 993987
#% 1016242
#% 1098449
#% 1275182
#% 1300591
#% 1651364
#% 1667783
#% 1667787

#index 960236
#* Scaling games to epic proportions
#@ Walker White;Alan Demers;Christoph Koch;Johannes Gehrke;Rajmohan Rajagopalan
#t 2007
#c 5
#% 286677
#% 333977
#% 402644
#% 567946
#% 749737
#% 875021
#! We introduce scalability for computer games as the next frontier for techniques from data management. A very important aspect of computer games is the artificial intelligence (AI) of non-player characters. To create interesting AI in games today, developers or players have to create complex, dynamic behavior for a very small number of characters, but neither the game engines nor the style of AI programming enables intelligent behavior that scales to a very large number of non-player characters. In this paper we make a first step towards truly scalable AI in computer games by modeling game AI as a data management problem. We present a highly expressive scripting language SGL that provides game designers and players with a data-driven AI scheme for customizing behavior for individual non-player characters. We use sophisticated query processing and indexing techniques to efficiently execute large numbers of SGL scripts, thus providing a framework for games with a truly epic number of non-player characters. Experiments show the efficacy of our solutions.

#index 960237
#* Indexing dataspaces
#@ Xin Dong;Alon Halevy
#t 2007
#c 5
#% 18614
#% 212287
#% 290703
#% 387427
#% 397359
#% 397360
#% 397375
#% 464883
#% 479465
#% 479803
#% 480656
#% 654450
#% 654452
#% 659990
#% 660000
#% 660011
#% 745461
#% 745468
#% 765466
#% 810052
#% 824695
#% 869535
#% 874876
#% 878916
#% 879610
#% 884628
#% 893110
#% 993953
#% 993987
#% 994015
#% 1275182
#! Dataspaces are collections of heterogeneous and partially unstructured data. Unlike data-integration systems that also offer uniform access to heterogeneous data sources, dataspaces do not assume that all the semantic relationships between sources are known and specified. Much of the user interaction with dataspaces involves exploring the data, and users do not have a single schema to which they can pose queries. Consequently, it is important that queries are allowed to specify varying degrees of structure, spanning keyword queries to more structure-aware queries. This paper considers indexing support for queries that combine keywords and structure. We describe several extensions to inverted lists to capture structure when it is present. In particular, our extensions incorporate attribute labels, relationships between data items, hierarchies of schema elements, and synonyms among schema elements. We describe experiments showing that our indexing techniques improve query efficiency by an order of magnitude compared with alternative approaches, and scale well with the size of the data.

#index 960238
#* Design of flash-based DBMS: an in-page logging approach
#@ Sang-Won Lee;Bongki Moon
#t 2007
#c 5
#% 117
#% 107692
#% 111368
#% 114582
#% 340700
#% 458544
#% 480096
#% 566138
#% 783735
#% 800543
#% 812965
#% 829901
#% 884979
#% 893215
#% 1016185
#% 1746774
#% 1820346
#! The popularity of high-density flash memory as data storage media has increased steadily for a wide spectrum of computing devices such as PDA's, MP3 players, mobile phones and digital cameras. More recently, computer manufacturers started launching new lines of mobile or portable computers that did away with magnetic disk drives altogether, replacing them with tens of gigabytes of NAND flash memory. Like EEPROM and magnetic disk drives, flash memory is non-volatile and retains its contents even when the power is turned off. As its capacity increases and price drops, flash memory will compete more successfully with lower-end, lower-capacity disk drives. It is thus not inconceivable to consider running a full database system on the flash-only computing platforms or running an embedded database system on the lightweight computing devices. In this paper, we present a new design called in-page logging (IPL) for flash memory based database servers. This new design overcomes the limitations of flash memory such as high write latency, and exploits unique characteristics of flash memory to achieve the best attainable performance for flash-based database servers. We show empirically that the IPL approach can yield considerable performance benefit over traditional design for disk-based database servers. We also show that the basic design of IPL can be elegantly extended to support transactional database recovery.

#index 960239
#* Approximate algorithms for K-anonymity
#@ Hyoungmin Park;Kyuseok Shim
#t 2007
#c 5
#% 248030
#% 300120
#% 341704
#% 481290
#% 576761
#% 800515
#% 801690
#% 810011
#% 824726
#% 864406
#% 864412
#% 1700134
#! When a table containing individual data is published, disclosure of sensitive information should be prohibitive. A naive approach for the problem is to remove identifiers such as name and social security number. However, linking attacks which joins the published table with other tables on some attributes, called quasi-identifier, may reveal the sensitive information. To protect privacy against linking attack, the notion of k-anonymity which makes each record in the table be indistinguishable with k-1 other records has been proposed previously. It is shown to be NP-Hard to k-anonymize a table minimizing the number of suppressed cells. To alleviate this, O(k log k)-approximation and O(k)-approximation algorithms were proposed in previous works. In this paper, we propose several approximation algorithms that guarantee O(log k)-approximation ratio and perform significantly better than the traditional algorithms. We also provide O(ß log k)-approximate algorithms which gracefully adjust their running time according to the tolerance é (≥ 1) of the approximation ratios. Experimental results confirm that our approximation algorithms perform significantly better than traditional approximation algorithms.

#index 960240
#* Auditing disclosure by relevance ranking
#@ Rakesh Agrawal;Alexandre Evfimievski;Jerry Kiernan;Raja Velu
#t 2007
#c 5
#% 210077
#% 726623
#% 765449
#% 803781
#% 1016172
#! Numerous widely publicized cases of theft and misuse of private information underscore the need for audit technology to identify the sources of unauthorized disclosure. We present an auditing methodology that ranks potential disclosure sources according to their proximity to the leaked records. Given a sensitive table that contains the disclosed data, our methodology prioritizes by relevance the past queries to the database that could have potentially been used to produce the sensitive table. We provide three conceptually different measures of proximity between the sensitive table and a query result. One measure is inspired by information retrieval in text processing, another is based on statistical record linkage, and the third computes the derivation probability of the sensitive table in a tree-based generative model. We also analyze the characteristics of the three measures and the corresponding ranking algorithms.

#index 960241
#* Threats to privacy in the forensic analysis of database systems
#@ Patrick Stahlberg;Gerome Miklau;Brian Neil Levine
#t 2007
#c 5
#% 13043
#% 114582
#% 232749
#% 235084
#% 338428
#% 390132
#% 528442
#% 642012
#% 750499
#% 757988
#% 798971
#% 867605
#% 874979
#% 902383
#% 917019
#% 963444
#% 963744
#% 963802
#% 963825
#% 978658
#% 978659
#% 1016171
#% 1337324
#% 1702373
#! The use of any modern computer system leaves unintended traces of expired data and remnants of users' past activities. In this paper, we investigate the unintended persistence of data stored in database systems. This data can be recovered by forensic analysis, and it poses a threat to privacy. First, we show how data remnants are preserved in database table storage, the transaction log, indexes, and other system components. Our evaluation of several real database systems reveals that deleted data is not securely removed from database storage and that users have little control over the persistence of deleted data. Second, we address the problem of unintended data retention by proposing a set of system transparency criteria: data retention should be avoided when possible, evident to users when it cannot be avoided, and bounded in time. Third, we propose specific techniques for secure record deletion and log expunction that increase the transparency of database systems, making them more resistant to forensic analysis.

#index 960242
#* Progressive and selective merge: computing top-k with ad-hoc ranking functions
#@ Dong Xin;Jiawei Han;Kevin C. Chang
#t 2007
#c 5
#% 18614
#% 152937
#% 223781
#% 248010
#% 248804
#% 273916
#% 322884
#% 333854
#% 393907
#% 397608
#% 465167
#% 727671
#% 796217
#% 806212
#% 824704
#% 875000
#% 875001
#% 893120
#% 893127
#% 893128
#! The family of threshold algorithm (ie, TA) has been widely studied for efficiently computing top-k queries. TA uses a sort-merge framework that assumes data lists are pre-sorted, and the ranking functions are monotone. However, in many database applications, attribute values are indexed by tree-structured indices (eg, B-tree, R-tree), and the ranking functions are not necessarily monotone. To answer top-k queries with ad-hoc ranking functions, this paper studies anindex-merge paradigm that performs progressive search over the space of joint states composed by multiple index nodes. We address two challenges for efficient query processing. First, to minimize the search complexity, we present a double-heap algorithm which supports not only progressive state search but also progressive state generation. Second, to avoid unnecessary disk access, we characterize a type of "empty-state" that does not contribute to the final results, and propose a new materialization model, join-signature, to prune empty-states. Our performance study shows that the proposed method achieves one order of magnitude speed-up over baseline solutions.

#index 960243
#* Spark: top-k keyword query in relational databases
#@ Yi Luo;Xuemin Lin;Wei Wang;Xiaofang Zhou
#t 2007
#c 5
#% 273910
#% 278831
#% 319273
#% 333854
#% 342708
#% 397378
#% 465167
#% 479803
#% 480819
#% 659990
#% 660011
#% 777931
#% 783474
#% 824693
#% 839172
#% 864456
#% 864459
#% 874894
#% 875017
#% 893108
#% 893126
#% 893128
#% 993987
#% 1015325
#! With the increasing amount of text data stored in relational databases, there is a demand for RDBMS to support keyword queries over text data. As a search result is often assembled from multiple relational tables, traditional IR-style ranking and query evaluation methods cannot be applied directly. In this paper, we study the effectiveness and the efficiency issues of answering top-k keyword query in relational database systems. We propose a new ranking formula by adapting existing IR techniques based on a natural notion of virtual document. Compared with previous approaches, our new ranking method is simple yet effective, and agrees with human perceptions. We also study efficient query processing methods for the new ranking method, and propose algorithms that have minimal accesses to the database. We have conducted extensive experiments on large-scale real databases using two popular RDBMSs. The experimental results demonstrate significant improvement to the alternative approaches in terms of retrieval effectiveness and efficiency.

#index 960244
#* Supporting ranking and clustering as generalized order-by and group-by
#@ Chengkai Li;Min Wang;Lipyeow Lim;Haixun Wang;Kevin Chen-Chuan Chang
#t 2007
#c 5
#% 191154
#% 210173
#% 227861
#% 227894
#% 273904
#% 280404
#% 280419
#% 296738
#% 310580
#% 316709
#% 320942
#% 333854
#% 333951
#% 462217
#% 466953
#% 479799
#% 479816
#% 479967
#% 566128
#% 765464
#% 810018
#% 866981
#% 900234
#% 1015317
#% 1504775
#% 1698996
#! The Boolean semantics of SQL queries cannot adequately capture the "fuzzy" preferences and "soft" criteria required in non-traditional data retrieval applications. One way to solve this problem is to add a flavor of "information retrieval" into database queries by allowing fuzzy query conditions and flexibly supporting grouping and ranking of the query results within the DBMS engine. While ranking is already supported by all major commercial DBMSs natively, support of flexibly grouping is still very limited (i.e., group-by). In this paper, we propose to generalize group-by to enable flexible grouping (clustering specifically) of the query results. Different from clustering in data mining applications, our focus is on supporting efficient clustering of Boolean results generated at query time. Moreover, we propose to integrate ranking and clustering with Boolean conditions, forming a new type of ClusterRank query to allow structured data retrieval. Such an integration is nontrivial in terms of both semantics and query processing. We investigate various semantics of this type of queries. To process such queries, a straightforward approach is to simply glue the techniques developed for ranking-only and clustering-only together. This approach is costly since both ranking and clustering are treated as blocking post-processing tasks upon Boolean query results by existing techniques. We propose a summary-based evaluation method that utilizes bitmap index to seamlessly integrate Boolean conditions, clustering, and ranking. Experimental study shows that our approach significantly outperforms the straightforward one and maintains high clustering quality.

#index 960245
#* Effective keyword-based selection of relational databases
#@ Bei Yu;Guoliang Li;Karen Sollins;Anthony K. H. Tung
#t 2007
#c 5
#% 194246
#% 287463
#% 333945
#% 340146
#% 340175
#% 344448
#% 413593
#% 481923
#% 567255
#% 660011
#% 722311
#% 730102
#% 764562
#% 766428
#% 778322
#% 818240
#% 824706
#% 874894
#% 874992
#% 875017
#% 993964
#% 993987
#% 1015325
#% 1016176
#! The wide popularity of free-and-easy keyword based searches over World Wide Web has fueled the demand for incorporating keyword-based search over structured databases. However, most of the current research work focuses on keyword-based searching over a single structured data source. With the growing interest in distributed databases and service oriented architecture over the Internet, it is important to extend such a capability over multiple structured data sources. One of the most important problems for enabling such a query facility is to be able to select the most useful data sources relevant to the keyword query. Traditional database summary techniques used for selecting unstructured datasources developed in IR literature are inadequate for our problem, as they do not capture the structure of the data sources. In this paper, we study the database selection problem for relational data sources, and propose a method that effectively summarizes the relationships between keywords in a relational database based on its structure. We develop effective ranking methods based on the keyword relationship summaries in order to select the most useful databases for a given keyword query. We have implemented our system on PlanetLab. In that environment we use extensive experiments with real datasets to demonstrate the effectiveness of our proposed summarization method.

#index 960246
#* FICSR: feedback-based inconsistency resolution and query processing on misaligned data sources
#@ Yan Qi;K. Selçuk Candan;Maria Luisa Sapino
#t 2007
#c 5
#% 663
#% 213981
#% 225829
#% 260021
#% 289011
#% 378409
#% 458753
#% 458822
#% 479783
#% 480134
#% 480645
#% 482089
#% 487028
#% 566373
#% 572314
#% 654468
#% 809235
#% 824763
#% 874971
#% 881824
#% 893087
#% 893167
#% 1650567
#! A critical reality in data integration is that knowledge from different sources may often be conflicting with each other. Conflict resolutioncan be costly and, if done without proper context, can be ineffective. In this paper, we propose a novel query-driven and feedback-based approach (FICSR1) to conflict resolution when integrating data sources. In particular, instead of relying on traditional model based definition of consistency, we introduce a ranked interpretation. This not only enables FICSR to deal with the complexity of the conflict resolution process, but also helps achieve a more direct match between the users' (subjective) interpretation of the data and the system's (objective) treatment of the available alternatives. Consequently, the ranked interpretation leads to new opportunities for bi-directional (data informsover ↔ user) feedback cycle for conflict resolution: given a query, (a) a preliminary ranking of candidate results on data can inform the user regarding constraints critical to the query, while (b) user feedback regarding the ranks can be exploited to inform the system about user's relevant domain knowledge. To enable this feedback process, we develop data structures and algorithms for efficient off-line conflict/agreement analysis of the integrated data as well as for on-line query processing, candidate result enumeration, and validity analysis. The results are brought together and evaluated in the FICSR system.

#index 960247
#* K-relevance: a spectrum of relevance for data sources impacting a query
#@ Jiansheng Huang;Jeffrey F. Naughton
#t 2007
#c 5
#% 13016
#% 152928
#% 201928
#% 227947
#% 318049
#% 348138
#% 427218
#% 480483
#% 480623
#% 481923
#% 632040
#% 803468
#% 893107
#% 893167
#! Applications ranging from grid management to sensor nets to web-based information integration and extraction can be viewed as receiving data from some number of autonomous remote data sources and then answering queries over this collected data. In such environments it is helpful to inform users which data sources are "relevant" to their query results. It is not immediately obvious what "relevant" should mean in this context, as different users will have different requirements. In this paper, rather than proposing a single definition of relevance, we propose a spectrum of definitions, which we term "k-relevance", for k ≥ 0. We give algorithms for identifying k-relevant data sources for relational queries and explore their efficiency both analytically and experimentally. Finally, we explore the impact of integrity constraints (including dependencies) and materialized views on the problem of computing and maintaining relevant data sources.

#index 960248
#* Cardinality estimation using sample views with quality assurance
#@ Per-Ake Larson;Wolfgang Lehner;Jingren Zhou;Peter Zabback
#t 2007
#c 5
#% 248821
#% 248822
#% 273908
#% 273909
#% 274152
#% 299989
#% 397404
#% 463285
#% 481749
#% 503719
#% 765424
#% 765455
#% 765456
#% 893137
#% 1015334
#! Accurate cardinality estimation is critically important to high-quality query optimization. It is well known that conventional cardinality estimation based on histograms or similar statistics may produce extremely poor estimates in a variety of situations, for example, queries with complex predicates, correlation among columns, or predicates containing user-defined functions. In this paper, we propose a new, general cardinality estimation technique that combines random sampling and materialized view technology to produce accurate estimates even in these situations. As a major innovation, we exploit feedback information from query execution and process control techniques to assure that estimates remain statistically valid when the underlying data changes. Experimental results based on a prototype implementation in Microsoft SQL Server demonstrate the practicality of the approach and illustrate the dramatic effects improved cardinality estimates may have.

#index 960249
#* Statistical analysis of sketch estimators
#@ Florin Rusu;Alin Dobra
#t 2007
#c 5
#% 214073
#% 273910
#% 492912
#% 723903
#% 749505
#% 765459
#% 816392
#% 824652
#% 864408
#% 874986
#% 1702986
#! Sketching techniques can provide approximate answers to aggregate queries either for data-streaming or distributed computation. Small space summaries that have linearity properties are required for both types of applications. The prevalent method for analyzing sketches uses moment analysis and distribution independent bounds based on moments. This method produces clean, easy to interpret, theoretical bounds that are especially useful for deriving asymptotic results. However, the theoretical bounds obscure fine details of the behavior of various sketches and they are mostly not indicative of which type of sketches should be used in practice. Moreover, no significant empirical comparison between various sketching techniques has been published, which makes the choice even harder. In this paper, we take a close look at the sketching techniques proposed in the literature from a statistical point of view with the goal of determining properties that indicate the actual behavior and producing tighter confidence bounds. Interestingly, the statistical analysis reveals that two of the techniques, Fast-AGMS and Count-Min, provide results that are in some cases orders of magnitude better than the corresponding theoretical predictions. We conduct an extensive empirical study that compares the different sketching techniques in order to corroborate the statistical analysis with the conclusions we draw from it. The study indicates the expected performance of various sketches, which is crucial if the techniques are to be used by practitioners. The overall conclusion of the study is that Fast-AGMS sketches are, for the full spectrum of problems, either the best, or close to the best, sketching technique. This makes Fast-AGMS sketches the preferred choice irrespective of the situation.

#index 960250
#* On synopses for distinct-value estimation under multiset operations
#@ Kevin Beyer;Peter J. Haas;Berthold Reinwald;Yannis Sismanis;Rainer Gemulla
#t 2007
#c 5
#% 1331
#% 2833
#% 25208
#% 69273
#% 117446
#% 190611
#% 243166
#% 245771
#% 278835
#% 299989
#% 336610
#% 397369
#% 411554
#% 480805
#% 481948
#% 519953
#% 654495
#% 723311
#% 725364
#% 788218
#% 864393
#% 1015256
#% 1729914
#! The task of estimating the number of distinct values (DVs) in a large dataset arises in a wide variety of settings in computer science and elsewhere. We provide DV estimation techniques that are designed for use within a flexible and scalable "synopsis warehouse" architecture. In this setting, incoming data is split into partitions and a synopsis is created for each partition; each synopsis can then be used to quickly estimate the number of DVs in its corresponding partition. By combining and extending a number of results in the literature, we obtain both appropriate synopses and novel DV estimators to use in conjunction with these synopses. Our synopses can be created in parallel, and can then be easily combined to yield synopses and DV estimates for arbitrary unions, intersections or differences of partitions. Our synopses can also handle deletions of individual partition elements. We use the theory of order statistics to show that our DV estimators are unbiased, and to establish moment formulas and sharp error bounds. Based on a novel limit theorem, we can exploit results due to Cohen in order to select synopsis sizes when initially designing the warehouse. Experiments and theory indicate that our synopses and estimators lead to lower computational costs and more accurate DV estimates than previous approaches.

#index 960251
#* Data currency in replicated DHTs
#@ Reza Akbarinia;Esther Pacitti;Patrick Valduriez
#t 2007
#c 5
#% 264263
#% 340175
#% 340176
#% 342374
#% 342375
#% 395145
#% 433980
#% 465063
#% 505869
#% 629098
#% 636116
#% 751065
#% 765469
#% 824689
#% 875020
#% 960186
#% 993994
#% 1568637
#! Distributed Hash Tables (DHTs) provide a scalable solution for data sharing in P2P systems. To ensure high data availability, DHTs typically rely on data replication, yet without data currency guarantees. Supporting data currency in replicated DHTs is difficult as it requires the ability to return a current replica despite peers leaving the network or concurrent updates. In this paper, we give a complete solution to this problem. We propose an Update Management Service (UMS) to deal with data availability and efficient retrieval of current replicas based on timestamping. For generating timestamps, we propose a Key-based Timestamping Service (KTS) which performs distributed timestamp generation using local counters. Through probabilistic analysis, we compute the expected number of replicas which UMS must retrieve for finding a current replica. Except for the cases where the availability of current replicas is very low, the expected number of retrieved replicas is typically small, e.g. if at least 35% of available replicas are current then the expected number of retrieved replicas is less than 3. We validated our solution through implementation and experimentation over a 64-node cluster and evaluated its scalability through simulation up to 10,000 peers using SimJava. The results show the effectiveness of our solution. They also show that our algorithm used in UMS achieves major performance gains, in terms of response time and communication cost, compared with a baseline algorithm.

#index 960252
#* P-ring: an efficient and robust P2P range index structure
#@ Adina Crainiceanu;Prakash Linga;Ashwin Machanavajjhala;Johannes Gehrke;Jayavel Shanmugasundaram
#t 2007
#c 5
#% 340175
#% 340176
#% 340297
#% 342375
#% 453509
#% 505869
#% 509685
#% 674136
#% 745498
#% 770901
#% 772022
#% 810034
#% 824706
#% 839334
#% 874970
#% 960186
#% 1016166
#! Peer-to-peer systems have emerged as a robust, scalable and decentralized way to share and publish data. In this paper, we propose P-Ring, a new P2P index structure that supports both equality and range queries. P-Ring is fault-tolerant, provides logarithmic search performance even for highly skewed data distributions and efficiently supports large sets of data items per peer. We experimentally evaluate P-Ring using both simulations and a real distributed deployment on PlanetLab, and we compare its performance with Skip Graphs, Online Balancing and Chord.

#index 960253
#* Homeviews: peer-to-peer middleware for personal data sharing applications
#@ Roxana Geambasu;Magdalena Balazinska;Steven D. Gribble;Henry M. Levy
#t 2007
#c 5
#% 36102
#% 60739
#% 176601
#% 264263
#% 286476
#% 320870
#% 324037
#% 340175
#% 393907
#% 598075
#% 765447
#% 845350
#% 874876
#% 874970
#% 1015281
#% 1015329
#% 1016136
#! This paper presents HomeViews, a peer-to-peer middleware system for building personal data management applications. HomeViews provides abstractions and services for data organization and distributed data sharing. The key innovation in HomeViews is the integration of three concepts: views and queries from databases, a capability-based protection model from operating systems, and a peer-to-peer distributed architecture. Using HomeViews, applications can (1)create views to organize files into dynamic collections, (2) share these views in a protected way across the Internet through simple exchange of capabilities, and (3) transparently integrate remote views and data into a user's local organizational structures. HomeViews operates in a purely peer-to-peer fashion, without the need for account administration or centralized data and protection management inherent in typical data-sharing systems. We have prototyped HomeViews, deployed it on a small network of Linux machines, and used it to develop two distributed data-sharing applications: a peer-to-peer version of the Gallery photo-sharing application and a simple read-only shared file system. Using measurements, we demonstrate the practicality and performance of our approach.

#index 960254
#* Fast data stream algorithms using associative memories
#@ Nagender Bandi;Ahmed Metwally;Divyakant Agrawal;Amr El Abbadi
#t 2007
#c 5
#% 248820
#% 333931
#% 452584
#% 492912
#% 548479
#% 569754
#% 576119
#% 642409
#% 654497
#% 726621
#% 779470
#% 821931
#% 850738
#% 857854
#% 968182
#% 981603
#% 1700144
#% 1713039
#! The primary goal of data stream research is to develop space and time efficient solutions for answering continuous on-line summarization queries. Research efforts over the last decade have resulted in a number of efficient algorithms with varying degrees of space and time complexities. While these techniques are developed in a standard CPU setting, many of their applications such as click-fraud detection and network-traffic summarization typically execute on special networking architectures called Network Processing Units (NPUs). These NPUs interface with special associative memories known as Ternary Content Addressable Memories (TCAMs) to provide gigabit rate forwarding at network routers. In this paper, we describe how the integrated architecture of NPU and TCAMs can be exploited towards achieving the goal of developing high-speed stream summarization solutions. We propose two TCAM-conscious solutions for the frequent elements problem in data streams and present a comprehensive evaluation of these techniques on a state-of-the-art networking platform.

#index 960255
#* Effective variation management for pseudo periodical streams
#@ Lv-an Tang;Bin Cui;Hongyan Li;Gaoshan Miao;Dongqing Yang;Xinbiao Zhou
#t 2007
#c 5
#% 379444
#% 379445
#% 427022
#% 428155
#% 466506
#% 576113
#% 577275
#% 578390
#% 654463
#% 654489
#% 726621
#% 729932
#% 729943
#% 729980
#% 765403
#% 765494
#% 777933
#% 810065
#% 824709
#% 824929
#% 844310
#% 875024
#% 993959
#% 1015262
#% 1016200
#% 1673620
#! Many database applications require the analysis and processing of data streams. In such systems, huge amounts of data arrive rapidly and their values change over time. The variations on streams typically imply some fundamental changes of the underlying objects and possess significant domain meanings. In some data streams, successive events seem to recur in a certain time interval, but the data indeed evolves with tiny differences as time elapses. This feature is called pseudo periodicity, which poses a non-trivial challenge to variation management in data streams. This paper presents our research effort in online variation management over such streams, and the idea can be applied to the problem domain of medical applications, such as patient vital signal monitoring. We propose a new method named Pattern Growth Graph (PGG) to detect and manage variations over pseudo periodical streams. PGG adopts the wave-pattern to capture the major information of data evolution and represent them compactly. With the help of wave-pattern matching algorithm, PGG detects the stream variations in a single pass over the stream data. PGG only stores the different segments of the pattern for incoming stream, and hence it can substantially compress the data without losing important information. The statistical information of PGG helps to distinguish meaningful data changes from noise and to reconstruct the stream with acceptable accuracy. Extensive experiments on real datasets containing millions of data items demonstrate the feasibility and effectiveness of the proposed scheme.

#index 960256
#* Efficient algorithms for evaluating xpath over streams
#@ Gang Gou;Rada Chirkova
#t 2007
#c 5
#% 397375
#% 576107
#% 576108
#% 654476
#% 659995
#% 659999
#% 731408
#% 791182
#% 801685
#% 803121
#% 809253
#% 814651
#% 864465
#% 993939
#% 993950
#% 1015338
#% 1016148
#! In this paper we address the problem of evaluating XPath queries over streaming XML data. We consider a practical XPath fragment called Univariate XPath, which includes the commonly used '/' and '//' axes and allows *-node tests and arbitrarily nested predicates. It is well known that this XPath fragment can be efficiently evaluated in O(|D||Q|) time in the non-streaming environment, where |D| is the document size and |Q| is the query size. However, this is not necessarily true in the streaming environment, since streaming algorithms have to satisfy stricter requirement than non-streaming algorithms, in that all data must be read sequentially in one pass. Therefore, it is not surprising that state-of-the-art stream-querying algorithms have higher time complexity than O(|D||Q|). In this paper we revisit the XPath stream-querying problem, and show that Univariate XPath can be efficiently evaluated in O|D||Q|) time in the streaming environment. Specifically, we propose two O(|D||Q|)-time stream-querying algorithms, LQ and EQ, which are based on the lazy strategy and on the eager strategy, respectively. To the best of our knowledge, LQ and EQ are the first XPath stream-querying algorithms that achieve O(|D||Q|) time performance. Further, our algorithms achieve O(|D||Q|) time performance without trading off space performance. Instead, they have better buffering-space performance than state-of-the-art stream-querying algorithms. In particular, EQ achieves optimal buffering-space performance. Our experimental results show that our algorithms have not only good theoretical complexity but also considerable practical performance advantages over existing algorithms.

#index 960257
#* Sketching probabilistic data streams
#@ Graham Cormode;Minos Garofalakis
#t 2007
#c 5
#% 2833
#% 214073
#% 273682
#% 333931
#% 378388
#% 397354
#% 397426
#% 480628
#% 519953
#% 593957
#% 783740
#% 816392
#% 847112
#% 864394
#% 873104
#% 874903
#% 893167
#% 977008
#% 991156
#% 993960
#% 1016201
#% 1700144
#! The management of uncertain, probabilistic data has recently emerged as a useful paradigm for dealing with the inherent unreliabilities of several real-world application domains, including data cleaning, information integration, and pervasive, multi-sensor computing. Unlike conventional data sets, a set of probabilistic tuples defines a probability distribution over an exponential number of possible worlds (i.e., "grounded", deterministic databases). This "possibleworlds" interpretation allows for clean query semantics but also raises hard computational problems for probabilistic database query processors. To further complicate matters, in many scenarios (e.g., large-scale process and environmental monitoring using multiple sensor modalities), probabilistic data tuples arrive and need to be processed in a streaming fashion; that is, using limited memory and CPU resources and without the benefit of multiple passes over a static probabilistic database. Such probabilistic data streams raise a host of new research challenges for stream-processing engines that, to date, remain largely unaddressed. In this paper, we propose the first space- and time-efficient algorithms for approximating complex aggregate queries (including, the number of distinct values and join/self-join sizes) over probabilistic data streams. Following the possible-worlds semantics, such aggregates essentially define probability distributions over the space of possible aggregation results, and our goal is to characterize such distributions through efficient approximations of their key moments (such as expectation and variance). Our algorithms offer strong randomized estimation guarantees while using only sublinear space in the size of the stream(s), and rely on novel, concise streaming sketch synopses that extend conventional sketching ideas to the probabilistic streams setting. Our experimental results verify the effectiveness of our approach.

#index 960258
#* Querying xml with update syntax
#@ Wenfei Fan;Gao Cong;Philip Bohannon
#t 2007
#c 5
#% 95618
#% 227896
#% 384978
#% 411759
#% 570875
#% 654476
#% 731408
#% 765450
#% 875007
#% 893106
#% 993939
#% 994015
#% 1015275
#% 1673665
#% 1728673
#! This paper investigates a class of transform queries proposed by XQuery Update [6]. A transform query is defined in terms of XML update syntax. When posed on an XML tree T, it returns another XML tree that would be produced by executing its embedded update on T, without destructive impact on T. Transform queries support a variety of applications including XML hypothetical queries, the simulation of updates on virtual views, and the enforcement of XML access control. In light of the wide-range of applications for transform queries, we develop automaton-based techniques for efficiently evaluating transform queries and for computing their compositions with user queries in standard XQuery. We provide (a)three algorithms to implement transform queries without change to existing XQuery processors,(b) a linear-time algorithm, based on a seamless integration of automaton execution and SAX parsing, to evaluate transform queries on large XML documents that are difficult to handle by existing XQuery engines, and (c) an algorithm to rewrite the composition of user queries and transform queries into a single efficient query in standard XQuery. We also present experimental results comparing the efficiency of our evaluation and composition algorithms for transform queries.

#index 960259
#* BLINKS: ranked keyword searches on graphs
#@ Hao He;Haixun Wang;Jun Yang;Philip S. Yu
#t 2007
#c 5
#% 58307
#% 202287
#% 309726
#% 333854
#% 397418
#% 479803
#% 642983
#% 654442
#% 765466
#% 810052
#% 810108
#% 824693
#% 824695
#% 874894
#% 875017
#% 993987
#% 1015258
#% 1015325
#% 1016135
#% 1016176
#! Query processing over graph-structured data is enjoying a growing number of applications. A top-k keyword search query on a graph finds the top k answers according to some ranking criteria, where each answer is a substructure of the graph containing all query keywords. Current techniques for supporting such queries on general graphs suffer from several drawbacks, e.g., poor worst-case performance, not taking full advantage of indexes, and high memory requirements. To address these problems, we propose BLINKS, a bi-level indexing and query processing scheme for top-k keyword search on graphs. BLINKS follows a search strategy with provable performance bounds, while additionally exploiting a bi-level index for pruning and accelerating the search. To reduce the index space, BLINKS partitions a data graph into blocks: The bi-level index stores summary information at the block level to initiate and guide search among blocks, and more detailed information for each block to accelerate search within blocks. Our experiments show that BLINKS offers orders-of-magnitude performance improvement over existing approaches.

#index 960260
#* Xpath on steroids: exploiting relational engines for xpath performance
#@ Haris Georgiadis;Vasilis Vassalos
#t 2007
#c 5
#% 479956
#% 480152
#% 570876
#% 654492
#% 654514
#% 742563
#% 745478
#% 765405
#% 783696
#% 800004
#% 800631
#% 806974
#% 810036
#% 824663
#% 824679
#% 824751
#% 837602
#% 875010
#% 884648
#% 993953
#% 994015
#% 1015298
#% 1016223
#% 1688278
#! A lot of research has been conducted by the database community on methods and techniques for efficient XPath processing, with great success. Despite the progress made, significant opportunities for optimization of XPath still exist. One key to further improvements is to utilize more effectively existing facilities of relational RDBSes for the processing of XPath queries. After taking a comprehensive look at such facilities, we present techniques for XPath processing that work by identifying the best relational join algorithms, indices and file organization strategies for XPath queries. Our techniques both reduce the latency of the resulting SQL translations and guarantee their pipelined execution. We also propose a new technique for XML reconstruction from relations-mapped XML that "splits the difference" between schema-aware and schema-oblivious XML-to-relational mapping for a significant performance improvement. An extensive experimental study confirms the performance benefits of our optimization techniques and shows that a system implementing these techniques on top of a commercial RDBMS is competitive with respect to query performance with other native and relational-based state-of-the-art XPath processing systems, commercial as well as research prototypes.

#index 960261
#* Identifying meaningful return information for XML keyword search
#@ Ziyang Liu;Yi Chen
#t 2007
#c 5
#% 273922
#% 340914
#% 342678
#% 397418
#% 654442
#% 810052
#% 863389
#% 864456
#% 875017
#% 881736
#% 893115
#% 993987
#% 1015258
#% 1016135
#! Keyword search enables web users to easily access XML data without the need to learn a structured query language and to study possibly complex data schemas. Existing work has addressed the problem of selecting qualified data nodes that match keywords and connecting them in a meaningful way, in the spirit of inferring a where clause in XQuery. However, how to infer the return clause for keyword search is an open problem. To address this challenge, we present an XML keyword search engine, XSeek, to infer the semantics of the search and identify return nodes effectively. XSeek recognizes possible entities and attributes inherently represented in the data. It also distinguishes between search predicates and return specifications in the keywords. Then based on the analysis of both XML data structures and keyword match patterns, XSeek generates return nodes. Extensive experimental studies show the effectiveness of XSeek.

#index 960262
#* QAGen: generating query-aware test databases
#@ Carsten Binnig;Donald Kossmann;Eric Lo;M. Tamer Özsu
#t 2007
#c 5
#% 136740
#% 287793
#% 320812
#% 322880
#% 427025
#% 463761
#% 479656
#% 824744
#% 893212
#% 902467
#% 1016216
#% 1718227
#! Today, a common methodology for testing a database management system (DBMS) is to generate a set of test databases and then execute queries on top of them. However, for DBMS testing, it would be a big advantage if we can control the input and/or the output (e.g., the cardinality) of each individual operator of a test query for a particular test case. Unfortunately, current database generators generate databases independent of queries. As a result, it is hard to guarantee that executing the test query on the generated test databases can obtain the desired (intermediate) query results that match the test case. In this paper, we propose a novel way for DBMS testing. Instead of first generating a test database and then seeing how well it matches a particular test case (or otherwise use a trial-and-error approach to generate another test database), we propose to generate a query-aware database for each test case. To that end, we designed a query-aware test database generator called QAGen. In addition to the database schema and the set of basic constraints defined on the base tables, QAGen takes the query and the set of constraints defined on the query as input, and generates a query-aware test database as output. The generated database guarantees that the test query can get the desired (intermediate) query results as defined in the test case. This approach of testing facilitates a wide range of DBMS testing tasks such as testing of memory managers and testing the cardinality estimation components of query optimizers.

#index 960263
#* Benchmarking declarative approximate selection predicates
#@ Amit Chandel;Oktie Hassanzadeh;Nick Koudas;Mohammad Sadoghi;Divesh Srivastava
#t 2007
#c 5
#% 235941
#% 248801
#% 262096
#% 279755
#% 280850
#% 311808
#% 406493
#% 420072
#% 480496
#% 480654
#% 577309
#% 654467
#% 765463
#% 864392
#% 875066
#% 893164
#% 993980
#% 1016219
#! Declarative data quality has been an active research topic. The fundamental principle behind a declarative approach to data quality is the use of declarative statements to realize data quality primitives on top of any relational data source. A primary advantage of such an approach is the ease of use and integration with existing applications. Over the last few years several similarity predicates have been proposed for common quality primitives (approximate selections, joins, etc) and have been fully expressed using declarative SQL statements. In this paper we propose new similarity predicates along with their declarative realization, based on notions of probabilistic information retrieval. In particular we show how language models and hidden Markov models can be utilized as similarity predicates for data quality and present their full declarative instantiation. We also show how other scoring methods from information retrieval, can be utilized in a similar setting. We then present full declarative specifications of previously proposed similarity predicates in the literature, grouping them into classes according to their primary characteristics. Finally, we present a thorough performance and accuracy study comparing a large number of similarity predicates for data cleaning operations. We quantify both their runtime performance as well as their accuracy for several types of common quality problems encountered in operational databases.

#index 960264
#* JouleSort: a balanced energy-efficiency benchmark
#@ Suzanne Rivoire;Mehul A. Shah;Parthasarathy Ranganathan;Christos Kozyrakis
#t 2007
#c 5
#% 194455
#% 340220
#% 341471
#% 411304
#% 435159
#% 657611
#% 828604
#% 834884
#% 843744
#% 863278
#% 870120
#% 874997
#% 963669
#! The energy efficiency of computer systems is an important concern in a variety of contexts. In data centers, reducing energy use improves operating cost, scalability, reliability, and other factors. For mobile devices, energy consumption directly affects functionality and usability. We propose and motivate JouleSort, an external sort benchmark, for evaluating the energy efficiency of a wide range of computer systems from clusters to handhelds. We list the criteria, challenges, and pitfalls from our experience in creating a fair energy-efficiency benchmark. Using a commercial sort, we demonstrate a JouleSort system that is over 3.5x as energy-efficient as last year's estimated winner. This system is quite different from those currently used in data centers. It consists of a commodity mobile CPU and 13 laptop drives connected by server-style I/O interfaces.

#index 960265
#* Storage workload estimation for database management systems
#@ Oguzhan Ozmen;Kenneth Salem;Mustafa Uysal;M. Hossein Sheikh Attar
#t 2007
#c 5
#% 248815
#% 397397
#% 452359
#% 459943
#% 459950
#% 481450
#% 497549
#% 657560
#% 663027
#% 783936
#% 810111
#% 820356
#% 822516
#% 835864
#% 837175
#% 850308
#! Modern storage systems are sophisticated. Simple direct-attached storage devices are giving way to storage systems that are shared, flexible, virtualized and network-attached. Today, storage systems have their own administrators, who use specialized tools and expertise to configure and manage storage resources. Although the separation of storage management and database management has many advantages, it also introduces problems. Database physical design and storage configuration are closely related tasks, and the separation makes it more difficult to achieve a good end-to-end design. In this paper, we attempt to close this gap by addressing the problem of predicting the storage workload that will be generated by a database management system. Specifically, we show how to translate a database workload description, together with a database physical design, into a characterization of the storage workload that will result. Such a characterization can be used by a storage administrator to guide storage configuration. The ultimate goal of this work is to enable effective end-to-end design and configuration spanning both the database and storage system tiers. We present an empirical assessment of the cost of workload prediction as well as the accuracy of the result.

#index 960266
#* How to barter bits for chronons: compression and bandwidth trade offs for database scans
#@ Allison L. Holloway;Vijayshankar Raman;Garret Swart;David J. DeWitt
#t 2007
#c 5
#% 193923
#% 287664
#% 322412
#% 451546
#% 464843
#% 571082
#% 824697
#% 864446
#% 875026
#% 893129
#% 893159
#% 1015332
#% 1016235
#! Two trends are converging to make the CPU cost of a table scan a more important component of database performance. First, table scans are becoming a larger fraction of the query processing workload, and second, large memories and compression are making table scans CPU, rather than disk bandwidth, bound. Data warehouse systems have found that they can avoid the unpredictability of joins and indexing and achieve good performance by using massive parallel processing to perform scans over compressed vertical partitions of a denormalized schema. In this paper we present a study of how to make such scans faster by the use of a scan code generator that produces code tuned to the database schema, the compression dictionaries, the queries being evaluated and the target CPU architecture. We investigate a variety of compression formats and propose two novel optimizations: tuple length quantization and a field length lookup table, for efficiently processing variable length fields and tuples. We present a detailed experimental study of the performance of generated scans against these compression formats, and use this to explore the trade off between compression quality and scan speed. We also introduce new strategies for removing instruction-level dependencies and increasing instruction-level parallelism, allowing for greater exploitation of multi-issue processors.

#index 960267
#* Intensional associations between data and metadata
#@ Divesh Srivastava;Yannis Velegrakis
#t 2007
#c 5
#% 22947
#% 90639
#% 183913
#% 184695
#% 229827
#% 246014
#% 248031
#% 331766
#% 342957
#% 378401
#% 462212
#% 480614
#% 536334
#% 644182
#% 745520
#% 800499
#% 803468
#% 814652
#% 864469
#% 910590
#% 1015303
#% 1016204
#% 1016212
#% 1711114
#! There is a growing need to associate a variety of metadata with the underlying data, but a simple, elegant approach to uniformly model and query both the data and the metadata has been elusive. In this paper, we argue that (1) the relational model augmented with queries as data values is a natural way to uniformly model data, arbitrary metadata and their associations, and (2) relational queries with a join mechanism augmented to permit matching of query result relations, instead of only atomic values, is an elegant way to uniformly query across data and metadata. We describe the architecture of a system we have prototyped for this purpose, demonstrate the generality of our approach and evaluate the performance of the system, in comparison with previous proposals for metadata management.

#index 960268
#* Updating a cracked database
#@ Stratos Idreos;Martin L. Kersten;Stefan Manegold
#t 2007
#c 5
#% 64791
#% 287672
#% 397398
#% 463917
#% 480153
#% 571056
#% 810111
#% 824697
#% 874878
#% 1016220
#! A cracked database is a datastore continuously reorganized based on operations being executed. For each query, the data of interest is physically reclustered to speed-up future access to the same, overlapping or even disjoint data. This way, a cracking DBMS self-organizes and adapts itself to the workload. So far, cracking has been considered for static databases only. In this paper, we introduce several novel algorithms for high-volume insertions, deletions and updates against a cracked database. We show that the nice performance properties of a cracked database can be maintained in a dynamic environment where updates interleave with queries. Our algorithms comply with the cracking philosophy, i.e., a table is informed on pending insertions and deletions, but only when the relevant data is needed for query processing just enough pending update actions are applied. We discuss details of our implementation in the context of an open-source DBMS and we show through a detailed experimental evaluation that our algorithms always manage to keep the cost of querying a cracked datastore with pending updates lower than the non-cracked case.

#index 960269
#* Log-based recovery for middleware servers
#@ Rui Wang;Betty Salzberg;David Lomet
#t 2007
#c 5
#% 86930
#% 114582
#% 248823
#% 399766
#% 545902
#% 617434
#% 617437
#% 631916
#% 660002
#% 683635
#% 745444
#% 1656075
#! We have developed new methods for log-based recovery for middleware servers which involve thread pooling, private in-memory states for clients, shared in-memory state and message interactions among middleware servers. Due to the observed rareness of crashes, relatively small size of shared state and infrequency of shared state read/write accesses, we are able to reduce the overhead of message logging and shared state logging while maintaining recovery independence. Checkpointing has a very small impact on ongoing activities while still reducing recovery time. Our recovery mechanism enables client private states to be recovered in parallel after a crash. On a commercial middleware server platform, we have implemented a recovery infrastructure prototype, which demonstrates the manageability of system complexity and shows promising performance results.

#index 960270
#* Leveraging aggregate constraints for deduplication
#@ Surajit Chaudhuri;Anish Das Sarma;Venkatesh Ganti;Raghav Kaushik
#t 2007
#c 5
#% 201889
#% 282435
#% 287222
#% 310516
#% 341704
#% 464890
#% 465052
#% 654467
#% 727668
#% 765548
#% 770782
#% 800590
#% 810014
#% 810019
#% 875066
#% 993980
#% 1347336
#% 1394227
#% 1663626
#! We show that aggregate constraints (as opposed to pairwise constraints) that often arise when integrating multiple sources of data, can be leveraged to enhance the quality of deduplication. However, despite its appeal, we show that the problem is challenging, both semantically and computationally. We define a restricted search space for deduplication that is intuitive in our context and we solve the problem optimally for the restricted space. Our experiments on real data show that incorporating aggregate constraints significantly enhances the accuracy of deduplication.

#index 960271
#* Leveraging data and structure in ontology integration
#@ Octavian Udrea;Lise Getoor;Renée J. Miller
#t 2007
#c 5
#% 307632
#% 348187
#% 378409
#% 458607
#% 480134
#% 572314
#% 660001
#% 735938
#% 800497
#% 810103
#% 893193
#% 993981
#% 1015326
#% 1275285
#% 1289178
#! There is a great deal of research on ontology integration which makes use of rich logical constraints to reason about the structural and logical alignment of ontologies. There is also considerable work on matching data instances from heterogeneous schema or ontologies. However, little work exploits the fact that ontologies include both data and structure. We aim to close this gap by presenting a new algorithm (ILIADS) that tightly integrates both data matching and logical reasoning to achieve better matching of ontologies. We evaluate our algorithm on a set of 30 pairs of OWL Lite ontologies with the schema and data matchings found by human reviewers. We compare against two systems - the ontology matching tool FCA-merge [28] and the schema matching tool COMA++ [1]. ILIADS shows an average improvement of 25% in quality over FCA-merge and a 11% improvement in recall over COMA++.

#index 960272
#* Compiling mappings to bridge applications and databases
#@ Sergey Melnik;Atul Adya;Philip A. Bernstein
#t 2007
#c 5
#% 43031
#% 286901
#% 288619
#% 479979
#% 480134
#% 481919
#% 489544
#% 572311
#% 790328
#% 809238
#% 809249
#% 850730
#% 864389
#% 874911
#% 874973
#% 875029
#% 893093
#% 960233
#% 960309
#% 960332
#% 1016152
#! Translating data and data access operations between applications and databases is a longstanding data management problem. We present a novel approach to this problem, in which the relationship between the application data and the persistent storage is specified using a declarative mapping, which is compiled into bidirectional views that drive the data transformation engine. Expressing the application model as a view on the database is used to answer queries, while viewing the database in terms of the application model allows us to leverage view maintenance algorithms for update translation. This approach has been implemented in a commercial product. It enables developers to interact with a relational database via a conceptual schema and an object oriented programming surface. We outline the implemented system and focus on the challenges of mapping compilation, which include rewriting queries under constraints and supporting non-relational constructs.

#index 960273
#* Extending relational query optimization to dynamic schemas for information integration in multidatabases
#@ Catharine M. Wyss;Felix I. Wyss
#t 2007
#c 5
#% 85086
#% 126335
#% 213969
#% 229827
#% 264263
#% 332166
#% 342957
#% 378409
#% 479968
#% 495407
#% 495417
#% 765433
#% 783471
#% 790843
#% 814652
#% 838519
#% 1016212
#% 1688252
#! This paper extends relational processing and optimization to the FISQL/FIRA languages for dynamic schema queries over multidatabases. Dynamic schema queries involve the creation and restructuring of metadata at runtime. We present a full implementation of a FISQL/FIRA engine, which includes subqueries and all transformational capabilities of FISQL/FIRA on distributed, multidatabase platforms. An important application of the system is to enhance traditional information architectures by enabling the creation and maintenance of dynamic wrappers and mapping queries at source databases within GAV, LAV, GLAV, peer-to-peer, or other integration frameworks. In addition to fully supporting FISQL/FIRA on multidatabases, our implementation introduces a bi-level optimization paradigm where purely relational sub-fragments of queries are pushed into source engines. This paradigm shares features of canonical distributed database processing, but has a new dimension through the extension of the relational model to dynamic schemas. We present empirical results showing the feasibility of optimization in this context, and discuss tradeoffs involved. Our system is the first to extend relational databases with these capabilities on this scale.

#index 960274
#* Sharing aggregate computation for distributed queries
#@ Ryan Huebsch;Minos Garofalakis;Joseph M. Hellerstein;Ion Stoica
#t 2007
#c 5
#% 74110
#% 275929
#% 397353
#% 629097
#% 654461
#% 654463
#% 654482
#% 654497
#% 801695
#% 803602
#% 810009
#% 810032
#% 824652
#% 875022
#% 1675421
#! An emerging challenge in modern distributed querying is to efficiently process multiple continuous aggregation queries simultaneously. Processing each query independently may be infeasible, so multi-query optimizations are critical for sharing work across queries. The challenge is to identify overlapping computations that may not be obvious in the queries themselves. In this paper, we reveal new opportunities for sharing work in the context of distributed aggregation queries that vary in their selection predicates. We identify settings in which a large set of q such queries can be answered by executing k . The k queries are revealed by analyzing a boolean matrix capturing the connection between data and the queries that they satisfy, in a manner akin to familiar techniques like Gaussian elimination. Indeed, we identify a class of linear aggregate functions (including SUM, COUNT and AVERAGE), and show that the sharing potential for such queries can be optimally recovered using standard matrix decompositions from computational linear algebra. For some other typical aggregation functions (including MIN and MAX) we find that optimal sharing maps to the NP-hard set basis problem. However, for those scenarios, we present a family of heuristic algorithms and demonstrate that they perform well for moderate-sized matrices. We also present a dynamic distributed system architecture to exploit sharing opportunities, and experimentally evaluate the benefits of our techniques via a novel, flexible random workload generator we develop for this setting.

#index 960275
#* Resource-adaptive real-time new event detection
#@ Gang Luo;Chunqiang Tang;Philip S. Yu
#t 2007
#c 5
#% 115661
#% 212665
#% 262042
#% 262043
#% 279755
#% 290830
#% 300176
#% 316546
#% 319876
#% 340883
#% 340916
#% 340995
#% 397133
#% 481450
#% 577297
#% 643014
#% 643016
#% 760812
#% 766444
#% 805848
#% 816196
#% 818215
#% 838536
#% 838537
#% 875006
#% 893153
#% 963596
#! In a document streaming environment, online detection of the first documents that mention previously unseen events is an open challenge. For this online new event detection (ONED) task, existing studies usually assume that enough resources are always available and focus entirely on detection accuracy without considering efficiency. Moreover, none of the existing work addresses the issue of providing an effective and friendly user interface. As a result, there is a significant gap between the existing systems and a system that can be used in practice. In this paper, we propose an ONED framework with the following prominent features. First, a combination of indexing and compression methods is used to improve the document processing rate by orders of magnitude without sacrificing much detection accuracy. Second, when resources are tight, a resource-adaptive computation method is used to maximize the benefit that can be gained from the limited resources. Third, when the new event arrival rate is beyond the processing capability of the consumer of the ONED system, new events are further filtered and prioritized before they are presented to the consumer. Fourth, implicit citation relationships are created among all the documents and used to compute the importance of document sources. This importance information can guide the selection of document sources. We implemented a prototype of our framework on top of IBM's Stream Processing Core middleware. We also evaluated the effectiveness of our techniques on the standard TDT5 benchmark. To the best of our knowledge, this is the first implementation of a real application in a large-scale stream processing system.

#index 960276
#* Distributed query evaluation with performance guarantees
#@ Gao Cong;Wenfei Fan;Anastasios Kementsietsidis
#t 2007
#c 5
#% 115661
#% 219211
#% 238087
#% 330305
#% 345693
#% 378412
#% 397374
#% 397375
#% 549327
#% 570880
#% 631868
#% 654485
#% 740758
#% 762652
#% 772022
#% 810070
#% 824706
#% 893106
#% 993939
#% 994015
#% 1015275
#% 1016166
#! Partial evaluation has recently proven an effective technique for evaluating Boolean XPath queries over a fragmented tree that is distributed over a number of sites. What left open is whether or not the technique is applicable to generic data-selecting XPath queries. In contrast to Boolean queries that return a single truth value, a generic XPath query returns a set of elements, and its evaluation introduces difficulties to avoiding excessive data shipping. This paper settles this question in positive by providing evaluation algorithms and optimizations for generic XPath queries in the same distributed and fragmented setting. These algorithms explore parallelism and retain the performance guarantees of their counterpart for Boolean queries, regardless of how the tree is fragmented and distributed. First, each site is visited at most three times, and down to at most twice when optimizations are in place. Second, the network traffic is determined by the final answer of the query, rather than the size of the tree, without incurring unnecessary data shipping. Third, the total computation is comparable to that of centralized algorithms on the tree stored in a single site. We show both analytically and experimentally that our algorithms and optimizations are scalable and efficient on large trees and complex XPath queries.

#index 960277
#* In-network execution of monitoring queries in sensor networks
#@ Xiaoyan Yang;Hock Beng Lim;Tamer M. Özsu;Kian Lee Tan
#t 2007
#c 5
#% 378388
#% 397353
#% 745442
#% 800502
#% 806214
#% 809256
#% 824715
#% 846057
#% 853011
#% 981636
#% 1015296
#% 1016178
#% 1716961
#! Sensor networks are widely used in many applications for collecting information from the physical environment. In these applications, it is usually necessary to track the relationships between sensor data readings within a time window to detect events of interest. However, it is difficult to detect such events by using the common aggregate or selection queries. We address the problem of processing window self-join in order to detect events of interest. Self-joins are useful in tracking correlations between different sensor readings, which can indicate an event of interest. We propose the Two-Phase Self-Join (TPSJ) scheme to efficiently evaluate self-join queries for event detection in sensor networks. Our TPSJ scheme takes advantage of the properties of the events and carries out data filtering during in-network processing. We discuss TPSJ execution with one window and we extend it for continuous event monitoring. Our experimental evaluation results indicate that the TPSJ scheme is effective in reducing the amount of radio transmissions during event detection.

#index 960278
#* Efficient exploitation of similar subexpressions for query processing
#@ Jingren Zhou;Per-Ake Larson;Johann-Christoph Freytag;Wolfgang Lehner
#t 2007
#c 5
#% 36117
#% 210208
#% 248787
#% 248811
#% 300166
#% 333962
#% 333965
#% 411750
#% 442714
#% 465007
#% 480158
#% 810022
#% 824739
#! Complex queries often contain common or similar subexpressions, either within a single query or among multiple queries submitted as a batch. If so, query execution time can be improved by evaluating a common subexpression once and reusing the result in multiple places. However, current query optimizers do not recognize and exploit similar subexpressions, even within the same query. We present an efficient, scalable, and principled solution to this long-standing optimization problem. We introduce a light-weight and effective mechanism to detect potential sharing opportunities among expressions. Candidate covering subexpressions are constructed and optimization is resumed to determine which, if any, such subexpressions to include in the final query plan. The chosen subexpression(s) are computed only once and the results are reused to answer other parts of queries. Our solution automatically applies to optimization of query batches, nested queries, and maintenance of multiple materialized views. It is the first comprehensive solution covering all aspects of the problem: detection, construction, and cost-based optimization. Experiments on Microsoft SQL Server show significant performance improvements with minimal overhead.

#index 960279
#* Query relaxation using malleable schemas
#@ Xuan Zhou;Julien Gaugaz;Wolf-Tilo Balke;Wolfgang Nejdl
#t 2007
#c 5
#% 213443
#% 290830
#% 300542
#% 321635
#% 333990
#% 340914
#% 348187
#% 397369
#% 458861
#% 572314
#% 642993
#% 654459
#% 715899
#% 765408
#% 765423
#% 769370
#% 787547
#% 800497
#% 800498
#% 893105
#% 913783
#% 1016135
#! In contrast to classical databases and IR systems, real-world information systems have to deal increasingly with very vague and diverse structures for information management and storage that cannot be adequately handled yet. While current object-relational database systems require clear and unified data schemas, IR systems usually ignore the structured information completely. Malleable schemas, as recently introduced, provide a novel way to deal with vagueness, ambiguity and diversity by incorporating imprecise and overlapping definitions of data structures. In this paper, we propose a novel query relaxation scheme that enables users to find best matching information by exploiting malleable schemas to effectively query vaguely structured information. Our scheme utilizes duplicates in differently described data sets to discover the correlations within a malleable schema, and then uses these correlations to appropriately relax the users' queries. In addition, it ranks results of the relaxed query according to their respective probability of satisfying the original query's intent. We have implemented the scheme and conducted extensive experiments with real-world data to confirm its performance and practicality.

#index 960280
#* Query suspend and resume
#@ Badrish Chandramouli;Christopher N. Bond;Shivnath Babu;Jun Yang
#t 2007
#c 5
#% 209907
#% 245996
#% 248793
#% 261139
#% 300127
#% 300167
#% 632755
#% 674194
#% 765434
#% 765456
#% 765467
#% 874999
#% 1124990
#! Suppose a long-running analytical query is executing on a database server and has been allocated a large amount of physical memory. A high-priority task comes in and we need to run it immediately with all available resources. We have several choices. We could swap out the old query to disk, but writing out a large execution state may take too much time. Another option is to terminate the old query and restart it after the new task completes, but we would waste all the work already performed by the old query. Yet another alternative is to periodically checkpoint the query during execution, but traditional synchronous checkpointing carries high overhead. In this paper, we advocate a database-centric approach to implementing query suspension and resumption, with negligible execution overhead, bounded suspension cost, and efficient resumption. The basic idea is to let each physical query operator perform lightweight checkpointing according to its own semantics, and coordinate asynchronous checkpoints among operators through a novel contracting mechanism. At the time of suspension, we find an optimized suspend plan for the query, which may involve a combination of dumping current state to disk and going back to previous checkpoints. The plan seeks to minimize the suspend/resume overhead while observing the constraint on suspension time. Our approach requires only small changes to the iterator interface, which we have implemented in the PREDATOR database system. Experiments with our implementation demonstrate significant advantages of our approach over traditional alternatives.

#index 960281
#* An efficient and accurate method for evaluating time series similarity
#@ Michael D. Morse;Jignesh M. Patel
#t 2007
#c 5
#% 65343
#% 174161
#% 227924
#% 320220
#% 333941
#% 462231
#% 477968
#% 480146
#% 534183
#% 564263
#% 577221
#% 631923
#% 654456
#% 659936
#% 659971
#% 729931
#% 769896
#% 809264
#% 810049
#% 993965
#% 1016195
#% 1815525
#! A variety of techniques currently exist for measuring the similarity between time series datasets. Of these techniques, the methods whose matching criteria is bounded by a specified ε threshold value, such as the LCSS and the EDR techniques, have been shown to be robust in the presence of noise, time shifts, and data scaling. Our work proposes a new algorithm, called the Fast Time Series Evaluation (FTSE) method, which can be used to evaluate such threshold value techniques, including LCSS and EDR. Using FTSE, we show that these techniques can be evaluated faster than using either traditional dynamic programming or even warp-restricting methods such as the Sakoe-Chiba band and the Itakura Parallelogram. We also show that FTSE can be used in a framework that can evaluate a richer range of ε threshold-based scoring techniques, of which EDR and LCSS are just two examples. This framework, called Swale, extends the ε threshold-based scoring techniques to include arbitrary match rewards and gap penalties. Through extensive empirical evaluation, we show that Swale can obtain greater accuracy than existing methods.

#index 960282
#* Adaptive location constraint processing
#@ Zhengdao Xu;Arno Jacobsen
#t 2007
#c 5
#% 114573
#% 201876
#% 273706
#% 299979
#% 300162
#% 300174
#% 321455
#% 347264
#% 442615
#% 631991
#% 654478
#% 709882
#% 800571
#% 810048
#% 814343
#% 846190
#% 996335
#% 1016252
#% 1562065
#% 1831231
#! An important problem for many location-based applications is the continuous evaluation of proximity relations among moving objects. These relations express whether a given set of objects is in a spatial constellation or in a spatial constellation relative to a given point of demarcation in the environment. We represent proximity relations as location constraints, which resemble standing queries over continuously changing location position information. The challenge lies in the continuous processing of large numbers of location constraints as the location of objects and the constraint load change. In this paper, we propose an adaptive location constraint indexing approach which adapts as the constraint load and movement pattern of the objects change. The approach takes correlations between constraints into account to further reduce processing time. We also introduce a new location update policy that detects constraint matches with fewer location update requests. Our approach stabilizes system performance, avoids oscillation, reduces constraint matching time by 70% for in-memory processing, and reduces secondary storage accesses by 80% for I/O-incurring environments.

#index 960283
#* Trajectory clustering: a partition-and-group framework
#@ Jae-Gil Lee;Jiawei Han;Kyu-Young Whang
#t 2007
#c 5
#% 210173
#% 273890
#% 280416
#% 427199
#% 566128
#% 659971
#% 732531
#% 769883
#% 769896
#% 810049
#% 818916
#% 881456
#% 993965
#! Existing trajectory clustering algorithms group similar trajectories as a whole, thus discovering common trajectories. Our key observation is that clustering trajectories as a whole could miss common sub-trajectories. Discovering common sub-trajectories is very useful in many applications, especially if we have regions of special interest for analysis. In this paper, we propose a new partition-and-group framework for clustering trajectories, which partitions a trajectory into a set of line segments, and then, groups similar line segments together into a cluster. The primary advantage of this framework is to discover common sub-trajectories from a trajectory database. Based on this partition-and-group framework, we develop a trajectory clustering algorithm TRACLUS. Our algorithm consists of two phases: partitioning and grouping. For the first phase, we present a formal trajectory partitioning algorithm using the minimum description length(MDL) principle. For the second phase, we present a density-based line-segment clustering algorithm. Experimental results demonstrate that TRACLUS correctly discovers common sub-trajectories from real trajectory data.

#index 960284
#* Keyword search on relational data streams
#@ Alexander Markowetz;Yin Yang;Dimitris Papadias
#t 2007
#c 5
#% 297191
#% 300167
#% 333938
#% 378388
#% 479782
#% 578560
#% 654442
#% 726621
#% 765497
#% 838492
#% 839172
#% 875017
#% 878299
#% 907579
#% 978475
#% 993987
#% 1015325
#% 1016176
#% 1016203
#! Increasing monitoring of transactions, environmental parameters, homeland security, RFID chips and interactions of online users rapidly establishes new data sources and application scenarios. In this paper, we propose keyword search on relational data streams (S-KWS) as an effective way for querying in such intricate and dynamic environments. Compared to conventional query methods, S-KWS has several benefits. First, it allows search for combinations of interesting terms without a-priori knowledge of the data streams in which they appear. Second, it hides the schema from the user and allows it to change, without the need for query re-writing. Finally, keyword queries are easy to express. Our contributions are summarized as follows. (i) We provide formal semantics for S-KWS, addressing the temporal validity and order of results. (ii) We propose an efficient algorithm for generating operator trees, applicable to arbitrary schemas. (iii) We integrate these trees into an operator mesh that shares common expressions. (iv) We develop techniques that utilize the operator mesh for efficient query processing. The techniques adapt dynamically to changes in the schema and input characteristics. Finally, (v) we present methods for purging expired tuples, minimizing either CPU, or memory requirements.

#index 960285
#* Towards keyword-driven analytical processing
#@ Ping Wu;Yannis Sismanis;Berthold Reinwald
#t 2007
#c 5
#% 223781
#% 309726
#% 452641
#% 459025
#% 479957
#% 654442
#% 660011
#% 857482
#% 875017
#% 893142
#% 993987
#% 1015325
#% 1016135
#% 1016203
#! Gaining business insights from data has recently been the focus of research and product development. On Line-Analytical Processing (OLAP) tools provide elaborate query languages that allow users to group and aggregate data in various ways, and explore interesting trends and patterns in the data. However, the dynamic nature of today's data along with the overwhelming detail at which data is provided, make it nearly impossible to organize the data in a way that a business analyst needs for thinking about the data. In this paper, we introduce "Keyword-Driven Analytical Processing" (KDAP), which combines intuitive keyword-based search with the power of aggregation in OLAP without having to spend considerable effort in organizing the data in terms that the business analyst understands. Our design point is around a user mentality that we frequently encounter: "users don't know how to specify what they want, but they know it when they see it". We present our complete solution framework, which implements various phases from disambiguating the keyword terms to organizing and ranking the results in dynamic facets, that allow the user to explore efficiently the aggregation space. We address specific issues that analysts encounter, like joins, groupings and aggregations, and we provide efficient and scalable solutions. We show, how KDAP can handle both categorical and numerical data equally well and, finally, we demonstrate the generality and applicability of KDAP to two different aspects of OLAP, namely, finding exceptions or surprises in the data and finding bellwether regions where local aggregates are highly correlated with global aggregates, using various experiments on real data.

#index 960286
#* A random walk approach to sampling hidden databases
#@ Arjun Dasgupta;Gautam Das;Heikki Mannila
#t 2007
#c 5
#% 1331
#% 268114
#% 273926
#% 333932
#% 427219
#% 480479
#% 480810
#% 765424
#% 765425
#% 869499
#! A large part of the data on the World Wide Web is hidden behind form-like interfaces. These interfaces interact with a hidden back-end database to provide answers to user queries. Generating a uniform random sample of this hidden database by using only the publicly available interface gives us access to the underlying data distribution. In this paper, we propose a random walk scheme over the query space provided by the interface to sample such databases. We discuss variants where the query space is visualized as a fixed and random ordering of attributes. We also propose techniques to further improve the sample quality by using a probabilistic rejection based approach. We conduct extensive experiments to illustrate the accuracy and efficiency of our techniques.

#index 960287
#* Addressing diverse user preferences in SQL-query-result navigation
#@ Zhiyuan Chen;Tao Li
#t 2007
#c 5
#% 160852
#% 172811
#% 234401
#% 270634
#% 273900
#% 311808
#% 330705
#% 342621
#% 376266
#% 420053
#% 443509
#% 449588
#% 458379
#% 465747
#% 577224
#% 577230
#% 754126
#% 765464
#% 766433
#% 818207
#% 875001
#% 875002
#% 875003
#% 1016203
#! Database queries are often exploratory and users often find their queries return too many answers, many of them irrelevant. Existing work either categorizes or ranks the results to help users locate interesting results. The success of both approaches depends on the utilization of user preferences. However, most existing work assumes that all users have the same user preferences, but in real life different users often have different preferences. This paper proposes a two-step solution to address the diversity issue of user preferences for the categorization approach. The proposed solution does not require explicit user involvement. The first step analyzes query history of all users in the system offline and generates a set of clusters over the data, each corresponding to one type of user preferences. When user asks a query, the second step presents to the user a navigational tree over clusters generated in the first step such that the user can easily select the subset of clusters matching his needs. The user then can browse, rank, or categorize the results in selected clusters. The navigational tree is automatically constructed using a cost-based algorithm which considers the cost of visiting both intermediate nodes and leaf nodes in the tree. An empirical study demonstrates the benefits of our approach.

#index 960288
#* Privacy preserving schema and data matching
#@ Monica Scannapieco;Ilya Figotin;Elisa Bertino;Ahmed K. Elmagarmid
#t 2007
#c 5
#% 214257
#% 252304
#% 271185
#% 285926
#% 572314
#% 575969
#% 578407
#% 587758
#% 654448
#% 763581
#% 819551
#% 824725
#% 864413
#% 864414
#% 875066
#% 913783
#% 1688260
#! In many business scenarios, record matching is performed across different data sources with the aim of identifying common information shared among these sources. However such need is often in contrast with privacy requirements concerning the data stored by the sources. In this paper, we propose a protocol for record matching that preserves privacy both at the data level and at the schema level. Specifically, if two sources need to identify their common data, by running the protocol they can compute the matching of their datasets without sharing their data in clear and only sharing the result of the matching. The protocol uses a third party, and maps records into a vector space in order to preserve their privacy. Experimental results show the efficiency of the matching protocol in terms of precision and recall as well as the good computational performance.

#index 960289
#* Hiding the presence of individuals from shared databases
#@ Mehmet Ercan Nergiz;Maurizio Atzori;Chris Clifton
#t 2007
#c 5
#% 443463
#% 576761
#% 577239
#% 800515
#% 810011
#% 824726
#% 864406
#% 864412
#% 864665
#% 874892
#% 893100
#% 1655465
#! Advances in information technology, and its use in research, are increasing both the need for anonymized data and the risks of poor anonymization. We present a metric, δ-presence, that clearly links the quality of anonymization to the risk posed by inadequate anonymization. We show that existing anonymization techniques are inappropriate for situations where δ-presence is a good metric (specifically, where knowing an individual is in the database poses a privacy risk), and present algorithms for effectively anonymizing to meet δ-presence. The algorithms are evaluated in the context of a real-world scenario, demonstrating practical applicability of the approach.

#index 960290
#* GhostDB: querying visible and hidden data without leaks
#@ Nicolas Anciaux;Mehdi Benzine;Luc Bouganim;Philippe Pucheral;Dennis Shasha
#t 2007
#c 5
#% 18614
#% 191154
#% 322884
#% 340933
#% 397367
#% 397395
#% 479753
#% 514165
#% 571059
#% 571094
#% 572304
#% 576761
#% 641769
#% 725292
#% 864412
#% 993943
#% 994006
#! Imagine that you have been entrusted with private data, such as corporate product information, sensitive government information, or symptom and treatment information about hospital patients. You may want to issue queries whose result will combine private and public data, but private data must not be revealed. GhostDB is an architecture and system to achieve this. You carry private data in a smart USB key (a large Flash persistent store combined with a tamper and snoop-resistant CPU and small RAM). When the key is plugged in, you can issue queries that link private and public data and be sure that the only information revealed to a potential spy is which queries you pose. Queries linking public and private data entail novel distributed processing techniques on extremely unequal devices (standard computer and smart USB key). This paper presents the basic framework to make this all work intuitively and efficiently.

#index 960291
#* M-invariance: towards privacy preserving re-publication of dynamic datasets
#@ Xiaokui Xiao;Yufei Tao
#t 2007
#c 5
#% 216970
#% 248030
#% 443463
#% 576762
#% 577239
#% 800514
#% 800515
#% 801690
#% 810011
#% 824726
#% 864406
#% 864412
#% 874892
#% 874988
#% 874989
#% 881483
#% 881497
#% 893100
#% 1700134
#% 1725659
#! The previous literature of privacy preserving data publication has focused on performing "one-time" releases. Specifically, none of the existing solutions supports re-publication of the microdata, after it has been updated with insertions and deletions. This is a serious drawback, because currently a publisher cannot provide researchers with the most recent dataset continuously. This paper remedies the drawback. First, we reveal the characteristics of the re-publication problem that invalidate the conventional approaches leveraging k-anonymity and l-diversity. Based on rigorous theoretical analysis, we develop a new generalization principle m-invariance that effectively limits the risk of privacy disclosure in re-publication. We accompany the principle with an algorithm, which computes privacy-guarded relations that permit retrieval of accurate aggregate information about the original microdata. Our theoretical results are confirmed by extensive experiments with real data.

#index 960292
#* Optimizing mpf queries: decision support and probabilistic inference
#@ Héctor Corrada Bravo;Raghu Ramakrishnan
#t 2007
#c 5
#% 215225
#% 351595
#% 388024
#% 400439
#% 411554
#% 458550
#% 481288
#% 496116
#% 824718
#% 824733
#% 857454
#% 1016201
#% 1269496
#% 1272302
#% 1389723
#% 1650691
#% 1650782
#% 1809993
#% 1810385
#! Managing uncertain data using probabilistic frameworks has attracted much interest lately in the database literature, and a central computational challenge is probabilistic inference. This paper presents a broad class of aggregate queries, called MPF queries, inspired by the literature on probabilistic inference in statistics and machine learning. An MPF (Marginalize a Product Function) query is an aggregate query over a stylized join of several relations. In probabilistic inference, this join corresponds to taking the product of several probability distributions, while the aggregate operation corresponds to marginalization. Probabilistic inference can be expressed directly as MPF queries in a relational setting, and therefore, by optimizing evaluation of MPF queries, we provide scalable support for probabilistic inference in database systems. To optimize MPF queries, we build on ideas from database query optimization as well as traditional algorithms such as Variable Elimination and Belief Propagation from the probabilistic inference literature. Although our main motivation for introducing MPF queries is to support easy expression and efficient evaluation of probabilistic inference in a DBMS, we observe that this class of queries is very useful for a range of decision support tasks. We present and optimize MPF queries in a general form where arbitrary functions (i.e., other than probability distributions) are handled, and demonstrate their value for decision support applications through a number of illustrative and natural examples.

#index 960293
#* From complete to incomplete information and back
#@ Lyublena Antova;Christoph Koch;Dan Olteanu
#t 2007
#c 5
#% 663
#% 94459
#% 102787
#% 114580
#% 137865
#% 227896
#% 273687
#% 328431
#% 366807
#% 384978
#% 479754
#% 752741
#% 810106
#% 826032
#% 864409
#% 864417
#% 893167
#% 1016201
#% 1661439
#% 1728680
#! Incomplete information arises naturally in numerous data management applications. Recently, several researchers have studied query processing in the context of incomplete information. Most work has combined the syntax of a traditional query language like relational algebra with a nonstandard semantics such as certain or ranked possible answers. There are now also languages with special features to deal with uncertainty. However, to the standards of the data management community, to date no language proposal has been made that can be considered a natural analog to SQL or relational algebra for the case of incomplete information. In this paper we propose such a language, World-set Algebra, which satisfies the robustness criteria and analogies to relational algebra that we expect. The language supports the contemplation on alternatives and can thus map from a complete database to an incomplete one comprising several possible worlds. We show that World-set Algebra is conservative over relational algebra in the sense that any query that maps from a complete database to a complete database (a complete-to-complete query) is equivalent to a relational algebra query. Moreover, we give an efficient algorithm for effecting this translation. We then study algebraic query optimization of such queries. We argue that query languages with explicit constructs for handling uncertainty allow for the more natural and simple expression of many real-world decision support queries. The results of this paper not only suggest a language for specifying queries in this way, but also allow for their efficient evaluation in any relational database management system.

#index 960294
#* Scalable approximate query processing with the DBO engine
#@ Christopher Jermaine;Subramanian Arumugam;Abhijit Pol;Alin Dobra
#t 2007
#c 5
#% 3771
#% 77967
#% 210353
#% 227883
#% 273908
#% 273909
#% 273910
#% 397370
#% 438135
#% 463260
#% 503719
#% 576104
#% 810055
#% 824713
#% 993956
#! This paper describes query processing in the DBO database system. Like other database systems designed for ad-hoc, analytic processing, DBO is able to compute the exact answer to queries over a large relational database in a scalable fashion. Unlike any other system designed for analytic processing, DBO can constantly maintain a guess as to the final answer to an aggregate query throughout execution, along with statistically meaningful bounds for the guess's accuracy. As DBO gathers more and more information, the guess gets more and more accurate, until it is 100% accurate as the query is completed. This allows users to stop the execution at any time that they are happy with the query accuracy, and encourages exploratory data analysis.

#index 960295
#* Efficient xml data dissemination with piggybacking
#@ Chee Yong Chan;Yuan Ni
#t 2007
#c 5
#% 241207
#% 271199
#% 338354
#% 465061
#% 480296
#% 570879
#% 576098
#% 612477
#% 654476
#% 731408
#% 800592
#% 800593
#% 824669
#% 864440
#% 994000
#! Content-based dissemination of XML data using the publish-subscribe paradigm is an effective means to deliver relevant data to interested data consumers. To meet the performance challenges of content-based filtering and routing, two key optimizations have been developed: the use of efficient indexes to speed up subscription filtering, and the use of effective aggregation algorithms to reduce the number of subscriptions. The effectiveness of both these techniques are, however, limited to locally improving the performance of individual routers. In this paper, we propose a novel and holistic optimization approach that allows a downstream router to leverage the subscription matchings done by upstream routers to reduce its own filtering work. This is achieved by piggybacking useful annotations to the XML document being forwarded. We explore several design options and tradeoffs of this novel optimization approach. Our experimental results demonstrate that our piggyback optimization achieves significant performance improvement under various conditions.

#index 960296
#* Boosting topic-based publish-subscribe systems with dynamic clustering
#@ Tova Milo;Tal Zur;Elad Verbin
#t 2007
#c 5
#% 336297
#% 338354
#% 340175
#% 342032
#% 443984
#% 555272
#% 635992
#% 643178
#% 659987
#% 674136
#% 812781
#% 835322
#% 875019
#% 879405
#% 978763
#% 1016180
#% 1671641
#% 1706222
#% 1849768
#! We consider in this paper a class of Publish-Subscribe (pub-sub) systems called topic-based systems, where users subscribe to topics and are notified on events that belong to those subscribed topics. With the recent flourishing of RSS news syndication, these systems are regaining popularity and are raising new challenging problems. In most of the modern topics-based systems, the events in each topic are delivered to the subscribers via a supporting, distributed, data structure (typically a multicast tree). Since peers in the network may come and go frequently, this supporting structure must be continuously maintained so that "holes" do not disrupt the events delivery. The dissemination of events in each topic thus incurs two main costs: (1) the actual transmission cost for the topic events,and (2) the maintenance cost for its supporting structure. This maintenance overhead becomes particularly dominating when a pub-sub system supports a large number of topics with moderate event frequency; a typical scenario in nowadays news syndication scene. The goal of this paper is to devise a method for reducing this maintenance overhead to the minimum. Our aim is not to invent yet another topic-based pub-sub system, but rather to develop a generic technique for better utilization of existing platforms. Our solution is based on a novel distributed clustering algorithm that utilizes correlations between user subscriptions to dynamically group topics together, into virtual topics (called topic-clusters), andt hereby unifies their supporting structures and reduces costs. Our technique continuously adapts the topic-clusters and the user subscriptions to the system state, and incurs only very minimal overhead. We have implemented our solution in the Tamara pub-sub system. Our experimental study shows this approach to be extremely effective, improving the performance by an order of magnitude.

#index 960297
#* Massively multi-query join processing in publish/subscribe systems
#@ Mingsheng Hong;Alan J. Demers;Johannes E. Gehrke;Christoph Koch;Mirek Riedewald;Walker M. White
#t 2007
#c 5
#% 271199
#% 333938
#% 384978
#% 480296
#% 481448
#% 731408
#% 791182
#% 800592
#% 801694
#% 814651
#% 824673
#% 864465
#% 875004
#% 993949
#% 993950
#% 1015338
#% 1016148
#% 1688281
#! There has been much recent interest in XML publish/subscribe systems. Some systems scale to thousands of concurrent queries, but support a limited query language (usually a fragment of XPath 1.0). Other systems support more expressive languages, but do not scale well with the number of concurrent queries. In this paper, we propose a set of novel query processing techniques, referred to as Massively Multi-Query Join Processing techniques, for processing a large number of XML stream queries involving value joins over multiple XML streams and documents. These techniques enable the sharing of representations of inputs to multiple joins, and the sharing of join computation. Our techniques are also applicable to relational event processing systems and publish/subscribe systems that support join queries. We present experimental results to demonstrate the effectiveness of our techniques. We are able to process thousands of XML messages with hundreds of thousands of join queries on real RSS feed streams. Our techniques gain more than two orders of magnitude speedup compared to the naive approach of evaluating such join queries.

#index 960298
#* Lazy, adaptive rid-list intersection, and its application to index anding
#@ Vijayshankar Raman;Lin Qiao;Wei Han;Inderpal Narang;Ying-Lin Chen;Kou-Horng Yang;Fen-Ling Ling
#t 2007
#c 5
#% 300167
#% 303072
#% 379411
#% 397375
#% 480803
#% 677322
#% 765435
#% 821939
#% 881734
#% 1667821
#! RID-List (row id list) intersection is a common strategy in query processing, used in star joins, column stores, and even search engines. To apply a conjunction of predicates on a table, a query process ordoes index lookups to form sorted RID-lists (or bitmap) of the rows matching each predicate, then intersects the RID-lists via an AND-tree, and finally fetches the corresponding rows to apply any residual predicates and aggregates. This process can be expensive when the RID-lists are large. Furthermore, the performance is sensitive to the order in which RID lists are intersected together, and to treating the right predicates as residuals. If the optimizer chooses a wrong order or a wrong residual, due to a poor cardinality estimate, the resulting plan can run orders of magnitude slower than expected. We present a new algorithm for RID-list intersection that is both more efficient and more robust than this standard algorithm. First, we avoid forming the RID-lists up front, and instead form this lazily as part of the intersection. This reduces the associated IO and sort cost significantly, especially when the data distribution is skewed. It also ameliorates the problem of wrong residual table selection. Second, we do not intersect the RID-lists via an AND-tree, because this is vulnerable to cardinality mis-estimations. Instead, we use an adaptive set intersection algorithm that performs well even when the cardinality estimates are wrong. We present detailed experiments of this algorithm on data with varying distributions to validate its efficiency and predictability.

#index 960299
#* Optimal top-down join enumeration
#@ David DeHaan;Frank Wm. Tompa
#t 2007
#c 5
#% 116043
#% 210166
#% 230543
#% 289265
#% 393907
#% 408638
#% 411554
#% 461895
#% 464056
#% 481930
#% 482115
#% 495283
#% 565457
#% 571062
#% 632031
#% 651607
#% 893165
#% 893173
#! Most contemporary database systems perform cost-based join enumeration using some variant of System-R's bottom-up dynamic programming method. The notable exceptions are systems based on the top-down transformational search of Volcano/Cascades. As recent work has demonstrated, bottom-up dynamic programming can attain optimality with respect to the shape of the join graph; no comparable results have been published for transformational search. However, transformational systems leverage benefits of top-down search not available to bottom-up methods. In this paper we describe a top-down join enumeration algorithm that is optimal with respect to the join graph. We present performance results demonstrating that a combination of optimal enumeration with search strategies such as branch-and-bound yields an algorithm significantly faster than those previously described in the literature. Although our algorithm enumerates the search space top-down, it does not rely on transformations and thus retains much of the architecture of traditional dynamic programming. As such, this work provides a migration path for existing bottom-up optimizers to exploit top-down search without drastically changing to the transformational paradigm.

#index 960300
#* Optimization of multi-version expensive predicates
#@ Iosif Lazaridis;Sharad Mehrotra
#t 2007
#c 5
#% 249985
#% 287461
#% 745503
#% 765435
#% 824714
#% 893154
#! Modern query optimizers need to take into account the performance of expensive user-defined predicates. Existing research has shown how to incorporate such predicates in a traditional cost-based query optimizer. In this paper we deal with the optimization of the expensive predicates themselves, showing how their cost can be reduced by utilizing cheaper, but less accurate, versions of the predicates to pre-filter tuples. We discuss the generalized tuple handling mechanism, which processes tuples along a fixed sequence of versions, as well as adaptive approaches that either split tuple streams into groups, or make routing decisions at the individual tuple level. We identify the lower bound to the problem of evaluating a multi-version selection predicate by an ideal individualized plan (IIP), and develop an optimal generalized plan (OGP). We then show how realistic individualized or grouped schemes can produce an intermediate cost between OGP and IIP, if tuples substantially deviate from the average stream behavior. Our algorithms are tested experimentally, identifying many of the issues that arise whenever multi-version predicates are used.

#index 960301
#* Progressive optimization in a shared-nothing parallel database
#@ Wook-Shin Han;Jack Ng;Volker Markl;Holger Kache;Mokhtar Kandil
#t 2007
#c 5
#% 211569
#% 264691
#% 397372
#% 458752
#% 479455
#% 481784
#% 650983
#% 765456
#% 1016208
#% 1016265
#! Commercial enterprise data warehouses are typically implemented on parallel databases due to the inherent scalability and performance limitation of a serial architecture. Queries used in such large data warehouses can contain complex predicates as well as multiple joins, and the resulting query execution plans generated by the optimizer may be sub-optimal due to mis-estimates of row cardinalities. Progressive optimization (POP) is an approach to detect cardinality estimation errors by monitoring actual cardinalities at run-time and to recover by triggering re-optimization with the actual cardinalities measured. However, the original serial POP solution is based on a serial processing architecture, and the core ideas cannot be readily applied to a parallel shared-nothing environment. Extending the serial POP to a parallel environment is a challenging problem since we need to determine when and how we can trigger re-optimization based on cardinalities collected from multiple independent nodes. In this paper, we present a comprehensive and practical solution to this problem, including several novel voting schemes whether to trigger re-optimization, a mechanism to reuse local intermediate results across nodes as a partitioned materialized view, several flavors of parallel checkpoint operators, and parallel checkpoint processing methods using efficient communication protocols. This solution has been prototyped in a leading commercial parallel DBMS. We have performed extensive experiments using the TPC-H benchmark and a real-world database. Experimental results show that our solution has negligible runtime overhead and accelerates the performance of complex OLAP queries by up to a factor of 22.

#index 960302
#* The case for a wide-table approach to manage sparse relational data sets
#@ Eric Chu;Jennifer Beckmann;Jeffrey Naughton
#t 2007
#c 5
#% 872
#% 64791
#% 269634
#% 287082
#% 348164
#% 480629
#% 482100
#% 572314
#% 602050
#% 610668
#% 993987
#% 1016135
#! A "sparse" data set typically has hundreds or even thousands of attributes, but most objects have non-null values for only a small number of these attributes. A popular view about sparse data is that it arises merely as the result of poor schema design. In this paper, we argue that rather than being the result of inept schema design,storing a sparse data set in a single table is the right way to proceed. However, for this to be the case, RDBMSs must provide sparse data management facilities that go beyond the previously studied requirement of storing such data sets efficiently. In particular, an RDBMS must 1) enable users to effectively build ad hoc queries over a very large number of attributes, and 2) support efficient evaluation of these queries over a wide, sparse table. We propose techniques that provide these capabilities, and argue that the single-table approach is a necessary component of self-managing database systems because it frees users from a tedious and potentially ineffective schema-design phase when managing sparse data sets.

#index 960303
#* Genome-scale disk-based suffix tree indexing
#@ Benjarath Phoophakdee;Mohammed J. Zaki
#t 2007
#c 5
#% 271801
#% 289010
#% 317163
#% 401771
#% 480484
#% 509775
#% 591632
#% 593764
#% 593861
#% 737254
#% 744092
#% 745510
#% 789004
#% 794239
#% 829998
#% 1016132
#% 1386499
#% 1671822
#! With the exponential growth of biological sequence databases, it has become critical to develop effective techniques for storing, querying, and analyzing these massive data. Suffix trees are widely used to solve many sequence-based problems, and they can be built in linear time and space, provided the resulting tree fits in main-memory. To index larger sequences, several external suffix tree algorithms have been proposed in recent years. However, they suffer from several problems such as susceptibility to data skew, non-scalability to genome-scale sequences, and non-existence of suffix links, which are crucial in various suffix tree based algorithms. In this paper, we target DNA sequences and propose a novel disk-based suffix tree algorithm called TRELLIS, which effectively scales up to genome-scale sequences. Specifically, it can index the entire human genome using 2GB of memory, in about 4 hours and can recover all its suffix links within 2 hours. TRELLIS was compared to various state-of-the-art persistent disk-based suffix tree construction algorithms, and was shown to outperform the best previous methods, both in terms of indexing time and querying time.

#index 960304
#* Fast and practical indexing and querying of very large graphs
#@ Silke Trißl;Ulf Leser
#t 2007
#c 5
#% 23651
#% 58365
#% 236409
#% 341704
#% 348181
#% 480081
#% 481434
#% 566109
#% 722530
#% 742563
#% 800534
#% 824692
#% 838518
#% 864462
#% 1688299
#% 1720616
#! Many applications work with graph-structured data. As graphs grow in size, indexing becomes essential to ensure sufficient query performance. We present the GRIPP index structure (GRaph Indexing based on Pre- and Postorder numbering) for answering reachability queries in graphs. GRIPP requires only linear time and space. Using GRIPP, we can answer reachability queries on graphs with 5 million nodes on average in less than 5 milliseconds, which is unrivaled by previous methods. We evaluate the performance and scalability of our approach on real and synthetic random and scale-free graphs and compare our approach to existing indexing schemes. GRIPP is implemented as stored procedure inside a relational database management system and can therefore very easily be integrated into existing graph-oriented applications.

#index 960305
#* Fg-index: towards verification-free query processing on graph databases
#@ James Cheng;Yiping Ke;Wilfred Ng;An Lu
#t 2007
#c 5
#% 288990
#% 378391
#% 397359
#% 397360
#% 466644
#% 479465
#% 480656
#% 481434
#% 601159
#% 629708
#% 654452
#% 729938
#% 769907
#% 769940
#% 824710
#% 850729
#% 915254
#% 1015336
#! Graphs are prevalently used to model the relationships between objects in various domains. With the increasing usage of graph databases, it has become more and more demanding to efficiently process graph queries. Querying graph databases is costly since it involves subgraph isomorphism testing, which is an NP-complete problem. In recent years, some effective graph indexes have been proposed to first obtain a candidate answer set by filtering part of the false results and then perform verification on each candidate by checking subgraph isomorphism. Query performance is improved since the number of subgraph isomorphism tests is reduced. However, candidate verification is still inevitable, which can be expensive when the size of the candidate answer set is large. In this paper, we propose a novel indexing technique that constructs a nested inverted-index, called FG-index, based on the set of Frequent subGraphs (FGs). Given a graph query that is an FG in the database, FG-index returns the exact set of query answers without performing candidate verification. When the query is an infrequent graph, FG-index produces a candidate answer set which is close to the exact answer set. Since an infrequent graph means the graph occurs in only a small number of graphs in the database, the number of subgraph isomorphism tests is small. To ensure that the index fits into the main memory, we propose a new notion of δ-Tolerance Closed Frequent Graphs (δ-TCFGs), which allows us to flexibly tune the size of the index in a parameterized way. Our extensive experiments verify that query processing using FG-index is orders of magnitude more efficient than using the state-of-the-art graph index.

#index 960306
#* Turning data into knowledge: challenges and opportunities at baidu.com
#@ William I. Chang
#t 2007
#c 5
#! The Chinese language search engine Baidu.com is the fourth most trafficked web site in the world, in what will likely soon become the world's largest user-base, China. We will outline three key systems: search engine, iknow.baidu.com, and advertising platform, and describe open problems encountered in the continuing process to improve Baidu.com's technology and services.

#index 960307
#* Databases on the web
#@ Raghu Ramakrishnan
#t 2007
#c 5
#! What role will database management play in the next generation of the web? I believe that a number of trends signal a growing and central role for the ideas and techniques that have emerged over the past three decades of database research. However, we will have to re-examine some basic issues and build a new generation of data management infrastructures in order to address many of the new demands made by web data management.

#index 960308
#* Webstudio: building infrastructure for web data management
#@ Ji-Rong Wen;Wei-Ying Ma
#t 2007
#c 5
#! To explore various ideas and algorithms for improving relevance of a search engine, we found it necessary to build an infrastructure to provide large-scale data management and data processing capabilities. WebStudio is an infrastructure we have constructed to provide an integrated development environment (IDE) for researchers and developers to use in quickly building prototypes and conducting experiments at Web-scale. It is also a Web data management system to allow users to easily store, access, and manipulate Web data.

#index 960309
#* Anatomy of the ADO.NET entity framework
#@ Atul Adya;José A. Blakeley;Sergey Melnik;S. Muralidhar
#t 2007
#c 5
#% 38691
#% 268303
#% 287631
#% 479979
#% 481919
#% 489544
#% 572311
#% 881740
#! Traditional client-server applications relegate query and persistence operations on their data to database systems. The database system operates on data in the form of rows and tables, while the application operates on data in terms of higher-level programming language constructs (classes, structures etc.). The impedance mismatch in the data manipulation services between the application and the database tier was problematic even in traditional systems. With the advent of service-oriented architectures (SOA), application servers and multi-tier applications, the need for data access and manipulation services that are well-integrated with programming environments and can operate in any tier has increased tremendously. Microsoft's ADO.NET Entity Framework is a platform for programming against data that raises the level of abstraction from the relational level to the conceptual (entity) level, and thereby significantly reduces the impedance mismatch for applications and data-centric services. This paper describes the key aspects of the Entity Framework, the overall system architecture, and the underlying technologies.

#index 960310
#* An in-depth look at the architecture of an object/relational mapper
#@ Patrick Connor Linskey;Marc Prud'hommeaux
#t 2007
#c 5
#% 387159
#! Object/relational mapping (ORM) tools can deliver a number of benefits to object-oriented, data-centric applications. These benefits are usually artifacts of the implementation of the tool used for the mapping, and not of the ORM paradigm itself. We will examine some of the significant features that the Kodo[1] ORM implementation delivers, by looking at the architecture of Kodo to see the source of these features.

#index 960311
#* Handling heterogeneous data sources in a SOA environment with service data objects (SDO)
#@ Luciano Resende
#t 2007
#c 5
#! Service Oriented Architecture (SOA) is gaining momentum in the development community based on its model for creating richer, reusable solutions at lower cost that can adjust to the agile business environments. The solutions built on this principle are composed of services that offer business functionality as a unit or as a whole. These services will access data from different data source types and potentially need to aggregate data from different data source types with different data formats. In this type of environment, handling of different forms of data poses a big challenge for application developers, and the challenge increases in size when you consider the broad diversity of programming models in use. Service Data Objects (SDO) is a specification for a programming model that unifies data programming across data source types and provides robust support for common application patterns in a disconnected way. Although Service Data Objects offer many other capabilities that are very useful for supporting integration with tools and other frameworks, this paper will focus on describing SDOs and how it can be used to handle heterogeneous data sources in SOA environments.

#index 960312
#* Recent database challenges in China on data consolidation and integration
#@ Yun Wang
#t 2007
#c 5
#! During the first phase of IT exploitation in China, a lot of isolated systems were developed and deployed over all the sites where the business took place. Each system typically dealt with one simple local business application. Since the mid 90's, data consolidation and data integration became critical to address the business need for cheaper cost, shorter time to the market as well as better business decision and control. Without consolidated and integrated data, there is no way to support new and complex applications for rapid and large scale business growth in China across lots of industries such as financial, telecom, government, manufacture and distribution. When database is used for companies and government agencies in China, the large Chinese population makes the database volume huge. A nation-wide bank or an insurance company has hundreds of millions accounts easily. A province-level telecom service provider has tens of millions subscribers. In certain domains, such as banking and telecom, many applications are executed as on-line transactions which ask for large transaction throughput with good response time. A province-level telecom service provider expects to handle up to 100 billion calls in a month. In this case, most of the phone calls will be pre-paid type and these calls need to be handled in near-real time. In order to satisfy the performance requirement, some applications implement application cache on application servers. This approach improves the application performance in getting data but it also introduces the complexity and longer delay when a failure in application occurs. Database management systems can provide client cache for powerful application servers in these days. As more and more application servers are added to respond large number of concurrent users and high volume transaction throughput, database management system must be highly scalable. Both logical and physical contentions should be avoided or greatly reduced. Continuous availability and high reliability are more critical for a consolidated large database because many users and applications rely on it. Both planned outage and unplanned outage should be avoided or minimized through appropriate duplication and redundancy. Smooth transition through cooperative versions of data helps reduce planned outages. Swift switch over to hot stand-by database system helps minimize the unplanned outages. Database management systems are expected to be autonomic and remain healthy in resource utilization with minimal administrative involvement. As a summary, data consolidation in China asks for low-cost, high performance database systems with high scalability, availability and reliability on very large database volume. Data integration in China often deals with both a horizontal layer of multiple lines of business and a vertical organizational structure. For government agencies, there are at least four levels across nation, province, district and county. There are a few industry specific data models and solutions to address the requirements of data integration. However, a common data integration platform has been constantly requested by many IT projects of large scale. In many cases, business control processes need to integrate business operation processes to support near-real time decision and control. There is a high demand for a practical and well-proven methodology with tool sets which can support data integration life cycle of architect, design, development, deployment and change management. Rapid economic and business growth in combination of the join to WTO created big demand in IT evolution across China business, industry and government. There are many demands and opportunities of data consolidation and integration with large data volume and system complexity. The demand of low-cost solutions makes it even more challenging.

#index 960313
#* InfiniteDB: a pc-cluster based parallel massive database management system
#@ Jianzhong Li;Hong Gao;Jizhou Luo;Shengfei Shi;Wei Zhang
#t 2007
#c 5
#% 188719
#% 442698
#% 442700
#% 443501
#% 462046
#% 480258
#% 480794
#% 591261
#% 591262
#% 902464
#! This paper describes a PC-cluster based parallel DBMS, InfiniteDB, developed by the authors. InfiniteDB aims at efficiently storing and processing of massive databases in response to the rapidly growing in database size and the need of high performance analyzing of massive databases. It supports the parallelisms of intra-query, inter-query, intra-operation, inter-operation and pipelining. It provides effective strategies for processing massive databases including the multiple data declustering methods, the declustering-aware algorithms for the execution of relational operations and other database operations, and the adaptive query optimization method. It also provides the functions of parallel data warehousing and data mining, the coordinator-wrapper mechanism to support the integration of heterogeneous information resources on the Internet, and the fault tolerant and resilient infrastructures. It has been used in many applications and has proved quite effective for storing and processing massive databases in practice.

#index 960314
#* eSagu™: a data warehouse enabled personalized agricultural advisory system
#@ P. Krishna Reddy;G. V. Ramaraju;G. S. Reddy
#t 2007
#c 5
#% 218149
#! In this paper, we explain a personalized agricultural advisory system called eSagu, which has been developed to improve the performance and utilization of agriculture technology and help Indian farmers. In eSagu, rather than visiting the crop in person, the agricultural expert delivers the expert advice at regular intervals (once in one or two weeks) to each farm by getting the crop status in the form of digital photographs and other information. During 2004-06, through eSagu, agricultural expert advices delivered for about 6000 farms covering six crops. The results show that the expert advices helped the farmers to achieve savings in capital investment and improved the crop yield. Mainly, the data warehouse of farm histories has been developed which is providing the crop related information to the agricultural expert in an integrated manner for generating a quality agricultural expert advice. In this paper, after explaining eSagu and its advantages, we discuss how data warehouse of farm histories is enabling agricultural expert to deliver a quality expert advice. We also discuss some research issues to improve the performance of eSagu.

#index 960315
#* LIPTUS: associating structured and unstructured information in a banking environment
#@ Manish A. Bhide;Ajay Gupta;Rahul Gupta;Prasan Roy;Mukesh K. Mohania;Zenita Ichhaporia
#t 2007
#c 5
#% 279755
#% 292681
#% 344447
#% 443956
#% 465754
#% 466266
#% 765519
#% 769884
#% 769892
#% 787549
#% 867052
#! Growing competition has made today's banks understand the value of knowing their customers better. In this paper, we describe a tool, LIPTUS, that associates the customer interactions (emails and transcribed phone calls) with customer and account profiles stored in an existing data warehouse. The associations discovered by LIPTUS enable analytics spanning the customer and account profiles on one hand and the meta-data associated or derived from the interaction (using text mining techniques) on the other. We illustrate the value derived from this consolidated analysis through specific customer intelligence applications. LIPTUS is today being extensively used in a large bank in India. A highlight of this paper is a discussion of the technical challenges encountered while building LIPTUS and deploying it on real-life customer data.

#index 960316
#* Effective and efficient update of xml in RDBMS
#@ Zhen Hua Liu;Muralidhar Krishnaprasad;James W. Warner;Rohan Angrish;Vikas Arora
#t 2007
#c 5
#% 286901
#% 287005
#% 333979
#% 385828
#% 1015339
#% 1016152
#% 1016223
#! Querying XML effectively and efficiently using declarative languages such as XQuery and XPath has been widely studied in both academic and industrial settings. Most RDBMS vendors now support XML as a native data type with SQL/XML and XQuery support over it. However, the problem of updating XML is still the subject of ongoing effort. Several SQL/XML update extensions have been implemented and an XQuery Update Facility is in the proposal phase to add an update capability to XQuery. There are a lot of challenges involved in updating XML, particularly identifying and updating partial fragments of XML while maintaining concurrency, transactional semantics and validity of the document. In this paper, we illustrate the XML update functionality provided by Oracle XML DB within the context of SQL/XML. This functionality has been developed and optimized based on actual customer use cases of querying and updating XML. We discuss our design philosophy, optimization details for providing capability of updating XML and compare it with the current XQuery Update Facility proposal with the goal of providing insight into incorporating the XQuery Update Facility in the SQL/XML standard in the future.

#index 960317
#* An XML transaction processing benchmark
#@ Matthias Nicola;Irina Kogan;Berni Schiefer
#t 2007
#c 5
#% 342677
#% 489048
#% 541480
#% 730031
#% 824750
#% 994015
#% 1016224
#% 1721253
#! XML database functionality has been emerging in "XML-only" databases as well as in the major relational database products. Yet, there is no industry standard XML database benchmark to evaluate alternative implementations. The research community has proposed several benchmarks which are all useful in their respective scope, such as evaluating XQuery processors. However, they do not aim to evaluate a database system in its entirety and do not represent all relevant characteristics of a real-world XML application. Often they only define read-only single-user tests on a single XML document. We have developed an application-oriented and domain-specific benchmark called "Transaction Processing over XML" (TPoX). It exercises all aspects of XML databases, including storage, indexing, logging, transaction processing, and concurrency control. Based on our analysis of real XML applications, TPoX simulates a financial multi-user workload with XML data conforming to the FIXML standard. In this paper we describe TPoX and present early performance results. We also make its implementation publicly available.

#index 960318
#* Why off-the-shelf RDBMSs are better at XPath than you might expect
#@ Torsten Grust;Jan Rittinger;Jens Teubner
#t 2007
#c 5
#% 287715
#% 333981
#% 397358
#% 397366
#% 397375
#% 479806
#% 479956
#% 480489
#% 504574
#% 654493
#% 659999
#% 765488
#% 814648
#% 875010
#% 960362
#% 994015
#% 1015298
#% 1016150
#% 1016224
#! To compensate for the inherent impedance mismatch between the relational data model (tables of tuples) and XML (ordered, unranked trees), tree join algorithms have become the prevalent means to process XML data in relational databases, most notably the TwigStack[6], structural join[1], and staircase join[13] algorithms. However, the addition of these algorithms to existing systems depends on a significant invasion of the underlying database kernel, an option intolerable for most database vendors. Here, we demonstrate that we can achieve comparable XPath performance without touching the heart of the system. We carefully exploit existing database functionality and accelerate XPath navigation by purely relational means: partitioned B-trees bring access costs to secondary storage to a minimum, while aggregation functions avoid an expensive computation and removal of duplicate result nodes to comply with the XPath semantics. Experiments carried out on IBM DB2 confirm that our approach can turn off-the-shelf database systems into efficient XPath processors.

#index 960319
#* Schema advisor for hybrid relational-XML DBMS
#@ Mirella M. Moro;Lipyeow Lim;Yuan-Chi Chang
#t 2007
#c 5
#% 380546
#% 533902
#% 534067
#% 659924
#% 810036
#% 824750
#% 860355
#% 1393699
#% 1393716
#! In response to the widespread use of the XML format for document representation and message exchange, major database vendors support XML in terms of persistence, querying and indexing. Specifically, the recently released IBM DB2 9 (for Linux, Unix and Windows) is a hybrid data server with optimized management of both XML and relational data. With the new option of storing and querying XML in a relational DBMS, data architects face the the decision of what portion of their data to persist as XML and what portion as relational data. This problem has not been addressed yet and represents a serious need in the industry. Hence, this paper describes ReXSA, a schema advisor tool that is being prototyped for IBM DB2 9. ReXSA proposes candidate database schemas given an information model of the enterprise data. It has the advantage of considering qualitative properties of the information model such as reuse, evolution and performance profiles for deciding how to persist the data. Finally, we show the viability and practicality of ReXSA by applying it to custom and real usecases.

#index 960320
#* Data streams go mainstream
#@ Robert Michael Lefler
#t 2007
#c 5
#! Data Stream Processing (DSP) has moved rapidly from being the focus of an invited talk at PODS 2002 to start-up companies in 2005 and 2006. Today the giant RDBMS vendors such as IBM, Microsoft, and Oracle have added, or are seeking to add, DSP capabilities to their respective product lines. This panel will discuss the likely trajectory of Data Stream Processing as it moves forward as a component of the market-leading RDBMS product lines.

#index 960321
#* Flexible and efficient access control in oracle
#@ Ravi Murthy;Eric Sedlar
#t 2007
#c 5
#% 164560
#% 204453
#% 211987
#% 243322
#% 462501
#% 644306
#% 765447
#! A single model for access control across the database and application server tiers is crucial to ensure consistent secure access to data in all the tiers. In this paper, we present the common model for access control within Oracle database and application tiers which is based on the standard WebDAV ACLs (Access Control Lists). Further, we discuss the flexible mechanisms for defining ACLs and associating them with data and various optimization techniques for efficiently evaluating ACLs in large scale enterprise applications.

#index 960322
#* A framework for enforcing application policies in database systems
#@ Lin Qiao;Basuki Soetarman;Gene Fuh;Adarsh Pannu;Baoqiu Cui;Thomas Beavin;William Kyu
#t 2007
#c 5
#% 340827
#% 635840
#% 764959
#% 764963
#% 770315
#% 770354
#% 870483
#% 882293
#% 1016221
#! As database systems have grown in terms of scale and complexity, administration tasks have become increasingly difficult and time consuming. A scarcity of skilled database professionals has meant that human costs have begun to dominate the total cost of ownership (TCO) of a database system. Database vendors are under immense pressure to provide solutions that make their products easy to administer in areas such as problem diagnostics, monitoring, query tuning, access control and system configuration. To address this issue, we have built a framework that allows control over many administration operations via the use of policies. Users can uniformly define, manage and enforce policies to affect disparate aspects of the system. In our framework, policies are declarative constructs that are comprised of type, scope, condition and action. Policy groups cover query monitoring and tuning, query prioritization, system configuration, access control, report generation, etc. Policy scope defines the domain over which policies apply. Policy actions are performed if certain conditions are true. This framework has been fully integrated into DB2 for z/OS V9. Using detailed system performance evaluations, we report that enforcement of policies is largely a function of data-collection granularity. Under the setting for normal monitoring with minimal report, the overhead on system performance is very low (0.1%).

#index 960323
#* Execution strategies for SQL subqueries
#@ Mostafa Elhemali;César A. Galindo-Legaria;Torsten Grabs;Milind M. Joshi
#t 2007
#c 5
#% 136740
#% 220425
#% 287005
#% 461897
#% 476622
#% 480115
#% 564426
#! Optimizing SQL subqueries has been an active area in database research and the database industry throughout the last decades. Previous work has already identified some approaches to efficiently execute relational subqueries. For satisfactory performance, proper choice of subquery execution strategies becomes even more essential today with the increase in decision support systems and automatically generated SQL, e.g., with ad-hoc reporting tools. This goes hand in hand with increasing query complexity and growing data volumes, which all pose challenges for an industrial-strength query optimizer. This current paper explores the basic building blocks that Microsoft SQL Server utilizes to optimize and execute relational subqueries. We start with indispensable prerequisites such as detection and removal of correlations for subqueries. We identify a full spectrum of fundamental subquery execution strategies such as forward and reverse lookup as well as set-based approaches, explain the different execution strategies for subqueries implemented in SQL Server, and relate them to the current state of the art. To the best of our knowledge, several strategies discussed in this paper have not been published before. An experimental evaluation complements the paper. It quantifies the performance characteristics of the different approaches and shows that indeed alternative execution strategies are needed in different circumstances, which make a cost-based query optimizer indispensable for adequate query performance.

#index 960324
#* Building statistical models and scoring with UDFs
#@ Carlos Ordonez
#t 2007
#c 5
#% 152934
#% 210173
#% 248014
#% 248792
#% 248813
#% 248816
#% 273900
#% 278011
#% 280521
#% 300213
#% 342704
#% 386381
#% 420076
#% 464998
#% 654445
#% 845220
#% 1015290
#! Multidimensional statistical models are generally computed outside a relational DBMS, exporting data sets. This article explains how fundamental multidimensional statistical models are computed inside the DBMS in a single table scan exploiting SQL and User-Defined Functions (UDFs). The techniques described herein are used in a commercial data mining tool, called Teradata Warehouse Miner. Specifically, we explain how correlation, linear regression, PCA and clustering, are integrated into the Teradata DBMS. Two major database processing tasks are discussed: building a model and scoring a data set based on a model. To build a model two summary matrices are shown to be common and essential for all linear models: the linear sum of points and the quadratic sum of cross-products of points. Since such matrices are generally significantly smaller than the data set, we explain how the remaining matrix operations to build the model can be quickly performed outside the DBMS. We first explain how to efficiently compute summary matrices with plain SQL queries. Then we present two sets of UDFs that work in a single table scan: an aggregate UDF to compute summary matrices and a set of scalar UDFs to score data sets. Experiments compare UDFs and SQL queries (running inside the DBMS) with C++ (running outside on exported files). In general, UDFs are faster than SQL queries and UDFs are more efficient than C++, due to long export times. Statistical models based on the summary matrices can be built outside the DBMS in just a few seconds. Aggregate and scalar UDFs scale linearly and require only one table scan, making them ideal to process large data sets.

#index 960325
#* AllInOneNews: development and evaluation of a large-scale news metasearch engine
#@ King-Lup Liu;Weiyi Meng;Jing Qiu;Clement Yu;Vijay Raghavan;Zonghuan Wu;Yiyao Lu;Hai He;Hongkun Zhao
#t 2007
#c 5
#% 194246
#% 230432
#% 280854
#% 287237
#% 330704
#% 342401
#% 344448
#% 420508
#% 443556
#% 443561
#% 479451
#% 479642
#% 481748
#% 567255
#% 607815
#% 643559
#% 724635
#% 805845
#% 978381
#% 1683874
#! AllInOneNews is the largest news metasearch engine in the world, connecting to over 1,000 news sites over 150 countries. Implementing a large-scale metasearch engine like AllInOneNews needs to overcome unique challenges not faced by building small metasearch engines such as developing highly scalable search engine selection techniques. In this paper, we discuss these unique challenges and our solutions to these challenges. We also discuss some novel features of AllInOneNews such as highly automated solution and semantic query match. This paper also reports the results of a comparative evaluation of three commercial news search systems, one search engine - Google News and two metasearch engines - Mamma News and AllInOneNews. Several measures such as effectiveness, diversity and time-sensitivity are used to perform the comparison. Another contribution of this paper is that we introduce a novel scheme to compare multiple news search systems in a combined measure that takes both relevance and time-sensitivity of retrieved information into consideration.

#index 960326
#* Map-reduce-merge: simplified relational data processing on large clusters
#@ Hung-chih Yang;Ali Dasdan;Ruey-Lung Hsiao;D. Stott Parker
#t 2007
#c 5
#% 115661
#% 479920
#% 845351
#% 954300
#% 1002142
#! Map-Reduce is a programming model that enables easy development of scalable parallel applications to process a vast amount of data on large clusters of commodity machines. Through a simple interface with two functions, map and reduce, this model facilitates parallel implementation of many real-world tasks such as data processing jobs for search engines and machine learning. However,this model does not directly support processing multiple related heterogeneous datasets. While processing relational data is a common need, this limitation causes difficulties and/or inefficiency when Map-Reduce is applied on relational operations like joins. We improve Map-Reduce into a new model called Map-Reduce-Merge. It adds to Map-Reduce a Merge phase that can efficiently merge data already partitioned and sorted (or hashed) by map and reduce modules. We also demonstrate that this new model can express relational algebra operators as well as implement several join algorithms.

#index 960327
#* BIwTL: a business information warehouse toolkit and language for warehousing simplification and automation
#@ Bin He;Rui Wang;Ying Chen;Ana Lelescu;James Rhodes
#t 2007
#c 5
#% 201928
#% 223781
#% 316709
#% 340301
#% 393641
#% 459299
#% 461921
#% 462074
#% 462204
#% 462217
#% 464706
#% 479476
#% 479795
#% 480134
#% 481604
#% 481933
#% 481951
#% 482093
#% 482098
#% 482110
#% 482111
#% 503875
#% 631946
#% 770327
#% 948784
#% 1015294
#! Rapidly leveraging information analytics technologies to mine the mounting information in structured and unstructured forms, derive business insights and improve decision making is becoming increasingly critical to today's business successes. One of the key enablers of the analytics technologies is an Information Warehouse Management System (IWMS) that processes different types and forms of information, builds, and maintains the information warehouse (IW) effectively. Although traditional multi-dimensional data warehousing techniques, coupled with the well-known ETL processes (Extract, Transform, Load) may meet some of the requirements in an IWMS, in general, they fall short on several major aspects: 1. They often lack comprehensive support for both structured and unstructured data processing; 2. they are database-centric and require detailed database and data warehouse knowledge to perform IWMS tasks, and hence they are tedious and time-consuming to operate and learn; 3. they are often inflexible and insufficient in coping with a wide variety of on-going IW maintenance tasks, such as adding new dimensions and handling regular and lengthy data updates with potential failures and errors. To cope with such issues, this paper describes an IWMS, called BIwTL (Business Information Warehouse Toolkit and Language), that automates and simplifies IWMS tasks by devising a high-level declarative information warehousing language, GIWL, and building the runtime system components for such a language. BIwTL hides system details, e.g., databases, full text indexers, and data warehouse models, from users by automatically generating appropriate runtime scripts and executing them based on the GIWL language specification. Moreover, BIwTL supports structured and unstructured information processing by embedding flexible data extraction and transformation capabilities, while ensuring high performance processing for large datasets. In addition, this paper systematically studied the core tasks around information warehousing and identified five key areas. In particular, we describe our technologies in three areas, i.e., constructing an IW, data loading, and maintaining an IW. We have implemented such technologies in BIwTL 1.0 and validated it in real world environments with a number of customers. Our experience suggests that BIwTL is light-weight, simple, efficient, and flexible.

#index 960328
#* The microsoft data platform
#@ David Campbell;Anil Nori
#t 2007
#c 5
#! Advances in hardware, storage, devices, connectivity, and web technology are changing the way applications are designed, deployed, and managed. Applications are increasingly becoming data-centric and data is everywhere, and in all tiers (from the client to the cloud). Data across multiple tiers requires data access and management capabilities across these tiers. The Microsoft Data Platform presents a vision for an end-to-end data platform that offers data services across all tiers.

#index 960329
#* GPUQP: query co-processing using graphics processors
#@ Rui Fang;Bingsheng He;Mian Lu;Ke Yang;Naga K. Govindaraju;Qiong Luo;Pedro V. Sander
#t 2007
#c 5
#% 479819
#% 654479
#% 765419
#% 810059
#% 874997
#! We present GPUQP, a relational query engine that employs both CPUs and GPUs (Graphics Processing Units) for in-memory query co-processing. GPUs are commodity processors traditionally designed for graphics applications. Recent research has shown that they can accelerate some database operations orders of magnitude over CPUs. So far, there has been little work on how GPUs can be programmed for heavy-duty database constructs, such as tree indexes and joins, and how well a full-fledged GPU query co-processor performs in comparison with their CPU counterparts. In this work, we explore the design decisions in using GPUs for query co-processing using both a graphics API and a general purpose programming model. We then demonstrate the processing flows as well as the performance results of our methods.

#index 960330
#* EaseDB: a cache-oblivious in-memory query processor
#@ Bingsheng He;Yinan Li;Qiong Luo;Dongqing Yang
#t 2007
#c 5
#% 386623
#% 480119
#% 1016238
#! We propose to demonstrate EaseDB, the first cache-oblivious queryprocessor for in-memory relational query processing. The cache-oblivious notion from the theory community refers to the property that no parameters in an algorithm or a data structure need to be tuned for a specific memory hierarchy for optimality. As a result, EaseDB automatically optimizes the cache performance as well as the overall performance of query processing on any memory hierarchy. We have developed a visualization interface to show the detailed performance of EaseDB in comparison with its cache-conscious counterpart, with both the parameters in the cache-conscious algorithms and the hardware platforms varied.

#index 960331
#* Online autoadmin: (physical design tuning)
#@ Nicolas Bruno;Surajit Chaudhuri
#t 2007
#c 5
#% 1016220
#% 1016221
#! Existing solutions for the automated physical design problem require explicit invocations of tuning tools and critically depend on DBAs gathering representative workloads manually. In this demonstration, we show an alternative approach to the physical design problem. Specifically, we demonstrate a novel monitoring/tuning DBMS component that we prototyped in Microsoft SQL Server 2005 as a server-side extension. This component is always-on and continuously modifies the current physical design reacting to varying workload or data characteristics. Our solution imposes low overhead and takes into account storage constraints, update statements, and the cost to create physical structures.

#index 960332
#* ADO.NET entity framework: raising the level of abstraction in data programming
#@ Pablo Castro;Sergey Melnik;Atul Adya
#t 2007
#c 5
#! The ADO.NET Entity Framework provides a persistence layer for .NET applications that allows developers to work at a higher level of abstraction when interacting with data and data-access interfaces. Developers can model and access their data using a conceptual schema that is mapped to a relational database via a flexible mapping. Interaction with the data can take place using a SQL-based data manipulation language and iterator APIs, or through an object-based domain model in the spirit of object-to-relational mappers.We demonstrate how the Entity Framework simplifies application development using sample scenarios. We illustrate how the data is modeled, queried and presented to the developer. We also show how the provided data programming infrastructure can result in easier-to-understand code by making its intent more explicit, as well as how it can help with maintenance by adding a level of indirection between the logical database schema and the conceptual model that applications operate on.

#index 960333
#* Exploiting self-monitoring sample views for cardinality estimation
#@ Per-Ake Larson;Wolfgang Lehner;Jingren Zhou;Peter Zabback
#t 2007
#c 5
#! Good cardinality estimates are critical for generating good execution plans during query optimization. Complex predicates, correlations between columns, and user-defined functions are extremely hard to handle when using the traditional histogram approach. This demo illustrates the use of sample views for cardinality estimations as prototyped in Microsoft SQL Server. We show the creation of sample views, discuss how they are exploited during query optimization, and explain their potential effect on query plans. In addition, we also show our implementation of maintenance policies using statistical quality control techniques based on query feedback.

#index 960334
#* ConEx: a system for monitoring queries
#@ Chaitanya Mishra;Maksims Volkovs
#t 2007
#c 5
#% 765467
#% 765468
#% 800589
#! We present a system, ConEx, for monitoring query execution in a relational database management system. ConEx offers a unified view of query execution, providing continuous visual feedback on the progress of the query, and the status of operators in the query evaluation plan. It incorporates novel techniques to dynamically estimate important parameters affecting query progress efficiently. We describe the design and features of ConEx, and discuss its technology.

#index 960335
#* Automated and on-demand provisioning of virtual machines for database applications
#@ Piyush Shivam;Azbayar Demberel;Pradeep Gunda;David Irwin;Laura Grit;Aydan Yumerefendi;Shivnath Babu;Jeff Chase
#t 2007
#c 5
#% 911449
#% 978458
#% 2092350
#! Utility computing delivers compute and storage resources to applications as an 'on-demand utility', much like electricity, from a distributed collection of computing resources. There is great interest in running database applications on utility resources (e.g., Oracle's Grid initiative) due to reduced infrastructure and management costs, higher resource utilization, and the ability to handle sudden load surges. Virtual Machine (VM) technology offers powerful mechanisms to manage a utility resource infrastructure. However, provisioning VMs for applications to meet system performance goals, e.g., to meet service level agreements (SLAs), is an open problem. We are building two systems at Duke - Shirako and NIMO - that collectively address this problem. Shirako is a toolkit for leasing VMs to an application from a utility resource infrastructure. NIMO learns application performance models using novel techniques based on active learning, and uses these models to guide VM provisioning in Shirako. We will demonstrate: (a) how NIMO learns performance models in an online and automatic fashion using active learning; and (b) how NIMO uses these models to do automated and on-demand provisioning of VMs in Shirako for two classes of database applications - multi-tier web services and computational science workflows.

#index 960336
#* URank: formulation and efficient evaluation of top-k queries in uncertain databases
#@ Mohamed A. Soliman;Ihab F. Ilyas;Kevin Chen-Chuan Chang
#t 2007
#c 5
#! Top-k processing in uncertain databases is semantically and computationally different from traditional top-k processing. The interplay between query scores and data uncertainty makes traditional techniques inapplicable. We introduce URank, a system that processes new probabilistic formulations of top-k queries inuncertain databases. The new formulations are based on marriage of traditional top-k semantics with possible worlds semantics. URank encapsulates a new processing framework that leverages existing query processing capabilities, and implements efficient search strategies that integrate ranking on scores with ranking on probabilities, to obtain meaningful answers for top-k queries.

#index 960337
#* A dynamic and flexible sensor network platform
#@ Rene Mueller;Jan S. Rellermeyer;Michael Duller;Gustavo Alonso;Donald Kossmann
#t 2007
#c 5
#! SwissQM is a novel sensor network platform for acquiring data from the real world. Instead of statically hand-crafted programs, SwissQM is a virtual machine capable of executing bytecode programs on the sensor nodes. By using a central and intelligent gateway, it is possible to either push aggregation and other operations into the network, or to execute them on the gateway. Since the gateway is built in an entirely modular style, it can be dynamically extended with new functionality such as user interfaces, user defined functions, or additional query optimizations. The goal of this demonstration is to show the flexibility and the unique features of SwissQM.

#index 960338
#* XTream: personal data streams
#@ Michael Duller;Rokas Tamosevicius;Gustavo Alonso;Donald Kossmann
#t 2007
#c 5
#% 654510
#% 726621
#! The real usability of data stream systems depends on the practical aspect of building applications on data streams. In this demo we show two possible applications on data streams implemented on our prototype platform XTream. One application integrates VoIP and E-Mail, the other one incorporates streams in a Smart Home setting. Using these applications we try to identify and discuss the functionality that data stream management systems should provide. Those attending the demo will be able to compose their own applications.

#index 960339
#* Travel time estimation using NiagaraST and latte
#@ Kristin Tufte;Jin Li;David Maier;Vassilis Papadimos;Robert L. Bertini;James Rucker
#t 2007
#c 5
#% 578391
#! To address increasing traffic congestion and its associated consequences, traffic managers are turning to intelligent transportation management. The latte project is extending data stream technology to handle queries that combine live streams with large data archives, motivated by needs in the Intelligent Transportation Systems (ITS) domain. In particular, we focus on queries that combine live data streams with large data archives. We demonstrate such stream-archive queries via the travel-time estimation problem. The demonstration uses the new latte system which has been developed using the NiagaraST stream processing system and the PORTAL transportation data archive.

#index 960340
#* Sonnet: an efficient distributed content-based dissemination broker
#@ Aoying Zhou;Weining Qian;Xueqing Gong;Minqi Zhou
#t 2007
#c 5
#% 1016180
#! In this demonstration, we present a prototype content-based dissemination broker, called Sonnet, which is built upon structured overlay network. It combines approximate filtering of XML packets with routing in the overlay network. Deliberate optimization technologies are implemented. The running and tracing of the system in a real-life application are to be demonstrated.

#index 960341
#* The OSIRIS-SE (stream-enabled) infrastructure for reliable data stream management on mobile devices
#@ Gert Brettlecker;Heiko Schuldt
#t 2007
#c 5
#% 528213
#% 767380
#% 767418
#% 1015355
#% 1728231
#! The proliferation of software and hardware sensors which continuously create large amounts of data has significantly facilitated novel types of applications such as healthcare telemonitoring or roadside traffic management. All these applications demand new mechanisms for online processing and analysis of relevant data coming from multiple data streams. Especially telemonitoring applications in healthcare require a high degree of reliability and must be able to be deployed in a distributed environment. We present OSIRIS-SE, an infrastructure for reliable data stream management in a failure-prone distributed setting including resource-limited mobile devices. OSIRIS-SE supports the combination of different data stream operators into stream processes and offers efficient coordinated operator check pointing for the execution of these stream processes. In order to support mobile devices, OSIRIS-SE is able to deal with multiple failures, offers fine-grained reliability at operator level, and supports decentralized stream process orchestration in a peer-to-peer fashion. Moreover, OSIRIS-SE is fully implemented in Java and thus can be run on different platforms. The demo shows the reliable execution of stream processes in a health monitoring application including a wearable ECG sensor, a Bluetooth enabled blood pressure sensor, and a web cam as data sources. Operators are hosted at mobile devices (PDAs, smart phones) of a patient and at a laptop computer which also acts as base station. An important feature of the demo is to show that sensor data can losslessly be processed by seamlessly migrating stream processing to other devices in the network even in case of multiple failures.

#index 960342
#* Cayuga: a high-performance event processing engine
#@ Lars Brenna;Alan Demers;Johannes Gehrke;Mingsheng Hong;Joel Ossher;Biswanath Panda;Mirek Riedewald;Mohit Thatte;Walker White
#t 2007
#c 5
#% 875004
#% 977010
#! We propose a demonstration of Cayuga, a complex event monitoring system for high speed data streams. Our demonstration will show Cayuga applied to monitoring Web feeds; the demo will illustrate the expressiveness of the Cayuga query language, the scalability of its query processing engine to high stream rates, and a visualization of the internals of the query processing engine.

#index 960343
#* AutoDomainMine: a graphical data mining system for process optimization
#@ Aparna S. Varde;Elke A. Rundensteiner;Richard D. Sisson
#t 2007
#c 5
#% 330780
#% 480132
#% 745480
#% 818916
#% 867869
#% 907559
#! This paper describes a graphical data mining system called AutoDomainMine. It is based on our proposed approach of integrating clustering and classification to mine scientific data stored in a database. The data consists of input conditions of scientific experiments and graphs plotted as their results. This system mines the stored data in order to submit exact or approximate ranked responses to user queries intended to optimize the scientific processes.

#index 960344
#* Quality is in the eye of the beholder: towards user-centric web-databases
#@ Huiming Qu;Jie Xu;Alexandros Labrinidis
#t 2007
#c 5
#% 571217
#! The proliferation of database-driven web sites (or web-databases) has brought upon a plethora of applications where both Quality of Service (QoS) and Quality of Data (QoD) are of paramount importance to the end users. In our previous work, we have proposed Quality Contracts, a comprehensive framework for specifying multiple dimensions of QoS/QoD; we have also developed user-centric admission control and scheduling algorithms in web databases, whose goal is to maximize overall system performance. In this work, we turn our attention to the user side of the equation. Specifically, we propose to demonstrate how the adaptation of Quality Contracts (QCs) by the users can lead to vastly different performance results, both from the user point of view (i.e., user satisfaction) and also from the system point of view. Towards this, we propose to structure our demo in the form of an interactive game, where participants will be playing the role of users continuously adapting their QCs over time, while "playing" against system-generated users, who follow predetermined QC adaptation policies. Finally, we also propose to illustrate the effect of different admission control and scheduling policies.

#index 960345
#* XmdvtoolQ:: quality-aware interactive data exploration
#@ Elke A. Rundensteiner;Matthew O. Ward;Zaixian Xie;Qingguang Cui;Charudatta V. Wad;Di Yang;Shiping Huang
#t 2007
#c 5
#% 434566
#% 789225
#% 789228
#% 853014
#% 910874
#! In this work, we describe our approach for making the interactive data exploration system, called XmdvTool, quality-aware to assure informed decision-making. XmdvToolQ, makes quality or lack thereof explicit for all stages of the data exploration process from raw data, to abstracted data, to the final visual displays, allowing users to query and navigate through data-, structure- and quality-spaces.

#index 960346
#* MySearchView: a customized metasearch engine generator
#@ Yiyao Lu;Zonghuan Wu;Hongkun Zhao;Weiyi Meng;King-Lup Liu;Vijay Raghavan;Clement Yu
#t 2007
#c 5
#% 232703
#% 342672
#% 344448
#% 607815
#% 643073
#% 643559
#% 660272
#% 724635
#% 729978
#% 805845
#% 1683874
#! In this paper, we describe MySearchView, a system for assembling search engines into metasearch engines. With this system, any user can create a metasearch engine by simply letting the system know the URLs of the search engines the user wants to be included and the metasearch engine will be built fully automatically. In this paper, the main steps of building metasearch engines will be sketched. We will also outline our plan to demonstrate all the features of MySearchView.

#index 960347
#* MashMaker: mashups for the masses
#@ Robert J. Ennals;Minos N. Garofalakis
#t 2007
#c 5

#index 960348
#* CLIDE: interactive query formulation for service oriented architectures
#@ Michalis Petropoulos;Alin Deutsch;Yannis Papakonstantinou
#t 2007
#c 5
#% 273923
#% 354421
#% 572311
#% 765447
#% 1016138

#index 960349
#* Query-based monitoring of BPEL business processes
#@ Catriel Beeri;Anat Eyal;Tova Milo;Alon Pilberg
#t 2007
#c 5
#% 465061
#% 1015276

#index 960350
#* User-centric personalized extensibility for data-driven web applications
#@ Nitin Gupta;Fan Yang;Alan J. Demers;Johannes Gehrke;Jayavel Shanmugasundaram
#t 2007
#c 5
#% 262249
#! We describe a novel programming model for building, extending, and personalizing web-based data-driven applications.

#index 960351
#* Mashup Feeds: continuous queries over web services
#@ Junichi Tatemura;Arsany Sawires;Oliver Po;Songting Chen;K. Selcuk Candan;Diviyakant Agrawal;Maria Goveas
#t 2007
#c 5
#% 784408
#% 893118
#! Mashup Feeds is a system that supports integrated web service feeds as continuous queries. We introduce collection-based stream processing semantics to enable information extraction by monitoring source evolution over time.

#index 960352
#* ORCHESTRA: facilitating collaborative data sharing
#@ Todd J. Green;Grigoris Karvounarakis;Nicholas E. Taylor;Olivier Biton;Zachary G. Ives;Val Tannen
#t 2007
#c 5
#% 481923
#% 654468
#% 826032

#index 960353
#* MIDST: model independent schema and data translation
#@ Paolo Atzeni;Paolo Cappellari;Giorgio Gianforme
#t 2007
#c 5
#% 458995
#% 800616
#% 824767
#% 1688267
#% 1720912
#! MIDST is a tool for the translation of schemas and databases from a model to another, in a framework that is flexible and extensible with respect to the family of models. The major novelties with respect to existing proposals consist in the generation of data-level translations and on the customizability of translations (at both schema and data level).

#index 960354
#* XANADUE: a system for detecting changes to XML data in tree-unaware relational databases
#@ Erwin Leonardi;Sourav S Bhowmick
#t 2007
#c 5
#% 25998
#% 943886
#! Recently, a number of main memory algorithms for detecting the changes to XML data have been proposed. These approaches are not suitable for detecting changes to large XML document as it requires a lot of memory to keep the two versions of XML documents in the memory. We have developed a novel XML change detection system, called XANADUE that uses traditional relational database engines for detecting changes to large XML data. In this approach, we store the XML documents in the relational database and issue SQL queries (whenever appropriate) to detect the changes. This demonstration will showcase the functionality of our system and the effectiveness of XML change detection in relational environment.

#index 960355
#* The TopX DB&IR engine
#@ Martin Theobald;Ralf Schenkel;Gerhard Weikum
#t 2007
#c 5
#% 643566
#% 845359
#% 1016183
#! This paper proposes a demo of the TopX search engine, an extensive framework for unified indexing, querying, and ranking of large collections of unstructured, semistructured, and structured data. TopX integrates efficient algorithms for top-k-style ranked retrieval with powerful scoring models for text and XML documents, as well as dynamic and self-tuning query expansion based on background ontologies.

#index 960356
#* Supporting entity search: a large-scale prototype search engine
#@ Tao Cheng;Xifeng Yan;Kevin Chen-Chuan Chang
#t 2007
#c 5
#! As the Web has evolved into a data-rich repository, with the standard page view," current search engines are increasingly inadequate. While we often search for various data "entities" (e.g. phone number, paper PDF, date), today's engines only take us indirectly to pages. Therefore, we propose the concept of entity search, a significant departure from traditional document retrieval. Towards our goal of supporting entity search, in the WISDM project at UIUC we build and evaluate our prototype search engine over a 2TB Web corpus. Our demonstration shows the feasibility and promise of a large-scale system architecture to support entity search.

#index 960357
#* Information discovery in loosely integrated data
#@ Heasoo Hwang;Andrey Balmin;Hamid Pirahesh;Berthold Reinwald
#t 2007
#c 5
#% 893145
#% 1016176
#! We model heterogeneous data sources with cross references, such as those crawled on the (enterprise) web, as a labeled graph with data objects as typed nodes and references or links as edges. Given the labeled data graph, we introduce flexible and efficient querying capabilities that go beyond existing capabilities by additionally discovering meaningful relationships between objects that satisfy keyword and/or structured query filters. We introduce the relationship search operator that exploits the link structure between data objects to rank objects related to the result of a filter. We implement the search operator using the ObjectRank [1] algorithm that uses the random surfer model. We study several alternatives for constructing summary graphs for query results that consist of individual and aggregate nodes that are somehow linked to qualifying result nodes. Some of the summary graphs are useful for presenting query results to the user, while others could be used to evaluate subsequent queries efficiently without considering all the nodes and links in the original data graph.

#index 960358
#* Managing information quality in e-science: the qurator workbench
#@ Paolo Missier;Suzanne M. Embury;Mark Greenwood;Alun Preece;Binling Jin
#t 2007
#c 5
#% 893169
#% 903332
#! Data-intensive e-science applications often rely on third-party data found in public repositories, whose quality is largely unknown. Although scientists are aware that this uncertainty may lead to incorrect scientific conclusions, in the absence of a quantitative characterization of data quality properties they find it difficult to formulate precise data acceptability criteria. We present an Information Quality management workbench, called Qurator, that supports data experts in the specification of personal quality models, and lets them derive effective criteria for data acceptability. The demo of our working prototype will illustrate our approach on a real e-science workflow for a bioinformatics application.

#index 960359
#* Integrating and querying taxonomies with quest in the presence of conflicts
#@ Yan Qi;K. Selçuk Candan;Maria Luisa Sapino;Keith W. Kintigh
#t 2007
#c 5
#% 663
#% 260021
#% 479783
#% 480134
#! We present the QUery-driven Exploration of Semistructured dataand meta-data with conflicTs and partial knowledge (QUEST) system for supporting the integration of scientific data and taxonomies in the presence of misalignments and conflicts. QUEST relies on a novel constraint-based data model that captures both value and structural conflicts and enables researchers to observe and resolve such misalignments in the integrated data by considering the context provided by the data requirements of given research questions.

#index 960360
#* Assisted querying using instant-response interfaces
#@ Arnab Nandi;H. V. Jagadish
#t 2007
#c 5
#% 232476
#% 793417
#% 1016135
#! We demonstrate a novel query interface that enables users to construct a rich search query without any prior knowledge of the underlying schema or data. The interface, which is in the form of a single text input box, interacts in real-time with the users as they type, guiding them through the query construction. We discuss the issues of schema and data complexity, result size estimation, and query validity; and provide novel approaches to solving these problems. We demonstrate our query interface on two popular applications; an enterprise-wide personnel search, and a biological information database.

#index 960361
#* Highly distributed XQuery with DXQ
#@ Mary F. Fernàndez;Trevor Jim;Kristi Morton;Nicola Onose;Jérôme Siméon
#t 2007
#c 5
#% 325745
#% 723448
#% 740844
#% 874978
#% 1050850
#% 1849766
#! Many modern applications, from Grid computing to RSS handling, need to support data processing in a distributed environment. Currently, most such applications are implemented using a general purpose programming language, which can be expensive to maintain, hard to configure and modify, and require hand optimization of the distributed data processing operations. We present Distributed XQuery (DXQ), a simple, yet powerful, extension of XQuery to support distributed applications. This extension includes the ability to deploy networks of XQuery servers, to remotely invoke XQuery programs on those servers, and to ship code between servers. Our demonstration presents two applications implemented in DXQ: the resolution algorithm of DNS, the Domain Name System, and the Narada overlay-network protocol. We show that our system can flexibly accommodate different patterns of distributed computation and present some simple but essential distributed optimizations.

#index 960362
#* A SQL: 1999 code generator for the pathfinder xquery compiler
#@ Torsten Grust;Manuel Mayr;Jan Rittinger;Sherif Sakr;Jens Teubner
#t 2007
#c 5
#% 824750
#% 875010
#% 960318
#% 994015
#% 1016150
#! The Pathfinder XQuery compiler has been enhanced by a new code generator that can target any SQL:1999-compliant relational database system(RDBMS). This code generator marks an important next step towards truly relational XQuery processing, a branch of database technology that aims to turn RDBMSs into highly efficient XML and XQuery processors without the need to invade the relational database kernel. Pathfinder, a retargetable front-end compiler, translates input XQuery expressions into DAG-shaped relational algebra plans. The code generator then turns these plans into sequences of either SQL:1999 statements or view definitions which jointly implement the (sometimes intricate) XQuery semantics. In a sense, this demonstration thus lets relational algebra and SQL swap their traditional roles in database query processing. The result is a code generator that (1) supports an almost complete dialect of XQuery, (2) can target any RDBMS with a SQL:1999 language interface, and (3) exhibits quite promising performance characteristics when run against high-volume XML data as well as complex XQuery expressions.

#index 960363
#* DaNaLIX: a domain-adaptive natural language interface for querying XML
#@ Yunyao Li;Ishan Chaudhuri;Huahai Yang;Satinder Singh;H. V. Jagadish
#t 2007
#c 5
#! We present DaNaLIX, a prototype domain-adaptive natural language interface for querying XML. Our system is an extension of NaLIX, a generic natural language interface for querying XML. While retaining the portability of a purely generic system like NaLIX, DaNaLIX can exploit domain knowledge, whenever available, to its advantage for query translation. More importantly, in DaNaLIX such domain knowledge does not have to be pre-defined; instead it can be automatically obtained from the interactions between a user and the system. In this demonstration, we describe the overall architecture of DaNaLIX. We also demonstrate how a generic system like DaNaLIX can take advantage of domain knowledge to improve its usability and query translation accuracy. In addition, we show DaNaLIX still possesses the portability of a generic system by using data collections from three different domains. Finally, we present how domain knowledge can be obtained through user interactions in an automatic fashion.

#index 960364
#* Event processing using database technology
#@ K. Mani Chandy;Dieter Gawlick
#t 2007
#c 5
#! This tutorial deals with applications that help systems and individuals respond to critical conditions in their environments. The identification of critical conditions requires correlating vast amounts of data within and outside an enterprise. Conditions that signal opportunities or threats are defined by complex patterns of data over time, space and other attributes. Systems and individuals have models (expectations) of behaviors of their environments, and applications notify them when reality - as determined by measurements and estimates - deviate from their expectations. Components of event systems are also sent information to validate their current models and when specific responses are required. Valuable information is that which supports or contradicts current expectations or that which requires an action on the part of the receiver. A major problem today is information overload; this problem can be solved by identifying what information is critical, complementing existing pull technology with sophisticated push technology, and filtering out non-critical data.

#index 960365
#* Provenance in databases
#@ Peter Buneman;Wang-Chiew Tan
#t 2007
#c 5
#% 318704
#% 378401
#% 462072
#% 803468
#% 825661
#% 864469
#% 874971
#% 875015
#% 893095
#% 893167
#% 900797
#% 976987
#% 1655437
#% 1661440
#% 1728174
#! The provenance of data has recently been recognized as central tothe trust one places in data. It is also important to annotation, todata integration and to probabilistic databases. Three workshops havebeen held on the topic, and it has been the focus of several researchprojects and prototype systems. This tutorial will attempt to providean overview of research in provenance in databases with a focus onrecent database research and technology in this area. This tutorialis aimed at a general database research audience and at people whowork with scientific data.

#index 960366
#* Mining large graphs and streams using matrix and tensor tools
#@ Christos Faloutsos;Tamara G. Kolda;Jimeng Sun
#t 2007
#c 5
#! Coevolving streams of numerical measurements, as well astime evolving graphs, can well be represented as tensors. Here we review the fundamental matrix and tensors tools forthe analysis and mining of large scale streams and graphs.

#index 960367
#* Mobile and embedded databases
#@ Anil Nori
#t 2007
#c 5
#! Recent advances in device technology and connectivity have paved the way for next generation applications that are data-driven, whose data can reside anywhere, can be accessed at any time, from any client. Also, advances in memory technology are driving the capacities of RAM and Flash higher, and their costs down. These trends lead to applications that are mobile, embedded, and data-centric. This tutorial presents an overview of the mobile and embedded database systems and their applications.

#index 960368
#* Streaming in a connected world: querying and tracking distributed data streams
#@ Graham Cormode;Minos Garofalakis
#t 2007
#c 5
#% 273682
#% 338380
#% 654443
#% 654488
#% 654497
#% 654510
#% 745442
#% 751027
#% 800582
#% 801695
#% 806214
#% 809258
#% 810009
#% 810031
#% 810095
#% 816392
#% 821921
#% 824652
#% 864435
#% 864444
#% 874908
#% 874994
#% 874995
#% 1016155
#% 1016178

#index 960369
#* System design issues in sensor databases
#@ Qiong Luo;Hejun Wu
#t 2007
#c 5
#% 309433
#% 429193
#% 433321
#% 654482
#% 731087
#% 745442
#% 751050
#% 785975
#% 788219
#% 797199
#% 800503
#% 800505
#% 810031
#% 810840
#% 863025
#% 874982
#% 874983
#% 874984
#% 889832
#% 906934
#% 1015599
#% 1769774
#% 1831268
#! In-network sensor query processing systems (ISQPs), or sensor databases, have been developed to acquire, process and aggregate data from wireless sensor networks (WSNs). Because WSNs are resource-limited and involve multiple layers of embedded software, the system design issues have a significant impact on the performance of sensor databases. Therefore, we propose this tutorial to study the state of the art on these issues with a focus on their interaction with query processing techniques. Our goal is to present the challenges and efforts in developing holistic, efficient ISQPs. Specifically, we will cover architectural design, scheduling, data-centric routing, and wireless medium access control. This tutorial is intended for database researchers who are interested in sensor networks.

#index 966969
#* Proceedings of the 6th ACM international workshop on Data engineering for wireless and mobile access
#@ Peter Scheuermann;Wang-Chien Lee;George Samaras;Dik Lun Lee
#t 2007
#c 5
#! Welcome to MobiDE'07 in Beijing! This is the sixth of a successful series of workshops that aim to bring together the data management, wireless networking, and mobile computing communities. This year's workshop, held in conjunction with SIGMOD 2007, continues its tradition of being the premier forum for researchers and technologists to discuss the state-of-the-art, present research results, experiences and contributions, and setting future directions in data management for mobile and wireless access. It is very fitting to have MOBIDE 2007 in Beijing, one year before the Olympic capital is engaged in a very ambitious process of enabling extensive tourist information to mobile users through the Olympia 2008 project. The workshop program presents new and controversial research ideas so as to foster interaction among researchers from around the world. The call for papers attracted 28 submissions. All submissions were reviewed by at least 3 members of the Program Committee. Many of these submissions were of very high quality, making the selection process quite competitive. As a result, 9 submissions were accepted as full papers, and 2 submissions that reported on promising works in progress were accepted as poster papers.

#index 976983
#* Proceedings of the twenty-sixth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems
#@ Phokion Kolaitis;Leonid Libkin
#t 2007
#c 5
#! This volume contains the proceedings of the Twenty-Sixth ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems (PODS 2007), held in Beijing, China, on June 11-13, 2007, in conjunction with the 2007 ACM SIGMOD International Conference on Management of Data. The proceedings include papers based on the keynote address by Dan Suciu and two invited tutorials by Nicole Schweikardt and Jan Van den Bussche, and 28 contributed papers that were selected by the Program Committee from 187 submissions. Most of these papers are extended abstracts and, while they have been read by the program committee, they have not been formally refereed. It is anticipated that many of them will appear in more polished form in scientific journals. The program committee selected the paper "Generalized Hypertree Decompositions: NP-Hardness and Tractable Variants" by Georg Gottlob, Zoltan Miklos and Thomas Schwentick as the recipient of the Best Paper Award, and the paper "XML Transformation by Tree-Walking Transducers with Invisible Pebbles" by Joost Engelfriet, Hendrik Jan Hoogeboom and Bart Samwel as the recipient of the Best Newcomer Award. Warmest congratulations to the authors. I thank all authors who submitted papers to the conference, and the members of the program committee for the enormous amount of work they have done. The program committee did not meet in person, but carried out extensive discussion during the electronic PC meeting.

#index 976984
#* Management of probabilistic data: foundations and challenges
#@ Nilesh Dalvi;Dan Suciu
#t 2007
#c 5
#% 44876
#% 68244
#% 136358
#% 183411
#% 191616
#% 201889
#% 215225
#% 216970
#% 235023
#% 265692
#% 333946
#% 388024
#% 442830
#% 480496
#% 527514
#% 571102
#% 598376
#% 654467
#% 751818
#% 754068
#% 765449
#% 765455
#% 793254
#% 800547
#% 809239
#% 810120
#% 824718
#% 845350
#% 864394
#% 864417
#% 873104
#% 874876
#% 874887
#% 893089
#% 893102
#% 893121
#% 893164
#% 893167
#% 893168
#% 974351
#% 977013
#% 991156
#% 993980
#% 1016178
#% 1269495
#% 1289473
#% 1661436
#% 1661439
#% 1688305
#% 1700137
#% 1728031
#! Many applications today need to manage large data sets with uncertainties. In this paper we describe the foundations of managing data where the uncertainties are quantified as probabilities. We review the basic definitions of the probabilistic data model, present some fundamental theoretical result for query evaluation on probabilistic databases, and discuss several challenges, open problems, and research directions.

#index 976985
#* Generalized hypertree decompositions: np-hardness and tractable variants
#@ Georg Gottlob;Zoltan Miklos;Thomas Schwentick
#t 2007
#c 5
#% 1675
#% 93660
#% 101944
#% 139180
#% 195529
#% 278054
#% 289424
#% 303886
#% 331899
#% 339937
#% 384978
#% 427161
#% 599549
#% 643572
#% 801688
#% 847068
#% 1069810
#% 1250546
#% 1289362
#! The generalized hypertree width GHW(H) of a hypergraph H is a measure of its cyclicity. Classes of conjunctive queries or constraint satisfaction problems whose associated hypergraphs have bounded GHW are known to be solvable in polynomial time. However,it has been an open problem for several years if for a fixed constant k and input hypergraph H it can be determined in polynomial time whether GHW(H)

#index 976986
#* Queries determined by views: pack your views
#@ Maarten Marx
#t 2007
#c 5
#% 207489
#% 299945
#% 378410
#% 430194
#% 643572
#% 665859
#% 809238
#% 822573
#% 1661430
#! A query Q is determined by a set of views V if, whenever V (I1) = V (I2) for two database instances I1, I2 then also Q(I1) = Q(I2). Does this imply that Q can be rewritten as a query Q0 that only uses the views V?. For first-order (FO) queries and view definitions over possibly infinite databases, the answer is yes, as follows from old results of Beth and Craig. We say that FO is complete for FO-to-FO rewritings. However, Nash, Segoufin and Vianu (2007) prove that if the query and the view definitions are given by conjunctive queries, then it might not be possible to formulate Q' as a conjunctive query. In other words, CQ is not complete for CQ-to-CQ rewritings. Here we consider queries and view definitions in the packed fragment (PF) of first-order logic. This is a generalization of the guarded fragment, a fragment of particular interest to database theory. Gottlob et.al. 2002 show that the guarded conjunctive queries are exactly the acyclic queries. Leinders et.al. 2005 characterize the entire guarded fragment by the semijoin algebra. We show that for both finite and unrestricted databases, PF is complete for PF-to-PF rewritings. The same holds for packed (unions of) conjunctive queries. In both cases, we provide algorithms for testing whether a query is determined by a set of views, and for actually rewriting Q to Q'. To compare: these problems are undecidable for full FO, and still open for conjunctive queries.

#index 976987
#* Provenance semirings
#@ Todd J. Green;Grigoris Karvounarakis;Val Tannen
#t 2007
#c 5
#% 663
#% 130189
#% 137770
#% 181038
#% 190638
#% 194290
#% 215225
#% 228817
#% 230551
#% 235023
#% 248033
#% 289266
#% 318704
#% 340691
#% 368248
#% 384978
#% 464372
#% 480289
#% 599549
#% 715288
#% 864394
#% 893095
#% 893167
#% 1728680
#! We show that relational algebra calculations for incomplete databases, probabilistic databases, bag semantics and why-provenance are particular cases of the same general algorithms involving semirings. This further suggests a comprehensive provenance representation that uses semirings of polynomials. We extend these considerations to datalog and semirings of formal power series. We give algorithms for datalog provenance calculation as well as datalog evaluation for incomplete and probabilistic databases. Finally, we show that for some semirings containment of conjunctive queries is the same as for standard set semantics.

#index 976988
#* Machine models and lower bounds for query processing
#@ Nicole Schweikardt
#t 2007
#c 5
#% 101797
#% 136540
#% 238182
#% 248023
#% 278835
#% 293705
#% 293720
#% 341100
#% 378388
#% 378392
#% 570879
#% 576108
#% 731408
#% 761484
#% 785130
#% 791182
#% 801685
#% 809253
#% 809255
#% 847113
#% 874901
#% 894646
#% 958231
#% 963308
#% 982759
#% 1015275
#% 1376720
#% 1661445
#% 1661446
#% 1718398
#! This paper gives an overview of recent work on machine models for processing massive amounts of data. The main focus is on generalizations of the classical data stream model where, apart from an "internal memory" of limited size, also a number of (potentially huge) streams may be used as "external memory devices".

#index 976989
#* Decision trees for entity identification: approximation algorithms and hardness results
#@ Venkatesan T. Chakaravarthy;Vinayaka Pandit;Sambuddha Roy;Pranjal Awasthi;Mukesh Mohania
#t 2007
#c 5
#% 232752
#% 318028
#% 331442
#% 420084
#% 685152
#% 781735
#% 980567
#% 1394120
#! We consider the problem of constructing decision trees for entity identification from a given relational table. The input is a table containing information about a set of entities over a fixed set of attributes and a probability distribution over the set of entities that specifies the likelihood of the occurrence of each entity. The goal is to construct a decision tree that identifies each entity unambiguously by testing the attribute values such that the average number of tests is minimized. This classical problem finds such diverse applications as efficient fault detection, species identification in biology, and efficient diagnosis in the field of medicine. Prior work mainly deals with the special case where the input table is binary and the probability distribution over the set of entities is uniform. We study the general problem involving arbitrary input tables and arbitrary probability distributions over the set of entities. We consider a natural greedy algorithm and prove an approximation guarantee of O(rK • log N), where N is the number of entities and K is the maximum number of distinct values of an attribute. The value rK is a suitably defined Ramsey number, which is at most log K. We show that it is NP-hard to approximate the problem within a factor of Ω(log N), even for binary tables (i.e. K=2). Thus, for the case of binary tables, our approximation algorithm is optimal up to constant factors (since r2=2). In addition, our analysis indicates a possible way of resolving a Ramsey-theoretic conjecture by Erdos.

#index 976990
#* XML transformation by tree-walking transducers with invisible pebbles
#@ Joost Engelfriet;Hendrik Jan Hoogeboom;Bart Samwel
#t 2007
#c 5
#% 3997
#% 17980
#% 279367
#% 427874
#% 483543
#% 483878
#% 630965
#% 743615
#% 801686
#% 809259
#% 821563
#% 850728
#% 874910
#% 907142
#% 993939
#% 1395000
#% 1700119
#% 1703199
#! The pebble tree automaton and the pebble tree transducer are enhanced by additionally allowing an unbounded number of "invisible" pebbles (as opposed to the usual ("visible" ones). The resulting pebble tree automata recognize the regular tree languages (i.e., can validate all generalized DTD's) and hence can find all matches of MSO definable n-ary patterns. Moreover, when viewed as a navigational device, they lead to an XPath-like formalism that has a path expression for every MSO definable binary pattern. The resulting pebbletree transducers can apply arbitrary MSO definable tests to (the observable part of) their configurations, they (still) have a decidable typechecking problem, and they can model the recursion mechanism of XSLT. The time complexity ofthe typechecking problem for conjunctive queries that use MSO definable binary patterns can often be reduced through the use of invisible pebbles.

#index 976991
#* The complexity of query containment in expressive fragments of XPath 2.0
#@ Balder ten Cate;Carsten Lutz
#t 2007
#c 5
#% 289287
#% 378393
#% 465051
#% 473117
#% 487257
#% 494344
#% 578964
#% 742056
#% 776194
#% 801686
#% 804841
#% 809236
#% 824798
#% 826029
#% 839147
#% 848763
#% 850728
#% 874910
#% 894435
#% 1661435
#! Query containment has been studied extensively for fragments of XPath 1.0. For instance, the problem is known to be ExpTime-complete for CoreXPath, the navigational core of XPath 1.0. Much less is known about query containment in (fragments of) the richer language XPath 2.0. In this paper, we consider extensions of CoreXPath with the following operators, which are all part of XPath 2.0 (except the last): path intersection, path equality, path complementation, for-loops, and transitive closure. For each combination of these operators, we determine the complexity of query containment, both with and without DTDs. It turns out to range from ExpTime (for extensions with path equality) and 2-ExpTime (for extensions with path intersection) to non-elementary (for extensions with path complementation or for-loops). In almost all cases, adding transitive closure on top has no further impact on the complexity. We also investigate the effect of dropping the upward and/or sibling axes, and show that this sometimes leads to a reduction in complexity.Since the languages we study include negation and conjunction infilters, our complexity results can equivalently be stated in terms ofsatisfiability.We also analyze the above languages in terms of succinctness.

#index 976992
#* Expressiveness and complexity of xml publishing transducers
#@ Wenfei Fan;Floris Geerts;Frank Neven
#t 2007
#c 5
#% 161292
#% 230142
#% 342829
#% 378394
#% 384978
#% 401124
#% 411759
#% 572305
#% 575379
#% 598376
#% 630965
#% 765443
#% 778122
#% 806215
#% 809235
#% 1670117
#! A number of languages have been developed for specifying XML publishing, i.e., transformations of relational data into XML trees. These languages generally describe the behaviors of a middleware controller that builds an output tree iteratively, issuing queries to a relational source and expanding the tree with the query results at each step. To study the complexity and expressive power of XML publishing languages, this paper proposes a notion of publishing transducers. Unlike automata for querying XML data, a publishing transducer generates a new XML tree rather than performing a query on an existing tree. We study a variety of publishing transducers based on what relational queries a transducer can issue, what temporary stores a transducer can use during tree generation, and whether or not some tree nodes are allowed to be virtual, i.e., excluded from the output tree. We first show how existing XML publishing languages can be characterized by such transducers. We then study the members ip, emptiness and equivalence problems for various classes of transducers and existing publishing languages. We establish lower and upper bounds, all matching except one, ranging from PTIME to undecidable. Finally, we investigate the expressive power of these transducers and existing languages. We show that when treated as relational query languages, different classes of transducers capture either complexity classes (e.g., PSPACE) or fragments of datalog (e.g., linear datalog). For tree generation, we establish connections between publishing transducers and logical transductions.

#index 976993
#* Maintaining bernoulli samples over evolving multisets
#@ Rainer Gemulla;Wolfgang Lehner;Peter J. Haas
#t 2007
#c 5
#% 1331
#% 248812
#% 379444
#% 479931
#% 480805
#% 765426
#% 808428
#% 824653
#% 864393
#% 893137
#! Random sampling has become a crucial component of modern data management systems. Although the literature on database sampling is large, there has been relatively little work on the problem of maintaining a sample in the presence of arbitrary insertions and deletions to the underlying dataset. Most existing maintenance techniques apply either to the insert-only case or to datasets that do not contain duplicates. In this paper, we provide a scheme that maintains a Bernoulli sample of an underlying multiset in the presence of an arbitrary stream of updates, deletions, and insertions. Importantly, the scheme never needs to access the underlying multiset. Such Bernoulli samples are easy to manipulate, and are well suited to parallel processing environments. Our method can be viewed as an enhancement of the "counting sample" scheme developed by Gibbons and Matias for estimating the frequency of highly frequent items. We show how the "tracking counters" used by our maintenance scheme can be exploited to estimate population frequencies, sums, and averages in an unbiased manner, with lower variance than the usual estimators based on a Bernoulli sample. The number of distinct items in the multiset can also be estimated without bias. Finally, we discuss certain problems of subsampling and merging that a rise in systems with limited memory resources or distributed processing, respectively.

#index 976994
#* Finding near neighbors through cluster pruning
#@ Flavio Chierichetti;Alessandro Panconesi;Prabhakar Raghavan;Mauro Sozio;Alessandro Tiberi;Eli Upfal
#t 2007
#c 5
#% 24108
#% 46809
#% 145895
#% 227937
#% 227939
#% 228097
#% 232766
#% 249321
#% 280452
#% 282552
#% 306893
#% 317933
#% 415033
#% 427199
#% 438137
#% 480304
#% 481460
#% 481956
#% 654466
#! Finding near(est) neighbors is a classic, difficult problem in data management and retrieval, with applications in text and image search,in finding similar objects and matching patterns. Here we study cluster pruning, an extremely simple randomized technique. During preprocessing we randomly choose a subset of data points to be leaders the remaining data points are partitioned by which leader is the closest. For query processing, we find the leader(s) closest to the query point. We then seek the nearest neighbors for the query point among only the points in the clusters of the closest leader(s). Recursion may be used in both preprocessing and in search. Such schemes seek approximate nearest neighbors that are "almost as good" as the nearest neighbors. How good are these approximations and how much do they save in computation. Our contributions are: (1) we quantify metrics that allow us to study the tradeoff between processing and the quality of the approximate nearest neighbors; (2) we give rigorous theoretical analysis of our schemes, under natural generative processes (generalizing Gaussian mixtures) for the data points; (3) experiments on both synthetic data from such generative processes, as well as on from a document corpus, confirming that we save orders of magnitude in query processing cost at modest compromises in the quality of retrieved points. In particular, we show that p-spheres, a state-of-the-art solution, is outperformed by our simple scheme whether the data points are stored in main or in external memo.

#index 976995
#* CWA-solutions for data exchange settings with target dependencies
#@ Andre Hernich;Nicole Schweikardt
#t 2007
#c 5
#% 129217
#% 264858
#% 465053
#% 599549
#% 801691
#% 806215
#% 809235
#% 809239
#% 823106
#% 826032
#% 874879
#% 874880
#% 874881
#% 874882
#! Data exchange deals with the following problem: given an instance over a source schema, a specification of the relationship between the source and the target,and dependencies on the target, construct an instance over a target schema that satisfies the given relationships and dependencies. Recently - for data exchange settings without target dependencies - Libkin (PODS'06) introduced a new concept of solutions based on the closed world assumption (so calledCWA-solutions), and showed that, in some respects, this new notion behaves better than the standard notion of solutions considered in previous papers on data exchange. The present paper extends Libkin's notion of CWA-solutions to data exchange settings with target dependencies. We show that, when restricting attention to data exchange settings with weakly acyclic target dependencies, this new notion behaves similarly as before: the core is the unique "minimal" CWA-solution, and computing CWA-solutions as well as certain answers to positive queries is possible in polynomial time and can be PTIME-hard. However, there may be more than one "maximal" CWA-solution. And going beyond the class of positive queries, we obtain that there are conjunctive queries with (just) one inequality, for which evaluating the certain answers is coNP-hard. Finally, we consider the EXISTENCE-OF-CWA-SOLUTIONS problem: while the problem is tractable for data exchange settings with weakly acyclic target dependencies, it turns out to be undecidable for general data exchange settings. As a consequence, we obtain that also the EXISTENCE-OF-UNIVERSAL-SOLUTIONS problem is undecidable in genera.

#index 976996
#* Quasi-inverses of schema mappings
#@ Ronald Fagin;Phokion G. Kolaitis;Lucian Popa;Wang-Chiew Tan
#t 2007
#c 5
#% 378409
#% 562454
#% 765540
#% 809239
#% 809249
#% 826032
#% 850730
#% 874881
#% 1015302
#! Schema mappings are high-level specifications that describe the relationship between two database schemas. Two operators on schema mappings, namely the composition operator and the inverse operator, are regarded as especially important. Progress on the study of the inverse operator was not made until very recently, as even finding the exact semantics of this operator turned out to be a fairly delicate task. Furthermore, this notion is rather restrictive, since it is rare that a schema mapping possesses an inverse. In this paper, we introduce and study the notion of a quasi-inverse of a schema mapping. This notion is a principled relaxation of the notion of an inverse of a schema mapping; intuitively, it is obtained from the notion of an inverse by not differentiating between instances that are equivalent for data-exchange purposes. For schema mappings specified by source-to-target tuple-generating dependencies (s-t tgds), we give a necessary and sufficient combinatorial condition for the existence of a quasi-inverse, and then use this condition to obtain both positive and negative results about the existence of quasi-inverses. In particular, we show that every LAV (local-as-view) schema mappinghas a quasi-inverse, but that there are schema mappings specified by full s-t tgds that have no quasi-inverse. After this, we study the language needed to express quasi-inverses of schema mappings specifiedby s-t tgds, and we obtain a complete characterization. We also characterize the language needed to express inverses of schema mappings, and thereby solve a problem left open in the earlier study of the inverse operator. Finally, we show that quasi-inverses can be used in many cases to recover the data that was exported by the original schemamapping when performing data exchange.

#index 976997
#* On reconciling data exchange, data integration, and peer data management
#@ Giuseppe De Giacomo;Domenico Lembo;Maurizio Lenzerini;Riccardo Rosati
#t 2007
#c 5
#% 248038
#% 378409
#% 384978
#% 572311
#% 576116
#% 723449
#% 749088
#% 765446
#% 801691
#% 801692
#% 806215
#% 809235
#% 809239
#% 809247
#% 809248
#% 826032
#% 850730
#% 874879
#% 874880
#% 874882
#% 874914
#% 893089
#% 1015302
#% 1223259
#% 1279213
#% 1673662
#! Data exchange and virtual data integration have been the subject of several investigations in the recent literature. At the same time, the notion of peer data management has emerged as a powerful abstraction of many forms of flexible and dynamic data-centere ddistributed systems. Although research on the above issues has progressed considerably in the last years, a clear understanding on how to combine data exchange and data integration in peer data management is still missing. This is the subject of the present paper. We start our investigation by first proposing a novel framework for peer data exchange, showing that it is a generalization of the classical data exchange setting. We also present algorithms for all the relevant data exchange tasks, and show that they can all be done in polynomial time with respect to data complexity. Based on the motivation that typical mappings and integrity constraints found in data integration are not captured by peer data exchange, we extend the framework to incorporate these features. One of the main difficulties is that the constraints of this new class are not amenable to materialization. We address this issue by resorting to a suitable combination of virtual and materialized data exchange, showing that the resulting framework is a generalization of both classical data exchange and classical data integration, and that the new setting incorporates the most expressive types of mapping and constraints considered in the two contexts. Finally, we present algorithms for all the relevant data management tasks also in the new setting, and show that, again, their data complexity is polynomial.

#index 976998
#* A crash course on database queries
#@ Jan Van den Bussche;Dirk Van Gucht;Stijn Vansummeren
#t 2007
#c 5
#% 22947
#% 103830
#% 123121
#% 140407
#% 168061
#% 189868
#% 197082
#% 205243
#% 210352
#% 241166
#% 247458
#% 294600
#% 299942
#% 300384
#% 318509
#% 342957
#% 346654
#% 384978
#% 411622
#% 427214
#% 464724
#% 479465
#% 482093
#% 562461
#% 572328
#% 575379
#% 598376
#% 643569
#% 711523
#% 722731
#% 736843
#% 737188
#% 745520
#% 799999
#% 801670
#% 805067
#% 805821
#% 809237
#% 809259
#% 814652
#% 826034
#% 910590
#% 912238
#% 924134
#% 943615
#% 1001598
#% 1661442
#% 1661443
#% 1728330
#! Complex database queries, like programs in general, can "crash", i.e., can raise runtime errors. We want to avoid crashes without losing expressive power, or we want to correctly predict the absence of crashes. We show how concepts and techniques from programming language theory, notably type systems and reflection, can be adaptedto this end. Of course, the specific nature of database queries (asopposed to general programs), also requires some new methods, andraises new questions.

#index 976999
#* The complexity of reasoning about pattern-based XML schemas
#@ Gjergji Kasneci;Thomas Schwentick
#t 2007
#c 5
#% 6242
#% 262724
#% 545382
#% 742566
#% 805911
#% 809235
#% 942354
#% 1700122
#% 1718404
#! In a recent paper, Martens et al. introduced a specification mechanism for XML tree languages, based on rules of the form (r,s), wherer, s are regular expressions. Sets of such rules can be interpreted in an existential or a universal fashion. An XML tree is existentially valid with respect to a rule set, if for each node there is a rule such that the root path of the node matches r and the children sequence of the node matchess. It is universally valid if each node matching r also matchess. This paper investigates the complexity of reasoning about such rule sets, in particular the satisfiability and the implication problem. Whereas, in general these reasoning problems are complete for EXPTIME, two important fragments are identified with PSPACE and PTIME complexity, respectively.

#index 977000
#* Monadic datalog over finite structures with bounded treewidth
#@ Georg Gottlob;Reinhard Pichler;Fang Wei
#t 2007
#c 5
#% 36683
#% 49315
#% 73005
#% 93660
#% 101944
#% 125557
#% 181220
#% 219474
#% 384978
#% 401124
#% 427161
#% 499513
#% 598376
#% 733595
#% 778122
#% 857282
#% 874889
#% 1250546
#% 1972413
#! Bounded treewidth and Monadic Second Order (MSO) logic have proved to be key concepts in establishing fixed-para-meter tractability results. Indeed, by Courcelle's Theorem we know: Any property of finite structures, which is expressible by an MSO sentence, can be decided in linear time (data complexity) if the structures have bounded treewidth. In principle, Courcelle's Theorem can be applied directly to construct concrete algorithms by transforming the MSO evaluation problem into a tree language recognition problem. The latter can then be solved via a finite tree automaton (FTA). However, this approach has turned out to be problematical, since even relatively simple MSO formulae may lead to a "state explosion" of the FTA. In this work we propose monadic datalog (i.e., data log where all intentional predicate symbols are unary) as an alternative method to tackle this class of fixed-parameter tractable problems. We show that if some property of finite structures is expressible in MSO then this property can also be expressed by means of a monadic datalog program over the structure plus the treedecomposition. Moreover, we show that the resulting fragment of datalogcan be evaluated in linear time (both w.r.t. the program size and w.r.t. the data size). This new approach is put to work by devising a new algorithm for the PRIMALITY problem (i.e., testing if some attribute in a relational schema is part of a key). We also report on experimental results with a prototype implementation.

#index 977001
#* Index-based multidimensional array queries: safety and equivalence
#@ Rona Machlin
#t 2007
#c 5
#% 36181
#% 39850
#% 120639
#% 123118
#% 137862
#% 210184
#% 223781
#% 237182
#% 248021
#% 248034
#% 248863
#% 273696
#% 273698
#% 273913
#% 335725
#% 435138
#% 562299
#% 563512
#% 730827
#% 803609
#! We propose a new multidimensional array query model giving array bounds and other shape-related metadata a central role. Arrays are treated as shaped maps from indices to values. Schemas are augmented by shape constraints. Queries also have shape preconditions. Within this framework, we introduce the index-based array queries expressing index reorganizations and value summarizations. We define them via adeclarative, rule-based language with shape-membership constraints inits rule bodies and subscripting and aggregation in its rule heads. We explore safety (including bounds analysis) and query equivalence for various subclasses divided according to the aggregator type, whether we allow disjunctions, and whether we allow (limited) Presburger arithmetic in index and shape terms. We show safety istractable in the nonarithmetic cases, while state safety remains in P in the arithmetic ones. We show that, for a class of monoid-based setand bag aggregators, equivalence reduces to equivalence of index-cores- core queries collecting array indices rather than values. Forset-aggregator queries, we give complete characterizations of equivalence in terms of containment maps and show the equivalenceproblems are in P in the nonarithmetic, conjunctive case and in coNP in all others.

#index 977002
#* Non-linear prefixes in query languages
#@ Antonio Badia;Stijn Vansummeren
#t 2007
#c 5
#% 36683
#% 118781
#% 131495
#% 210183
#% 341404
#% 342387
#% 362199
#% 384978
#% 421931
#% 463894
#% 464525
#% 479460
#% 582129
#% 706675
#! In first order logic there are two main extensions to quantification: generalized quantifiers and non-linear prefixes. While generalized quantifiers have been explored from a database perspective, non-linear prefixes have not-most likely because of complexity concerns. In this paper we first illustrate the usefulness of non-linear prefixes in query languages by means of example queries. We then introduce the subject formally, distinguishing between two forms of non-linearity: branching and cumulation. To escape complexity concerns, we focus on monadic quantifiers. In this context, we show that branching does not extend the expressive power of first order logic when it is interpreted over finite models, while cumulation does not extend the expressive power when it is interpreted over bounded models. Branching and cumulation do, however, allow us to formulate some queries in a succinct and elegant manner. When branching and cumulation are interpreted over infinite models, we show that the resulting language can be embedded in an infinitary logic proposed by Libkin. We also discuss non-linear prefixes from an algorithmic point of view.

#index 977003
#* Reasoning about XML update constraints
#@ Bogdan Cautis;Serge Abiteboul;Tova Milo
#t 2007
#c 5
#% 292677
#% 299944
#% 333858
#% 333979
#% 378393
#% 384978
#% 398752
#% 465061
#% 489284
#% 514027
#% 566391
#% 630971
#% 733268
#% 742056
#% 804841
#% 809235
#% 826031
#% 865387
#% 874877
#% 877241
#% 994000
#% 1705007
#! We introduce in this paper a class of constraints for describing howan XML document can evolve, namely XML update constraints. For these constraints, we study the implication problem, giving algorithms and complexity results for constraints of varying expressive power. Besides classical constraint implication, we also consider an instance-based approach. More precisely, we study implication with respect to a current tree instance, resulting from a series of unknown updates. The main motivation of our work is reasoning about data integrity under update restrictions in contexts where owners may lose control over their data, such as in publishing or exchange.

#index 977004
#* Polynomial time fragments of XPath with variables
#@ Emmanuel Filiot;Joachim Niehren;Jean-Marc Talbot;Sophie Tison
#t 2007
#c 5
#% 1195
#% 23614
#% 401124
#% 655430
#% 814648
#% 821610
#% 850728
#% 986480
#% 1661435
#% 1661450
#% 1682389
#% 1700125
#! Variables are the distinguishing new feature of XPath 2.0 which permits to select n-tuples of nodes in trees. It is known that the Core of XPath 2.0 captures n-ary first-order (FO) queries modulo linear time transformations. In this paper, we distinguish a fragment of Core XPath 2.0 that remains FO-complete with respect ton-ary queries while enjoying polynomial-time query answering.

#index 977005
#* Optimization of continuous queries with shared expensive filters
#@ Kamesh Munagala;Utkarsh Srivastava;Jennifer Widom
#t 2007
#c 5
#% 116044
#% 152940
#% 190611
#% 287461
#% 297191
#% 300167
#% 300179
#% 325398
#% 333848
#% 341672
#% 378388
#% 397353
#% 408396
#% 656701
#% 765435
#% 785096
#% 824714
#% 1016157
#% 1016178
#! We consider the problem of optimizing and executing multiple continuous queries, where each query is a conjunction of filters and each filter may occur in multiple queries. When filters are expensive, significant performance gains are achieved by sharing filter evaluations across queries. A shared execution strategy in our scenario can either be fixed, in which filters are evaluated in the same predetermined order for all input, or adaptive, in which the next filter to be evaluated is chosen at runtime based on the results of the filters evaluated so far. We show that as filter costs increase, the best adaptive strategy is superior to any fixed strategy, despite the overhead of adaptivity. We show that itis NP-hard to find the optimal adaptive strategy, even if we are willing to approximate within any factor smaller than m where m is the number of queries. We then present a greedy adaptive execution strategy and show that it approximates the best adaptive strategy to within a factor O(log2m log n) where n is the number of distinct filters. We also give a precomputation technique that can reduce the execution overhead of adaptive strategies.

#index 977006
#* Variance estimation over sliding windows
#@ Linfeng Zhang;Yong Guan
#t 2007
#c 5
#% 378388
#% 379445
#% 397443
#% 576113
#% 725366
#% 745533
#% 801696
#% 878246
#% 993961
#! Capturing characteristics of large data streams has received considerable attention. The constraints in space and time restrict the data stream processing to only one pass (or a small number of passes). Processing data streams over sliding windows make the problem more difficult and challenging. In this paper, we address the problem of maintaining ∈-approximate variance of data streams over sliding windows. To our knowledge, the best existing algorithm requires O(1/∈2 log N) space, though the lower bound for this problem is Ω(1/∈ log N). We propose the first ∈-approximation algorithm to this problem that is optimal in both space and worst case time. Our algorithm requires O(1/∈ log N) space. Furthermore, its running time is O(1) in worst case.

#index 977007
#* Marrying words and trees
#@ Rajeev Alur
#t 2007
#c 5
#% 288513
#% 465061
#% 492901
#% 545382
#% 765274
#% 956600
#% 1673671
#% 1675930
#% 1676015
#% 1703544
#% 1722308
#% 1728116
#% 1733511
#! Traditionally, data that has both linear and hierarchical structure, such as annotated linguistic data, is modeled using ordered trees and queried using tree automata. In this paper, we argue that nested words and automata over nested words offer a better way to capture and process the dual structure. Nested words generalize both words and ordered trees, and allow both word and tree operations. We study various classes of automata over nested words, and show that while they enjoy expressiveness and succinctness benefits over word and tree automata, their analysis complexity and closure properties are analogous to the corresponding word and tree special cases. In particular, we show that finite-state nested word automata can be exponentially more succinct than tree automata, and pushdown nested word automata include the two incomparable classes of context-free word languages and context-free tree languages.

#index 977008
#* Estimating statistical aggregates on probabilistic data streams
#@ T. S. Jayram;Andrew McGregor;S. Muthukrishnan;Erik Vee
#t 2007
#c 5
#% 2833
#% 215225
#% 278835
#% 333931
#% 378388
#% 519953
#% 654497
#% 749542
#% 765291
#% 765499
#% 813786
#% 824733
#% 847163
#% 866773
#% 893121
#% 960257
#% 991156
#% 1016201
#! The probabilistic-stream model was introduced by Jayram et al. [20].It is a generalization of the data stream model that issuited to handling "probabilistic" data, where each item of the stream represents a probability distribution over a set of possible events. Therefore, a probabilistic stream determines a distribution over apotentially exponential number of classical "deterministic" streams where each item is deterministically one of the domain values. Designing efficient aggregation algorithms for probabilistic data is crucial for handling uncertainty in data-centric applications such as OLAP. Such algorithms are also useful in a variety of other setting including analyzing search engine traffic and aggregation in sensor networks. We present algorithms for computing commonly used aggregates ona probabilistic stream. We present the first one pass streaming algorithms for estimating the expected mean of a probabilistic stream, improving upon results in [20]. Next, we consider the problem of estimating frequency moments for probabilistic data. We propose a general approach to obtain unbiased estimators working over probabilistic data by utilizing unbiased estimators designed for standard streams. Applying this approach, we extend a classical data stream algorithm to obtain a one-pass algorithm for estimating F2, the second frequency moment. We present the first known streaming algorithms forestimating F0, the number of distinct items on probabilistic streams.Our work also gives an efficient one-pass algorithm for estimatingthe median of a probabilistic stream.

#index 977009
#* Sketching unaggregated data streams for subpopulation-size queries
#@ Edith Cohen;Nick Duffield;Haim Kaplan;Carsten Lund;Mikkel Thorup
#t 2007
#c 5
#% 227883
#% 248812
#% 440428
#% 446438
#% 580976
#% 646233
#% 654497
#% 725372
#% 763999
#% 770892
#% 808507
#% 808509
#% 942353
#% 967028
#! IP packet streams consist of multiple interleaving IP flows. Statistical summaries of these streams, collected for different measurement periods, are used for characterization of traffic, billing, anomaly detection, inferring traffic demands, configuring packet filters and routing protocols, and more. While queries are posed over the set of flows, the summarization algorithmis applied to the stream of packets. Aggregation of traffic into flows before summarization requires storage of per-flow counters, which is often infeasible. Therefore, the summary has to be produced over the unaggregated stream. An important aggregate performed over a summary is to approximate the size of a subpopulation of flows that is specified a posteriori. For example, flows belonging to an application such as Web or DNS or flows that originate from a certain Autonomous System. We design efficient streaming algorithms that summarize unaggregated streams and provide corresponding unbiased estimators for subpopulation sizes. Our summaries outperform, in terms of estimates accuracy, those produced by packet sampling deployed by Cisco's sampled NetFlow, the most widely deployed such system. Performance of our best method, step sample-and-hold is close to that of summaries that can be obtainedfrom pre-aggregated traffic.

#index 977010
#* What is "next" in event processing?
#@ Walker White;Mirek Riedewald;Johannes Gehrke;Alan Demers
#t 2007
#c 5
#% 3703
#% 227951
#% 320187
#% 462230
#% 480938
#% 481448
#% 569257
#% 591579
#% 678222
#% 875004
#% 1180873
#% 1688281
#! Event processing systems have wide applications ranging from managing events from RFID readers to monitoring RSS feeds. Consequently, there exists much work on them in the literature. The prevalent use of these systems is on-line recognition of patterns that are sequences of correlated events in event streams. Query semantics and implementation efficiency are inherently determined by the underlying temporal model: how events are sequenced (what is the "next" event), and how the time stamp of an event is represented. Many competing temporal models for event systems have been proposed, with no consensus on which approach is best. We take a foundational approach to this problem. We create a formal framework and present event system design choices as axioms. The axioms are grouped into standard axioms and desirable axioms. Standard axioms are common to the design of all event systems. Desirable axioms are not always satisfied, but are useful for achieving high performance. Given these axioms, we prove several important results. First, we show that there is a unique model up to isomorphism that satisfies the standard axioms and supports associativity, so our axioms are a sound and complete axiomatization of associative time stamps in eventsystems. This model requires time stamps with unbounded representations. We present a slightly weakened version of associativity that permits a temporal model with bounded representations. We show that adding the boundedness condition also results in a unique model, so again our axiomatization is sound and complete. We believe this model is ideally suited to be the standard temporal model for complex event processing.

#index 977011
#* Privacy, accuracy, and consistency too: a holistic solution to contingency table release
#@ Boaz Barak;Kamalika Chaudhuri;Cynthia Dwork;Satyen Kale;Frank McSherry;Kunal Talwar
#t 2007
#c 5
#% 300184
#% 333876
#% 466462
#% 576110
#% 576111
#% 576760
#% 576761
#% 576762
#% 756489
#% 809245
#% 810028
#% 864412
#% 963242
#% 1374007
#% 1670071
#% 1707132
#% 1740518
#! The contingency table is a work horse of official statistics, the format of reported data for the US Census, Bureau of Labor Statistics, and the Internal Revenue Service. In many settings such as these privacy is not only ethically mandated, but frequently legally as well. Consequently there is an extensive and diverse literature dedicated to the problems of statistical disclosure control in contingency table release. However, all current techniques for reporting contingency tables fall short on at leas one of privacy, accuracy, and consistency (among multiple released tables). We propose a solution that provides strong guarantees for all three desiderata simultaneously. Our approach can be viewed as a special case of a more general approach for producing synthetic data: Any privacy-preserving mechanism for contingency table release begins with raw data and produces a (possibly inconsistent) privacy-preserving set of marginals. From these tables alone-and hence without weakening privacy--we will find and output the "nearest" consistent set of marginals. Interestingly, this set is no farther than the tables of the raw data, and consequently the additional error introduced by the imposition of consistency is no more than the error introduced by the privacy mechanism itself. The privacy mechanism of [20] gives the strongest known privacy guarantees, with very little error. Combined with the techniques of the current paper, we therefore obtain excellent privacy, accuracy, and consistency among the tables. Moreover, our techniques are surprisingly efficient. Our techniques apply equally well to the logical cousin of the contingency table, the OLAP cube.

#index 977012
#* On the complexity of managing probabilistic XML data
#@ Pierre Senellart;Serge Abiteboul
#t 2007
#c 5
#% 663
#% 94459
#% 191616
#% 215225
#% 234905
#% 289270
#% 408638
#% 442830
#% 480102
#% 507180
#% 664841
#% 800547
#% 993985
#% 1016201
#% 1688305
#! In [3], we introduced a framework for querying and updating probabilistic information over unordered labeled trees, the probabilistic tree model. The data model is based on trees where nodes are annotated with conjunctions of probabilistic event variables. We briefly described an implementation and scenarios of usage. We develop here a mathematical foundation for this model. In particular, we present complexity results. We identify a very large class of queries for which simple variations of querying and updating algorithms from [3] compute the correct answer. A main contribution is a full complexity analysis of queries and updates. We also exhibit a decision procedure for the equivalence of probabilistic trees and prove it is in co-RP. Furthermore, we study the issue of removing less probable possible worlds, and that of validating a probabilistic tree against a DTD. We show that these two problems are intractable in the most general case.

#index 977013
#* The dichotomy of conjunctive queries on probabilistic structures
#@ Nilesh Dalvi;Dan Suciu
#t 2007
#c 5
#% 150197
#% 204286
#% 215225
#% 235023
#% 265692
#% 442830
#% 600496
#% 810098
#% 976984
#! We show that for every conjunctive query, the complexity of evaluating it on a probabilistic database is either PTIME or P-complete, and we give an algorithm for deciding whether a given conjunctive query is PTIME or P-complete. The dichotomy property is a fundamental result on query evaluation on probabilistic databases and it gives a complete classification of the complexity of conjunctive queries.

#index 977014
#* Maximally joining probabilistic data
#@ Benny Kimelfeld;Yehoshua Sagiv
#t 2007
#c 5
#% 39702
#% 115964
#% 172933
#% 209725
#% 213983
#% 215225
#% 235023
#% 289350
#% 289424
#% 289425
#% 442830
#% 480102
#% 576099
#% 809242
#% 893149
#% 949368
#% 993985
#% 1700126
#% 1700137
#! Conceptually, the common approach to manipulating probabilistic data is to evaluate relational queries and then calculate the probability of each tuple in the result. This approach ignores the possibility that the probabilities of complete answers are too low and, hence, partial answers (with sufficiently high probabilities) become important. Therefore, we consider the semantics in which answers are maximal (i.e., have the smallest degree of incompleteness), subject tothe constraint that the probability is still above a given threshold. We investigate the complexity of joining relations under the above semantics. In contrast to the deterministic case, this approach gives rise to two different enumeration problems. The first is finding all maximal sets of tuples that are join consistent, connected and have a joint probability above the threshold. The second is computing all maximal tuples that are answers of partial joins and have a probability above the threshold. Both problems are tractable under data complexity. We also consider query-and-data complexity, which rules out as efficient the following naive algorithm: compute all partial answers and then choose the maximal ones among those with probabilities above the threshold. We give efficient algorithms for several, important special cases. We also show that, in general, the first problem is NP-hard whereas the secondis #P-hard.

#index 985976
#* Efficient columnar storage in B-trees
#@ Goetz Graefe
#t 2007
#c 5
#% 64430
#% 122307
#% 286258
#% 287672
#% 287715
#% 300194
#% 428152
#% 464843
#% 465161
#% 480119
#% 824697
#! Column-oriented storage formats have been proposed for query processing in relational data warehouses, specifically for fast scans over non-indexed columns. This short note proposes a data compression method that reuses traditional on-disk B-tree structures with only minor changes yet achieves storage density and scan performance comparable to specialized columnar designs. The advantage of the proposed method over alternative storage structures is that traditional algorithms can be reused, e.g., for assembling rows with multiple columns, bulk insertion and deletion, logging and recovery, consistency checking, etc.

#index 985977
#* Finding shapes in a set of points
#@ Kenneth A. Ross;David Vespe;David Hessing;Pranay Jain
#t 2007
#c 5
#% 411554
#% 479819
#! We present a tool for querying a set of points for geometric shapes. This tool was developed as part of a larger project studying the architecture of 13th century French churches. We present a query language for specifying shapes. We describe an implementation of the tool that optimizes and executes shape queries. We describe the performance of the tool, and show examples of how it can be used to analyze building floorplans. The tool is available for download over the internet.

#index 985978
#* General Hierarchical Model (GHM) to measure similarity of time series
#@ Xinqiang Zuo;Xiaoming Jin
#t 2007
#c 5
#% 227924
#% 310545
#% 310580
#% 333941
#% 460862
#% 477479
#% 480146
#% 534183
#% 631923
#% 662750
#% 769880
#% 800574
#% 810049
#% 1016195
#! Similarity query is a frequent subroutine in time series database to find the similar time series of the given one. In this process, similarity measure plays a very important part. The previous methods do not consider the relation between point correspondences and the importance (role) of the points on the content of time series during measuring similarity, resulting in their low accuracies in many real applications. In the paper, we propose a General Hierarchical Model (GHM), which determines the point correspondences by the hierarchies of points. It partitions the points of time series into different hierarchies, and then the points are restricted to be compared with the ones in the same hierarchy. The practical methods can be implemented based on the model with any real requirements, e.g. FFT Hierarchical Measures (FHM) given in this paper. And the hierarchical filtering methods of GHM are provided for range and k-NN queries respectively. Finally, two common data sets were used in k-NN query and clustering experiments to test the effectiveness of our approach and others. The time performance comparisons of all the tested methods were performed using the synthetic data set with various sizes. The experimental results show the superiority of our approach over the competitors. And we also give the experimental powers of the filtering methods proposed in the queries.

#index 985979
#* A parallel general-purpose synthetic data generator
#@ Joseph E. Hoag;Craig W. Thompson
#t 2007
#c 5
#% 172913
#% 741995
#% 824744
#% 862899
#% 893212
#! PSDG is a parallel synthetic data generator designed to generate "industrial sized" data sets quickly using cluster computing. PSDG depends on SDDL, a synthetic data description language that provides flexibility in the types of data we can generate.

#index 985980
#* XQuery layers
#@ Daniele Braga;Alessandro Campi;Stefano Ceri;Paola Spoletini
#t 2007
#c 5
#% 157134
#% 745423
#% 814647
#% 845349
#% 845361
#% 1673666
#! XML is becoming widespread as data interoperability standard in many application domains. An increasing number of researchers and professionals, who are not computer scientists (although they may have a strong technical background), needs to query and transform XML data during their working activities. Such tasks typically require simple queries and partial awareness of the XML data model, in the context of a given, domain-specific XML-based protocol. The W3C community has proposed XQuery as the standard query language for XML [9]. XQuery has a huge expressive power - as it encompasses features belonging both to query and functional languages, but it may be considered as too complex for the above user profiles; well-designed subsets of XQuery are sufficient to satisfy their needs. In this paper, we propose six layered subsets of XQuery, targeted to cover user communities with increasing needs. The initial layers are based upon XQBE (XQuery By Example), a visual XML query language, strictly less expressive than XQuery. We argue that the first layers are easier to learn and to master than the full language, also thanks to the availability of simple visual interfaces, and that these layers cover most of the needs of many user communities.

#index 985981
#* Static analysis of XML processing with data values
#@ Luc Segoufin
#t 2007
#c 5
#% 172941
#% 175464
#% 283011
#% 292269
#% 345757
#% 398752
#% 427027
#% 431006
#% 564264
#% 566956
#% 570649
#% 572461
#% 578570
#% 578964
#% 581901
#% 643569
#% 725176
#% 742056
#% 769518
#% 776157
#% 809236
#% 812921
#% 821604
#% 848763
#% 874877
#% 888014
#% 888015
#% 894435
#% 945025
#% 993939
#% 1673664
#% 1673671
#% 1700125
#% 1718404
#! Most theoretical work on static analysis for XML and its query languages models XML documents by labeled ordered unranked trees, where the labels are from a finite set. Attribute values are usually ignored. This was quite successful for many applications like, to mention only some of them, the study of the navigational fragment of XPath known as Core-XPath [20], a complete picture of the complexity of satisfiability and inclusion of XPath queries without joins [4], XML-schema design with no integrity constraints [28, 31], type-checking [40]. The reader is invited to have a look at the Database Theory columns [32, 37, 40, 39] for an introduction on this rich production.

#index 985982
#* Stefano Ceri speaks out on many-book researchers and one-startup researchers, web modeling, the vanishing US-Europe research gap, the semantic web services train, and more
#@ Marianne Winslett
#t 2007
#c 5
#! Welcome to this installment of ACM SIGMOD Record's series of interviews with distinguished members of the database community. I'm Marianne Winslett, and today we are at the L3S Research Institute in Hannover, Germany, where I am spending the fall of 2006. I have here with me Stefano Ceri, who is a Professor of Information Engineering at the Politecnico di Milano. Stefano's research interests include distributed, deductive, active, and object-oriented databases, and XML query languages; his recent work is on the design of Web applications. He is a co-inventor of WebML (Web Modeling Language), and has a startup company that is commercializing WebRatio, a product based on WebML. Stefano was a founder of EDBT and is still a member of the EDBT Endowment, and he has been a member of the VLDB Endowment for twelve years. He was an editor of ACM Transactions on Database Systems and IEEE Transactions on Software Engineering. He is a coauthor of 9 books, and his PhD is from the Politecnico di Milano. So, Stefano, welcome!

#index 985983
#* Developmental informatics at IIT Bombay
#@ Anil Bahuman;Chaitra Bahuman;Malati Baru;Subhasri Duttagupta;Krithi Ramamritham
#t 2007
#c 5
#% 800543
#% 956606
#! IIT Bombay's Developmental Informatics Lab is a cross disciplinary group consisting of 6 faculty, 30 research staff and several students. The lab is working towards increasing access to information -- through the use of internet and communication technologies -- to communities in the developing world especially rural and small town India. The lab is supported by Indian Government funding sources as well as corporate and multi-lateral agencies to solve technical problems in local communities in sustainable ways. This paper focuses on two mature projects of the lab -- one caters to Indian farmers while another helps with the education of tribal populations.

#index 985984
#* Report on the First International VLDB Workshop on Clean Databases (CleanDB 2006)
#@ Dongwon Lee;Chen Li
#t 2007
#c 5
#! In this report, we provide a summary of the First Int'l VLDB Workshop on Clean Databases (CleanDB 2006), which took place at Seoul, Korea, on September 11, 2006, in conjunction with the 32nd Int'l Conference on Very Large Data Bases (VLDB).

#index 985985
#* High diversity transforms multimedia information retrieval into a cross-cutting field: report on the 8th Workshop on Multimedia Information Retrieval
#@ James Z. Wang;Nozha Boujemaa;Yixin Chen
#t 2007
#c 5
#% 903597
#% 903598
#% 903599
#! Indexing and retrieval of large quantity of multimedia data is a highly challenging and growingly important problem for the computer science research community. Researchers in multimedia, databases, computer vision, machine learning, signal and image processing and statistics have worked on multimedia information retrieval (MIR) for over a decade. A number of significant technological advances have been achieved in this field. Some of the techniques have been applied to application areas such as art image retrieval, biomedical image and video retrieval, education, sensor networks, large-scale online personal and professional photo sharing communities, classification and filtering of images on the Web, scientific content, computer forensics, threat assessment and security applications more generally.

#index 985986
#* Frequently-asked questions about double-blind reviewing
#@ Richard Snodgrass
#t 2007
#c 5
#% 896021
#% 927029
#! As announced in an editorial [Snodgrass 2007], ACM Transactions on Database Systems has adopted double-blind reviewing, in which the identities of the author and the reviewer are not known to each other. That editorial provided a comprehensive analysis of the costs of double-blind reviewing (DBR) and a detailed examination of the decision.

#index 1021194
#* Database research opportunities in computer games
#@ Walker White;Christoph Koch;Nitin Gupta;Johannes Gehrke;Alan Demers
#t 2007
#c 5
#% 235114
#% 402644
#% 422875
#% 750026
#% 930255
#% 960236
#% 983107
#% 993949
#! In this paper, we outline several ways in which the database community can contribute to the development of technology for computer games. We outline the architecture of different types of computer games, and show how database technology plays a role in their design. From this, we identify several new research directions to improve the utilization of this technology in computer games.

#index 1021195
#* Simple off the shelf abstractions for XML schema
#@ Wim Martens;Frank Neven;Thomas Schwentick
#t 2007
#c 5
#% 70235
#% 152835
#% 257873
#% 378392
#% 427027
#% 754074
#% 772031
#% 782853
#% 835398
#% 848763
#% 893098
#% 894435
#% 957534
#% 976999
#% 1022285
#% 1408538
#% 1408540
#% 1661444
#! Although the advent of XML Schema [25] has rendered DTDs obsolete, research on practical XML optimization is mostly biased towards DTDs and tends to largely ignore XSDs (some notable exceptions non-withstanding). One of the underlying reasons is most probably the perceived simplicity of DTDs versus the alleged impenetrability of XML Schema. Indeed, optimization w.r.t. DTDs has a local flavor and usually reduces to reasoning about the accustomed formalism of regular expressions. XSDs, on the other hand, even when sufficiently stripped down, are related to the less pervious class of unranked regular tree automata [6, 19, 20, 21]. Recent results on the structural expressiveness of XSDs [19], however, show that XSDs are in fact much closer to DTDs than to tree automata, leveraging the possibility to directly extend techniques for DTD-based XML optimization to the realm of XML Schema. The goal of the present paper is to present the results in [19] in an easy and accessible way. At the same time, we discuss possible applications, related research, and future research directions. Throughout the paper, we try to restrict notation to a minimum. We refer to [19] for further details.

#index 1021196
#* Overview and semantic issues of text mining
#@ Anna Stavrianou;Periklis Andritsos;Nicolas Nicoloyannis
#t 2007
#c 5
#% 118731
#% 194267
#% 226545
#% 260001
#% 279755
#% 280817
#% 316509
#% 321635
#% 332658
#% 344447
#% 375017
#% 464434
#% 465754
#% 466808
#% 496419
#% 587959
#% 709066
#% 719299
#% 722308
#% 722904
#% 746885
#% 748482
#% 748550
#% 748583
#% 748873
#% 756826
#% 757091
#% 785375
#% 786497
#% 794527
#% 828958
#% 829970
#% 829971
#% 836145
#% 838508
#% 839620
#% 843716
#% 854646
#% 864416
#% 865770
#% 869606
#% 879595
#% 882008
#% 893089
#% 939332
#% 940028
#% 1022236
#% 1272078
#% 1275285
#% 1734219
#! Text mining refers to the discovery of previously unknown knowledge that can be found in text collections. In recent years, the text mining field has received great attention due to the abundance of textual data. A researcher in this area is requested to cope with issues originating from the natural language particularities. This survey discusses such semantic issues along with the approaches and methodologies proposed in the existing literature. It covers syntactic matters, tokenization concerns and it focuses on the different text representation techniques, categorisation tasks and similarity measures suggested.

#index 1021197
#* Kyu-Young Whang speaks out: on academia and startups in Korea, probabilistic counting, main-memory query optimization, how to avoid being a hostage of pressure publishing, and more
#@ Marianne Winslett
#t 2007
#c 5
#! Welcome to this installment of ACM SIGMOD Record's series of interviews with distinguished members of the database community. I'm Marianne Winslett, and today we are at the ICDE 2007 Conference in Istanbul. I have here with me Kyu-Young Whang, who is a professor of computer science at Korea Advanced Institute of Science and Technology, and the director of the Advanced Information Technology Research Center. Before joining KAIST, Kyu-Young was a member of technical staff at IBM TJ Watson Research Center. His research interests are very broad, encompassing many different kinds of database systems as well as data mining and data streaming. He is an editor-in-chief of the VLDB Journal, a former member of the VLDB Endowment, and an IEEE Fellow. Kyu-Young's PhD is from Stanford. So, Kyu-Young, welcome!

#index 1021198
#* Boon Thau Loo speaks out: on his SIGMOD dissertation award, better networking through datalog, life as an assistant professor, and more
#@ Marianne Winslett
#t 2007
#c 5
#! Welcome to this installment of ACM SIGMOD Record's series of interviews with distinguished members of the database community. I'm Marianne Winslett, and today we are in Alexandria, Virginia. I have here with me Boon Thau Loo, who is an Assistant Professor of Computer and Information Science at the University of Pennsylvania. Boon is the recipient of the 2007 ACM SIGMOD Dissertation Award for his thesis entitled "The Design and Implementation of Declarative Networks." Boon's PhD is from the University of California at Berkeley, where his advisors were Joe Hellerstein and Ion Stoica. So, Boon, welcome!

#index 1021199
#* Community systems research at Yahoo!
#@  Community Systems Group
#t 2007
#c 5
#% 213981
#% 262112
#% 280849
#% 333854
#% 442700
#% 805841
#% 881500
#% 960234
#% 963669
#% 1004299
#% 1022235
#! The web and its continued evolution present unprecedented opportunities for database researchers and practitioners to deliver unique user experiences that are not possible traditionally, e.g., mass collaborations through (automatically) established online communities and exploration of large scale structured information. Along with these opportunities, however, come significant challenges. The challenges are two-fold: systems, the infrastructures that allow us to deliver information at scale; and community, the applications that deliver the next generation of web experiences centered around people and social networks. In this paper, we describe the ongoing research efforts within the Community Systems group here at Yahoo! Research to address these challenges.

#index 1021200
#* Report on the First International Workshop on Database Preservation (PresDB'07)
#@ Vassilis Christophides;Peter Buneman
#t 2007
#c 5
#! The need to preserve scientific, scholarly and cultural data has long been recognized. These data sets are valuable and many of them are either impossible to reproduce (e.g. climate and demographic data) or can only be recovered at enormous costs (e.g. data from high energy physics experiments). While substantial investment has been made in archiving and preserving conventional forms of these objects, such as documents, images and numerical data in some file format, the need to preserve entire databases has only recently emerged. Databases differ from fixed digital objects studied in the past, in that they change over time, they have internal structure, and they include schemas and integrity constraints, which are basic for the current and future interpretation of the data. Increasingly, database technology is being used in the storage of large numerical scientific data sets.

#index 1023962
#* Proceedings of the 4th international workshop on XQuery implementation, experience and perspectives
#@ Jayavel Shanmugasundaram;Jerome Simeon
#t 2007
#c 5
#! XIME-P 2007 invites original research contributions as well as reports on industrial efforts on the implementation, utilization, and overall prospects of XQuery. Like the 2004 (Paris), 2005 (Baltimore) and 2006 (Chicago) editions of the XIME-P workshop series, XIME-P 2007 will be held just after and in cooperation with the ACM SIGMOD/PODS conference, this time in Beijing, China. On 23 January 2007, the family of XQuery specifications became W3C Recommendations. As the work on XQuery now moves toward even more powerful features, such as updates, full-text or scripting extensions, XIME-P 2007 is the ideal event to discuss the state XQuery and weigh on its future. One of the fascinating aspects of XQuery is that work on the language specification itself, its implementation, and its application happens at the intersection of databases, document processing, and programming languages. Computer science research and industry has thus found quite a number of promising -- sometimes completely disjoint -- avenues to approach challenges in the XQuery domain. This diversity in contributions and attendees has been a source of lively discussions, panels, and lead to an interesting technical program for previous XIME-P editions. For 2007, we will try to underline this diversity. Given that XQuery has become a W3C Recommendation, XIME-P 2007 explicitly welcomes contributions which relate to standards-compliant treatments of XQuery. Technically, this aspect can be challenging --- especially when conformance and efficiency seem to be at odds (when in reality they need not be). For 2007, we encourage forward-looking contributions that explore what future evolutions to XQuery would be useful for novel applications. Such applications may include, but are not limited to, XQuery support for Web services applications, distributed programming with XQuery, search applications with XQuery, and support for Workflow applications. The XIME-P 2007 program will feature talks on research as well as industrial efforts on the implementation and utilization of XQuery.

#index 1024474
#* Estimating the selectivity of tf-idf based cosine similarity predicates
#@ Sandeep Tata;Jignesh M. Patel
#t 2007
#c 5
#% 577309
#% 632062
#% 769446
#% 824684
#% 1016219
#! An increasing number of database applications today require sophisticated approximate string matching capabilities. Examples of such application areas include data integration and data cleaning. Cosine similarity has proven to be a robust metric for scoring the similarity between two strings, and it is increasingly being used in complex queries. An immediate challenge faced by current database optimizers is to find accurate and efficient methods for estimating the selectivity of cosine similarity predicates. To the best of our knowledge, there are no known methods for this problem. In this paper, we present the first approach for estimating the selectivity of tf.idf based cosine similarity predicates. We evaluate our approach on three different real datasets and show that our method often produces estimates that are within 40% of the actual selectivity.

#index 1024475
#* Cardinality estimation for the optimization of queries on ontologies
#@ E. Patrick Shironoshita;Michael T. Ryan;Mansur R. Kabuka
#t 2007
#c 5
#% 248014
#% 397364
#% 452869
#% 738955
#% 765434
#% 765456
#% 800008
#% 824755
#% 864448
#! An effective, accurate algorithm for cardinality estimation of queries on ontology models of data is presented. The algorithm relies on the decomposition of queries into query pattern paths, where each path produces a set of values for each variable within the result form of the query. In order to estimate the total number of result set parameters for each path, a set of statistics is compiled on the properties of the ontology. Experimental analysis has shown that the algorithm produces estimates with high accuracy and with high correlation to actual values. Thus, this algorithm can be used as the cornerstone of an effective optimization strategy for queries on diverse, heterogeneous data sources modeled as ontologies.

#index 1024476
#* Navigational XPath: calculus and algebra
#@ Balder ten Cate;Maarten Marx
#t 2007
#c 5
#% 191611
#% 421945
#% 465065
#% 564264
#% 801686
#% 821563
#% 824798
#% 850728
#% 874910
#% 893208
#% 976991
#% 977004
#% 993939
#% 1661435
#% 1669576
#! We survey expressivity results for navigational fragments of XPath 1.0 and 2.0, as well as Regular XPath&ap;. We also investigate algebras for these fragments.

#index 1024477
#* Georg Gottlob speaks out
#@ Marianne Winslett
#t 2007
#c 5
#! Welcome to this installment of ACM SIGMOD Record's series of interviews with distinguished members of the database community. I'm Marianne Winslett, and today we are at the SIGMOD 2006 conference in Chicago, Illinois. I have here with me Georg Gottlob, who is a professor of computing science at Oxford University. Before that he was a professor at the Technical University of Vienna for many years. His research interests lie in database theory, logic, AI, and complexity. Georg is a co-founder of Lixto Corporation, and until recently he was the editor in chief of AI Communications. Georg's PhD is from the Technical University of Vienna. So, Georg, welcome!

#index 1024478
#* Report on the Third International Workshop on XQuery Implementation, Experience and Perspectives (XIME-P 2006)
#@ Michael Carey;Torsten Grust
#t 2007
#c 5
#! On June 30, 2006, XIME-P 2006, the International Workshop on XQuery Implementation, Experience and Perspectives was held. This workshop marks the third event in a workshop series whose primary aim is to shed light on XQuery systems, specification aspects, foundations of the language, and the many perceivable shapes it may take on in the future. Like the two previous workshops of 2004 (Paris) and 2005 (Baltimore), XIME-P 2006 was held as a co-located workshop of the ACM SIGMOD Conference, which this year was held in Chicago, IL, USA. The 2006 edition was co-chaired by Michael Carey and Torsten Grust. The workshop web site can be found at www.ximep-2006.org.

#index 1024479
#* Report on ACM Workshop on Health Information and Knowledge Management (HIKM 2006)
#@ Li Xiong;Yuni Xia
#t 2007
#c 5
#% 907446
#% 907447
#% 907448
#% 907449
#% 907450
#% 907451
#% 907452
#! Health information technology is receiving a tremendous amount of attention as a strategic area that will benefit the society in the 21st century. The continued advances in healthcare such as digitization of medical records, creation of central record systems, development of healthcare data warehouses increasingly pose new challenges to information and knowledge management. The high stakes and unique characteristics of healthcare data such as the long-term value of the data, varied data quality, the complexity of the data, the privacy constraints, as well as the availability requirements in emergent situations require a special treatment of traditional information management techniques.

#index 1024480
#* Event report on iiWAS 2006 and MoMM 2006, Yogyakarta, December 2006
#@ Eric Pardede
#t 2007
#c 5
#! In this paper, we report on two events held in Yogyakarta, Indonesia from 4--6 December 2006. These conferences are the Eight International Conference on Information Integration & Web-Services and Applications (iiWAS 2006) and the Fourth International Conference on Advances in Mobile Computing and Multimedia (MoMM 2006).

#index 1038042
#* Peter Chen speaks out: on paths to fame, the roots of the ER model in human language, the ER model in software engineering, the need for ER databases, and more
#@ Marianne Winslett
#t 2004
#c 5

#index 1050774
#* A critical look at the TAB benchmark for physical design tools
#@ Nicolas Bruno
#t 2007
#c 5
#% 480158
#% 482100
#% 632100
#% 765431
#% 778724
#% 810026
#% 810027
#% 820356
#% 893130
#% 1016220
#% 1016221
#% 1207101
#! There has recently been considerable research on physical design tuning algorithms. At the same time, there is only one published methodology to evaluate the quality of different, competing approaches: the TAB benchmark. In this paper we describe our experiences with TAB. We first report an experimental evaluation of TAB on our latest prototype for physical design tuning. We then identify certain weakness in the benchmark and briefly comment on alternatives to improve its usefulness.

#index 1050775
#* Nulls, three-valued logic, and ambiguity in SQL: critiquing date's critique
#@ Claude Rubinson
#t 2007
#c 5
#% 112498
#% 282431
#% 385721
#% 927183
#! Date's popular critique of SQL's three-valued logic [4, 3] purports to demonstrate that SQL queries can produce erroneous results when nulls are present in the database. I argue that this critique is flawed in that Date misinterprets the meaning of his example query. In fact, SQL returns the correct answer to the query posed; Date, however, believes that he is asking a different question. Although his critique is flawed, I agree with Date's general conclusion: SQL's use of nulls and three-valued logic introduces a startling amount of complexity into seemingly straightforward queries.

#index 1050776
#* Estimating the selectivity of tf-idf based cosine similarity predicates
#@ Sandeep Tata;Jignesh M. Patel
#t 2007
#c 5
#% 577309
#% 632062
#% 769446
#% 824684
#% 1016219
#! An increasing number of database applications today require sophisticated approximate string matching capabilities. Examples of such application areas include data integration and data cleaning. Cosine similarity has proven to be a robust metric for scoring the similarity between two strings, and it is increasingly being used in complex queries. An immediate challenge faced by current database optimizers is to find accurate and efficient methods for estimating the selectivity of cosine similarity predicates. To the best of our knowledge, there are no known methods for this problem. In this paper, we present the first approach for estimating the selectivity of tf.idf based cosine similarity predicates. We evaluate our approach on three different real datasets and show that our method often produces estimates that are within 40% of the actual selectivity.

#index 1050777
#* A data-oriented survey of context models
#@ Cristiana Bolchini;Carlo A. Curino;Elisa Quintarelli;Fabio A. Schreiber;Letizia Tanca
#t 2007
#c 5
#% 300170
#% 446642
#% 514870
#% 649802
#% 665856
#% 723451
#% 745971
#% 749238
#% 757481
#% 790013
#% 795253
#% 799696
#% 809790
#% 818207
#% 826460
#% 851396
#% 863913
#% 870297
#% 870300
#% 870315
#% 870744
#% 893881
#% 940695
#% 1046669
#% 1392007
#% 1668039
#% 1671648
#! Context-aware systems are pervading everyday life, therefore context modeling is becoming a relevant issue and an expanding research field. This survey has the goal to provide a comprehensive evaluation framework, allowing application designers to compare context models with respect to a given target application; in particular we stress the analysis of those features which are relevant for the problem of data tailoring. The contribution of this paper is twofold: a general analysis framework for context models and an up-to-date comparison of the most interesting, data-oriented approaches available in the literature.

#index 1050778
#* Intel Mash Maker: join the web
#@ Rob Ennals;Eric Brewer;Minos Garofalakis;Michael Shadle;Prashant Gandhi
#t 2007
#c 5
#% 333997
#% 810108
#% 845350
#% 860091
#% 860745
#% 880727
#% 902794
#% 956700
#% 997023
#% 1655411
#! Intel® Mash Maker is an interactive tool that tracks what the user is doing and tries to infer what information and visualizations they might find useful for their current task. Mash Maker uses structured data from existing web sites to create new "mashed up" interfaces combining information from many sources. The Intel® Mash Maker client is currently implemented as an extension to the FireFox web browser. Mash Maker adds a toolbar to the browser that shows buttons representing enhancements that Mash Maker believes the user might want to apply to the current page. An enhancement might combine the data on the page with data from another source, or visualize data in a new way. Mash Maker is intended to be an integral part of the way the user browses information, rather than being a special tool that a user uses when they want to create mashups. In order to create mashups from normal websites, Mash Maker must first extract structured data from them. If the web site does not provide RDF data, then Mash Maker extracts structured data from the raw HTML using a community-maintained database of extractors, where each extractor describes how to extract structured data from a particular kind of web site.

#index 1050779
#* Ricardo Baeza-Yates speaks out: on CS research in Latin America, his multi-continent commute for Yahoo!, how to get real data in academia, and web mining
#@ Marianne Winslett;Ricardo Baeza-Yates
#t 2007
#c 5
#! Welcome to this installment of ACM SIGMOD Record's series of interviews with distinguished members of the database community. I'm Marianne Winslett, and today we are at the ICDE 2007 conference in Istanbul. I have here with me Ricardo Baeza-Yates, who is the vice president of Yahoo! Research in Europe and Latin America. Before that, he was a professor of computer science at the University of Chile for many years. His research interests include information retrieval, databases, algorithms, and user interfaces, and he is a co-author of one of the most widely used books on information retrieval. Ric's PhD is from the University of Waterloo. So, Ric, welcome!

#index 1050780
#* Data and web management research at Politecnico di Milano
#@ Stefano Ceri;Cristiana Bolchini;Daniele Braga;Marco Brambilla;Alessandro Campi;Sara Comai;Piero Fraternali;Pier Luca Lanzi;Marco Masseroli;Maristella Matera;Mauro Negri;Giuseppe Pelagatti;Giuseppe Pozzi;Elisa Quintarelli;Fabio A. Schreiber;Letizia Tanca
#t 2007
#c 5
#% 425200
#% 452842
#% 452858
#% 641769
#% 657879
#% 728293
#% 737353
#% 749636
#% 805855
#% 810088
#% 810175
#% 814647
#% 825632
#% 870300
#% 879743
#% 903573
#% 910629
#% 927034
#% 939160
#% 949182
#% 960410
#% 998756
#% 1048492
#% 1206614
#% 1208236
#% 1246534
#% 1599295
#% 1599307
#% 1599308
#% 1666113
#% 1696296
#% 1778932
#! Research in data management at Politecnico di Milano has a long and solid tradition; forefront books on distributed databases, conceptual database design, logical databases, and active databases contributed to shape the foundations of this discipline in the last two decades. Historically, our work has addressed both all aspects of innovation in the technology of modern data management systems and the consequent support of design methods and tools.

#index 1050781
#* Report on the First International Workshop on Ranking in Databases (DBRank'07)
#@ Ihab F. Ilyas;Gautam Das
#t 2007
#c 5
#! This report summarizes the presentations, keynotes and discussions that took place during the first international workshop on ranking in databases (DBRank'07). The workshop was held on April 16, 2007, in conjunction with ICDE in Istanbul, Turkey.

#index 1050782
#* Report on the Fourth International Workshop on Data Management for Sensor Networks (DMSN 2007)
#@ Magdalena Balazinska;Amol Deshpande;Alexandros Labrinidis;Qiong Luo;Samuel Madden;Jun Yang
#t 2007
#c 5
#! Sensor networks enable an unprecedented level of access to the physical world, and hold tremendous potential to revolutionize many application domains. Research on sensor networks spans many areas of computer science, and there are now major conferences, e.g., IPSN and SenSys, devoted to sensor networks. However, there is no focused forum for discussion of early and innovative work on data management in sensor networks. The International Workshop on Data Management for Sensor Networks (DMSN), inaugurated in 2004, aims to fill this significant gap in the database and sensor network communities.

#index 1050783
#* Report on the First VLDB Workshop on Management of Uncertain Data (MUD)
#@ Ander de Keijzer;Maurice van Keulen;Alex Dekhtyar
#t 2007
#c 5
#% 874876
#! On Monday September 24th, we organized the first international VLDB workshop on Management of Uncertain Data [dKvKD07]. The idea of this workshop arose a year earlier at the Twente Data Management Workshop on Uncertainty in Databases [dKvK06]. The TDM is a bi-annual workshop organized by the Database group of the University of Twente, for which each time a different topic is chosen. The participants of TDM 2006 were enthusiastic about the topic "Uncertainty in Databases" and strongly expressed the wish for a follow-up co-located with an international conference. To fulfill this wish, we organized the MUD-workshop at VLDB.

#index 1052062
#* Proceedings of the 3rd international workshop on Data management on new hardware
#@ Anastassia Ailamaki;Qiong Luo
#t 2007
#c 5
#! The aim of this one-day workshop is to bring together researchers who are interested in optimizing database performance on modern computing infrastructure by designing new data management techniques and tools.

#index 1061889
#* Distributed databases and peer-to-peer databases: past and present
#@ Angela Bonifati;Panos K. Chrysanthis;Aris M. Ouksel;Kai-Uwe Sattler
#t 2008
#c 5
#% 121
#% 41898
#% 85086
#% 163046
#% 198465
#% 264263
#% 269083
#% 286916
#% 340175
#% 340176
#% 348182
#% 381812
#% 397441
#% 465063
#% 571217
#% 723446
#% 765446
#% 791020
#% 810034
#% 824706
#% 824763
#% 824794
#% 912245
#% 976997
#% 1015327
#% 1016165
#% 1394025
#! The need for large-scale data sharing between autonomous and possibly heterogeneous decentralized systems on the Web gave rise to the concept of P2P database systems. Decentralized databases are, however, not new. Whereas a definition for a P2P database system can be readily provided, a comparison with the more established decentralized models, commonly referred to as distributed, federated and multi-databases, is more likely to provide a better insight to this new P2P data management technology. Thus, in the paper, by distinguishing between db-centric and P2P-centric features, we examine features common to these database systems as well as other ad-hoc features that solely characterize P2P databases. We also provide a non-exhaustive taxonomy of the most prominent research efforts toward the realization of full-fledged P2P databases.

#index 1061890
#* A survey on querying encrypted XML documents for databases as a service
#@ Ozan Ünay;Taflan I. Gündem
#t 2008
#c 5
#% 286849
#% 397367
#% 659992
#% 664705
#% 725292
#% 765448
#% 800521
#% 864667
#% 884523
#% 893099
#% 915809
#% 1016189
#% 1703056
#! "Database as a service" paradigm has gained a lot of interest in recent years. This has raised questions about the security of data in the servers. Firms outsourcing their XML databases to untrusted parties started to look for new ways to securely store data and efficiently query them. In this paper, encrypted XML documents, their crypto index structures and query processing using these structures are investigated. A comparison of various algorithms in the literature is given.

#index 1061891
#* BP-Mon: query-based monitoring of BPEL business processes
#@ Catriel Beeri;Anat Eyal;Tova Milo;Alon Pilberg
#t 2008
#c 5
#% 465061
#% 654477
#% 800609
#% 800633
#% 893117
#% 960349
#% 993949
#% 1015276
#% 1022252
#! A Business Process (BP for short) consists of some business activities undertaken by one or more organizations in pursuit of some particular goal. It often interacts with other BPs of the same or other organizations and the software implementing it is rather complex. Two complementary instruments facilitate the design, development, and management of this complex software. The first is the use of standards. In particular, the recent BPEL standard (Business Process Execution Language [5]) provides an XML-based language to describe the operational logic and execution flow of the BP, as well as the interfaces it exposes to other BPs. A BP specification written in BPEL can be automatically compiled into an actual code that implements the BP, and can be executed on a BPEL server. The second instrument is the use of supporting BP management tools for (1) designing the BP BPEL specifications, (2) analyzing the design, (3) monitoring the BPs at run time, and (4) analyzing, posteriorly, the process execution traces (logs). Together they provide an essential infrastructure for companies to design business processes, optimize them, reduce operational costs, and ultimately increase competitiveness.

#index 1061892
#* Serge Abiteboul speaks out: on building a research group in Europe, how he got involved in a startup, why systems papers shouldn't have to include measurements, the value of object databases, and more
#@ Marianne Winslett
#t 2008
#c 5

#index 1061893
#* Data management projects at Google
#@ Michael Cafarella;Edward Chang;Andrew Fikes;Alon Halevy;Wilson Hsieh;Alberto Lerner;Jayant Madhavan;S. Muthukrishnan
#t 2008
#c 5
#% 723279
#% 845350
#% 954300
#% 960237
#% 963669
#% 1002142
#% 1022259
#! This article describes some of the ongoing research projects related to structured data management at Google today. The organization of Google encourages research scientists to work closely with engineering teams. As a result, the research projects tend to be motivated by real needs faced by Google's products and services, and solutions are put into production and tested rapidly. In addition, because of the sheer scale at which Google operates, the engineering challenges faced by Google's services often require research innovations.

#index 1061894
#* The repeatability experiment of SIGMOD 2008
#@ I. Manolescu;L. Afanasiev;A. Arion;J. Dittrich;S. Manegold;N. Polyzotis;K. Schnaitter;P. Senellart;S. Zoupanos;D. Shasha
#t 2008
#c 5
#! SIGMOD 2008 was the first database conference that offered to test submitters' programs against their data to verify the experiments published. This paper discusses the rationale for this effort, the community's reaction, our experiences, and advice for future similar efforts.

#index 1061895
#* Report from the Third International Workshop on Computer Vision Meets Databases (CVDB 2007)
#@ Laurent Amsaleg;Björn pór Jónsson;Vincent Oria
#t 2008
#c 5
#! This report summarizes the presentations and discussions of the Third International Workshop on Computer Vision meets Databases, or CVDB 2007, which was held in Beijing, China, on June 10, 2007. The workshop was colocated with the 2007 ACM SIGMOD/PODS conferences and attended by twenty-five participants.

#index 1061896
#* Databases and Web 2.0 panel at VLDB 2007
#@ Sihem Amer-Yahia;Volker Markl;Alon Halevy;AnHai Doan;Gustavo Alonso;Donald Kossmann;Gerhard Weikum
#t 2008
#c 5
#% 855601
#! Web 2.0 refers to a set of technologies that enables indviduals to create and share content on the Web. The types of content that are shared on Web 2.0 are quite varied and include photos and videos (e.g., Flickr, YouTube), encyclopedic knowledge (e.g., Wikipedia), the blogosphere, social book-marking and even structured data (e.g., Swivel, Many-eyes). One of the important distinguishing features of Web 2.0 is the creation of communities of users. Online communities such as LinkedIn, Friendster, Facebook, MySpace and Orkut attract millions of users who build networks of their contacts and utilize them for social and professional purposes. In a nutshell, Web 2.0 offers an architecture of participation and democracy that encourages users to add value to the application as they use it.

#index 1061897
#* Report on the First International Workshop on Mining Graphs and Complex Structures (MGCS'07)
#@ Lawrence B. Holder;Xifeng Yan
#t 2008
#c 5
#! The fast accumulation of graph data is witnessed in a wide range of scientific and commercial domains. Typical graph data include chemical compounds, circuits, biological networks, computer networks, 2D/3D models, XML, RDF and workflows. Graph is regarded as a critical data type for knowledge discovery in bioinformatics, chemical informatics, computer vision, informational retrieval, computer security, semantic web, social science, etc., just to name a few. Unfortunately, due to the lack of graph management and mining tools, it is hard, if not impossible, for users to search and analyze any reasonably large collection of graphs. There is an imminent need for scalable methods for mining and search in graphs and other complex structures.

#index 1061898
#* Report on the Sixth ACM Workshop on Privacy in the Electronic Society (WPES 2007)
#@ Adam J. Lee;Ting Yu
#t 2008
#c 5
#! The world is transforming into an electronic society where almost every aspect of our lives is increasingly computerized and interconnected. Such a transformation has profoundly changed the scope, the scale and the level of automation for information collection, storage, analysis and dissemination. It, on the one hand, has and continues to enable new and better services. On the other hand, it inevitably increases the degree of privacy concerns.

#index 1061899
#* Report on the Tenth ACM International Workshop on Data Warehousing and OLAP (DOLAP'07)
#@ Torben Bach Pedersen;Il-Yeol Song
#t 2008
#c 5
#% 1016599
#% 1016600
#% 1016601
#% 1016602
#% 1016603
#% 1016604
#% 1016605
#% 1016606
#% 1016607
#% 1016608
#% 1016609
#% 1016610
#% 1016611
#% 1016612
#% 1016613
#% 1016614
#! This paper presents an overview of DOLAP'07, the 10th ACM International Workshop on Data Warehousing and OLAP, held on November 9, 2007 in Lisbon, Portugal in conjunction with CIKM'07, the ACM 16th Conference on Information and Knowledge Management.

#index 1061900
#* Report on the Principles of Provenance Workshop
#@ James Cheney;Peter Buneman;Bertram Ludäscher
#t 2008
#c 5
#% 318704
#% 803468
#% 825661
#% 875015
#% 948929
#% 976987
#% 1396742
#% 1661440
#% 1916164
#! Provenance, or records of the origin, context, custody, derivation or other historical information about a (digital) object, has recently become an important research topic in a number of areas, particularly databases. However, there has been little interaction between researchers across subdisciplines of computer science working on related problems. This article reports on a workshop on Principles of Provenance held in Edinburgh, Scotland in November 2007, which facilitated interaction among researchers working on provenance in databases, security, information retrieval, Semantic Web, and software engineering settings, as well as developers and database administrators who are currently working with provenance in practice, or foresee the need to do so in the near future.

#index 1063466
#* Proceedings of the 2008 ACM SIGMOD international conference on Management of data
#@ Laks V. S. Lakshmanan;Raymond T. Ng;Dennis Shasha
#t 2008
#c 5
#! Welcome to SIGMOD 2008! We think you will find both the conference and the setting to be invigorating. The natural timeless beauty of British Columbia will provide a fitting counterpoint to the dynamism of our field in which large scale, high performance, and ever more intelligent database systems are being conceived and deployed. This dynamism is reflected in our (extreme) keynote presentations, tutorials, research papers, demonstrations, industrial papers, and product presentations. The only unfortunate side of our program is that the five parallel session structure may prevent you from hearing every talk in which you are interested. The conference statistics give an indication of how SIGMOD's selectivity. Out of 435 submitted research papers, we accepted 78; out of 40 submitted industrial papers, we accepted 15; out of 94 demo submissions, we accepted 30; and out of 15 tutorial submissions, we accepted 5. Reviewing is an imperfect art, so we may have rejected some papers that we should have accepted, but we hope the written reviews have helped authors improve their papers for future submissions. The main methodological innovation in SIGMOD this year has been the repeatability option. Papers submitting experiments were invited to submit code and data to enable the pioneering members of the repeatability committee to verify that the experiments worked as advertised. Any paper satisfying the repeatability criteria will include the sentence "The results in this paper were verified by the SIGMOD repeatability committee." The goal is to count our field among the repeatable sciences and to pave the way for the archiving of code and data. The response to this initiative has been overwhelmingly positive and we look forward to a greater participation by all members of the community in the future.

#index 1063467
#* Extreme data mining
#@ Sridhar Ramaswamy
#t 2008
#c 5
#! At Google, the quality and speed of statistical data mining algorithms directly affects the usefulness of our search results and the relevance of our targeted advertising. One of the things that makes planet-wide, high throughput, 24/7 data mining so interesting is that all parts of the software stack are involved. This talk will walk up the stack, from the physical machines in warehouse-sized data centers, through networking and secondary storage abstractions to the distributed numerical methods and high throughput training and serving algorithms needed to support online logs processing and machine learning. We will also discuss the significant infrastructure and algorithmic impacts of batch versus online training: both data mining modes have essential roles in Google.

#index 1063468
#* Extreme visualization: squeezing a billion records into a million pixels
#@ Ben Shneiderman
#t 2008
#c 5
#% 172811
#% 172825
#% 202036
#% 265479
#% 270768
#% 286639
#% 301234
#% 339184
#% 378404
#% 434559
#% 434613
#% 441058
#% 564886
#% 568562
#% 619859
#% 641145
#% 641185
#% 641189
#% 641192
#% 641202
#% 662813
#% 726032
#% 729928
#% 757720
#% 789216
#% 813962
#% 821698
#% 824712
#% 841785
#% 844528
#% 860097
#% 863372
#% 910864
#% 910872
#% 954910
#% 1137814
#% 1214048
#! Database searches are usually performed with query languages and form fill in templates, with results displayed in tabular lists. However, excitement is building around dynamic queries sliders and other graphical selectors for query specification, with results displayed by information visualization techniques. These filtering techniques have proven to be effective for many tasks in which visual presentations enable discovery of relationships, clusters, outliers, gaps, and other patterns. Scaling visual presentations from millions to billions of records will require collaborative research efforts in information visualization and database management to enable rapid aggregation, meaningful coordinated windows, and effective summary graphics. This paper describes current and proposed solutions (atomic, aggregated, and density plots) that facilitate sense-making for interactive visual exploration of billion record data sets.

#index 1063469
#* Extreme streaming: business optimization driving algorithmic challenges
#@ William O'Connell
#t 2008
#c 5
#! Organizations are striving for competitive advantage. As a result, business optimization is being pushed to new heights in terms of volume and speed. Areas such as customer profitability, campaign profitability or customer insight for better service are driving new analytical challenges as well as new algorithms over large volumes of data. Using Telecom as an example, this talk will discuss business demands which are driving an evolution of analytics over extremely high volumes of streaming data being ingested into a warehouse -- this business direction is forcing algorithms to evolve. In this environment, we are forced to deal with (1) massive continuous ingest rates, data changes/updates while the same data is being consumed by applications, users and processes, (2) dealing with scalability without disruption, and (3) overcoming the physical limits of the infrastructure (e.g., IO). This talk will address both how these technical challenges are being addressed at a high-level, as well as known problem areas for further research. This talk will also highlight issues of new algorithmic approaches arising from the business needs and the research issues associated with them. Examples include (1) streaming analytics, (2) social analytical algorithms integrated into data mining approaches within these continuously streamed environments and (3) new data types such as voice and textual analytics for customer calling pattern and churn analysis.

#index 1063470
#* Capacity constrained assignment in spatial databases
#@ Leong Hou U;Man Lung Yiu;Kyriakos Mouratidis;Nikos Mamoulis
#t 2008
#c 5
#% 86950
#% 122671
#% 213165
#% 287466
#% 300162
#% 421124
#% 427199
#% 480093
#% 757988
#% 765438
#% 942346
#% 1022250
#! Given a point set P of customers (e.g., WiFi receivers) and a point set Q of service providers (e.g., wireless access points), where each q ∈ Q has a capacity q.k, the capacity constrained assignment (CCA) is a matching M ⊆ Q × P such that (i) each point q ∈ Q (p ∈ P) appears at most k times (at most once) in M, (ii) the size of M is maximized (i.e., it comprises min{|P|, ∑q∈Qq.k} pairs), and (iii) the total assignment cost (i.e., the sum of Euclidean distances within all pairs) is minimized. Thus, the CCA problem is to identify the assignment with the optimal overall quality; intuitively, the quality of q's service to p in a given (q, p) pair is anti-proportional to their distance. Although max-flow algorithms are applicable to this problem, they require the complete distance-based bipartite graph between Q and P. For large spatial datasets, this graph is expensive to compute and it may be too large to fit in main memory. Motivated by this fact, we propose efficient algorithms for optimal assignment that employ novel edge-pruning strategies, based on the spatial properties of the problem. Additionally, we develop approximate (i.e., suboptimal) CCA solutions that provide a trade-off between result accuracy and computation cost, abiding by theoretical quality guarantees. A thorough experimental evaluation demonstrates the efficiency and practicality of the proposed techniques.

#index 1063471
#* ST2B-tree: a self-tunable spatio-temporal b+-tree index for moving objects
#@ Su Chen;Beng Chin Ooi;Kian-Lee Tan;Mario A. Nascimento
#t 2008
#c 5
#% 273890
#% 286929
#% 300174
#% 443397
#% 480587
#% 482100
#% 765453
#% 765454
#% 772839
#% 800572
#% 810061
#% 870306
#% 945789
#% 1015320
#% 1016193
#% 1046510
#! In a moving objects database (MOD) the dataset and the workload change frequently. As the locations of objects change in space and time, the data distribution also changes and the answer for a same query over the same region may vary widely over time. As a result, traditional static indexes are not able to perform well and it is critical to develop self-tuning indexes that can be reconfigured automatically based on the state of the system. Towards this goal we propose the ST2B-tree, a Self-Tunable Spatio-Temporal B+-Tree index for MODs, which is amenable to tuning. Frequent updates to its subtrees allows rebuilding (tuning) a subtree using a different set of reference points and different grid size without significant overhead. We also present an online tuning framework for the ST2B-tree, where the tuning is conducted online and automatically without human intervention, also not interfering with regular functions of the MOD. Our extensive experiments show that the self-tuning process minimizes the effectiveness degradation of the index caused by workload changes at the cost of virtually no overhead.

#index 1063472
#* Scalable network distance browsing in spatial databases
#@ Hanan Samet;Jagan Sankaranarayanan;Houman Alborzi
#t 2008
#c 5
#% 23266
#% 32899
#% 90745
#% 201876
#% 254749
#% 287466
#% 319508
#% 443208
#% 526840
#% 572860
#% 657739
#% 813718
#% 818938
#% 824723
#% 826277
#% 836179
#% 893162
#% 1011871
#% 1015321
#% 1016199
#% 1068601
#! An algorithm is presented for finding the k nearest neighbors in a spatial network in a best-first manner using network distance. The algorithm is based on precomputing the shortest paths between all possible vertices in the network and then making use of an encoding that takes advantage of the fact that the shortest paths from vertex u to all of the remaining vertices can be decomposed into subsets based on the first edges on the shortest paths to them from u. Thus, in the worst case, the amount of work depends on the number of objects that are examined and the number of links on the shortest paths to them from q, rather than depending on the number of vertices in the network. The amount of storage required to keep track of the subsets is reduced by taking advantage of their spatial coherence which is captured by the aid of a shortest path quadtree. In particular, experiments on a number of large road networks as well as a theoretical analysis have shown that the storage has been reduced from O(N3) to O(N1.5) (i.e., by an order of magnitude equal to the square root). The precomputation of the shortest paths along the network essentially decouples the process of computing shortest paths along the network from that of finding the neighbors, and thereby also decouples the domain S of the query objects and that of the objects from which the neighbors are drawn from the domain V of the vertices of the spatial network. This means that as long as the spatial network is unchanged, the algorithm and underlying representation of the shortest paths in the spatial network can be used with different sets of objects.

#index 1063473
#* Discovering bucket orders from full rankings
#@ Jianlin Feng;Qiong Fang;Wilfred Ng
#t 2008
#c 5
#% 272510
#% 326303
#% 330769
#% 564281
#% 654466
#% 801673
#% 805798
#% 823353
#% 881515
#% 915327
#% 975034
#% 991162
#% 991163
#! Discovering a bucket order B from a collection of possibly noisy full rankings is a fundamental problem that relates to various applications involving rankings. Informally, a bucket order is a total order that allows "ties" between items in a bucket. A bucket order B can be viewed as a "representative" that summarizes a given set of full rankings {T1, T2, ..., Tm}, or conversely B can be an "approximation" of some "ground truth" G where the rankings {T1, T2, ..., Tm} are simply the "linear extensions" of G. Current work of finding bucket orders such as the dynamic programming algorithm is mainly developed from the "representative" perspective, which maximizes items' intra-bucket similarity when forming a bucket. The underlying idea of maximizing intra-bucket similarity is realized via minimizing the sum of the deviations of median ranks within a bucket. In contrast, from the "approximation" perspective, since each observed full ranking Ti is simply a linear extension of the given "ground truth" bucket order G, items in a big bucket b in G are forced to have different median ranks, and as a result b will have a big sum of deviations. Thus, minimizing the sum of deviations may result in an undesirable scenario that big buckets are mostly decomposed into small ones. In this paper, we propose a novel heuristic called Abnormal Rank Gap to capture the inter-bucket dissimilarity for better bucket forming. In addition, we propose to use the "closeness" on multiple quantile ranks to determine if two items should be put into the same bucket. We develop a novel bucket order discovering method termed the Bucket Gap algorithm. Our extensive experiments demonstrate that the Bucket Gap algorithm significantly outperforms the major related work, i.e., the Bucket Pivot algorithm. In particular, the error distance of the generated bucket order can be reduced by about 30% on a real paleontological dataset and the noise tolerance can be increased from 30% to 50% in the synthetic dataset.

#index 1063474
#* Ad-hoc aggregations of ranked lists in the presence of hierarchies
#@ Nilesh Bansal;Sudipto Guha;Nick Koudas
#t 2008
#c 5
#% 46803
#% 330769
#% 340936
#% 378404
#% 480330
#% 643566
#% 728195
#% 763882
#% 869482
#% 875001
#% 893126
#% 893128
#% 1016183
#% 1022269
#% 1022278
#% 1022338
#% 1272396
#! A variety of web sites and web based services produce textual lists at varying time granularities ranked according to several criteria. For example, Google Trends produces lists of popular query keywords which can be visualized according to several criteria. At Flickr, lists of popular tags used to tag the images uploaded can be visualized as a cloud based on their popularity. Identification of the k most popular terms can be easily conducted by utilizing well known rank aggregation algorithms. In this paper we take a different approach to information discovery from such ranked lists. We maintain the same rank aggregation framework but we elevate terms at a higher level by making use of popular term hierarchies commonly available. Under such a transformation we show that typical early stopping certificates available for rank aggregation algorithms are no longer applicable. Based on this observation, in this paper, we present a probabilistic framework for early stopping in this setting. We introduce a relaxed version of the rank aggregation problem involving a deterministic stopping condition with user specified precision. We introduce an algorithm pH -- RA for the solution of this problem. In addition we introduce techniques to improve the performance of pH -- RAeven further via precomputation utilizing a sparse set system. Through a detailed experimental evaluation using synthetic and real datasets we demonstrate the efficiency of our framework.

#index 1063475
#* ARCube: supporting ranking aggregate queries in partially materialized data cubes
#@ Tianyi Wu;Dong Xin;Jiawei Han
#t 2008
#c 5
#% 210182
#% 223781
#% 227880
#% 227894
#% 248806
#% 252304
#% 273916
#% 290703
#% 333854
#% 333925
#% 397378
#% 397388
#% 420053
#% 463760
#% 479646
#% 479795
#% 571045
#% 659993
#% 765418
#% 810018
#% 874975
#% 875001
#% 875002
#% 893126
#% 893127
#% 893128
#% 960243
#% 993996
#% 1015294
#% 1016173
#% 1022276
#% 1698966
#! Supporting ranking queries in database systems has been a popular research topic recently. However, there is a lack of study on supporting ranking queries in data warehouses where ranking is on multidimensional aggregates instead of on measures of base facts. To address this problem, we propose a query execution model to answer different types of ranking aggregate queries based on a unified, partial cube structure, ARCube. The query execution model follows a candidate generation and verification framework, where the most promising candidate cells are generated using a set of high-level guiding cells. We also identify a bounding principle for effective pruning: once a guiding cell is pruned, all of its children candidate cells can be pruned. We further address the problem of efficient online candidate aggregation and verification by developing a chunk-based execution model to verify a bulk of candidates within a bounded memory buffer. Our extensive performance study shows that the new framework not only leads to an order of magnitude performance improvements over the state-of-the-art method, but also is much more flexible in terms of the types of ranking aggregate queries supported.

#index 1063476
#* Towards identity anonymization on graphs
#@ Kun Liu;Evimaria Terzi
#t 2008
#c 5
#% 70370
#% 248030
#% 800515
#% 801690
#% 853532
#% 864412
#% 904307
#% 956511
#% 1074831
#% 1206763
#% 1415851
#! The proliferation of network data in various application domains has raised privacy concerns for the individuals involved. Recent studies show that simply removing the identities of the nodes before publishing the graph/social network data does not guarantee privacy. The structure of the graph itself, and in its basic form the degree of the nodes, can be revealing the identities of individuals. To address this issue, we study a specific graph-anonymization problem. We call a graph k-degree anonymous if for every node v, there exist at least k-1 other nodes in the graph with the same degree as v. This definition of anonymity prevents the re-identification of individuals by adversaries with a priori knowledge of the degree of certain nodes. We formally define the graph-anonymization problem that, given a graph G, asks for the k-degree anonymous graph that stems from G with the minimum number of graph-modification operations. We devise simple and efficient algorithms for solving this problem. Our algorithms are based on principles related to the realizability of degree sequences. We apply our methods to a large spectrum of synthetic and real datasets and demonstrate their efficiency and practical utility.

#index 1063477
#* Dynamic anonymization: accurate statistical analysis with privacy preservation
#@ Xiaokui Xiao;Yufei Tao
#t 2008
#c 5
#% 149
#% 67453
#% 263982
#% 297186
#% 379248
#% 419405
#% 443463
#% 576110
#% 576111
#% 576761
#% 729930
#% 800559
#% 801690
#% 809244
#% 809245
#% 810010
#% 810011
#% 810028
#% 824726
#% 864406
#% 864412
#% 881483
#% 893100
#% 893101
#% 960239
#% 960289
#% 960291
#% 1022246
#% 1022247
#% 1022248
#% 1022264
#% 1022265
#% 1022266
#% 1700134
#% 1740518
#! A statistical database (StatDB) retrieves only aggregate results, as opposed to individual tuples. This paper investigates the construction of a privacy preserving StatDB that can (i) accurately answer an infinite number of counting queries, and (ii) effectively protect privacy against an adversary that may have acquired all the previous query results. The core of our solutions is a novel technique called dynamic anonymization. Specifically, given a query, we on the fly compute a tailor-made anonymized version of the microdata, which maximizes the precision of the query result. Privacy preservation is achieved by ensuring that the combination of all the versions deployed to process the past queries does not allow accurate inference of sensitive information. Extensive experiments with real data confirm that our technique enables highly effective data analysis, while offering strong privacy guarantees.

#index 1063478
#* Private queries in location based services: anonymizers are not necessary
#@ Gabriel Ghinita;Panos Kalnis;Ali Khoshgozaran;Cyrus Shahabi;Kian-Lee Tan
#t 2008
#c 5
#% 152934
#% 213981
#% 443397
#% 443463
#% 593711
#% 593800
#% 743280
#% 800559
#% 812799
#% 843877
#% 893151
#% 907397
#% 911803
#% 956531
#% 972316
#% 1013611
#% 1058620
#% 1409349
#% 1409350
#% 1729021
#% 1740517
#! Mobile devices equipped with positioning capabilities (e.g., GPS) can ask location-dependent queries to Location Based Services (LBS). To protect privacy, the user location must not be disclosed. Existing solutions utilize a trusted anonymizer between the users and the LBS. This approach has several drawbacks: (i) All users must trust the third party anonymizer, which is a single point of attack. (ii) A large number of cooperating, trustworthy users is needed. (iii) Privacy is guaranteed only for a single snapshot of user locations; users are not protected against correlation attacks (e.g., history of user movement). We propose a novel framework to support private location-dependent queries, based on the theoretical work on Private Information Retrieval (PIR). Our framework does not require a trusted third party, since privacy is achieved via cryptographic techniques. Compared to existing work, our approach achieves stronger privacy for snapshots of user locations; moreover, it is the first to provide provable privacy guarantees against correlation attacks. We use our framework to implement approximate and exact algorithms for nearest-neighbor search. We optimize query execution by employing data mining techniques, which identify redundant computations. Contrary to common belief, the experimental results suggest that PIR approaches incur reasonable overhead and are applicable in practice.

#index 1063479
#* Near-optimal algorithms for shared filter evaluation in data stream systems
#@ Zhen Liu;Srinivasan Parthasarathy;Anand Ranganathan;Hao Yang
#t 2008
#c 5
#% 152940
#% 271199
#% 272005
#% 287461
#% 300179
#% 333848
#% 333962
#% 338354
#% 341672
#% 397353
#% 480296
#% 480649
#% 570879
#% 659996
#% 765435
#% 793898
#% 805893
#% 874896
#% 977005
#% 990342
#% 1015360
#% 1740645
#! We consider the problem of evaluating multiple overlapping queries defined on data streams, where each query is a conjunction of multiple filters and each filter may be shared across multiple queries. Efficient support for overlapping queries is a critical issue in the emerging data stream systems, and this is particularly the case when filters are expensive in terms of their computational complexity and processing time. This problem generalizes other well-known problems such as pipelined filter ordering and set cover, and is not only NP-Hard but also hard to approximate within a factor of o(log n) from the optimum, where n is the number of queries. In this paper, we present two near-optimal approximation lgorithms with provably-good performance guarantees for the evaluation of overlapping queries. We present an edge-coverage based Greedy algorithm which achieves an approximation ratio of (1 + log(n) + log(α)), where n is the number of queries and α is the average number of filters in a query. We also present a randomized, fast and easily parallelizable Harmonic algorithm which achieves an approximation ratio of 2β, where β is the maximum number of filters in a query. We have implemented these algorithms in a prototype system, and evaluated their performance using extensive experiments in the context of multimedia stream analysis. The results show that our Greedy algorithm consistently outperforms other known algorithms under various settings and scales well as the numbers of queries and filters increase.

#index 1063480
#* Efficient pattern matching over event streams
#@ Jagrati Agrawal;Yanlei Diao;Daniel Gyllstrom;Neil Immerman
#t 2008
#c 5
#% 13014
#% 271199
#% 317105
#% 333938
#% 459001
#% 464221
#% 480938
#% 481448
#% 482088
#% 503905
#% 631974
#% 763881
#% 810096
#% 824747
#% 838512
#% 875004
#% 903341
#% 909019
#% 977010
#% 981724
#% 1020760
#% 1206641
#% 1688281
#! Pattern matching over event streams is increasingly being employed in many areas including financial services, RFIDbased inventory management, click stream analysis, and electronic health systems. While regular expression matching is well studied, pattern matching over streams presents two new challenges: Languages for pattern matching over streams are significantly richer than languages for regular expression matching. Furthermore, efficient evaluation of these pattern queries over streams requires new algorithms and optimizations: the conventional wisdom for stream query processing (i.e., using selection-join-aggregation) is inadequate. In this paper, we present a formal evaluation model that offers precise semantics for this new class of queries and a query evaluation framework permitting optimizations in a principled way. We further analyze the runtime complexity of query evaluation using this model and develop a suite of techniques that improve runtime efficiency by exploiting sharing in storage and processing. Our experimental results provide insights into the various factors on runtime performance and demonstrate the significant performance gains of our sharing techniques.

#index 1063481
#* Scalable regular expression matching on data streams
#@ Anirban Majumder;Rajeev Rastogi;Sriram Vanama
#t 2008
#c 5
#% 282600
#% 321327
#% 342372
#% 490588
#% 571046
#% 725308
#% 726620
#% 791182
#% 889657
#% 903341
#% 909019
#% 1016180
#! Regular Expression (RE) matching has important applications in the areas of XML content distribution and network security. In this paper, we present the end-to-end design of a high performance RE matching system. Our system combines the processing efficiency of Deterministic Finite Automata (DFA) with the space efficiency of Non-deterministic Finite Automata (NFA) to scale to hundreds of REs. In experiments with real-life RE data on data streams, we found that a bulk of the DFA transitions are concentrated around a few DFA states. We exploit this fact to cache only the frequent core of each DFA in memory as opposed to the entire DFA (which may be exponential in size). Further, we cluster REs such that REs whose interactions cause an exponential increase in the number of states are assigned to separate groups -- this helps to improve cache hits by controlling the overall DFA size. To the best of our knowledge, ours is the first end-to-end system capable of matching REs at high speeds and in their full generality. Through a clever combination of RE grouping, and static and dynamic caching, it is able to perform RE matching at high speeds, even in the presence of limited memory. Through experiments with real-life data sets, we show that our RE matching system convincingly outperforms a state-of-the-art Network Intrusion Detection tool with support for efficient RE matching.

#index 1063482
#* CRD: fast co-clustering on large datasets utilizing sampling-based matrix decomposition
#@ Feng Pan;Xiang Zhang;Wei Wang
#t 2008
#c 5
#% 309128
#% 347192
#% 466675
#% 469422
#% 643008
#% 729918
#% 769883
#% 769928
#% 823343
#% 823396
#% 864476
#% 870226
#% 881468
#! The problem of simultaneously clustering columns and rows (co-clustering) arises in important applications, such as text data mining, microarray analysis, and recommendation system analysis. Compared with the classical clustering algorithms, co-clustering algorithms have been shown to be more effective in discovering hidden clustering structures in the data matrix. The complexity of previous co-clustering algorithms is usually O(m X n), where m and n are the numbers of rows and columns in the data matrix respectively. This limits their applicability to data matrices involving a large number of columns and rows. Moreover, some huge datasets can not be entirely held in main memory during co-clustering which violates the assumption made by the previous algorithms. In this paper, we propose a general framework for fast co-clustering large datasets, CRD. By utilizing recently developed sampling-based matrix decomposition methods, CRD achieves an execution time linear in m and n. Also, CRD does not require the whole data matrix be in the main memory. We conducted extensive experiments on both real and synthetic data. Compared with previous co-clustering algorithms, CRD achieves competitive accuracy but with much less computational cost.

#index 1063483
#* Outlier-robust clustering using independent components
#@ Christian Böhm;Christos Faloutsos;Claudia Plant
#t 2008
#c 5
#% 210173
#% 248792
#% 273890
#% 300131
#% 300136
#% 466425
#% 765439
#% 770830
#% 810047
#% 881462
#! How can we efficiently find a clustering, i.e. a concise description of the cluster structure, of a given data set which contains an unknown number of clusters of different shape and distribution and is contaminated by noise? Most existing clustering methods are restricted to the Gaussian cluster model and are very sensitive to noise. If the cluster content follows a non-Gaussian distribution and/or the data set contains a few outliers belonging to no cluster, then the computed data distribution does not match well the true data distribution, or an unnaturally high number of clusters is required to represent the true data distribution of the data set. In this paper we propose OCI (Outlier-robust Clustering using Independent Components), a clustering method which overcomes these problems by (1) applying the exponential power distribution (EPD) as cluster model which is a generalization of Gaussian, uniform, Laplacian and many other distribution functions, (2) applying the Independent Component Analysis (ICA) for both determining the main directions inside a cluster as well as finding split planes in a top-down clustering approach, and (3) defining an efficient and effective filter for outliers, based on EPD and ICA. Our method is parameter-free and as a top-down clustering approach very efficient. An extensive experimental evaluation shows both the accuracy of the obtained clustering result as well as the efficiency of our method.

#index 1063484
#* Efficient EMD-based similarity search in multimedia databases via flexible dimensionality reduction
#@ Marc Wichterich;Ira Assent;Philipp Kranen;Thomas Seidl
#t 2008
#c 5
#% 248797
#% 259617
#% 391420
#% 481947
#% 635694
#% 718437
#% 812483
#% 864398
#% 910885
#% 1688294
#! The Earth Mover's Distance (EMD) was developed in computer vision as a flexible similarity model that utilizes similarities in feature space to define a high quality similarity measure in feature representation space. It has been successfully adopted in a multitude of applications with low to medium dimensionality. However, multimedia applications commonly exhibit high-dimensional feature representations for which the computational complexity of the EMD hinders its adoption. An efficient query processing approach that mitigates and overcomes this effect is crucial. We propose novel dimensionality reduction techniques for the EMD in a filter-and-refine architecture for efficient lossless retrieval. Thorough experimental evaluation on real world data sets demonstrates a substantial reduction of the number of expensive high-dimensional EMD computations and thus remarkably faster response times. Our techniques are fully flexible in the number of reduced dimensions, which is a novel feature in approximation techniques for the EMD.

#index 1063485
#* Monochromatic and bichromatic reverse skyline search over uncertain databases
#@ Xiang Lian;Lei Chen
#t 2008
#c 5
#% 213975
#% 427199
#% 465167
#% 480661
#% 654480
#% 654487
#% 659937
#% 772835
#% 810049
#% 824728
#% 864394
#% 864396
#% 976788
#% 1016191
#% 1016202
#% 1022203
#% 1022226
#% 1206646
#% 1206689
#% 1206716
#% 1206717
#% 1206781
#% 1408794
#% 1669490
#! Reverse skyline queries over uncertain databases have many important applications such as sensor data monitoring and business planning. Due to the existence of uncertainty in many real-world data, answering reverse skyline queries accurately and efficiently over uncertain data has become increasingly important. In this paper, we model the probabilistic reverse skyline query on uncertain data, in both monochromatic and bichromatic cases, and propose effective pruning methods to reduce the search space of query processing. Moreover, efficient query procedures have been presented seamlessly integrating the proposed pruning methods. Extensive experiments have demonstrated the efficiency and effectiveness of our proposed approach with various experimental settings.

#index 1063486
#* Angle-based space partitioning for efficient parallel skyline computation
#@ Akrivi Vlachou;Christos Doulkeridis;Yannis Kotidis
#t 2008
#c 5
#% 285932
#% 340176
#% 411694
#% 465167
#% 479649
#% 480671
#% 806212
#% 824706
#% 864451
#% 864453
#% 873881
#% 878649
#% 907529
#% 968270
#% 993954
#% 1688253
#% 1727523
#! Recently, skyline queries have attracted much attention in the database research community. Space partitioning techniques, such as recursive division of the data space, have been used for skyline query processing in centralized, parallel and distributed settings. Unfortunately, such grid-based partitioning is not suitable in the case of a parallel skyline query, where allpartitions are examined at the same time, since many data partitions do not contribute to the overall skyline set, resulting in a lot of redundant processing. In this paper we propose a novel angle-based space partitioning scheme using the hyperspherical coordinates of the data points. We demonstrate both formally as well as through an exhaustive set of experiments that this new scheme is very suitable for skyline query processing in a parallel share-nothing architecture. The intuition of our partitioning technique is that the skyline points are equally spread to all partitions. We also show that partitioning the data according to the hyperspherical coordinates manages to increase the average pruning power of points within a partition. Our novel partitioning scheme alleviates most of the problems of traditional grid partitioning techniques, thus managing to reduce the response time and share the computational workload more fairly. As demonstrated by our experimental study, our technique outperforms grid partitioning in all cases, thus becoming an efficient and scalable solution for skyline query processing in parallel environments.

#index 1063487
#* Categorical skylines for streaming data
#@ Nikos Sarkas;Gautam Das;Nick Koudas;Anthony K. H. Tung
#t 2008
#c 5
#% 386207
#% 388196
#% 465167
#% 468074
#% 479648
#% 480671
#% 654480
#% 765455
#% 800555
#% 810024
#% 824670
#% 849816
#% 864495
#% 875012
#% 875023
#% 993954
#% 1016196
#% 1022217
#% 1022225
#% 1058620
#% 1688273
#! The problem of skyline computation has attracted considerable research attention. In the categorical domain the problem becomes more complicated, primarily due to the partially-ordered nature of the attributes of tuples. In this paper, we initiate a study of streaming categorical skylines. We identify the limitations of existing work for offline categorical skyline computation and realize novel techniques for the problem of maintaining the skyline of categorical data in a streaming environment. In particular, we develop a lightweight data structure for indexing the tuples in the streaming buffer, that can gracefully adapt to tuples with many attributes and partially ordered domains of any size and complexity. Additionally, our study of the dominance relation in the dual space allows us to utilize geometric arrangements in order to index the categorical skyline and efficiently evaluate dominance queries. Lastly, a thorough experimental study evaluates the efficiency of the proposed techniques.

#index 1063488
#* Building a database on S3
#@ Matthias Brantner;Daniela Florescu;David Graf;Donald Kossmann;Tim Kraska
#t 2008
#c 5
#% 9241
#% 116073
#% 201869
#% 210202
#% 286929
#% 336201
#% 340297
#% 397295
#% 403195
#% 760777
#% 770313
#% 923570
#% 998845
#% 1016241
#% 1022200
#! There has been a great deal of hype about Amazon's simple storage service (S3). S3 provides infinite scalability and high availability at low cost. Currently, S3 is used mostly to store multi-media documents (videos, photos, audio) which are shared by a community of people and rarely updated. The purpose of this paper is to demonstrate the opportunities and limitations of using S3 as a storage system for general-purpose database applications which involve small objects and frequent updates. Read, write, and commit protocols are presented. Furthermore, the cost ($), performance, and consistency properties of such a storage system are studied.

#index 1063489
#* Paths to stardom: calibrating the potential of a peer-based data management system
#@ Mihai Lupu;Beng Chin Ooi;Y. C. Tay
#t 2008
#c 5
#% 27190
#% 105972
#% 259065
#% 340175
#% 340176
#% 401980
#% 646237
#% 654468
#% 723453
#% 765444
#% 810034
#% 824706
#% 839329
#% 873996
#% 874970
#% 874978
#% 960252
#% 960276
#% 963648
#% 963874
#% 1386265
#% 1700057
#% 1768420
#! As peer-to-peer (P2P) networks become more familiar to the database community, intense interest has built up in using their scalability and resilience properties to scale database applications. Indexing methods are adapted on top of P2P networks and querying methods are developed to handle the data distribution on different nodes. These procedures largely depend on how nodes are connected to each other. So far, limited attempts have been made to compare all these systems in a generalized framework. This is because the systems are quite different from each other, and there are so many of them that brute force comparison is practically impossible. Fortunately, it has recently been observed that a large subset of the most important P2P networks share a common algebraic and combinatorial base, in the form of Cayley graphs. The specific requirements of Peer-based Data Management Systems (PDMS), such as query completeness, range queries, load balancing, communication overhead, and scalability are strongly related to the properties of the underlying graphs, and naturally, some graphs are better than others. We conduct a comprehensive graph-theoretic analysis from the point of view of PDMS and identify the necessary conditions for a graph to be considered a potential network structure for a PDMS. In so doing, we provide a basis for the future development of such networks. We complement our analytical study with extensive experimental results and identify three measures that provide significant information about the potential of a [Cayley] graph to support the requirements of a PDMS.

#index 1063490
#* Just-in-time query retrieval over partially indexed data on structured P2P overlays
#@ Sai Wu;Jianzhong Li;Beng Chin Ooi;Kian-Lee Tan
#t 2008
#c 5
#% 64791
#% 186522
#% 333990
#% 340175
#% 340176
#% 453509
#% 463917
#% 505869
#% 580528
#% 608616
#% 723445
#% 723448
#% 729343
#% 745498
#% 765258
#% 765433
#% 768520
#% 770901
#% 772021
#% 839334
#% 847499
#% 903174
#% 960252
#% 1015281
#% 1016165
#% 1688254
#! Structured peer-to-peer (P2P) overlays have been successfully employed in many applications to locate content. However, they have been less effective in handling massive amounts of data because of the high overhead of maintaining indexes. In this paper, we propose PISCES, a Peer-based system that Indexes Selected Content for Efficient Search. Unlike traditional approaches that index all data, PISCES identifies a subset of tuples to index based on some criteria (such as query frequency, update frequency, index cost, etc.). In addition, a coarse-grained range index is built to facilitate the processing of queries that cannot be fully answered by the tuple-level index. More importantly, PISCES can adaptively self-tune to optimize the subset of tuples to be indexed. That is, the (partial) index in PISCES is built in a Just-In-Time (JIT) manner. Beneficial tuples for current users are pulled for indexing while indexed tuples with infrequent access and high maintenance cost are discarded. We also introduce a light-weight monitoring scheme for structured networks to collect the necessary statistics. We have conducted an extensive experimental study on PlanetLab to illustrate the feasibility, practicality and efficiency of PISCES. The results show that PISCES incurs lower maintenance cost and offers better search and query efficiency compared to existing methods.

#index 1063491
#* Efficient storage scheme and query processing for supply chain management using RFID
#@ Chun-Hee Lee;Chin-Wan Chung
#t 2008
#c 5
#% 333981
#% 397366
#% 480489
#% 654450
#% 654451
#% 745461
#% 745479
#% 824747
#% 824748
#% 864470
#% 893102
#% 893103
#% 893157
#% 902325
#% 915814
#% 1016227
#% 1016228
#% 1688279
#% 1698904
#! As the size of an RFID tag becomes smaller and the price of the tag gets lower, RFID technology has been applied to a wide range of areas. Recently, RFID has been adopted in the business area such as supply chain management. Since companies can get movement information for products easily using the RFID technology, it is expected to revolutionize supply chain management. However, the amount of RFID data in supply chain management is huge. Therefore, it requires much time to extract valuable information from RFID data for supply chain management. In this paper, we define query templates for tracking queries and path oriented queries to analyze the supply chain. We then propose an effective path encoding scheme to encode the flow information for products. To retrieve the time information for products efficiently, we utilize a numbering scheme used in the XML area. Based on the path encoding scheme and the numbering scheme, we devise a storage scheme to process tracking queries and path oriented queries efficiently. Finally, we propose a method which translates the queries to SQL queries. Experimental results show that our approach can process the queries efficiently. On the average, our approach is about 680 times better than a recent technique in terms of query performance.

#index 1063492
#* Relational-style XML query
#@ Taro L. Saito;Shinichi Morishita
#t 2008
#c 5
#% 278445
#% 330627
#% 390132
#% 458850
#% 480489
#% 562456
#% 654442
#% 742566
#% 765408
#% 765422
#% 771227
#% 810044
#% 810052
#% 893097
#% 944093
#% 994015
#% 1016135
#% 1915874
#! We study the problem of querying relational data embedded in XML. Relational data can be represented by various tree structures in XML. However, current XML query methods, such as XPath and XQuery, demand explicit path expressions, and thus it is quite difficult for users to produce correct XML queries in the presence of structural variations. To solve this problem, we introduce a novel query method that automatically discovers various XML structures derived from relational data. A challenge in implementing our method is to reduce the cost of enumerating all possible tree structures that match the query. We show that the notion of functional dependencies has an important role in generating efficient query schedules that avoid irrelevant tree structures. Our proposed method, the relational-style XML query, has several advantages over traditional XML data management. These include removing the burden of designing strict tree-pattern schemas, enhancing the descriptions of relational data with XML's rich semantics, and taking advantage of schema evolution capability of XML. In addition, the independence of query statements from the underlying XML structure is advantageous for integrating XML data from several sources. We present extensive experimental results that confirm the scalability and tolerance of our query method for various sizes of XML data containing structural variations.

#index 1063493
#* Query biased snippet generation in XML search
#@ Yu Huang;Ziyang Liu;Yi Chen
#t 2008
#c 5
#% 262036
#% 342678
#% 397130
#% 449749
#% 654442
#% 810052
#% 863389
#% 864450
#% 875003
#% 956599
#% 960243
#% 960259
#% 960261
#% 987208
#% 1015258
#% 1016135
#% 1019060
#% 1077150
#! Snippets are used by almost every text search engine to complement ranking scheme in order to effectively handle user searches, which are inherently ambiguous and whose relevance semantics are difficult to assess. Despite the fact that XML is a standard representation format of web data, research on generating result snippets for XML search remains untouched. In this paper we present a system, eXtract, which addresses this important yet open problem. We identify that a good XML result snippet should be a self-contained meaningful information unit of a small size that effectively summarizes this query result and differentiates it from others, according to which users can quickly assess the relevance of the query result. We have designed and implemented a novel algorithm to satisfy these requirements and verified its efficiency and effectiveness through experiments.

#index 1063494
#* Cooperative XPath caching
#@ Kostas Lillis;Evaggelia Pitoura
#t 2008
#c 5
#% 340175
#% 401983
#% 413659
#% 481916
#% 654483
#% 733593
#% 740758
#% 783698
#% 821923
#% 824690
#% 824706
#% 907458
#% 1015260
#% 1015327
#% 1016134
#% 1725999
#! Motivated by the fact that XML is increasingly being used in distributed applications, we propose building a cooperative caching scheme for XML documents. Our scheme allows sharing cache content among a number of peers. To facilitate sharing, a distributed prefix-based index is built based on the queries whose results are cached. In the loosely-coupled sharing approach, each peer stores in its local cache results of its own queries and just publishes the associated queries to the index. In the tightly-coupled approach, each peer is assigned a specific part of the query space and stores in its local cache the results of the corresponding queries. Both approaches result in a dynamic organization of content that evolves over time based on the query load, the number of peers and the overall storage available. We present a number of associated design choices such as using a DHT for distributing the prefix-based index and a proactive cache replacement policy. We also report on a number of experiments that show the benefits of cooperative caching and highlight the pros and cons of loosely and tightly coupled cache sharing.

#index 1063495
#* XML query optimization in the presence of side effects
#@ Giorgio Ghelli;Nicola Onose;Kristoffer Rose;Jerome Simeon
#t 2008
#c 5
#% 142073
#% 163444
#% 248788
#% 264996
#% 292231
#% 348130
#% 736843
#% 864401
#% 875028
#% 894440
#% 936441
#% 994015
#% 1408530
#% 1661451
#% 1725653
#% 1728673
#! The emergence of database languages with side effects, notably for XML, raises significant challenges for database compilers and optimizers. In this paper, we extend an algebra for the W3C XML query language with operations that allow data to be immediately updated. We study the impact of that extension on logical optimization, join detection, and pipelining. The main result of this work is to show that, with proper care, a number of important optimizations based on nested relational algebras remain applicable in the presence of side effects. Our approach relies on an analysis of the conditions that must be checked in order for algebraic rewritings to hold. An implementation and experimental results demonstrate the effectiveness of the approach.

#index 1063496
#* Cost-based variable-length-gram selection for string collections to support approximate queries efficiently
#@ Xiaochun Yang;Bin Wang;Chen Li
#t 2008
#c 5
#% 210189
#% 273705
#% 333679
#% 480654
#% 654467
#% 745489
#% 765463
#% 783499
#% 824678
#% 824684
#% 863382
#% 864392
#% 875066
#% 893164
#% 956458
#% 956506
#% 1022218
#% 1022227
#% 1206665
#% 1221065
#! Approximate queries on a collection of strings are important in many applications such as record linkage, spell checking, and Web search, where inconsistencies and errors exist in data as well as queries. Several existing algorithms use the concept of "grams," which are substrings of strings used as signatures for the strings to build index structures. A recently proposed technique, called VGRAM, improves the performance of these algorithms by using a carefully chosen dictionary of variable-length grams based on their requencies in the string collection. Since an index structure using fixed-length grams can be viewed as a special case of VGRAM, a fundamental problem arises naturally: what is the relationship between the gram dictionary and the performance of queries? We study this problem in this paper. We propose a dynamic programming algorithm for computing a tight lower bound on the number of common grams shared by two similar strings in order to improve query performance. We analyze how a gram dictionary affects the index structure of the string collection and ultimately the performance of queries. We also propose an algorithm for automatically computing a dictionary of high-quality grams for a workload of queries. Our experiments on real data sets show the improvement on query performance achieved by these techniques. To our best knowledge, this study is the first cost-based quantitative approach to deciding good grams for approximate string queries.

#index 1063497
#* Approximate embedding-based subsequence matching of time series
#@ Vassilis Athitsos;Panagiotis Papapetrou;Michalis Potamias;George Kollios;Dimitrios Gunopulos
#t 2008
#c 5
#% 172949
#% 201893
#% 227857
#% 248798
#% 286550
#% 310545
#% 316526
#% 330932
#% 342828
#% 397381
#% 443517
#% 451653
#% 462231
#% 464994
#% 479649
#% 479973
#% 480133
#% 480307
#% 534183
#% 578407
#% 650248
#% 654456
#% 723700
#% 727870
#% 729931
#% 731409
#% 745496
#% 799396
#% 809264
#% 810065
#% 810067
#% 893163
#% 993965
#% 1022237
#% 1699793
#! A method for approximate subsequence matching is introduced, that significantly improves the efficiency of subsequence matching in large time series data sets under the dynamic time warping (DTW) distance measure. Our method is called EBSM, shorthand for Embedding-Based Subsequence Matching. The key idea is to convert subsequence matching to vector matching using an embedding. This embedding maps each database time series into a sequence of vectors, so that every step of every time series in the database is mapped to a vector. The embedding is computed by applying full dynamic time warping between reference objects and each database time series. At runtime, given a query object, an embedding of that object is computed in the same manner, by running dynamic time warping between the reference objects and the query. Comparing the embedding of the query with the database vectors is used to efficiently identify relatively few areas of interest in the database sequences. Those areas of interest are then fully explored using the exact DTW-based subsequence matching algorithm. Experiments on a large, public time series data set produce speedups of over one order of magnitude compared to brute-force search, with very small losses (

#index 1063498
#* Sampling time-based sliding windows in bounded space
#@ Rainer Gemulla;Wolfgang Lehner
#t 2008
#c 5
#% 327
#% 1331
#% 299989
#% 379444
#% 414993
#% 479648
#% 482123
#% 783741
#% 864393
#% 893138
#% 960250
#% 1036076
#! Random sampling is an appealing approach to build synopses of large data streams because random samples can be used for a broad spectrum of analytical tasks. Users are often interested in analyzing only the most recent fraction of the data stream in order to avoid outdated results. In this paper, we focus on sampling schemes that sample from a sliding window over a recent time interval; such windows are a popular and highly comprehensible method to model recency. In this setting, the main challenge is to guarantee an upper bound on the space consumption of the sample while using the allotted space efficiently at the same time. The difficulty arises from the fact that the number of items in the window is unknown in advance and may vary significantly over time, so that the sampling fraction has to be adjusted dynamically. We consider uniform sampling schemes, which produce each sample of the same size with equal probability, and stratified sampling schemes, in which the window is divided into smaller strata and a uniform sample is maintained per stratum. For uniform sampling, we prove that it is impossible to guarantee a minimum sample size in bounded space. We then introduce a novel sampling scheme called bounded priority sampling (BPS), which requires only bounded space. We derive a lower bound on the expected sample size and show that BPS quickly adapts to changing data rates. For stratified sampling, we propose a merge-based stratification scheme (MBS), which maintains strata of approximately equal size. Compared to naive stratification, MBS has the advantage that the sample is evenly distributed across the window, so that no part of the window is over- or underrepresented. We conclude the paper with a feasibility study of our algorithms on large real-world datasets.

#index 1063499
#* Mining relationships among interval-based events for classification
#@ Dhaval Patel;Wynne Hsu;Mong Li Lee
#t 2008
#c 5
#% 136350
#% 197394
#% 319244
#% 463903
#% 464996
#% 729946
#% 844326
#% 975048
#% 1250160
#% 1389009
#! Existing temporal pattern mining assumes that events do not have any duration. However, events in many real world applications have durations, and the relationships among these events are often complex. These relationships are modeled using a hierarchical representation that extends Allen's interval algebra. However, this representation is lossy as the exact relationships among the events cannot be fully recovered. In this paper, we augment the hierarchical representation with additional information to achieve a lossless representation. An efficient algorithm called IEMiner is designed to discover frequent temporal patterns from interval-based events. The algorithm employs two optimization techniques to reduce the search space and remove non-promising candidates. From the discovered temporal patterns, we build an interval-based classifier called IEClassifier to differentiate closely related classes. Experiments on both synthetic and real world datasets indicate the efficiency and scalability of the proposed approach, as well as the improved accuracy of IEClassifier.

#index 1063500
#* Graphs-at-a-time: query language and access methods for graph databases
#@ Huahai He;Ambuj K. Singh
#t 2008
#c 5
#% 248014
#% 251063
#% 268797
#% 268799
#% 344549
#% 378391
#% 386623
#% 481434
#% 562456
#% 619786
#% 722530
#% 765429
#% 800534
#% 810068
#% 824692
#% 864425
#% 864462
#% 885468
#% 903341
#% 905847
#% 937108
#% 960304
#% 960305
#% 1022280
#% 1688299
#! With the prevalence of graph data in a variety of domains, there is an increasing need for a language to query and manipulate graphs with heterogeneous attributes and structures. We propose a query language for graph databases that supports arbitrary attributes on nodes, edges, and graphs. In this language, graphs are the basic unit of information and each query manipulates one or more collections of graphs. To allow for flexible compositions of graph structures, we extend the notion of formal languages from strings to the graph domain. We present a graph algebra extended from the relational algebra in which the selection operator is generalized to graph pattern matching and a composition operator is introduced for rewriting matched graphs. Then, we investigate access methods of the selection operator. Pattern matching over large graphs is challenging due to the NP-completeness of subgraph isomorphism. We address this by a combination of techniques: use of neighborhood subgraphs and profiles, joint reduction of the search space, and optimization of the search order. Experimental results on real and synthetic large graphs demonstrate that our graph specific optimizations outperform an SQL-based implementation by orders of magnitude.

#index 1063501
#* Graph summarization with bounded error
#@ Saket Navlakha;Rajeev Rastogi;Nisheeth Shrivastava
#t 2008
#c 5
#% 36672
#% 227883
#% 268079
#% 274612
#% 281214
#% 290830
#% 408396
#% 479969
#% 479984
#% 480124
#% 480306
#% 604754
#% 654497
#% 656242
#% 656281
#% 656282
#% 660000
#% 754117
#% 787098
#% 799747
#% 823395
#% 824711
#% 894441
#% 960108
#% 993995
#% 1002036
#! We propose a highly compact two-part representation of a given graph G consisting of a graph summary and a set of corrections. The graph summary is an aggregate graph in which each node corresponds to a set of nodes in G, and each edge represents the edges between all pair of nodes in the two sets. On the other hand, the corrections portion specifies the list of edge-corrections that should be applied to the summary to recreate G. Our representations allow for both lossless and lossy graph compression with bounds on the introduced error. Further, in combination with the MDL principle, they yield highly intuitive coarse-level summaries of the input graph G. We develop algorithms to construct highly compressed graph representations with small sizes and guaranteed accuracy, and validate our approach through an extensive set of experiments with multiple real-life graph data sets. To the best of our knowledge, this is the first work to compute graph summaries using the MDL principle, and use the summaries (along with corrections) to compress graphs with bounded error.

#index 1063502
#* Mining significant graph patterns by leap search
#@ Xifeng Yan;Hong Cheng;Jiawei Han;Philip S. Yu
#t 2008
#c 5
#% 152934
#% 280409
#% 299985
#% 342604
#% 420126
#% 466644
#% 577214
#% 629708
#% 722920
#% 729938
#% 765429
#% 813990
#% 840863
#% 915228
#% 915350
#% 960305
#% 976826
#% 1117006
#% 1272179
#% 1558464
#% 1673557
#! With ever-increasing amounts of graph data from disparate sources, there has been a strong need for exploiting significant graph patterns with user-specified objective functions. Most objective functions are not antimonotonic, which could fail all of frequency-centric graph mining algorithms. In this paper, we give the first comprehensive study on general mining method aiming to find most significant patterns directly. Our new mining framework, called LEAP (Descending Leap Mine), is developed to exploit the correlation between structural similarity and significance similarity in a way that the most significant pattern could be identified quickly by searching dissimilar graph patterns. Two novel concepts, structural leap search and frequency descending mining, are proposed to support leap search in graph pattern space. Our new mining method revealed that the widely adopted branch-and-bound search in data mining literature is indeed not the best, thus sketching a new picture on scalable graph pattern discovery. Empirical results show that LEAP achieves orders of magnitude speedup in comparison with the state-of-the-art method. Furthermore, graph classifiers built on mined patterns outperform the up-to-date graph kernel method in terms of efficiency and accuracy, demonstrating the high promise of such patterns.

#index 1063503
#* CSV: visualizing and mining cohesive subgraphs
#@ Nan Wang;Srinivasan Parthasarathy;Kian-Lee Tan;Anthony K. H. Tung
#t 2008
#c 5
#% 201893
#% 273890
#% 300076
#% 328257
#% 431105
#% 465160
#% 466644
#% 481290
#% 498852
#% 660011
#% 679321
#% 729984
#% 765132
#% 765413
#% 778111
#% 785383
#% 810064
#% 823357
#% 833120
#% 864460
#% 875468
#% 881553
#% 1015325
#! Extracting dense sub-components from graphs efficiently is an important objective in a wide range of application domains ranging from social network analysis to biological network analysis, from the World Wide Web to stock market analysis. Motivated by this need recently we have seen several new algorithms to tackle this problem based on the (frequent) pattern mining paradigm. A limitation of most of these methods is that they are highly sensitive to parameter settings, rely on exhaustive enumeration with exponential time complexity, and often fail to help the users understand the underlying distribution of components embedded within the host graph. In this article we propose an approximate algorithm, to mine and visualize cohesive subgraphs (dense sub components) within a large graph. The approach, refereed to as Cohesive Subgraph Visualization (CSV) relies on a novel mapping strategy that maps edges and nodes to a multi-dimensional space wherein dense areas in the mapped space correspond to cohesive subgraphs. The algorithm then walks through the dense regions in the mapped space to output a visual plot that effectively captures the overall dense sub-component distribution of the graph. Unlike extant algorithms with exponential complexity, CSV has a complexity of O(V2logV) when fixing the parameter mapping dimension, where V corresponds to the number of vertices in the graph, although for many real datasets the performance is typically sub-quadratic. We demonstrate the utility of CSV as a stand-alone tool for visual graph exploration and as a pre-filtering step to significantly scale up exact subgraph mining algorithms such as CLAN.

#index 1063504
#* Privacy-MaxEnt: integrating background knowledge in privacy quantification
#@ Wenliang Du;Zhouxuan Teng;Zutao Zhu
#t 2008
#c 5
#% 73441
#% 152934
#% 211044
#% 226495
#% 300184
#% 333876
#% 464822
#% 577233
#% 800514
#% 800515
#% 810011
#% 823310
#% 864406
#% 864412
#% 881546
#% 893100
#% 993988
#% 1022266
#! Privacy-Preserving Data Publishing (PPDP) deals with the publication of microdata while preserving people' private information in the data. To measure how much private information can be preserved, privacy metrics is needed. An essential element for privacy metrics is the measure of how much adversaries can know about an individual's sensitive attributes (SA) if they know the individual's quasi-identifiers (QI), i.e., we need to measure P(SA|QI). Such a measure is hard to derive when adversaries' background knowledge has to be considered. We propose a systematic approach, Privacy-MaxEnt, to integrate background knowledge in privacy quantification. Our approach is based on the maximum entropy principle. We treat all the conditional probabilities P(SA|QI) as unknown variables; we treat the background knowledge as the constraints of these variables; in addition, we also formulate constraints from the published data. Our goal becomes finding a solution to those variables (the probabilities) that satisfy all these constraints. Although many solutions may exist, the most unbiased estimate of P(SA|QI) is the one that achieves the maximum entropy.

#index 1063505
#* Preservation of proximity privacy in publishing numerical sensitive data
#@ Jiexing Li;Yufei Tao;Xiaokui Xiao
#t 2008
#c 5
#% 263982
#% 300184
#% 321186
#% 379248
#% 419405
#% 443463
#% 576111
#% 576761
#% 577239
#% 729930
#% 800514
#% 800515
#% 801690
#% 809245
#% 810011
#% 810028
#% 864406
#% 864412
#% 874892
#% 874988
#% 881483
#% 881497
#% 881546
#% 881551
#% 883236
#% 893100
#% 893101
#% 960239
#% 960289
#% 960291
#% 993943
#% 1022246
#% 1022247
#% 1022264
#% 1022265
#% 1022266
#% 1700134
#% 1725659
#% 1740518
#! We identify proximity breach as a privacy threat specific to numerical sensitive attributes in anonymized data publication. Such breach occurs when an adversary concludes with high confidence that the sensitive value of a victim individual must fall in a short interval --- even though the adversary may have low confidence about the victim's actual value. None of the existing anonymization principles (e.g., k-anonymity, l-diversity, etc.) can effectively prevent proximity breach. We remedy the problem by introducing a novel principle called (ε, m)-anonymity. Intuitively, the principle demands that, given a QI-group G, for every sensitive value x in G, at most 1/m of the tuples in G can have sensitive values "similar" to x, where the similarity is controlled by ε. We provide a careful analytical study of the theoretical characteristics of (ε, m)-anonymity, and the corresponding generalization algorithm. Our findings are verified by experiments with real data.

#index 1063506
#* Stream firewalling of xml constraints
#@ Michael Benedikt;Alan Jeffrey;Ruy Ley-Wild
#t 2008
#c 5
#% 3873
#% 339055
#% 465061
#% 480296
#% 654476
#% 654477
#% 824798
#% 982759
#% 994000
#% 994015
#% 1016180
#% 1022330
#% 1407292
#% 1721253
#! As XML-based messages have become common in many client-server protocols, there is a need to protect application servers from invalid or dangerous messages. This leads to the XML stream firewalling problem; that of applying integrity constraints against a large number of simultaneous streams. We conduct the first investigation of a constraint engine optimized for the generation of XML stream firewalls. We isolate a class of DTDs and XPath constraints which support the generation of low-space filters, and provide algorithms for generating firewalls with low per-input-character time and per-stream space. We give experimental results which show that we have achieved these goals in practice.

#index 1063507
#* Generating targeted queries for database testing
#@ Chaitanya Mishra;Nick Koudas;Calisto Zuzarte
#t 2008
#c 5
#% 116084
#% 137885
#% 172913
#% 317437
#% 397371
#% 479656
#% 741995
#% 824744
#% 902467
#% 960248
#% 960262
#% 1015256
#% 1016216
#% 1022297
#% 1022307
#! Tools for generating test queries for databases do not explicitly take into account the actual data in the database. As a consequence, such tools cannot guarantee suitable coverage of test cases commonly required for database testing. In this paper, we investigate the problem of generating queries that satisfy cardinality constraints on intermediate subexpressions when executed on a given test database. Such queries are required to test the performance of a database system under different operating conditions. We formally analyze this problem, quantify its difficulty and follow up this analysis with a description of a practical algorithm which utilizes sampling and space pruning techniques to quickly generate test queries that have desired properties. We present the results of an experimental evaluation of our approach as implemented in an open source data manager, demonstrating the utility of our proposal.

#index 1063508
#* Relational joins on graphics processors
#@ Bingsheng He;Ke Yang;Rui Fang;Mian Lu;Naga Govindaraju;Qiong Luo;Pedro Sander
#t 2008
#c 5
#% 58352
#% 83133
#% 112212
#% 115661
#% 236681
#% 282235
#% 410276
#% 479819
#% 479821
#% 480608
#% 566122
#% 654479
#% 765419
#% 771088
#% 810059
#% 824697
#% 824720
#% 864446
#% 873339
#% 874615
#% 874997
#% 893219
#% 896784
#% 904362
#% 988651
#% 1016214
#% 1051712
#% 1206754
#! We present a novel design and implementation of relational join algorithms for new-generation graphics processing units (GPUs). The most recent GPU features include support for writing to random memory locations, efficient inter-processor communication, and a programming model for general-purpose computing. Taking advantage of these new features, we design a set of data-parallel primitives such as split and sort, and use these primitives to implement indexed or non-indexed nested-loop, sort-merge and hash joins. Our algorithms utilize the high parallelism as well as the high memory bandwidth of the GPU, and use parallel computation and memory optimizations to effectively reduce memory stalls. We have implemented our algorithms on a PC with an NVIDIA G80 GPU and an Intel quad-core CPU. Our GPU-based join algorithms are able to achieve a performance improvement of 2-7X over their optimized CPU-based counterparts.

#index 1063509
#* Optimizing complex queries with multiple relation instances
#@ Yu Cao;Gopal C. Das;Chee-Yong Chan;Kian-Lee Tan
#t 2008
#c 5
#% 136740
#% 152943
#% 221395
#% 248795
#% 248807
#% 300166
#% 565469
#% 643570
#% 742563
#% 810039
#% 824755
#% 960278
#% 1022262
#% 1022312
#! Today's query processing engines do not take advantage of the multiple occurrences of a relation in a query to improve performance. Instead, each instance is treated as a distinct relation and has its own independent table access method. In this paper, we present MAPLE, a Multi-instance-Aware PLan Evaluation engine that enables multiple instances of a relation to share one physical scan (called SharedScan) with limited buffer space. During execution, as SharedScan pulls a tuple for any instance, that tuple is also pushed to the buffers of other instances with matching predicates. To avoid buffer overflow, a novel interleaved execution strategy is proposed: whenever an instance's buffer becomes full, the execution is temporarily switched to a drainer (an ancestor blocking operator of the instance) to consume all the tuples in the buffer. Thus, the execution is interleaved between normal processing and drainers. We also propose a cost-based approach to generate a plan to maximize the shared scan benefit as well as to avoid interleaved execution deadlocks. MAPLE is light-weight and can be easily integrated into existing RDBMS executors. We have implemented MAPLE in PostgreSQL, and our experimental study on the TPC-DS benchmark shows significant reduction in execution time.

#index 1063510
#* Dynamic programming strikes back
#@ Guido Moerkotte;Thomas Neumann
#t 2008
#c 5
#% 86947
#% 201927
#% 210166
#% 220425
#% 334006
#% 411554
#% 463276
#% 465170
#% 581485
#% 800577
#% 824751
#% 893165
#% 894440
#% 960299
#% 1721255
#% 1721259
#! Two highly efficient algorithms are known for optimally ordering joins while avoiding cross products: DPccp, which is based on dynamic programming, and Top-Down Partition Search, based on memoization. Both have two severe limitations: They handle only (1) simple (binary) join predicates and (2) inner joins. However, real queries may contain complex join predicates, involving more than two relations, and outer joins as well as other non-inner joins. Taking the most efficient known join-ordering algorithm, DPccp, as a starting point, we first develop a new algorithm, DPhyp, which is capable to handle complex join predicates efficiently. We do so by modeling the query graph as a (variant of a) hypergraph and then reason about its connected subgraphs. Then, we present a technique to exploit this capability to efficiently handle the widest class of non-inner joins dealt with so far. Our experimental results show that this reformulation of non-inner joins as complex predicates can improve optimization time by orders of magnitude, compared to known algorithms dealing with complex join predicates and non-inner joins. Once again, this gives dynamic programming a distinct advantage over current memoization techniques.

#index 1063511
#* Adding magic to an optimising datalog compiler
#@ Damien Sereni;Pavel Avgustinov;Oege de Moor
#t 2008
#c 5
#% 14513
#% 23902
#% 58376
#% 77940
#% 86943
#% 105243
#% 172889
#% 210207
#% 277328
#% 349117
#% 435130
#% 479938
#% 630954
#% 809234
#% 1063738
#% 1672191
#% 1732822
#! The magic-sets transformation is a useful technique for dramatically improving the performance of complex queries, but it has been observed that this transformation can also drastically reduce the performance of some queries. Successful implementations of magic in previous work require integration with the database optimiser to make appropriate decisions to guide the transformation (the sideways information passing strategy, or SIPS). This paper reports on the addition of the magic-sets transformation to a fully automatic optimising compiler from Datalog to SQL with no support from the database optimiser. We present an algorithm for making a good choice of SIPS using heuristics based on the sizes of relations. To achieve this, we define an abstract interpretation of Datalog programs to estimate the sizes of relations in the program. The effectiveness of our technique is evaluated over a substantial set of over a hundred queries, and in the context of the other optimisations performed by our compiler. It is shown that using the SIPS chosen by our algorithm, query performance is often significantly improved, as expected, but more importantly performance is never significantly degraded on queries that cannot benefit from magic.

#index 1063512
#* Efficient aggregation for graph summarization
#@ Yuanyuan Tian;Richard A. Hankins;Jignesh M. Patel
#t 2008
#c 5
#% 288652
#% 385505
#% 434557
#% 453548
#% 629708
#% 731608
#% 754117
#% 769940
#% 810094
#% 867050
#% 868089
#% 916086
#% 945495
#% 989654
#% 1038346
#! Graphs are widely used to model real world objects and their relationships, and large graph datasets are common in many application domains. To understand the underlying characteristics of large graphs, graph summarization techniques are critical. However, existing graph summarization methods are mostly statistical (studying statistics such as degree distributions, hop-plots and clustering coefficients). These statistical methods are very useful, but the resolutions of the summaries are hard to control. In this paper, we introduce two database-style operations to summarize graphs. Like the OLAP-style aggregation methods that allow users to drill-down or roll-up to control the resolution of summarization, our methods provide an analogous functionality for large graph datasets. The first operation, called SNAP, produces a summary graph by grouping nodes based on user-selected node attributes and relationships. The second operation, called k-SNAP, further allows users to control the resolutions of summaries and provides the "drill-down" and "roll-up" abilities to navigate through summaries with different resolutions. We propose an efficient algorithm to evaluate the SNAP operation. In addition, we prove that the k-SNAP computation is NP-complete. We propose two heuristic methods to approximate the k-SNAP results. Through extensive experiments on a variety of real and synthetic datasets, we demonstrate the effectiveness and efficiency of the proposed methods.

#index 1063513
#* Efficient algorithms for exact ranked twig-pattern matching over graphs
#@ Gang Gou;Rada Chirkova
#t 2008
#c 5
#% 4035
#% 58365
#% 210182
#% 330678
#% 333854
#% 379482
#% 397375
#% 410276
#% 539108
#% 654442
#% 660011
#% 824693
#% 864462
#% 960259
#% 993987
#% 1013630
#% 1015258
#% 1022244
#! Querying large-scale graph-structured data with twig patterns is attracting growing interest. Generally, a twig pattern could have an extremely large, potentially exponential, number of matches in a graph. Retrieving and returning to the user this many answers may both incur high computational overhead and overwhelm the user. In this paper we propose two efficient algorithms, DP-B and DP-P, for retrieving top-ranked twig-pattern matches from large graphs. Our first algorithm, DP-B, is able to retrieve exact top-ranked answer matches from potentially exponentially many matches in time and space linear in the size of our data inputs even in the worst case. Further, beyond the linear-cost result of DP-B, our second algorithm, DP-P, could take far less than linear time and space cost in practice. To the best of our knowledge, our algorithms are the first to have these performance properties. Our experimental results demonstrate the high performance of both algorithms on large datasets. We also analyze and compare the performance trade-off between DP-B and DP-P from the theoretical and practical viewpoints.

#index 1063514
#* Efficiently answering reachability queries on very large directed graphs
#@ Ruoming Jin;Yang Xiang;Ning Ruan;Haixun Wang
#t 2008
#c 5
#% 14513
#% 47573
#% 58365
#% 70370
#% 88051
#% 379482
#% 824692
#% 864462
#% 960304
#% 1058620
#% 1688299
#! Efficiently processing queries against very large graphs is an important research topic largely driven by emerging real world applications, as diverse as XML databases, GIS, web mining, social network analysis, ontologies, and bioinformatics. In particular, graph reachability has attracted a lot of research attention as reachability queries are not only common on graph databases, but they also serve as fundamental operations for many other graph queries. The main idea behind answering reachability queries in graphs is to build indices based on reachability labels. Essentially, each vertex in the graph is assigned with certain labels such that the reachability between any two vertices can be determined by their labels. Several approaches have been proposed for building these reachability labels; among them are interval labeling (tree cover) and 2-hop labeling. However, due to the large number of vertices in many real world graphs (some graphs can easily contain millions of vertices), the computational cost and (index) size of the labels using existing methods would prove too expensive to be practical. In this paper, we introduce a novel graph structure, referred to as path-tree, to help labeling very large graphs. The path-tree cover is a spanning subgraph of G in a tree shape. We demonstrate both analytically and empirically the effectiveness of our new approaches.

#index 1063515
#* Minimization of tree pattern queries with constraints
#@ Ding Chen;Chee-Yong Chan
#t 2008
#c 5
#% 291299
#% 333989
#% 397374
#% 465051
#% 650962
#% 733593
#% 814648
#% 893098
#% 1015267
#! Tree pattern queries (TPQs) provide a natural and easy formalism to query tree-structured XML data, and the efficient processing of such queries has attracted a lot of attention. Since the size of a TPQ is a key determinant of its evaluation cost, recent research has focused on the problem of query minimization using integrity constraints to eliminate redundant query nodes; specifically, TPQ minimization has been studied for the class of forward and subtype constraints (FT-constraints). In this paper, we explore the TPQ minimization problem further for a richer class of FBST-constraints that includes not only FT-constraints but also backward and sibling constraints. By exploiting the properties of minimal queries under FBST-constraints, we propose efficient algorithms to both compute a single minimal query as well as enumerate all minimal queries. In addition, we also develop more efficient minimization algorithms for the previously studied class of FT-constraints. Our experimental study demonstrates the effectiveness and efficiency of query minimization using FBST-constraints.

#index 1063516
#* Query-based partitioning of documents and indexes for information lifecycle management
#@ Soumyadeb Mitra;Marianne Winslett;Windsor W. Hsu
#t 2008
#c 5
#% 1921
#% 290703
#% 296646
#% 340887
#% 766472
#% 893171
#% 907504
#% 987215
#% 987216
#! Regulations require businesses to archive many electronic documents for extended periods of time. Given the sheer volume of documents and the response time requirements, documents that are unlikely to ever be accessed should be stored on an inexpensive device (such as tape), while documents that are likely to be accessed should be placed on a more expensive, higher-performance device. Unfortunately, traditional data partitioning techniques either require substantial manual involvement, or are not suitable for read-rarely workloads. In this paper, we present a novel technique to address this problem. We estimate the future access likelihood for a document based on past workloads of keyword queries and the click-through behavior for top-K query answers, then use this information to drive partitioning decisions. Our overall best scheme, the document-split inverted index, does not require any parameter tuning and yet performs close to the optimal partitioning strategy. Experiments show that document-split partitioning improves performance on a large intranet query workload by a factor of 4 when we add a fast storage server that holds 20% of the data.

#index 1063517
#* Skippy: a new snapshot indexing method for time travel in the storage manager
#@ Ross Shaull;Liuba Shrira;Hao Xu
#t 2008
#c 5
#% 69316
#% 86953
#% 152904
#% 182672
#% 286472
#% 287070
#% 308497
#% 391237
#% 403195
#% 442967
#% 480096
#% 571296
#% 800545
#% 810114
#% 830695
#% 963669
#% 978174
#% 978445
#% 979743
#% 1016158
#% 1206704
#! The storage manager of a general-purpose database system can retain consistent disk page level snapshots and run application programs "back-in-time" against long-lived past states, virtualized to look like the current state. This opens the possibility that functions, such as on-line trend analysis and audit, formerly available in specialized temporal databases, can become available to general applications in general-purpose databases. Up to now, in-place updating database systems had no satisfactory way to run programs on-line over long-lived, disk page level, copy-on-write snapshots, because there was no efficient indexing method for such snapshots. We describe Skippy, a new indexing approach that solves this problem. Using Skippy, database application code can run against an arbitrarily old snapshot, and iterate over snapshot ranges, as efficiently it can access recent snapshots, for all update workloads. Performance evaluation of Skippy, based on theoretical analysis and experimental measurements, indicates that the new approach provides efficient access to snapshots at low cost.

#index 1063518
#* OLAP on sequence data
#@ Eric Lo;Ben Kao;Wai-Shing Ho;Sau Dan Lee;Chun Kit Chui;David W. Cheung
#t 2008
#c 5
#% 18614
#% 172950
#% 191154
#% 227883
#% 273916
#% 279164
#% 333850
#% 338609
#% 479450
#% 479795
#% 482088
#% 503731
#% 503878
#% 644230
#% 864470
#% 893157
#% 993958
#% 1016173
#! Many kinds of real-life data exhibit logical ordering among their data items and are thus sequential in nature. However, traditional online analytical processing (OLAP) systems and techniques were not designed for sequence data and they are incapable of supporting sequence data analysis. In this paper, we propose the concept of Sequence OLAP, or S-OLAP for short. The biggest distinction of S-OLAP from traditional OLAP is that a sequence can be characterized not only by the attributes' values of its constituting items, but also by the subsequence/substring patterns it possesses. This paper studies many aspects related to Sequence OLAP. The concepts of sequence cuboid and sequence data cube are introduced. A prototype S-OLAP system is built in order to validate the proposed concepts. The prototype is able to support "pattern-based" grouping and aggregation, which is currently not supported by any OLAP system. The implementation details of the prototype system as well as experimental results are presented.

#index 1063519
#* Improving suffix array locality for fast pattern matching on disk
#@ Ranjan Sinha;Simon Puglisi;Alistair Moffat;Andrew Turpin
#t 2008
#c 5
#% 143306
#% 184486
#% 215404
#% 235941
#% 289010
#% 300312
#% 503225
#% 544049
#% 751623
#% 770226
#% 789004
#% 790903
#% 829998
#% 936965
#% 956437
#% 960303
#% 1083926
#! The suffix tree (or equivalently, the enhanced suffix array) provides efficient solutions to many problems involving pattern matching and pattern discovery in large strings, such as those arising in computational biology. Here we address the problem of arranging a suffix array on disk so that querying is fast in practice. We show that the combination of a small trie and a suffix array-like blocked data structure allows queries to be answered as much as three times faster than the best alternative disk-based suffix array arrangement. Construction of our data structure requires only modest processing time on top of that required to build the suffix tree, and requires negligible extra memory.

#index 1063520
#* Ranking queries on uncertain data: a probabilistic threshold approach
#@ Ming Hua;Jian Pei;Wenjie Zhang;Xuemin Lin
#t 2008
#c 5
#% 663
#% 32879
#% 190611
#% 209725
#% 333854
#% 463114
#% 599545
#% 654487
#% 824728
#% 864394
#% 864455
#% 893167
#% 976984
#% 977013
#% 1016201
#% 1016202
#% 1022203
#% 1044478
#% 1206645
#% 1206646
#! Uncertain data is inherent in a few important applications such as environmental surveillance and mobile object tracking. Top-k queries (also known as ranking queries) are often natural and useful in analyzing uncertain data in those applications. In this paper, we study the problem of answering probabilistic threshold top-k queries on uncertain data, which computes uncertain records taking a probability of at least p to be in the top-k list where p is a user specified probability threshold. We present an efficient exact algorithm, a fast sampling algorithm, and a Poisson approximation based algorithm. An empirical study using real and synthetic data sets verifies the effectiveness of probabilistic threshold top-k queries and the efficiency of our methods.

#index 1063521
#* MCDB: a monte carlo approach to managing uncertain data
#@ Ravi Jampani;Fei Xu;Mingxi Wu;Luis Leopoldo Perez;Christopher Jermaine;Peter J. Haas
#t 2008
#c 5
#% 215225
#% 227883
#% 716983
#% 824764
#% 864417
#% 864435
#% 874976
#% 893168
#% 893189
#% 907562
#% 925382
#% 940697
#% 960294
#% 977014
#% 992830
#% 1000502
#% 1022203
#% 1022204
#% 1022205
#% 1022206
#% 1206747
#! To deal with data uncertainty, existing probabilistic database systems augment tuples with attribute-level or tuple-level probability values, which are loaded into the database along with the data itself. This approach can severely limit the system's ability to gracefully handle complex or unforeseen types of uncertainty, and does not permit the uncertainty model to be dynamically parameterized according to the current state of the database. We introduce MCDB, a system for managing uncertain data that is based on a Monte Carlo approach. MCDB represents uncertainty via "VG functions," which are used to pseudorandomly generate realized values for uncertain attributes. VG functions can be parameterized on the results of SQL queries over "parameter tables" that are stored in the database, facilitating what-if analyses. By storing parameters, and not probabilities, and by estimating, rather than exactly computing, the probability distribution over possible query answers, MCDB avoids many of the limitations of prior systems. For example, MCDB can easily handle arbitrary joint probability distributions over discrete or continuous attributes, arbitrarily complex SQL queries, and arbitrary functionals of the query-result distribution such as means, variances, and quantiles. To achieve good performance, MCDB uses novel query processing techniques, executing a query plan exactly once, but over "tuple bundles" instead of ordinary tuples. Experiments indicate that our enhanced functionality can be obtained with acceptable overheads relative to traditional systems.

#index 1063522
#* Query efficiency in probabilistic XML models
#@ Benny Kimelfeld;Yuri Kosharovsky;Yehoshua Sagiv
#t 2008
#c 5
#% 39702
#% 42984
#% 58608
#% 115964
#% 116987
#% 265692
#% 378393
#% 397375
#% 465044
#% 977012
#% 977013
#% 977014
#% 993985
#% 1015267
#% 1016201
#% 1022204
#% 1044440
#% 1063720
#% 1408537
#% 1688305
#! Various known models of probabilistic XML can be represented as instantiations of abstract p-documents. Such documents have, in addition to ordinary nodes, distributional nodes that specify the probabilistic process of generating a random document. Within this abstraction, families of pdocuments, which are natural extensions and combinations of previous models, are considered. The focus is on efficiency of applying twig queries (with projection) to p-documents. A closely related issue is the ability to (efficiently) translate a given document of one family into another family. Furthermore, both of these tasks have two variants that correspond to the value-based and object-based semantics. The translation relationships among different families of p-documents are studied. An efficient algorithm for evaluating twig queries over one specific family is given. This algorithm generalizes a known algorithm and significantly improves its running time, both analytically and experimentally. It is shown that this family is the maximal, among the ones considered, for which query evaluation is tractable. For the rest, efficient approximate algorithms for query evaluation are presented.

#index 1063523
#* Event queries on correlated probabilistic streams
#@ Christopher Ré;Julie Letchner;Magdalena Balazinksa;Dan Suciu
#t 2008
#c 5
#% 44876
#% 95730
#% 215225
#% 340699
#% 388024
#% 443502
#% 765449
#% 788954
#% 818434
#% 824747
#% 873104
#% 875004
#% 876005
#% 893102
#% 893167
#% 896018
#% 902729
#% 953324
#% 960257
#% 975457
#% 977008
#% 977010
#% 977013
#% 991156
#% 1016201
#% 1022206
#% 1688281
#% 1760888
#% 1769665
#! A major problem in detecting events in streams of data is that the data can be imprecise (e.g. RFID data). However, current state-ofthe-art event detection systems such as Cayuga [14], SASE [46] or SnoopIB[1], assume the data is precise. Noise in the data can be captured using techniques such as hidden Markov models. Inference on these models creates streams of probabilistic events which cannot be directly queried by existing systems. To address this challenge we propose Lahar1, an event processing system for probabilistic event streams. By exploiting the probabilistic nature of the data, Lahar yields a much higher recall and precision than deterministic techniques operating over only the most probable tuples. By using a novel static analysis and novel algorithms, Lahar processes data orders of magnitude more efficiently than a naïve approach based on sampling. In this paper, we present Lahar's static analysis and core algorithms. We demonstrate the quality and performance of our approach through experiments with our prototype implementation and comparisons with alternate methods.

#index 1063524
#* Serializable isolation for snapshot databases
#@ Michael J. Cahill;Uwe Röhm;Alan D. Fekete
#t 2008
#c 5
#% 201869
#% 277333
#% 287230
#% 320902
#% 403195
#% 632092
#% 709864
#% 746768
#% 783784
#% 809252
#% 814649
#% 979743
#% 993362
#% 1206782
#! Many popular database management systems offer snapshot isolation rather than full serializability. There are well-known anomalies permitted by snapshot isolation that can lead to violations of data consistency by interleaving transactions that individually maintain consistency. Until now, the only way to prevent these anomalies was to modify the applications by introducing artificial locking or update conflicts, following careful analysis of conflicts between all pairs of transactions. This paper describes a modification to the concurrency control algorithm of a database management system that automatically detects and prevents snapshot isolation anomalies at runtime for arbitrary applications, thus providing serializable isolation. The new algorithm preserves the properties that make snapshot isolation attractive, including that readers do not block writers and vice versa. An implementation and performance study of the algorithm are described, showing that the throughput approaches that of snapshot isolation in most cases.

#index 1063525
#* Middleware-based database replication: the gaps between theory and practice
#@ Emmanuel Cecchet;George Candea;Anastasia Ailamaki
#t 2008
#c 5
#% 201869
#% 210179
#% 255077
#% 397606
#% 480310
#% 480325
#% 609940
#% 717164
#% 742057
#% 793894
#% 793895
#% 800516
#% 810043
#% 850309
#% 893147
#% 938074
#% 960202
#% 963871
#% 983491
#% 1009832
#% 1066740
#% 1428352
#! The need for high availability and performance in data management systems has been fueling a long running interest in database replication from both academia and industry. However, academic groups often attack replication problems in isolation, overlooking the need for completeness in their solutions, while commercial teams take a holistic approach that often misses opportunities for fundamental innovation. This has created over time a gap between academic research and industrial practice. This paper aims to characterize the gap along three axes: performance, availability, and administration. We build on our own experience developing and deploying replication systems in commercial and academic settings, as well as on a large body of prior related work. We sift through representative examples from the last decade of open-source, academic, and commercial database replication systems and combine this material with case studies from real systems deployed at Fortune 500 customers. We propose two agendas, one for academic research and one for industrial R&D, which we believe can bridge the gap within 5-10 years. This way, we hope to both motivate and help researchers in making the theory and practice of middleware-based database replication more relevant to each other.

#index 1063526
#* On efficient top-k query processing in highly distributed environments
#@ Akrivi Vlachou;Christos Doulkeridis;Kjetil Nørvåg;Michalis Vazirgiannis
#t 2008
#c 5
#% 333854
#% 333951
#% 465167
#% 480330
#% 763882
#% 766469
#% 766671
#% 768521
#% 800509
#% 806212
#% 824704
#% 864451
#% 870255
#% 875023
#% 986217
#% 1016207
#% 1022243
#% 1715596
#! Lately the advances in centralized database management systems show a trend towards supporting rank-aware query operators, like top-k, that enable users to retrieve only the most interesting data objects. A challenging problem is to support rank-aware queries in highly distributed environments. In this paper, we present a novel approach, called SPEERTO, for top-k query processing in large-scale peer-to-peer networks, where the dataset is horizontally distributed over the peers. Towards this goal, we explore the applicability of the skyline operator for efficiently routing top-k queries in a large super-peer network. Relying on a thresholding scheme, SPEERTO returns the exact results progressively to the user, while the number of queried super-peers and transferred data is minimized. Finally, we propose different variations of SPEERTO that allow balancing between transferred data volume and response time. Through simulations we demonstrate the feasibility of our approach.

#index 1063527
#* Efficient bulk insertion into a distributed ordered table
#@ Adam Silberstein;Brian F. Cooper;Utkarsh Srivastava;Erik Vee;Ramana Yerneni;Raghu Ramakrishnan
#t 2008
#c 5
#% 86465
#% 116086
#% 208047
#% 340670
#% 458615
#% 479470
#% 479473
#% 481622
#% 850621
#% 857498
#% 963669
#% 998842
#% 998845
#% 1002142
#% 1002149
#% 1021199
#% 1289193
#! We study the problem of bulk-inserting records into tables in a system that horizontally range-partitions data over a large cluster of shared-nothing machines. Each table partition contains a contiguous portion of the table's key range, and must accept all records inserted into that range. Examples of such systems include BigTable[8] at Google, and PNUTS [15] at Yahoo! During bulk inserts into an existing table, if most of the inserted records end up going into a small number of data partitions, the obtained throughput may be very poor due to ineffective use of cluster parallelism. We propose a novel approach in which a planning phase is invoked before the actual insertions. By creating new partitions and intelligently distributing partitions across machines, the planning phase ensures that the insertion load will be well-balanced. Since there is a tradeoff between the cost of moving partitions and the resulting throughput gain, the planning phase must minimize the sum of partition movement time and insertion time. We show that this problem is a variation of NP-hard bin-packing, reduce it to a problem of packing vectors, and then give a solution with provable approximation guarantees. We evaluate our approach on a prototype system deployed on a cluster of 50 machines, and show that it yields significant improvements over more naïve techniques.

#index 1063528
#* Sampling cube: a framework for statistical olap over sampling data
#@ Xiaolei Li;Jiawei Han;Zhijun Yin;Jae-Gil Lee;Yizhou Sun
#t 2008
#c 5
#% 210182
#% 273916
#% 333929
#% 376266
#% 464215
#% 480499
#% 641976
#% 654446
#% 654467
#% 722929
#% 765518
#% 824733
#% 824734
#% 893167
#% 926881
#% 987193
#% 993996
#% 1015294
#% 1016173
#% 1016174
#% 1022205
#! Sampling is a popular method of data collection when it is impossible or too costly to reach the entire population. For example, television show ratings in the United States are gathered from a sample of roughly 5,000 households. To use the results effectively, the samples are further partitioned in a multidimensional space based on multiple attribute values. This naturally leads to the desirability of OLAP (Online Analytical Processing) over sampling data. However, unlike traditional data, sampling data is inherently uncertain, i.e., not representing the full data in the population. Thus, it is desirable to return not only query results but also the confidence intervals indicating the reliability of the results. Moreover, a certain segment in a multidimensional space may contain none or too few samples. This requires some additional analysis to return trustable results. In this paper we propose a Sampling Cube framework, which efficiently calculates confidence intervals for any multidimensional query and uses the OLAP structure to group similar segments to increase sampling size when needed. Further, to handle high dimensional data, a Sampling Cube Shell method is proposed to effectively reduce the storage requirement while still preserving query result quality.

#index 1063529
#* Querying continuous functions in a database system
#@ Arvind Thiagarajan;Samuel Madden
#t 2008
#c 5
#% 190332
#% 248802
#% 274153
#% 301191
#% 315005
#% 436099
#% 466506
#% 477204
#% 480459
#% 527115
#% 527198
#% 571224
#% 874976
#% 906944
#% 993099
#! Many scientific, financial, data mining and sensor network applications need to work with continuous, rather than discrete data e.g., temperature as a function of location, or stock prices or vehicle trajectories as a function of time. Querying raw or discrete data is unsatisfactory for these applications -- e.g., in a sensor network, it is necessary to interpolate sensor readings to predict values at locations where sensors are not deployed. In other situations, raw data can be inaccurate owing to measurement errors, and it is useful to fit continuous functions to raw data and query the functions, rather than raw data itself -- e.g., fitting a smooth curve to noisy sensor readings, or a smooth trajectory to GPS data containing gaps or outliers. Existing databases do not support storing or querying continuous functions, short of brute-force discretization of functions into a collection of tuples. We present FunctionDB, a novel database system that treats mathematical functions as first-class citizens that can be queried like traditional relations. The key contribution of FunctionDB is an efficient and accurate algebraic query processor - for the broad class of multi-variable polynomial functions, FunctionDB executes queries directly on the algebraic representation of functions without materializing them into discrete points, using symbolic operations: zero finding, variable substitution, and integration. Even when closed form solutions are intractable, FunctionDB leverages symbolic approximation operations to improve performance. We evaluate FunctionDB on real data sets from a temperature sensor network, and on traffic traces from Boston roads. We show that operating in the functional domain has substantial advantages in terms of accuracy (15-30%) and up to order of magnitude (10x-100x) performance wins over existing approaches that represent models as discrete collections of points.

#index 1063530
#* An efficient filter for approximate membership checking
#@ Kaushik Chakrabarti;Surajit Chaudhuri;Venkatesh Ganti;Dong Xin
#t 2008
#c 5
#% 163073
#% 290703
#% 321327
#% 322884
#% 323410
#% 333854
#% 479973
#% 480654
#% 546266
#% 632029
#% 765463
#% 864392
#% 864415
#% 875001
#% 893164
#% 1387756
#! We consider the problem of identifying sub-strings of input text strings that approximately match with some member of a potentially large dictionary. This problem arises in several important applications such as extracting named entities from text documents and identifying biological concepts from biomedical literature. In this paper, we develop a filter-verification framework, and propose a novel in-memory filter structure. That is, we first quickly filter out sub-strings that cannot match with any dictionary member, and then verify the remaining sub-strings against the dictionary. Our method does not produce false negatives. We demonstrate the efficiency and effectiveness of our filter over real datasets, and show that it significantly outperforms the previous best-known methods in terms of both filtering power and computation time.

#index 1063531
#* Finding frequent items in probabilistic data
#@ Qin Zhang;Feifei Li;Ke Yi
#t 2008
#c 5
#% 248812
#% 278835
#% 333925
#% 420072
#% 446438
#% 479795
#% 481290
#% 569754
#% 576119
#% 654467
#% 654487
#% 730046
#% 765414
#% 800582
#% 810020
#% 824728
#% 864394
#% 874906
#% 893089
#% 893167
#% 893189
#% 894443
#% 960257
#% 960293
#% 977008
#% 992830
#% 993960
#% 1016178
#% 1016201
#% 1022203
#% 1022206
#% 1206772
#! Computing statistical information on probabilistic data has attracted a lot of attention recently, as the data generated from a wide range of data sources are inherently fuzzy or uncertain. In this paper, we study an important statistical query on probabilistic data: finding the frequent items. One straightforward approach to identify the frequent items in a probabilistic data set is to simply compute the expected frequency of an item and decide if it exceeds a certain fraction of the expected size of the whole data set. However, this simple definition misses important information about the internal structure of the probabilistic data and the interplay among all the uncertain entities. Thus, we propose a new definition based on the possible world semantics that has been widely adopted for many query types in uncertain data management, trying to find all the items that are likely to be frequent in a randomly generated possible world. Our approach naturally leads to the study of ranking frequent items based on confidence as well. Finding likely frequent items in probabilistic data turns out to be much more difficult. We first propose exact algorithms for offline data with either quadratic or cubic time. Next, we design novel sampling-based algorithms for streaming data to find all approximately likely frequent items with theoretically guaranteed high probability and accuracy. Our sampling schemes consume sublinear memory and exhibit excellent scalability. Finally, we verify the effectiveness and efficiency of our algorithms using both real and synthetic data sets with extensive experimental evaluations.

#index 1063532
#* Interactive generation of integrated schemas
#@ Laura Chiticariu;Phokion G. Kolaitis;Lucian Popa
#t 2008
#c 5
#% 22948
#% 39702
#% 378409
#% 384978
#% 442861
#% 458607
#% 480969
#% 529190
#% 572314
#% 810078
#% 960233
#% 960271
#% 993981
#% 1015326
#% 1022317
#% 1044441
#% 1289178
#! Schema integration is the problem of creating a unified target schema based on a set of existing source schemas that relate to each other via specified correspondences. The unified schema gives a standard representation of the data, thus offering a way to deal with the heterogeneity in the sources. In this paper, we develop a method and a design tool that provide: 1) adaptive enumeration of multiple interesting integrated schemas, and 2) easy-to-use capabilities for refining the enumerated schemas via user interaction. Our method is a departure from previous approaches to schema integration, which do not offer a systematic exploration of the possible integrated schemas. The method operates at a logical level, where we recast each source schema into a graph of concepts with Has-A relationships. We then identify matching concepts in different graphs by taking into account the correspondences between their attributes. For every pair of matching concepts, we have two choices: merge them into one integrated concept or keep them as separate concepts. We develop an algorithm that can systematically output, without duplication, all possible integrated schemas resulting from the previous choices. For each integrated schema, the algorithm also generates a mapping from the source schemas to the integrated schema that has precise information-preserving properties. Furthermore, we avoid a full enumeration, by allowing users to specify constraints on the merging process, based on the schemas produced so far. These constraints are then incorporated in the enumeration of the subsequent schemas. The result is an adaptive and interactive enumeration method that significantly reduces the space of alternative schemas, and facilitates the selection of the final integrated schema.

#index 1063533
#* Pay-as-you-go user feedback for dataspace systems
#@ Shawn R. Jeffery;Michael J. Franklin;Alon Y. Halevy
#t 2008
#c 5
#% 273694
#% 320432
#% 333990
#% 375017
#% 448827
#% 572314
#% 576214
#% 577238
#% 751818
#% 765409
#% 765425
#% 818221
#% 823348
#% 830529
#% 835045
#% 837835
#% 845350
#% 1015256
#% 1063534
#% 1274894
#! A primary challenge to large-scale data integration is creating semantic equivalences between elements from different data sources that correspond to the same real-world entity or concept. Dataspaces propose a pay-as-you-go approach: automated mechanisms such as schema matching and reference reconciliation provide initial correspondences, termed candidate matches, and then user feedback is used to incrementally confirm these matches. The key to this approach is to determine in what order to solicit user feedback for confirming candidate matches. In this paper, we develop a decision-theoretic framework for ordering candidate matches for user confirmation using the concept of the value of perfect information (VPI). At the core of this concept is a utility function that quantifies the desirability of a given state; thus, we devise a utility function for dataspaces based on query result quality. We show in practice how to efficiently apply VPI in concert with this utility function to order user confirmations. A detailed experimental evaluation on both real and synthetic datasets shows that the ordering of user feedback produced by this VPI-based approach yields a dataspace with a significantly higher utility than a wide range of other ordering strategies. Finally, we outline the design of Roomba, a system that utilizes this decision-theoretic framework to guide a dataspace in soliciting user feedback in a pay-as-you-go manner.

#index 1063534
#* Bootstrapping pay-as-you-go data integration systems
#@ Anish Das Sarma;Xin Dong;Alon Halevy
#t 2008
#c 5
#% 22948
#% 77652
#% 211044
#% 226495
#% 348187
#% 415949
#% 458607
#% 480969
#% 488766
#% 572314
#% 654458
#% 654459
#% 660001
#% 765433
#% 845350
#% 943035
#% 945786
#% 993982
#% 1016163
#% 1022259
#% 1063533
#% 1720904
#! Data integration systems offer a uniform interface to a set of data sources. Despite recent progress, setting up and maintaining a data integration application still requires significant upfront effort of creating a mediated schema and semantic mappings from the data sources to the mediated schema. Many application contexts involving multiple data sources (e.g., the web, personal information management, enterprise intranets) do not require full integration in order to provide useful services, motivating a pay-as-you-go approach to integration. With that approach, a system starts with very few (or inaccurate) semantic mappings and these mappings are improved over time as deemed necessary. This paper describes the first completely self-configuring data integration system. The goal of our work is to investigate how advanced of a starting point we can provide a pay-as-you-go system. Our system is based on the new concept of a probabilistic mediated schema that is automatically created from the data sources. We automatically create probabilistic schema mappings between the sources and the mediated schema. We describe experiments in multiple domains, including 50-800 data sources, and show that our system is able to produce high-quality answers with no human intervention.

#index 1063535
#* Supporting OLAP operations over imperfectly integrated taxonomies
#@ Yan Qi;K. Selçuk Candan;Junichi Tatemura;Songting Chen;Fenglin Liao
#t 2008
#c 5
#% 260021
#% 378409
#% 410276
#% 480645
#% 617864
#% 728755
#% 765464
#% 800009
#% 809250
#% 833135
#% 879041
#% 893121
#% 893167
#% 896024
#% 903016
#% 960246
#% 960271
#% 960285
#% 960287
#% 1015326
#% 1022205
#! OLAP is an important tool in decision support. With the help of domain knowledge, such as hierarchies of attribute values, OLAP helps the user observe the effects of various decisions. One assumption of most OLAP operations is that the available domain knowledge is precise. In particular, they assume that the hierarchy of values over which the user can navigate forms a taxonomy. In this paper, we first note that when multiple heterogeneous data sources are involved in the gathering of the data and the associated domain knowledge, the integrated knowledge-base, constructed by combining locally available taxonomies based on the concept matchings, may not be a taxonomy itself. Specifically, existence of intersections among concepts from different sources compromises the tree-structure of the integrated taxonomy and prevents effective use of hierarchical navigation techniques, such as drill-down and roll-up. To cope with this, we introduce concept un-classification, where a select few of the concepts are eliminated to ensure that the remaining structure is a navigable taxonomy, without concept intersections. Since un-classifying an originally classified data is not desirable, we consider ways to minimize un-classification in the process. We introduce a cost model which captures the imprecision caused by the un-classification process and we formulate the problem of finding an un-classification strategy which eliminates intersections and which adds minimal imprecision to the resulting structure. We show that, when performed naively, this task can be very costly and thus we propose a bottom-up preprocessing strategy which supports basic navigational analytics operations, such as drill-down and roll-up. Experiments over synthetic and real-life data verified the effectiveness and efficiency of our approach.

#index 1063536
#* SQAK: doing more with keywords
#@ Sandeep Tata;Guy M. Lohman
#t 2008
#c 5
#% 408396
#% 654442
#% 659990
#% 754116
#% 765462
#% 875017
#% 960245
#% 960285
#% 993987
#% 1015258
#% 1015310
#% 1015325
#% 1016135
#% 1016176
#% 1022286
#% 1688287
#! Today's enterprise databases are large and complex, often relating hundreds of entities. Enabling ordinary users to query such databases and derive value from them has been of great interest in database research. Today, keyword search over relational databases allows users to find pieces of information without having to write complicated SQL queries. However, in order to compute even simple aggregates, a user is required to write a SQL statement and can no longer use simple keywords. This not only requires the ordinary user to learn SQL, but also to learn the schema of the complex database in detail in order to correctly construct the required query. This greatly limits the options of the user who wishes to examine a database in more depth. As a solution to this problem, we propose a framework called SQAK1 (SQL Aggregates using Keywords) that enables users to pose aggregate queries using simple keywords with little or no knowledge of the schema. SQAK provides a novel and exciting way to trade-off some of the expressive power of SQL in exchange for the ability to express a large class of aggregate queries using simple keywords. SQAK accomplishes this by taking advantage of the data in the database and the schema (tables, attributes, keys, and referential constraints). SQAK does not require any changes to the database engine and can be used with any existing database. We demonstrate using several experiments that SQAK is effective and can be an enormously powerful tool for ordinary users.

#index 1063537
#* EASE: an effective 3-in-1 keyword search method for unstructured, semi-structured and structured data
#@ Guoliang Li;Beng Chin Ooi;Jianhua Feng;Jianyong Wang;Lizhu Zhou
#t 2008
#c 5
#% 213981
#% 248010
#% 330678
#% 654442
#% 659990
#% 660011
#% 810052
#% 824693
#% 824695
#% 824703
#% 863389
#% 875017
#% 956599
#% 960235
#% 960243
#% 960245
#% 960259
#% 960261
#% 960284
#% 993987
#% 1015258
#% 1015325
#% 1019060
#! Conventional keyword search engines are restricted to a given data model and cannot easily adapt to unstructured, semi-structured or structured data. In this paper, we propose an efficient and adaptive keyword search method, called EASE, for indexing and querying large collections of heterogenous data. To achieve high efficiency in processing keyword queries, we first model unstructured, semi-structured and structured data as graphs, and then summarize the graphs and construct graph indices instead of using traditional inverted indices. We propose an extended inverted index to facilitate keyword-based search, and present a novel ranking mechanism for enhancing search effectiveness. We have conducted an extensive experimental study using real datasets, and the results show that EASE achieves both high search efficiency and high accuracy, and outperforms the existing approaches significantly.

#index 1063538
#* A graph method for keyword-based selection of the top-K databases
#@ Quang Hieu Vu;Beng Chin Ooi;Dimitris Papadias;Anthony K. H. Tung
#t 2008
#c 5
#% 18616
#% 194246
#% 287463
#% 321635
#% 397418
#% 413593
#% 567255
#% 654442
#% 660011
#% 766428
#% 810052
#% 818240
#% 824693
#% 875017
#% 960243
#% 960245
#% 960259
#% 960261
#% 960284
#% 993987
#% 1015258
#% 1015325
#% 1016135
#! While database management systems offer a comprehensive solution to data storage, they require deep knowledge of the schema, as well as the data manipulation language, in order to perform effective retrieval. Since these requirements pose a problem to lay or occasional users, several methods incorporate keyword search (KS) into relational databases. However, most of the existing techniques focus on querying a single DBMS. On the other hand, the proliferation of distributed databases in several conventional and emerging applications necessitates the support for keyword-based data sharing and querying over multiple DMBSs. In order to avoid the high cost of searching in numerous, potentially irrelevant, databases in such systems, we propose G-KS, a novel method for selecting the top-K candidates based on their potential to contain results for a given query. G-KSsummarizes each database by a keyword relationship graph, where nodes represent terms and edges describe relationships between them. Keyword relationship graphs are utilized for computing the similarity between each database and a KS query, so that, during query processing, only the most promising databases are searched. An extensive experimental evaluation demonstrates that G-KS outperforms the current state-of-the-art technique on all aspects, including precision, recall, efficiency, space overhead and flexibility of accommodating different semantics.

#index 1063539
#* Keyword proximity search in complex data graphs
#@ Konstantin Golenberg;Benny Kimelfeld;Yehoshua Sagiv
#t 2008
#c 5
#% 39702
#% 316546
#% 330678
#% 333854
#% 340914
#% 340995
#% 397133
#% 397418
#% 459260
#% 660011
#% 824693
#% 824703
#% 874894
#% 875017
#% 893187
#% 909448
#% 993987
#% 1015325
#% 1058253
#% 1673660
#% 1674716
#! In keyword search over data graphs, an answer is a nonredundant subtree that includes the given keywords. An algorithm for enumerating answers is presented within an architecture that has two main components: an engine that generates a set of candidate answers and a ranker that evaluates their score. To be effective, the engine must have three fundamental properties. It should not miss relevant answers, has to be efficient and must generate the answers in an order that is highly correlated with the desired ranking. It is shown that none of the existing systems has implemented an engine that has all of these properties. In contrast, this paper presents an engine that generates all the answers with provable guarantees. Experiments show that the engine performs well in practice. It is also shown how to adapt this engine to queries under the OR semantics. In addition, this paper presents a novel approach for implementing rankers destined for eliminating redundancy. Essentially, an answer is ranked according to its individual properties (relevancy) and its intersection with the answers that have already been presented to the user. Within this approach, experiments with specific rankers are described.

#index 1063540
#* Configuration-parametric query optimization for physical design tuning
#@ Nicolas Bruno;Rimma V. Nehme
#t 2008
#c 5
#% 248815
#% 461895
#% 480955
#% 481925
#% 482100
#% 495283
#% 632100
#% 810026
#% 893130
#% 1016220
#% 1016221
#% 1022293
#! Automated physical design tuning for database systems has recently become an active area of research and development. Existing tuning tools explore the space of feasible solutions by repeatedly optimizing queries in the input workload for several candidate configurations. This general approach, while scalable, often results in tuning sessions waiting for results from the query optimizer over 90% of the time. In this paper we introduce a novel approach, called Configuration-Parametric Query Optimization, that drastically improves the performance of current tuning tools. By issuing a single optimization call per query, we are able to generate a compact representation of the optimization space that can then produce very efficiently execution plans for the input query under arbitrary configurations. Our experiments show that our technique speeds-up query optimization by 30x to over 450x with virtually no loss in quality, and effectively eliminates the optimization bottleneck in existing tuning tools. Our techniques open the door for new, more sophisticated optimization strategies by eliminating the main bottleneck of current tuning tools.

#index 1063541
#* Automatic virtual machine configuration for database workloads
#@ Ahmed A. Soror;Umar Farooq Minhas;Ashraf Aboulnaga;Kenneth Salem;Peter Kokosielis;Sunil Kamath
#t 2008
#c 5
#% 77990
#% 201925
#% 210199
#% 481131
#% 723288
#% 812968
#% 812969
#% 820403
#% 820424
#% 837175
#% 869522
#% 893178
#% 956527
#% 960335
#% 981522
#% 993933
#% 1076578
#% 1142407
#% 1207095
#! Virtual machine monitors are becoming popular tools for the deployment of database management systems and other enterprise software applications. In this paper, we consider a common resource consolidation scenario, in which several database management system instances, each running in a virtual machine, are sharing a common pool of physical computing resources. We address the problem of optimizing the performance of these database management systems by controlling the configurations of the virtual machines in which they run. These virtual machine configurations determine how the shared physical resources will be allocated to the different database instances. We introduce a virtualization design advisor that uses information about the anticipated workloads of each of the database systems to recommend workload-specific configurations offine. Furthermore, runtime information collected after the deployment of the recommended configurations can be used to refine the recommendation. To estimate the effect of a particular resource allocation on workload performance, we use the query optimizer in a new what-if mode. We have implemented our approach using both PostgreSQL and DB2, and we have experimentally evaluated its effectiveness using DSS and OLTP workloads.

#index 1063542
#* Column-stores vs. row-stores: how different are they really?
#@ Daniel J. Abadi;Samuel R. Madden;Nabil Hachem
#t 2008
#c 5
#% 191154
#% 287349
#% 289282
#% 397395
#% 442850
#% 464177
#% 465169
#% 480821
#% 571056
#% 765417
#% 810039
#% 824697
#% 864446
#% 875026
#% 893129
#% 985976
#% 993967
#% 1138538
#% 1206647
#! There has been a significant amount of excitement and recent work on column-oriented database systems ("column-stores"). These database systems have been shown to perform more than an order of magnitude better than traditional row-oriented database systems ("row-stores") on analytical workloads such as those found in data warehouses, decision support, and business intelligence applications. The elevator pitch behind this performance difference is straightforward: column-stores are more I/O efficient for read-only queries since they only have to read from disk (or from memory) those attributes accessed by a query. This simplistic view leads to the assumption that one can obtain the performance benefits of a column-store using a row-store: either by vertically partitioning the schema, or by indexing every column so that columns can be accessed independently. In this paper, we demonstrate that this assumption is false. We compare the performance of a commercial row-store under a variety of different configurations with a column-store and show that the row-store performance is significantly slower on a recently proposed data warehouse benchmark. We then analyze the performance difference and show that there are some important differences between the two systems at the query executor level (in addition to the obvious differences at the storage layer level). Using the column-store, we then tease apart these differences, demonstrating the impact on performance of a variety of column-oriented query execution techniques, including vectorized query processing, compression, and a new join algorithm we introduce in this paper. We conclude that while it is not impossible for a row-store to achieve some of the performance advantages of a column-store, changes must be made to both the storage layer and the query executor to fully obtain the benefits of a column-oriented approach.

#index 1063543
#* OLTP through the looking glass, and what we found there
#@ Stavros Harizopoulos;Daniel J. Abadi;Samuel Madden;Michael Stonebraker
#t 2008
#c 5
#% 27057
#% 107720
#% 114582
#% 148195
#% 172939
#% 239979
#% 286836
#% 300194
#% 307360
#% 340175
#% 403195
#% 408638
#% 442700
#% 442832
#% 461865
#% 473075
#% 479769
#% 479819
#% 479821
#% 480119
#% 482070
#% 745471
#% 824697
#% 893146
#% 963656
#% 963669
#% 998842
#% 998845
#% 1002142
#% 1016238
#% 1022298
#! Online Transaction Processing (OLTP) databases include a suite of features - disk-resident B-trees and heap files, locking-based concurrency control, support for multi-threading - that were optimized for computer technology of the late 1970's. Advances in modern processors, memories, and networks mean that today's computers are vastly different from those of 30 years ago, such that many OLTP databases will now fit in main memory, and most OLTP transactions can be processed in milliseconds or less. Yet database architecture has changed little. Based on this observation, we look at some interesting variants of conventional database systems that one might build that exploit recent hardware trends, and speculate on their performance through a detailed instruction-level breakdown of the major components involved in a transaction processing database system (Shore) running a subset of TPC-C. Rather than simply profiling Shore, we progressively modified it so that after every feature removal or optimization, we had a (faster) working system that fully ran our workload. Overall, we identify overheads and optimizations that explain a total difference of about a factor of 20x in raw performance. We also show that there is no single "high pole in the tent" in modern (memory resident) database systems, but that substantial time is spent in logging, latching, locking, B-tree, and buffer management operations.

#index 1063544
#* Efficient provenance storage
#@ Adriane P. Chapman;H. V. Jagadish;Prakash Ramanan
#t 2008
#c 5
#% 139005
#% 233049
#% 300153
#% 397349
#% 462072
#% 480483
#% 504161
#% 569934
#% 570875
#% 604458
#% 659997
#% 664819
#% 772836
#% 875015
#% 893167
#% 900797
#% 978444
#% 1013532
#% 1016204
#% 1017302
#% 1034470
#% 1042656
#% 1042658
#% 1069035
#% 1396743
#% 1692849
#! As the world is increasingly networked and digitized, the data we store has more and more frequently been chopped, baked, diced and stewed. In consequence, there is an increasing need to store and manage provenance for each data item stored in a database, describing exactly where it came from, and what manipulations have been applied to it. Storage of the complete provenance of each data item can become prohibitively expensive. In this paper, we identify important properties of provenance that can be used to considerably reduce the amount of storage required. We identify three different techniques: a family of factorization processes and two methods based on inheritance, to decrease the amount of storage required for provenance. We have used the techniques described in this work to significantly reduce the provenance storage costs associated with constructing MiMI [22], a warehouse of data regarding protein interactions, as well as two provenance stores, Karma [31] and PReServ [20], produced through workflow execution. In these real provenance sets, we were able to reduce the size of the provenance by up to a factor of 20. Additionally, we show that this reduced store can be queried efficiently and further that incremental changes can be made inexpensively.

#index 1063545
#* Efficient lineage tracking for scientific workflows
#@ Thomas Heinis;Gustavo Alonso
#t 2008
#c 5
#% 81709
#% 234905
#% 281847
#% 288790
#% 288990
#% 318704
#% 451429
#% 465150
#% 632040
#% 654470
#% 654493
#% 765129
#% 797452
#% 803468
#% 824799
#% 825661
#% 879803
#% 891642
#% 893189
#% 993983
#% 1016179
#% 1016204
#% 1396738
#% 1676616
#! Data lineage and data provenance are key to the management of scientific data. Not knowing the exact provenance and processing pipeline used to produce a derived data set often renders the data set useless from a scientific point of view. On the positive side, capturing provenance information is facilitated by the widespread use of workflow tools for processing scientific data. The workflow process describes all the steps involved in producing a given data set and, hence, captures its lineage. On the negative side, efficiently storing and querying workflow based data lineage is not trivial. All existing solutions use recursive queries and even recursive tables to represent the workflows. Such solutions do not scale and are rather inefficient. In this paper we propose an alternative approach to storing lineage information captured as a workflow process. We use a space and query efficient interval representation for dependency graphs and show how to transform arbitrary workflow processes into graphs that can be stored using such representation. We also characterize the problem in terms of its overall complexity and provide a comprehensive performance evaluation of the approach.

#index 1063546
#* Discovering topical structures of databases
#@ Wensheng Wu;Berthold Reinwald;Yannis Sismanis;Rajesh Manjrekar
#t 2008
#c 5
#% 57514
#% 263981
#% 307632
#% 333990
#% 397369
#% 406493
#% 462644
#% 465919
#% 572314
#% 765462
#% 790846
#% 800530
#% 818916
#% 830529
#% 893115
#% 893145
#% 893193
#% 1661428
#% 1705191
#% 1729914
#! The increasing complexity of enterprise databases and the prevalent lack of documentation incur significant cost in both understanding and integrating the databases. Existing solutions addressed mining for keys and foreign keys, but paid little attention to more high-level structures of databases. In this paper, we consider the problem of discovering topical structures of databases to support semantic browsing and large-scale data integration. We describe iDisc, a novel discovery system based on a multi-strategy learning framework. iDisc exploits varied evidence in database schema and instance values to construct multiple kinds of database representations. It employs a set of base clusterers to discover preliminary topical clusters of tables from database representations, and then aggregate them into final clusters via meta-clustering. To further improve the accuracy, we extend iDisc with novel multiple-level aggregation and clusterer boosting techniques. We introduce a new measure on table importance and propose an approach to discovering cluster representatives to facilitate semantic browsing. An important feature of our framework is that it is highly extensible, where additional database representations and base clusterers may be easily incorporated into the framework. We have extensively evaluated iDisc using large real-world databases and results show that it discovers topical structures with a high degree of accuracy.

#index 1063547
#* Toward best-effort information extraction
#@ Warren Shen;Pedro DeRose;Robert McCann;AnHai Doan;Raghu Ramakrishnan
#t 2008
#c 5
#% 438135
#% 782759
#% 801668
#% 824718
#% 864394
#% 875064
#% 893167
#% 893168
#% 915277
#% 956455
#% 1022235
#% 1022288
#% 1057143
#% 1063533
#% 1206717
#% 1409422
#% 1713478
#! Current approaches to develop information extraction (IE) programs have largely focused on producing precise IE results. As such, they suffer from three major limitations. First, it is often difficult to execute partially specified IE programs and obtain meaningful results, thereby producing a long "debug loop". Second, it often takes a long time before we can obtain the first meaningful result (by finishing and running a precise IE program), thereby rendering these approaches impractical for time-sensitive IE applications. Finally, by trying to write precise IE programs we may also waste a significant amount of effort, because an approximate result -- one that can be produced quickly -- may already be satisfactory in many IE settings. To address these limitations, we propose iFlex, an IE approach that relaxes the precise IE requirement to enable best-effort IE. In iFlex, a developer U uses a declarative language to quickly write an initial approximate IE program P with a possible-worlds semantics. Then iFlex evaluates P using an approximate query processor to quickly extract an approximate result. Next, U examines the result, and further refines P if necessary, to obtain increasingly more precise results. To refine P, U can enlist a next-effort assistant, which suggests refinements based on the data and the current version of P. Extensive experiments on real-world domains demonstrate the utility of the iFlex approach.

#index 1063548
#* Handling data skew in parallel joins in shared-nothing systems
#@ Yu Xu;Pekka Kostamaa;Xin Zhou;Liang Chen
#t 2008
#c 5
#% 115661
#% 152924
#% 340595
#% 442729
#% 442923
#% 444245
#% 453661
#% 463095
#% 480596
#% 480608
#% 480761
#% 480966
#% 511189
#% 609707
#! Parallel processing continues to be important in large data warehouses. The processing requirements continue to expand in multiple dimensions. These include greater volumes, increasing number of concurrent users, more complex queries, and more applications which define complex logical, semantic, and physical data models. Shared nothing parallel database management systems [16] can scale up "horizontally" by adding more nodes. Most parallel algorithms, however, do not take into account data skew. Data skew occurs naturally in many applications. A query processing skewed data not only slows down its response time, but generates hot nodes, which become a bottleneck throttling the overall system performance. Motivated by real business problems, we propose a new join geography called PRPD (Partial Redistribution & Partial Duplication) to improve the performance and scalability of parallel joins in the presence of data skew in a shared-nothing system. Our experimental results show that PRPD significantly speeds up query elapsed time in the presence of data skew. Our experience shows that eliminating system bottlenecks caused by data skew improves the throughput of the whole system which is important in parallel data warehouses that often run high concurrency workloads.

#index 1063549
#* Efficient and scalable statistics gathering for large databases in Oracle 11g
#@ Sunil Chakkappen;Thierry Cruanes;Benoit Dageville;Linan Jiang;Uri Shaft;Hong Su;Mohamed Zait
#t 2008
#c 5
#% 2833
#% 58348
#% 69273
#% 102786
#% 243166
#% 299989
#% 328431
#% 387427
#% 411554
#% 479914
#% 480805
#% 481749
#% 741995
#% 893137
#% 893175
#% 960250
#% 976993
#% 1016225
#! Large tables are often decomposed into smaller pieces called partitions in order to improve query performance and ease the data management. Query optimizers rely on both the statistics of the entire table and the statistics of the individual partitions to select a good execution plan for a SQL statement. In Oracle 10g, we scan the entire table twice, one pass for gathering the table level statistics and the other pass for gathering the partition level statistics. A consequence of this gathering method is that, when the data in some partitions change, not only do we need to scan the changed partitions to gather the partition level statistics, but also we have to scan the entire table again to gather the table level statistics. Oracle 11g adopts a one-pass distinct sampling based method which can accurately derive the table level statistics from the partition level statistics. When data change, Oracle only re-gathers the statistics for the changed partitions and then derives the table level statistics without touching the unchanged partitions. To the best of our knowledge, although the one-pass distinct sampling has been researched in academia for some years, Oracle is the first commercial database that implements the technique. We have performed extensive experiments on both benchmark data and real customer data. Our experiments illustrate the this new method is highly accurate and has significantly better performance than the old method used in Oracle 10g.

#index 1063550
#* Grouping and optimization of XPath expressions in DB2® pureXML
#@ Andrey Balmin;Fatma Özcan;Ashutosh Singh;Edison Ting
#t 2008
#c 5
#% 70370
#% 116043
#% 236416
#% 397366
#% 397375
#% 413564
#% 428146
#% 570875
#% 570876
#% 745518
#% 781453
#% 800577
#% 803121
#% 810036
#% 810083
#% 810119
#% 824750
#% 864401
#% 875010
#% 881733
#% 881734
#% 1015273
#% 1015274
#% 1015276
#% 1016223
#! Several XML DBMSs support XQuery and/or SQL/XML languages, which are based on navigational primitives in the form of XPath expressions. Typically, these systems either model each XPath step as a separate query plan operator, or employ holistic approaches that can evaluate multiple steps of a single XPath expression. There have also been proposals to execute as many XPath expressions as possible within a single FLWOR block simultaneously in a data streaming context. We observe that blindly combining all possible XPath expressions for concurrent execution can result in significant performance degradation in a database system. We identify two main problems with this strategy. First, the simple strategy of grouping all XPath expressions on a single document does not always work if the query involves more than one data source or has nested query blocks. Second, merging XPath expressions may result in unnecessary execution of branches that can be filtered by predicates in other branches or elsewhere in the query. To rectify these problems, IBM® DB2® pureXML" adopts a combination of heuristic-based rewrite transformations, to decide which XPath expressions should be grouped for concurrent evaluation, and cost-based optimization to globally order the groups within the query execution plan, and locally order the branches within individual groups. Experimental evaluation confirms that selectively grouping multiple XPath expressions allows for better query evaluation performance and reduces the query optimization complexity. These optimization techniques have been implemented as part of IBM DB2 9.5 (pureXML).

#index 1063551
#* A case for flash memory ssd in enterprise database applications
#@ Sang-Won Lee;Bongki Moon;Chanik Park;Jae-Myung Kim;Sang-Woo Kim
#t 2008
#c 5
#% 117
#% 201869
#% 427195
#% 442918
#% 459944
#% 463759
#% 480096
#% 632034
#% 745471
#% 776773
#% 804750
#% 864422
#% 893146
#% 960238
#% 1022298
#% 1052068
#! Due to its superiority such as low access latency, low energy consumption, light weight, and shock resistance, the success of flash memory as a storage alternative for mobile computing devices has been steadily expanded into personal computer and enterprise server markets with ever increasing capacity of its storage. However, since flash memory exhibits poor performance for small-to-moderate sized writes requested in a random order, existing database systems may not be able to take full advantage of flash memory without elaborate flash-aware data structures and algorithms. The objective of this work is to understand the applicability and potential impact that flash memory SSD (Solid State Drive) has for certain type of storage spaces of a database server where sequential writes and random reads are prevalent. We show empirically that up to more than an order of magnitude improvement can be achieved in transaction processing by replacing magnetic disk with flash memory SSD for transaction log, rollback segments, and temporary table spaces.

#index 1063552
#* .NET database programmability and extensibility in microsoft SQL server
#@ José A. Blakeley;Vineet Rao;Isaac Kunen;Adam Prout;Mat Henaire;Christian Kleinerman
#t 2008
#c 5
#% 273937
#% 489544
#% 765475
#% 960309
#% 1728354
#! The integration of the .NET Common Language Runtime (CLR) into the SQL Server DBMS enables rich business logic written in modern .NET languages to run close to the data. Database application developers can write business logic as functions, stored procedures, and triggers. They can also extend the native capabilities of the DBMS by adding new scalar data types, and aggregates. A previous paper [2] described the architecture and design principles of the integration of the CLR inside SQL Server. Here we present new aspects of this work. First, we describe the extensibility contracts for user-defined types and aggregates in detail. Second, we present the advances to the CLR integration in SQL Server 2008 which significantly enhances the breath of applications supported by SQL Server. In particular, we describe the support for large (greater than 8000 byte) user-defined types and aggregates, multiple-input user-defined aggregates, and order-aware table valued functions. Third, we show how we leveraged scalar type extensibility to provide a hierarchical identifier data type that enables encoding of keys describing hierarchies as well as built-in support for spatial applications. This support includes both flat- and round-earth spatial types, as well as a spatial index. Fourth, we present how we use Language Integrated Query (LINQ) enhancements in .NET languages to improve developer productivity when creating routines that require data access. Finally, we present preliminary performance results showing the efficiency of streaming TVFs and aggregates relative to equivalent native features.

#index 1063553
#* Pig latin: a not-so-foreign language for data processing
#@ Christopher Olston;Benjamin Reed;Utkarsh Srivastava;Ravi Kumar;Andrew Tomkins
#t 2008
#c 5
#% 47369
#% 53706
#% 204928
#% 273908
#% 420053
#% 954300
#% 960326
#% 963669
#% 983467
#% 998845
#% 1002142
#% 1021199
#! There is a growing need for ad-hoc analysis of extremely large data sets, especially at internet companies where innovation critically depends on being able to analyze terabytes of data collected every day. Parallel database products, e.g., Teradata, offer a solution, but are usually prohibitively expensive at this scale. Besides, many of the people who analyze this data are entrenched procedural programmers, who find the declarative, SQL style to be unnatural. The success of the more procedural map-reduce programming model, and its associated scalable implementations on commodity hardware, is evidence of the above. However, the map-reduce paradigm is too low-level and rigid, and leads to a great deal of custom user code that is hard to maintain, and reuse. We describe a new language called Pig Latin that we have designed to fit in a sweet spot between the declarative style of SQL, and the low-level, procedural style of map-reduce. The accompanying system, Pig, is fully implemented, and compiles Pig Latin into physical plans that are executed over Hadoop, an open-source, map-reduce implementation. We give a few examples of how engineers at Yahoo! are using Pig to dramatically reduce the time required for the development and execution of their data analysis tasks, compared to using Hadoop directly. We also report on a novel debugging environment that comes integrated with Pig, that can lead to even higher productivity gains. Pig is an open-source, Apache-incubator project, and available for general use.

#index 1063554
#* Supporting table partitioning by reference in oracle
#@ George Eadon;Eugene Inseok Chong;Shrikanth Shankar;Ananth Raghavan;Jagannathan Srinivasan;Souripriya Das
#t 2008
#c 5
#% 58381
#% 286242
#% 411734
#% 745542
#% 765431
#% 824697
#% 1001630
#% 1022236
#! Partitioning is typically employed on large-scale data to improve manageability, availability, and performance. However, for tables connected by a referential constraint (capturing a parent-child relationship), the current approaches require individually partitioning each table thereby burdening the user with the task of maintaining the tables equi-partitioned, which not only is cumbersome but also error prone. This paper proposes a new partitioning method (partition by reference) that allows tables with a parent-child relationship to be logically equi-partitioned by inheriting the partition key from the parent table without duplicating the key columns. The partitioning key is resolved through an existing parent-child relationship, enforced by an active referential constraint. This logical dependency is used to automatically i) cascade partition maintenance operations performed on parent table to child tables, and ii) handle migration of child rows when partition key or parent key in parent table is updated, as a single atomic operation. This method has been introduced in Oracle Database 11gR1 with support for tables with both single level and composite partitioning methods. The paper describes the key concepts of table partitioning by reference method, discusses the design and implementation challenges, and presents an experimental study covering a usage scenario common in Information Life Cycle Management (ILM) applications.

#index 1063555
#* SPADE: the system s declarative stream processing engine
#@ Bugra Gedik;Henrique Andrade;Kun-Lung Wu;Philip S. Yu;Myungcheol Doo
#t 2008
#c 5
#% 788216
#% 806985
#% 875006
#% 995806
#% 1022232
#% 1022302
#% 1022311
#% 1206786
#! In this paper, we present Spade - the System S declarative stream processing engine. System S is a large-scale, distributed data stream processing middleware under development at IBM T. J. Watson Research Center. As a front-end for rapid application development for System S, Spade provides (1) an intermediate language for flexible composition of parallel and distributed data-flow graphs, (2) a toolkit of type-generic, built-in stream processing operators, that support scalar as well as vectorized processing and can seamlessly inter-operate with user-defined operators, and (3) a rich set of stream adapters to ingest/publish data from/to outside sources. More importantly, Spade automatically brings performance optimization and scalability to System S applications. To that end, Spade employs a code generation framework to create highly-optimized applications that run natively on the Stream Processing Core (SPC), the execution and communication substrate of System S, and take full advantage of other System S services. Spade allows developers to construct their applications with fine granular stream operators without worrying about the performance implications that might exist, even in a distributed system. Spade's optimizing compiler automatically maps applications into appropriately sized execution units in order to minimize communication overhead, while at the same time exploiting available parallelism. By virtue of the scalability of the System S runtime and Spade's effective code generation and optimization, we can scale applications to a large number of nodes. Currently, we can run Spade jobs on ≈ 500 processors within more than 100 physical nodes in a tightly connected cluster environment. Spade has been in use at IBM Research to create real-world streaming applications, ranging from monitoring financial market feeds to radio telescopes to semiconductor fabrication lines.

#index 1063556
#* Query-aware partitioning for monitoring massive network data streams
#@ Theodore Johnson;Muthu S. Muthukrishnan;Vladislav Shkapenyuk;Oliver Spatscheck
#t 2008
#c 5
#% 115661
#% 300179
#% 330305
#% 378388
#% 397397
#% 654497
#% 659917
#% 726621
#% 765404
#% 781700
#% 803602
#% 824664
#% 894646
#% 963595
#% 979303
#% 1016271
#! Data Stream Management Systems (DSMS) are gaining acceptance for applications that need to process very large volumes of data in real time. The load generated by such applications frequently exceeds by far the computation capabilities of a single centralized server. In particular, a single-server instance of our DSMS, Gigascope, cannot keep up with the processing demands of the new OC-786 networks, which can generate more than 100 million packets per second. In this paper, we explore a mechanism for the distributed processing of very high speed data streams. Existing distributed DSMSs employ two mechanisms for distributing the load across the participating machines: partitioning of the query execution plans and partitioning of the input data stream in a query-independent fashion. However, for a large class of queries, both approaches fail to reduce the load as compared to centralized system, and can even lead to an increase in the load. In this paper we present an alternative approach - query-aware data stream partitioning that allows for more efficient scaling. We present methods for analyzing any given query set and choose the optimal partitioning scheme, and show how to reconcile potentially conflicting requirements that different queries might place on partitioning. We conclude with experiments on a small cluster of processing nodes on high-rate network traffic feed that demonstrates with different query sets that our methods effectively distribute the load across all processing nodes and facilitate efficient scaling whenever more processing nodes becomes available.

#index 1063557
#* Helping satisfy multiple objectives during a service desk conversation
#@ Ullas Nambiar;Himanshu Gupta;Raju Balakrishnan;Mukesh Mohania
#t 2008
#c 5
#% 36408
#% 287631
#% 376266
#% 387427
#% 509533
#% 624479
#% 741441
#% 769877
#% 769884
#% 782759
#% 800590
#% 830526
#% 864416
#% 875003
#% 893143
#% 1022320
#! Agents manning a service desk have the unenviable task of satisfying multiple conflicting objectives. Specifically, businesses require the agents to meet pre-specified customer satisfaction levels while keeping the cost of operations low or meeting sales targets, objectives that end up being complementary. Additional complexity is introduced by the fact that the objectives are often inter-dependent and have to be met in real-time. Moreover, business might change the objectives from time to time e.g. from reducing cost of operation to increasing sales of slow moving product. In this paper, we describe CallAssist - a speech enabled real-time dialog management system that dynamically helps agents in building a conversation that meets the various business objectives while satisfying customer requirements. An added benefit of our solution is the ability to adapt to changing business needs without incurring agent re-training costs. We provide evaluation results displaying the efficiency and effectiveness of our system.

#index 1063558
#* Oracle database replay
#@ Leonidas Galanis;Supiti Buranawatanachoke;Romain Colle;Benoît Dageville;Karl Dias;Jonathan Klein;Stratos Papadomanolakis;Leng Leng Tan;Venkateshwaran Venkataramani;Yujun Wang;Graham Wood
#t 2008
#c 5
#% 27220
#% 172913
#% 252997
#% 479656
#% 741995
#% 772180
#% 840514
#% 960262
#% 963458
#% 983794
#% 1016216
#! This paper presents Oracle Database Replay, a novel approach to testing changes to the relational database management system component of an information system (software upgrades, hardware changes etc). Database Replay makes it possible to subject a test system to a real production system workload, which helps identify all potential problems before implementing the planned changes on the production system. Any interesting workload period of a production database system can be captured with minimal overhead. The captured workload can be used to drive a test system while maintaining the concurrency and load characteristics of the real production workload. Therefore, the test results using database replay can provide very high assurance in determining the impact of changes to a production system before applying these changes. This paper presents the architecture of Database Replay as well as experimental results that demonstrate its usefulness as testing methodology.

#index 1063559
#* Damia: data mashups for intranet applications
#@ David E. Simmen;Mehmet Altinel;Volker Markl;Sriram Padmanabhan;Ashutosh Singh
#t 2008
#c 5
#% 136740
#% 810050
#% 893087
#! Increasingly large numbers of situational applications are being created by enterprise business users as a by-product of solving day-to-day problems. In efforts to address the demand for such applications, corporate IT is moving toward Web 2.0 architectures. In particular, the corporate intranet is evolving into a platform of readily accessible data and services where communities of business users can assemble and deploy situational applications. Damia is a web style data integration platform being developed to address the data problem presented by such applications, which often access and combine data from a variety of sources. Damia allows business users to quickly and easily create data mashups that combine data from desktop, web, and traditional IT sources into feeds that can be consumed by AJAX, and other types of web applications. This paper describes the key features and design of Damia's data integration engine, which has been packaged with Mashup Hub, an enterprise feed server currently available for download on IBM alphaWorks. Mashup Hub exposes Damia's data integration capabilities in the form of a service that allows users to create hosted data mashups.

#index 1063560
#* Effective and efficient semantic web data management over DB2
#@ Li Ma;Chen Wang;Jing Lu;Feng Cao;Yue Pan;Yong Yu
#t 2008
#c 5
#% 458831
#% 519567
#% 561740
#% 577305
#% 824697
#% 851283
#% 1015335
#% 1022236
#% 1022336
#% 1667768
#% 1684010
#% 1696286
#% 1696332
#% 1713479
#% 1713480
#! With the fast growth of Semantic Web, more and more RDF data and ontologies are created and widely used in Web applications and enterprise information systems. It is reported that the W3C Linking Open Data community project consists of over two billion RDF triples, which are interlinked by about three million RDF links. Recently, efficient RDF data management on top of relational databases gains particular attentions from both Semantic Web community and database community. In this paper, we present effective and efficient Semantic Web data management over DB2, including efficient schema and indexes design for storage, practical ontology reasoning support, and an effective SPARQL-to-SQL translation method for RDF query. Moreover, we show the performance and scalability of our system by an evaluation among well-known RDF stores and discuss future work.

#index 1063561
#* Multi-tenant databases for software as a service: schema-mapping techniques
#@ Stefan Aulbach;Torsten Grust;Dean Jacobs;Alfons Kemper;Jan Rittinger
#t 2008
#c 5
#% 248829
#% 286258
#% 287082
#% 335725
#% 480629
#% 742563
#% 824697
#% 864445
#% 923676
#% 1016212
#% 1022236
#% 1041566
#! In the implementation of hosted business services, multiple tenants are often consolidated into the same database to reduce total cost of ownership. Common practice is to map multiple single-tenant logical schemas in the application to one multi-tenant physical schema in the database. Such mappings are challenging to create because enterprise applications allow tenants to extend the base schema, e.g., for vertical industries or geographic regions. Assuming the workload stays within bounds, the fundamental limitation on scalability for this approach is the number of tables the database can handle. To get good consolidation, certain tables must be shared among tenants and certain tables must be mapped into fixed generic structures such as Universal and Pivot Tables, which can degrade performance. This paper describes a new schema-mapping technique for multi-tenancy called Chunk Folding, where the logical tables are vertically partitioned into chunks that are folded together into different physical multi-tenant tables and joined as needed. The database's "meta-data budget" is divided between application-specific conventional tables and a large fixed set of generic structures called Chunk Tables. Good performance is obtained by mapping the most heavily-utilized parts of the logical schemas into the conventional tables and the remaining parts into Chunk Tables that match their structure as closely as possible. We present the re sults of several experiments designed to measure the efficacy of Chunk Folding and describe the multi-tenant database testbed in which these experiments were performed.

#index 1063562
#* Spatial indexing in microsoft SQL server 2008
#@ Yi Fang;Marc Friedman;Giri Nair;Michael Rys;Ana-Elisa Schmid
#t 2008
#c 5
#% 25152
#% 86950
#% 86952
#% 116065
#% 201880
#% 210189
#% 213975
#% 237187
#% 248798
#% 254749
#% 397396
#% 411694
#% 427199
#% 430750
#% 464007
#% 480093
#% 481599
#% 527004
#% 527026
#% 745495
#% 1016224
#! Microsoft SQL Server 2008 adds built-in support for 2-dimensional spatial data types for both planar and geodetic geometries to address the increasing demands for managing location-aware data. SQL Server 2008 also adds indexing capabilities that, together with the necessary plan selections done by the query optimizer, provide efficient processing of spatial queries. This paper will present an overview of the spatial indexing implementation in SQL Server 2008 and outline how the indexing is implemented and how the cost-based query optimizer chooses among the different plans.

#index 1063563
#* SGL: a scalable language for data-driven games
#@ Robert Albright;Alan Demers;Johannes Gehrke;Nitin Gupta;Hooyeon Lee;Rick Keilty;Gregory Sadowski;Ben Sowell;Walker White
#t 2008
#c 5
#% 960236
#% 983107
#% 1021194
#! We propose to demonstrate SGL, a language and system for writing computer games using data management techniques. We will demonstrate a complete game built using the system, and show how complex game behavior can be expressed in a declarative scripting language. The demo will also illustrate the workflow necessary to modify a game and include a visualization of the relational operations that are executed as the game runs.

#index 1063564
#* The DBO database system
#@ Florin Rusu;Fei Xu;Luis Leopoldo Perez;Mingxi Wu;Ravi Jampani;Chris Jermaine;Alin Dobra
#t 2008
#c 5
#% 227883
#% 273910
#% 274152
#% 960294
#! We demonstrate our prototype of the DBO database system. DBO is designed to facilitate scalable analytic processing over large data archives. DBO's analytic processing performance is competitive with other database systems; however, unlike any other existing research or industrial system, DBO maintains a statistically meaningful guess to the final answer to a query from start to finish during query processing. This guess may be quite accurate after only a few seconds or minutes, while answering a query exactly may take hours. This can result in significant savings in both user and computer time, since a user can abort a query as soon as he or she is happy with the guess' accuracy.

#index 1063565
#* Stretch 'n' shrink: resizing queries to user preferences
#@ Chaitanya Mishra;Nick Koudas
#t 2008
#c 5
#% 137885
#% 227894
#% 643566
#% 893105
#% 902467
#! We present Stretch 'n' Shrink, a query design framework that explicitly takes into account user preferences about the desired answer size, and subsequently modifies the query with user feedback to meet this target. Our system has been prototyped inside an open source data manager, and requires minimal modifications to the database engine.

#index 1063566
#* Incorporating string transformations in record matching
#@ Arvind Arasu;Surajit Chaudhuri;Kris Ganjam;Raghav Kaushik
#t 2008
#c 5
#% 479973
#% 810107
#% 875066
#% 893164
#% 1206615
#! Today's record matching infrastructure does not allow a flexible way to account for synonyms such as "Robert" and "Bob" which refer to the same name, and more general forms of string transformations such as abbreviations. We expand the problem of record matching to take such user-defined string transformations as input. These transformations coupled with an underlying similarity function are used to define the similarity between two strings. We demonstrate the effectiveness of this approach via a fuzzy match operation that is used to lookup an input record against a table of records, where we have an additional table of transformations as input. We demonstrate an improvement in record matching quality and efficient retrieval based on our index structure that is cognizant of transformations.

#index 1063567
#* SEMMO: a scalable engine for massively multiplayer online games
#@ Nitin Gupta;Alan J. Demers;Johannes E. Gehrke
#t 2008
#c 5
#% 179839
#% 805474
#% 960236
#! We propose to demonstrate SEMMO, a consistency server for MMOs. The key features of SEMMO are its novel distributed consistency protocol and system architecture. The distributed nature of the engine allows the clients to perform all computations locally; the only computation that the central server performs is to determine the serialization order of game actions. We will demo SEMMO through a game called Manhattan Pals, and show how we can exploit game semantics in order to support large-scale MMOs. In the demo, avatars of the audience will be able to play Manhattan Pals and thus experience various scalability and consistency effects of an MMO.

#index 1063568
#* Orion 2.0: native support for uncertain data
#@ Sarvjeet Singh;Chris Mayfield;Sagar Mittal;Sunil Prabhakar;Susanne Hambrusch;Rahul Shah
#t 2008
#c 5
#% 824764
#% 907562
#% 1016202
#% 1102984
#% 1206735
#! Orion is a state-of-the-art uncertain database management system with built-in support for probabilistic data as first class data types. In contrast to other uncertain databases, Orion supports both attribute and tuple uncertainty with arbitrary correlations. This enables the database engine to handle both discrete and continuous pdfs in a natural and accurate manner. The underlying model is closed under the basic relational operators and is consistent with Possible Worlds Semantics. We demonstrate how Orion simplifies the design and enhances the capabilities of two example applications: managing sensor data (continuous uncertainty) and inferring missing values (discrete uncertainty).

#index 1063569
#* Building a global location search service
#@ Vibhuti Sengar;Tanuja Joshi;Joseph M. Joy;Samarth Prakash
#t 2008
#c 5
#% 319508
#% 356410
#% 654467
#% 1035399
#! We present a crosslingual location search service that works across multiple countries and deals effectively with ambiguous and ill-formed queries. The system returns a ranked list of spatial regions (points, lines and polygons) that best match users' text queries, which can range from postal addresses to unstructured queries that list a few col-located map features. The system's robustness comes from a novel approach that exploits spatial coherence to identify viable interpretations of input text. Unlike existing state of the art geocoding systems, our system requires no region-specific rules, training or customization, and thus may be built to cover any region for which detailed map data is available, making it possible, for the first time, to rapidly build location search services for new regions, and more generally, approximate text search over arbitrary spatial repositories. Our system has been shown to outperform commercial geocoding systems, especially when spelling or format variations are introduced. We demonstrate our sys-tem's capabilities by showing results for a variety of text queries over a large dataset from several countries with widely differing address formats.

#index 1063570
#* Freebase: a collaboratively created graph database for structuring human knowledge
#@ Kurt Bollacker;Colin Evans;Praveen Paritosh;Tim Sturge;Jamie Taylor
#t 2008
#c 5
#! Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Freebase currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.

#index 1063571
#* Querying and re-using workflows with VsTrails
#@ Carlos E. Scheidegger;Huy T. Vo;David Koop;Juliana Freire;Claudio T. Silva
#t 2008
#c 5
#% 660001
#% 878224
#% 1013532
#% 1728161
#! We show how work flow systems can be augmented to leverage provenance information to enhance usability. In particular, we will demonstrate new mechanisms and intuitive user interfaces designed to allow users to query work flows by example and to refine work flows by analogies. These techniques are implemented in VisTrails, an open-source provenance-enabled scientific work flow system that can be combined with a wide range of tools, libraries, and visualization systems. We will show di erent scenarios where these techniques can be used to simplify the notoriously hard tasks of creating and refining work flows.

#index 1063572
#* HERMES: aggregative LBS via a trajectory DB engine
#@ Nikos Pelekis;Elias Frentzos;Nikos Giatrakos;Yannis Theodoridis
#t 2008
#c 5
#% 458865
#% 461923
#% 480473
#% 873099
#% 927131
#% 976715
#% 982595
#% 1404760
#% 1688318
#! We present HERMES, a prototype system based on a powerful query language for trajectory databases, which enables the support of aggregative Location-Based Services (LBS). The key observation that motivates HERMES is that the more the knowledge in hand about the trajectory of a mobile user, the better the exploitation of the advances in spatio-temporal query processing for providing intelligent LBS. HERMES is fully incorporated into a state-of-the-art Object-Relational DBMS, and its demonstration illustrates its flexibility and usefulness for delivering custom-defined LBS.

#index 1063573
#* SchemaScope: a system for inferring and cleaning XML schemas
#@ Geert Jan Bex;Frank Neven;Stijn Vansummeren
#t 2008
#c 5
#% 431034
#% 504573
#% 772031
#% 845589
#% 893098
#% 894435
#% 1022285
#! We present SchemaScope, a system to derive Document Type Definitions and XML schema from a corpus of sample XML documents. Tools are provided to visualize, clean and refine existing or inferred schemas. A number of use cases illustrate the versatility of the system, as well as various types of applications.

#index 1063574
#* DiMaC: a system for cleaning disguised missing data
#@ Ming Hua;Jian Pei
#t 2008
#c 5
#% 17144
#% 185079
#% 878941
#% 989667
#% 1272290
#! In some applications such as filling in a customer information form on the web, some missing values may not be explicitly represented as such, but instead appear as potentially valid data values. Such missing values are known as disguised missing data, which may impair the quality of data analysis severely. The very limited previous studies on cleaning disguised missing data highly rely on domain background knowledge in specific applications and may not work well for the cases where the disguise values are inliers. Recently, we have studied the problem of cleaning disguised missing data systematically, and proposed an effective heuristic approach [2]. In this paper, we describe a demonstration of DiMaC, a Disguised Missing Data Cleaning system which can find the frequently used disguise values in data sets without requiring any domain background knowledge. In this demo, we will show (1) the critical techniques of finding suspicious disguise values; (2) the architecture and user interface of DiMaC system; (3) an empirical case study on both real and synthetic data sets, which verifies the effectiveness and the efficiency of the techniques; (4) some challenges arising from real applications and several direction for future work.

#index 1063575
#* An xml index advisor for DB2
#@ Iman Elghandour;Ashraf Aboulnaga;Daniel C. Zilio;Fei Chiang;Andrey Balmin;Kevin Beyer;Calisto Zuzarte
#t 2008
#c 5
#% 632100
#% 650962
#% 810026
#% 881733
#% 960317
#% 1206667
#! XML database systems are expected to handle increasingly complex queries over increasingly large and highly structured XML databases. An important problem that needs to be solved for these systems is how to choose the best set of indexes for a given workload. We have developed an XML Index Advisor that solves this XML index recommendation problem and is tightly coupled with the query optimizer of the database system. We have implemented our XML Index Advisor for DB2. In this demonstration we showcase the new query optimizer modes that we added to DB2, the index recommendation process, and the effectiveness of the recommended indexes.

#index 1063576
#* Clip: a tool for mapping hierarchical schemas
#@ Alessandro Raffio;Daniele Braga;Stefano Ceri;Paolo Papotti;Mauricio A. Hernández
#t 2008
#c 5
#% 810021
#% 824763
#% 893094
#% 993981
#% 1206614
#! Many data integration solutions in the market today include visual tools for schema mapping. Users connect schema elements with lines that are interpreted as high-level logical expressions capturing the relationship between source and target data-sets. These expressions are compiled into queries or programs that convert source-side data instances into target-side instances. In this demo we showcase Clip, an XML Schema mapping tool. Clip is distinguished from existing tools in that mappings explicitly specify structural transformations in addition to value correspondences. We show how Clip's users enter mappings by drawing lines and how these lines are translated into XQuery.

#index 1063577
#* UQBE: uncertain query by example for web service mashup
#@ Junichi Tatemura;Songting Chen;Fenglin Liao;Oliver Po;K. Selcuk Candan;Divyakant Agrawal
#t 2008
#c 5
#% 874876
#% 893118
#% 893167
#% 893196
#% 1022259
#% 1206612
#! The UQBE is a mashup tool for non-programmers that supports query-by-example (QBE) over a schema made up by the user without knowing the schema of the original sources. Based on automated schema matching with uncertainty, the UQBE system returns the best confident results. The system lets the user refine them interactively. A tuple in the query result is associated with lineage that is a boolean formula over schema matching decisions representing underlying conditions on which the corresponding tuple is included in the result. Given binary feedbacks on tuples by the user, which are possibly imprecise, the system solves it as an optimization problem to refine confidence values of matching decisions. The demo features graphical user interaction on the UQBE system, including querying and refinement.

#index 1063578
#* Muse: a system for understanding and designing mappings
#@ Bogdan Alexe;Laura Chiticariu;Renée J. Miller;Daniel Pepper;Wang-Chiew Tan
#t 2008
#c 5
#% 333988
#% 810078
#% 893094
#% 893095
#% 993981
#% 1206612
#! Schema mappings are logical assertions that specify the relationships between a source and a target schema in a declarative way. The specification of such mappings is a fundamental problem in information integration. Mappings can be generated by existing mapping systems (semi-)automatically from a visual specification between two schemas. In general, the well-known 80-20 rule applies for mapping generation tools. They can automate 80% of the work, covering common cases and creating a mapping that is close to correct. However, ensuring complete correctness can still require intricate manual work to perfect portions of the mapping. Previous research on mapping understanding and refinement and anecdotal evidence from mapping designers suggest that the mapping design process can be perfected by using data examples to explain the mapping and alternative mappings. We demonstrate Muse, a data example driven mapping design tool currently implemented on top of the Clio schema mapping system. Muse leverages data examples that are familiar to a designer to illustrate nuances of how a small change to a mapping specification changes its semantics. We demonstrate how Muse can differentiate between alternative mapping specifications and infer the desired mapping semantics based on the designer's actions on a short sequence of simple data examples.

#index 1063579
#* NAGA: harvesting, searching and ranking knowledge
#@ Gjergji Kasneci;Fabian M. Suchanek;Georgiana Ifrim;Shady Elbassuoni;Maya Ramanath;Gerhard Weikum
#t 2008
#c 5
#% 660011
#% 956501
#% 956564
#% 956574
#% 1206702
#! The presence of encyclopedic Web sources, such as Wikipedia, the Internet Movie Database (IMDB), World Factbook, etc. calls for new querying techniques that are simple and yet more expressive than those provided by standard keyword-based search engines. Searching for explicit knowledge needs to consider inherent semantic structures involving entities and relationships. In this demonstration proposal, we describe a semantic search system named NAGA. NAGA operates on a knowledge graph, which contains millions of entities and relationships derived from various encyclopedic Web sources, such as the ones above. NAGA's graph-based query language is geared towards expressing queries with additional semantic information. Its scoring model is based on the principles of generative language models, and formalizes several desiderata such as confidence, informativeness and compactness of answers. We propose a demonstration of NAGA which will allow users to browse the knowledge base through a user interface, enter queries in NAGA's query language and tune the ranking parameters to test various ranking aspects.

#index 1063580
#* The Spicy system: towards a notion of mapping quality
#@ Angela Bonifati;Giansalvatore Mecca;Alessandro Pappalardo;Salvatore Raunich;Gianvito Summa
#t 2008
#c 5
#% 572314
#% 824763
#% 893095
#% 945786
#% 960233
#% 993981
#% 1044442
#% 1915878
#! We introduce the Spicy system, a novel approach to the problem of automatically selecting the best mappings among two data sources. Known schema mapping algorithms rely on value correspondences -- i.e. correspondences among semantically related attributes -- to produce complex transformations among data sources. Spicy brings together schema matching and mapping generation tools to further automate this process. A key observation, here, is that the quality of the mappings is strongly influenced by the quality of the input correspondences. To address this problem, Spicy adopts a three-layer architecture, in which a schema matching module is used to provide input to a mapping generation module. Then, a third module, the mapping verification module, is used to check candidate mappings and choose the ones that represent better transformations of the source into the target. At the core of the system stands a new technique for comparing the structure and actual content of trees, called structural analysis. Experimental results show that our mapping discovery algorithm achieves both good scalability and high precision in mapping selection.

#index 1063581
#* XArch: archiving scientific and reference data
#@ Heiko Müller;Peter Buneman;Ioannis Koltsidas
#t 2008
#c 5
#% 235914
#% 330627
#% 742561
#% 745445
#% 994001
#! Database archiving is important for the retrieval of old versions of a database and for temporal queries over the history of data. We demonstrate XArch, a management system for maintaining, populating, and querying archives of hierarchical data. XArch is based on a nested merge approach that efficiently stores multiple versions of hierarchical data in a compact archive. By merging elements into one data structure, any specific version is retrievable from the archive in a single pass over the data and efficient tracking of object history is possible. XArch implements this approach and extends it in two important ways. First, in order to merge large hierarchical data sets, elements need to be sorted according to their key values. We developed an efficient algorithm for sorting hierarchical data in secondary storage and modified the nested merge algorithm accordingly. Second, we designed and implemented a declarative query language that enables one both to view data from particular versions and to track the history of objects. We demonstrate this using both molecular biology and demographic reference data as examples.

#index 1063582
#* LearnPADS: automatic tool generation from ad hoc data
#@ Kathleen Fisher;David Walker;Kenny Q. Zhu
#t 2008
#c 5
#% 654469
#% 809123
#% 848307
#% 912487
#% 1024194
#% 1410422
#! In this demonstration, we will present LEARNPADS, a fully automatic system for generating ad hoc data processing tools. When presented with a collection of ad hoc data, the system (1) analyzes the data, (2) infers a PADS [4, 5] description, (3) generates parser, printer, validation and traversal libraries and (4) links these libraries with format-independent tool suites to form stand-alone applications. These applications provide statistical analysis, XML conversion, CSV conversion, the ability to query with the Galax XQuery engine [3], and the ability to graph selected data elements, all directly from ASCII ad hoc data without human intervention. SIGMOD attendees will see both the user experience with LEARNPADS and the internals of the multi-phase inference algorithm which lies at the heart of the system.

#index 1063583
#* Borealis-R: a replication-transparent stream processing system for wide-area monitoring applications
#@ Jeong-Hyon Hwang;Sanghoon Cha;Uǧur Cetintemel;Stan Zdonik
#t 2008
#c 5
#% 765470
#% 800583
#% 810008
#% 1206600
#! Borealis-R is a replication-based system for both fast and highly-available processing of data streams over wide-area networks. In Borealis-R, multiple operator replicas send outputs to downstream replicas, allowing each replica to use whichever data arrives first. To further reduce latency, replicas run without coordination, possibly processing data in different orders. Despite this flexibility, Borealis-R guarantees that applications always receive the same results as in the non-replicated, failure-free case. In addition, Borealis-R deploys replicas at select network locations to effectively improve performance as well as availability. We demonstrate the strengths of Borealis-R using a live wide-area monitoring application. We show that Borealis-R outperforms previous solutions in terms of latency and that it uses system resources efficiently by carefully deploying and discarding replicas.

#index 1063584
#* Tinycasper: a privacy-preserving aggregate location monitoring system in wireless sensor networks
#@ Chi-Yin Chow;Mohamed F. Mokbel;Tian He
#t 2008
#c 5
#% 114573
#% 281506
#% 309430
#% 581040
#% 791779
#% 864662
#% 893151
#! This demo presents a privacy-preserving aggregate location monitoring system, namely, TinyCasper, in which we can monitor moving objects in wireless sensor networks while preserving their location privacy. TinyCasper consists of two main modules, in-network location anonymization and aggregate query processing over anonymized locations. In the first module, trusted wireless sensor nodes collaborate with each other to anonymize users' exact locations by a cloaked spatial region that satisfies a prespecified privacy requirement. On the other side, the aggregate query processing module collects and analyzes the cloaked spatial regions reported from the wireless sensor nodes to support aggregate and alarm queries over anonymized locations. The prototype of TinyCasper is implemented on a physical test-bed on the TinyOS/Mote platform with 39 MICAz motes.

#index 1063585
#* The Demaq system: declarative development of distributed applications
#@ Alexander Böhm;Erich Marth;Carl-Christian Kanne
#t 2008
#c 5
#% 345758
#% 570876
#! The goal of the Demaq project is to investigate a novel way of thinking about distributed applications that are based on the asynchronous exchange of XML messages. Unlike today's solutions that rely on imperative programming languages and multi-tiered application servers, Demaq uses a declarative language for implementing the application logic as a set of rules. A rule compiler transforms the application specifications into execution plans against the message history. The plans are evaluated using our optimized runtime engine. This allows us to leverage existing knowledge about declarative query processing for optimizing distributed applications.

#index 1063586
#* ProSem: scalable wide-area publish/subscribe
#@ Badrish Chandramouli;Jun Yang;Pankaj K. Agarwal;Albert Yu;Ying Zheng
#t 2008
#c 5
#% 338354
#% 661478
#% 736390
#% 875019
#% 1022275
#% 1688281
#% 1849768
#! We demonstrate ProSem, a scalable wide-area publish/subscribe system that supports complex, stateful subscriptions as well as simple ones. One unique feature of ProSem is its cost-based joint optimization of both subscription processing and notification dissemination. ProSem uses novel reformulation techniques to expose new alternatives for processing and disseminating data using standard stateless content-driven network components.

#index 1063587
#* A demonstration of Cascadia through a digital diary application
#@ Nodira Khoussainova;Evan Welbourne;Magdalena Balazinska;Gaetano Borriello;Garrett Cole;Julie Letchner;Yang Li;Christopher Ré;Dan Suciu;Jordan Walke
#t 2008
#c 5
#% 481448
#% 1065043
#% 1113048
#% 1668029
#! The Cascadia system provides RFID-based pervasive computing applications with an infrastructure for specifying, extracting and managing meaningful high-level events from raw RFID data. Cascadia allows application developers and even users to specify events of interest using either a declarative query language or a graphical interface with an intuitive visual language. Cascadia then effectively extracts these events from data in spite of the unreliability of RFID technology and the inherent ambiguity in event extraction. We demonstrate Cascadia's technique through a digital diary application in the form of a calendar. Cascadia automatically populates the calendar with meaningful events for the user. We use data collected in a building-wide RFID deployment.

#index 1063588
#* From del.icio.us to x.qui.site: recommendations in social tagging sites
#@ Sihem Amer-Yahia;Alban Galland;Julia Stoyanovich;Cong Yu
#t 2008
#c 5
#% 333854
#! We present X.QUI.SITE, a scalable system for managing recommendations for social tagging sites like del.icio.us. seamlessly incorporates various user behaviors into the recommendations and aims to recommend not only items of interest, but also other relevant information like interesting people and/or topics. Explanations are also provided so that users can obtain a better understanding of the recommendations and decide which recommendations to pursue further. We discuss the technical challenges involved in characterizing different user behaviors and in efficiently computing recommendation explanations.

#index 1063589
#* Enriching topic-based publish-subscribe systems with related content
#@ Rubi Boim;Tova Milo
#t 2008
#c 5
#% 340175
#% 505869
#% 674136
#% 812781
#% 960296
#% 978763
#% 1706222
#% 1849768
#! This demonstration presents RMFinder (Related Messages Finder), a system that retains the simplicity and efficiency of topic-based P2P pub-sub, while providing a richer service where users can automatically receive all messages related to those in the topics to which they are subscribed. RMFinder is based on a novel, dynamic, distributed clustering algorithm, that takes advantage of similarities between topic messages to group topics together, into topic-clusters. The clusters adjust automatically to shifts in the focus of the messages published by the topics, as well as to changes in the users interest, and allow for an effective delivery of related messages with minimal overhead.

#index 1063590
#* XRPC: distributed XQuery and update processing with heterogeneous XQuery engines
#@ Ying Zhang;Peter Boncz
#t 2008
#c 5
#% 875010
#% 1016150
#% 1022210
#! We demonstrate XRPC, a minimal XQuery extension that enables distributed querying between heterogeneous XQuery engines. The XRPC language extension enhances the existing concept of XQuery functions with the Remote Procedure Call (RPC) paradigm. XRPC is orthogonal to all XQuery features, including the XQuery Update Facility (XQUF). Note that executing xquf updating functions over XRPC leads to the phenomenon of distributed transactions. XRPC achieves heterogeneity by an open SOAP-based network protocol, that can be implemented by any engine, and an XRPC Wrapper that allows even XRPC-oblivious XQuery engines to handle XRPC requests efficiently. XRPC is fully implemented in the open-source MonetDB/XQuery engine, and is demonstrated here to co-operate with Saxon, Galax and X-Hive through the XRPC wrapper. This demonstration will focus on the following features of XRPC: (i) glue-less interaction between AJAX style webbased applications with XQuery databases thanks to the SOAP-based nature of the XRPC network protocol, (ii) the efficiency of XRPC communication also for voluminous interserver communication thanks to the Bulk RPC feature that optimizes network communication and exposes set-at-a-time opportunities to the underlying XQuery engines, (iii) the interoperability between different XQuery engines that can handle both distributed transactions (both read-only requests and updates) (iv) support and performance trade-offs of two different isolation levels for distributed transactions among different XQuery engines.

#index 1063591
#* XQuery in the browser
#@ Ghislain Fourny;Donald Kossmann;Tim Kraska;Markus Pilman;Daniela Florescu
#t 2008
#c 5
#! Over the years, the browser has become a complete runtime environment for client-side programs. The main scripting language used towards this purpose is JavaScript, which was designed so as to program the browser. A lot of extensions and new layers have been built on top of it to allow e.g. DOM navigation and manipulation. However, JavaScript has become a victim of its own success and is used way beyond its possibilities, leading to increased code complexity. We suggest to reduce programming complexity by proposing XQuery as a client-side programming language. We wrote an extension for Microsoft Internet Explorer, based on the Zorba XQuery engine, which allows execution of XQuery scripts in the browser. An extension for Firefox is on the way as well. This paper demonstrates how client-side applications in XQuery look like and what they can do within a very small amount of code.

#index 1063592
#* BibNetMiner: mining bibliographic information networks
#@ Yizhou Sun;Tianyi Wu;Zhijun Yin;Hong Cheng;Jiawei Han;Xiaoxin Yin;Peixiang Zhao
#t 2008
#c 5
#% 823342
#% 893124
#% 893127
#% 989682
#% 1022314
#! Online bibliographic databases, such as DBLP in computer science and PubMed in medical sciences, contain abundant information about research publications in different fields. Each such database forms a gigantic information network (hence called BibNet), connecting in complex ways research papers, authors, conferences/journals, and possibly citation information as well, and provides a fertile land for information network analysis. Our BibNetMiner is designed for sophisticated information network mining on such bibliographic databases. In this demo, we will take the DBLP database as an example, demonstrate several attractive functions of BibNetMiner, including clustering, ranking and profiling of conferences and authors based on the research subfields. A user-friendly, visualization-enhanced interface will be provided to facilitate interactive exploration of a bibliographic database. This project will serve as an example to demonstrate the power of links in information network mining. Since the dataset is large and the network is heterogeneous, such a study will benefit the research on the analysis of massive heterogeneous information networks.

#index 1063593
#* Provenance and scientific workflows: challenges and opportunities
#@ Susan B. Davidson;Juliana Freire
#t 2008
#c 5
#% 202409
#% 346653
#% 504161
#% 803468
#% 825661
#% 893117
#% 896027
#% 929519
#% 954295
#% 960234
#% 960365
#% 1004604
#% 1013532
#% 1013546
#% 1042649
#% 1042651
#% 1042654
#% 1042655
#% 1042658
#% 1042659
#% 1042661
#% 1042665
#% 1048864
#% 1081399
#% 1206750
#% 1692849
#% 1728161
#% 1728173
#% 1728182
#! Provenance in the context of workflows, both for the data they derive and for their specification, is an essential component to allow for result reproducibility, sharing, and knowledge re-use in the scientific community. Several workshops have been held on the topic, and it has been the focus of many research projects and prototype systems. This tutorial provides an overview of research issues in provenance for scientific workflows, with a focus on recent literature and technology in this area. It is aimed at a general database research audience and at people who work with scientific data and workflows. We will (1) provide a general overview of scientific workflows, (2) describe research on provenance for scientific workflows and show in detail how provenance is supported in existing systems; (3) discuss emerging applications that are enabled by provenance; and (4) outline open problems and new directions for database-related research.

#index 1063594
#* Object/relational mapping 2008: hibernate and the entity data model (edm)
#@ Elizabeth J. O'Neil
#t 2008
#c 5
#% 287631
#% 933061
#% 960233
#% 960272
#% 960309
#! Object/Relational Mapping (ORM) provides a methodology and mechanism for object-oriented systems to hold their long-term data safely in a database, with transactional control over it, yet have it expressed when needed in program objects. Instead of bundles of special code for this, ORM encourages models and use of constraints for the application, which then runs in a context set up by the ORM. Today's web applications are particularly well-suited to this approach, as they are necessarily multithreaded and thus are prone to race conditions unless the interaction with the database is very carefully implemented. The ORM approach was first realized in Hibernate, an open source project for Java systems started in 2002, and this year is joined by Microsoft's Entity Data Model for .NET systems. Both are described here.

#index 1063595
#* Query answering techniques on uncertain and probabilistic data: tutorial summary
#@ Jian Pei;Ming Hua;Yufei Tao;Xuemin Lin
#t 2008
#c 5
#% 663
#% 32879
#% 190611
#% 209725
#% 442830
#% 480102
#% 599545
#% 654487
#% 785098
#% 824718
#% 824728
#% 824733
#% 864394
#% 864455
#% 893121
#% 893167
#% 893189
#% 907562
#% 976984
#% 977013
#% 977014
#% 983259
#% 1016201
#% 1016202
#% 1022203
#% 1022205
#% 1022206
#% 1022259
#% 1044478
#% 1063520
#% 1206645
#% 1206646
#% 1207234
#% 1720764
#! Uncertain data are inherent in some important applications, such as environmental surveillance, market analysis, and quantitative economics research. Due to the importance of those applications and the rapidly increasing amount of uncertain data collected and accumulated, analyzing large collections of uncertain data has become an important task and has attracted more and more interest from the database community. Recently, uncertain data management has become an emerging hot area in database research and development. In this tutorial, we systematically review some representative studies on answering various queries on uncertain and probabilistic data.

#index 1063596
#* Information fusion in wireless sensor networks
#@ Eduardo F. Nakamura;Antonio A. F. Loureiro
#t 2008
#c 5
#% 248183
#% 253755
#% 366513
#% 427022
#% 731098
#% 737946
#% 751026
#% 751038
#% 751057
#% 777786
#% 806214
#% 887949
#% 925317
#% 978154
#% 990805
#% 1428344
#% 1756360
#% 1757640
#! Wireless sensor networks (WSNs) are commonly treated as a distributed database system that is accessed by means of a query language. However, the computation of such queries are usually performed by information fusion techniques. Information fusion has been used by applications to detect/classify events, track targets, and filter noisy measurements. In this tutorial, we discuss some of the information fusion techniques that are currently used in WSNs.

#index 1063597
#* Introduction to recommender systems
#@ Joseph A. Konstan
#t 2008
#c 5
#! Recommender systems help users find the information, products, and other people they most want to find. This tutorial provides participants with a hands-on learning experience about using recommender system technologies. After completing this tutorial, participants will understand the range of technologies being used for recommender systems, including collaborative filtering, rules-based systems, and information filtering.

#index 1063598
#* Corrigendum to "efficient similarity search and classification via rank aggregation" by Ronald Fagin, Ravi Kumar and D. Sivakumar (proc. SIGMOD'03)
#@ Alexandr Andoni;Ronald Fagin;Ravi Kumar;Mihai Patrascu;D. Sivakumar
#t 2008
#c 5
#% 654466
#% 879397

#index 1063708
#* Proceedings of the twenty-seventh ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems
#@ Phokion Kolaitis;Maurizio Lenzerini
#t 2008
#c 5
#! This volume contains the proceedings of the Twenty-Seventh ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems (PODS 2008), held in Vancouver, Canada, on June 9-11, 2008, in conjunction with the 2008 ACM SIGMOD International Conference on Management of Data. The proceedings include one paper based on the keynote address by Peter Buneman, two papers based on the invited tutorials by Mikolaj Bojanczyk and Wenfei Fan, and 28 contributed papers that were selected by the Program Committee from 159 submissions. Most of these papers are preliminary reports on work in progress. While they have been read by program committee members, they have not been formally refereed. Many of them will probably appear in more polished and detailed form in scientific journals. The program committee selected the paper "Estimating PageRank on Graph Streams" by Atish Das Sarma, Sreenivas Gollapudi and Rina Panigrahy for the PODS 2008 Best Paper Award, and the paper "Evaluating Rank Joins with Optimal Cost" by Karl Schnaitter and Neoklis Polyzotis for the PODS 2008 Best Newcomer Award. Warmest congratulations to the authors of these papers. I thank all authors who submitted papers to the symposium, and the members of the program committee for the enormous amount of work they have done. The program committee did not meet in person, but carried out extensive discussions during the electronic PC meeting.

#index 1063709
#* Curated databases
#@ Peter Buneman;James Cheney;Wang-Chiew Tan;Stijn Vansummeren
#t 2008
#c 5
#% 663
#% 56081
#% 172925
#% 189868
#% 229827
#% 287268
#% 297283
#% 318704
#% 378401
#% 435477
#% 445449
#% 481935
#% 531444
#% 566114
#% 572328
#% 598747
#% 617859
#% 722731
#% 730038
#% 742561
#% 772031
#% 795390
#% 810114
#% 810115
#% 832825
#% 858455
#% 864394
#% 864469
#% 875015
#% 879809
#% 885368
#% 893095
#% 893098
#% 907551
#% 929519
#% 976984
#% 976987
#% 1015307
#% 1022285
#% 1024194
#% 1035310
#% 1055754
#% 1063581
#% 1063725
#% 1063735
#% 1063736
#% 1408533
#% 1408534
#% 1408540
#% 1661440
#% 1661444
#% 1728174
#! Curated databases are databases that are populated and updated with a great deal of human effort. Most reference works that one traditionally found on the reference shelves of libraries -- dictionaries, encyclopedias, gazetteers etc. -- are now curated databases. Since it is now easy to publish databases on the web, there has been an explosion in the number of new curated databases used in scientific research. The value of curated databases lies in the organization and the quality of the data they contain. Like the paper reference works they have replaced, they usually represent the efforts of a dedicated group of people to produce a definitive description of some subject area. Curated databases present a number of challenges for database research. The topics of annotation, provenance, and citation are central, because curated databases are heavily cross-referenced with, and include data from, other databases, and much of the work of a curator is annotating existing data. Evolution of structure is important because these databases often evolve from semistructured representations, and because they have to accommodate new scientific discoveries. Much of the work in these areas is in its infancy, but it is beginning to provide suggest new research for both theory and practice. We discuss some of this research and emphasize the need to find appropriate models of the processes associated with curated databases.

#index 1063710
#* The recovery of a schema mapping: bringing exchanged data back
#@ Marcelo Arenas;Jorge Pérez;Cristian Riveros
#t 2008
#c 5
#% 289384
#% 465053
#% 765540
#% 801691
#% 810021
#% 826032
#% 850730
#% 874879
#% 874880
#% 874882
#% 960233
#% 976996
#% 976997
#% 997492
#! A schema mapping is a specification that describes how data from a source schema is to be mapped to a target schema. Once the data has been transferred from the source to the target, a natural question is whether one can undo the process and recover the initial data, or at least part of it. In fact, it would be desirable to find a reverse schema mapping from target to source that specifies how to bring the exchanged data back. In this paper, we introduce the notion of a recovery of a schema mapping: it is a reverse mapping M' for a mapping M that recovers sound data with respect to M. We further introduce an order relation on recoveries. This allows us to choose mappings that recover the maximum amount of sound information. We call such mappings maximum recoveries. We study maximum recoveries in detail, providing a necessary and sufficient condition for their existence. In particular, we prove that maximum recoveries exist for the class of mappings specified by FO-to-CQ source-to-target dependencies. This class subsumes the class of source-to-target tuple-generating dependencies used in previous work on data exchange. For the class of mappings specified by FO-to-CQ dependencies, we provide an exponential-time algorithm for computing maximum recoveries, and a simplified version for full dependencies that works in quadratic time. We also characterize the language needed to express maximum recoveries, and we include a detailed comparison with the notion of inverse (and quasi-inverse) mapping previously proposed in the data exchange literature. In particular, we show that maximum recoveries strictly generalize inverses. We study the complexity of some decision problems related to the notions of recovery and maximum recovery. Finally, we report our initial results about a relaxed notion of maximal recovery, showing that it strictly generalizes the notion of maximum recovery.

#index 1063711
#* On the complexity of deriving schema mappings from database instances
#@ Pierre Senellart;Georg Gottlob
#t 2008
#c 5
#% 451
#% 234979
#% 273683
#% 273687
#% 289282
#% 378409
#% 396021
#% 408396
#% 465057
#% 480824
#% 550415
#% 572314
#% 598679
#% 801676
#% 809239
#% 976996
#% 993437
#% 1045828
#% 1702472
#! We introduce a theoretical framework for discovering relationships between two database instances over distinct and unknown schemata. This framework is grounded in the context of data exchange. We formalize the problem of understanding the relationship between two instances as that of obtaining a schema mapping so that a minimum repair of this mapping provides a perfect description of the target instance given the source instance. We show that this definition yields "intuitive" results when applied on database instances derived from each other by basic operations. We study the complexity of decision problems related to this optimality notion in the context of different logical languages and show that, even in very restricted cases, the problem is of high complexity.

#index 1063712
#* Towards a theory of schema-mapping optimization
#@ Ronald Fagin;Phokion G. Kolaitis;Alan Nash;Lucian Popa
#t 2008
#c 5
#% 140410
#% 156703
#% 289266
#% 378409
#% 384978
#% 465053
#% 749088
#% 765540
#% 806215
#% 809235
#% 809239
#% 809247
#% 810078
#% 826032
#% 850730
#% 874880
#% 874881
#% 927032
#% 960233
#% 960272
#% 976996
#% 993981
#% 1015302
#% 1063724
#! A schema mapping is a high-level specification that describes the relationship between two database schemas. As schema mappings constitute the essential building blocks of data exchange and data integration, an extensive investigation of the foundations of schema mappings has been carried out in recent years. Even though several different aspects of schema mappings have been explored in considerable depth, the study of schema-mapping optimization remains largely uncharted territory to date. In this paper, we lay the foundation for the development of a theory of schema-mapping optimization. Since schema mappings are constructs that live at the logical level of information integration systems, the first step is to introduce concepts and to develop techniques for transforming schema mappings to "equivalent" ones that are more manageable from the standpoint of data exchange or of some other data interoperability task. In turn, this has to start by introducing and studying suitable notions of "equivalence" between schema mappings. To this effect, we introduce the concept of data-exchange equivalence and the concept of conjunctive-query equivalence. These two concepts of equivalence are natural relaxations of the classical notion of logical equivalence; the first captures indistinguishability for data-exchange purposes, while the second captures indistinguishability for conjunctive-query-answering purposes. Moreover, they coincide with logical equivalence on schema mappings specified by source-to-target tuple-generating dependencies (s-t tgds), but differ on richer classes of dependencies, such as second-order tuple-generating dependencies (SO tgds) and sets of s-t tgds and target tuple-generating dependencies (target tgds). After exploring the basic properties of these three notions of equivalence between schema mappings, we focus on the following question: under what conditions is a schema mapping conjunctive-query equivalent to a schema mapping specified by a finite set of s-t tgds? We answer this question by obtaining complete characterizations for schema mappings that are specified by an SO tgd and for schema mappings that are specified by a finite set of s-t tgds and target tgds, and have terminating chase. These characterizations involve boundedness properties of the cores of universal solutions.

#index 1063713
#* Evaluating rank joins with optimal cost
#@ Karl Schnaitter;Neoklis Polyzotis
#t 2008
#c 5
#% 278831
#% 480819
#% 643566
#% 777931
#% 810018
#% 912239
#% 983263
#% 1022277
#! In the rank join problem, we are given a set of relations and a scoring function, and the goal is to return the join results with the top K scores. It is often the case in practice that the inputs may be accessed in ranked order and the scoring function is monotonic. These conditions allow for efficient algorithms that solve the rank join problem without reading all of the input. In this paper, we present a thorough analysis of such rank join algorithms. A strong point of our analysis is that it is based on a more general problem statement than previous work, making it more relevant to the execution model that is employed by database systems. One of our results indicates that the well known HRJN algorithm has shortcomings, because it does not stop reading its input as soon as possible. We find that it is NP-hard to overcome this weakness in the general case, but cases of limited query complexity are tractable. We prove the latter with an algorithm that infers provably tight bounds on the potential benefit of reading more input in order to stop as soon as possible. As a result, the algorithm achieves a cost that is within a constant factor of optimal.

#index 1063714
#* Effective characterizations of tree logics
#@ Mikolaj Bojanńzyk
#t 2008
#c 5
#% 1195
#% 24752
#% 201904
#% 212207
#% 241166
#% 335867
#% 473117
#% 505560
#% 505765
#% 548447
#% 821610
#% 836138
#% 850728
#% 892715
#% 944331
#% 982401
#% 1055098
#% 1393502
#% 1707172
#% 1914828
#! A survey of effective characterizations of tree logics. If L is a logic, then an effective characterization for L is an algorithm, which inputs a tree automaton and replies if the recognized language can be defined by a formula in L. The logics L considered include path testable languages, frontier testable languages, fragments of Core XPath, and fragments of monadic second-order logic.

#index 1063716
#* Estimating PageRank on graph streams
#@ Atish Das Sarma;Sreenivas Gollapudi;Rina Panigrahy
#t 2008
#c 5
#% 66649
#% 228716
#% 232709
#% 268079
#% 273907
#% 278835
#% 293720
#% 333931
#% 379443
#% 656750
#% 749451
#% 765291
#% 805897
#% 809258
#% 813786
#% 847112
#% 847113
#% 847115
#% 869492
#% 874902
#% 874904
#% 898311
#% 985896
#% 1404193
#% 1682599
#% 1699388
#% 1916580
#! This study focuses on computations on large graphs (e.g., the web-graph) where the edges of the graph are presented as a stream. The objective in the streaming model is to use small amount of memory (preferably sub-linear in the number of nodes n) and a few passes. In the streaming model, we show how to perform several graph computations including estimating the probability distribution after a random walk of length l, mixing time, and the conductance. We estimate the mixing time M of a random walk in Õ(nα+Mα√n+√Mn/ α) space and Õ(√Mα) passes. Furthermore, the relation between mixing time and conductance gives us an estimate for the conductance of the graph. By applying our algorithm for computing probability distribution on the web-graph, we can estimate the PageRank p of any node up to an additive error of √εp in Õ(√M/α) passes and Õ(min(nα + 1/ε √M/α + 1/ε Mα, αn√Mα + 1/ε √M/α)) space, for any α ∈ (0, 1]. In particular, for ε = M/n, by setting α = M--1/2, we can compute the approximate PageRank values in Õ(nM--1/4) space and Õ(M3/4) passes. In comparison, a standard implementation of the PageRank algorithm will take O(n) space and O(M) passes.

#index 1063717
#* A generic flow algorithm for shared filter ordering problems
#@ Zhen Liu;Srinivasan Parthasarathy;Anand Ranganathan;Hao Yang
#t 2008
#c 5
#% 554
#% 109653
#% 152940
#% 201936
#% 257871
#% 300167
#% 300169
#% 341672
#% 453556
#% 479938
#% 593887
#% 656701
#% 656764
#% 765435
#% 781735
#% 800505
#% 805762
#% 874896
#% 977005
#% 1063479
#% 1700123
#! We consider a fundamental flow maximization problem that arises during the evaluation of multiple overlapping queries defined on a data stream, in a heterogenous parallel environment. Each query is a conjunction of boolean filters, and each filter could be shared across multiple queries. We are required to design an evaluation plan that evaluates filters against stream items in order to determine the set of queries satisfied by each item. The evaluation plan specifies for each item: (i) the subset of filters evaluated for this item and the order of their evaluations, and (ii) the processor on which each filter evaluation occurs. Our goal is to design an evaluation plan which maximizes the total throughput (flow) of the stream handled by the plan, without violating the processor capacities. Filter ordering has received extensive attention in single-processor settings, with the objective of minimizing the total cost of filter evaluations: in particular, efficient (approximation) algorithms are known for various important versions of min-cost filter ordering. Min-cost filter ordering problem for a single processor is a special case of our flow maximization for parallel processors. Our main contribution in this work is a generic flow-maximization algorithm, which assumes the availability of a min-cost filter ordering algorithm for a single processor, and uses this to iteratively construct a solution to the flow-maximization problem for heterogenous parallel processors. We show that the approximation ratio of our flow-maximization strategy is essentially the same as that of the underlying min-cost filter ordering algorithm. Our result, along with existing results on min-cost filter ordering, enables the optimization of several important versions of filter ordering in parallel environments.

#index 1063718
#* Time-decaying aggregates in out-of-order streams
#@ Graham Cormode;Flip Korn;Srikanta Tirthapura
#t 2008
#c 5
#% 278835
#% 378388
#% 379445
#% 453512
#% 576112
#% 576113
#% 578391
#% 654508
#% 765404
#% 783740
#% 800582
#% 801696
#% 809258
#% 816392
#% 853012
#% 874903
#% 874906
#% 874974
#% 878246
#% 1029101
#% 1206633
#% 1392293
#% 1673279
#% 1713039
#! Processing large data streams is now a major topic in data management. The data involved can be truly massive, and the required analyses complex. In a stream of sequential events such as stock feeds, sensor readings, or IP traffic measurements, data tuples pertaining to recent events are typically more important than older ones. This can be formalized via time-decay functions, which assign weights to data based on the age of data. Decay functions such as sliding windows and exponential decay have been studied under the assumption of well-ordered arrivals, i.e., data arrives in non-decreasing order of time stamps. However, data quality issues are prevalent in massive streams (due to network asynchrony and delays etc.), and correct arrival order is not guaranteed. We focus on the computation of decayed aggregates such as range queries, quantiles, and heavy hitters on out-of-order streams, where elements do not necessarily arrive in increasing order of timestamps. Existing techniques such as Exponential Histograms and Waves are unable to handle out-of-order streams. We give the first deterministic algorithms for approximating these aggregates under popular decay functions such as sliding window and polynomial decay. We study the overhead of allowing out-of-order arrivals when compared to well-ordered arrivals, both analytically and experimentally. Our experiments confirm that these algorithms can be applied in practice, and compare the relative performance of different approaches for handling out-of-order arrivals.

#index 1063719
#* Approximating predicates and expressive queries on probabilistic databases
#@ Christoph Koch
#t 2008
#c 5
#% 191616
#% 227883
#% 265692
#% 598376
#% 729449
#% 810098
#% 893167
#% 960293
#% 960294
#% 977013
#% 1016201
#% 1022341
#% 1068580
#% 1206717
#% 1661439
#! We study complexity and approximation of queries in an expressive query language for probabilistic databases. The language studied supports the compositional use of confidence computation. It allows for a wide range of new use cases, such as the computation of conditional probabilities and of selections based on predicates that involve marginal and conditional probabilities. These features have important applications in areas such as data cleaning and the processing of sensor data. We establish techniques for efficiently computing approximate query results and for estimating the error incurred by queries. The central difficulty is due to selection predicates based on approximated values, which may lead to the unreliable selection of tuples. A database may contain certain singularities at which approximation of predicates cannot be achieved; however, the paper presents an algorithm that provides efficient approximation otherwise.

#index 1063720
#* Incorporating constraints in probabilistic XML
#@ Sara Cohen;Benny Kimelfeld;Yehoshua Sagiv
#t 2008
#c 5
#% 68244
#% 115964
#% 116987
#% 136358
#% 397375
#% 398752
#% 464589
#% 465044
#% 630971
#% 960851
#% 977012
#% 977013
#% 977014
#% 993985
#% 1016201
#% 1022204
#% 1063522
#% 1408537
#% 1688305
#! Constraints are important not just for maintaining data integrity, but also because they capture natural probabilistic dependencies among data items. A probabilistic XML database (PXDB) is the probability sub-space comprising the instances of a p-document that satisfy a set of constraints. In contrast to existing models that can express probabilistic dependencies, it is shown that query evaluation is tractable in PXDBs. The problems of sampling and determining well-definedness (i.e., whether the above subspace is nonempty) are also tractable. Furthermore, queries and constraints can include the aggregate functions count, max, min and ratio. Finally, this approach can be easily extended to allow a probabilistic interpretation of constraints.

#index 1063721
#* Query evaluation with soft-key constraints
#@ Abhay Jha;Vibhor Rastogi;Dan Suciu
#t 2008
#c 5
#% 44876
#% 144070
#% 215225
#% 388024
#% 442830
#% 442934
#% 480102
#% 765455
#% 850430
#% 864417
#% 893167
#% 915340
#% 976984
#% 977013
#% 983845
#% 1016201
#% 1269495
#% 1269815
#% 1408537
#% 1728682
#! Key Violations often occur in real-life datasets, especially in those integrated from different sources. Enforcing constraints strictly on these datasets is not feasible. In this paper we formalize the notion of soft-key constraints on probabilistic databases, which allow for violation of key constraint by penalizing every violating world by a quantity proportional to the violation. To represent our probabilistic database with constraints, we define a class of markov networks, where we can do query evaluation in PTIME. We also study the evaluation of conjunctive queries on relations with soft keys and present a dichotomy that separates this set into those in PTIME and the rest which are #P-Hard.

#index 1063722
#* Answering aggregate queries in data exchange
#@ Foto Afrati;Phokion G. Kolaitis
#t 2008
#c 5
#% 582130
#% 765540
#% 806215
#% 809235
#% 809247
#% 810020
#% 810078
#% 823106
#% 826032
#% 850730
#% 874880
#% 874881
#% 874882
#% 912245
#% 927032
#% 976995
#% 976996
#% 1044476
#! Data exchange, also known as data translation, has been extensively investigated in recent years. One main direction of research has focused on the semantics and the complexity of answering first-order queries in the context of data exchange between relational schemas. In this paper, we initiate a systematic investigation of the semantics and the complexity of aggregate queries in data exchange, and make a number of conceptual and technical contributions. Data exchange is a context in which incomplete information arises, hence one has to cope with a set of possible worlds, instead of a single database. Three different sets of possible worlds have been explored in the study of the certain answers of first-order queries in data exchange: the set of possible worlds of all solutions, the set of possible worlds of all universal solutions, and a set of possible worlds derived from the CWA-solutions. We examine each of these sets and point out that none of them is suitable for aggregation in data exchange, as each gives rise to rather trivial semantics. Our analysis also reveals that, to have meaningful semantics for aggregation in data exchange, a strict closed world assumption has to be adopted in selecting the set of possible worlds. For this, we introduce and study the set of the endomorphic images of the canonical universal solution as a set of possible worlds for aggregation in data exchange. Our main technical result is that for schema mappings specified by source-to-target tgds, there are polynomial-time algorithms for computing the range semantics of every scalar aggregation query, where the range semantics of an aggregate query is the greatest lower bound and the least upper bound of the values that the query takes over the set of possible worlds. Among these algorithms, the more sophisticated one is the algorithm for the average operator, which makes use of concepts originally introduced in the study of the core of the universal solutions in data exchange. We also show that if, instead of range semantics, we consider possible answer semantics, then it is an NP-complete problem to tell if a number is a possible answer of a given scalar aggregation query with the average operator.

#index 1063723
#* Data exchange and schema mappings in open and closed worlds
#@ Leonid Libkin;Cristina Sirangelo
#t 2008
#c 5
#% 663
#% 7348
#% 94459
#% 248038
#% 267604
#% 287313
#% 332166
#% 366807
#% 378409
#% 465053
#% 480249
#% 801691
#% 806215
#% 809239
#% 823106
#% 826032
#% 850730
#% 874882
#% 893093
#% 893095
#% 927032
#% 960233
#% 976995
#% 976997
#% 993981
#% 1015302
#! In the study of data exchange one usually assumes an open-world semantics, making it possible to extend instances of target schemas. An alternative closed-world semantics only moves 'as much data as needed' from the source to the target to satisfy constraints of a schema mapping. It avoids some of the problems exhibited by the open-world semantics, but limits the expressivity of schema mappings. Here we propose a mixed approach: one can designate different attributes of target schemas as open or closed, to combine the additional expressivity of the open-world semantics with the better behavior of query answering in closed worlds. We define such schema mappings, and show that they cover a large space of data exchange solutions with two extremes being the known open and closed-world semantics. We investigate the problems of query answering and schema mapping composition, and prove two trichotomy theorems, classifying their complexity based on the number of open attributes. We find conditions under which schema mappings compose, extending known results to a wide range of closed-world mappings. We also provide results for restricted classes of queries and mappings guaranteeing lower complexity.

#index 1063724
#* The chase revisited
#@ Alin Deutsch;Alan Nash;Jeff Remmel
#t 2008
#c 5
#% 583
#% 129217
#% 248038
#% 287316
#% 287339
#% 289321
#% 289384
#% 378409
#% 384978
#% 465053
#% 599549
#% 749088
#% 765432
#% 806215
#% 821606
#% 826032
#% 874879
#% 874880
#% 912245
#% 1015271
#% 1700141
#! We revisit the standard chase procedure, studying its properties and applicability to classical database problems. We settle (in the negative) the open problem of decidability of termination of the standard chase, and we provide sufficient termination conditions which are strictly less over-conservative than the best previously known. We investigate the adequacy of the standard chase for checking query containment under constraints, constraint implication and computing certain answers in data exchange, gaining a deeper understanding by separating the algorithm from its result. We identify the properties of the chase result that are essential to the above applications, and we introduce the more general notion of F-universal model set, which supports query and constraint languages that are closed under a class F of mappings. By choosing F appropriately, we extend prior results to existential first-order queries and ∀∃-firstorder constraints. We show that the standard chase is incomplete for finding universal model sets, and we introduce the extended core chase which is complete, i.e. finds an F-universal model set when it exists. A key advantage of the new chase is that the same algorithm can be applied for all mapping classes F of interest, simply by modifying the set of constraints given as input. Even when restricted to the typical input in prior work, the new chase supports certain answer computation and containment/implication tests in strictly more cases than the incomplete standard chase.

#index 1063725
#* Dependencies revisited for improving data quality
#@ Wenfei Fan
#t 2008
#c 5
#% 663
#% 838
#% 36244
#% 101956
#% 213972
#% 224743
#% 242237
#% 264858
#% 273687
#% 286998
#% 287793
#% 296539
#% 366807
#% 378409
#% 384978
#% 398752
#% 420072
#% 491358
#% 491695
#% 519568
#% 576116
#% 582130
#% 727668
#% 752741
#% 778320
#% 809239
#% 810019
#% 810020
#% 814475
#% 833132
#% 864417
#% 866986
#% 874881
#% 879041
#% 893114
#% 903332
#% 913783
#% 949372
#% 960293
#% 976984
#% 1022222
#% 1022228
#% 1025173
#% 1054480
#% 1063709
#% 1086124
#% 1206764
#% 1661426
#% 1661438
#% 1673673
#% 1683883
#% 1705008
#% 1705010
#! Dependency theory is almost as old as relational databases themselves, and has traditionally been used to improve the quality of schema, among other things. Recently there has been renewed interest in dependencies for improving the quality of data. The increasing demand for data quality technology has also motivated revisions of classical dependencies, to capture more inconsistencies in real-life data, and to match, repair and query the inconsistent data. This paper aims to provide an overview of recent advances in revising classical dependencies for improving data quality.

#index 1063726
#* Epistemic privacy
#@ Alexandre Evfimievski;Ronald Fagin;David P. Woodruff
#t 2008
#c 5
#% 114502
#% 188086
#% 212290
#% 576110
#% 576111
#% 765449
#% 809244
#% 809245
#% 893101
#% 993943
#% 1016172
#% 1206679
#! We present a novel definition of privacy in the framework of offline (retroactive) database query auditing. Given information about the database, a description of sensitive data, and assumptions about users' prior knowledge, our goal is to determine if answering a past user's query could have led to a privacy breach. According to our definition, an audited property A is private, given the disclosure of property B, if no user can gain confidence in A by learning B, subject to prior knowledge constraints. Privacy is not violated if the disclosure of B causes a loss of confidence in A. The new notion of privacy is formalized using the well-known semantics for reasoning about knowledge, where logical properties correspond to sets of possible worlds (databases) that satisfy these properties. Database users are modelled as either possibilistic agents whose knowledge is a set of possible worlds, or as probabilistic agents whose knowledge is a probability distribution on possible worlds. We analyze the new privacy notion, show its relationship with the conventional approach, and derive criteria that allow the auditor to test privacy efficiently in some important cases. In particular, we prove characterization theorems for the possibilistic case, and study in depth the probabilistic case under the assumption that all database records are considered a-priori independent by the user, as well as under more relaxed (or absent) prior-knowledge assumptions. In the probabilistic case we show that for certain families of distributions there is no efficient algorithm to test whether an audited property A is private given the disclosure of a property B, assuming P ` NP. Nevertheless, for many interesting families, such as the family of product distributions, we obtain algorithms that are efficient both in theory and in practice.

#index 1063727
#* On searching compressed string collections cache-obliviously
#@ Paolo Ferragina;Roberto Grossi;Ankur Gupta;Rahul Shah;Jeffrey Scott Vitter
#t 2008
#c 5
#% 101926
#% 252608
#% 271801
#% 282232
#% 287715
#% 290703
#% 379390
#% 593909
#% 836511
#% 846094
#% 847099
#% 869542
#% 874900
#% 910181
#% 936965
#% 956507
#% 987259
#% 991181
#% 1080906
#% 1404819
#% 1404885
#% 1916564
#! Current data structures for searching large string collections either fail to achieve minimum space or cause too many cache misses. In this paper we discuss some edge linearizations of the classic trie data structure that are simultaneously cache-friendly and compressed. We provide new insights on front coding [24], introduce other novel linearizations, and study how close their space occupancy is to the information-theoretic minimum. The moral is that they are not just heuristics. Our second contribution is a novel dictionary encoding scheme that builds upon such linearizations and achieves nearly optimal space, offers competitive I/O-search time, and is also conscious of the query distribution. Finally, we combine those data structures with cache-oblivious tries [2, 5] and obtain a succinct variant whose space is close to the information-theoretic minimum.

#index 1063728
#* Approximation algorithms for clustering uncertain data
#@ Graham Cormode;Andrew McGregor
#t 2008
#c 5
#% 160226
#% 210173
#% 248790
#% 263182
#% 325409
#% 347211
#% 568629
#% 743962
#% 765282
#% 765291
#% 785121
#% 893167
#% 915307
#% 959934
#% 960257
#% 977008
#% 991230
#% 992830
#% 1206640
#% 1614964
#% 1669893
#! There is an increasing quantity of data with uncertainty arising from applications such as sensor network measurements, record linkage, and as output of mining algorithms. This uncertainty is typically formalized as probability density functions over tuple values. Beyond storing and processing such data in a DBMS, it is necessary to perform other data analysis tasks such as data mining. We study the core mining problem of clustering on uncertain data, and define appropriate natural generalizations of standard clustering optimization criteria. Two variations arise, depending on whether a point is automatically associated with its optimal center, or whether it must be assigned to a fixed cluster no matter where it is actually located. For uncertain versions of k-means and k-median, we show reductions to their corresponding weighted versions on data with no uncertainties. These are simple in the unassigned case, but require some care for the assigned version. Our most interesting results are for uncertain k-center, which generalizes both traditional k-center and k-median objectives. We show a variety of bicriteria approximation algorithms. One picks O(kε--1log2n) centers and achieves a (1 + ε) approximation to the best uncertain k-centers. Another picks 2k centers and achieves a constant factor approximation. Collectively, these results are the first known guaranteed approximation algorithms for the problems of clustering uncertain data.

#index 1063729
#* Approximation algorithms for co-clustering
#@ Aris Anagnostopoulos;Anirban Dasgupta;Ravi Kumar
#t 2008
#c 5
#% 296738
#% 334092
#% 341672
#% 342621
#% 347211
#% 469422
#% 589430
#% 659967
#% 729437
#% 731279
#% 743962
#% 765545
#% 765548
#% 765549
#% 778215
#% 785121
#% 790636
#% 801681
#% 823328
#% 874912
#% 989573
#% 1014670
#! Co-clustering is the simultaneous partitioning of the rows and columns of a matrix such that the blocks induced by the row/column partitions are good clusters. Motivated by several applications in text mining, market-basket analysis, and bioinformatics, this problem has attracted severe attention in the past few years. Unfortunately, to date, most of the algorithmic work on this problem has been heuristic in nature. In this work we obtain the first approximation algorithms for the co-clustering problem. Our algorithms are simple and obtain constant-factor approximation solutions to the optimum. We also show that co-clustering is NP-hard, thereby complementing our algorithmic result.

#index 1063730
#* The power of two min-hashes for similarity search among hierarchical data objects
#@ Sreenivas Gollapudi;Rina Panigrahy
#t 2008
#c 5
#% 66654
#% 210212
#% 249321
#% 311808
#% 347225
#% 349489
#% 479973
#% 546123
#% 616528
#% 762054
#% 805746
#% 806218
#% 824676
#% 847166
#% 907534
#% 1206601
#! In this study we propose sketching algorithms for computing similarities between hierarchical data. Specifically, we look at data objects that are represented using leaf-labeled trees denoting a set of elements at the leaves organized in a hierarchy. Such representations are richer alternatives to a set. For example, a document can be represented as a hierarchy of sets wherein chapters, sections, and paragraphs represent different levels in the hierarchy. Such a representation is richer than viewing the document simply as a set of words. We measure distance between trees using the best possible super-imposition that minimizes the number of mismatched leaf labels. Our distance measure is equivalent to an Earth Mover's Distance measure since the leaf-labeled trees of height one can be viewed as sets and can be recursively extended to trees of larger height by viewing them as set of sets. We compute sketches of arbitrary weighted trees and analyze them in the context of locality-sensitive hashing (LSH) where the probability of two sketches matching is high when two trees are similar and low when the two trees are far under the given distance measure. Specifically, we compute sketches of such trees by propagating min-hash computations up the tree. Furthermore, we show that propagating one min-hash results in poor sketch properties while propagating two min-hashes results in good sketches.

#index 1063731
#* Static analysis of active XML systems
#@ Serge Abiteboul;Luc Segoufin;Victor Vianu
#t 2008
#c 5
#% 101955
#% 333855
#% 333857
#% 378411
#% 384978
#% 576091
#% 769518
#% 770373
#% 801671
#% 809236
#% 810053
#% 834987
#% 874885
#% 881742
#% 888014
#% 888015
#% 942360
#% 985981
#% 1072645

#index 1063732
#* Complexity and composition of synthesized web services
#@ Wenfei Fan;Floris Geerts;Wouter Gelade;Frank Neven;Antonella Poggi
#t 2008
#c 5
#% 11819
#% 36181
#% 198465
#% 237190
#% 321054
#% 328424
#% 384978
#% 408396
#% 591778
#% 630964
#% 754120
#% 786874
#% 799801
#% 824702
#% 824806
#% 826030
#% 831888
#% 874885
#% 940867
#% 942360
#% 1389438
#! The paper investigates fundamental decision problems and composition synthesis for Web services commonly found in practice. We propose a notion of synthesized Web services (ASTs) to specify the behaviors of the services. Upon receiving a sequence of input messages, an AST issues multiple queries to a database and generates actions, in parallel; it produces external messages and database updates by synthesizing the actions parallelly generated. In contrast to previous models for Web services, ASTs advocate parallel processing and (deterministic) synthesis of actions. We classify ASTs based on what queries an AST can issue, how the synthesis of actions is expressed, and whether unbounded input sequences are allowed in a single interaction session. We show that the behaviors of Web services supported by various prior models, data-driven or not, can be specified by different AST classes. For each of these classes we study the non-emptiness, validation and equivalence problems, and establish matching upper and lower bounds on these problems. We also provide complexity bounds on composition synthesis for these AST classes, identifying decidable cases.

#index 1063733
#* XPath evaluation in linear time
#@ Mikolaj Bojańczyk;Pawel Parys
#t 2008
#c 5
#% 69625
#% 427027
#% 814648
#% 874877
#% 1129529
#% 1914048
#% 1916596
#! We consider a fragment of XPath where attribute values can only be tested for equality. We show that for any fixed unary query in this fragment, the set of nodes that satisfy the query can be calculated in time linear in the document size.

#index 1063734
#* XPath, transitive closure logic, and nested tree walking automata
#@ Balder ten Cate;Luc Segoufin
#t 2008
#c 5
#% 349958
#% 473121
#% 544094
#% 591777
#% 809236
#% 824798
#% 826029
#% 850728
#% 874910
#% 976990
#% 976991
#% 982403
#% 993939
#% 1661435
#% 1673665
#% 1703199
#! We consider the navigational core of XPath, extended with two operators: the Kleene star for taking the transitive closure of path expressions, and a subtree relativisation operator, allowing one to restrict attention to a specific subtree while evaluating a subexpression. We show that the expressive power of this XPath dialect equals that of FO(MTC), first order logic extended with monadic transitive closure. We also give a characterization in terms of nested tree-walking automata. Using the latter we then proceed to show that the language is strictly less expressive than MSO. This solves an open question about the relative expressive power of FO(MTC) and MSO on trees. We also investigate the complexity for our XPath dialect. We show that query evaluation be done in polynomial time (combined complexity), but that satisfiability and query containment (as well as emptiness for our automaton model) are 2ExpTime-complete (it is ExpTime-complete for Core XPath).

#index 1063735
#* Local Hoare reasoning about DOM
#@ Philippa A. Gardner;Gareth D. Smith;Mark J. Wheelhouse;Uri D. Zarfaty
#t 2008
#c 5
#% 333978
#% 912491
#% 951593
#% 1063735
#% 1345073
#% 1397978
#% 1664096
#! The W3C Document Object Model (DOM) specifies an XML update library. DOM is written in English, and is therefore not compositional and not complete. We provide a first step towards a compositional specification of DOM. Unlike DOM, we are able to work with a minimal set of commands and obtain a complete reasoning for straight-line code. Our work transfers O'Hearn, Reynolds and Yang's local Hoare reasoning for analysing heaps to XML, viewing XML as an in-place memory store as does DOM. In particular, we apply recent work by Calcagno, Gardner and Zarfaty on local Hoare reasoning about simple tree update to this real-world DOM application. Our reasoning not only formally specifies a significant subset of DOM Core Level 1, but can also be used to verify, for example, invariant properties of simple Javascript programs.

#index 1063736
#* Annotated XML: queries and provenance
#@ J. Nathan Foster;Todd J. Green;Val Tannen
#t 2008
#c 5
#% 663
#% 189868
#% 245655
#% 273703
#% 384978
#% 479956
#% 563512
#% 571040
#% 800547
#% 864401
#% 866986
#% 976987
#% 977012
#% 986483
#% 993985
#% 1022258
#% 1063709
#% 1396742
#% 1661440
#% 1661447
#% 1661449
#% 1688305
#% 1700124
#! We present a formal framework for capturing the provenance of data appearing in XQuery views of XML. Building on previous work on relations and their (positive) query languages, we decorate unordered XML with annotations from commutative semirings and show that these annotations suffice for a large positive fragment of XQuery applied to this data. In addition to tracking provenance metadata, the framework can be used to represent and process XML with repetitions, incomplete XML, and probabilistic XML, and provides a basis for enforcing access control policies in security applications. Each of these applications builds on our semantics for XQuery, which we present in several steps: we generalize the semantics of the Nested Relational Calculus (NRC) to handle semiring-annotated complex values, we extend it with a recursive type and structural recursion operator for trees, and we define a semantics for XQuery on annotated XML by translation into this calculus.

#index 1063737
#* Near-optimal dynamic replication in unstructured peer-to-peer networks
#@ Mauro Sozio;Thomas Neumann;Gerhard Weikum
#t 2008
#c 5
#% 4652
#% 176535
#% 187079
#% 297675
#% 325412
#% 349973
#% 379373
#% 446428
#% 749562
#% 803470
#% 816489
#% 850621
#% 857107
#% 873998
#% 963556
#% 963587
#% 963588
#% 963605
#% 978765
#% 1112006
#% 1293166
#% 1703495
#! Replicating data in distributed systems is often needed for availability and performance. In unstructured peer-to-peer networks, with epidemic messaging for query routing, replicating popular data items is also crucial to ensure high probability of finding the data within a bounded search distance from the requestor. This paper considers such networks and aims to maximize the probability of successful search. Prior work along these lines has analyzed the optimal degrees of replication for data items with non-uniform but global request rates, but did not address the issue of where replicas should be placed and was very very limited in the capabilities for handling heterogeneity and dynamics of network and workload. This paper presents the integrated P2R2 algorithm for dynamic replication that addresses all these issues, and determines both the degrees of replication and the placement of the replicas in a provably near-optimal way. We prove that the P2R2 algorithm can guarantee a successful-search probability that is within a factor of 2 of the optimal solution. The algorithm is efficient and can handle workload evolution. We prove that, whenever the access patterns are in steady state, our algorithm converges to the desired near-optimal placement. We further show by simulations that the convergence rate is fast and that our algorithm outperforms prior methods.

#index 1063738
#* Type inference for datalog and its application to query optimisation
#@ Oege de Moor;Damien Sereni;Pavel Avgustinov;Mathieu Verbaere
#t 2008
#c 5
#% 11797
#% 54225
#% 58346
#% 64414
#% 85199
#% 123085
#% 137701
#% 137873
#% 140410
#% 152907
#% 159876
#% 164369
#% 237181
#% 237257
#% 315198
#% 345695
#% 346654
#% 349117
#% 384978
#% 442833
#% 459265
#% 465043
#% 476446
#% 532101
#% 562154
#% 570702
#% 590310
#% 801697
#% 963226
#% 1009032
#% 1655198
#% 1721027
#% 1721223
#! Certain variants of object-oriented Datalog can be compiled to Datalog with negation. We seek to apply optimisations akin to virtual method resolution (a well-known technique in compiling Java and other OO languages) to improve efficiency of the resulting Datalog programs. The effectiveness of such optimisations strongly depends on the precision of the underlying type inference algorithm. Previous work on type inference for Datalog has focussed on Cartesian abstractions, where the type of each field is computed separately. Such Cartesian type inference is inherently imprecise in the presence of field equalities. We propose a type system where equalities are tracked, and present a type inference algorithm. The algorithm is proved sound. We also prove that it is optimal for Datalog without negation, in the sense that the inferred type is as tight as possible. Extensive experiments with our type-based optimisations, in a commercial implementation of object-oriented Datalog, confirm the benefits of this non-Cartesian type inference algorithm.

#index 1063739
#* Shape sensitive geometric monitoring
#@ Izchak Sharfman;Assaf Schuster;Daniel Keren
#t 2008
#c 5
#% 214073
#% 378388
#% 379445
#% 397353
#% 465754
#% 492912
#% 632090
#% 654443
#% 763708
#% 800582
#% 801696
#% 808428
#% 810009
#% 824652
#% 864444
#% 872778
#% 874994
#% 874995
#% 891929
#% 991154
#% 993949
#% 993960
#% 993961
#% 1016155
#! A fundamental problem in distributed computation is the distributed evaluation of functions. The goal is to determine the value of a function over a set of distributed inputs, in a communication efficient manner. Specifically, we assume that each node holds a time varying input vector, and we are interested in determining, at any given time, whether the value of an arbitrary function on the average of these vectors crosses a predetermined threshold. In this paper, we introduce a new method for monitoring distributed data, which we term shape sensitive geometric monitoring. It is based on a geometric interpretation of the problem, which enables to define local constraints on the data received at the nodes. It is guaranteed that as long as none of these constraints has been violated, the value of the function does not cross the threshold. We generalize previous work on geometric monitoring, and solve two problems which seriously hampered its performance: as opposed to the constraints used so far, which depend only on the current values of the local input vectors, here we incorporate their temporal behavior into the constraints. Also, the new constraints are tailored to the geometric properties of the specific function which is being monitored, while the previous constraints were generic. Experimental results on real world data reveal that using the new geometric constraints reduces communication by up to three orders of magnitude in comparison to existing approaches, and considerably narrows the gap between existing results and a newly defined lower bound on the communication complexity.

#index 1063740
#* Tree-width and functional dependencies in databases
#@ Isolde Adler
#t 2008
#c 5
#% 268708
#% 303886
#% 535150
#% 599549
#% 847068
#% 976985
#% 993437
#% 1000772
#% 1289362
#! Conjunctive query (CQ) evaluation on relational databases is NP-complete in general. Several restrictions, like bounded tree-width and bounded hypertree-width, allow polynomial time evaluations.We extend the framework in the presence of functional dependencies. Our exteAnded CQ evaluation problem has a concise equivalent formulation in terms of the homomorphism problem (HOM) for non-relational structures. We introduce the notions of "closure tree-width" and "hyperclosure tree-width" for arbitrary structures, and we prove that HOM (and hence CQ) restricted to bounded (hyper)closure tree-width becomes tractable. There are classes of structures with bounded closure tree-width but unbounded tree-width. Similar statements hold for hyperclosure tree-width and hypertree-width, and for hyperclosure tree-width and closure tree-width. It follows from a result by Gottlob, Miklós, and Schwentick that for fixed k ≥ 2, deciding whether a given structure has hyperclosure tree-width at most k, is NP-complete. We prove an analogous statement for closure tree-width. Nevertheless, for given k we can approximate k-bounded closure tree-width in polynomial time.

#index 1065591
#* 2007 Test-of-time Award “Online Aggregation”
#@ Joseph M. Hellerstein;Peter J. Haas;Helen J. Wang
#t 2007
#c 5

#index 1065592
#* 2007 Dissertation Award “Declarative Networking”
#@ Joseph M. Hellerstein;Ion Stoica;Boon Thau Loo
#t 2007
#c 5

#index 1065593
#* 2007 SIGMOD Edger F. Codd Innovations Award: “Research Principles Revealed”
#@ Jennifer Widom
#t 2007
#c 5

#index 1065636
#* ACM SIGMOD Record - Tribute to honor Jim Gray
#@ 
#t 2008
#c 5

#index 1065637
#* My biggest fan
#@ Marianne Winslett
#t 2008
#c 5

#index 1065638
#* Jim Gray speaks out: on chasing the object-relational rainbow, why performance is a nonissue, bad ideas that went good, reinventing the field, sailboats, lunatic fringe papers, whether to try for a home run, and more
#@ Marianne Winslett
#t 2008
#c 5

#index 1065639
#* Ode to a sailor
#@ Donna Carnes
#t 2008
#c 5

#index 1065640
#* A tribute, not a memorial: understanding ambiguous loss
#@ Pauline Boss
#t 2008
#c 5
#% 805077
#! In this paper, I discuss ambiguous loss, why it is so traumatizing, what to do to lower the distress when someone disappears without a trace, and why a tribute is more appropriate than a memorial. The paper is dedicated to the family, friends, and colleagues of Jim Gray.

#index 1065641
#* The amateur search
#@ Michael Olson
#t 2008
#c 5
#! In the days that followed Jim's disappearance at sea, his family, friends and colleagues came together in a remarkable effort to help find him. The group gathered, analyzed and filtered information from many sources. In concert with the Coast Guard, the results were used to focus the search in the places where Jim and his sailboat Tenacious were most likely to be. The scope and breadth -- indeed, the very existence -- of this amateur search effort reflect Jim at his best. The group was at first composed of those whose lives he had touched directly, who used skills and knowledge he had helped them to gain in hopes of finding him. As time passed, others joined in who were inspired by the unfolding story. In the end, thousands came together to search for Jim.

#index 1065642
#* Thanks to the US Coast Guard
#@ Paula Hawthorn
#t 2008
#c 5

#index 1065643
#* Jim Gray at Berkeley
#@ Michael A. Harrison
#t 2008
#c 5
#% 288811
#% 288832
#% 1037721
#! Jim Gray spent a decade as a student and researcher at Berkeley. In action, he is remembered for his breadth, his depth, and his generosity.

#index 1065644
#* Knowledge and wisdom
#@ Pat Helland
#t 2008
#c 5
#% 210179
#% 403195
#% 531907
#! Jim Gray was my mentor and friend for almost 25 years. Even before we met, he was changing my life through his writings. Jim continually and patiently nudged me to strive for more, accomplish more, and to pass these gifts on to others. He embodied those attributes that I most want to emulate. Jim influenced those around him both by sharing in his immense knowledge and by listening and guiding. Even more, he created a community that fostered growth and sharing across competitive boundaries with a clear ethical compass. This paper summarizes my experiences with Jim's approach to nurturing and mentoring.

#index 1065645
#* 500 special relationships: Jim as a mentor to faculty and students
#@ Ed Lazowska
#t 2008
#c 5
#% 14204
#% 451553
#! There are hundreds of us in academia and industry who consider ourselves FoJ: Friends of Jim. Jim invested so deeply and so uniquely in every one of us that we each felt we must be "the chosen one." And we were: Jim had so much to give that he was able to choose hundreds of us, shaping our careers and our lives.

#index 1065646
#* Why did Jim Gray win the Turing Award?
#@ Michael Stonebraker
#t 2008
#c 5
#! This short paper is intended to describe for the layman why Jim Gray won so many awards, culminating in his being selected to receive the 1998 ACM Turing Award, arguably the "Nobel Prize of Computer Science". It briefly summarizes his main contributions to our field.

#index 1065647
#* Jim Gray: his contribution to industry
#@ David Vaskevitch
#t 2008
#c 5
#! Jim Gray was successful in many aspects of his life. Not only did he have a profound influence on the computer industry, but also on all of the people that he interacted with along the way. It is this balance contribution to society that makes Jim truly unique.

#index 1065648
#* A "Gap Bridger"
#@ Richard Rashid
#t 2008
#c 5
#% 300171
#% 300185
#% 845351
#! I knew Jim Gray as a colleague, a friend and an employee. He created the first Microsoft Research group outside of our initial research lab in Redmond, WA. Jim's impact on industry and science was not limited to his own area of research. He championed new ways of thinking about how computer science and information processing could be integrated into other areas of research. In so doing; he will have a lasting impact on the rate of progress in many disciplines. His success stemmed not just from what he knew and what he could do with his knowledge, but also from who he was as a person. Jim was a "Gap Bridger" -- someone who could connect people, groups, companies and disciplines.

#index 1065649
#* Jim Gray at IBM: the transaction processing revolution
#@ Bruce G. Lindsay
#t 2008
#c 5
#% 317988
#% 319549
#% 320902
#% 403195
#% 531907
#% 990408
#% 993443
#! While at the IBM San Jose Research Laboratory, in the 1970's, Dr. Jim Gray defined and developed the fundamental concepts and techniques that underlie on-line transaction processing systems. Jim Gray's pivotal contributions enabled cost efficient, on-line processing to replace paper and batch processing systems. Today, online transaction processing powers the record keeping systems that drive today's commerce, services, and government.

#index 1065650
#* Jim Gray's Tandem contributions
#@ John Nauman;Wendy Bartlett
#t 2008
#c 5
#! This paper discusses the contributions and accomplishments of Jim Gray while he was at Tandem Computers between 1980 and the early 1990s.

#index 1065651
#* Not just correct, but correct and fast: a look at one of Jim Gray's contributions to database system performance
#@ David J. DeWitt;Charles Levine
#t 2008
#c 5
#% 14204
#% 482070
#! This paper examines Jim Gray's role in the specification of the debit/credit benchmark. The publication of this benchmark in a 1985 paper launched a benchmark war among the vendors that resulted in dramatic improvements in database system performance in the years following its publication. It was the genesis of the TPC, an industry consortium which has reshaped the benchmark landscape. Descendents of this benchmark continue to this day to be an important metric of modern transaction processing systems.

#index 1065652
#* Scaleability and immortality
#@ C Gordon Bell
#t 2008
#c 5
#% 325062
#% 342974
#% 344236
#% 848039
#! James Nicholas Gray's understanding and experimentation gave him a special perspective. From 1995 his commitment was building indefinitely scalable tools by working on really hard data-intensive application problems with other scientific disciplines. His attention to research for both understanding and use made him a unique researcher. Jim pioneered a new kind of 21st century science based on data analytics, requiring computer scientists to collaborate as an equal with scientists in other fields. Gray was an advocate and principal supporter of the MyLifeBits project aimed at extending our memory, memex. The by-product includes digital immortality that we herein speculate and explore.

#index 1065653
#* Is there life outside transactions?: writing the transaction processing book
#@ Andreas Reuter
#t 2008
#c 5
#% 17550
#% 68010
#% 86938
#% 114582
#% 403195
#% 866984
#! In this article I will reflect on the writing of "Transaction Processing -- Concepts and Techniques" [1], which appeared at Morgan Kaufmann Publishers in 1992. The process of writing had many aspects of a typical software project: In the end, the book was more than twice as thick as we had planned, it covered only 3/4 of the material that we wanted to cover, and completing it took much longer than we had anticipated. Nevertheless, it was a moderate success and served as a basic reference for many developers in the industry for at least 10 years after its publication. It was translated to Chinese and Japanese, and occasionally one still finds references to it -- despite the fact that (apart from simple bug fixes) there has been no technical update of the material, and the book deals with "outdated" subjects like transaction processing and client/server architectures.

#index 1065654
#* TerraServer and the Russia adventure...
#@ Tom Barclay
#t 2008
#c 5
#! Jim Gray made many friends and few enemies in his career. Many of the projects he worked on were inspired by one or more of his friends and collaborators in industry and academia. The original idea for an "image server" happened in the early 1990s when Jim was at Digital. Jim had met Jeff Dozier while working on the National Research Council's "Computing the Future" committee [NRC 1994]. Jeff became Dean of the Bren School of Environmental Sciences and chair of UC Santa Barbara's Alexandria Digital Library advisory board. Jeff asked Jim to serve on the board. During an Alexandria board meeting, Jeff and Jim discussed the idea for a "geospatial image server". All they needed was a lot of accessible imagery, someone that wanted it done, and someone foolish enough to load a few tera-bytes of data. The cold war ended enabling the high resolution imagery marketplace. Microsoft SQL Server wanted a scalability internet demonstration. And I walked into Jim Gray's lab looking for a reason to move back to San Francisco. And that's where my 10 year project with Jim Gray and TerraServer begins...

#index 1065655
#* The Sloan Digital Sky Survey and beyond
#@ Alexander S. Szalay
#t 2008
#c 5
#% 300185
#% 721053
#% 848039
#% 974351
#! Our collaboration with Jim Gray has created some of the world's largest astronomy databases, and has enabled us to test many avant-garde ideas in practice. The astronomers have been very receptive to these and embraced Jim as a 'card carrying member' of their community. Jim's contributions have made a permanent mark on astronomy, and eScience in general.

#index 1065656
#* Building the WorldWide Telescope
#@ Curtis Wong
#t 2008
#c 5
#% 300171
#! This paper is talks about the critical role that Jim Gray played in the creation of the WorldWide Telescope software. Contrary to what you might think it wasn't his database brilliance that made it happen, it was his generosity in sharing credit, inspiring, nurturing and connecting people, in this case that made it possible for the pieces to come into place and make it happen. Without Jim Gray's work with Alex Szalay on Sky Server and their ongoing support and encouragement, the software that is named in their honor would not exist today.

#index 1065657
#* Search survey for S/V Tenacious: Gulf of Farallones and approaches to San Francisco Bay
#@ Ed Saade
#t 2008
#c 5
#! On January 28th, 2007, Jim Gray sailed his 40 foot sailboat, Tenacious, on a day cruise to the Farallon Islands off San Francisco and was reported overdue when he didn't return as scheduled. The immediate and comprehensive above water search for Jim and Tenacious was suspended on February 16, 2007. Shortly after this, Fugro Pelagos, Inc., conducted an extensive search of the seabed to look for evidence of the whereabouts of the sailing vessel, Tenacious. Approximately 1000km2 was surveyed between February, 2007 and May 31st, 2007 when the effort was suspended. This paper summarizes the search survey for S/V Tenacious, the areas and techniques used in the survey, the findings of the survey, and the results and recommendations provided by Fugro Pelagos, Inc.

#index 1065658
#* Exploring ocean data
#@ James G. Bellingham;Mike Godin
#t 2008
#c 5
#% 845351
#! In fall of 2004, we met Jim Gray and began to converse about the data needs of ocean scientists. The conversations ultimately led to the development of a unique portal for exploring multidisciplinary data sets, which we call the Metadata Oriented Query Assistant (MOQuA). At the time, we were working with an extremely rich data set which included measurements from ships, satellites, aircraft, moorings, and a variety of underwater robots. The data set also included output from both atmospheric and ocean models. We initially made the data available via a state-of-the-art data server. However, serious users did not use the server, instead approaching us for copies of the relevant portions. Our experience convinced us that we needed a far more capable portal, but framing the seemingly divergent needs of ocean scientists in a way which could be satisfied via an intuitive interface was challenging. In the course of many conversations and interactions with Jim, we realized that we needed to structure the interaction around questions rather than visualizations, and this simple insight lead to the development of MOQuA system.

#index 1070305
#* Proceedings of the 1st international workshop on Testing database systems
#@ Leo Giakoumakis;Donald Kossmann
#t 2008
#c 5

#index 1090317
#* Proceedings of the 2nd SIGMOD PhD workshop on Innovative database research
#@ Chee-Yong Chan;Qing Li
#t 2008
#c 5
#! The second SIGMOD Ph.D. Workshop on Innovative Database Research (IDAR), which is co-located with the 2008 ACM SIGMOD/PODS Conference, was held in Vancouver, Canada on June 13, 2008. The workshop provides a forum for Ph.D. students, who are working on topics related to the SIGMOD conference series, to present, discuss, and receive feedback on their research. This year's workshop features 7 paper presentations which were selected from a total of 14 research submissions. In addition, the workshop program includes a keynote address by Prof. Michael Benedikt.

#index 1090318
#* The design and implementation of an OLAP system for sequence data analysis
#@ Chun Kit Chui
#t 2008
#c 5
#% 172950
#% 273916
#% 279164
#% 333850
#% 479450
#% 479795
#% 482088
#% 503731
#% 503878
#% 644230
#% 864470
#% 893157
#% 993958
#% 1022297
#! We design a novel online analytical processing system for sequence data analysis (the S-OLAP system). The biggest distinction of S-OLAP from traditional OLAP is that a sequence can be characterized not only by the attributes' values of its constituting items, but also by the subsequence/substring patterns it possesses. Traditional OLAP systems and techniques were not designed for sequence data and thus they are incapable of supporting sequence data analysis. This paper describes my Ph.D research on the design and implementation of the S-OLAP system. The proposed system is able to support "pattern-based" grouping and aggregation, which is currently not supported by any OLAP system. The preliminary ideas, significant research issues as well as major challenges of this ongoing project are presented.

#index 1090319
#* Research on personal dataspace management
#@ Yukun Li;Xiaofeng Meng
#t 2008
#c 5
#% 286258
#% 339375
#% 480629
#% 763824
#% 845350
#% 869536
#% 874876
#% 893119
#% 960237
#% 1015325
#% 1022257
#% 1022259
#% 1682428
#! Explosion of the amount of digital information has made Personal Information Management(PIM) become a hot topic. Personal data is always distributed, rough-and-tumble, personalized, heterogenous and evolutionary, which brings much challenge to to effective and efficient Personal Dataspace Management (PDSM). In the paper, by highlighting the importance of users in Personal Dataspace Management System(PDSMS), we proposed a user-centered framework. We first show research issues, related work, main research problems and challenges in this area. We then introduce the current research work and the preliminary results. Finally, the research plan of my PhD project is presented for discussion.

#index 1090320
#* A framework for web service discovery: service's reuse, quality, evolution and user's data handling
#@ Uddam Chukmol
#t 2008
#c 5
#% 722477
#% 800007
#% 803600
#% 837401
#% 850915
#% 900802
#% 900861
#% 974706
#% 1016160
#% 1389195
#% 1395956
#% 1599335
#% 1661809
#! With proliferation of published Web services, the task of finding relevant ones for the developers of service oriented application becomes more and more difficult. Several existing tools or mechanisms allow this discovery; however, those approaches often skip different elements such as service's quality, reuse, evolution and users' comment making the discovery result feebly relevant to requesters' need and prevent the requesters from using up-to-date and available web services efficiently. In this research work, we present a framework for web service discovery taking into account the reuse of web service search result through caching technique, the quality of service through qualitative test result, the web service's evolution through version track technique and we provide a novel scheme of discovery using user's annotating information.

#index 1090321
#* A study of communities and influence in blogosphere
#@ Nitin Agarwal
#t 2008
#c 5
#% 281214
#% 282905
#% 577360
#% 729923
#% 794513
#% 881051
#% 912461
#% 918842
#% 936341
#% 957992
#% 1035589
#% 1116559
#! Blogging becomes a popular way for a Web user to publish information on the Web. Bloggers write blog posts, share likes and dislikes, voice opinions, provide suggestions, and report news. In this work we study influential bloggers in both community as well as individual blogs. We synthesize virtual communities by aggregating individual blogs with similar interests. We formulate the problem for identifying influential bloggers and synthesizing virtual communities from individual blogs, present a preliminary model, discuss the challenges, and pave the way for building a robust model that allows finding various types of the influentials. To illustrate these issues, we conduct experiments with data from a real-world blog site, evaluate multi-facets of the problem, and discuss unique challenges. We conclude with interesting findings and future work.

#index 1090322
#* Explicit and default negation in databases and logic programs
#@ Navin Viswanath
#t 2008
#c 5
#% 663
#% 36539
#% 36556
#% 68140
#% 77167
#% 98624
#% 98682
#% 103705
#% 130379
#% 147604
#% 165212
#% 268779
#% 322880
#% 417607
#% 442740
#% 864394
#% 1410996
#! Both relational databases and logic programs adopt some form of nonmonotonic reasoning in order to infer negative information. Relational databases adopt the Closed World Assumption (CWA) of Reiter. It can be easily seen that much greater expressivity is gained if negative information can be explicitly stated. This turns out to be especially true in the presence of uncertain or incomplete information in the database. The aim of this work is to investigate the handling of incomplete information in an open world setting. For logic programs, the class of extended logic programs has already been defined and studied. Of the programs declared contradictory by most of the semantics proposed so far, some programs may in fact have an appropriate meaning. In this paper, we present a new semantics for logic programs with explicit negation where the derivation of a literal is tied to the derivation of its complement.

#index 1090323
#* Mechanisms for database intrusion detection and response
#@ Ashish Kamra;Elisa Bertino;Guy Lebanon
#t 2008
#c 5
#% 289519
#% 306097
#% 483530
#% 507694
#% 583844
#% 615098
#% 633782
#% 661361
#% 725307
#% 932269
#% 993943
#% 1072639
#% 1706199
#% 1711256
#! Data represent today a valuable asset for companies and organizations and must be protected. Most of an organization's sensitive and proprietary data resides in a Database Management System (DBMS). The focus of this thesis is to develop advanced security solutions for protecting the data residing in a DBMS. Our strategy is to develop an Intrusion Detection (ID) mechanism, implemented within the database server, that is capable of detecting anomalous user requests to a DBMS. The key idea is to learn profiles of users and applications interacting with a database. A database request that deviates from these profiles is then termed as anomalous. A major component of this work involves prototype implementation of this ID mechanism in the Post-greSQL database server. We also propose to augment the ID mechanism with an Intrusion Response engine that is capable of issuing an appropriate response to an anomalous database request.

#index 1090324
#* Approximate private information retrieval
#@ André Madeira
#t 2008
#c 5
#% 593711
#% 765279
#% 766286
#% 889625
#% 1386192
#% 1386206
#! We present efficient protocols for the 1-out-of-n single-server Computationally-Private Information Retrieval (CPIR) problem for l-bit strings. In particular, our results achieve simultaneously polylogarithmic extra storage, communication, and Client and Server local computational complexities, improving the best current bounds. Most designs for CPIR exploit the communication vs. computation trade-off, whereas ours exploit instead trade-offs between accuracy and the metrics mentioned above. The work is of a theoretical nature. In a nutshell, our polylogarithmic claims show that it is possible to substantially reduce the mentioned complexity measures at the expense of accuracy of the results. The main indirect (practical) implication of this work is to show that CPIR is actually viable for very large databases. We also present some open questions and future directions.

#index 1126241
#* Best Newcomer Award: Evaluating rank joins with optimal costs
#@ Carl Schneider;Neoklis Polyzotis
#t 2008
#c 5

#index 1126242
#* PODS Alberto O. Mendelzon Test-of-Time Award 2008
#@ Moshe Vardi
#t 2008
#c 5

#index 1126243
#* SIGMOD 10-year Test-of-Time Award: “Integration of heterogeneous databases without common domains using queries based on textual simularity”
#@ William Cohen
#t 2008
#c 5

#index 1126244
#* SIGMOD Best Dissertation Award: “Efficient query processing over inconsistent databases”
#@ Ariel Fuxman
#t 2008
#c 5

#index 1129950
#* Proceedings of the 4th international workshop on Data management on new hardware
#@ Qiong Luo;Kenneth A. Ross
#t 2008
#c 5
#! The aim of this one-day workshop is to bring together researchers who are interested in optimizing database performance on modern computing infrastructure by designing new data management techniques and tools.

#index 1134501
#* The Claremont report on database research
#@ Rakesh Agrawal;Anastasia Ailamaki;Philip A. Bernstein;Eric A. Brewer;Michael J. Carey;Surajit Chaudhuri;AnHai Doan;Daniela Florescu;Michael J. Franklin;Hector Garcia-Molina;Johannes Gehrke;Le Gruenwald;Laura M. Haas;Alon Y. Halevy;Joseph M. Hellerstein;Yannis E. Ioannidis;Hank F. Korth;Donald Kossmann;Samuel Madden;Roger Magoulas;Beng Chin Ooi;Tim O'Reilly;Raghu Ramakrishnan;Sunita Sarawagi;Michael Stonebraker;Alexander S. Szalay;Gerhard Weikum
#t 2008
#c 5
#% 111378
#% 218149
#% 275367
#% 339369
#% 805821
#% 1056069
#! In late May, 2008, a group of database researchers, architects, users and pundits met at the Claremont Resort in Berkeley, California to discuss the state of the research field and its impacts on practice. This was the seventh meeting of this sort in twenty years, and was distinguished by a broad consensus that we are at a turning point in the history of the field, due both to an explosion of data and usage scenarios, and to major shifts in computing hardware and platforms. Given these forces, we are at a time of opportunity for research impact, with an unusually large potential for influential results across computing, the sciences and society. This report details that discussion, and highlights the group's consensus view of new focus areas, including new database engine architectures, declarative programming languages, the interplay of structured and unstructured data, cloud data services, and mobile and virtual worlds. We also report on discussions of the community's growth, including suggestions for changes in community processes to move the research agenda forward, and to enhance impact on a broader audience.

#index 1134502
#* A critique of Claude Rubinson's paper nulls, three - valued logic, and ambiguity in SQL: critiquing Date's critique
#@ C. J. Date
#t 2008
#c 5
#% 927183
#% 1050775
#% 1197439

#index 1134503
#* Null values in SQL
#@ John Grant
#t 2008
#c 5
#% 114723
#% 282431
#% 810120
#% 836134
#% 1050775
#! In various writings over the past 20 years, such as [3], Date has pointed out that SQL produces incorrect answers to some queries where a null value is included in a table. In a recent article in the ACM SIGMOD Record, [8], Rubinson states that "Date misinterprets the meaning of his example query" and "SQL returns the correct answer to the query posed". The purpose of this article is to show that, contrary to Rubinson's claim, Date's critique of query evaluation in the presence of null values in SQL is completely justified.

#index 1134504
#* The ORCHESTRA Collaborative Data Sharing System
#@ Zachary G. Ives;Todd J. Green;Grigoris Karvounarakis;Nicholas E. Taylor;Val Tannen;Partha Pratim Talukdar;Marie Jacob;Fernando Pereira
#t 2008
#c 5
#% 152928
#% 235023
#% 237190
#% 286836
#% 287000
#% 342375
#% 378409
#% 384978
#% 465053
#% 505869
#% 572311
#% 654442
#% 654468
#% 715288
#% 809248
#% 824693
#% 826032
#% 857502
#% 874882
#% 874971
#% 893095
#% 893148
#% 893167
#% 949372
#% 960352
#% 961152
#% 976987
#% 993981
#% 993987
#% 1015281
#% 1016176
#% 1016201
#% 1022258
#% 1127413
#! Sharing structured data today requires standardizing upon a single schema, then mapping and cleaning all of the data. This results in a single queriable mediated data instance. However, for settings in which structured data is being collaboratively authored by a large community, e.g., in the sciences, there is often a lack of consensus about how it should be represented, what is correct, and which sources are authoritative. Moreover, such data is seldom static: it is frequently updated, cleaned, and annotated. The ORCHESTRA collaborative data sharing system develops a new architecture and consistency model for such settings, based on the needs of data sharing in the life sciences. In this paper we describe the basic architecture and implementation of the ORCHESTRA system, and summarize some of the open challenges that arise in this setting.

#index 1134505
#* AnHai Doan speaks out on his ACM Dissertation Award, schema matching, following your passion, least publishable units, and more
#@ Marianne Winslett
#t 2008
#c 5

#index 1134506
#* Paper and proposal reviews: is the process flawed?
#@ Henry F. Korth;Philip A. Bernstein;Mary Fernandez;Le Gruenwald;Phokion G. Kolaitis;Kathryn McKinley;Tamer Ozsu
#t 2008
#c 5
#! At the 2008 Computing Research Association Conference at Snowbird, the authors participated in a panel addressing the issue of paper and proposal reviews. This short paper summarizes the panelists' presentations and audience commentary. It concludes with some observations and suggestions on how we might address this issue in the near-term future.

#index 1134507
#* The conference reviewing crisis and a proposed solution
#@ H. V. Jagadish
#t 2008
#c 5
#% 983773
#% 1072082
#! In Computer Science, we have developed a vibrant conference culture, which has served us well thus far. However, with the growth of our field, the number of submissions to many conferences has sky-rocketed, leading to a downward spiral in reviewing quality and author satisfaction. This article proposes to break this downward spiral for the database community through JDMR, a journal for short "conference style" papers with rapid turn-around. An initial step toward this vision has been taken by VLDB.

#index 1134508
#* DB&IR integration: report on the Dagstuhl seminar
#@ Sihem Amer-Yahia;Djoerd Hiemstra;Thomas Roelleke;Divesh Srivastava;Gerhard Weikum
#t 2008
#c 5
#! This paper is based on a five-day workshop on "Ranked XML Querying" that took place in Schloss Dagstuhl in Germany in March 2008 and was attended by 27 people from three different research communities: database systems (DB), information retrieval (IR), and Web. The seminar title was interpreted in an IR-style "andish" sense (it covered also subsets of (Ranking, XML, Querying), with larger sets being favored) rather than the DB-style strictly conjunctive manner. So in essence, the seminar really addressed the integration of DB and IR technologies with Web 2.0 being an important target area.

#index 1134509
#* Report on the 9th international workshop on web information and data management (WIDM 2007)
#@ Irini Fundulaki;Neoklis Polyzotis
#t 2008
#c 5

#index 1183368
#* SystemT: a system for declarative information extraction
#@ Rajasekar Krishnamurthy;Yunyao Li;Sriram Raghavan;Frederick Reiss;Shivakumar Vaithyanathan;Huaiyu Zhu
#t 2009
#c 5
#% 411554
#% 464434
#% 465919
#% 875064
#% 1022288
#% 1206687
#! As applications within and outside the enterprise encounter increasing volumes of unstructured data, there has been renewed interest in the area of information extraction (IE) -- the discipline concerned with extracting structured information from unstructured text. Classical IE techniques developed by the NLP community were based on cascading grammars and regular expressions. However, due to the inherent limitations of grammarbased extraction, these techniques are unable to: (i) scale to large data sets, and (ii) support the expressivity requirements of complex information tasks. At the IBM Almaden Research Center, we are developing SystemT, an IE system that addresses these limitations by adopting an algebraic approach. By leveraging well-understood database concepts such as declarative queries and costbased optimization, SystemT enables scalable execution of complex information extraction tasks. In this paper, we motivate the SystemT approach to information extraction. We describe our extraction algebra and demonstrate the effectiveness of our optimization techniques in providing orders of magnitude reduction in the running time of complex extraction tasks.

#index 1183369
#* Information extraction challenges in managing unstructured data
#@ AnHai Doan;Jeffrey F. Naughton;Raghu Ramakrishnan;Akanksha Baid;Xiaoyong Chai;Fei Chen;Ting Chen;Eric Chu;Pedro DeRose;Byron Gao;Chaitanya Gokhale;Jiansheng Huang;Warren Shen;Ba-Quy Vuong
#t 2009
#c 5
#% 334031
#% 504443
#% 810108
#% 874876
#% 1022235
#% 1022288
#% 1022289
#% 1063547
#% 1127409
#% 1183368
#% 1206701
#% 1206800
#! Over the past few years, we have been trying to build an end-to-end system at Wisconsin to manage unstructured data, using extraction, integration, and user interaction. This paper describes the key information extraction (IE) challenges that we have run into, and sketches our solutions. We discuss in particular developing a declarative IE language, optimizing for this language, generating IE provenance, incorporating user feedback into the IE process, developing a novel wiki-based user interface for feedback, best-effort IE, pushing IE into RDBMSs, and more. Our work suggests that IE in managing unstructured data can open up many interesting research challenges, and that these challenges can greatly benefit from the wealth of work on managing structured data that has been carried out by the database community.

#index 1183370
#* Purple SOX extraction management system
#@ Philip Bohannon;Srujana Merugu;Cong Yu;Vipul Agarwal;Pedro DeRose;Arun Iyer;Ankur Jain;Vinay Kakade;Mridul Muralidharan;Raghu Ramakrishnan;Warren Shen
#t 2009
#c 5
#% 782759
#% 867052
#% 875064
#% 911553
#% 1022235
#% 1022288
#% 1063544
#% 1206701
#% 1692849
#! We describe the Purple SOX (PSOX) EMS, a prototype Extraction Management System currently being built at Yahoo!. The goal of the PSOX EMS is to manage a large number of sophisticated extraction pipelines across different application domains, at the web scale and with minimum human involvement. Three key value propositions are described: extensibility, the ability to swap in and out extraction operators; explainability, the ability to track the provenance of extraction results; and social feedback support, the facility for gathering and reconciling multiple, potentially conflicting sources.

#index 1183371
#* Building query optimizers for information extraction: the SQoUT project
#@ Alpa Jain;Panagiotis Ipeirotis;Luis Gravano
#t 2009
#c 5
#% 283136
#% 301241
#% 393844
#% 504443
#% 754068
#% 782759
#% 815884
#% 864416
#% 874992
#% 997488
#% 1022288
#% 1083692
#% 1174746
#% 1206687
#% 1206799
#% 1206862
#% 1206986
#% 1289516
#! Text documents often embed data that is structured in nature. This structured data is increasingly exposed using information extraction systems, which generate structured relations from documents, introducing an opportunity to process expressive, structured queries over text databases. This paper discusses our SQoUT1 project, which focuses on processing structured queries over relations extracted from text databases. We show how, in our extraction-based scenario, query processing can be decomposed into a sequence of basic steps: retrieving relevant text documents, extracting relations from the documents, and joining extracted relations for queries involving multiple relations. Each of these steps presents different alternatives and together they form a rich space of possible query execution strategies. We identify execution efficiency and output quality as the two critical properties of a query execution, and argue that an optimization approach needs to consider both properties. To this end, we take into account the userspecified requirements for execution efficiency and output quality, and choose an execution strategy for each query based on a principled, cost-based comparison of the alternative execution strategies.

#index 1183372
#* Domain adaptation of information extraction models
#@ Rahul Gupta;Sunita Sarawagi
#t 2009
#c 5
#% 344568
#% 464434
#% 770866
#% 874707
#% 983814
#% 983845
#% 1166537
#% 1261539
#! Domain adaptation refers to the process of adapting an extraction model trained in one domain to another related domain with only unlabeled data. We present a brief survey of existing methods of retraining models to best exploit labeled data from a related domain. These approaches that involve expensive model retraining are not practical when a large number of new domains have to be handled in an operational setting. We describe our approach for adapting record extraction models that exploits the regularity within a domain to jointly label records without retraining any model.

#index 1183373
#* The YAGO-NAGA approach to knowledge discovery
#@ Gjergji Kasneci;Maya Ramanath;Fabian Suchanek;Gerhard Weikum
#t 2009
#c 5
#% 830520
#% 872020
#% 881505
#% 881539
#% 956501
#% 956564
#% 997488
#% 1019061
#% 1022235
#% 1022288
#% 1055735
#% 1092530
#% 1166537
#% 1206687
#% 1206702
#% 1206817
#% 1275182
#% 1409954
#! This paper gives an overview on the YAGO-NAGA approach to information extraction for building a conveniently searchable, large-scale, highly accurate knowledge base of common facts. YAGO harvests infoboxes and category names of Wikipedia for facts about individual entities, and it reconciles these with the taxonomic backbone of WordNet in order to ensure that all entities have proper classes and the class system is consistent. Currently, the YAGO knowledge base contains about 19 million instances of binary relations for about 1.95 million entities. Based on intensive sampling, its accuracy is estimated to be above 95 percent. The paper presents the architecture of the YAGO extractor toolkit, its distinctive approach to consistency checking, its provisions for maintenance and further growth, and the query engine for YAGO, coined NAGA. It also discusses ongoing work on extensions towards integrating fact candidates extracted from natural-language text sources.

#index 1183374
#* Webpage understanding: beyond page-level search
#@ Zaiqing Nie;Ji-Rong Wen;Wei-Ying Ma
#t 2009
#c 5
#% 629661
#% 754078
#% 766462
#% 766464
#% 805896
#% 881505
#% 956501
#% 989662
#% 1117681
#% 1130979
#! In this paper we introduce the webpage understanding problem which consists of three subtasks: webpage segmentation, webpage structure labeling, and webpage text segmentation and labeling. The problem is motivated by the search applications we have been working on including Microsoft Academic Search, Windows Live Product Search and Renlifang Entity Relationship Search. We believe that integrated webpage understanding will be an important direction for future research in Web mining.

#index 1183375
#* Web-scale extraction of structured data
#@ Michael J. Cafarella;Jayant Madhavan;Alon Halevy
#t 2009
#c 5
#% 334031
#% 340146
#% 504443
#% 654459
#% 754068
#% 756964
#% 765409
#% 800497
#% 809418
#% 955762
#% 956564
#% 993964
#% 1019061
#% 1063534
#% 1127393
#% 1127557
#% 1264778
#% 1275182
#! A long-standing goal of Web research has been to construct a unified Web knowledge base. Information extraction techniques have shown good results on Web inputs, but even most domain-independent ones are not appropriate for Web-scale operation. In this paper we describe three recent extraction systems that can be operated on the entire Web (two of which come from Google Research). The TextRunner system focuses on raw natural language text, the WebTables system focuses on HTML-embedded tables, and the deep-web surfacing system focuses on "hidden" databases. The domain, expressiveness, and accuracy of extracted data can depend strongly on its source extractor; we describe differences in the characteristics of data produced by the three extractors. Finally, we discuss a series of unique data applications (some of which have already been prototyped) that are enabled by aggregating extractedWeb information.

#index 1183376
#* Using Wikipedia to bootstrap open information extraction
#@ Daniel S. Weld;Raphael Hoffmann;Fei Wu
#t 2009
#c 5
#% 240955
#% 452641
#% 464434
#% 466078
#% 756964
#% 830520
#% 838060
#% 850430
#% 936912
#% 939601
#% 956500
#% 956564
#% 1019061
#% 1055735
#% 1083704
#% 1083705
#% 1089602
#% 1183280
#% 1206800
#% 1269815
#% 1269899
#% 1275182
#% 1275758

#index 1183377
#* Modeling and querying probabilistic XML data
#@ Benny Kimelfeld;Yehoshua Sagiv
#t 2009
#c 5
#% 8387
#% 39702
#% 42984
#% 58608
#% 115964
#% 136358
#% 172933
#% 292675
#% 333989
#% 397375
#% 465044
#% 800547
#% 893149
#% 977012
#% 977013
#% 977014
#% 992830
#% 993985
#% 1022204
#% 1036075
#% 1063522
#% 1063720
#% 1408537
#% 1688305
#% 1972413

#index 1183378
#* On Query Algebras for Probabilistic Databases
#@ Christoph Koch
#t 2009
#c 5
#% 58608
#% 94459
#% 265692
#% 314829
#% 384978
#% 893167
#% 960293
#% 992830
#% 1063521
#% 1063568
#% 1063719
#% 1127376
#% 1180007
#% 1180009
#% 1206717
#% 1408537
#! This article proposes a core query algebra for probabilistic databases. In essence, this core is part of the query languages of most probabilistic database systems proposed so far, but is sometimes hidden in complex language definitions. We give a formal definition of the algebra and illustrate it by examples. We then survey the current state of knowledge regarding the expressive power and complexity of this core.

#index 1183379
#* Surajit Chaudhuri speaks out on how data mining led him to self-tuning databases: how he does tech transfer, life as a research manager, the fragmentation of database research, and more
#@ Marianne Winslett
#t 2009
#c 5

#index 1183380
#* The ETH Zurich systems group and enterprise computing center
#@ Gustavo Alonso;Donald Kossmann;Timothy Roscoe;Nesime Tatbul;Andrew Baumann;Carsten Binnig;Peter Fischer;Oriana Riva;Jens Teubner
#t 2009
#c 5
#% 983480
#% 1022208
#% 1044434
#% 1053447
#% 1063488
#% 1063591
#% 1153888
#% 1181012
#% 1181292
#% 1415534
#% 1468246

#index 1183381
#* How NOT to review a paper: the tools and techniques of the adversarial reviewer
#@ Graham Cormode
#t 2009
#c 5
#% 69257
#% 164792
#% 844674
#% 1046316
#! There are several useful guides available for how to review a paper in Computer Science [10, 6, 12, 7, 2]. These are soberly presented, carefully reasoned and sensibly argued. As a result, they are not much fun. So, as a contrast, this note is a checklist of how not to review a paper. It details techniques that are unethical, unfair, or just plain nasty. Since in Computer Science we often present arguments about how an adversary would approach a particular problem, this note describes the adversary's strategy.

#index 1183382
#* A report on the first european conference on software architecture (ECSA'2007)
#@ Carlos E. Cuesta;Esperanza Marcos
#t 2009
#c 5
#% 1100683

#index 1183383
#* Report on international workshop on privacy and anonymity in the information society (PAIS 2008)
#@ Li Xiong;Traian Marius Truta;Farshad Fotouhi
#t 2009
#c 5
#% 1065552
#% 1065553
#% 1065554
#% 1065555
#% 1065556
#% 1065557
#% 1065558
#% 1065559
#% 1065560

#index 1183384
#* Report on the IFIP WG5.8 international workshop on enterprise interoperability (IWEI 2008)
#@ Marten van Sinderen;Pontus Johnson;Lea Kutvonen
#t 2009
#c 5

#index 1183385
#* First workshop on very large digital libraries -- VLDL 2008
#@ Paolo Manghi;Pasquale Pagano;Pavel Zezula
#t 2009
#c 5

#index 1183386
#* First workshop on transforming and weaving ontologies in model driven engineering (TWOMDE 2008)
#@ Fernando Silva Parreiras;Jeff Z Pan;Uwe Assmann;Jakob Herinksson
#t 2009
#c 5
#! The First International Workshop on Transforming and Weaving Ontologies in Model Driven Engineering (TWOMDE 2008), affiliated with the 11th International Conference on Model Driven Engineering Languages and Systems (MoD-ELS2008), brought together researchers and practitioners from the modeling community with experience or interest in MDE and in Knowledge Representation to discuss about: (1) how the scientific and technical results around ontologies, ontology languages and their corresponding reasoning technologies can be used fruitfully in MDE; (2) the role of ontologies in supporting model transformation; (3) and how ontologies can improve designing domain specific languages.

#index 1215240
#* Proceedings of the First International Workshop on Keyword Search on Structured Data
#@ M. Tamer Özsu;Yi Chen;Lei Chen
#t 2009
#c 5
#! Information search is an indispensable component of our lives. Web search engines are widely used for searching textual documents, images, and video. However, there are also vast collections of structured and semi-structured data both on the Web and in enterprises, such as relational databases, XML data, etc. Traditionally, to access these resources, a user must learn structured or semi-structured query languages, and must be able to access data schemas, which are most likely heterogeneous, complex, and fastevolving. To relieve web and scientific users from the learning curve and enable them to easily access structured and semi-structured data, there is a growing research interest to support keyword search on these data sources. The first International Workshop on Keyword Search on Structured Data (KEYS 2009) is held in Providence, Rhode Island, USA on 28th June, 2009, in conjunction with SIGMOD 2009 conference, and aims to encourage researchers from both academia and industry communities to discuss the opportunities and challenges in keyword search on (semi-)structured data, and to present the key issues and novel techniques in this area. In response to the call for papers, KEYS 2009 has attracted 25 submissions. The submissions are highly diversified, coming from Canada, China, Germany, Italy, Japan, Greece, Singapore, Sweden, Thailand, and USA, resulting in an international final program. All submissions were peer reviewed by three program committee members. The program committee selected 6 full research papers and 4 demo and poster papers for inclusion in the proceeding. The accepted papers covered a wide range of research topics and novel applications on keyword search on structured data.

#index 1215803
#* Derivability, redundancy and consistency of relations stored in large data banks
#@ E. F. E, F. Codd
#t 2009
#c 5
#% 1137035
#! The large, integrated data banks of the future will contain many relations of various degrees in stored form. It will not be unusual for this set of stored relations to be redundant. Two types of redundancy are defined and discussed. One type may be employed to improve accessibility of certain kinds of information which happen to be in great demand. When either type of redundancy exists, those responsible for control of the data bank should know about it and have some means of detecting any "logical" inconsistencies in the total set of stored relations. Consistency checking might be helpful in tracking down unauthorized (and possibly fraudulent) changes in the data bank contents.

#index 1215804
#* An X-ray on web-available XML schemas
#@ Alberto H.F. Laender;Mirella M. Moro;Cristiano Nascimento;Patrícia Martins
#t 2009
#c 5
#% 244109
#% 291299
#% 322415
#% 342447
#% 479956
#% 659924
#% 772031
#% 783696
#% 809236
#% 835398
#% 845589
#% 879213
#% 956718
#% 1046506
#% 1065125
#! XML has conquered its place as the most used standard for representing Web data. An XML schema may be employed for similar purposes of those from database schemas. There are different languages to write an XML schema, such as DTD and XSD. In this paper, we provide a general view, an X-Ray, on Web-available XSD files by identifying which XSD constructs are more and less frequently used. Furthermore, we provide an evolution perspective, showing results from XSD files collected in 2005 and 2008. Hence, we can also draw some conclusions on what trends seem to exist in XSD usage. The results of such study provide relevant information for developers of XML applications, tools and algorithms in which the schema has a distinguished role.

#index 1215805
#* Rethinking cost and performance of database systems
#@ Daniela Florescu;Donald Kossmann
#t 2009
#c 5
#% 210177
#% 227872
#% 300167
#% 300177
#% 378414
#% 392578
#% 397295
#% 397402
#% 480474
#% 571217
#% 1022200
#% 1022298
#% 1063488
#% 1390334
#! Traditionally, database systems were optimized in the following way: "Given a set of machines, try to minimize the response time of each request." This paper argues that today, users would like a database system to optimize the opposite question: "Given a response time goal for each request, try to minimize the number of machines (i.e., cost in $)." Furthermore, this paper gives an example that demonstrates that the new optimization problem may result in a totally different system architecture.

#index 1215806
#* Logical foundations of relational data exchange
#@ Pablo Barceló
#t 2009
#c 5
#% 583
#% 663
#% 287339
#% 289384
#% 378409
#% 465053
#% 801676
#% 801691
#% 806215
#% 809239
#% 809247
#% 809248
#% 823106
#% 825671
#% 826032
#% 874879
#% 874880
#% 874881
#% 874882
#% 976995
#% 976997
#% 1039061
#% 1054485
#% 1063710
#% 1063722
#% 1063723
#% 1063724
#% 1180002
#% 1217116
#% 1217117

#index 1215807
#* Data integration in mashups
#@ Giusy Di Lorenzo;Hakim Hacid;Hye-young Paik;Boualem Benatallah
#t 2009
#c 5
#% 22948
#% 170602
#% 399551
#% 572314
#% 711965
#% 842032
#% 860745
#% 893087
#% 956568
#% 956587
#% 960347
#% 968327
#% 1006367
#% 1022328
#% 1022351
#% 1048457
#% 1055747
#% 1063559
#% 1127582
#% 1131155
#% 1131156
#% 1136063
#% 1188958
#! Mashup is a new application development approach that allows users to aggregate multiple services to create a service for a new purpose. Even if the Mashup approach opens new and broader opportunities for data/service consumers, the development process still requires the users to know not only how to write code using programming languages, but also how to use the different Web APIs from different services. In order to solve this problem, there is increasing effort put into developing tools which are designed to support users with little programming knowledge in Mashup applications development. The objective of this study is to analyze the richnesses and weaknesses of the Mashup tools with respect to the data integration aspect.

#index 1215808
#* Gerhard Weikum speaks out on why we should go for the grand challenges, why SQL is too
#@ Marianne Winslett
#t 2009
#c 5

#index 1217113
#* Proceedings of the twenty-eighth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems
#@ Jan Paredaens;Jianwen Su
#t 2009
#c 5
#! This volume contains the proceedings of the Twenty-eighth ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems (PODS 2009), held in Providence, Rhode Island, on June 29--July 1, 2009, in conjunction with the 2009 ACM SIGMOD International Conference on Management of Data. The proceedings include 3 invited papers and 26 contributed papers. One invited paper is based on the keynote address by Raghu Ramakrishnan, while the other two are based on the invited tutorials by Leonid Libkin and Lars Arge, respectively. In addition, the announcement of the 2009 ACM PODS Alberto O. Mendelzon Test-of-Time Award also appears in the proceedings. The program committee selected 26 contributed papers for presentation at the conference from 97 submissions by authors from 21 countries. Most of the selected papers are preliminary reports on work in progress. While they have been read by the program committee members, they have not been formally refereed. It is expected that many of them will eventually appear in more polished and detailed form in scientific journals. In addition, the program committee also selected the paper "Size and Treewidth Bounds for Conjunctive Queries" by Georg Gottlob, Stephanie Lee, and Gregory Valiant for the PODS 2009 Best Paper Award, and the paper "XPath Evaluation in Linear Time with Polynomial Combined Complexity" by Pawel Parys for the PODS 2009 Best Student Paper Award, a new award for the PODS conferences. Warmest congratulations to the authors of these papers.

#index 1217114
#* A web of concepts
#@ Nilesh Dalvi;Ravi Kumar;Bo Pang;Raghu Ramakrishnan;Andrew Tomkins;Philip Bohannon;Sathiya Keerthi;Srujana Merugu
#t 2009
#c 5
#% 275915
#% 279164
#% 333679
#% 428148
#% 479807
#% 480648
#% 480824
#% 572314
#% 743359
#% 754068
#% 800551
#% 800590
#% 810014
#% 843716
#% 874876
#% 875064
#% 893167
#% 893168
#% 915340
#% 916788
#% 937552
#% 983899
#% 987276
#% 992830
#% 993980
#% 1004299
#% 1022235
#% 1061900
#% 1065125
#% 1083658
#% 1127393
#% 1206701
#% 1217172
#% 1700140
#! We make the case for developing a web of concepts by starting with the current view of web (comprised of hyperlinked pages, or documents, each seen as a bag of words), extracting concept-centric metadata, and stitching it together to create a semantically rich aggregate view of all the information available on the web for each concept instance. The goal of building and maintaining such a web of concepts presents many challenges, but also offers the promise of enabling many powerful applications, including novel search and information discovery paradigms. We present the goal, motivate it with example usage scenarios and some analysis of Yahoo! logs, and discuss the challenges in building and leveraging such a web of concepts. We place this ambitious research agenda in the context of the state of the art in the literature, and describe various ongoing efforts at Yahoo! Research that are related.

#index 1217115
#* Generalized schema-mappings: from termination to tractability
#@ Bruno Marnette
#t 2009
#c 5
#% 465053
#% 465057
#% 806215
#% 809247
#% 874880
#% 1063723
#% 1063724
#! Data-Exchange is the problem of creating new databases according to a high-level specification called a schema-mapping while preserving the information encoded in a source database. This paper introduces a notion of generalized schema-mapping that enriches the standard schema-mappings (as defined by Fagin et al) with more expressive power. It then proposes a more general and arguably more intuitive notion of semantics that rely on three criteria: Soundness, Completeness and Laconicity (non-redundancy and minimal size). These semantics are shown to coincide precisely with the notion of cores of universal solutions in the framework of Fagin, Kolaitis and Popa. It is also well-defined and of interest for larger classes of schema-mappings and more expressive source databases (with null-values and equality constraints). After an investigation of the key properties of generalized schema-mappings and their semantics, a criterion called Termination of the Oblivious Chase (TOC) is identified that ensures polynomial data-complexity. This criterion strictly generalizes the previously known criterion of Weak-Acyclicity. To prove the tractability of TOC schema-mappings, a new polynomial time algorithm is provided that, unlike the algorithm of Gottlob and Nash from which it is inspired, does not rely on the syntactic property of Weak-Acyclicity. As the problem of deciding whether a Schema-mapping satisfies the TOC criterion is only recursively enumerable, a more restrictive criterion called Super-weak Acylicity (SwA) is identified that can be decided in Polynomial-time while generalizing substantially the notion of Weak-Acyclicity.

#index 1217116
#* Reverse data exchange: coping with nulls
#@ Ronald Fagin;Phokion G. Kolaitis;Lucian Popa;Wang-Chiew Tan
#t 2009
#c 5
#% 583
#% 384978
#% 562454
#% 765540
#% 809249
#% 826032
#% 850730
#% 893094
#% 993981
#% 997492
#% 1015302
#% 1036084
#% 1054485
#% 1063710
#! An inverse of a schema mapping M is intended to "undo" what M does, thus providing a way to perform "reverse" data exchange. In recent years, three different formalizations of this concept have been introduced and studied, namely, the notions of an inverse of a schema mapping, a quasi-inverse of a schema mapping, and a maximum recovery of a schema mapping. The study of these notions has been carried out in the context in which source instances are restricted to consist entirely of constants, while target instances may contain both constants and labeled nulls. This restriction on source instances is crucial for obtaining some of the main technical results about these three notions, but, at the same time, limits their usefulness, since reverse data exchange naturally leads to source instances that may contain both constants and labeled nulls. We develop a new framework for reverse data exchange that supports source instances that may contain nulls, thus overcoming the semantic mismatch between source and target instances of the previous formalizations. The development of this new framework requires a careful reformulation of all the important notions, including the notions of the identity schema mapping, inverse, and maximum recovery. To this effect, we introduce the notions of extended identity schema mapping, extended inverse, and maximum extended recovery, by making systematic use of the homomorphism relation on instances. We give results concerning the existence of extended inverses and of maximum extended recoveries, and results concerning their applications to reverse data exchange and query answering. Moreover, we show that maximum extended recoveries can be used to capture in a quantitative way the amount of information loss embodied in a schema mapping specified by source-to-target tuple-generating dependencies.

#index 1217117
#* XML schema mappings
#@ Shun'ichi Amano;Leonid Libkin;Filip Murlak
#t 2009
#c 5
#% 332166
#% 333858
#% 378409
#% 398752
#% 479783
#% 570877
#% 660001
#% 742566
#% 772031
#% 809239
#% 824660
#% 826032
#% 850730
#% 865766
#% 874877
#% 874879
#% 893093
#% 893095
#% 927032
#% 960233
#% 993981
#% 1015302
#% 1026963
#% 1039061
#% 1039062
#% 1054485
#% 1063710
#% 1408529
#% 1682381
#! Relational schema mappings have been extensively studied in connection with data integration and exchange problems, but mappings between XML schemas have not received the same amount of attention. Our goal is to develop a theory of expressive XML schema mappings. Such mappings should be able to use various forms of navigation in a document, and specify conditions on data values. We develop a language for XML schema mappings, and concentrate on three types of problems: static analysis of mappings, their complexity, and their composition. We look at static analysis problems related to various flavors of consistency: for example, whether it is possible to map some document of a source schema into a document of the target schema, or whether all documents of a source schema can be mapped. We classify the complexity of these problems. We then move to the complexity of mappings themselves, i.e., recognizing pairs of documents such that one can be mapped into the other, and provide a classification based on sets of features used in mappings. Finally we look at composition of XML schema mappings. We study its complexity and show that it is harder to achieve closure under composition for XML than for relational mappings. Nevertheless, we find a robust class of XML schema mappings that have good complexity properties and are closed under composition.

#index 1217118
#* The ACM PODS Alberto O. Mendelzon test-of-time-award 2009
#@ Catriel Beeri;Phokion G. Kolaitis;Christos H. Papadimitriou
#t 2009
#c 5

#index 1217119
#* Size and treewidth bounds for conjunctive queries
#@ Georg Gottlob;Stephanie Tien Lee;Gregory J. Valiant
#t 2009
#c 5
#% 583
#% 71306
#% 93660
#% 169841
#% 198465
#% 210353
#% 248014
#% 262723
#% 287316
#% 287339
#% 318049
#% 378409
#% 384978
#% 465057
#% 503535
#% 599549
#% 809239
#% 847068
#% 857502
#% 958014
#% 1141493
#% 1269931
#! This paper provides new worst-case bounds for the size and treewith of the result Q(D) of a conjunctive query Q to a database D. We derive bounds for the result size |Q(D)| in terms of structural properties of Q, both in the absence and in the presence of keys and functional dependencies. These bounds are based on a novel "coloring" of the query variables that associates a coloring number C(Q) to each query Q. Using this coloring number, we derive tight bounds for the size of Q(D) in case (i) no functional dependencies or keys are specified, and (ii) simple (one-attribute) keys are given. These results generalize recent size-bounds for join queries obtained by Atserias, Grohe, and Marx (FOCS 2008). An extension of our coloring technique also gives a lower bound for |Q(D)| in the general setting of a query with arbitrary functional dependencies. Our new coloring scheme also allows us to precisely characterize (both in the absence of keys and with simple keys) the treewidth-preserving queries--the queries for which the output treewidth is bounded by a function of the input treewidth. Finally we characterize the queries that preserve the sparsity of the input in the general setting with arbitrary functional dependencies.

#index 1217120
#* XPath evaluation in linear time with polynomial combined complexity
#@ Pawel Parys
#t 2009
#c 5
#% 186
#% 427027
#% 498538
#% 814648
#% 1063733
#% 1129529
#% 1388798
#! We consider a fragment of XPath 1.0, where attribute and text values may be compared. We show that for any unary query in this fragment, the set of nodes that satisfy the query can be calculated in time linear in the document size and polynomial in the size of the query. The previous algorithm for this fragment also had linear data complexity but exponential complexity in the query size.

#index 1217121
#* The finite model theory toolbox of a database theoretician
#@ Leonid Libkin
#t 2009
#c 5
#% 6109
#% 6787
#% 183682
#% 188350
#% 191611
#% 224744
#% 303891
#% 307249
#% 307258
#% 342387
#% 401124
#% 449224
#% 473125
#% 598376
#% 733595
#% 778122
#% 874877
#% 927769
#% 942354
#% 1061656
#% 1562065
#! For many years, finite model theory was viewed as the backbone of database theory, and database theory in turn supplied finite model theory with key motivations and problems. By now, finite model theory has built a large arsenal of tools that can easily be used by database theoreticians without going to the basics such as combinatorial games. We survey such tools here, focusing not on how they are proved, but rather on how to apply them, as-is, in various questions that come up in database theory.

#index 1217122
#* A general datalog-based framework for tractable query answering over ontologies
#@ Andrea Calì;Georg Gottlob;Thomas Lukasiewicz
#t 2009
#c 5
#% 583
#% 53385
#% 53388
#% 101646
#% 264704
#% 265104
#% 286860
#% 287339
#% 342829
#% 378409
#% 384978
#% 490909
#% 562303
#% 576116
#% 599549
#% 736407
#% 826032
#% 855914
#% 869463
#% 992925
#% 992962
#% 1063724
#% 1269726
#% 1409909
#% 1416180
#! In this paper, we introduce a family of expressive extensions of Datalog, called Datalog+/-, as a new paradigm for query answering over ontologies. The Datalog+/- family admits existentially quantified variables in rule heads, and has suitable restrictions to ensure highly efficient ontology querying. We show in particular that Datalog+/- generalizes the DL-Lite family of tractable description logics, which are the most common tractable ontology languages in the context of the Semantic Web and databases. We also show how stratified negation can be added to Datalog+/- while keeping ontology querying tractable. Furthermore, the Datalog+/- family is of interest in its own right and can, moreover, be used in various contexts such as data integration and data exchange.

#index 1217123
#* Satisfiability and relevance for queries over active documents
#@ Serge Abiteboul;Pierre Bourhis;Bogdan Marinoiu
#t 2009
#c 5
#% 13016
#% 101623
#% 190332
#% 330274
#% 366807
#% 416035
#% 473117
#% 479770
#% 481128
#% 481786
#% 576106
#% 733593
#% 765420
#% 770168
#% 801671
#% 809236
#% 814648
#% 866986
#% 997023
#% 1016325
#% 1063731
#% 1072645
#% 1181304
#% 1206616
#% 1206775
#% 1408529
#! ManyWeb applications are based on dynamic interactions between Web components exchanging flows of information. Such a situation arises for instance in mashup systems [22] or when monitoring distributed autonomous systems [6]. This is a challenging problem that has generated recently a lot of attention; see Web 2.0 [38]. For capturing interactions between Web components, we use active documents interacting with the rest of the world via streams of updates. Their input streams specify updates to the document (in the spirit of RSS feeds), whereas their output streams are defined by queries on the document. In most of the paper, the focus is on input streams where the updates are only insertions, although we do consider also deletions. We introduce and study two fundamental concepts in this setting, namely, satisfiability and relevance. Some fact is satisfiable for an active document and a query if it has a chance to be in the result of the query in some future state. Given an active document and a query, a call in the document is relevant if the data brought by this call has a chance to impact the answer to the query. We analyze the complexity of computing satisfiability in our core model (insertions only) and for extensions (e.g., with deletions). We also analyze the complexity of computing relevance in the core model.

#index 1217124
#* Relative information completeness
#@ Wenfei Fan;Floris Geerts
#t 2009
#c 5
#% 663
#% 11817
#% 67457
#% 248038
#% 264858
#% 268764
#% 273687
#% 366807
#% 384978
#% 472987
#% 480249
#% 481128
#% 481786
#% 576116
#% 726626
#% 809238
#% 903332
#% 943614
#% 943616
#% 1022222
#% 1054480
#% 1063725
#% 1130461
#% 1661426
#! The paper investigates the question of whether a partially closed database has complete information to answer a query. In practice an enterprise often maintains master data Dm, a closed-world database. We say that a database D is partially closed if it satisfies a set V of containment constraints of the form "q(D) is a subset of p(Dm)", where q is a query in a language Lc and p is a projection query. The part of D not constrained by (Dm,V) is open, from which some tuples may be missing. The database D is said to be complete for a query Q relative to (Dm,V) if for all partially closed extensions D' of D, Q(D')=Q(D), i.e., adding tuples to D either violates some constraints in V or does not change the answer to Q. We first show that the proposed model can also capture the consistency of data, in addition to its relative completeness. Indeed, integrity constraints studied for consistency can be expressed as containment constraints. We then study two problems. One is to decide, given Dm, V, a query Q in a language Lq and a partially closed database D, whether D is complete for Q relative to (Dm,V). The other is to determine, given Dm, V and Q, whether there exists a partially closed database that is complete for Q relative to (Dm,V). We establish matching lower and upper bounds on these problems for a variety of languages Lq and Lc. We also provide characterizations for a database to be relatively complete, and for a query to allow a relatively complete database, when Lq and Lc are conjunctive queries.

#index 1217125
#* Relationship privacy: output perturbation for queries with joins
#@ Vibhor Rastogi;Michael Hay;Gerome Miklau;Dan Suciu
#t 2009
#c 5
#% 115491
#% 428647
#% 576111
#% 765449
#% 864412
#% 956511
#% 963241
#% 1022246
#% 1063476
#% 1063726
#% 1083653
#% 1127360
#% 1206763
#% 1415851
#% 1670071
#% 1740518
#! We study privacy-preserving query answering over data containing relationships. A social network is a prime example of such data, where the nodes represent individuals and edges represent relationships. Nearly all interesting queries over social networks involve joins, and for such queries, existing output perturbation algorithms severely distort query answers. We propose an algorithm that significantly improves utility over competing techniques, typically reducing the error bound from polynomial in the number of nodes to polylogarithmic. The algorithm is, to the best of our knowledge, the first to answer such queries with acceptable accuracy, even for worst-case inputs. The improved utility is achieved by relaxing the privacy condition. Instead of ensuring strict differential privacy, we guarantee a weaker (but still quite practical) condition based on adversarial privacy. To explain precisely the nature of our relaxation in privacy, we provide a new result that characterizes the relationship between ε-indistinguishability~(a variant of the differential privacy definition) and adversarial privacy, which is of independent interest: an algorithm is ε-indistinguishable iff it is private for a particular class of adversaries (defined precisely herein). Our perturbation algorithm guarantees privacy against adversaries in this class whose prior distribution is numerically bounded.

#index 1217126
#* An efficient rigorous approach for identifying statistically significant frequent itemsets
#@ Adam Kirsch;Michael Mitzenmacher;Andrea Pietracaprina;Geppino Pucci;Eli Upfal;Fabio Vandin
#t 2009
#c 5
#% 152934
#% 210160
#% 248012
#% 316709
#% 342597
#% 420073
#% 443092
#% 546821
#% 769913
#% 818434
#% 823336
#% 824710
#% 835018
#% 867877
#% 881472
#% 985041
#% 1176891
#! As advances in technology allow for the collection, storage, and analysis of vast amounts of data, the task of screening and assessing the significance of discovered patterns is becoming a major challenge in data mining applications. In this work, we address significance in the context of frequent itemset mining. Specifically, we develop a novel methodology to identify a meaningful support threshold s* for a dataset, such that the number of itemsets with support at least s* represents a substantial deviation from what would be expected in a random dataset with the same number of transactions and the same individual item frequencies. These itemsets can then be flagged as statistically significant with a small false discovery rate. Our methodology hinges on a Poisson approximation to the distribution of the number of itemsets in a random dataset with support at least s, for any s greater than or equal to a minimum threshold smin. We obtain this result through a novel application of the Chen-Stein approximation method, which is of independent interest. Based on this approximation, we develop an efficient parametric multi-hypothesis test for identifying the desired threshold s*. A crucial feature of our approach is that, unlike most previous work, it takes into account the entire dataset rather than individual discoveries. It is therefore better able to distinguish between significant observations and random fluctuations. We present extensive experimental results to substantiate the effectiveness of our methodology.

#index 1217127
#* Similarity caching
#@ Flavio Chierichetti;Ravi Kumar;Sergei Vassilvitskii
#t 2009
#c 5
#% 735
#% 1156
#% 4683
#% 54221
#% 232752
#% 244264
#% 249321
#% 261358
#% 319473
#% 391210
#% 479973
#% 498386
#% 577302
#% 580668
#% 777988
#% 1131117
#% 1190099
#! We introduce the similarity caching problem, a variant of classical caching in which an algorithm can return an element from the cache that is similar, but not necessarily identical, to the query element. We are motivated by buffer management questions in approximate nearest-neighbor applications, especially in the context of caching targeted advertisements on the web. Formally, we assume the queries lie in a metric space, with distance function d(.,.). A query p is considered a cache hit if there is a point q in the cache that is sufficiently close to p, i.e., for a threshold radius r, we have d(p,q) ≤ r. The goal is then to minimize the number of cache misses, vis-à-vis the optimal algorithm. As with classical caching, we use the competitive ratio to measure the performance of different algorithms. While similarity caching is a strict generalization of classical caching, we show that unless the algorithm is allowed extra power (either in the size of the cache or the threshold r) over the optimal offline algorithm, the problem is intractable. We then proceed to quantify the hardness as a function of the complexity of the underlying metric space. We show that the problem becomes easier as we proceed from general metric spaces to those of bounded doubling dimension, and to Euclidean metrics. Finally, we investigate several extensions of the problem: dependence of the threshold r on the query and a smoother trade-off between the cache-miss cost and the query-query similarity.

#index 1217128
#* Indexing uncertain data
#@ Pankaj K. Agarwal;Siu-Wing Cheng;Yufei Tao;Ke Yi
#t 2009
#c 5
#% 1679
#% 14515
#% 41684
#% 72237
#% 212867
#% 321052
#% 415038
#% 818938
#% 824728
#% 893189
#% 959978
#% 983259
#% 1016201
#% 1016202
#% 1147652
#% 1164818
#% 1164838
#! Querying uncertain data has emerged as an important problem in data management due to the imprecise nature of many measurement data. In this paper we study answering range queries over uncertain data. Specifically, we are given a collection P of n points in R, each represented by its one-dimensional probability density function (pdf). The goal is to build an index on P such that given a query interval I and a probability threshold τ, we can quickly report all points of P that lie in I with probability at least τ. We present various indexing schemes with linear or near-linear space and logarithmic query time. Our schemes support pdf's that are either histograms or more complex ones such as Gaussian or piecewise algebraic. They also extend to the external memory model in which the goal is to minimize the number of disk accesses when querying the index.

#index 1217129
#* Optimal sampling from sliding windows
#@ Vladimir Braverman;Rafail Ostrovsky;Carlo Zaniolo
#t 2009
#c 5
#% 1331
#% 178991
#% 214073
#% 243166
#% 248812
#% 273908
#% 300132
#% 338394
#% 378388
#% 379443
#% 379444
#% 379445
#% 397443
#% 460787
#% 519953
#% 548654
#% 576113
#% 578388
#% 578389
#% 580702
#% 725366
#% 745534
#% 749449
#% 763999
#% 765436
#% 777988
#% 785339
#% 788215
#% 789000
#% 801696
#% 803602
#% 805744
#% 808428
#% 809263
#% 810033
#% 813786
#% 824653
#% 847112
#% 847114
#% 847115
#% 847163
#% 866696
#% 866773
#% 874571
#% 874902
#% 874906
#% 893138
#% 893154
#% 894646
#% 918001
#% 957604
#% 989512
#% 991154
#% 993960
#% 1002033
#% 1015296
#% 1019711
#% 1029101
#% 1039679
#% 1063498
#% 1141469
#% 1669552
#% 1682599
#% 1720748
#% 1727969
#! A sliding windows model is an important case of the streaming model, where only the most "recent" elements remain active and the rest are discarded in a stream. The sliding windows model is important for many applications (see, e.g., Babcock, Babu, Datar, Motwani and Widom (PODS 02); and Datar, Gionis, Indyk and Motwani (SODA 02)). There are two equally important types of the sliding windows model -- windows with fixed size, (e.g., where items arrive one at a time, and only the most recent n items remain active for some fixed parameter n), and bursty windows (e.g., where many items can arrive in "bursts" at a single step and where only items from the last t steps remain active, again for some fixed parameter t). Random sampling is a fundamental tool for data streams, as numerous algorithms operate on the sampled data instead of on the entire stream. Effective sampling from sliding windows is a nontrivial problem, as elements eventually expire. In fact, the deletions are implicit; i.e., it is not possible to identify deleted elements without storing the entire window. The implicit nature of deletions on sliding windows does not allow the existing methods (even those that support explicit deletions, e.g., Cormode, Muthukrishnan and Rozenbaum (VLDB 05); Frahling, Indyk and Sohler (SOCG 05)) to be directly "translated" to the sliding windows model. One trivial approach to overcoming the problem of implicit deletions is that of over-sampling. When k samples are required, the over-sampling method maintains k'k samples in the hope that at least k samples are not expired. The obvious disadvantages of this method are twofold: (a) It introduces additional costs and thus decreases the performance; and (b) The memory bounds are not deterministic, which is atypical for streaming algorithms (where even small probability events may eventually happen for a stream that is big enough). Babcock, Datar and Motwani (SODA 02), were the first to stress the importance of improvements to over-sampling. They formally introduced the problem of sampling from sliding windows and improved the over-sampling method for sampling with replacement. Their elegant solutions for sampling with replacement are optimal in expectation, and thus resolve disadvantage (a) mentioned above. Unfortunately, the randomized bounds do not resolve disadvantage (b) above. Interestingly, all algorithms that employ the ideas of Babcock, Datar and Motwani have the same central problem of having to deal with randomized complexity (see e.g., Datar and Muthukrishnan (ESA 02); Chakrabarti, Cormode and McGregor (SODA 07)). Further, the proposed solutions of Babcock, Datar and Motwani for sampling without replacement are based on the criticized over-sampling method and thus do not solve problem (a). Therefore, the question of whether we can solve sampling on sliding windows optimally (i.e., resolving both disadvantages) is implicit in the paper of Babcock, Datar and Motwani and has remained open for all variants of the problem. In this paper we answer these questions affirmatively and provide optimal sampling schemas for all variants of the problem, i.e., sampling with or without replacement from fixed or bursty windows. Specifically, for fixed-size windows, we provide optimal solutions that require O(k) memory; for bursty windows, we show algorithms that require O(klogn), which is optimal since it matches the lower bound by Gemulla and Lehner (SIGMOD 08). In contrast to the work of of Babcock, Datar and Motwani, our solutions have deterministic bounds. Thus, we prove a perhaps somewhat surprising fact: the memory complexity of the sampling-based algorithm for all variants of the sliding windows model is comparable with that of streaming models (i.e., without the sliding windows). This is the first result of this type, since all previous "translations" of sampling-based algorithms to sliding windows incur randomized memory guarantees only.

#index 1217130
#* Space-optimal heavy hitters with strong error bounds
#@ Radu Berinde;Graham Cormode;Piotr Indyk;Martin J. Strauss
#t 2009
#c 5
#% 273916
#% 333925
#% 344400
#% 479795
#% 492912
#% 548479
#% 569754
#% 765291
#% 783740
#% 809265
#% 894646
#% 963258
#% 991154
#% 993960
#% 1015293
#% 1127608
#% 1141523
#% 1700144
#! The problem of finding heavy hitters and approximating the frequencies of items is at the heart of many problems in data stream analysis. It has been observed that several proposed solutions to this problem can outperform their worst-case guarantees on real data. This leads to the question of whether some stronger bounds can be guaranteed. We answer this in the positive by showing that a class of "counter-based algorithms" (including the popular and very space-efficient FREQUENT and SPACESAVING algorithms) provide much stronger approximation guarantees than previously known. Specifically, we show that errors in the approximation of individual elements do not depend on the frequencies of the most frequent elements, but only on the frequency of the remaining "tail." This shows that counter-based methods are the most space-efficient (in fact, space-optimal) algorithms having this strong error bound. This tail guarantee allows these algorithms to solve the "sparse recovery" problem. Here, the goal is to recover a faithful representation of the vector of frequencies, f. We prove that using space O(k), the algorithms construct an approximation f* to the frequency vector f so that the L1 error ||f -- f*||1 is close to the best possible error minf2 ||f2 -- f||1, where f2 ranges over all vectors with at most k non-zero entries. This improves the previously best known space bound of about O(k log n) for streams without element deletions (where n is the size of the domain from which stream elements are drawn). Other consequences of the tail guarantees are results for skewed (Zipfian) data, and guarantees for accuracy of merging multiple summarized streams.

#index 1217131
#* Optimal tracking of distributed heavy hitters and quantiles
#@ Ke Yi;Qin Zhang
#t 2009
#c 5
#% 278835
#% 333931
#% 378388
#% 569754
#% 576119
#% 600560
#% 654443
#% 654488
#% 800582
#% 810009
#% 824652
#% 864444
#% 874903
#% 874994
#% 894443
#% 993960
#% 993969
#% 1016178
#% 1039695
#% 1063739
#% 1127608
#! We consider the the problem of tracking heavy hitters and quantiles in the distributed streaming model. The heavy hitters and quantiles are two important statistics for characterizing a data distribution. Let A be a multiset of elements, drawn from the universe U={1,...,u}. For a given 0 ≤ Φ ≤ 1, the Φ-heavy hitters are those elements of A whose frequency in A is at least Φ |A|; the Φ-quantile of A is an element x of U such that at most Φ|A| elements of A are smaller than A and at most (1-Φ)|A| elements of A are greater than x. Suppose the elements of A are received at k remote sites over time, and each of the sites has a two-way communication channel to a designated coordinator, whose goal is to track the set of Φ-heavy hitters and the Φ-quantile of A approximately at all times with minimum communication. We give tracking algorithms with worst-case communication cost O(k/ε ⋅ log n) for both problems, where n is the total number of items in A, and ε is the approximation error. This substantially improves upon the previous known algorithms. We also give matching lower bounds on the communication costs for both problems, showing that our algorithms are optimal. We also consider a more general version of the problem where we simultaneously track the Φ-quantiles for all 0 ≤ Φ ≤ 1.

#index 1217132
#* Worst-case efficient range search indexing: invited tutorial
#@ Lars Arge
#t 2009
#c 5
#% 56081
#% 252304
#% 273714
#% 281731
#% 317933
#% 336569
#% 341100
#% 344424
#% 411694
#% 427199
#% 443130
#% 571296
#% 580214
#% 723366
#% 765430
#! In this tutorial we will describe some of the recent advances in the development of worst-case efficient range search indexing structures, that is, structures for storing a set of data points such that the points in a axis-parallel (hyper-) query rectangle can be found efficiently (with as few disk accesses - or I/Os - as possible). We first quickly discuss the well-known and optimal structure for the one-dimensional version of the problem, the B-tree [10, 12], along with its variants weight-balanced B-trees [9], multi-version (or persistent) B-trees [6, 11, 13, 22] and buffer-trees [4]. Then we discuss the external priority search tree [8], which solves a restricted version of the two-dimensional version of the problem where the query rectangle is unbounded on one side. This structure is then used in a range tree index structure [8, 21] that answers general two-dimensional queries in the same number of I/Os as the B-tree in the one-dimensional case, but using super-linear space. We also describe the linear space kdB-tree [19, 20] and O-tree [17] index structures that also solve the problem efficiently (but using more I/Os than the range tree). A detailed presentation of all the the above structures can be found in lecture notes by the author [5]. Finally, we also discuss lower bounds techniques, most notably the theory of indexability [16], that can be used to prove that both the range tree and kdB-tree/O-tree are optimal among query efficient and linear space structures, respectively [2, 8, 17], as well as recent index structures for higher-dimensional range search indexing [1]. We end by mentioning various R-tree variant [7, 18, 15] that can be used to solve the extended version of range search indexing where the queries as well as the data are (hyper-) rectangles. More comprehensive surveys of efficient index structures can be found in [3, 14, 23].

#index 1217133
#* Secondary indexing in one dimension: beyond b-trees and bitmap indexes
#@ Rasmus Pagh;Srinivasa Rao Satti
#t 2009
#c 5
#% 41684
#% 227861
#% 248814
#% 273904
#% 322884
#% 453533
#% 600478
#% 723366
#% 857498
#% 893158
#% 960298
#% 983260
#% 1016131
#% 1058620
#% 1206655
#% 1407167
#! Let ∑ be a finite, ordered alphabet, and consider a string x=χ1χ2... χn ∈ ∑n. A secondary index for x answers alphabet range queries of the form: Given a range [αl,αr] ⊆ ∑, return the set I[αl,αr] = {i |χi ∈ [αl,αr]}. Secondary indexes are heavily used in relational databases and scientific data analysis. It is well-known that the obvious solution, storing a dictionary for the set ∪i{χi} with a position set associated with each character, does not always give optimal query time. In this paper we give the first theoretically optimal data structure for the secondary indexing problem. In the I/O model, the amount of data read when answering a query is within a constant factor of the minimum space needed to represent the set I[αl,αr], assuming that the size of internal memory is (|∑| lg n)δ blocks, for some constant δ 0. The space usage of the data structure is O(nlg |∑|) bits in the worst case, and we further show how to bound the size of the data structure in terms of the 0th order entropy of x. We show how to support updates achieving various time-space trade-offs. We also consider an approximate version of the basic secondary indexing problem where a query reports a superset of I[αl,αr] containing each element not in I[αl,αr] with probability at most ∈, where ∈ 0 is the false positive probability. For this problem the amount of data that needs to be read by the query algorithm is reduced to O(|I(αl,αr]| lg(1/∈)) bits.

#index 1217134
#* Dynamic indexability and lower bounds for dynamic one-dimensional range query indexes
#@ Ke Yi
#t 2009
#c 5
#% 208047
#% 237204
#% 248015
#% 248016
#% 273714
#% 289316
#% 303087
#% 338426
#% 344424
#% 449318
#% 453533
#% 479470
#% 479974
#% 805733
#% 1211590
#! The B-tree is a fundamental external index structure that is widely used for answering one-dimensional range reporting queries. Given a set of N keys, a range query can be answered in O(logB NoverM + KoverB) I/Os, where B is the disk block size, K the output size, and M the size of the main memory buffer. When keys are inserted or deleted, the B-tree is updated in O(logB N) I/Os, if we require the resulting changes to be committed to disk right away. Otherwise, the memory buffer can be used to buffer the recent updates, and changes can be written to disk in batches, which significantly lowers the amortized update cost. A systematic way of batching up updates is to use the logarithmic method, combined with fractional cascading, resulting in a dynamic B-tree that supports insertions in O(1overB log NoverM) I/Os and queries in O(log NoverM + KoverB) I/Os. Such bounds have also been matched by several known dynamic B-tree variants in the database literature. Note that, however, the query cost of these dynamic B-trees is substantially worse than the O(logB NoverM + KoverB) bound of the static B-tree by a factor of ?(log B). In this paper, we prove that for any dynamic one dimensional range query index structure with query cost O(q + KoverB) and amortized insertion cost O(u/B), the tradeoff q · log(u/q) = ©(log B) must hold if q = O(log B). For most reasonable values of the parameters, we have NoverM = BO(1), in which case our query-insertion tradeoff implies that the bounds mentioned above are already optimal. We also prove a lower bound of u · log q = ©(log B), which is relevant for larger values of q. Our lower bounds hold in a dynamic version of the indexability model, which is of independent interests. Dynamic indexability is a clean yet powerful model for studying dynamic indexing problems, and can potentially lead to more interesting complexity results.

#index 1217135
#* Satisfiability of downward XPath with data equality tests
#@ Diego Figueira
#t 2009
#c 5
#% 6242
#% 814648
#% 874877
#% 874910
#% 1039062
#% 1063734
#% 1673664
#% 1700125
#! In this work we investigate the satisfiability problem for the logic XPath(↓*, ↓,=), that includes all downward axes as well as equality and inequality tests. We address this problem in the absence of DTDs and the sibling axis. We prove that this fragment is decidable, and we nail down its complexity, showing the problem to be ExpTime-complete. The result also holds when path expressions allow closure under the Kleene star operator. To obtain these results, we introduce a new automaton model over data trees that captures XPath(↓*, ↓,=) and has an ExpTime emptiness problem. Furthermore, we give the exact complexity of several downward-looking fragments.

#index 1217136
#* Equivalence of nested queries with mixed semantics
#@ David DeHaan
#t 2009
#c 5
#% 12966
#% 27056
#% 34242
#% 36181
#% 54712
#% 123118
#% 137867
#% 164364
#% 164376
#% 189868
#% 190638
#% 198465
#% 237181
#% 247577
#% 268790
#% 273698
#% 287005
#% 289266
#% 300138
#% 384978
#% 463735
#% 480091
#% 481293
#% 481604
#% 482081
#% 565259
#% 599549
#% 765447
#% 801769
#% 807679
#% 810022
#% 874883
#% 938789
#% 943615
#% 1016140
#% 1092008
#% 1124990
#! We consider the problem of deciding query equivalence for a conjunctive language in which queries output complex objects composed from a mixture of nested, unordered collection types. Using an encoding of nested objects as flat relations, we translate the problem to deciding the equivalence between encodings output by relational conjunctive queries. This encoding equivalence cleanly unifies and generalizes previous results for deciding equivalence of conjunctive queries evaluated under various processing semantics. As part of our characterization of encoding equivalence, we define a normal form for encoding queries and contend that this normal form offers new insight into the fundamental principles governing the behaviour of nested aggregation.

#index 1217137
#* Equivalence of SQL queries in presence of embedded dependencies
#@ Rada Chirkova;Michael R. Genesereth
#t 2009
#c 5
#% 36181
#% 137867
#% 198465
#% 237190
#% 248034
#% 273696
#% 384978
#% 393907
#% 464056
#% 465190
#% 572311
#% 599549
#% 716433
#% 826032
#% 857502
#% 874883
#% 874884
#% 874973
#% 1063724
#! We consider the problem of finding equivalent minimal-size reformulations of SQL queries in presence of embedded dependencies [1]. Our focus is on select-project-join (SPJ) queries with equality comparisons, also known as safe conjunctive (CQ) queries, possibly with grouping and aggregation. For SPJ queries, the semantics of the SQL standard treats query answers as multisets (bags), whereas the stored relations are treated either as sets, which is called bag-set semantics, or as bags, which is called bag semantics. (Under set semantics, both query answers and stored relations are treated as sets.) In the context of the above Query-Reformulation Problem, we develop a comprehensive framework for equivalence of CQ queries under bag and bag-set semantics in presence of embedded dependencies, and make a number of conceptual and technical contributions. Specifically, we develop equivalence tests for CQ queries in presence of arbitrary sets of embedded dependencies under bag and bag-set semantics, under the condition that chase [10] under set semantics (set-chase) on the inputs terminates. We also present equivalence tests for CQ queries with grouping and aggregation in presence of embedded dependencies. We use our equivalence tests to develop sound and complete (whenever set-chase on the inputs terminates) algorithms for solving instances of the Query-Reformulation Problem with CQ queries under each of bag and bag-set semantics, as well as for instances of the problem with aggregate queries. Our contributions are clearly applicable beyond the Query-Reformulation Problem considered in this paper. Specifically, the results of this paper can be used in developing algorithms for rewriting CQ queries and queries in more expressive languages (e.g., including grouping and aggregation, or arithmetic comparisons) using views in presence of embedded dependencies, under bag or bag-set semantics for query evaluation.

#index 1217138
#* Running tree automata on probabilistic XML
#@ Sara Cohen;Benny Kimelfeld;Yehoshua Sagiv
#t 2009
#c 5
#% 115964
#% 190254
#% 256604
#% 265692
#% 299976
#% 397375
#% 398752
#% 401124
#% 465044
#% 465059
#% 473125
#% 598376
#% 800547
#% 907602
#% 917999
#% 949370
#% 977012
#% 977013
#% 993985
#% 1016201
#% 1022204
#% 1063522
#% 1063719
#% 1063720
#% 1688305
#% 1972413
#! Tree automata (specifically, bottom-up and unranked) form a powerful tool for querying and maintaining validity of XML documents. XML with uncertain data can be modeled as a probability space of labeled trees, and that space is often represented by a tree with distributional nodes. This paper investigates the problem of evaluating a tree automaton over such a representation, where the goal is to compute the probability that the automaton accepts a random possible world. This problem is generally intractable, but for the case where the tree automaton is deterministic (and its transitions are defined by deterministic string automata), an efficient algorithm is presented. The paper discusses the applications of this result, including the ability to sample and to evaluate queries (e.g., in monadic second-order logic) while requiring a-priori conformance to a schema (e.g., DTD). XML schemas also include attribute constraints, and the complexity of key, foreign-key and inclusion constraints are studied in the context of probabilistic XML. Finally, the paper discusses the generalization of the results to an extended data model, where distributional nodes can repeatedly sample the same subtree, thereby adding another exponent to the size of the probability space.

#index 1217139
#* XML with incomplete information: models, properties, and query answering
#@ Pablo Barceló;Leonid Libkin;Antonella Poggi;Cristina Sirangelo
#t 2009
#c 5
#% 663
#% 6711
#% 94459
#% 248038
#% 378409
#% 384978
#% 465053
#% 576116
#% 826032
#% 865766
#% 866986
#% 977012
#% 1039061
#% 1039062
#% 1063720
#% 1063735
#% 1083337
#% 1086124
#% 1408529
#% 1682381
#! We study models of incomplete information for XML, their computational properties, and query answering. While our approach is motivated by the study of relational incompleteness, incomplete information in XML documents may appear not only as null values but also as missing structural information. Our goal is to provide a classification of incomplete descriptions of XML documents, and separate features - or groups of features - that lead to hard computational problems from those that admit efficient algorithms. Our classification of incomplete information is based on the combination of null values with partial structural descriptions of documents. The key computational problems we consider are consistency of partial descriptions, representability of complete documents by incomplete ones, and query answering. We show how factors such as schema information, the presence of node ids, and missing structural information affect the complexity of these main computational problems, and find robust classes of incomplete XML descriptions that permit tractable query evaluation.

#index 1217140
#* Distributed XML design
#@ Serge Abiteboul;Georg Gottlob;Marco Manna
#t 2009
#c 5
#% 70235
#% 100049
#% 273897
#% 299944
#% 309729
#% 475997
#% 503593
#% 562461
#% 600179
#% 654485
#% 724844
#% 809260
#% 1072645
#% 1386383
#% 1408540
#% 1688304
#! A distributed XML document is an XML document that spans several machines or Web repositories. We assume that a distribution design of the document tree is given, providing an XML tree some of whose leaves are "docking points", to which XML subtrees can be attached. These subtrees may be provided and controlled by peers at remote locations, or may correspond to the result of function calls, e.g., Web services. If a global type τ, e.g. a DTD, is specified for a distributed document T, it would be most desirable to be able to break this type into a collection of local types, called a local typing, such that the document satisfies τ if and only if each peer (or function) satisfies its local type. In this paper we lay out the fundamentals of a theory of local typing and provide formal definitions of three main variants of locality: local typing, maximal local typing, and perfect typing, the latter being the most desirable. We study the following relevant decision problems: (i) given a typing for a design, determine whether it is local, maximal local, or perfect; (ii) given a design, establish whether a (maximal) local, or perfect typing does exist. For some of these problems we provide tight complexity bounds (polynomial space), while for the others we show exponential upper bounds. A main contribution is a polynomial-space algorithm for computing a perfect typing in this context, if it exists.

#index 1217141
#* Consensus answers for queries over probabilistic databases
#@ Jian Li;Amol Deshpande
#t 2009
#c 5
#% 663
#% 215225
#% 235023
#% 330769
#% 442830
#% 654487
#% 728195
#% 864394
#% 864417
#% 893168
#% 960293
#% 976984
#% 976987
#% 977008
#% 991163
#% 1016178
#% 1016201
#% 1022206
#% 1063520
#% 1063728
#% 1068448
#% 1091267
#% 1127377
#% 1127415
#% 1206645
#% 1206646
#% 1206893
#% 1207234
#% 1728680
#! We address the problem of finding a "best" deterministic query answer to a query over a probabilistic database. For this purpose, we propose the notion of a consensus world (or a consensus answer) which is a deterministic world (answer) that minimizes the expected distance to the possible worlds (answers). This problem can be seen as a generalization of the well-studied inconsistent information aggregation problems (e.g. rank aggregation) to probabilistic databases. We consider this problem for various types of queries including SPJ queries, Top-k ranking queries, group-by aggregate queries, and clustering. For different distance metrics, we obtain polynomial time optimal or approximation algorithms for computing the consensus answers (or prove NP-hardness). Most of our results are for a general probabilistic database model, called and/xor tree model, which significantly generalizes previous probabilistic database models like x-tuples and block-independent disjoint models, and is of independent interest.

#index 1217142
#* Exceeding expectations and clustering uncertain data
#@ Sudipto Guha;Kamesh Munagala
#t 2009
#c 5
#% 54221
#% 271130
#% 296760
#% 325398
#% 325409
#% 334092
#% 415062
#% 593903
#% 593913
#% 743962
#% 749517
#% 765296
#% 785096
#% 785097
#% 785098
#% 813745
#% 836526
#% 991152
#% 1039704
#% 1063728
#% 1479497
#% 1676011
#% 1699381
#! Database technology is playing an increasingly important role in understanding and solving large-scale and complex scientific and societal problems and phenomena, for instance, understanding biological networks, climate modeling, electronic markets, etc. In these settings, uncertainty or imprecise information is a pervasive issue that becomes a serious impediment to understanding and effectively utilizing such systems. Clustering is one of the key problems in this context. In this paper we focus on the problem of clustering, specifically the k-center problem. Since the problem is NP-Hard in deterministic setting, a natural avenue is to consider approximation algorithms with a bounded performance ratio. In an earlier paper Cormode and McGregor had considered certain variants of this problem, but failed to provide approximations that preserved the number of centers. In this paper we remedy the situation and provide true approximation algorithms for a wider class of these problems. However, the key aspect of this paper is to devise general techniques for optimization under uncertainty. We show that a particular formulation which uses the contribution of a random variable above its expectation is useful in this context. We believe these techniques will find wider applications in optimization under uncertainty.

#index 1217143
#* Computing all skyline probabilities for uncertain data
#@ Mikhail J. Atallah;Yinian Qi
#t 2009
#c 5
#% 672
#% 2115
#% 288976
#% 465167
#% 654487
#% 772835
#% 810098
#% 824728
#% 893167
#% 960336
#% 1016202
#% 1022203
#% 1022224
#% 1022226
#% 1022266
#% 1029019
#% 1044478
#% 1063485
#% 1063520
#% 1127377
#% 1206716
#% 1206717
#% 1206781
#% 1206866
#! Skyline computation is widely used in multi-criteria decision making. As research in uncertain databases draws increasing attention, skyline queries with uncertain data have also been studied, e.g. probabilistic skylines. The previous work requires "thresholding" for its efficiency -- the efficiency relies on the assumption that points with skyline probabilities below a certain threshold can be ignored. But there are situations where "thresholding" is not desirable -- low probability events cannot be ignored when their consequences are significant. In such cases it is necessary to compute skyline probabilities of all data items. We provide the first algorithm for this problem whose worst-case time complexity is sub-quadratic. The techniques we use are interesting in their own right, as they rely on a space partitioning technique combined with using the existing dominance counting algorithm. The effectiveness of our algorithm is experimentally verified.

#index 1217144
#* Proceedings of the 2009 ACM SIGMOD International Conference on Management of data
#@ Carsten Binnig;Benoit Dageville;Uğur Çetintemel;Stan Zdonik;Donald Kossmann
#t 2009
#c 5
#! It is our great pleasure to welcome you to the 35th ACM SIGMOD Conference. This year's conference took place in beautiful Providence, capital of Rhode Island, the Ocean State, and home to renowned schools and top-flight chefs. SIGMOD 2009 continues its tradition as a premier forum for the presentation of research results, industry developments, tutorials, and demos in the general area of data management. In addition, SIGMOD 2009 hosts a variety of collocated workshops, the new researcher symposium, and the ACM SIGMOD programming contest which was held for the first time in 2009. The call for research papers attracted 430 submissions of which 397 were peer-reviewed. (30 submissions did not comply with SIGMOD's double-blind reviewing policy; a few submissions did not comply with the length constraints or were withdrawn by the authors.) Of the 397 reviewed submissions, 63 papers were accepted for presentation at the conference. Furthermore, five tutorials, eighteen industry papers, and thirty-two demos were invited for presentation. The conference highlights include two keynotes, one by Hasso Plattner and one by Martin Wattenberg and Fernanda Viegas. Furthermore, the conference features five additional invited talks in the areas of human-computer interaction and systems, and a special session to celebrate the 40 th anniversary of the invention of the relational data model.

#index 1217145
#* A common database approach for OLTP and OLAP using an in-memory column database
#@ Hasso Plattner
#t 2009
#c 5
#% 201869
#% 201951
#% 223781
#% 286258
#% 393530
#% 442705
#% 479821
#% 824697
#% 875026
#% 921816
#% 1063561
#% 1206624
#! When SQL and the relational data model were introduced 25 years ago as a general data management concept, enterprise software migrated quickly to this new technology. It is fair to say that SQL and the various implementations of RDBMSs became the backbone of enterprise systems. In those days. we believed that business planning, transaction processing and analytics should reside in one single system. Despite the incredible improvements in computer hardware, high-speed networks, display devices and the associated software, speed and flexibility remained an issue. The nature of RDBMSs, being organized along rows, prohibited us from providing instant analytical insight and finally led to the introduction of so-called data warehouses. This paper will question some of the fundamentals of the OLAP and OLTP separation. Based on the analysis of real customer environments and experience in some prototype implementations, a new proposal for an enterprise data management concept will be presented. In our proposal, the participants in enterprise applications, customers, orders, accounting documents, products, employees etc. will be modeled as objects and also stored and maintained as such. Despite that, the vast majority of business functions will operate on an in memory representation of their objects. Using the relational algebra and a column-based organization of data storage will allow us to revolutionize transactional applications while providing an optimal platform for analytical data processing. The unification of OLTP and OLAP workloads on a shared architecture and the reintegration of planning activities promise significant gains in application development while simplifying enterprise systems drastically. The latest trends in computer technology -- e.g. blade architecture, multiple CPUs per blade with multiple cores per CPU allow for a significant parallelization of application processes. The organization of data in columns supports the parallel use of cores for filtering and aggregation. Elements of application logic can be implemented as highly efficient stored procedures operating on columns. The vast increase in main memory combined with improvements in L1--, L2--, L3--caching, together with the high data compression rate column storage will allow us to support substantial data volumes on one single blade. Distributing data across multiple blades using a shared nothing approach provides further scalability.

#index 1217146
#* Transforming data access through public visualization
#@ Fernanda B. Viégas;Martin Wattenberg
#t 2009
#c 5
#! Data visualization has historically been accessible only to the elite in academia, business, and government. It was "serious" technology, created by experts for experts. In recent years, however, web-based visualizations--ranging from political art projects to news stories--have reached audiences of millions. Meanwhile, new initiatives in government, aimed at all citizens, point to an era of increased transparency. What will this new era of data transparency look like--and what are the implications for technologists who work with data? To help answer this question, we report on recent research into public data analysis and visualization. Many of our results come from Many Eyes, a "living laboratory" web site where people may upload their own data, create interactive visualizations, and carry on conversations. Political discussions, citizen activism, religious discussions, game playing, and educational exchanges all happen on the site. To further support these scenarios, and the users they represent, will require continued innovation in data presentation and interaction.

#index 1217147
#* Authenticated join processing in outsourced databases
#@ Yin Yang;Dimitris Papadias;Stavros Papadopoulos;Panos Kalnis
#t 2009
#c 5
#% 381870
#% 397402
#% 513367
#% 566391
#% 725292
#% 745532
#% 761411
#% 765448
#% 810016
#% 810042
#% 824701
#% 874980
#% 960290
#% 960298
#% 1015281
#% 1022213
#% 1022214
#% 1022267
#% 1127363
#% 1206573
#% 1206751
#% 1206816
#% 1669498
#! Database outsourcing requires that a query server constructs a proof of result correctness, which can be verified by the client using the data owner's signature. Previous authentication techniques deal with range queries on a single relation using an authenticated data structure (ADS). On the other hand, authenticated join processing is inherently more complex than ranges since only the base relations (but not their combination) are signed by the owner. In this paper, we present three novel join algorithms depending on the ADS availability: (i) Authenticated Indexed Sort Merge Join (AISM), which utilizes a single ADS on the join attribute, (ii) Authenticated Index Merge Join (AIM) that requires an ADS (on the join attribute) for both relations, and (iii) Authenticated Sort Merge Join (ASM), which does not rely on any ADS. We experimentally demonstrate that the proposed methods outperform two benchmark algorithms, often by several orders of magnitude, on all performance metrics, and effectively shift the workload to the outsourcing service. Finally, we extend our techniques to complex queries that combine multi-way joins with selections and projections.

#index 1217148
#* Privacy integrated queries: an extensible platform for privacy-preserving data analysis
#@ Frank D. McSherry
#t 2009
#c 5
#% 67453
#% 248030
#% 809245
#% 864412
#% 960291
#% 977011
#% 983467
#% 1029084
#% 1083653
#% 1129382
#% 1468421
#% 1670071
#% 1732708
#% 1740518
#! We report on the design and implementation of the Privacy Integrated Queries (PINQ) platform for privacy-preserving data analysis. PINQ provides analysts with a programming interface to unscrubbed data through a SQL-like language. At the same time, the design of PINQ's analysis language and its careful implementation provide formal guarantees of differential privacy for any and all uses of the platform. PINQ's unconditional structural guarantees require no trust placed in the expertise or diligence of the analysts, substantially broadening the scope for design and deployment of privacy-preserving data analysis, especially by non-experts.

#index 1217149
#* Secure outsourced aggregation via one-way chains
#@ Suman Nath;Haifeng Yu;Haowen Chan
#t 2009
#c 5
#% 2833
#% 278835
#% 319994
#% 519953
#% 654483
#% 745442
#% 745532
#% 781894
#% 783741
#% 810042
#% 824701
#% 838424
#% 874980
#% 881074
#% 904984
#% 1010135
#% 1025681
#% 1035194
#% 1058348
#% 1254364
#% 1706190
#! We consider the Outsourced Aggregation model, where sensing services outsource their sensor data collection and aggregation tasks to third-party service providers called aggregators. As aggregators can be untrusted or compromised, it is essential for a sensing service to be able to verify the correctness of aggregation results. This work presents SECOA, a framework with a family of novel and optimally-secure protocols for secure outsourced aggregation. Our framework is based on a unified use of one-way chains. It supports a large and diverse set of aggregate functions, can have multiple hierarchically organized aggregators, can deterministically detect any malicious aggregation behavior without communication with sensors, and incurs a small and workload-independent communication load on sensors. We also present extensive evaluation results to demonstrate the feasibility of our framework.

#index 1217150
#* Dependency-aware reordering for parallelizing query optimization in multi-core CPUs
#@ Wook-Shin Han;Jinsoo Lee
#t 2009
#c 5
#% 43161
#% 58376
#% 70370
#% 86949
#% 140385
#% 159337
#% 166189
#% 169843
#% 191608
#% 198068
#% 271802
#% 290879
#% 397471
#% 398804
#% 444474
#% 480430
#% 654472
#% 834885
#% 862957
#% 884433
#% 893165
#% 911426
#% 961015
#% 1063510
#% 1127367
#% 1133537
#% 1142432
#! The state of the art commercial query optimizers employ cost-based optimization and exploit dynamic programming (DP) to find the optimal query execution plan (QEP) without evaluating redundant sub-plans. The number of alternative QEPs enumerated by the DP query optimizer can increase exponentially, as the number of joins in the query increases. Recently, by exploiting the coming wave of multi-core processor architectures, a state of the art parallel optimization algorithm [14], referred to as PDPsva, has been proposed to parallelize the "time-consuming" DP query optimization process itself. While PDPsva significantly extends the practical use of DP to queries having up to 20-25 tables, it has several limitations: 1) supporting only the size-driven DP enumerator, 2) statically allocating search space, and 3) not fully exploiting parallelism. In this paper, we propose the first generic solution for parallelizing any type of bottom-up optimizer, including the graph-traversal driven type, and for supporting dynamic search allocation and full parallelism. This is a challenging problem, since recently developed, state of art DP optimizers such as DPcpp [21] and DPhyp [22] are very difficult to parallelize due to tangled dependencies in the join pairs they generate. Unless the solution is very carefully devised, a lot of synchronization conflicts are bound to occur. By viewing a serial bottom-up optimizer as one which generates a totally ordered sequence of join pairs in a streaming fashion, we propose a novel concept of dependency-aware reordering, which minimizes waiting time caused by dependencies of join pairs. To maximize parallelism, we also introduce a series of novel performance optimization techniques: 1) pipelining of join pair generation and plan generation; 2) the synchronization-free global MEMO; and 3) threading across dependencies. Through extensive experiments with various query topologies, we show that our solution supports any type of bottom up optimization, achieving linear speedup for each type. Despite the fact that our solution is generic, due to sophisticated optimization techniques, our generic parallel optimizer outperforms PDPsva tailored to size-driven enumeration. Experimental results also show that our solution is much more robust than PDPsva with respect to search space allocation.

#index 1217151
#* Query processing techniques for solid state drives
#@ Dimitris Tsirogiannis;Stavros Harizopoulos;Mehul A. Shah;Janet L. Wiener;Goetz Graefe
#t 2009
#c 5
#% 3771
#% 136740
#% 172331
#% 286258
#% 427195
#% 451767
#% 565473
#% 571059
#% 824697
#% 893129
#% 960238
#% 1063542
#% 1063551
#% 1092673
#% 1127390
#% 1127391
#% 1127428
#% 1129952
#% 1129953
#% 1207002
#! Solid state drives perform random reads more than 100x faster than traditional magnetic hard disks, while offering comparable sequential read and write bandwidth. Because of their potential to speed up applications, as well as their reduced power consumption, these new drives are expected to gradually replace hard disks as the primary permanent storage media in large data centers. However, although they may benefit applications that stress random reads immediately, they may not improve database applications, especially those running long data analysis queries. Database query processing engines have been designed around the speed mismatch between random and sequential I/O on hard disks and their algorithms currently emphasize sequential accesses for disk-resident data. In this paper, we investigate data structures and algorithms that leverage fast random reads to speed up selection, projection, and join operations in relational query processing. We first demonstrate how a column-based layout within each page reduces the amount of data read during selections and projections. We then introduce FlashJoin, a general pipelined join algorithm that minimizes accesses to base and intermediate relational data. FlashJoin's binary join kernel accesses only the join attributes, producing partial results in the form of a join index. Subsequently, its fetch kernel retrieves the attributes for later nodes in the query plan as they are needed. FlashJoin significantly reduces memory and I/O requirements for each join in the query. We implemented these techniques inside Postgres and experimented with an enterprise SSD drive. Our techniques improved query runtimes by up to 6x for queries ranging from simple relational scans and joins to full TPC-H queries.

#index 1217152
#* FlashLogging: exploiting flash devices for synchronous logging performance
#@ Shimin Chen
#t 2009
#c 5
#% 210179
#% 248825
#% 273894
#% 399766
#% 403195
#% 745444
#% 810043
#% 829901
#% 854366
#% 893147
#% 921816
#% 951778
#% 960238
#% 960269
#% 978390
#% 1022298
#% 1052068
#% 1063543
#% 1063551
#% 1127391
#% 1127428
#% 1129952
#% 1129953
#% 1174228
#% 1468269
#! Synchronous transactional logging is the central mechanism for ensuring data persistency and recoverability in database systems. Unfortunately, magnetic disks are ill-suited for the small sequential write pattern of synchronous logging. Alternative solutions (e.g., backup servers or sophisticated battery-backed write caches in high-end disk arrays) are either expensive or complicated. In this paper, we exploit flash devices for synchronous logging based on the observation that flash devices support small sequential writes well. Comparing a wide variety of flash devices, we find that USB flash drives are a good match for this task because of its unique characteristics: widely available USB ports, hot-plug capability useful for coping with flash wear, and low price so that multiple drives are affordable. We propose FlashLogging, a logging solution that exploits multiple (USB) flash drives for synchronous logging. We identify and address four challenges: (i) efficiently exploiting multiple flash drives for logging; (ii) coping with the large variance of write latencies because of device erasure operations; (iii) efficient recovery processing; and (iv) combining flash drives and disks for better logging and recovery performance. We implemented our solution within MySQL-InnoDB. Our real machine experiments running online transaction processing workloads (TPCC) show that FlashLogging achieves up to 5.7X improvements over magnetic-disk-based logging, and obtains up to 98.6% of the ideal performance. We further compare our design with one that uses Solid-State Drives (SSDs), and find that although SSDs improve logging performance, multiple USB flash drives can achieve comparable or better performance with much lower price.

#index 1217153
#* Efficiently incorporating user feedback into information extraction and integration programs
#@ Xiaoyong Chai;Ba-Quy Vuong;AnHai Doan;Jeffrey F. Naughton
#t 2009
#c 5
#% 13016
#% 201929
#% 286836
#% 286929
#% 333990
#% 480483
#% 765409
#% 782759
#% 801668
#% 845350
#% 874992
#% 893193
#% 960365
#% 1022235
#% 1022258
#% 1022288
#% 1063532
#% 1063533
#% 1063547
#% 1127409
#% 1166537
#% 1183369
#% 1183370
#% 1183373
#% 1206687
#% 1206701
#% 1206800
#% 1217171
#! Many applications increasingly employ information extraction and integration (IE/II) programs to infer structures from unstructured data. Automatic IE/II are inherently imprecise. Hence such programs often make many IE/II mistakes, and thus can significantly benefit from user feedback. Today, however, there is no good way to automatically provide and process such feedback. When finding an IE/II mistake, users often must alert the developer team (e.g., via email or Web form) about the mistake, and then wait for the team to manually examine the program internals to locate and fix the mistake, a slow, error-prone, and frustrating process. In this paper we propose a solution for users to directly provide feedback and for IE/II programs to automatically process such feedback. In our solution a developer U uses hlog, a declarative IE/II language, to write an IE/II program P. Next, U writes declarative user feedback rules that specify which parts of P's data (e.g., input, intermediate, or output data) users can edit, and via which user interfaces. Next, the so-augmented program P is executed, then enters a loop of waiting for and incorporating user feedback. Given user feedback F on a data portion of P, we show how to automatically propagate F to the rest of P, and to seamlessly combine F with prior user feedback. We describe the syntax and semantics of hlog, a baseline execution strategy, and then various optimization techniques. Finally, we describe experiments with real-world data that demonstrate the promise of our solution.

#index 1217154
#* Uncertainty management in rule-based information extraction systems
#@ Eirinaios Michelakis;Rajasekar Krishnamurthy;Peter J. Haas;Shivakumar Vaithyanathan
#t 2009
#c 5
#% 226495
#% 283136
#% 314739
#% 450454
#% 464434
#% 815876
#% 815915
#% 854646
#% 854813
#% 855020
#% 893168
#% 903014
#% 936822
#% 938763
#% 939909
#% 983912
#% 1016201
#% 1022288
#% 1036075
#% 1063521
#% 1089602
#% 1127378
#% 1127415
#% 1183368
#% 1206687
#% 1206717
#% 1264785
#% 1269815
#% 1279274
#% 1279275
#% 1290034
#% 1700554
#% 1705178
#! Rule-based information extraction is a process by which structured objects are extracted from text based on user-defined rules. The compositional nature of rule-based information extraction also allows rules to be expressed over previously extracted objects. Such extraction is inherently uncertain, due to the varying precision associated with the rules used in a specific extraction task. Quantifying this uncertainty is crucial for querying the extracted objects in probabilistic databases, and for improving the recall of extraction tasks that use compositional rules. In this paper, we provide a probabilistic framework for handling the uncertainty in rule-based information extraction. Specifically, for each extraction task, we build a parametric exponential model of uncertainty that captures the interaction between the different rules, as well as the compositional nature of the rules; the exponential form of our model follows from maximum-entropy considerations. We also give model-decomposition techniques that make the learning algorithms scalable to large numbers of rules and constraints. Experiments over multiple real-world extraction tasks confirm that our approach yields accurate probability estimates with only a small performance overhead. Moreover, our framework supports incremental pay-as-you-go improvements in the accuracy of probability estimates as new rules, data, or constraints are added.

#index 1217155
#* Skip-and-prune: cosine-based top-k query processing for efficient context-sensitive document retrieval
#@ Jong Wook Kim;K. Selçuk Candan
#t 2009
#c 5
#% 68717
#% 115465
#% 249989
#% 278831
#% 287466
#% 319273
#% 321635
#% 406493
#% 465167
#% 480671
#% 643566
#% 805877
#% 806212
#% 875001
#% 1011871
#% 1015258
#% 1015317
#% 1016183
#% 1022234
#% 1022278
#% 1063474
#! Keyword search and ranked retrieval together emerged as popular data access paradigms for various kinds of data, from web pages to XML and relational databases. A user can submit keywords without knowing much (sometimes nothing) about the complex structure underlying a data collection, yet the system can identify, rank, and return a set of relevant matches by exploiting statistics about the distribution and structure of the data. Keyword-based data models are also suitable for capturing user's search context in terms of weights associated to the keywords in the query. Given a search context, the data in the database can also be re-interpreted for semantically correct retrieval. This option, however, is often ignored as the cost of re-assessing the content in the database naively tends to be prohibitive. In this paper, we first argue that top-k query processing can help tackle this challenge by re-assessing only the relevant parts of the database, efficiently. A road-block in this process, however, is that most efficient implementations of top-k query processing assume that the scoring function is monotonic, whereas the cosine-based scoring function needed for re-interpretation of content based on user context is not. In this paper, we develop an efficient top-k query processing algorithm, skip-and-prune (SnP), which is able to process top-k queries under cosine-based non-monotonic scoring functions. We compare the use of proposed algorithm against the alternative implementations of the context-aware retrieval, including naive top-k, accumulator-based inverted files, and full-scan. The experiment results show that while being fast, naive top-k is not an effective solution due to the non-monotonicity of underlying scoring function. The proposed technique, SnP, however, matches the precision of accumulator-based inverted files and full-scan, yet it is orders of magnitude faster than these.

#index 1217156
#* Attacks on privacy and deFinetti's theorem
#@ Daniel Kifer
#t 2009
#c 5
#% 243299
#% 246831
#% 297186
#% 576111
#% 576761
#% 727904
#% 765449
#% 798509
#% 800515
#% 810010
#% 810011
#% 810012
#% 864390
#% 864406
#% 874892
#% 874988
#% 881507
#% 883233
#% 893100
#% 937550
#% 956511
#% 956557
#% 1000526
#% 1016201
#% 1022246
#% 1022247
#% 1022265
#% 1022266
#% 1063504
#% 1073969
#% 1080356
#% 1127360
#% 1206745
#% 1650665
#% 1670071
#! In this paper we present a method for reasoning about privacy using the concepts of exchangeability and deFinetti's theorem. We illustrate the usefulness of this technique by using it to attack a popular data sanitization scheme known as Anatomy. We stress that Anatomy is not the only sanitization scheme that is vulnerable to this attack. In fact, any scheme that uses the random worlds model, i.i.d. model, or tuple-independent model needs to be re-evaluated. The difference between the attack presented here and others that have been proposedin the past is that we do not need extensive background knowledge. An attacker only needs to know the nonsensitive attributes of one individual in the data, and can carry out this attack just by building a machine learning model over the sanitized data. The reason this attack is successful is that it exploits a subtle flaw in the way prior work computed the probability of disclosure of a sensitive attribute. We demonstrate this theoretically, empirically, and with intuitive examples. We also discuss how this generalizes to many other privacy schemes.

#index 1217157
#* Secure kNN computation on encrypted databases
#@ Wai Kit Wong;David Wai-lok Cheung;Ben Kao;Nikos Mamoulis
#t 2009
#c 5
#% 300184
#% 345132
#% 397367
#% 576761
#% 577233
#% 765448
#% 812799
#% 849461
#% 864412
#% 864413
#% 893127
#% 893151
#% 960242
#% 1063478
#% 1206766
#% 1409349
#% 1663641
#! Service providers like Google and Amazon are moving into the SaaS (Software as a Service) business. They turn their huge infrastructure into a cloud-computing environment and aggressively recruit businesses to run applications on their platforms. To enforce security and privacy on such a service model, we need to protect the data running on the platform. Unfortunately, traditional encryption methods that aim at providing "unbreakable" protection are often not adequate because they do not support the execution of applications such as database queries on the encrypted data. In this paper we discuss the general problem of secure computation on an encrypted database and propose a SCONEDB Secure Computation ON an Encrypted DataBase) model, which captures the execution and security requirements. As a case study, we focus on the problem of k-nearest neighbor (kNN) computation on an encrypted database. We develop a new asymmetric scalar-product-preserving encryption (ASPE) that preserves a special type of scalar product. We use APSE to construct two secure schemes that support kNN computation on encrypted data; each of these schemes is shown to resist practical attacks of a different background knowledge level, at a different overhead cost. Extensive performance studies are carried out to evaluate the overhead and the efficiency of the schemes.

#index 1217158
#* Privacy preservation of aggregates in hidden databases: why and how?
#@ Arjun Dasgupta;Nan Zhang;Gautam Das;Surajit Chaudhuri
#t 2009
#c 5
#% 204453
#% 268114
#% 300184
#% 340146
#% 340827
#% 397378
#% 575969
#% 576761
#% 586838
#% 654448
#% 740764
#% 783692
#% 809244
#% 810028
#% 869499
#% 893101
#% 907563
#% 937550
#% 943875
#% 956534
#% 960286
#% 964073
#% 993964
#% 1127393
#% 1127557
#% 1206906
#% 1740518
#! Many websites provide form-like interfaces which allow users to execute search queries on the underlying hidden databases. In this paper, we explain the importance of protecting sensitive aggregate information of hidden databases from being disclosed through individual tuples returned by the search queries. This stands in contrast to the traditional privacy problem where individual tuples must be protected while ensuring access to aggregating information. We propose techniques to thwart bots from sampling the hidden database to infer aggregate information. We present theoretical analysis and extensive experiments to illustrate the effectiveness of our approach.

#index 1217159
#* A comparison of approaches to large-scale data analysis
#@ Andrew Pavlo;Erik Paulson;Alexander Rasin;Daniel J. Abadi;David J. DeWitt;Samuel Madden;Michael Stonebraker
#t 2009
#c 5
#% 479905
#% 479920
#% 723279
#% 750960
#% 875029
#% 925912
#% 963669
#% 983467
#% 993387
#% 1023419
#% 1063553
#% 1127559
#! There is currently considerable enthusiasm around the MapReduce (MR) paradigm for large-scale data analysis [17]. Although the basic control flow of this framework has existed in parallel SQL database management systems (DBMS) for over 20 years, some have called MR a dramatically new computing model [8, 17]. In this paper, we describe and compare both paradigms. Furthermore, we evaluate both kinds of systems in terms of performance and development complexity. To this end, we define a benchmark consisting of a collection of tasks that we have run on an open source version of MR as well as on two parallel DBMSs. For each task, we measure each system's performance for various degrees of parallelism on a cluster of 100 nodes. Our results reveal some interesting trade-offs. Although the process to load data into and tune the execution of parallel DBMSs took much longer than the MR system, the observed performance of these DBMSs was strikingly better. We speculate about the causes of the dramatic performance difference and consider implementation concepts that future systems should take from both kinds of architectures.

#index 1217160
#* Asynchronous view maintenance for VLSD databases
#@ Parag Agrawal;Adam Silberstein;Brian F. Cooper;Utkarsh Srivastava;Raghu Ramakrishnan
#t 2009
#c 5
#% 13016
#% 116086
#% 152928
#% 201928
#% 210210
#% 227944
#% 227947
#% 300141
#% 340300
#% 340301
#% 480623
#% 791180
#% 800501
#% 800562
#% 857498
#% 978404
#% 998845
#% 1022221
#% 1061893
#% 1127560
#% 1237169
#! The query models of the recent generation of very large scale distributed (VLSD) shared-nothing data storage systems, including our own PNUTS and others (e.g. BigTable, Dynamo, Cassandra, etc.) are intentionally simple, focusing on simple lookups and scans and trading query expressiveness for massive scale. Indexes and views can expand the query expressiveness of such systems by materializing more complex access paths and query results. In this paper, we examine mechanisms to implement indexes and views in a massive scale distributed database. For web applications, minimizing update latencies is critical, so we advocate deferring the work of maintaining views and indexes as much as possible. We examine the design space, and conclude that two types of view implementations, called remote view tables (RVTs) and local view tables (LVTs), provide good tradeoff between system throughput and minimizing view staleness. We describe how to construct and maintain such view tables, and how they can be used to implement indexes, group-by-aggregate views, equijoin views and selection views. We also introduce and analyze a consistency model that makes it easier for application developers to cope with the impact of deferred view maintenance. An empirical evaluation quantifies the maintenance costs of our views, and shows that they can significantly improve the cost of evaluating complex queries.

#index 1217161
#* ZStream: a cost-based query processor for adaptively detecting composite events
#@ Yuan Mei;Samuel Madden
#t 2009
#c 5
#% 37972
#% 333938
#% 397353
#% 411554
#% 480621
#% 481448
#% 763881
#% 765437
#% 875004
#% 1063480
#% 1063481
#! Composite (or Complex) event processing (CEP) systems search sequences of incoming events for occurrences of user-specified event patterns. Recently, they have gained more attention in a variety of areas due to their powerful and expressive query language and performance potential. Sequentiality (temporal ordering) is the primary way in which CEP systems relate events to each other. In this paper, we present a CEP system called ZStream to efficiently process such sequential patterns. Besides simple sequential patterns, ZStream is also able to detect other patterns, including conjunction, disjunction, negation and Kleene closure. Unlike most recently proposed CEP systems, which use non-deterministic finite automata (NFA's) to detect patterns, ZStream uses tree-based query plans for both the logical and physical representation of query patterns. By carefully designing the underlying infrastructure and algorithms, ZStream is able to unify the evaluation of sequence, conjunction, disjunction, negation, and Kleene closure as variants of the join operator. Under this framework, a single pattern in ZStream may have several equivalent physical tree plans, with different evaluation costs. We propose a cost model to estimate the computation costs of a plan. We show that our cost model can accurately capture the actual runtime behavior of a plan, and that choosing the optimal plan can result in a factor of four or more speedup versus an NFA based approach. Based on this cost model and using a simple set of statistics about operator selectivity and data rates, ZStream is able to adaptively and seamlessly adjust the order in which it detects patterns on the fly. Finally, we describe a dynamic programming algorithm used in our cost model to efficiently search for an optimal query plan for a given pattern.

#index 1217162
#* Exploiting context analysis for combining multiple entity resolution systems
#@ Zhaoqi Chen;Dmitri V. Kalashnikov;Sharad Mehrotra
#t 2009
#c 5
#% 132938
#% 201889
#% 251145
#% 310516
#% 460812
#% 577238
#% 577247
#% 577263
#% 722902
#% 729913
#% 766199
#% 799701
#% 800530
#% 803762
#% 805885
#% 810014
#% 818275
#% 819550
#% 838435
#% 844317
#% 871766
#% 915273
#% 915340
#% 916786
#% 926881
#% 967274
#% 1074054
#% 1119132
#% 1206842
#% 1271267
#% 1271313
#% 1390383
#% 1408793
#! Entity Resolution (ER) is an important real world problem that has attracted significant research interest over the past few years. It deals with determining which object descriptions co-refer in a dataset. Due to its practical significance for data mining and data analysis tasks many different ER approaches has been developed to address the ER challenge. This paper proposes a new ER Ensemble framework. The task of ER Ensemble is to combine the results of multiple base-level ER systems into a single solution with the goal of increasing the quality of ER. The framework proposed in this paper leverages the observation that often no single ER method always performs the best, consistently outperforming other ER techniques in terms of quality. Instead, different ER solutions perform better in different contexts. The framework employs two novel combining approaches, which are based on supervised learning. The two approaches learn a mapping of the clustering decisions of the base-level ER systems, together with the local context, into a combined clustering decision. The paper empirically studies the framework by applying it to different domains. The experiments demonstrate that the proposed framework achieves significantly higher disambiguation quality compared to the current state of the art solutions.

#index 1217163
#* Entity resolution with iterative blocking
#@ Steven Euijong Whang;David Menestrina;Georgia Koutrika;Martin Theobald;Hector Garcia-Molina
#t 2009
#c 5
#% 55366
#% 201889
#% 310516
#% 328186
#% 329790
#% 350103
#% 420072
#% 460812
#% 577238
#% 766199
#% 800590
#% 810014
#% 913783
#% 915242
#% 1196287
#% 1201863
#% 1217163
#% 1250576
#! Entity Resolution (ER) is the problem of identifying which records in a database refer to the same real-world entity. An exhaustive ER process involves computing the similarities between pairs of records, which can be very expensive for large datasets. Various blocking techniques can be used to enhance the performance of ER by dividing the records into blocks in multiple ways and only comparing records within the same block. However, most blocking techniques process blocks separately and do not exploit the results of other blocks. In this paper, we propose an iterative blocking framework where the ER results of blocks are reflected to subsequently processed blocks. Blocks are now iteratively processed until no block contains any more matching records. Compared to simple blocking, iterative blocking may achieve higher accuracy because reflecting the ER results of blocks to other blocks may generate additional record matches. Iterative blocking may also be more efficient because processing a block now saves the processing time for other blocks. We implement a scalable iterative blocking system and demonstrate that iterative blocking can be more accurate and efficient than blocking for large datasets.

#index 1217164
#* A grammar-based entity representation framework for data cleaning
#@ Arvind Arasu;Raghav Kaushik
#t 2009
#c 5
#% 3888
#% 36683
#% 314740
#% 333943
#% 410276
#% 654467
#% 729913
#% 769884
#% 818244
#% 875066
#% 913783
#% 960263
#% 1055735
#% 1063530
#% 1092530
#% 1206615
#% 1669469
#! Fundamental to data cleaning is the need to account for multiple data representations. We propose a formal framework that can be used to reason about and manipulate data representations. The framework is declarative and combines elements of a generative grammar with database querying. It also incorporates actions in the spirit of programming language compilers. This framework has multiple applications such as parsing and data normalization. Data normalization is interesting in its own right in preparing data for analysis as well as in pre-processing data for further cleansing. We empirically study the utility of the framework over several real-world data cleaning scenarios and find that with the right normalization, often the need for further cleansing is minimized.

#index 1217165
#* Generating example data for dataflow programs
#@ Christopher Olston;Shubham Chopra;Utkarsh Srivastava
#t 2009
#c 5
#% 11813
#% 89758
#% 273908
#% 346656
#% 419400
#% 462072
#% 481102
#% 598499
#% 726621
#% 960262
#% 963669
#% 983467
#% 1063553
#! While developing data-centric programs, users often run (portions of) their programs over real data, to see how they behave and what the output looks like. Doing so makes it easier to formulate, understand and compose programs correctly, compared with examination of program logic alone. For large input data sets, these experimental runs can be time-consuming and inefficient. Unfortunately, sampling the input data does not always work well, because selective operations such as filter and join can lead to empty results over sampled inputs, and unless certain indexes are present there is no way to generate biased samples efficiently. Consequently new methods are needed for generating example input data for data-centric programs. We focus on an important category of data-centric programs, dataflow programs, which are best illustrated by displaying the series of intermediate data tables that occur between each pair of operations. We introduce and study the problem of generating example intermediate data for dataflow programs, in a manner that illustrates the semantics of the operators while keeping the example data small. We identify two major obstacles that impede naive approaches, namely (1) highly selective operators and (2) noninvertible operators, and offer techniques for dealing with these obstacles. Our techniques perform well on real dataflow programs used at Yahoo! for web analytics.

#index 1217166
#* A framework for testing query transformation rules
#@ Hicham G. Elmongui;Vivek Narasayya;Ravishankar Ramamurthy
#t 2009
#c 5
#% 69389
#% 116043
#% 248014
#% 341672
#% 397390
#% 408396
#% 479656
#% 481925
#% 565457
#% 902467
#% 960262
#% 1016220
#% 1022307
#% 1063507
#% 1070308
#! In order to enable extensibility, modern query optimizers typically leverage a transformation rule based framework. Testing individual rule correctness as well as correctness of rule interactions is crucial in verifying the functionality of a query optimizer. While there has been a lot of work on how to architect optimizers for extensibility using a rule based framework, there has been relatively little work on how to test such optimizers. In this paper we present a framework for testing query transformation rules which enables: (a) efficient generation of queries that exercise a particular transformation rule or a set of rules and (b) efficient execution of corresponding test suites for correctness testing.

#index 1217167
#* Cross-tier, label-based security enforcement for web applications
#@ Brian J. Corcoran;Nikhil Swamy;Michael Hicks
#t 2009
#c 5
#% 202152
#% 320629
#% 323619
#% 465001
#% 736843
#% 765447
#% 818384
#% 875015
#% 875029
#% 998833
#% 1051896
#% 1080342
#% 1216556
#% 1259853
#% 1404050
#! This paper presents SELinks, a programming language focused on building secure multi-tier web applications. SELinks provides a uniform programming model, in the style of LINQ and Ruby on Rails, with language syntax for accessing objects residing either in the database or at the server. Object-level security policies are expressed as fully-customizable, first-class labels which may themselves be subject to security policies. Access to labeled data is mediated via trusted, user-provided policy enforcement functions. SELinks has two novel features that ensure security policies are enforced correctly and efficiently. First, SELinks implements a type system called Fable that allows a protected object's type to refer to its protecting label. The type system can check that labeled data is never accessed directly by the program without first consulting the appropriate policy enforcement function. Second, SELinks compiles policy enforcement code to database-resident user-defined functions that can be called directly during query processing. Database-side checking avoids transferring data to the server needlessly, while still allowing policies to be expressed in a customizable and portable manner. Our experience with two sizable web applications, a modelhealth-care database and a secure wiki with fine-grained security policies, indicates that cross-tier policy enforcement in SELinks is flexible, relatively easy to use, and, when compared to a single-tier approach, improves throughput by nearly an order of magnitude. SELinks is freely available.

#index 1217168
#* Dictionary-based order-preserving string compression for main memory column stores
#@ Carsten Binnig;Stefan Hildenbrand;Franz Färber
#t 2009
#c 5
#% 252608
#% 282232
#% 287715
#% 288578
#% 290703
#% 300194
#% 319671
#% 333942
#% 333953
#% 428152
#% 442832
#% 461896
#% 464843
#% 464987
#% 479819
#% 824697
#% 864446
#% 875026
#% 893129
#% 910582
#% 984065
#% 1015288
#% 1739419
#! Column-oriented database systems [19, 23] perform better than traditional row-oriented database systems on analytical workloads such as those found in decision support and business intelligence applications. Moreover, recent work [1, 24] has shown that lightweight compression schemes significantly improve the query processing performance of these systems. One such a lightweight compression scheme is to use a dictionary in order to replace long (variable-length) values of a certain domain with shorter (fixedlength) integer codes. In order to further improve expensive query operations such as sorting and searching, column-stores often use order-preserving compression schemes. In contrast to the existing work, in this paper we argue that orderpreserving dictionary compression does not only pay off for attributes with a small fixed domain size but also for long string attributes with a large domain size which might change over time. Consequently, we introduce new data structures that efficiently support an order-preserving dictionary compression for (variablelength) string attributes with a large domain size that is likely to change over time. The main idea is that we model a dictionary as a table that specifies a mapping from string-values to arbitrary integer codes (and vice versa) and we introduce a novel indexing approach that provides efficient access paths to such a dictionary while compressing the index data. Our experiments show that our data structures are as fast as (or in some cases even faster than) other state-of-the-art data structures for dictionaries while being less memory intensive.

#index 1217169
#* Self-organizing tuple reconstruction in column-stores
#@ Stratos Idreos;Martin L. Kersten;Stefan Manegold
#t 2009
#c 5
#% 824697
#% 875026
#% 875062
#% 893129
#% 893130
#% 960268
#% 1016186
#% 1016220
#! Column-stores gained popularity as a promising physical design alternative. Each attribute of a relation is physically stored as a separate column allowing queries to load only the required attributes. The overhead incurred is on-the-fly tuple reconstruction for multi-attribute queries. Each tuple reconstruction is a join of two columns based on tuple IDs, making it a significant cost component. The ultimate physical design is to have multiple presorted copies of each base table such that tuples are already appropriately organized in multiple different orders across the various columns. This requires the ability to predict the workload, idle time to prepare, and infrequent updates. In this paper, we propose a novel design, partial sideways cracking, that minimizes the tuple reconstruction cost in a self-organizing way. It achieves performance similar to using presorted data, but without requiring the heavy initial presorting step itself. Instead, it handles dynamic, unpredictable workloads with no idle time and frequent updates. Auxiliary dynamic data structures, called cracker maps, provide a direct mapping between pairs of attributes used together in queries for tuple reconstruction. A map is continuously physically reorganized as an integral part of query evaluation, providing faster and reduced data access for future queries. To enable flexible and self-organizing behavior in storage-limited environments, maps are materialized only partially as demanded by the workload. Each map is a collection of separate chunks that are individually reorganized, dropped or recreated as needed. We implemented partial sideways cracking in an open-source column-store. A detailed experimental analysis demonstrates that it brings significant performance benefits for multi-attribute queries.

#index 1217170
#* An architecture for recycling intermediates in a column-store
#@ Milena G. Ivanova;Martin L. Kersten;Niels J. Nes;Romulo A.P. Gonçalves
#t 2009
#c 5
#% 169844
#% 300166
#% 333962
#% 333965
#% 342955
#% 397398
#% 442850
#% 465151
#% 480158
#% 745536
#% 864446
#% 960278
#% 982557
#% 997495
#% 1021950
#% 1044448
#% 1089604
#% 1130823
#% 1206730
#% 1394485
#! Automatically recycling (intermediate) results is a grand challenge for state-of-the-art databases to improve both query response time and throughput. Tuples are loaded and streamed through a tuple-at-a-time processing pipeline avoiding materialization of intermediates as much as possible. This limits the opportunities for reuse of overlapping computations to DBA-defined materialized views and function/result cache tuning. In contrast, the operator-at-a-time execution paradigm produces fully materialized results in each step of the query plan. To avoid resource contention, these intermediates are evicted as soon as possible. In this paper we study an architecture that harvests the by-products of the operator-at-a-time paradigm in a column store system using a lightweight mechanism, the recycler. The key challenge then becomes selection of the policies to admit intermediates to the resource pool, their retention period, and the eviction strategy when facing resource limitations. The proposed recycling architecture has been implemented in an open-source system. An experimental analysis against the TPC-H ad-hoc decision support benchmark and a complex, real-world application (SkyServer) demonstrates its effectiveness in terms of self-organizing behavior and its significant performance gains. The results indicate the potentials of recycling intermediates and charters a route for further development of database kernels.

#index 1217171
#* Optimizing complex extraction programs over evolving text data
#@ Fei Chen;Byron J. Gao;AnHai Doan;Jun Yang;Raghu Ramakrishnan
#t 2009
#c 5
#% 279164
#% 330604
#% 577310
#% 731406
#% 782759
#% 810108
#% 874992
#% 875064
#% 956535
#% 1019061
#% 1022235
#% 1022288
#% 1022289
#% 1166537
#% 1183369
#% 1183373
#% 1206687
#% 1206701
#% 1270363
#% 1392437
#! Most information extraction (IE) approaches have considered only static text corpora, over which we apply IE only once. Many real-world text corpora however are dynamic. They evolve over time, and so to keep extracted information up to date we often must apply IE repeatedly, to consecutive corpus snapshots. Applying IE from scratch to each snapshot can take a lot of time. To avoid doing this, we have recently developed Cyclex, a system that recycles previous IE results to speed up IE over subsequent corpus snapshots. Cyclex clearly demonstrated the promise of the recycling idea. The work itself however is limited in that it considers only IE programs that contain a single IE ``blackbox.'' In practice, many IE programs are far more complex, containing multiple IE blackboxes connected in a compositional ``workflow.'' In this paper, we present Delex, a system that removes the above limitation. First we identify many difficult challenges raised by Delex, including modeling complex IE programs for recycling purposes, implementing the recycling process efficiently, and searching for an optimal execution plan in a vast plan space with different recycling alternatives. Next we describe our solutions to these challenges. Finally, we describe extensive experiments with both rule-based and learning-based IE programs over two real-world data sets, which demonstrate the utility of our approach.

#index 1217172
#* Robust web extraction: an approach based on a probabilistic tree-edit model
#@ Nilesh Dalvi;Philip Bohannon;Fei Sha
#t 2009
#c 5
#% 66654
#% 275915
#% 348146
#% 428148
#% 465765
#% 479807
#% 480648
#% 480824
#% 615768
#% 824737
#% 826007
#% 869575
#% 875031
#% 893206
#% 940343
#% 1124449
#% 1271981
#% 1665128
#% 1700500
#! On script-generated web sites, many documents share common HTML tree structure, allowing wrappers to effectively extract information of interest. Of course, the scripts and thus the tree structure evolve over time, causing wrappers to break repeatedly, and resulting in a high cost of maintaining wrappers. In this paper, we explore a novel approach: we use temporal snapshots of web pages to develop a tree-edit model of HTML, and use this model to improve wrapper construction. We view the changes to the tree structure as suppositions of a series of edit operations: deleting nodes, inserting nodes and substituting labels of nodes. The tree structures evolve by choosing these edit operations stochastically. Our model is attractive in that the probability that a source tree has evolved into a target tree can be estimated efficiently--in quadratic time in the size of the trees--making it a potentially useful tool for a variety of tree-evolution problems. We give an algorithm to learn the probabilistic model from training examples consisting of pairs of trees, and apply this algorithm to collections of web-page snapshots to derive HTML-specific tree edit models. Finally, we describe a novel wrapper-construction framework that takes the tree-edit model into account, and compare the quality of resulting wrappers to that of traditional wrappers on synthetic and real HTML document examples.

#index 1217173
#* Combining keyword search and forms for ad hoc querying of databases
#@ Eric Chu;Akanksha Baid;Xiaoyong Chai;AnHai Doan;Jeffrey Naughton
#t 2009
#c 5
#% 55914
#% 660011
#% 765464
#% 864512
#% 875017
#% 893172
#% 990386
#% 993987
#% 1015325
#% 1044470
#% 1127406
#! A common criticism of database systems is that they are hard to query for users uncomfortable with a formal query language. To address this problem, form-based interfaces and keyword search have been proposed; while both have benefits, both also have limitations. In this paper, we investigate combining the two with the hopes of creating an approach that provides the best of both. Specifically, we propose to take as input a target database and then generate and index a set of query forms offline. At query time, a user with a question to be answered issues standard keyword search queries; but instead of returning tuples, the system returns forms relevant to the question. The user may then build a structured query with one of these forms and submit it back to the system for evaluation. In this paper, we address challenges that arise in form generation, keyword search over forms, and ranking and displaying these forms. We explore techniques to tackle these challenges, and present experimental results suggesting that the approach of combining keyword search and form-based interfaces is promising.

#index 1217174
#* Ranking distributed probabilistic data
#@ Feifei Li;Ke Yi;Jeffrey Jestes
#t 2009
#c 5
#% 333854
#% 654443
#% 654467
#% 654487
#% 766671
#% 768521
#% 800582
#% 810009
#% 810098
#% 822531
#% 824704
#% 824709
#% 864394
#% 864455
#% 874995
#% 893189
#% 960274
#% 960293
#% 982752
#% 992830
#% 1016178
#% 1044478
#% 1063520
#% 1063521
#% 1070886
#% 1127376
#% 1127415
#% 1206717
#% 1206765
#% 1206772
#% 1206781
#% 1206893
#! Ranking queries are essential tools to process large amounts of probabilistic data that encode exponentially many possible deterministic instances. In many applications where uncertainty and fuzzy information arise, data are collected from multiple sources in distributed, networked locations, e.g., distributed sensor fields with imprecise measurements, multiple scientific institutes with inconsistency in their scientific data. Due to the network delay and the economic cost associated with communicating large amounts of data over a network, a fundamental problem in these scenarios is to retrieve the global top-k tuples from all distributed sites with minimum communication cost. Using the well founded notion of the expected rank of each tuple across all possible worlds as the basis of ranking, this work designs both communication- and computation-efficient algorithms for retrieving the top-k tuples with the smallest ranks from distributed sites. Extensive experiments using both synthetic and real data sets confirm the efficiency and superiority of our algorithms over the straightforward approach of forwarding all data to the server.

#index 1217175
#* Top-k queries on uncertain data: on score distribution and typical answers
#@ Tingjian Ge;Stan Zdonik;Samuel Madden
#t 2009
#c 5
#% 115608
#% 644560
#% 654487
#% 799140
#% 801673
#% 906944
#% 976984
#% 1016201
#% 1063520
#% 1063521
#% 1075132
#% 1127375
#% 1147662
#% 1206717

#index 1217176
#* Secondary-storage confidence computation for conjunctive queries with inequalities
#@ Dan Olteanu;Jiewen Huang
#t 2009
#c 5
#% 314829
#% 386158
#% 893167
#% 976984
#% 977013
#% 992830
#% 1063521
#% 1068580
#% 1127376
#% 1206987
#% 1217251
#% 1272349
#! This paper investigates the problem of efficiently computing the confidences of distinct tuples in the answers to conjunctive queries with inequalities ( Our contributions are of both theoretical and practical importance. We define a class of tractable queries with inequalities, and generalize existing results on #P-hardness of query evaluation, now in the presence of inequalities. For the tractable queries, we introduce a confidence computation technique based on efficient compilation of the lineage of the query answer into Ordered Binary Decision Diagrams (OBDDs), whose sizes are linear in the number of variables of the lineage. We implemented a secondary-storage variant of our technique in PostgreSQL. This variant does not need to materialize the OBDD, but computes, in one scan over the lineage, the probabilities of OBDD fragments and combines them on the fly. Experiments with probabilistic TPC-H data show up to two orders of magnitude improvements when compared with state-of-the-art approaches.

#index 1217177
#* Query simplification: graceful degradation for join-order optimization
#@ Thomas Neumann
#t 2009
#c 5
#% 554
#% 43161
#% 86949
#% 210166
#% 315024
#% 411554
#% 443409
#% 465165
#% 479938
#% 480430
#% 481429
#% 562668
#% 565457
#% 571091
#% 571094
#% 771952
#% 893165
#% 960299
#% 1063510
#! Join ordering is one of the most important, but also most challenging problems of query optimization. In general finding the optimal join order is NP-hard. Existing dynamic programming algorithms exhibit exponential runtime even for the restricted, but highly relevant class of star joins. Therefore, it is infeasible to find the optimal join order when the query includes a large number of joins. Existing approaches for large queries switch to greedy heuristics or randomized algorithms at some point, which can degrade query execution performance by orders of magnitude. We propose a new paradigm for optimizing large queries: when a query is too complex to be optimized exactly, we simplify the query's join graph until the optimization problem becomes tractable within a given time budget. During simplification, we apply safe simplifications before more risky ones. This way join ordering problems are solved optimally if possible, and gracefully degrade with increasing query complexity. This paper presents a general framework for query simplification and a strategy for directing the simplification process. Extensive experiments with different kinds of queries, different join-graph structures, and different cost functions indicate that query simplification is very robust and outperforms previous methods for join-order optimization.

#index 1217178
#* Robust and efficient algorithms for rank join evaluation
#@ Jonathan Finger;Neoklis Polyzotis
#t 2009
#c 5
#% 136740
#% 333854
#% 465167
#% 480819
#% 777931
#% 810018
#% 983263
#% 993954
#% 1022276
#% 1022277
#% 1022278
#% 1063713
#% 1070886
#% 1206972
#! In the rank join problem we are given a relational join R1 x R2 and a function that assigns numeric scores to the join tuples, and the goal is to return the tuples with the highest score. This problem lies at the core of processing top-k SQL queries, and recent studies have introduced specialized operators that solve the rank join problem by accessing only a subset of the input tuples. A desirable property for such operators is instance-optimality, i.e., their I/O cost should remain within a factor of the optimal for different inputs. However, a recent theoretical study has shown that existing rank join operators are not instance-optimal even though they have been shown to perform well empirically. The same study proposed the PBRJRRoverFR operator that was proved to be instance-optimal, but its performance was not tested empirically and in fact it was hinted that its complexity can be high. Thus, the following important question is raised: Is it possible to design a rank join operator that is both instance-optimal and computationally efficient? In this paper we provide an answer to this challenging question. We perform an empirical study of PBRJRRoverFR and show that its computational cost can offset the benefits of instance-optimality. Using the insights gained by the study, we develop the novel FRPA operator that addresses the efficiency bottlenecks of PBRJRRoverFR. We prove that FRPA is instance-optimal in general and more specifically that it never performs more I/O than PBRJRRoverFR. FRPA is the first operator that possesses these properties and is thus of interest in the theoretical study of rank join operators. We further identify cases where the overhead of FRPA becomes significant, and propose the FRPA operator that automatically adapts its overhead to the characteristics of the input. An extensive experimental study validates the effectiveness of the new operators and demonstrates that they offer significant performance improvements (up to an order of magnitude) over the state-of-the-art.

#index 1217179
#* Incremental maintenance of length normalized indexes for approximate string matching
#@ Marios Hadjieleftheriou;Nick Koudas;Divesh Srivastava
#t 2009
#c 5
#% 317933
#% 333854
#% 387427
#% 480654
#% 765463
#% 864392
#% 875066
#% 893164
#% 913783
#% 960263
#% 1016219
#% 1063496
#% 1206665
#% 1206677
#! Approximate string matching is a problem that has received a lot of attention recently. Existing work on information retrieval has concentrated on a variety of similarity measures TF/IDF, BM25, HMM, etc.) specifically tailored for document retrieval purposes. As new applications that depend on retrieving short strings are becoming popular(e.g., local search engines like YellowPages.com, Yahoo!Local, and Google Maps) new indexing methods are needed, tailored for short strings. For that purpose, a number of indexing techniques and related algorithms have been proposed based on length normalized similarity measures. A common denominator of indexes for length normalized measures is that maintaining the underlying structures in the presence of incremental updates is inefficient, mainly due to data dependent, precomputed weights associated with each distinct token and string. Incorporating updates usually is accomplished by rebuilding the indexes at regular time intervals. In this paper we present a framework that advocates lazy update propagation with the following key feature: Efficient, incremental updates that immediately reflect the new data in the indexes in a way that gives strict guarantees on the quality of subsequent query answers. More specifically, our techniques guarantee against false negatives and limit the number of false positives produced. We implement a fully working prototype and illustrate that the proposed ideas work really well in practice for real datasets.

#index 1217180
#* E = MC3: managing uncertain enterprise data in a cluster-computing environment
#@ Fei Xu;Kevin Beyer;Vuk Ercegovac;Peter J. Haas;Eugene J. Shekita
#t 2009
#c 5
#% 54968
#% 74151
#% 115661
#% 244608
#% 573023
#% 810098
#% 867065
#% 893189
#% 960326
#% 963669
#% 978404
#% 1063521
#% 1063568
#% 1127378
#% 1127560
#% 1183377
#% 1191550
#! Modern enterprises must manage uncertain data for purposes of risk assessment and decisionmaking under uncertainty. The Monte Carlo approach embodied in the MCDB system of Jampani et al. is well suited for such a task. MCDB can support industrial strength business-intelligence queries over uncertain warehouse data. Moreover, MCDB's extensible approach to specifying uncertainty can also capture complex stochastic prediction models, allowing sophisticated ``what-if'' analyses within the DBMS. The MCDB computations can be highly CPU intensive, but offer the potential for massive parallelization. To realize this potential, we provide a new system, called MC3 (Monte Carlo Computation on a Cluster), that extends the MCDB approach to the map-reduce processing framework. MC3 can exploit the robustness and scalability of map-reduce, and can handle data stored in non-relational formats. We show how MCDB query plans over ``tuple bundles'' can be translated to sequences of map-reduce operations over nested data, and describe different parallelization schemes. We also provide and analyze several novel distributed algorithms for adding pseudorandom number seeds to tuple bundles. These algorithms ensure statistical correctness of the Monte-Carlo computations while minimizing the seed length. Our experiments show that MC3 can scale well for a variety of workloads.

#index 1217181
#* Indexing correlated probabilistic databases
#@ Bhargav Kanagal;Amol Deshpande
#t 2009
#c 5
#% 663
#% 235023
#% 388024
#% 824728
#% 864417
#% 893168
#% 960292
#% 960293
#% 976984
#% 1016201
#% 1016202
#% 1022259
#% 1063523
#% 1127376
#% 1127378
#% 1127415
#% 1206717
#% 1206772
#% 1206877
#% 1207010
#% 1270250
#% 1289387
#! With large amounts of correlated probabilistic data being generated in a wide range of application domains including sensor networks, information extraction, event detection etc., effectively managing and querying them has become an important research direction. While there is an exhaustive body of literature on querying independent probabilistic data, supporting efficient queries over large-scale, correlated databases remains a challenge. In this paper, we develop efficient data structures and indexes for supporting inference and decision support queries over such databases. Our proposed hierarchical data structure is suitable both for in-memory and disk-resident databases. We represent the correlations in the probabilistic database using a junction tree over the tuple-existence or attribute-value random variables, and use tree partitioning techniques to build an index structure over it. We show how to efficiently answer inference and aggregation queries using such an index, resulting in orders of magnitude performance benefits in most cases. In addition, we develop novel algorithms for efficiently keeping the index structure up-to-date as changes (inserts, updates) are made to the probabilistic database. We present a comprehensive experimental study illustrating the benefits of our approach to query processing in probabilistic databases.

#index 1217182
#* Estimating the confidence of conditional functional dependencies
#@ Graham Cormode;Lukasz Golab;Korn Flip;Andrew McGregor;Divesh Srivastava;Xi Zhang
#t 2009
#c 5
#% 1331
#% 164368
#% 189872
#% 238182
#% 249238
#% 481779
#% 577261
#% 765455
#% 809258
#% 847112
#% 864393
#% 893137
#% 1015310
#% 1022222
#% 1022228
#% 1054480
#% 1127382
#% 1127443
#% 1206764
#% 1206961
#% 1700144
#! Conditional functional dependencies (CFDs) have recently been proposed as extensions of classical functional dependencies that apply to a certain subset of the relation, as specified by a pattern tableau. Calculating the support and confidence of a CFD (i.e., the size of the applicable subset and the extent to which it satisfies the CFD)gives valuable information about data semantics and data quality. While computing the support is easier, computing the confidence exactly is expensive if the relation is large, and estimating it from a random sample of the relation is unreliable unless the sample is large. We study how to efficiently estimate the confidence of a CFD with a small number of passes (one or two) over the input using small space. Our solutions are based on a variety of sampling and sketching techniques, and apply when the pattern tableau is known in advance, and also the harder case when this is given after the data have been seen. We analyze our algorithms, and show that they can guarantee a small additive error; we also show that relative errors guarantees are not possible. We demonstrate the power of these methods empirically, with a detailed study using both real and synthetic data. These experiments show that it is possible to estimate the CFD confidence very accurately with summaries which are much smaller than the size of the data they represent.

#index 1217183
#* Scalable skyline computation using object-based space partitioning
#@ Shiming Zhang;Nikos Mamoulis;David W. Cheung
#t 2009
#c 5
#% 287414
#% 480671
#% 800512
#% 806212
#% 810024
#% 824670
#% 824671
#% 864451
#% 864452
#% 875012
#% 989652
#% 993954
#% 993957
#% 1022203
#% 1022224
#% 1022225
#% 1022242
#% 1044463
#% 1063485
#% 1063486
#% 1063487
#% 1092017
#% 1127433
#% 1201865
#% 1206780
#% 1688253
#% 1688273
#! The skyline operator returns from a set of multi-dimensional objects a subset of superior objects that are not dominated by others. This operation is considered very important in multi-objective analysis of large datasets. Although a large number of skyline methods have been proposed, the majority of them focuses on minimizing the I/O cost. However, in high dimensional spaces, the problem can easily become CPU-bound due to the large number of computations required for comparing objects with current skyline points while scanning the database. Based on this observation, we propose a dynamic indexing technique for skyline points that can be integrated into state-of-the-art sort-based skyline algorithms to boost their computational performance. The new indexing and dominance checking approach is supported by a theoretical analysis, while our experiments show that it scales well with the input size and dimensionality not only because unnecessary dominance checks are avoided but also because it allows efficient dominance checking with the help of bitwise operations.

#index 1217184
#* Minimizing the communication cost for continuous skyline maintenance
#@ Zhenjie Zhang;Reynold Cheng;Dimitris Papadias;Anthony K.H. Tung
#t 2009
#c 5
#% 70050
#% 279165
#% 442615
#% 465167
#% 480671
#% 654443
#% 654480
#% 654488
#% 765402
#% 800555
#% 810024
#% 810048
#% 824654
#% 824670
#% 824671
#% 832568
#% 849816
#% 864453
#% 864495
#% 875011
#% 875012
#% 893150
#% 993954
#% 1022203
#% 1022224
#% 1022225
#% 1022226
#% 1688253
#! Existing work in the skyline literature focuses on optimizing the processing cost. This paper aims at minimization of the communication overhead in client-server architectures, where a server continuously maintains the skyline of dynamic objects. Our first contribution is a Filter method that avoids transmission of updates from objects that cannot influence the skyline. Specifically, each object is assigned a filter so that it needs to issue an update only if it violates its filter. Filter achieves significant savings over the naive approach of transmitting all updates. Going one step further, we introduce the concept of frequent skyline query over a sliding window(FSQW). The motivation is that snapshot skylines are not very useful in streaming environments because they keep changing over time. Instead, FSQW reports the objects that appear in the skylines of at least θ ⋅ s of the s most recent timestamps (0

#index 1217185
#* Kernel-based skyline cardinality estimation
#@ Zhenjie Zhang;Yin Yang;Ruichu Cai;Dimitris Papadias;Anthony Tung
#t 2009
#c 5
#% 116084
#% 273906
#% 287414
#% 288976
#% 289148
#% 403035
#% 458873
#% 479648
#% 480671
#% 482092
#% 729437
#% 800555
#% 803119
#% 806212
#% 810024
#% 824671
#% 824672
#% 849816
#% 864451
#% 864452
#% 875011
#% 875012
#% 893150
#% 903013
#% 993954
#% 1022203
#% 1022224
#% 1022225
#% 1092017
#% 1207004
#% 1700131
#! The skyline of a d-dimensional dataset consists of all points not dominated by others. The incorporation of the skyline operator into practical database systems necessitates an efficient and effective cardinality estimation module. However, existing theoretical work on this problem is limited to the case where all d dimensions are independent of each other, which rarely holds for real datasets. The state of the art Log Sampling (LS) technique simply applies theoretical results for independent dimensions to non-independent data anyway, sometimes leading to large estimation errors. To solve this problem, we propose a novel Kernel-Based (KB) approach that approximates the skyline cardinality with nonparametric methods. Extensive experiments with various real datasets demonstrate that KB achieves high accuracy, even in cases where LS fails. At the same time, despite its numerical nature, the efficiency of KB is comparable to that of LS. Furthermore, we extend both LS and KB to the k-dominant skyline, which is commonly used instead of the conventional skyline for high-dimensional data.

#index 1217186
#* Why not?
#@ Adriane Chapman;H. V. Jagadish
#t 2009
#c 5
#% 318704
#% 397349
#% 480483
#% 504161
#% 632040
#% 751797
#% 810042
#% 860052
#% 875046
#% 879803
#% 893167
#% 946526
#% 1016204
#% 1017302
#% 1022295
#% 1042656
#% 1127409
#% 1396743
#! As humans, we have expectations for the results of any action, e.g. we expect at least one student to be returned when we query a university database for student records. When these expectations are not met, traditional database users often explore datasets via a series of slightly altered SQL queries. Yet most database access is via limited interfaces that deprive end users of the ability to alter their query in any way to garner better understanding of the dataset and result set. Users are unable to question why a particular data item is Not in the result set of a given query. In this work, we develop a model for answers to WHY NOT? queries. We show through a user study the usefulness of our answers, and describe two algorithms for finding the manipulation that discarded the data item of interest. Moreover, we work through two different methods for tracing the discarded data item that can be used with either algorithm. Using our algorithms, it is feasible for users to find the manipulation that excluded the data item of interest, and can eliminate the need for exhausting debugging.

#index 1217187
#* Query by output
#@ Quoc Trung Tran;Chee-Yong Chan;Srinivasan Parthasarathy
#t 2009
#c 5
#% 18614
#% 333986
#% 334007
#% 375017
#% 442877
#% 459008
#% 465167
#% 765462
#% 769902
#% 835018
#% 864456
#% 902467
#% 960234
#% 960262
#% 1063507
#% 1063546
#% 1063709
#% 1127419
#% 1167472
#! It has recently been asserted that the usability of a database is as important as its capability. Understanding the database schema, the hidden relationships among attributes in the data all play an important role in this context. Subscribing to this viewpoint, in this paper, we present a novel data-driven approach, called Query By Output (QBO), which can enhance the usability of database systems. The central goal of QBO is as follows: given the output of some query Q on a database D, denoted by Q(D), we wish to construct an alternative query Q′ such that Q(D) and Q′ (D) are instance-equivalent. To generate instance-equivalent queries from Q(D), we devise a novel data classification-based technique that can handle the at-least-one semantics that is inherent in the query derivation. In addition to the basic framework, we design several optimization techniques to reduce processing overhead and introduce a set of criteria to rank order output queries by various notions of utility. Our framework is evaluated comprehensively on three real data sets and the results show that the instance-equivalent queries we obtain are interesting and that the approach is scalable and robust to queries of different selectivities.

#index 1217188
#* Detecting and resolving unsound workflow views for correct provenance analysis
#@ Peng Sun;Ziyang Liu;Susan B. Davidson;Yi Chen
#t 2009
#c 5
#% 263441
#% 511666
#% 598747
#% 720183
#% 763515
#% 765129
#% 791837
#% 832825
#% 900797
#% 1023485
#% 1035751
#% 1042649
#% 1060033
#% 1063544
#% 1063545
#% 1063593
#% 1083691
#% 1127581
#% 1174014
#% 1180022
#% 1206750
#% 1206843
#% 1408534
#% 1415576
#% 1678894
#% 1692849
#! Workflow views abstract groups of tasks in a workflow into high level composite tasks, in order to reuse sub-workflows and facilitate provenance analysis. However, unless a view is carefully designed, it may not preserve the dataflow between tasks in the workflow, i.e., it may not be sound. Unsound views can be misleading and cause incorrect provenance analysis. This paper studies the problem of efficiently identifying and correcting unsound workflow views with minimal changes. In particular, given a workflow view, we wish to split each unsound composite task into the minimal number of tasks, such that the resulting view is sound. We prove that this problem is NP-hard by reduction from independent set. We then propose two local optimality conditions (weak and strong), and design polynomial time algorithms for correcting unsound views to meet these conditions. Experiments show that our proposed algorithms are effective and efficient, and that the strong local optimality algorithm produces better solutions than the weak local optimality algorithm with little processing overhead.

#index 1217189
#* Quality and efficiency in high dimensional nearest neighbor search
#@ Yufei Tao;Ke Yi;Cheng Sheng;Panos Kalnis
#t 2009
#c 5
#% 86950
#% 201876
#% 249321
#% 252304
#% 264161
#% 271801
#% 280452
#% 287466
#% 300136
#% 318703
#% 333854
#% 435141
#% 443329
#% 443396
#% 443517
#% 465014
#% 479649
#% 479816
#% 479973
#% 480304
#% 632011
#% 632035
#% 654466
#% 656800
#% 659921
#% 745496
#% 749529
#% 762054
#% 800570
#% 805905
#% 814646
#% 847166
#% 898309
#% 1022281
#% 1206695
#! Nearest neighbor (NN) search in high dimensional space is an important problem in many applications. Ideally, a practical solution (i) should be implementable in a relational database, and (ii) its query cost should grow sub-linearly with the dataset size, regardless of the data and query distributions. Despite the bulk of NN literature, no solution fulfills both requirements, except locality sensitive hashing (LSH). The existing LSH implementations are either rigorous or adhoc. Rigorous-LSH ensures good quality of query results, but requires expensive space and query cost. Although adhoc-LSH is more efficient, it abandons quality control, i.e., the neighbor it outputs can be arbitrarily bad. As a result, currently no method is able to ensure both quality and efficiency simultaneously in practice. Motivated by this, we propose a new access method called the locality sensitive B-tree (LSB-tree) that enables fast high-dimensional NN search with excellent quality. The combination of several LSB-trees leads to a structure called the LSB-forest that ensures the same result quality as rigorous-LSH, but reduces its space and query cost dramatically. The LSB-forest also outperforms adhoc-LSH, even though the latter has no quality guarantee. Besides its appealing theoretical properties, the LSB-tree itself also serves as an effective index that consumes linear space, and supports efficient updates. Our extensive experiments confirm that the LSB-tree is faster than (i) the state of the art of exact NN search by two orders of magnitude, and (ii) the best (linear-space) method of approximate retrieval by an order of magnitude, and at the same time, returns neighbors with much better quality.

#index 1217190
#* Continuous obstructed nearest neighbor queries in spatial databases
#@ Yunjun Gao;Baihua Zheng
#t 2009
#c 5
#% 10124
#% 86950
#% 201876
#% 235114
#% 261733
#% 287466
#% 397377
#% 427199
#% 461923
#% 464859
#% 465004
#% 480093
#% 527187
#% 566699
#% 799775
#% 824723
#% 839701
#% 975094
#% 993955
#% 1068776
#% 1181225
#% 1206931
#% 1408837
#% 1408860
#! In this paper, we study a novel form of continuous nearest neighbor queries in the presence of obstacles, namely continuous obstructed nearest neighbor (CONN) search. It considers the impact of obstacles on the distance between objects, which is ignored by most of spatial queries. Given a data set P, an obstacle set O, and a query line segment q in a two-dimensional space, a CONN query retrieves the nearest neighbor of each point on q according to the obstructed distance, i.e., the shortest path between them without crossing any obstacle. We formulate CONN search, analyze its unique properties, and develop algorithms for exact CONN query processing, assuming that both P and O are indexed by conventional data-partitioning indices (e.g., R-trees). Our methods tackle the CONN retrieval by performing a single query for the entire query segment, and only process the data points and obstacles relevant to the final result, via a novel concept of control points and an efficient quadratic-based split point computation algorithm. In addition, we extend our solution to handle the continuous obstructed k-nearest neighbor (COkNN) search, which finds the k (≥1)nearest neighbors to every point along q based on obstructed distances. A comprehensive experimental evaluation using both real and synthetic datasets has been conducted to demonstrate the efficiency and effectiveness of our proposed algorithms.

#index 1217191
#* Monitoring path nearest neighbor in road networks
#@ Zaiben Chen;Heng Tao Shen;Xiaofang Zhou;Jeffrey Xu Yu
#t 2009
#c 5
#% 201876
#% 287466
#% 300162
#% 413797
#% 729850
#% 729851
#% 745464
#% 803539
#% 810061
#% 813718
#% 813973
#% 824723
#% 893092
#% 993955
#% 1015321
#% 1016199
#% 1063472
#% 1127438
#% 1720757
#! This paper addresses the problem of monitoring the k nearest neighbors to a dynamically changing path in road networks. Given a destination where a user is going to, this new query returns the k-NN with respect to the shortest path connecting the destination and the user's current location, and thus provides a list of nearest candidates for reference by considering the whole coming journey. We name this query the k-Path Nearest Neighbor query (k-PNN). As the user is moving and may not always follow the shortest path, the query path keeps changing. The challenge of monitoring the k-PNN for an arbitrarily moving user is to dynamically determine the update locations and then refresh the k-PNN efficiently. We propose a three-phase Best-first Network Expansion (BNE) algorithm for monitoring the k-PNN and the corresponding shortest path. In the searching phase, the BNE finds the shortest path to the destination, during which a candidate set that guarantees to include the k-PNN is generated at the same time. Then in the verification phase, a heuristic algorithm runs for examining candidates' exact distances to the query path, and it achieves significant reduction in the number of visited nodes. The monitoring phase deals with computing update locations as well as refreshing the k-PNN in different user movements. Since determining the network distance is a costly process, an expansion tree and the candidate set are carefully maintained by the BNE algorithm, which can provide efficient update on the shortest path and the k-PNN results. Finally, we conduct extensive experiments on real road networks and show that our methods achieve satisfactory performance.

#index 1217192
#* Cost based plan selection for xpath
#@ Haris Georgiadis;Minas Charalambides;Vasilis Vassalos
#t 2009
#c 5
#% 340144
#% 397358
#% 397375
#% 479806
#% 654514
#% 745463
#% 765488
#% 781453
#% 800577
#% 824663
#% 824667
#% 864401
#% 864441
#% 864515
#% 875010
#% 881734
#% 884638
#% 960260
#% 960318
#% 993953
#% 994015
#% 1015298
#% 1016224
#% 1127572
#% 1217192
#% 1669523
#! We present a complete XPath cost-based optimization and execution framework and demonstrate its effectiveness and efficiency for a variety of queries and datasets. The framework is based on a logical XPath algebra with novel features and operators and a comprehensive set of rewriting rules that together enable us to algebraically capture many existing and novel processing strategies for XPath queries. An important part of the framework is PSA, a very efficient cost-based plan selection algorithm for XPath queries. In the presented experimental evaluation, PSA picked the cheapest estimated query plan in 100% of the cases. Our cost-based query optimizer independent of the underlying physical data model and storage system and of the available logical operator implementations, depending on a set of well-defined APIs. We also present an implementation of those APIs, including primitive access methods, a large pool of physical operators, statistics estimators and cost models, and experimentally demonstrate the effectiveness of our end-to-end query optimization system.

#index 1217193
#* ROX: run-time optimization of XQueries
#@ Riham Abdel Kader;Peter Boncz;Stefan Manegold;Maurice van Keulen
#t 2009
#c 5
#% 58375
#% 102784
#% 153215
#% 172900
#% 210353
#% 248793
#% 273908
#% 300167
#% 378414
#% 397364
#% 397371
#% 397375
#% 458836
#% 465018
#% 479465
#% 480488
#% 742563
#% 765423
#% 765456
#% 765488
#% 800505
#% 810017
#% 875010
#% 1016149
#% 1022271
#% 1026989
#! Optimization of complex XQueries combining many XPath steps and joins is currently hindered by the absence of good cardinality estimation and cost models for XQuery. Additionally, the state-of-the-art of even relational query optimization still struggles to cope with cost model estimation errors that increase with plan size, as well as with the effect of correlated joins and selections. In this research, we propose to radically depart from the traditional path of separating the query compilation and query execution phases, by having the optimizer execute, materialize partial results, and use sampling based estimation techniques to observe the characteristics of intermediates. The proposed technique takes as input a Join Graph where the edges are either equi-joins or XPath steps, and the execution environment provides value- and structural-join algorithms, as well as structural and value-based indices. While run-time optimization with sampling removes many of the vulnerabilities of classical optimizers, it brings its own challenges with respect to keeping resource usage under control, both with respect to the materialization of intermediates, as well as the cost of plan exploration using sampling. Our approach deals with these issues by limiting the run-time search space to so-called "zero-investment algorithms for which sampling can be guaranteed to be strictly linear in sample size. All operators and XML value indices used by ROX for sampling have the zero-investment property. We perform extensive experimental evaluation on large XML datasets that shows that our run-time query optimizer finds good query plans in a robust fashion and has limited run-time overhead.

#index 1217194
#* Scalable join processing on very large RDF graphs
#@ Thomas Neumann;Gerhard Weikum
#t 2009
#c 5
#% 136740
#% 172889
#% 210207
#% 289282
#% 300167
#% 330305
#% 481429
#% 481621
#% 565473
#% 824755
#% 874876
#% 893165
#% 956564
#% 956664
#% 960299
#% 1022236
#% 1026989
#% 1055731
#% 1055735
#% 1060608
#% 1127402
#% 1127431
#% 1127610
#% 1206597
#% 1269903
#% 1409918
#% 1409954
#! With the proliferation of the RDF data format, engines for RDF query processing are faced with very large graphs that contain hundreds of millions of RDF triples. This paper addresses the resulting scalability problems. Recent prior work along these lines has focused on indexing and other physical-design issues. The current paper focuses on join processing, as the fine-grained and schema-relaxed use of RDF often entails star- and chain-shaped join queries with many input streams from index scans. We present two contributions for scalable join processing. First, we develop very light-weight methods for sideways information passing between separate joins at query run-time, to provide highly effective filters on the input streams of joins. Second, we improve previously proposed algorithms for join-order optimization by more accurate selectivity estimations for very large RDF graphs. Experimental studies with several RDF datasets, including the UniProt collection, demonstrate the performance gains of our approach, outperforming the previously fastest systems by more than an order of magnitude.

#index 1217195
#* Top-k generation of integrated schemas based on directed and weighted correspondences
#@ Ahmed Radwan;Lucian Popa;Ioana R. Stanoi;Akmal Younis
#t 2009
#c 5
#% 22948
#% 442861
#% 443698
#% 458607
#% 480969
#% 529190
#% 572314
#% 660001
#% 758483
#% 960271
#% 993981
#% 1015326
#% 1044441
#% 1063532
#% 1289178
#% 1729914
#% 1730010
#! Schema integration is the problem of creating a unified target schema based on a set of existing source schemas and based on a set of correspondences that are the result of matching the source schemas. Previous methods for schema integration rely on the exploration, implicit or explicit, of the multiple design choices that are possible for the integrated schema. Such exploration relies heavily on user interaction; thus, it is time consuming and labor intensive. Furthermore, previous methods have ignored the additional information that typically results from the schema matching process, that is, the weights and in some cases the directions that are associated with the correspondences. In this paper, we propose a more automatic approach to schema integration that is based on the use of directed and weighted correspondences between the concepts that appear in the source schemas. A key component of our approach is a novel top-k ranking algorithm for the automatic generation of the best candidate schemas. The algorithm gives more weight to schemas that combine the concepts with higher similarity or coverage. Thus, the algorithm makes certain decisions that otherwise would likely be taken by a human expert. We show that the algorithm runs in polynomial time and moreover has good performance in practice.

#index 1217196
#* Core schema mappings
#@ Giansalvatore Mecca;Paolo Papotti;Salvatore Raunich
#t 2009
#c 5
#% 583
#% 129217
#% 198465
#% 333988
#% 480134
#% 806215
#% 826032
#% 893094
#% 893114
#% 993981
#% 1022222
#% 1022258
#% 1039063
#% 1044442
#% 1063712
#% 1127589
#% 1181235
#% 1206614
#! Research has investigated mappings among data sources under two perspectives. On one side, there are studies of practical tools for schema mapping generation; these focus on algorithms to generate mappings based on visual specifications provided by users. On the other side, we have theoretical researches about data exchange. These study how to generate a solution - i.e., a target instance - given a set of mappings usually specified as tuple generating dependencies. However, despite the fact that the notion of a core of a data exchange solution has been formally identified as an optimal solution, there are yet no mapping systems that support core computations. In this paper we introduce several new algorithms that contribute to bridge the gap between the practice of mapping generation and the theory of data exchange. We show how, given a mapping scenario, it is possible to generate an executable script that computes core solutions for the corresponding data exchange problem. The algorithms have been implemented and tested using common runtime engines to show that they guarantee very good performances, orders of magnitudes better than those of known algorithms that compute the core as a post-processing step.

#index 1217197
#* A gauss function based approach for unbalanced ontology matching
#@ Qian Zhong;Hanyu Li;Juanzi Li;Guotong Xie;Jie Tang;Lizhu Zhou;Yue Pan
#t 2009
#c 5
#% 333990
#% 405391
#% 413582
#% 465914
#% 479783
#% 480645
#% 660001
#% 790852
#% 810078
#% 824775
#% 830529
#% 896031
#% 941135
#% 956571
#% 960271
#% 993982
#% 1063532
#% 1409921
#% 1705186
#% 1728980
#% 1730033
#! Ontology matching, aiming to obtain semantic correspondences between two ontologies, has played a key role in data exchange, data integration and metadata management. Among numerous matching scenarios, especially the applications cross multiple domains, we observe an important problem, denoted as unbalanced ontology matching which requires to find the matches between an ontology describing a local domain knowledge and another ontology covering the information over multiple domains, is not well studied in the community. In this paper, we propose a novel Gauss Function based ontology matching approach to deal with this unbalanced ontology matching issue. Given a relative lightweight ontology which represents the local domain knowledge, we extract a "similar" sub-ontology from the corresponding heavyweight ontology and then carry out the matching procedure between this lightweight ontology and the newly generated sub-ontology. The sub-ontology generation is based on the influences between concepts in the heavyweight ontology. We propose a Gauss Function based method to properly calculate the influence values between concepts. In addition, we perform an extensive experiment to verify the effectiveness and efficiency of our proposed approach by using OAEI2007 tasks. Experimental results clearly demonstrate that our solution outperforms the existing methods in terms of precision, recall and elapsed time.

#index 1217198
#* Keyword search in databases: the power of RDBMS
#@ Lu Qin;Jeffrey Xu Yu;Lijun Chang
#t 2009
#c 5
#% 114577
#% 136740
#% 245788
#% 248014
#% 289282
#% 300166
#% 442667
#% 481288
#% 659990
#% 660011
#% 824693
#% 864456
#% 874894
#% 875017
#% 960243
#% 960259
#% 960278
#% 960284
#% 993987
#% 1015325
#% 1016176
#% 1021948
#% 1026960
#% 1063537
#% 1063539
#% 1127353
#% 1127445
#% 1207007
#! Keyword search in relational databases (RDBs) has been extensively studied recently. A keyword search (or a keyword query) in RDBs is specified by a set of keywords to explore the interconnected tuple structures in an RDB that cannot be easily identified using SQL on RDBMS. In brief, it finds how the tuples containing the given keywords are connected via sequences of connections (foreign key references) among tuples in an RDB. Such interconnected tuple structures can be found as connected trees up to a certain size, sets of tuples that are reachable from a root tuple within a radius, or even multi-center subgraphs within a radius. In the literature, there are two main approaches. One is to generate a set of relational algebra expressions and evaluate every such expression using SQL on an RDBMS directly or in a middleware on top of an RDBMS indirectly. Due to a large number of relational algebra expressions needed to process, most of the existing works take a middleware approach without fully utilizing RDBMSs. The other is to materialize an RDB as a graph and find the interconnected tuple structures using graph-based algorithms in memory. In this paper we focus on using SQL to compute all the interconnected tuple structures for a given keyword query. We use three types of interconnected tuple structures to achieve that and we control the size of the structures. We show that the current commercial RDBMSs are powerful enough to support such keyword queries in RDBs efficiently without any additional new indexing to be built and maintained. The main idea behind our approach is tuple reduction. In our approach, in the first reduction step, we prune tuples that do not participate in any results using SQL, and in the second join step, we process the relational algebra expressions using SQL over the reduced relations. We conducted extensive experimental studies using two commercial RDBMSs and two large real datasets, and we report the efficiency of our approaches in this paper.

#index 1217199
#* Efficient type-ahead search on relational data: a TASTIER approach
#@ Guoliang Li;Shengyue Ji;Chen Li;Jianhua Feng
#t 2009
#c 5
#% 21142
#% 131061
#% 262869
#% 274612
#% 278500
#% 654442
#% 659990
#% 660011
#% 766461
#% 781169
#% 810052
#% 824693
#% 875017
#% 879610
#% 956599
#% 960243
#% 960259
#% 960261
#% 987276
#% 993987
#% 1015258
#% 1015325
#% 1016135
#% 1016176
#% 1022220
#% 1044480
#% 1063537
#% 1190092
#% 1190166
#% 1206926
#% 1217200
#! Existing keyword-search systems in relational databases require users to submit a complete query to compute answers. Often users feel "left in the dark" when they have limited knowledge about the data, and have to use a try-and-see approach for modifying queries and finding answers. In this paper we propose a novel approach to keyword search in the relational world, called Tastier. A Tastier system can bring instant gratification to users by supporting type-ahead search, which finds answers "on the fly" as the user types in query keywords. A main challenge is how to achieve a high interactive speed for large amounts of data in multiple tables, so that a query can be answered efficiently within milliseconds. We propose efficient index structures and algorithms for finding relevant answers on-the-fly by joining tuples in the database. We devise a partition-based method to improve query performance by grouping highly relevant tuples and pruning irrelevant tuples efficiently. We also develop a technique to answer a query efficiently by predicting the highly relevant complete queries for the user. We have conducted a thorough experimental evaluation of the proposed techniques on real data sets to demonstrate the efficiency and practicality of this new search paradigm.

#index 1217200
#* Extending autocompletion to tolerate errors
#@ Surajit Chaudhuri;Raghav Kaushik
#t 2009
#c 5
#% 82523
#% 333679
#% 333854
#% 479973
#% 480654
#% 546101
#% 656307
#% 766461
#% 864392
#% 875066
#% 875292
#% 879610
#% 893164
#% 960234
#% 960360
#% 1001397
#% 1022220
#% 1127425
#% 1146954
#! Autocompletion is a useful feature when a user is doing a look up from a table of records. With every letter being typed, autocompletion displays strings that are present in the table containing as their prefix the search string typed so far. Just as there is a need for making the lookup operation tolerant to typing errors, we argue that autocompletion also needs to be error-tolerant. In this paper, we take a first step towards addressing this problem. We capture input typing errors via edit distance. We show that a naive approach of invoking an offline edit distance matching algorithm at each step performs poorly and present more efficient algorithms. Our empirical evaluation demonstrates the effectiveness of our algorithms.

#index 1217201
#* DDE: from dewey to a fully dynamic XML labeling scheme
#@ Liang Xu;Tok Wang Ling;Huayu Wu;Zhifeng Bao
#t 2009
#c 5
#% 333981
#% 378412
#% 397366
#% 480489
#% 745479
#% 765488
#% 810052
#% 838506
#% 858178
#% 864400
#% 956599
#% 1046512
#% 1099783
#! Labeling schemes lie at the core of query processing for many XML database management systems. Designing labeling schemes for dynamic XML documents is an important problem that has received a lot of research attention. Existing dynamic labeling schemes, however, often sacrifice query performance and introduce additional labeling cost to facilitate arbitrary updates even when the documents actually seldom get updated. Since the line between static and dynamic XML documents is often blurred in practice, we believe it is important to design a labeling scheme that is compact and efficient regardless of whether the documents are frequently updated or not. In this paper, we propose a novel labeling scheme called DDE (for Dynamic DEwey) which is tailored for both static and dynamic XML documents. For static documents, the labels of DDE are the same as those of dewey which yield compact size and high query performance. When updates take place, DDE can completely avoid re-labeling and its label quality is most resilient to the number and order of insertions compared to the existing approaches. In addition, we introduce Compact DDE (CDDE) which is designed to optimize the performance of DDE for insertions. Both DDE and CDDE can be incorporated into existing systems and applications that are based on dewey labeling scheme with minimum efforts. Experiment results demonstrate the benefits of our proposed labeling schemes over the previous approaches.

#index 1217202
#* Simplifying XML schema: effortless handling of nondeterministic regular expressions
#@ Geert Jan Bex;Wouter Gelade;Wim Martens;Frank Neven
#t 2009
#c 5
#% 262724
#% 288434
#% 397364
#% 478932
#% 480822
#% 572314
#% 577353
#% 772031
#% 772032
#% 835398
#% 845589
#% 857282
#% 879213
#% 893098
#% 894435
#% 957534
#% 976789
#% 1016148
#% 1016255
#% 1022285
#% 1039062
#% 1055754
#% 1063573
#% 1130846
#% 1408540
#% 1415324
#! Whether beloved or despised, XML Schema is momentarily the only industrially accepted schema language for XML and is unlikely to become obsolete any time soon. Nevertheless, many nontransparent restrictions unnecessarily complicate the design of XSDs. For instance, complex content models in XML Schema are constrained by the infamous unique particle attribution (UPA) constraint. In formal language theoretic terms, this constraint restricts content models to deterministic regular expressions. As the latter constitute a semantic notion and no simple corresponding syntactical characterization is known, it is very difficult for non-expert users to understand exactly when and why content models do or do not violate UPA. In the present paper, we therefore investigate solutions to relieve users from the burden of UPA by automatically transforming nondeterministic expressions into concise deterministic ones defining the same language or constituting good approximations. The presented techniques facilitate XSD construction by reducing the design task at hand more towards the complexity of the modeling task. In addition, our algorithms can serve as a plug-in for any model management tool which supports export to XML Schema format.

#index 1217203
#* FlexRecs: expressing and combining flexible recommendations
#@ Georgia Koutrika;Benjamin Bercovitz;Hector Garcia-Molina
#t 2009
#c 5
#% 5379
#% 80217
#% 124010
#% 173879
#% 202011
#% 220709
#% 234992
#% 330687
#% 342687
#% 397155
#% 424021
#% 428272
#% 452563
#% 522882
#% 734590
#% 783438
#% 813966
#% 956521
#% 982678
#% 987671
#% 1127475
#% 1206837
#% 1650569
#! Recommendation systems have become very popular but most recommendation methods are `hard-wired' into the system making experimentation with and implementation of new recommendation paradigms cumbersome. In this paper, we propose FlexRecs, a framework that decouples the definition of a recommendation process from its execution and supports flexible recommendations over structured data. In FlexRecs, a recommendation approach can be defined declaratively as a high-level parameterized workflow comprising traditional relational operators and new operators that generate or combine recommendations. We describe a prototype flexible recommendation engine that realizes the proposed framework and we present example workflows and experimental results that show its potential for capturing multiple, existing or novel, recommendations easily and having a flexible recommendation system that combines extensibility with reasonable performance.

#index 1217204
#* Efficient approximate entity extraction with edit distance constraints
#@ Wei Wang;Chuan Xiao;Xuemin Lin;Chengqi Zhang
#t 2009
#c 5
#% 2324
#% 189867
#% 289282
#% 333679
#% 450070
#% 480654
#% 765463
#% 766441
#% 769884
#% 783484
#% 799691
#% 824684
#% 864392
#% 864415
#% 870896
#% 893164
#% 956458
#% 956506
#% 960263
#% 960270
#% 1022218
#% 1022227
#% 1022229
#% 1055684
#% 1063496
#% 1063530
#% 1063566
#% 1127368
#% 1127425
#% 1127426
#% 1131827
#% 1206665
#% 1206677
#% 1206754
#% 1314663
#! Named entity recognition aims at extracting named entities from unstructured text. A recent trend of named entity recognition is finding approximate matches in the text with respect to a large dictionary of known entities, as the domain knowledge encoded in the dictionary helps to improve the extraction performance. In this paper, we study the problem of approximate dictionary matching with edit distance constraints. Compared to existing studies using token-based similarity constraints, our problem definition enables us to capture typographical or orthographical errors, both of which are common in entity extraction tasks yet may be missed by token-based similarity constraints. Our problem is technically challenging as existing approaches based on q-gram filtering have poor performance due to the existence of many short entities in the dictionary. Our proposed solution is based on an improved neighborhood generation method employing novel partitioning and prefix pruning techniques. We also propose an efficient document processing algorithm that minimizes unnecessary comparisons and enumerations and hence achieves good scalability. We have conducted extensive experiments on several publicly available named entity recognition datasets. The proposed algorithm outperforms alternative approaches by up to an order of magnitude.

#index 1217205
#* GAMPS: compressing multi sensor data by grouping and amplitude scaling
#@ Sorabh Gandhi;Suman Nath;Subhash Suri;Jie Liu
#t 2009
#c 5
#% 69316
#% 172949
#% 227857
#% 227924
#% 273704
#% 333941
#% 443369
#% 460862
#% 548654
#% 600184
#% 662750
#% 751027
#% 765445
#% 801684
#% 804675
#% 824709
#% 878302
#% 1022238
#% 1022239
#% 1078390
#% 1127428
#% 1387822
#! We consider the problem of collectively approximating a set of sensor signals using the least amount of space so that any individual signal can be efficiently reconstructed within a given maximum (L∞) error ε. The problem arises naturally in applications that need to collect large amounts of data from multiple concurrent sources, such as sensors, servers and network routers, and archive them over a long period of time for offline data mining. We present GAMPS, a general framework that addresses this problem by combining several novel techniques. First, it dynamically groups multiple signals together so that signals within each group are correlated and can be maximally compressed jointly. Second, it appropriately scales the amplitudes of different signals within a group and compresses them within the maximum allowed reconstruction error bound. Our schemes are polynomial time O(α, β approximation schemes, meaning that the maximum (L∞) error is at most α ε and it uses at most β times the optimal memory. Finally, GAMPS maintains an index so that various queries can be issued directly on compressed data. Our experiments on several real-world sensor datasets show that GAMPS significantly reduces space without compromising the quality of search and query.

#index 1217206
#* Optimizing i/o-intensive transactions in highly interactive applications
#@ Mohamed A. Sharaf;Panos K. Chrysanthis;Alexandros Labrinidis;Cristiana Amza
#t 2009
#c 5
#% 9241
#% 29493
#% 63223
#% 77990
#% 117903
#% 159079
#% 201869
#% 342369
#% 444529
#% 481127
#% 571215
#% 615514
#% 745471
#% 770369
#% 783784
#% 795408
#% 824746
#% 860354
#% 864447
#% 864540
#% 1206909
#% 1207191
#% 1245054
#! The performance provided by an interactive online database system is typically measured in terms of meeting certain pre-specified Service Level Agreements (SLAs), with expected transaction latency being the most commonly used type of SLA. This form of SLA acts as a soft deadline for each transaction, and user satisfaction can be measured in terms of minimizing tardiness, that is, the deviation from SLA. This objective is further complicated for I/O-intensive transactions, where the storage system becomes the performance bottleneck. Moreover, common I/O scheduling policies employed by the Operating System with a goal of improving I/O throughput or average latency may run counter to optimizing per-transaction performance since the Operating System is typically oblivious to the application high-level SLA specifications. In this paper, we propose a new SLA-aware policy for scheduling I/O requests of database transactions. Our proposed policy synergistically combines novel deadline-aware scheduling policies for database transactions with features of Operating System scheduling policies designed for improving I/O throughput. This enables our proposed policy to dynamically adapt to workload and consistently provide the best performance.

#index 1217207
#* A revised r*-tree in comparison with related index structures
#@ Norbert Beckmann;Bernhard Seeger
#t 2009
#c 5
#% 86950
#% 137887
#% 164360
#% 172939
#% 191595
#% 227864
#% 227939
#% 273888
#% 273941
#% 321455
#% 342828
#% 397396
#% 427199
#% 479473
#% 479645
#% 480133
#% 481455
#% 481589
#% 481599
#% 481956
#% 502775
#% 631963
#% 765430
#% 837687
#! In this paper we present an improved redesign of the R*-tree that is entirely suitable for running within a DBMS. Most importantly, an insertion is guaranteed to be restricted to a single path because re-insertion could be abandoned. We re-engineered both, subtree choice and split algorithm, to be more robust against specific data distributions and insertion orders, as well as peculiarities often found in real multidimensional data sets. This comes along with a substantial reduction in CPU-time. Our experimental setup covers a wide range of different artificial and real data sets. The experimental comparison shows that the search performance of our revised R*-tree is superior to that of its three most important competitors. In comparison to its predecessor, the original R*-tree, the creation of a tree is substantially faster, while the I/O cost required for processing queries is improved by more than 30% on average for two- and three-dimensional data. For higher dimensional data, particularly for real data sets, much larger improvements are achieved.

#index 1217208
#* 3-HOP: a high-compression indexing scheme for reachability query
#@ Ruoming Jin;Yang Xiang;Ning Ruan;David Fuhry
#t 2009
#c 5
#% 47573
#% 55327
#% 58365
#% 88051
#% 94589
#% 379482
#% 480405
#% 810072
#% 824692
#% 864462
#% 960304
#% 1019798
#% 1044451
#% 1063512
#% 1063514
#% 1206685
#% 1688299
#! Reachability queries on large directed graphs have attracted much attention recently. The existing work either uses spanning structures, such as chains or trees, to compress the complete transitive closure, or utilizes the 2-hop strategy to describe the reachability. Almost all of these approaches work well for very sparse graphs. However, the challenging problem is that as the ratio of the number of edges to the number of vertices increases, the size of the compressed transitive closure grows very large. In this paper, we propose a new 3-hop indexing scheme for directed graphs with higher density. The basic idea of 3-hop indexing is to use chain structures in combination with hops to minimize the number of structures that must be indexed. Technically, our goal is to find a 3-hop scheme over dense DAGs (directed acyclic graphs) with minimum index size. We develop an efficient algorithm to discover a transitive closure contour, which yields near optimal index size. Empirical studies show that our 3-hop scheme has much smaller index size than state-of-the-art reachability query schemes such as 2-hop and path-tree when DAGs are not very sparse, while our query time is close to path-tree, which is considered to be one of the best reachability query schemes.

#index 1217209
#* Serial and parallel methods for i/o efficient suffix tree construction
#@ Amol Ghoting;Konstantin Makarychev
#t 2009
#c 5
#% 235941
#% 262045
#% 289010
#% 468476
#% 480484
#% 593861
#% 744092
#% 745510
#% 789004
#% 794239
#% 829998
#% 960303
#% 1015330
#% 1116726
#% 1117032
#% 1386499
#% 1671822
#! Over the past three decades, the suffix tree has served as a fundamental data structure in string processing. However, its widespread applicability has been hindered due to the fact that suffix tree construction does not scale well with the size of the input string. With advances in data collection and storage technologies, large strings have become ubiquitous, especially across emerging applications involving text, time series, and biological sequence data. To benefit from these advances, it is imperative that we realize a scalable suffix tree construction algorithm. To deal with the aforementioned challenge, the past few years have seen the emergence of several disk-based suffix tree construction algorithms. However, construction times continue to be daunting -- for e.g., indexing the entire Human genome still takes over 30 hours on a system with 2 gigabytes of physical memory. In this paper, first, we empirically demonstrate and argue that all existing suffix tree construction algorithms have a severe limitation -- to glean reasonable disk I/O efficiency, the input string being indexed must fit in main memory. This limitation is attributed to the poor locality properties of existing suffix tree construction algorithms and inhibits both sequential and parallel scalability. To deal with this limitation, second, we show that through careful algorithm design, one of the simplest suffix tree construction algorithms can be re-architected to build a suffix tree in a tiled fashion, allowing the implementation to maintain a constant working set size and fixed memory footprint when indexing strings of any size. Third, we show how improved locality of reference coupled with effective collective communication facilitates an efficient parallelization on massively parallel systems like the IBM Blue Gene/L. Finally, we empirically show that the proposed approach affords improvements of several orders of magnitude when indexing large strings. Furthermore, we demonstrate that the proposed parallelization is scalable and allows one to index the entire Human genome on a 1024 processor system in under 15 minutes.

#index 1217210
#* Data warehouse technology by infobright
#@ Dominik Ślezak;Victoria Eastwood
#t 2009
#c 5
#% 296738
#% 366687
#% 393255
#% 427307
#% 765468
#% 777988
#% 864446
#% 918001
#% 960266
#% 1022202
#% 1022304
#% 1026989
#% 1063483
#% 1125005
#% 1127565
#% 1127968
#% 1206595
#% 1206849
#% 1914458
#! We discuss Infobright technology with respect to its main features and architectural differentiators. We introduce the upcoming research and development projects that may be of special interest to the academic and industry communities.

#index 1217211
#* Stream warehousing with DataDepot
#@ Lukasz Golab;Theodore Johnson;J. Spencer Seidel;Vladislav Shkapenyuk
#t 2009
#c 5
#% 227944
#% 273943
#% 287213
#% 465007
#% 824739
#% 859024
#% 864393
#% 1054480
#% 1127381
#% 1206944
#% 1217182
#! We describe DataDepot, a tool for generating warehouses from streaming data feeds, such as network-traffic traces, router alerts, financial tickers, transaction logs, and so on. DataDepot is a streaming data warehouse designed to automate the ingestion of streaming data from a wide variety of sources and to maintain complex materialized views over these sources. As a streaming warehouse, DataDepot is similar to Data Stream Management Systems (DSMSs) with its emphasis on temporal data, best-effort consistency, and real-time response. However, as a data warehouse, DataDepot is designed to store tens to hundreds of terabytes of historical data, allow time windows measured in years or decades, and allow both real-time queries on recent data and deep analyses on historical data. In this paper we discuss the DataDepot architecture, with an emphasis on several of its novel and critical features. DataDepot is currently being used for five very large warehousing projects within AT&T; one of these warehouses ingests 500 Mbytes per minute (and is growing). We use these installations to illustrate streaming warehouse use and behavior, and design choices made in developing DataDepot. We conclude with a discussion of DataDepot applications and the efficacy of some optimizations.

#index 1217212
#* Peta-scale data warehousing at Yahoo!
#@ Mona Ahuja;Cheng Che Chen;Ravi Gottapu;Jörg Hallmann;Waqar Hasan;Richard Johnson;Maciek Kozyrczak;Ramesh Pabbati;Neeta Pandit;Sreenivasulu Pokuri;Krishna Uppala
#t 2009
#c 5
#% 77654
#% 115661
#% 248807
#% 382211
#% 1068005
#% 1138538
#% 1189164
#! Insights based on detailed data on consumer behavior, product performance and marketplace behavior are driving innovation and competition in the internet space. We introduce Everest, a SQL-compliant data warehousing engine, based on a column architecture that we have built and deployed at Yahoo!. In contrast to commercially available engines, this massively parallel engine, based on commodity hardware, offers scale, flexibility, specialized analytic operations, and lower administrative & hardware costs. In this paper, we describe the business motivation and the software and deployment architecture of Everest. The engine is in production at Yahoo! since 2007 and currently manages over six petabytes of data.

#index 1217213
#* Advances in flash memory SSD technology for enterprise database applications
#@ Sang-Won Lee;Bongki Moon;Chanik Park
#t 2009
#c 5
#% 159079
#% 632034
#% 770289
#% 770367
#% 776773
#% 902938
#% 921816
#% 960238
#% 1063551
#! The past few decades have witnessed a chronic and widening imbalance among processor bandwidth, disk capacity, and access speed of disk. According to Amdhal's law, the performance enhancement possible with a given improvement is limited by the amount that the improved feature is used. This implies that the performance enhancement of an OLTP system would be seriously limited without a considerable improvement in I/O throughput. Since the market debut of flash memory SSD a few years ago, we have made a continued effort to overcome its poor random write performance and to provide stable and sufficient I/O bandwidth. In this paper, we present three different flash memory SSD models prototyped recently by Samsung Electronics. We then show how the flash memory SSD technology has advanced to reverse the widening trend of performance gap between processors and storage devices. We also demonstrate that even a single flash memory drive can outperform a level-0 RAID with eight enterprise class 15k-RPM disk drives with respect to transaction throughput, cost effectiveness and energy consumption.

#index 1217214
#* Parallelizing extensible query optimizers
#@ Florian M. Waas;Joseph M. Hellerstein
#t 2009
#c 5
#% 1791
#% 32890
#% 43162
#% 116043
#% 207210
#% 210166
#% 300196
#% 333965
#% 334006
#% 411554
#% 442850
#% 481121
#% 482115
#% 495283
#% 564428
#% 571062
#% 581803
#% 812964
#% 1015334
#% 1127367
#% 1127442
#% 1127968
#! Query optimization is the most computationally complex task in a database management systems. In many query optimizers, faster CPUs and increased RAM can translate directly to better query plans and thus better overall system performance. Although memory size continues to scale with Moore's Law, processor speeds are leveling off. Chip manufacturers are now focusing on multicore designs that integrate increasing numbers of cores in a single CPU. Query optimizers need to be parallelized in order to continue enjoying the growth trend of Moore's Law. In this paper, we address this problem in the context of the extensible optimizer architectures found in many commercial database systems. We identify the key data dependencies inherent in the dynamic programming at the heart of these optimizers. We use this insight both to design a flexible parallel query optimization implementation, and to assess the opportunities for parallelism in this context. The proposed solutions can serve as a blueprint for retrofitting existing industry-grade optimizers to leverage multicore architectures, without requiring significant rework of the underlying infrastructure.

#index 1217215
#* A data warehouse appliance for the mass market
#@ Ravi Krishnamurthy
#t 2009
#c 5
#! Vast majority of the data warehouses have less than few terabytes of data and their performance for complex queries on traditional database systems are often not very satisfactory. Data warehouse appliances have been announced by vendors (HP Oracle Exadata Storage server, HP Neoview, Neteeza etc.) to address this burgeoning need. Most of these involve creating a large parallel database systems using scale-out of commodity machines and/or pushing filters into disk retrieval system to reduce the data coming to memory; these done along the lines pioneered by research projects such as Gamma, Bubba and other prior database machine research. These approaches deliver performance by deploying many CPUs, large amount of memory, large number of disk-heads & disk space and in effect extracting performance by under utilizing the resources -- albeit very inexpensive commodity resources. In contrast we propose a database system in a box (i.e., a single system) that can deliver high performance for complex queries while utilizing much less resources (memory, disks etc.); i.e., better resource utilization and therefore lower cost. This approach consists of using column store (pioneered in the Bubba project) which has the effect of 1) reducing the need for large number of disk heads (i.e., I/O bandwidth); and 2) reducing the need for large amount of memory for achieving memory-resident query execution. Having mitigated the disk I/O problem using column store & memory, the Von Neumann bottleneck becomes the force majeure. This problem has been pursued by database researchers in the context of cache-conscious query execution. Unfortunately, traditional CPUs provide limited control to "page" the data into the cache and retain it there to leverage the cache effectively. Our approach is to leverage a custom dataflow machine that can be coupled with a large memory and thereby practically eliminating the Von Neumann bottleneck. Besides mitigating this bottleneck, the exploitation of fine-grained pipelined and operator parallelism in hardware provides significant performance improvement. This results in a low-cost high-performance database appliance for vast majority of the data warehouse market. Kickfire has shown that such an appliance can deliver both price/performance and raw performance as compared to the competitive approaches. Note that this high performance appliance does not preclude leveraging scale-out; i.e., it can itself be used to scale-out to a much larger database in the future.

#index 1217216
#* A comparison of flexible schemas for software as a service
#@ Stefan Aulbach;Dean Jacobs;Alfons Kemper;Michael Seibold
#t 2009
#c 5
#% 210177
#% 480629
#% 864445
#% 893176
#% 960302
#% 1054227
#% 1063561
#% 1127561
#% 1666126
#! A multi-tenant database system for Software as a Service (SaaS) should offer schemas that are flexible in that they can be extended different versions of the application and dynamically modified while the system is on-line. This paper presents an experimental comparison of five techniques for implementing flexible schemas for SaaS. In three of these techniques, the database "owns" the schema in that its structure is explicitly defined in DDL. Included here is the commonly-used mapping where each tenant is given their own private tables, which we take as the baseline, and a mapping that employs Sparse Columns in Microsoft SQL Server. These techniques perform well, however they offer only limited support for schema evolution in the presence of existing data. Moreover they do not scale beyond a certain level. In the other two techniques, the application "owns" the schema in that it is mapped into generic structures in the database. Included here are XML in DB2 and Pivot Tables in HBase. These techniques give the application complete control over schema evolution, however they can produce a significant decrease in performance. We conclude that the ideal database for SaaS has not yet been developed and offer some suggestions as to how it should be designed.

#index 1217217
#* The design of the force.com multitenant internet application development platform
#@ Craig D. Weissman;Steve Bobrowski
#t 2009
#c 5
#! Force.com is the preeminent on-demand application development platform in use today, supporting some 55,000+ organizations. Individual enterprises and commercial software-as-a-service (SaaS) vendors trust the platform to deliver robust, reliable, Internet-scale applications. To meet the extreme demands of its large user population, Force.com's foundation is a metadatadriven software architecture that enables multitenant applications. The focus of this paper is multitenancy, a fundamental design approach that can dramatically improve SaaS application management. This paper defines multitenancy, explains its benefits, and demonstrates why metadata-driven architectures are the premier choice for implementing multitenancy.

#index 1217218
#* Filtered statistics
#@ Pawel Terlecki;Hardik Bati;Cesar Galindo-Legaria;Peter Zabback
#t 2009
#c 5
#% 397371
#% 458874
#% 765427
#% 765455
#% 903014
#% 1015256
#% 1015334
#% 1063561
#% 1127561
#% 1206806
#! Column statistics are an important element of cardinality estimation frameworks. More accurate estimates allow the optimizer of a RDBMS to generate better plans and improve the overall system's efficiency. This paper introduces filtered statistics, which model value distribution over a set of rows restricted by a predicate. This feature, available in Microsoft SQL Server, can be used to handle column correlation, as well as focus on interesting data ranges. In particular, it fits well for scenarios with logical subtables, like flexible schema or multi-tenant applications. Integration with the existing cardinality estimation infrastructure is presented.

#index 1217219
#* Partial join order optimization in the paraccel analytic database
#@ Yijou Chen;Richard L. Cole;William J. McKenna;Sergei Perfilov;Aman Sinha;Eugene Szedenits, Jr.
#t 2009
#c 5
#% 58376
#% 168251
#% 315024
#% 411554
#% 479938
#% 481930
#% 571062
#% 705675
#% 732896
#% 1008325
#! The ParAccel Analytic Database is a fast shared-nothing parallel relational database system with a columnar orientation, adaptive compression, memory-centric design, and an enhanced query optimizer. This modern object-oriented optimizer and its optimizer framework, known as Volt, provide efficient bulk and instance level query expression representation, multiple expression managers, and rule and cost-based expression transformation organized via multiple optimizer instances. Volt has been applied to the problem of ordering very large numbers of joins by partially ordering them for subsequent optimization using standard dynamic programming. Performance analyses show the framework's utility and the optimizer's effectiveness.

#index 1217220
#* Dynamic plan generation for parameterized queries
#@ Ahmad Ghazal;Dawit Seid;Bhashyam Ramesh;Alain Crolotte;Manjula Koppuravuri;Vinod G
#t 2009
#c 5
#% 172900
#% 248793
#% 338712
#% 571088
#% 765456
#% 770354
#% 810113
#% 1127568
#! Query processing in a DBMS typically involves two distinct phases: compilation, which generates the best plan and its corresponding execution steps, and execution, which evaluates these steps against database objects. For some queries, considerable resource savings can be achieved by skipping the compilation phase when the same query was previously submitted and its plan was already cached. In a number of important applications the same query, called a Parameterized Query (PQ), is repeatedly submitted in the same basic form but with different parameter values. PQ's are extensively used in both data update (e.g. batch update programs) and data access queries. There are tradeoffs associated with caching and re-using query plans such as space utilization and maintenance cost. Besides, pre-compiled plans may be suboptimal for a particular execution due to various reasons including data skew and inability to exploit value-based query transformation like materialized view rewrite and unsatisfiable predicate elimination. We address these tradeoffs by distinguishing two types of plans for PQ's: generic and specific plans. Generic plans are pre-compiled plans that are independent of the actual parameter values. Prior to execution, parameter values are plugged in to generic plans. In specific plans, parameter values are plugged prior to the compilation phase. This paper provides a practical framework for dynamically deciding between specific and generic plans for PQ's based on a mix of rule and cost based heuristics which are implemented in the Teradata 12.0 DBMS.

#index 1217221
#* Ordering, distinctness, aggregation, partitioning and DQP optimization in sybase ASE 15
#@ Mihnea Andrei;Xun Cheng;Sudipto Chowdhuri;Curtis Johnson;Edwin Seputis
#t 2009
#c 5
#% 210169
#% 248014
#% 334006
#% 411554
#% 442850
#% 445771
#% 458550
#% 480811
#% 481608
#% 565457
#% 571045
#% 871762
#! The Sybase ASE RDBMS version 15 was subject to major enhancements, including semantic partitions and a full QP rewrite. The new ASE QP supports horizontal and vertical parallel processing over semantically partitioned tables, and many other modern QP techniques, as cost-based eager aggregation and cost-based join relocation DQP. In the new query optimizer, the ordering, distinctness, aggregation, partitioning, and DQP optimizations were based on a common framework: plan fragment equivalence classes and logical properties. Our main outcomes are a) an eager enforcement policy for ordering, partitioning and DQP location; b) a distinctness and aggregation optimization policy, opportunistically based on the eager ordering enforcement, and which has an optimization-time computational complexity similar to join processing; c) support for the user to force all of the above optimizer decisions, still guaranteeing a valid plan, based on the Abstract Plan technology. We describe the implementation of this solution in the ASE 15 optimizer. Finally, we give our experimental results: the generation of such complex plans comes with a small increase of the optimizer's SS size, hence within an acceptable optimization time; at execution, we have obtained performance improvements of orders of magnitude for some queries.

#index 1217222
#* Taming the storage dragon: the adventures of hoTMaN
#@ Shahram Ghandeharizadeh;Andrew Goodney;Chetan Sharma;Chris Bissell;Felipe Carino;Naveen Nannapaneni;Alex Wergeles;Aber Whitcomb
#t 2009
#c 5
#% 25017
#% 83235
#% 442700
#% 1022298
#% 1065121
#% 1728238
#! HoTMaN (HoT-standby MaNager) is a joint project between MySpace and USC Database Laboratory to design and develop a tool to ensure a 24x7 up-time and ease administration of Terabytes of storage that sits underneath hundreds of database servers. The HoTMaN tool's innovation and uniqueness is that it can, with a few clicks, perform operational tasks that require hundreds of keyboard strokes by "trusted trained" experts. With HoTMaN, MySpace can within minutes migrate the relational database(s) of a failed server to a hot-standby. A process that could take over 1 hour and had a high potential for human error is now performed reliably. A database internal to HoTMaN captures all virtual disks, volume and file configurations associated with each SQL Server and candidate hot-standby servers where SQL server processing could be migrated. HoTMaN is deployed in production and its current operational benefits include: (i) enhanced availability of data, and (ii) planned maintenance and patching.

#index 1217223
#* Compensation-aware data types in RDBMS
#@ Aravind Yalamanchi;Dieter Gawlick
#t 2009
#c 5
#% 4618
#% 9241
#% 32897
#% 86938
#% 172878
#% 480425
#% 481752
#! In a traditional database system, the transaction management protocols and mechanisms are constrained by the fundamental properties of atomicity, consistency, isolation, and durability (ACID). A transaction management system with strict ACID properties typically employs read and write locks, held for the duration of the transaction, to protect its uncommitted data from being seen and modified by some other transaction. While this approach is effective for applications involving short execution times and relatively small number of concurrent operations, it is too restrictive for applications that involve reactive, long-lived, and complex transactions. The common denominator of such applications is the need for transactions to read and possibly modify uncommitted data values [1] and for the database system to still retain the ability to abort a transaction and the ability to recover from failures. This paper proposes a Business Transaction framework that allows long lasting, discontinuous, and resumable transactions to perform shared updates to common data by holding semantic locks on the modified rows. Under this framework, basic SQL data types are made compensation-aware by associating domain-specific shared update semantics with them. These semantics ensure that each data modification operation is compatible with other uncommitted activity on the same data and that the operation can be undone, if needed, without resorting to cascading aborts. This paper describes the key concepts and presents our approach for supporting shared updates in Oracle RDBMS.

#index 1217224
#* Access control in the aqualogic data services platform
#@ Vinayak Borkar;Michael Carey;Daniel Engovatov;Dmitry Lychagin;Panagiotis Reveliotis;Joshua Spiegel;Sachin Thatte;Till Westmann
#t 2009
#c 5
#% 875028
#% 893174
#% 1206803
#! The AquaLogic Data Services Platform (ALDSP) is a middleware platform for building data services that integrate and provide operations over data drawn from spanning multiple heterogeneous information sources. A data service consists of an XML Schema instance, describing its information content, and a collection of XQuery functions and procedures that comprise its set of operations. This paper describes access control in ALDSP. We describe ALDSP's securable resource hierarchy, its fine-grained access control capabilities for securing portions of data service schemas, how XQuery can be used to specify data-driven security policies, and how user identity mapping is supported. We then provide an in-depth overview of how ALDSP works, including implementation techniques to keep access control checking from interacting badly with view rewriting, query optimization, and caching.

#index 1217225
#* Building community-centric information exploration applications on social content sites
#@ Sihem Amer-Yahia;Jian Huang;Cong Yu
#t 2009
#c 5
#% 319705
#% 481290
#% 722904
#% 813966
#% 848640
#% 1001299
#% 1063553
#% 1206904
#! Social content sites [4], which integrate traditional content sites with social networking features, have recently emerged as an exciting new trend on the Web. Users on those sites share content and form various communities based on explicit friendship, shared interest and common user properties. Recently, we proposed SOCIALSCOPE, a three-layered architecture to address the information management challenges in social content sites. In this paper, we focus on the information discovery and the information presentation layers, and describe how our previously proposed language, Jelly [3], is supported in SOCIALSCOPE to build community-centric information exploration applications on social content sites.

#index 1217226
#* QoX-driven ETL design: reducing the cost of ETL consulting engagements
#@ Alkis Simitsis;Kevin Wilkinson;Malu Castellanos;Umeshwar Dayal
#t 2009
#c 5
#% 36117
#% 300166
#% 333848
#% 366617
#% 385321
#% 800563
#% 1016606
#% 1181213
#% 1206731
#% 1206944
#% 1720926
#! As business intelligence becomes increasingly essential for organizations and as it evolves from strategic to operational, the complexity of Extract-Transform-Load (ETL) processes grows. In consequence, ETL engagements have become very time consuming, labor intensive, and costly. At the same time, additional requirements besides functionality and performance need to be considered in the design of ETL processes. In particular, the design quality needs to be determined by an intricate combination of different metrics like reliability, maintenance, scalability, and others. Unfortunately, there are no methodologies, modeling languages or tools to support ETL design in a systematic, formal way for achieving these quality requirements. The current practice handles them with ad-hoc approaches only based on designers' experience. This results in either poor designs that do not meet the quality objectives or costly engagements that require several iterations to meet them. A fundamental shift that uses automation in the ETL design task is the only way to reduce the cost of these engagements while obtaining optimal designs. Towards this goal, we present a novel approach to ETL design that incorporates a suite of quality metrics, termed QoX, at all stages of the design process. We discuss the challenges and tradeoffs among QoX metrics and illustrate their impact on alternative designs.

#index 1217227
#* Query optimizers: time to rethink the contract?
#@ Surajit Chaudhuri
#t 2009
#c 5
#% 13018
#% 86949
#% 102784
#% 116043
#% 172900
#% 210353
#% 248014
#% 248793
#% 248795
#% 273901
#% 273908
#% 273909
#% 300167
#% 333986
#% 397371
#% 411554
#% 463444
#% 480803
#% 503535
#% 810017
#% 824682
#% 824756
#% 864426
#% 1015256
#% 1015334
#% 1016221
#% 1022292
#% 1026989
#% 1127441
#% 1206720
#% 1206942
#! Query Optimization is expected to produce good execution plans for complex queries while taking relatively small optimization time. Moreover, it is expected to pick the execution plans with rather limited knowledge of data and without any additional input from the application. We argue that it is worth rethinking this prevalent model of the optimizer. Specifically, we discuss how the optimizer may benefit from leveraging rich usage data and from application input. We conclude with a call to action to further advance query optimization technology.

#index 1217228
#* Design for interaction
#@ Daniel Tunkelang
#t 2009
#c 5
#! Research in information retrieval has focused on presenting the most relevant results to a user in response to a free-text search query. Research in database systems assumes a model where the user enters a formal query, and the results are exactly those the user requested. Neither community has emphasized user interaction—a critical concern for practical information access. As William Goffman noted in the 1960s and Nick Belkin continually reminds us today, the relationship between a document and query, though necessary, is not sufficient to determine relevance—yet ranked retrieval approaches rely heavily or exclusively on this relationship. Meanwhile, recent work on database usability by Jeff Naughton and H.V. Jagadish surfaces the rigidity of database systems that return nothing unless users know how to formulate precise queries. This talk presents human-computer information retrieval (HCIR) as a general approach that addresses some of the key challenges facing both research communities. A vision first put forward by Gary Marchionini, HCIR expects people and systems to work together to implement information access. Such an approach requires rethinking information access not as a matching or ranking problem, but rather as a communication problem. Specifically, we need interfaces that optimize the bidirectional communication between the user and the system, thus optimizing the symbiotic division of labor between the two. This talk reviews the history of HCIR efforts and presents ongoing work to implement the HCIR vision. In particular, it presents an interactive set retrieval approach that responds to queries with an overview of the user's current context and an organized set of options for incremental exploration.

#index 1217229
#* Voyagers and voyeurs: supporting social data analysis
#@ Jeffrey Heer
#t 2009
#c 5
#! In recent years, researchers and entrepreneurs have introduced new online services for data collection and analysis, often relying on interactive visualizations to enable mass interaction with data. These sites represent the first step in what looks to be a growing online phenomenon: social data analysis, that is, collective analysis of data supported by social interaction. Engaging crowds of both experts and non-experts in the process of data exploration has applications ranging from political transparency to business intelligence to citizen science. Achieving this vision, however, will require further innovation in the design of systems for collaborative data management. In this talk, I will highlight recent efforts to use the web as a platform for collectively creating, managing, and analyzing data. I will share how web citizens are collaborating with data, and discuss how we might design systems to catalyze social forms of data management and exploration.

#index 1217230
#* Augmented social cognition: using social web technology to enhance the ability of groups to remember, think, and reason
#@ Ed H. Chi
#t 2009
#c 5
#% 216249
#% 344438
#% 840404
#% 857478
#% 860483
#% 954955
#% 955306
#% 1047347
#% 1047408
#% 1047414
#% 1065406
#% 1070493
#% 1132899
#% 1132901
#% 1183138
#% 1183273
#% 1183274
#! We are experiencing a new Social Web, where people share, communicate, commiserate, and conflict with each other. As evidenced by systems like Wikipedia, twitter, and delicious.com, these environments are turning people into social information foragers and sharers. Groups interact to resolve conflicts and jointly make sense of topic areas from "Obama vs. Clinton" to "Islam." PARC's Augmented Social Cognition researchers -- who come from cognitive psychology, computer science, HCI, CSCW, and other disciplines -- focus on understanding how to "enhance a group of people's ability to remember, think, and reason". Through Social Web systems like social bookmarking sites, blogs, Wikis, and more, we can finally study, in detail, these types of enhancements on a very large scale. Here we summarize recent work and early findings such as: (1) how conflict and coordination have played out in Wikipedia, and how social transparency might affect reader trust; (2) how decreasing interaction costs might change participation in social tagging systems; and (3) how computation can help organize user-generated content and metadata.

#index 1217231
#* Storage class memory: technology, systems and applications
#@ Richard F. Freitas
#t 2009
#c 5
#! Storage Class Memory (SCM) is the term used to describe a new class of solid-state, nonvolatile memory technologies. There are several (≥10) such technologies currently under active research and development that are vying to become the SCM of choice. As a group, their performance is nearer memory than storage while their cost is nearer storage than memory. In the systems designer's eye, these performance and cost characteristics blur the historic distinction between memory and storage. Exploiting SCM will require significant changes in the design of memory and storage systems. These changes at the system level will force or enable changes at the application level as well as opening up new application opportunities. In this talk we will discuss several of today's leading SCM technologies, discuss their potential impact on the design of memory and storage systems, consider the changes that might be needed to existing applications and explore some of the potential opportunities for future applications.

#index 1217232
#* Distributed data-parallel computing using a high-level programming language
#@ Michael Isard;Yuan Yu
#t 2009
#c 5
#% 86929
#% 115661
#% 442698
#% 442700
#% 765473
#% 961244
#% 963669
#% 983467
#% 1063553
#% 1127559
#% 1127968
#% 1468421
#! The Dryad and DryadLINQ systems offer a new programming model for large scale data-parallel computing. They generalize previous execution environments such as SQL and MapReduce in three ways: by providing a general-purpose distributed execution engine for data-parallel applications; by adopting an expressive data model of strongly typed .NET objects; and by supporting general-purpose imperative and declarative operations on datasets within a traditional high-level programming language. A DryadLINQ program is a sequential program composed of LINQ expressions performing arbitrary side-effect-free operations on datasets, and can be written and debugged using standard .NET development tools. The DryadLINQ system automatically and transparently translates the data-parallel portions of the program into a distributed execution plan which is passed to the Dryad execution platform. Dryad, which has been in continuous operation for several years on production clusters made up of thousands of computers, ensures efficient, reliable execution of this plan on a large compute cluster. This paper describes the programming model, provides a high-level overview of the design and implementation of the Dryad and DryadLINQ systems, and discusses the tradeoffs and connections to parallel and distributed databases.

#index 1217233
#* Large-scale uncertainty management systems: learning and exploiting your data
#@ Shivnath Babu;Sudipto Guha;Kamesh Munagala
#t 2009
#c 5
#% 165663
#% 300167
#% 415062
#% 576218
#% 757953
#% 765257
#% 765435
#% 809256
#% 810017
#% 813744
#% 862540
#% 874897
#% 893118
#% 893168
#% 898271
#% 963244
#% 991152
#% 1016178
#% 1022261
#% 1022272
#% 1022347
#% 1039704
#% 1063728
#% 1083643
#% 1206851
#% 1217142
#% 1468266
#% 1700123
#% 2092350
#! The database community has made rapid strides in capturing, representing, and querying uncertain data. Probabilistic databases capture the inherent uncertainty in derived tuples as probability estimates. Data acquisition and stream systems can produce succinct summaries of very large and time-varying datasets. This tutorial addresses the natural next step in harnessing uncertain data: How can we efficiently and quantifiably determine what, how, and how much to learn in order to make good decisions based on the imprecise information available. The material in this tutorial is drawn from a range of fields including database systems, control and information theory, operations research, convex optimization, and statistical learning. The focus of the tutorial is on the natural constraints that are imposed in a database context and the demands of imprecise information from an optimization point of view. We look both into the past as well as into the future; to discuss general tools and techniques that can serve as a guide to database researchers and practitioners, and to enumerate the challenges that lie ahead.

#index 1217234
#* FPGA: what's in it for a database?
#@ Rene Mueller;Jens Teubner
#t 2009
#c 5
#% 397361
#% 438239
#% 765419
#% 785284
#% 831186
#% 874997
#% 960254
#% 1011733
#% 1022232
#% 1022311
#% 1127400
#% 1127563
#% 1157687
#% 1207014
#! While there seems to be a general agreement that next years' systems will include many processing cores, it is often overlooked that these systems will also include an increasing number of different cores (we already see dedicated units for graphics or network processing). Orchestrating the diversity of processing functionality is going to be a major challenge in the upcoming years, be it to optimize for performance or for minimal energy consumption. We expect field-programmable gate arrays (FPGAs or "programmable hardware") to soon play the role of yet another processing unit, found in commodity computers. It is clear that the new resource is going to be too precious to be ignored by database systems, but it is unclear how FPGAs could be integrated into a DBMS. With a focus on database use, this tutorial introduces into the emerging technology, demonstrates its potential, but also pinpoints some challenges that need to be addressed before FPGA-accelerated database systems can go mainstream. Attendees will gain an intuition of an FPGA development cycle, receive guidelines for a "good" FPGA design, but also learn the limitations that hardware-implemented database processing faces. Our more high-level ambition is to spur a broader interest in database processing on novel hardware technology.

#index 1217235
#* Keyword search on structured and semi-structured data
#@ Yi Chen;Wei Wang;Ziyang Liu;Xuemin Lin
#t 2009
#c 5
#% 330678
#% 479803
#% 654442
#% 810052
#% 824693
#% 824792
#% 839172
#% 863389
#% 864456
#% 874876
#% 874894
#% 875017
#% 893187
#% 956599
#% 960234
#% 960235
#% 960243
#% 960245
#% 960259
#% 960261
#% 960285
#% 960363
#% 993987
#% 1015258
#% 1016135
#% 1022290
#% 1022318
#% 1044480
#% 1063493
#% 1063536
#% 1063537
#% 1063538
#% 1063539
#% 1127413
#% 1127423
#% 1127424
#% 1127445
#% 1127570
#% 1127585
#% 1181222
#% 1181246
#% 1181282
#% 1181284
#% 1206722
#% 1206760
#% 1206801
#% 1206843
#% 1206910
#% 1206911
#% 1206926
#% 1206957
#% 1206997
#! Empowering users to access databases using simple keywords can relieve the users from the steep learning curve of mastering a structured query language and understanding complex and possibly fast evolving data schemas. In this tutorial, we give an overview of the state-of-the-art techniques for supporting keyword search on structured and semi-structured data, including query result definition, ranking functions, result generation and top-k query processing, snippet generation, result clustering, query cleaning, performance optimization, and search quality evaluation. Various data models will be discussed, including relational data, XML data, graph-structured data, data streams, and workflows. We also discuss applications that are built upon keyword search, such as keyword based database selection, query generation, and analytical processing. Finally we identify the challenges and opportunities of future research to advance the field.

#index 1217236
#* Database research in computer games
#@ Alan Demers;Johannes Gehrke;Christoph Koch;Ben Sowell;Walker White
#t 2009
#c 5
#% 402644
#% 960236
#% 1207009
#! This tutorial presents an overview of the data management issues faced by computer games today. While many games do not use databases directly, they still have to process large amounts of data, and could benefit from the application of database technology. Other games, such as massively multiplayer online games (MMOs), must communicate with commercial databases and have their own unique challenges. In this tutorial we will present the state-of-the-art of data management in games that we learned from our interaction with various game studios. We will show how the issues involved motivate current research, and illustrate several possibilities for future work. Our tutorial will start with a description of data-driven design, which is the source of many of the data management issues that games face. We will show some of the tools that game developers use to create and manage content. We will discuss how this type of design can affect performance, and the data structures and techniques that developers use to ensure that the game is responsive. We will discuss the problem of consistency in games, and how games ensure that players all share the same view of the world. Finally, we will examine some of the engineering issues that game developers have to deal with when interacting with traditional databases. This tutorial is intended to be self-contained, and provides the background necessary for understanding how databases and database technology are relevant to computer games. This tutorial is accessible to students and researchers who, while perhaps not hardcore gamers themselves, are interested in ways in which they can use their expertise to solve problems in computer games.

#index 1217237
#* Anonymized data: generation, models, usage
#@ Graham Cormode;Divesh Srivastava
#t 2009
#c 5
#% 94459
#% 443463
#% 576761
#% 810120
#% 823358
#% 864394
#% 864412
#% 881463
#% 893100
#% 956511
#% 1022344
#% 1063476
#% 1083709
#% 1127360
#% 1127361
#% 1127417
#% 1206714
#% 1206763
#% 1415851
#! Data anonymization techniques have been the subject of intense investigation in recent years, for many kinds of structured data, including tabular, graph and item set data. They enable publication of detailed information, which permits ad hoc queries and analyses, while guaranteeing the privacy of sensitive information in the data against a variety of attacks. In this tutorial, we aim to present a unified framework of data anonymization techniques, viewed through the lens of uncertainty. Essentially, anonymized data describes a set of possible worlds, one of which corresponds to the original data. We show that anonymization approaches such as suppression, generalization, perturbation and permutation generate different working models of uncertain data, some of which have been well studied, while others open new directions for research. We demonstrate that the privacy guarantees offered by methods such as k-anonymization and l-diversity can be naturally understood in terms of similarities and differences in the sets of possible worlds that correspond to the anonymized data. We describe how the body of work in query evaluation over uncertain databases can be used for answering ad hoc queries over anonymized data in a principled manner. A key benefit of the unified approach is the identification of a rich set of new problems for both the Data Anonymization and the Uncertain Data communities.

#index 1217238
#* PRIMA: archiving and querying historical data with evolving schemas
#@ Hyun J. Moon;Carlo A. Curino;Myungwon Ham;Carlo Zaniolo
#t 2009
#c 5
#% 442967
#% 527786
#% 1015271
#% 1126566
#% 1127411
#% 1127421
#! Schema evolution poses serious challenges in historical data management. Traditionally, historical data have been archived either by (i) migrating them into the current schema version that is well-understood by users but compromising archival quality, or (ii) by maintaining them under the original schema version in which the data was originally created, leading to perfect archival quality, but forcing users to formulate queries against complex histories of evolving schemas. In the PRIMA system, we achieve the best of both approaches, by (i) archiving historical data under the schema version under which they were originally created, and (ii) letting users express temporal queries using the current schema version. Thus, in PRIMA, the system rewrites the queries to the (potentially many) pertinent versions of the evolving schema. Moreover, the system o ers automatic documentation of the schema history, and allows the users to pose temporal queries over the metadata history itself. The proposed demonstration highlights the system features exploiting both a synthetic-educational running example and the real-life evolution histories (schemas and data), which include hundreds of schema versions from Wikipedia and Ensembl. The demonstration off ers a thorough walk-through of the system features and a hands-on system testing phase, where the audiences are invited to directly interact with the advanced query interface of PRIMA.

#index 1217239
#* DejaVu: declarative pattern matching over live and archived streams of events
#@ Nihal Dindar;Baris Güç;Patrick Lau;Asli Ozal;Merve Soner;Nesime Tatbul
#t 2009
#c 5
#% 810096
#% 878299
#% 923029
#! DejaVu is an event processing system that integrates declarative pattern matching over live and archived streams of events on top of a novel system architecture. We propose to demonstrate the key aspects of the DejaVu query language and architecture using two different application scenarios, namely a smart RFID library system and a financial market data analysis application. The demonstration will illustrate how DejaVu can uniformly handle one-time, continuous, and hybrid pattern matching queries over live and archived stream stores, using highly interactive visual monitoring tools including one that is based on the Second Life virtual world.

#index 1217240
#* StreamShield: a stream-centric approach towards security and privacy in data stream environments
#@ Rimma V. Nehme;Hyo-Sang Lim;Elisa Bertino;Elke A. Rundensteiner
#t 2009
#c 5
#% 308087
#% 408947
#% 838415
#% 1026989
#% 1206727
#! We propose to demonstrate the StreamShield, a system designed to address the problem of security and privacy in the context of Data Stream Management Systems (DSMSs). In StreamShield, continuous access control is enforced by taking a novel "stream-centric" approach towards security. Security policies are not persistently stored on the server, but rather are depicted by security metadata, called "security punctuations", and get embedded into streams together with the data. We distinguish between two types of security punctuations: (1) the "data security punctuations" (dsps) describing the data-side security policies, and (2) the "query security punctuations" (qsps) representing the query-side security policies. The advantages of such stream-centric security model include flexibility, dynamicity and speed of enforcement. Furthermore, DSMSs can adapt to not only data-related but also to security-related selectivities, which helps reduce the waste of resources, when few subjects have access to streaming data.

#index 1217241
#* Supporting a spectrum of out-of-order event processing technologies: from aggressive to conservative methodologies
#@ Mingzhu Wei;Mo Liu;Ming Li;Denis Golovnya;Elke A. Rundensteiner;Kajal Claypool
#t 2009
#c 5
#% 481448
#% 578391
#% 771230
#% 800583
#% 801694
#% 875004
#% 1207016
#! This demonstration presents a complex event processing system which focuses on out-of-order handling. State-of-the-art event stream processing technology experiences significant challenges when faced with out-of-order data arrival including huge system latencies, missing results, and incorrect result generation. We propose two out-of-order handling techniques, conservative and aggressive strategies. We will show the efficiency of our techniques and how they can satisfy various QoS requirements of different applications.

#index 1217242
#* ELMR: lightweight mobile health records
#@ Arvind Kumar;Amey Purandare;Jay Chen;Arthur Meacham;Lakshminarayanan Subramanian
#t 2009
#c 5
#% 1047383
#! Cell phones are increasingly being used as common clients for a wide suite of distributed, database-centric healthcare applications in developing regions. This is particularly true for rural developing regions where the bulk of the healthcare is handled by health workers due to lack of doctors; the widespread availability of cellular services have made mobile devices as an important computing platform for enabling healthcare applications for these health workers. Unfortunately, the current SQL model for distributed client/server systems is far too heavy-weight for these applications, particularly in light of the high communications cost and extremely limited data transmission capacity available in these environments. In this demonstration, we describe the Efficient Lightweight Mobile Records (ELMR) system that provides a practical and lightweight database access protocol for accessing and updating records remotely from mobile devices under an extremely bandwidth and cost-constrained Short Messaging Service (SMS) channel comprising of 140 byte packets. We have implemented ELMR using the RMS functionality in J2ME, and integrated it into an HIV treatment application we are developing for use by African health workers.

#index 1217243
#* Bridging the application and DBMS divide using static analysis and dynamic profiling
#@ Surajit Chaudhuri;Vivek Narasayya;Manoj Syamala
#t 2009
#c 5
#% 903261
#% 1022308
#% 1207033
#! Relational database management systems (RDBMSs) today serve as the backend for many real-world data intensive applications. Database developers use data access APIs such as ADO.NET to execute SQL queries and access data. While modern program analysis and code profilers are extensively used during the software development life cycle, there is a significant gap in these technologies for database applications because these tools have little or no understanding of data access APIs or the DBMS. We have developed tools that: (a) Enhance traditional static analysis of programs by leveraging understanding of database APIs to help developers identify security, correctness and performance problems in the application. This enables such problems to be detected early in the application lifecycle. (b) Extend the existing DBMS and application profiling infrastructure to enable correlation of application events with DBMS events. This allows profiling across application, data access and DBMS layers. We demonstrate how our tools enable a rich class of analysis, tuning and profiling tasks that are otherwise not possible today.

#index 1217244
#* Interactive plan hints for query optimization
#@ Nicolas Bruno;Surajit Chaudhuri;Ravishankar Ramamurthy
#t 2009
#c 5
#% 285924
#% 1206942
#! Commercial database systems expose query hints to fix poor plans produced by the query optimizer. However, current query hints are not flexible enough to deal with a variety of non-trivial scenarios, and can be at times cumbersome for DBAs to interact with. In this demonstration we present a framework that enables visual specification of hints to influence the optimizer to pick better plans. Our framework goes considerably beyond existing hinting mechanisms and significantly improves the usability of such functionality.

#index 1217245
#* What's on the grapevine?
#@ Albert Angel;Nick Koudas;Nikos Sarkas;Divesh Srivastava
#t 2009
#c 5
#% 1022338
#% 1063570
#% 1275182
#! User generated content and social media (in the form of blogs, wikis, online video, microblogs, etc) are proliferating online. Grapevine conducts large scale data analysis on the social media collective, distilling and extracting information in real time. It aims to track entities and stories of interest in millions of blog posts, thousands of tweets, news items, etc., daily. Grapevine facilitates the interactive exploration of content, allowing users to discover interesting or surprising stories, optionally narrowed down on a specific demographic of interest (e.g. "What are Torontonians talking about on blogs?", "What are popular stories across news sources in Canada?", "What are financiers in Texas blogging about today?"). Stories of interest can be explored in a variety of ways, such as modifying their scope, obtaining related content (blog posts, news, etc), and examining their temporal evolution.

#index 1217246
#* Interactive anonymization of sensitive data
#@ Xiaokui Xiao;Guozhang Wang;Johannes Gehrke
#t 2009
#c 5
#% 443463
#% 810011
#% 937550
#% 1206678
#! There has been much recent work on algorithms for limiting disclosure in data publishing, however they have not been put to use in any toolkit for practicioners. We will demonstrate CAT, the Cornell Anonymization Toolkit, designed for interactive anonymization. CAT has an interface that is easy to use; it guides users through the process of preparing a dataset for publication while limiting disclosure through the identification of records that have high risk under various attacker models.

#index 1217247
#* The perm provenance management system in action
#@ Boris Glavic;Gustavo Alonso
#t 2009
#c 5
#% 318704
#% 1181296
#% 1206861
#! In this demonstration we present the Perm provenance management system (PMS). Perm is capable of computing, storing and querying provenance information for the relational data model. Provenance is computed by using query rewriting techniques to annotate tuples with provenance information. Thus, provenance data and provenance computations are represented as relational data and queries and, hence, can be queried, stored and optimized using standard relational database techniques. This demo shows the complete Perm system and lets attendants examine in detail the process of query rewriting and provenance retrieval in Perm, the most complete data provenance system available today. For example, Perm supports lazy and eager provenance computation, external provenance and various contribution semantics.

#index 1217248
#* A decisions query language (DQL): high-level abstraction for mathematical programming over databases
#@ Alexander Brodsky;Mayur M. Bhot;Manasa Chandrashekar;Nathan E. Egge;X. Sean Wang
#t 2009
#c 5
#% 1029601
#! The demonstrated, high-level decisions query language DQL combines the decision optimization capability of mathematical programming and the data manipulation capability of traditional database query languages. DQL benefits application developers in two aspects. First, it avoids a conceptual impedance mismatch between mathematical programming and data access and makes decision optimization functionality readily accessible to database programmers with no prior experience in operations research. Second, a tight integration provides unique opportunities for more efficient evaluation as compared to a loosely coupled system. This demonstration uses an emergency response scenario to illustrate the power of the language and its implementation.

#index 1217249
#* FERRY: database-supported program execution
#@ Torsten Grust;Manuel Mayr;Jan Rittinger;Tom Schreiber
#t 2009
#c 5
#% 5379
#% 737199
#% 875029
#% 997039
#% 1016150
#! We demonstrate the language Ferry and its editing, compilation, and execution environment FerryDeck. Ferry's type system and operations match those of scripting or programming languages; its compiler has been designed to emit (bundles of) compliant and efficient SQL:1999 statements. Ferry acts as glue that permits a programming style in which developers access database tables using their programming language's own syntax and idioms -- the Ferry-expressible fragments of such programs may be executed by a relational database back-end, i.e., close to the data. The demonstrator FerryDeck implements compile-and-execute-as-you-type interactivity for Ferry and offers a variety of (graphical) hooks to explore and inspect this approach to database-supported program execution.

#index 1217250
#* Extreme visualisation of query optimizer search space
#@ Anisoara Nica;Daniel Scott Brotherston;David William Hillis
#t 2009
#c 5
#% 843682
#% 1207098
#! This demonstration showcases a system for visualizing and analyzing search spaces generated by the SQL Anywhere optimizer during the optimization process of a SQL statement. SQL Anywhere dynamically optimizes each statement every time it is executed. The decisions made by the optimizer during the optimization process are both cost-based and heuristics adapted to the current state of the server and the database instance. Many performance issues can be understood and resolved by analyzing the search space generated when optimizing a certain request. In our experience, there are two main classes of performance issues related to the decisions made by a query optimizer:(1) a request is very slow due to a suboptimal access plan; and (2) a request has a different, less optimal access plan than a previous execution. We have enhanced SQL Anywhere to log, in a very compact format, its search space during the optimization process when tracing mode is on. These search space logs can be used for performance analysis in the absence of the database instances or of extra information about the SQL Anywhere server state at the time the logs were generated. This demonstration introduces the SearchSpaceAnalyzer System, a research prototype used to analyze the search spaces of the SQL Anywhere optimizer. The system visualizes and analyzes (1) a single search space and (2) the differences between two search spaces generated for the same query by two different optimization processes. The SearchSpaceAnalyze System can be used for the analysis of any query optimizer search spaces as long as the logged data is recorded using the syntax understood by the system.

#index 1217251
#* MayBMS: a probabilistic database management system
#@ Jiewen Huang;Lyublena Antova;Christoph Koch;Dan Olteanu
#t 2009
#c 5
#% 314829
#% 1127376
#% 1206717
#% 1206987
#! MayBMS is a state-of-the-art probabilistic database management system which leverages the strengths of previous database research for achieving scalability. As a proof of concept for its ease of use, we have built on top of MayBMS a Web-based application that offers NBA-related information based on what-if analysis of team dynamics using data available at www.nba.com.

#index 1217252
#* ORDEN: outlier region detection and exploration in sensor networks
#@ Conny Franke;Michael Gertz
#t 2009
#c 5
#% 1159292
#! Sensor networks play a central role in applications that monitor variables in geographic areas such as the traffic volume on roads or the temperature in the environment. A key feature users are often interested in when employing such systems is the detection of unusual phenomena, that is, anomalous values measured by the sensors. In this demonstration, we present a system, called ORDEN, that allows for the detection and (visual) exploration of outliers and anomalous events in sensor networks in real-time. In particular, the system constructs outlier regions from anomalous sensor measurements to provide for a comprehensive description of the spatial extent of phenomena of interest. With our system, users can interactively explore displayed outlier regions and investigate the heterogeneity within individual regions using different parameter and threshold settings. Using real-world sensor data streams from different application domains, we demonstrate the effectiveness and utility of our system.

#index 1217253
#* Exploring biomedical databases with BioNav
#@ Abhijith Kashyap;Vagelis Hristidis;Michalis Petropoulos;Sotiria Tavoulari
#t 2009
#c 5
#% 765464
#% 960287
#% 1206993
#! We demonstrate the BioNav system, a novel search interface for biomedical databases, such as PubMed. BioNav enables users to navigate large number of query results by categorizing them using MeSH; a comprehensive concept hierarchy used by PubMed. Once the query results are organized into a navigation tree, BioNav reveals only a small subset of the concept nodes at each step, selected such that the expected user navigation cost is minimized. In contrast, previous works expand the hierarchy in a predefined static manner, without navigation cost modeling. BioNav is available at http://db.cse.buffalo.edu/bionav.

#index 1217254
#* MobileMiner: a real world case study of data mining in mobile communication
#@ Tengjiao Wang;Bishan Yang;Jun Gao;Dongqing Yang;Shiwei Tang;Haoyu Wu;Kedong Liu;Jian Pei
#t 2009
#c 5
#% 469422
#% 823347
#% 1023265
#% 1176867
#! Mobile communication data analysis has been often used as a background application to motivate many data mining problems. However, very few data mining researchers have a chance to see a working data mining system on real mobile communication data. In this demo, we showcase our new system MobileMiner on a real mobile communication data set, which presents a case study of business solutions using state-of-the-art data mining techniques. MobileMiner adaptively profiles users' behavior from their calling and moving record streams. Customer segmentation and social community analysis can be conducted based on user profiles. We show how data mining techniques can help in mobile communication data analysis. Moreover, we also show some interesting observations which still cannot be mined by the current techniques, and thus may motivate new research and development.

#index 1217255
#* Fast and dynamic OLAP exploration using UDFs
#@ Zhibo Chen;Carlos Ordonez;Carlos Garcia-Alvarado
#t 2009
#c 5
#% 223781
#% 1015294
#% 1131093
#! OLAP is a set of database exploratory techniques to efficiently retrieve multiple sets of aggregations from a large dataset. Generally, these techniques have either involved the use of an external OLAP server or required the dataset to be exported to a specialized OLAP tool for more efficient processing. In this work, we show that OLAP techniques can be performed within a modern DBMS without external servers or the exporting of datasets, using standard SQL queries and UDFs. The main challenge of such approach is that SQL and UDFs are not as flexible as the C language to explore the OLAP lattice and therefore it is more difficult to develop optimizations. We compare three different ways of performing OLAP exploration: plain SQL queries, a UDF implementing a lattice structure, and a UDF programming the star cube structure. We demonstrate how such methods can be used to efficiently explore typical OLAP datasets.

#index 1217256
#* AIDE: ad-hoc intents detection engine over query logs
#@ Yunliang Jiang;Hui-Ting Yang;Kevin Chen-chuan Chang;Yi-Shin Chen
#t 2009
#c 5
#% 590523
#% 754068
#% 956503
#! While keyword queries have become the "standard" query language of web search and many other database applications, their brevity and unstructuredness make it difficult to detect what users really want. In this demonstration, we aim to detect such hidden query intents, which we define as the frequent phrases that users co-ask with the query term, by exploring query logs. Toward building an online search system AIDE, we offer users the function to detect general and unique intents using arbitrary ad-hoc queries at run time. We will also demonstrate the effectiveness of the system which achieves indexing and searching over 14M MSN query log records.

#index 1217257
#* Exploring schema repositories with schemr
#@ Kuang Chen;Jayant Madhavan;Alon Halevy
#t 2009
#c 5
#% 431103
#% 572314
#% 801413
#% 893115
#% 1206886
#! Schemr is a schema search engine, and provides users the ability to search for and visualize schemas stored in a metadata repository. Users may search by keywords and by example -- using schema fragments as query terms. Schemr uses a novel search algorithm, based on a combination of text search and schema matching techniques, as well as a structurally-aware scoring metric. Schemr presents search results in a GUI that allows users to explore which elements match and how well they do. The GUI supports interactions, including panning, zooming, layout and drilling-in. We demonstrate schema search and visualization, introduce Schemr as a new component of the information integration toolbox, and discuss its benefits in several applications.

#index 1217258
#* Search your memory ! - an associative memory based desktop search system
#@ Jidong Chen;Hang Guo;Wentao Wu;Chunxin Xie
#t 2009
#c 5
#% 451595
#% 642983
#% 1016176
#% 1047409
#% 1667783
#! We present XSearcher, an associative memory based desktop search system, which exploits associations by creating semantic links of personal desktop resources from explicit and implicit user activities. With these links, associations among memory fragments can be built or rebuilt in a user's brain during a search. The personalized ranking scheme uses these links together with a user's personal preferences to rank results by both relevance and importance. XSearcher enhances traditional keyword based search systems since it is closer to the way that human associative memory works.

#index 1217259
#* ExQueX: exploring and querying XML documents
#@ Benny Kimelfeld;Yehoshua Sagiv;Gidi Weber
#t 2009
#c 5
#% 465068
#% 810101
#% 838492
#% 1015258
#% 1016135
#! ExQueX is an interactive system for exploring and querying XML documents. The exploration is done by searching, ranking and filtering, and it enables users to discover relationships that exist in a given document. The results of the exploration can be used either directly as tree queries or as building blocks in the process of formulating more complex queries. The latter is done by formulating an abstract tree query, whose edges represent relationships that are resolved by using concrete results from the exploration phase. ExQueX facilitates fast and clear understanding of both the contents and complex structures of XML documents.

#index 1217260
#* CourseRank: a social system for course planning
#@ Benjamin Bercovitz;Filip Kaliszan;Georgia Koutrika;Henry Liou;Zahra Mohammadi Zadeh;Hector Garcia-Molina
#t 2009
#c 5
#% 860021
#% 869525
#% 1055739
#! Special-purpose social sites can offer valuable services to well-defined, closed, communities, e.g., in a university or in a corporation. The purpose of this demo is to show the challenges, special features and potential of a focused social system in action through CourseRank, a course evaluation and planning social system.

#index 1217261
#* SmartCIS: integrating digital and physical environments
#@ Mengmeng Liu;Svilen R. Mihaylov;Zhuowei Bao;Marie Jacob;Zachary G. Ives;Boon Thau Loo;Sudipto Guha
#t 2009
#c 5
#% 378409
#% 479452
#% 654482
#% 731480
#% 874976
#% 878299
#% 1015281
#% 1026962
#% 1063587
#% 1083759

#index 1217262
#* DataLens: making a good first impression
#@ Bin Liu;H. V. Jagadish
#t 2009
#c 5
#% 210173
#% 333854
#% 443531
#% 479816
#% 765464
#% 875957
#% 879565
#% 894444
#% 960287
#% 1022314
#% 1206925
#! When a database query has a large number of results, the user can only be shown one page of results at a time. One popular approach is to rank results such that the "best" results appear first. This approach is well-suited for information retrieval, and for some database queries, such as similarity queries or under-specified (or keyword) queries with known (or guessable) user preferences. However, standard database query results comprise a set of tuples, with no associated ranking. It is typical to allow users the ability to sort results on selected attributes, but no actual ranking is defined. An alternative approach is not to try to show the estimated best results on the first page, but instead to help users learn what is available in the whole result set and direct them to finding what they need. We present DataLens, a framework that: i) generates the most representative data points to display on the first page without sorting or ranking, ii) allows users to drill-down to more similar items in a hierarchical fashion, and iii) dynamically adjusts the representatives based on the user's new query conditions. To the best of our knowledge, DataLens is the first to allow hierarchical database result browsing and searching at the same time.

#index 1217263
#* MEDIALIFE: from images to a life chronicle
#@ Amarnath Gupta;Setareh Rafatirad;Mingyan Gao;Ramesh Jain
#t 2009
#c 5
#% 824692
#% 845339
#% 893197
#% 1206774

#index 1217264
#* Enabling enterprise mashups over unstructured text feeds with InfoSphere MashupHub and SystemT
#@ David E. Simmen;Frederick Reiss;Yunyao Li;Suresh Thalamati
#t 2009
#c 5
#% 893087
#% 1063559
#% 1183368
#% 1206687
#! Enterprise mashup scenarios often involve feeds derived from data created primarily for eye consumption, such as email, news, calendars, blogs, and web feeds. These data sources can test the capabilities of current data mashup products, as the attributes needed to perform join, aggregation, and other operations are often buried within unstructured feed text. Information extraction technology is a key enabler in such scenarios, using annotators to convert unstructured text into structured information that can facilitate mashup operations. Our demo presents the integration of SystemT, an information extraction system from IBM Research, with IBM's InfoSphere MashupHub. We show how to build domain-specific annotators with SystemT's declarative rule language, AQL, and how to use these annotators to combine structured and unstructured information in an enterprise mashup.

#index 1217265
#* Answering web queries using structured data sources
#@ Stelios Paparizos;Alexandros Ntoulas;John Shafer;Rakesh Agrawal
#t 2009
#c 5
#% 660011
#% 765506
#% 810101
#% 1015325
#% 1022234
#% 1022236
#% 1022289
#! In web search today, a user types a few keywords which are then matched against a large collection of unstructured web pages. This leaves a lot to be desired for when the best answer to a query is contained in structured data stores and/or when the user includes some structural semantics in the query. In our work, we include information from structured data sources into web results. Such sources can vary from fully relational DBs, to flat tables and XML files. In addition, we take advantage of information in such sources to automatically extract corresponding semantics from the query and use them appropriately in improving the overall relevance of results. For this demonstration, we show how we effectively capture, annotate and translate web user queries such as 'popular digital camera around $425' returning results from a shopping-like DB.

#index 1217266
#* HDSampler: revealing data behind web form interfaces
#@ Anirban Maiti;Arjun Dasgupta;Nan Zhang;Gautam Das
#t 2009
#c 5
#% 960286
#% 1206906
#! A large number of online databases are hidden behind the web. Users to these systems can form queries through web forms to retrieve a small sample of the database. Sampling such hidden databases is widely desired for understanding the nature and quality of data stored in them. We have developed HDSampler, which to the best of our knowledge is the first practical system for sampling structured hidden web databases. It enables efficient sampling of the databases and accurate answering of aggregate queries, to provide analysts with valuable information for data analytics, as well as help power a multitude of third-party applications such as web-mashups and meta-search engines. For the purpose of this demo, we present an instance of HDSampler on Google Base - a content-rich hidden web database maintained by Google. By using HDSampler, the demo reveals a snapshot of the marginal distribution of various attributes of Google Base in a matter of minutes.

#index 1217267
#* Hermes: a travel through semantics on the data web
#@ Haofen Wang;Thomas Penin;Kaifeng Xu;Junquan Chen;Xinruo Sun;Linyun Fu;Qiaoling Liu;Yong Yu;Thanh Tran;Peter Haase;Rudi Studer
#t 2009
#c 5
#% 940694
#% 1055897
#% 1063579
#% 1206910
#! The Web as a global information space is developing from a Web of documents to a Web of data. This development opens new ways for addressing complex information needs. Search is no longer limited to matching keywords against documents, but instead complex information needs can be expressed in a structured way, with precise answers as results. In this paper, we demonstrate Hermes, an infrastructure for data web search. To provide an end-user oriented interface, we support expressive keyword search by translating user information needs into structured queries. We integrate heterogeneous web data sources with automatically computed mappings. Schema-level mappings are exploited in constructing structured queries against the integrated schema. These structured queries are decomposed into queries against the local web data sources, which are then processed in a distributed way.

#index 1217268
#* Vispedia: on-demand data integration for interactive visualization and exploration
#@ Bryan Chan;Justin Talbot;Leslie Wu;Nathan Sakunkoo;Mike Cammarano;Pat Hanrahan
#t 2009
#c 5
#% 1013556
#% 1127393
#% 1134501
#% 1147593
#% 1409954
#! Wikipedia is an example of the large, collaborative, semi-structured data sets emerging on the Web. Typically, before these data sets can be used, they must transformed into structured tables via data integration. We present Vispedia, a Web-based visualization system which incorporates data integration into an iterative, interactive data exploration and analysis process. This reduces the upfront cost of using heterogeneous data sets like Wikipedia. Vispedia is driven by a keyword-query-based integration interface implemented using a fast graph search. The search occurs interactively over DBpedia's semantic graph of Wikipedia, without depending on the existence of a structured ontology. This combination of data integration and visualization enables a broad class of non-expert users to more effectively use the semi-structured data available on the Web.

#index 1247782
#* Proceedings of the Second International Workshop on Testing Database Systems
#@ 
#t 2009
#c 5

#index 1426441
#* Proceedings of the twenty-ninth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems
#@ Jan Paredaens;Dirk Van Gucht
#t 2010
#c 5
#! This volume contains the proceedings of the Twenty-Ninth ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems (PODS 2010), held in Indianapolis, Indiana, USA, on June 7-9, 2010, in conjunction with the 2010 ACM SIGMOD International Conference on Management of Data. The proceedings include one paper based on the keynote address by Joe Hellerstein, two papers based on the invited tutorials, the first by Gerhard Weikum and Martin Theobald, and the second by T. S. Jayram, and 27 contributed papers that were selected by the Program Committee from 113 submissions. Most of these papers are preliminary reports on work in progress. While they have been read by program committee members, they have not been formally refereed. Many of them will probably appear in more polished and detailed form in scientific journals. The program committee selected the paper "An Optimal Algorithm for the Distinct Elements Problem" by Daniel M. Kane, Jelani Nelson, and David P. Woodruff for the PODS 2010 Best Paper Award. In addition, the announcement of the 2010 ACM PODS Alberto O. Mendelzon Test-of-Time Award also appears in the proceedings. This year, two papers were given this award: "Typechecking for XML Transformers" by Tova Milo, Dan Suciu, and Victor Vianu, and "Integrity Constraints for XML" by Wenfei Fan and Jérôme Siméon. Warmest congratulations to the authors of these papers. I thank all authors who submitted papers to the symposium, and the members of the program committee for the enormous amount of work they have done. The program committee did not meet in person, but carried out extensive discussions during the electronic PC meeting.

#index 1426442
#* Datalog redux: experience and conjecture
#@ Joseph M. Hellerstein
#t 2010
#c 5
#! There is growing urgency in computer science circles regarding an impending crisis in parallel programming. Emerging computing platforms, from multicore processors to cloud computing, predicate their performance growth on the development of software to harness parallelism. For the first time in the history of computing, the progress of Moore's Law depends on the productivity of software engineers. Unfortunately, parallel and distributed programming today is challenging even for the best programmers, and simply unworkable for the majority. There has never been a more urgent need for breakthroughs in programming models and languages. While parallel programming in general is considered very difficult, data parallelism has been very successful. The relational algebra parallelizes easily over large datasets, and SQL programmers have long reaped the benefits of parallelism without modifications to their code. This point has been rediscovered and amplified via recent enthusiasm for MapReduce programming and "Big Data", which have turned data parallelism into common culture across computing. As a result, it is increasingly attractive to tackle the challenge of parallel programming on the firm common ground of data parallelism: start with an easy-to-parallelize kernel-relational algebra-and extend it to general-purpose computation. This approach has clear precedents in database theory, where it has long been known that classical relational languages have natural Turing-complete extensions. At the same time that this crisis has been evolving, variants of Datalog have been seen cropping up in a wide range of practical settings, from security to robotics to compiler analysis. Over the past seven years, we have been exploring the use of Datalog-inspired languages in a variety of systems projects, with a focus on inherently parallel tasks in networking and distributed systems. The experience has been largely positive: we have demonstrated full-featured Datalog-based system implementations that are orders of magnitude more compact than equivalent imperatively-implemented systems, with competitive performance and significantly accelerated software evolution. Evidence is mounting that Datalog can serve as the basis of a much simpler family of languages for programming serious parallel and distributed software. This raises many questions that should warm the heart of a database theoretician. How does the complexity hierarchy of logic languages relate to parallel models of computation? Is there a suitable Coordination Complexity model that captures the realities of modern parallel hardware, where computation is cheap and coordination is expensive? Can the lens of logic provide better focus on what is "hard" to parallelize, what is "embarrassingly parallel", and points in between? Does our understanding of non-monotonic reasoning shed light on the ability of loosely-coupled distributed systems to guarantee eventual consistency? And finally, a question close to the heart of the PODS conference: if Datalog has been The Answer all these years, is parallel and distributed programming The Question it has been waiting for? In this talk and the paper that accompanies it, I present design patterns that arose in our experience building distributed and parallel software in the style of Datalog, and use them to motivate some initial conjectures relating to the questions above. The full paper was not available at the time these proceedings were printed, but can be found online by searching for the phrase "Springtime for Datalog".

#index 1426443
#* Expressive languages for path queries over graph-structured data
#@ Pablo Barcelo;Carlos Hurtado;Leonid Libkin;Peter Wood
#t 2010
#c 5
#% 17274
#% 99340
#% 101925
#% 197751
#% 235941
#% 248025
#% 268797
#% 289328
#% 291299
#% 333845
#% 415004
#% 449472
#% 562454
#% 577372
#% 587563
#% 657754
#% 1068354
#% 1166490
#% 1226488
#% 1297471
#% 1307976
#% 1396751
#% 1426962
#% 1718501
#! For many problems arising in the setting of graph querying (such as finding semantic associations in RDF graphs, exact and approximate pattern matching, sequence alignment, etc.), the power of standard languages such as the widely studied conjunctive regular path queries (CRPQs) is insufficient in at least two ways. First, they cannot output paths and second, more crucially, they cannot express relations among paths. We thus propose a class of extended CRPQs, called ECRPQs, which add regular relations on tuples of paths, and allow path variables in the heads of queries. We provide several examples of their usefulness in querying graph structured data, and study their properties. We analyze query evaluation and representation of tuples of paths in the output by means of automata. We present a detailed analysis of data and combined complexity of queries, and consider restrictions that lower the complexity of ECRPQs to that of relational conjunctive queries. We study the containment problem, and look at further extensions with first-order features, and with non-regular relations that express arithmetic properties of paths, based on the lengths and numbers of occurrences of labels.

#index 1426444
#* Transducing Markov sequences
#@ Benny Kimelfeld;Christopher Ré
#t 2010
#c 5
#% 39702
#% 42984
#% 115964
#% 190254
#% 269922
#% 275304
#% 281764
#% 292675
#% 306715
#% 443781
#% 464058
#% 464434
#% 465054
#% 598376
#% 643566
#% 654487
#% 656708
#% 746921
#% 809145
#% 810098
#% 816181
#% 874894
#% 977013
#% 977014
#% 993950
#% 1016178
#% 1016201
#% 1058253
#% 1063522
#% 1063523
#% 1063719
#% 1111701
#% 1127414
#% 1180007
#% 1206732
#% 1206735
#% 1206772
#% 1206877
#% 1206879
#% 1207010
#% 1217138
#% 1217181
#% 1217251
#% 1328091
#% 1739001
#% 1854399
#! A Markov sequence is a basic statistical model representing uncertain sequential data, and it is used within a plethora of applications, including speech recognition, image processing, computational biology, radio-frequency identification (RFID), and information extraction. The problem of querying a Markov sequence is studied under the conventional semantics of querying a probabilistic database, where queries are formulated as finite-state transducers. Specifically, the complexity of two main problems is analyzed. The first problem is that of computing the confidence (probability) of an answer. The second is the enumeration of the answers in the order of decreasing confidence (with the generation of the top-k answers as a special case), or in an approximate order thereof. In particular, it is shown that enumeration in any sub-exponential-approximate order is generally intractable (even for some fixed transducers), and a matching upper bound is obtained through a proposed heuristic. Due to this hardness, a special consideration is given to restricted (yet common) classes of transducers that extract matches of a regular expression (subject to prefix and suffix constraints), and it is shown that these classes are, indeed, significantly more tractable.

#index 1426445
#* Positive higher-order queries
#@ Michael Benedikt;Gabriele Puppis;Huy Vu
#t 2010
#c 5
#% 58346
#% 114580
#% 123121
#% 164382
#% 213982
#% 237181
#% 248031
#% 287336
#% 289266
#% 342829
#% 384978
#% 411557
#% 458861
#% 464540
#% 575382
#% 591778
#% 598376
#% 599549
#% 736843
#% 893105
#% 912238
#% 993437
#% 1068354
#% 1180003
#% 1209667
#% 1270570
#! We investigate a higher-order query language that embeds operators of the positive relational algebra within the simply-typed λ-calculus. Our language allows one to succinctly define ordinary positive relational algebra queries (conjunctive queries and unions of conjunctive queries) and, in addition, second-order query functionals, which allow the transformation of CQs and UCQs in a generic (i.e., syntax-independent) way. We investigate the equivalence and containment problems for this calculus, which subsumes traditional CQ/UCQ containment. Query functionals are said to be equivalent if the output queries are equivalent, for each possible input query, and similarly for containment. These notions of containment and equivalence depend on the class of (ordinary relational algebra) queries considered. We show that containment and equivalence are decidable when query variables are restricted to positive relational algebra and we identify the precise complexity of the problem. We also identify classes of functionals where containment is tractable. Finally, we provide upper bounds to the complexity of the containment problem when functionals act over other classes.

#index 1426446
#* The ACM PODS Alberto O. Mendelzon test-of-time-award 2010
#@ Jianwen Su;Phokion G. Kolaitis
#t 2010
#c 5

#index 1426447
#* An optimal algorithm for the distinct elements problem
#@ Daniel M. Kane;Jelani Nelson;David P. Woodruff
#t 2010
#c 5
#% 2833
#% 36119
#% 158790
#% 190611
#% 243166
#% 269686
#% 274152
#% 278835
#% 336610
#% 370597
#% 379443
#% 397369
#% 411554
#% 480805
#% 481948
#% 519953
#% 578389
#% 654495
#% 723898
#% 743963
#% 749451
#% 765291
#% 937911
#% 957604
#% 960250
#% 1050677
#% 1086033
#% 1255263
#% 1484158
#% 1729914
#! We give the first optimal algorithm for estimating the number of distinct elements in a data stream, closing a long line of theoretical research on this problem begun by Flajolet and Martin in their seminal paper in FOCS 1983. This problem has applications to query optimization, Internet routing, network topology, and data mining. For a stream of indices in {1,...,n}, our algorithm computes a (1 ± ε)-approximation using an optimal O(1/ε-2 + log(n)) bits of space with 2/3 success probability, where 0O(1) worst-case time, and can report an estimate at any point midstream in O(1) worst-case time, thus settling both the space and time complexities simultaneously. We also give an algorithm to estimate the Hamming norm of a stream, a generalization of the number of distinct elements, which is useful in data cleaning, packet tracing, and database auditing. Our algorithm uses nearly optimal space, and has optimal O(1) update and reporting times.

#index 1426448
#* Understanding cardinality estimation using entropy maximization
#@ Christopher Ré;Dan Suciu
#t 2010
#c 5
#% 102784
#% 210353
#% 214073
#% 232891
#% 273682
#% 333986
#% 384978
#% 480803
#% 482092
#% 757953
#% 818434
#% 824682
#% 850430
#% 864426
#% 956456
#% 977013
#% 1015256
#% 1070888
#% 1127376
#% 1166535
#% 1206720
#% 1328180
#% 1661439
#% 1700137
#! Cardinality estimation is the problem of estimating the number of tuples returned by a query; it is a fundamentally important task in data management, used in query optimization, progress estimation, and resource provisioning. We study cardinality estimation in a principled framework: given a set of statistical assertions about the number of tuples returned by a fixed set of queries, predict the number of tuples returned by a new query. We model this problem using the probability space, over possible worlds, that satisfies all provided statistical assertions and maximizes entropy. We call this the Entropy Maximization model for statistics (MaxEnt). In this paper we develop the mathematical techniques needed to use the MaxEnt model for predicting the cardinality of conjunctive queries.

#index 1426449
#* From information to knowledge: harvesting entities and relationships from web sources
#@ Gerhard Weikum;Martin Theobald
#t 2010
#c 5
#% 198055
#% 279755
#% 283136
#% 312860
#% 331772
#% 334031
#% 405391
#% 445448
#% 452991
#% 459483
#% 480648
#% 480824
#% 498297
#% 504443
#% 511733
#% 516772
#% 531459
#% 654469
#% 748691
#% 754068
#% 756964
#% 772300
#% 786515
#% 798723
#% 800590
#% 801668
#% 826032
#% 830520
#% 850430
#% 874876
#% 875066
#% 881539
#% 913206
#% 915340
#% 935763
#% 939628
#% 943834
#% 956501
#% 956551
#% 956564
#% 956574
#% 960271
#% 997488
#% 1000502
#% 1019061
#% 1022234
#% 1022235
#% 1022259
#% 1022288
#% 1022289
#% 1026960
#% 1036075
#% 1052713
#% 1055732
#% 1055735
#% 1065125
#% 1074223
#% 1084592
#% 1092530
#% 1117028
#% 1127393
#% 1127402
#% 1127601
#% 1130922
#% 1135150
#% 1166471
#% 1166537
#% 1174746
#% 1181316
#% 1183368
#% 1183369
#% 1183375
#% 1183376
#% 1190065
#% 1190070
#% 1190118
#% 1200291
#% 1201360
#% 1201863
#% 1206687
#% 1206701
#% 1206702
#% 1206862
#% 1217154
#% 1217171
#% 1223597
#% 1250397
#% 1265135
#% 1269899
#% 1270258
#% 1270341
#% 1275182
#% 1292475
#% 1292565
#% 1300591
#% 1305622
#% 1328142
#% 1328199
#% 1328200
#% 1338685
#% 1355026
#% 1372745
#% 1392465
#% 1400146
#% 1409954
#% 1455643
#% 1467732
#% 1486254
#% 1697416
#% 1698594
#% 1727149
#! There are major trends to advance the functionality of search engines to a more expressive semantic level. This is enabled by the advent of knowledge-sharing communities such as Wikipedia and the progress in automatically extracting entities and relationships from semistructured as well as natural-language Web sources. Recent endeavors of this kind include DBpedia, EntityCube, KnowItAll, ReadTheWeb, and our own YAGO-NAGA project (and others). The goal is to automatically construct and maintain a comprehensive knowledge base of facts about named entities, their semantic classes, and their mutual relations as well as temporal contexts, with high precision and high recall. This tutorial discusses state-of-the-art methods, research opportunities, and open challenges along this avenue of knowledge harvesting.

#index 1426450
#* Optimal sampling from distributed streams
#@ Graham Cormode;S. Muthukrishnan;Ke Yi;Qin Zhang
#t 2010
#c 5
#% 1331
#% 333931
#% 345611
#% 379444
#% 654443
#% 800582
#% 810009
#% 824652
#% 864444
#% 874994
#% 874995
#% 894443
#% 1039695
#% 1063498
#% 1063739
#% 1217129
#% 1217131
#! A fundamental problem in data management is to draw a sample of a large data set, for approximate query answering, selectivity estimation, and query planning. With large, streaming data sets, this problem becomes particularly difficult when the data is shared across multiple distributed sites. The challenge is to ensure that a sample is drawn uniformly across the union of the data while minimizing the communication needed to run the protocol and track parameters of the evolving data. At the same time, it is also necessary to make the protocol lightweight, by keeping the space and time costs low for each participant. In this paper, we present communication-efficient protocols for sampling (both with and without replacement) from k distributed streams. These apply to the case when we want a sample from the full streams, and to the sliding window cases of only the W most recent items, or arrivals within the last w time units. We show that our protocols are optimal, not just in terms of the communication used, but also that they use minimal or near minimal (up to logarithmic factors) time to process each new item, and space to operate.

#index 1426451
#* Incremental query evaluation in a ring of databases
#@ Christoph Koch
#t 2010
#c 5
#% 13016
#% 23622
#% 65916
#% 84275
#% 98469
#% 101922
#% 152928
#% 175440
#% 188352
#% 198466
#% 201929
#% 209729
#% 210210
#% 245652
#% 287324
#% 288888
#% 342955
#% 384978
#% 399073
#% 427218
#% 473142
#% 480623
#% 481608
#% 562129
#% 562302
#% 582133
#% 598376
#% 910084
#% 976987
#% 1180014
#% 1328080
#! This paper approaches the incremental view maintenance problem from an algebraic perspective. We construct the algebraic structure of a ring of databases and use it as the foundation of the design of a query calculus that allows to express powerful aggregate queries. The query calculus inherits key properties of the ring, such as having a normal form of polynomials and being closed under computing inverses and delta queries. The k-th delta of a polynomial query of degree k without nesting is purely a function of the update, not of the database. This gives rise to a method of eliminating expensive query operators such as joins from programs that perform incremental view maintenance. The main result is that, for non-nested queries, each individual aggregate value can be incrementally maintained using a constant amount of work. This is not possible for nonincremental evaluation.

#index 1426452
#* Fast Manhattan sketches in data streams
#@ Jelani Nelson;David P. Woodruff
#t 2010
#c 5
#% 34077
#% 273908
#% 273916
#% 278835
#% 333925
#% 419377
#% 479795
#% 480146
#% 481609
#% 492912
#% 578389
#% 749451
#% 749505
#% 806217
#% 809258
#% 809265
#% 813730
#% 816392
#% 824652
#% 824653
#% 874903
#% 879397
#% 881484
#% 963258
#% 1002032
#% 1019786
#% 1039579
#% 1039658
#% 1063718
#% 1086033
#% 1217130
#% 1379544
#% 1484118
#% 1484157
#% 1484158
#% 1740517
#% 1763253
#! The L1-distance, also known as the Manhattan or taxicab distance, between two vectors x, y in Rn is ∑_{i=1}overn |xi-y_i|. Approximating this distance is a fundamental primitive on massive databases, with applications to clustering, nearest neighbor search, network monitoring, regression, sampling, and support vector machines. We give the first 1-pass streaming algorithm for this problem in the turnstile model with O*(1/ε2) space and O*(1) update time. The O* notation hides polylogarithmic factors in ε, n, and the precision required to store vector entries. All previous algorithms either required Ω(1/ε3) space or Ω(1/ε2) update time and/or could not work in the turnstile model (i.e., support an arbitrary number of updates to each coordinate). Our bounds are optimal up to O*(1) factors.

#index 1426453
#* Semantic query optimization in the presence of types
#@ Michael Meier;Michael Schmidt;Fang Wei;Georg Lausen
#t 2010
#c 5
#% 583
#% 69272
#% 123085
#% 152907
#% 189739
#% 205851
#% 237181
#% 273702
#% 289384
#% 300168
#% 378409
#% 398752
#% 411569
#% 416034
#% 442833
#% 463919
#% 476446
#% 572311
#% 717141
#% 826032
#% 857502
#% 912245
#% 943616
#% 992962
#% 1043800
#% 1063724
#% 1063738
#% 1206987
#% 1217115
#% 1217122
#% 1328190
#% 1344656
#% 1424588
#% 1721223
#! Both semantic and type-based query optimization rely on the idea that queries often exhibit non-trivial rewritings if the state space of the database is restricted. Despite their close connection, these two problems to date have always been studied separately. We present a unifying, logic-based framework for query optimization in the presence of data dependencies and type information. It builds upon the classical chase algorithm and extends existing query minimization techniques to considerably larger classes of queries and dependencies. In particular, our setting requires chasing conjunctive queries (possibly with union and negation) in the presence of dependencies containing negation and disjunction. We study the applicability of the chase in this setting, develop novel conditions that guarantee its termination, identify fragments for which minimal query computation is always possible (w.r.t. a generic cost function), and investigate the complexity of related decision problems.

#index 1426454
#* Optimizing linear counting queries under differential privacy
#@ Chao Li;Michael Hay;Vibhor Rastogi;Gerome Miklau;Andrew McGregor
#t 2010
#c 5
#% 757953
#% 963241
#% 977011
#% 1061644
#% 1198224
#% 1414540
#% 1426322
#% 1740518
#! Differential privacy is a robust privacy standard that has been successfully applied to a range of data analysis tasks. But despite much recent work, optimal strategies for answering a collection of related queries are not known. We propose the matrix mechanism, a new algorithm for answering a workload of predicate counting queries. Given a workload, the mechanism requests answers to a different set of queries, called a query strategy, which are answered using the standard Laplace mechanism. Noisy answers to the workload queries are then derived from the noisy answers to the strategy queries. This two stage process can result in a more complex correlated noise distribution that preserves differential privacy but increases accuracy. We provide a formal analysis of the error of query answers produced by the mechanism and investigate the problem of computing the optimal query strategy in support of a given workload. We show this problem can be formulated as a rank-constrained semidefinite program. Finally, we analyze two seemingly distinct techniques, whose similar behavior is explained by viewing them as instances of the matrix mechanism.

#index 1426455
#* Universally optimal privacy mechanisms for minimax agents
#@ Mangesh Gupte;Mukund Sundararajan
#t 2010
#c 5
#% 576110
#% 576761
#% 963241
#% 963242
#% 1029084
#% 1061644
#% 1080356
#% 1141473
#% 1198224
#% 1414540
#% 1670071
#% 1740518
#! A scheme that publishes aggregate information about sensitive data must resolve the trade-off between utility to information consumers and privacy of the database participants. Differential privacy [5] is a well-established definition of privacy--this is a universal guarantee against all attackers, whatever their side-information or intent. Can we have a similar universal guarantee for utility? There are two standard models of utility considered in decision theory: Bayesian and minimax [13]. Ghosh et. al. [8] show that a certain "geometric mechanism" gives optimal utility to all Bayesian information consumers. In this paper, we prove a similar result for minimax information consumers. Our result also works for a wider class of information consumers which includes Bayesian information consumers and subsumes the result from [8]. We model information consumers as minimax (risk-averse) agents, each endowed with a loss-function which models their tolerance to inaccuracies and each possessing some side-information about the query. Further, information consumers are rational in the sense that they actively combine information from the mechanism with their side-information in a way that minimizes their loss. Under this assumption of rational behavior, we show that for every fixed count query, the geometric mechanism is universally optimal for all minimax information consumers. Additionally, our solution makes it possible to release query results, when information consumers are at different levels of privacy, in a collusion-resistant manner.

#index 1426456
#* Towards an axiomatization of statistical privacy and utility
#@ Daniel Kifer;Bing-Rong Lin
#t 2010
#c 5
#% 22242
#% 67453
#% 576110
#% 576111
#% 576761
#% 809245
#% 864412
#% 956557
#% 963241
#% 963242
#% 977011
#% 996348
#% 1022246
#% 1022247
#% 1029084
#% 1061644
#% 1063726
#% 1198224
#% 1198227
#% 1206678
#% 1217125
#% 1217156
#% 1287870
#% 1328175
#% 1381029
#% 1670071
#% 1732708
#% 1740518
#! "Privacy" and "utility" are words that frequently appear in the literature on statistical privacy. But what do these words really mean? In recent years, many problems with intuitive notions of privacy and utility have been uncovered. Thus more formal notions of privacy and utility, which are amenable to mathematical analysis, are needed. In this paper we present our initial work on an axiomatization of privacy and utility. In particular, we study how these concepts are affected by randomized algorithms. Our analysis yields new insights into the construction of both privacy definitions and mechanisms that generate data according to such definitions. In particular, it characterizes a class of relaxations of differential privacy and shows that desirable outputs of a differentially private mechanism are best interpreted as certain graphs rather than query answers or synthetic data.

#index 1426457
#* Information complexity: a tutorial
#@ T. S. Jayram
#t 2010
#c 5
#% 2833
#% 115608
#% 211670
#% 238182
#% 278835
#% 290647
#% 347223
#% 410958
#% 519953
#% 600560
#% 656769
#% 749451
#% 751684
#% 771386
#% 805744
#% 809258
#% 879397
#% 894646
#% 1029140
#% 1164911
#% 1379544
#% 1484082
#! The recent years have witnessed the overwhelming success of algorithms that operate on massive data. Several computing paradigms have been proposed for massive data set algorithms such as data streams, sketching, sampling etc. and understanding their limitations is a fundamental theoretical challenge. In this survey, we describe the information complexity paradigm that has proved successful in obtaining tight lower bounds for several well-known problems. Information complexity quantifies the amount of information about the inputs that must be necessarily propagated by any algorithm in solving a problem. We describe the key ideas of this paradigm, and highlight the beautiful interplay of techniques arising from diverse areas such as information theory, statistics and geometry.

#index 1426458
#* Capturing missing tuples and missing values
#@ Wenfei Fan;Floris Geerts
#t 2010
#c 5
#% 663
#% 11817
#% 67457
#% 94459
#% 118359
#% 137871
#% 248038
#% 264858
#% 268764
#% 273687
#% 366807
#% 378409
#% 384978
#% 398752
#% 480249
#% 481128
#% 481786
#% 809238
#% 809239
#% 1063725
#% 1083337
#% 1130461
#% 1217124
#% 1424602
#% 1433975
#% 1661426
#! Databases in real life are often neither entirely closed-world nor entirely open-world. Indeed, databases in an enterprise are typically partially closed, in which a part of the data is constrained by master data that contains complete information about the enterprise in certain aspects [21]. It has been shown that despite missing tuples, such a database may turn out to have complete information for answering a query [9]. This paper studies partially closed databases from which both tuples and values may be missing. We specify such a database in terms of conditional tables constrained by master data, referred to as c-instances. We first propose three models to characterize whether a c-instance T is complete for a query Q relative to master data. That is, depending on how missing values in T are instantiated, the answer to Q in T remains unchanged when new tuples are added. We then investigate four problems, to determine (a) whether a given c-instance is complete for a query Q, (b) whether there exists a c-instance that is complete for Q relative to master data available, (c) whether a c-instance is a minimal-size database that is complete for Q, and (d) whether there exists a c-instance of a bounded size that is complete for Q. We establish matching lower and upper bounds on these problems for queries expressed in a variety of languages, in each of the three models for specifying relative completeness.

#index 1426459
#* On the first-order expressibility of computing certain answers to conjunctive queries over uncertain databases
#@ Jef Wijsen
#t 2010
#c 5
#% 36683
#% 273687
#% 289424
#% 384978
#% 778122
#% 810020
#% 826032
#% 836134
#% 838543
#% 864417
#% 949372
#% 992830
#% 1179999
#% 1200291
#% 1217251
#% 1224935
#% 1347336
#% 1408532
#% 1728684
#! A natural way for capturing uncertainty in the relational data model is by having relations that violate their primary key constraint, that is, relations in which distinct tuples agree on the primary key. A repair (or possible world) of a database is then obtained by selecting a maximal number of tuples without ever selecting two distinct tuples that have the same primary key value. For a Boolean query q, CERTAINTY(q) is the problem that takes as input a database db and asks whether q evaluates to true on every repair of db. We are interested in determining queries q for which CERTAINTY(q) is first-order expressible (and hence in the low complexity class AC0). For queries q in the class of conjunctive queries without self-join, we provide a necessary syntactic condition for first-order expressibility of CERTAINTY(q). For acyclic queries, this necessary condition is also a sufficient condition. So we obtain a decision procedure for first-order expressibility of CERTAINTY(q) when q is acyclic and without self-join. We also show that if CERTAINTY(q) is first-order expressible, its first-order definition, commonly called (certain) first-order rewriting, can be constructed in a rather straightforward way.

#index 1426460
#* Certain answers for XML queries
#@ Claire David;Leonid Libkin;Filip Murlak
#t 2010
#c 5
#% 663
#% 2984
#% 94459
#% 129217
#% 163444
#% 366807
#% 384978
#% 576116
#% 599549
#% 736843
#% 765432
#% 801677
#% 806215
#% 809239
#% 826031
#% 826032
#% 850730
#% 865766
#% 866317
#% 866986
#% 960233
#% 993981
#% 1022204
#% 1039061
#% 1183377
#% 1215806
#% 1217117
#% 1217139
#% 1408529
#% 1424599
#! The notion of certain answers arises when one queries incompletely specified databases, e.g., in data integration and exchange scenarios, or databases with missing information. While in the relational case this notion is well understood, there is no natural analog of it for XML queries that return documents. We develop an approach to defining certain answers for such XML queries, and apply it in the settings of incomplete information and XML data exchange. We first revisit the relational case, and show how to present the key concepts related to certain answers in a new model-theoretic language. This new approach naturally extends to XML. We prove a number of generic, application-independent results about computability and complexity of certain answers produced by it. We then turn our attention to a pattern-based XML query language with trees as outputs, and present a technique for computing certain answers that relies on the notion of a basis of a set of trees. We show how to compute such bases for documents with nulls and for documents arising in data exchange scenarios, and provide complexity bounds. While in general complexity of query answering in XML data exchange could be high, we exhibit a natural class of XML schema mappings for which not only query answering, but also many static analysis problems can be solved efficiently.

#index 1426461
#* Computing query probability with incidence algebras
#@ Nilesh Dalvi;Karl Schnaitter;Dan Suciu
#t 2010
#c 5
#% 17485
#% 196418
#% 204286
#% 265692
#% 289266
#% 571102
#% 976984
#% 977013
#% 1016201
#% 1206987
#% 1217176
#% 1270261
#% 1279353
#% 1343690
#% 1417109
#! We describe an algorithm that evaluates queries over probabilistic databases using Mobius' inversion formula in incidence algebras. The queries we consider are unions of conjunctive queries (equivalently: existential, positive First Order sentences), and the probabilistic databases are tuple-independent structures. Our algorithm runs in PTIME on a subset of queries called "safe" queries, and is complete, in the sense that every unsafe query is hard for the class FP#P. The algorithm is very simple and easy to implement in practice, yet it is non-obvious. Mobius' inversion formula, which is in essence inclusion-exclusion, plays a key role for completeness, by allowing the algorithm to compute the probability of some safe queries even when they have some subqueries that are unsafe. We also apply the same lattice-theoretic techniques to analyze an algorithm based on lifted conditioning, and prove that it is incomplete.

#index 1426462
#* On probabilistic fixpoint and Markov chain query languages
#@ Daniel Deutch;Christoph Koch;Tova Milo
#t 2010
#c 5
#% 194290
#% 217824
#% 341672
#% 384978
#% 598376
#% 723928
#% 893189
#% 960293
#% 992830
#% 1022341
#% 1063719
#% 1138894
#% 1180007
#% 1180009
#% 1183378
#% 1275150
#! We study highly expressive query languages such as datalog, fixpoint, and while-languages on probabilistic databases. We generalize these languages such that computation steps (e.g. datalog rules) can fire probabilistically. We define two possible semantics for such query languages, namely inflationary semantics where the results of each computation step are added to the current database and noninflationary queries that induce a random walk in-between database instances. We then study the complexity of exact and approximate query evaluation under these semantics.

#index 1426463
#* Foundations of schema mapping management
#@ Marcelo Arenas;Jorge Pérez;Juan L. Reutter;Cristian Riveros
#t 2010
#c 5
#% 198465
#% 237190
#% 289384
#% 328429
#% 458607
#% 572307
#% 572311
#% 654457
#% 765540
#% 806215
#% 809239
#% 809249
#% 810021
#% 826032
#% 850730
#% 960233
#% 976996
#% 997492
#% 1015326
#% 1044441
#% 1180001
#% 1217116
#% 1270567
#% 1328194
#% 1433975
#! In the last few years, a lot of attention has been paid to the specification and subsequent manipulation of schema mappings, a problem which is of fundamental importance in metadata management. There have been many achievements in this area, and semantics have been defined for operators on schema mappings such as composition and inverse. However, little research has been pursued towards providing formal tools to compare schema mappings, in terms of their ability to transfer data and avoid storing redundant information, which has hampered the development of foundations for more complex operators as many of them involve these notions. In this paper, we address the problem of providing foundations for metadata management by developing an order to compare the amount of information transferred by schema mappings. From this order we derive several other criteria to compare mappings, we provide tools to deal with these criteria, and we show their usefulness in defining and studying schema mapping operators. More precisely, we show how the machinery developed can be used to study the extract and merge operators, that have been identified as fundamental for the development of a metadata management framework. We also use our machinery to provide simpler proofs for some fundamental results regarding the inverse operator, and we give an effective characterization for the decidability of the well-known schema evolution problem.

#index 1426464
#* Schema design for XML repositories: complexity and tractability
#@ Wim Martens;Matthias Niewerth;Thomas Schwentick
#t 2010
#c 5
#% 148108
#% 215675
#% 262724
#% 299944
#% 382963
#% 1217140
#% 1217202
#% 1397920
#% 1504036
#% 1733524
#! Abiteboul et al. initiated the systematic study of distributed XML documents consisting of several logical parts, possibly located on different machines. The physical distribution of such documents immediately raises the following question: how can a global schema for the distributed document be broken up into local schemas for the different logical parts? The desired set of local schemas should guarantee that, if each logical part satisfies its local schema, then the distributed document satisfies the global schema. Abiteboul et al. proposed three levels of desirability for local schemas: local typing, maximal local typing, and perfect local typing. Immediate algorithmic questions are: (i) given a typing, determine whether it is local, maximal local, or perfect, and (ii) given a document and a schema, establish whether a (maximal) local or perfect typing exists. This paper improves the open complexity results in their work and initiates the study of (i) and (ii) for schema restrictions arising from the current standards: DTDs and XML Schemas with deterministic content models. The most striking result is that these restrictions yield tractable complexities for the perfect typing problem. Furthermore, an open problem in Formal Language Theory is settled: deciding language primality for deterministic finite automata is pspace-complete.

#index 1426465
#* Simplifying XML schema: single-type approximations of regular tree languages
#@ Wouter Gelade;Tomasz Idziaszek;Wim Martens;Frank Neven
#t 2010
#c 5
#% 70235
#% 299944
#% 343978
#% 404772
#% 504573
#% 845589
#% 848763
#% 894435
#% 949370
#% 1021195
#% 1022285
#% 1217202
#% 1504036
#% 1914828
#! XML Schema Definitions (XSDs) can be adequately abstracted by the single-type regular tree languages. It is well-known, that these form a strict subclass of the robust class of regular unranked tree languages. Sadly, in this respect, XSDs are not closed under the basic operations of union and set difference, complicating important tasks in schema integration and evolution. The purpose of this paper is to investigate how the union and difference of two XSDs can be approximated within the framework of single-type regular tree languages. We consider both optimal lower and upper approximations. We also address the more general question of how to approximate an arbitrary regular tree language by an XSD and consider the complexity of associated decision problems.

#index 1426466
#* Characterizing schema mappings via data examples
#@ Bogdan Alexe;Phokion G. Kolaitis;Wang-Chiew Tan
#t 2010
#c 5
#% 70673
#% 180945
#% 236025
#% 289384
#% 333988
#% 334025
#% 378409
#% 384978
#% 809239
#% 810078
#% 826032
#% 893095
#% 1063711
#% 1063712
#% 1127370
#% 1180001
#% 1206612
#% 1217165
#! Schema mappings are high-level specifications that describe the relationship between two database schemas; they are considered to be the essential building blocks in data exchange and data integration, and have been the object of extensive research investigations. Since in real-life applications schema mappings can be quite complex, it is important to develop methods and tools for understanding, explaining, and refining schema mappings. A promising approach to this effect is to use "good" data examples that illustrate the schema mapping at hand. We develop a foundation for the systematic investigation of data examples and obtain a number of results on both the capabilities and the limitations of data examples in explaining and understanding schema mappings. We focus on schema mappings specified by source-to-target tuple generating dependencies (s-t tgds) and investigate the following problem: which classes of s-t tgds can be "uniquely characterized" by a finite set of data examples? Our investigation begins by considering finite sets of positive and negative examples, which are arguably the most natural choice of data examples. However, we show that they are not powerful enough to yield interesting unique characterizations. We then consider finite sets of universal examples, where a universal example is a pair consisting of a source instance and a universal solution for that source instance. We unveil a tight connection between unique characterizations via universal examples and the existence of Armstrong bases (a relaxation of the classical notion of Armstrong databases). On the positive side, we show that every schema mapping specified by LAV s-t tgds is uniquely characterized by a finite set of universal examples with respect to the class of LAV s-t tgds. Moreover, this positive result extends to the much broader classes of n-modular schema mappings, n a positive integer. Finally, we show that, on the negative side, there are schema mappings specified by GAV s-t tgds that are not uniquely characterized by any finite set of universal examples and negative examples with respect to the class of GAV s-t tgds (hence also with respect to the class of all s-t tgds).

#index 1426467
#* Understanding queries in a search database system
#@ Ronald Fagin;Benny Kimelfeld;Yunyao Li;Sriram Raghavan;Shivakumar Vaithyanathan
#t 2010
#c 5
#% 39702
#% 118031
#% 118032
#% 118040
#% 144029
#% 289144
#% 570877
#% 590523
#% 824693
#% 874894
#% 875061
#% 879634
#% 956543
#% 993987
#% 994033
#% 1058253
#% 1063539
#% 1183368
#% 1206687
#% 1206760
#% 1217198
#% 1217235
#% 1223495
#! It is well known that a search engine can significantly benefit from an auxiliary database, which can suggest interpretations of the search query by means of the involved concepts and their interrelationship. The difficulty is to translate abstract notions like concept and interpretation into a concrete search algorithm that operates over the auxiliary database. To surpass existing heuristics, there is a need for a formal basis, which is realized in this paper through the framework of a search database system, where an interpretation is identified as a parse. It is shown that the parses of a query can be generated in polynomial time in the combined size of the input and the output, even if parses are restricted to those having a nonempty evaluation. Identifying that one parse is more specific than another is important for ranking answers, and this framework captures the precise semantics of being more specific; moreover, performing this comparison between parses is tractable. Lastly, the paper studies the problem of finding the most specific parses. Unfortunately, this problem turns out to be intractable in the general case. However, under reasonable assumptions, the parses can be enumerated in an order of decreasing specificity, with polynomial delay and polynomial space.

#index 1426468
#* A learning algorithm for top-down XML transformations
#@ Aurelien Lemay;Sebastian Maneth;Joachim Niehren
#t 2010
#c 5
#% 31215
#% 287482
#% 296885
#% 333990
#% 427874
#% 443654
#% 452226
#% 743615
#% 745462
#% 809259
#% 907142
#% 911086
#% 993981
#% 1055754
#% 1124449
#% 1193641
#% 1693705
#! A generalization from string to trees and from languages to translations is given of the classical result that any regular language can be learned from examples: it is shown that for any deterministic top-down tree transformation there exists a sample set of polynomial size (with respect to the minimal transducer) which allows to infer the translation. Until now, only for string transducers and for simple relabeling tree transducers, similar results had been known. Learning of deterministic top-down tree transducers (dtops) is far more involved because a dtop can copy, delete, and permute its input subtrees. Thus, complex dependencies of labeled input to output paths need to be maintained by the algorithm. First, a Myhill-Nerode theorem is presented for dtops, which is interesting on its own. This theorem is then used to construct a learning algorithm for dtops. Finally, it is shown how our result can be applied to xml transformations (e.g. xslt programs). For this, a new dtd-based encoding of unranked trees by ranked ones is presented. Over such encodings, dtops can realize many practically interesting xml transformations which cannot be realized on firstchild/next-sibling encodings.

#index 1426469
#* Cache-oblivious hashing
#@ Rasmus Pagh;Zhewei Wei;Ke Yi;Qin Zhang
#t 2010
#c 5
#% 36182
#% 36306
#% 38226
#% 41684
#% 190611
#% 287317
#% 580699
#% 593909
#% 723897
#% 764572
#% 833367
#% 963268
#% 993508
#% 1039659
#% 1054482
#% 1068534
#% 1131789
#% 1202567
#% 1238464
#% 1426295
#! The hash table, especially its external memory version, is one of the most important index structures in large databases. Assuming a truly random hash function, it is known that in a standard external hash table with block size b, searching for a particular key only takes expected average t_q=1+1/2Ω(b) disk accesses for any load factor α bounded away from $1$. However, such near-perfect performance is achieved only when b is known and the hash table is particularly tuned for working with such a blocking. In this paper we study if it is possible to build a cache-oblivious hash table that works well with any blocking. Such a hash table will automatically perform well across all levels of the memory hierarchy and does not need any hardware-specific tuning, an important feature in autonomous databases. We first show that linear probing, a classical collision resolution strategy for hash tables, can be easily made cache-oblivious but it only achieves t_q = 1 + O(αb). Then we demonstrate that it is possible to obtain t_q = 1 + 1/2Ω(b), thus matching the cache-aware bound, if the following two conditions hold: (a) b is a power of 2; and (b) every block starts at a memory address divisible by b. Both conditions hold on a real machine, although they are not stated in the cache-oblivious model. Interestingly, we also show that neither condition is dispensable: if either of them is removed, the best obtainable bound is t_q=1+O(αb), which is exactly what linear probing achieves.

#index 1426470
#* Performance guarantees for B-trees with different-sized atomic keys
#@ Michael A. Bender;Haodong Hu;Bradley C. Kuszmaul
#t 2010
#c 5
#% 186
#% 1876
#% 41684
#% 47710
#% 76223
#% 137808
#% 271801
#% 281655
#% 286835
#% 287715
#% 296253
#% 317933
#% 318921
#% 320435
#% 451179
#% 498538
#% 604653
#% 847099
#% 874900
#% 1304989
#! Most B-tree papers assume that all N keys have the same size K, that F = B/K keys fit in a disk block, and therefore that the search cost is O(logf+1 N) block transfers. When keys have variable size, however, B-tree operations have no nontrivial performance guarantees. This paper provides B-tree-like performance guarantees on dictionaries that contain keys of different sizes in a model in which keys must be stored and compared as opaque objects. The resulting atomic-key dictionaries exhibit performance bounds in terms of the average key size and match the bounds when all keys are the same size. Atomic key dictionaries can be built with minimal modification to the B-tree structure, simply by choosing the pivot keys properly. This paper describes both static and dynamic atomic-key dictionaries. In the static case, if there are N keys with average size K, the search cost is O(⌈K/B⌉ log1+⌈K/B⌉ N) expected transfers. The paper proves that it is not possible to transform these expected bounds into worst-case bounds. The cost to build the tree is O(NK) operations and O(NK/B) transfers if all keys are presented in sorted order. If not, the cost is the sorting cost. For the dynamic dictionaries, the amortized cost to insert a key κ of arbitrary length at an arbitrary rank is dominated by the cost to search for κ. Specifically the amortized cost to insert a key κ of arbitrary length and random rank is O(⌈K/B⌉ log1+⌈K/B⌉ N + |κ| /B) transfers. A dynamic-programming algorithm is shown for constructing a search tree with minimal expected cost.

#index 1426471
#* When data dependencies over SQL tables meet the logics of paradox and S-3
#@ Sven Hartmann;Sebastian Link
#t 2010
#c 5
#% 663
#% 2188
#% 7348
#% 55899
#% 114579
#% 119965
#% 184793
#% 267604
#% 279167
#% 279170
#% 287295
#% 287333
#% 287478
#% 287754
#% 287792
#% 289237
#% 289305
#% 289336
#% 289350
#% 322880
#% 346917
#% 598376
#% 722844
#% 742566
#% 771227
#% 826032
#% 949367
#% 1046416
#% 1181329
#% 1200330
#% 1273436
#% 1307740
#% 1337609
#% 1381083
#% 1410670
#! We study functional and multivalued dependencies over SQL tables with NOT NULL constraints. Under a no-information interpretation of null values we develop tools for reasoning. We further show that in the absence of NOT NULL constraints the associated implication problem is equivalent to that in propositional fragments of Priest's paraconsistent Logic of Paradox. Subsequently, we extend the equivalence to Boolean dependencies and to the presence of NOT NULL constraints using Schaerf and Cadoli's S-3 logics where S corresponds to the set of attributes declared NOT NULL. The findings also apply to Codd's interpretation "value at present unknown" utilizing a weak possible world semantics. Our results establish NOT NULL constraints as an effective mechanism to balance the expressiveness and tractability of consequence relations, and to control the degree by which the existing classical theory of data dependencies can be soundly approximated in practice.

#index 1426472
#* The power of tree projections: local consistency, greedy algorithms, and larger islands of tractability
#@ Gianluigi Greco;Francesco Scarcello
#t 2010
#c 5
#% 451
#% 997
#% 139180
#% 289287
#% 289424
#% 289425
#% 339937
#% 384978
#% 535150
#% 643572
#% 847068
#% 874914
#% 927017
#% 993437
#% 1000772
#% 1039063
#% 1042383
#% 1061964
#% 1074858
#% 1105393
#% 1164915
#% 1224352
#% 1916545
#! Enforcing local consistency is a well-known technique to simplify the evaluation of conjunctive queries. It consists of repeatedly taking the semijion between every pair of (relations associated with) query atoms, until the procedure stabilizes. If some relation becomes empty, then the query has an empty answer. Otherwise, we cannot say anything in general, unless we have some information on the structure of the given query. In fact, a fundamental result in database theory states that the class of queries for which---on every database---local consistency entails global consistency is precisely the class of acyclic queries. In the last few years, several efforts have been made to define structural decomposition methods isolating larger classes of nearly-acyclic queries, yet retaining the same nice properties as acyclic ones. In particular, it is known that queries having bounded (generalized) hypertree-width can be evaluated in polynomial time, and that this structural property is also sufficient to guarantee that local consistency solves the problem, as for acyclic queries. However, the precise power of such an approach was an open problem: Is it the case that bounded generalized hypertree-width is also a necessary condition to guarantee that local consistency entails global consistency? In this paper, we positively answer this question, and go beyond. Firstly, we precisely characterize the power of local consistency procedures in the more general framework of tree projections, where a query Q and a set V of views (i.e., resources that can be used to answer Q) are given, and where one looks for an acyclic hypergraph covering Q and covered by Q---all known structural decomposition methods are just special cases of this framework, defining their specific set of resources. We show that the existence of tree projections of certain subqueries is a necessary and sufficient condition to guarantee that local consistency entails global consistency. In particular, tight characterizations are given not only for the decision problem, but also when answers restricted to variables covered by some view have to be computed. Secondly, we consider greedy tree-projections that are easy to compute, and we study how far they can be from arbitrary tree-projections, which are intractable in general. Finally, we investigate classes of instances not included in those having tree projections, and which can be easily recognized and define either new islands of tractability, or islands of quasi-tractability.

#index 1426473
#* Proceedings of the 1st ACM symposium on Cloud computing
#@ Joseph M. Hellerstein;Surajit Chaudhuri;Mendel Rosenblum
#t 2010
#c 5
#! Welcome to the inaugural ACM Symposium of Cloud Computing. This symposium is co-sponsored by the ACM Special Interest Group on Management of Data (SIGMOD) and the ACM Special Interest Group on Operating Systems (SIGOPS). Both these communities share a common interest in the rapidly developing field of Cloud Computing, i.e., large scale distributed systems that can manage massive volumes of data and yet deliver reliable and efficient service. Although traditionally these two communities have had close interactions on various topics, to the best of our knowledge this is the first time that they are co-sponsoring a symposium with active participation and shared responsibilities from both the communities. This year, SOCC is being held in conjunction with ACM SIGMOD, the flagship conference of the database community. Next year, SOCC will be held in conjunction with ACM SOSP, the premier conference for operating systems. The goal for such co-location is to facilitate more effective networking across the two communities. The conferences of both the systems and database communities have always encouraged participation of both academic researchers and industrial participants, and SOCC is no exception. In our Call for Papers, we sought not only full-length research papers, but also position papers and industrial papers. We had a strong response to our call and were pleasantly surprised to get 119 submissions. Each paper was reviewed by at least two members of the Program Committee. Given the limited duration of the symposium we were able to accept only 23 papers (18 research papers, 4 position papers, and 1 industrial paper) for presentation at this symposium. Thus, the acceptance ratio was slightly below 20%. Our technical program also consists of three distinguished speakers from the industry to provide us with insights from their experience in the field, to ground and inspire our thinking on future directions.

#index 1426500
#* Proceedings of the 2010 ACM SIGMOD International Conference on Management of data
#@ Ahmed Elmagarmid;Divyakant Agrawal
#t 2010
#c 5
#! Benvenuti! Welcome to an exciting week in the city of Indianapolis for the 2010 ACM SIGMOD Conference. Please familiarize yourself with the diverse fun activities the city has to offer, whether you are interested in professional sports, museums, restaurants and nightlife, the blues, shopping or strolls along the canals. The city has something for everyone and especially accompanying family members. The city is children friendly with a world class children museum and the Indianapolis Zoo which is within a walking distance. For those registered for the conference, we have a program of nightly social events that is going to get your party spirit up. The highlight of the social events is a lavish banquet at the Eiteljorg Museum of American Art that will feature a truly international dinner and a native Indian music. As usual, we will have a PODS reception on Sunday to be held at the Hyatt hotel (conference hotel), a SIGMOD reception on Monday to be held at the Indiana Roof Ballroom just across the street from the conference hotel. The venue for the Monday reception is unique and not to be missed, a Microsoft hosted reception on Tuesday night at the conference hotel. All this in addition to the usual meals provided on certain days. Because of the generous support from our sponsors (Platinum) IBM, Microsoft, Oracle, and SAP, (Gold) Google, Netezza, Sybase, and Yahoo! Labs, (Silver) Greenplum, MARK, and NEC, we were able to keep the conference fees to a minimum. We want to thank the SIGMOD EC for all the help and support they gave us through Lisa Singh, who was a tireless and relentless advocate and a sounding board. We also thank all the members of the organizing committee who gave selflessly of their time and energy. A list of all the organizing committee is included in these proceedings. The program features some novel ideas this year. One is the collocation of a Symposium on Cloud computing that is co-sponsored by SIGMOD and SIGOPS, and as usual we will have eight very exciting workshops, five industry sessions, four tutorials, four demo sessions, one panel, five industry sessions, and two plenary poster sessions. The main program is larger and broader than ever before. The call for research papers resulted in 384 submissions that conformed to SIGMOD's double-blind reviewing policy. All submissions were reviewed by at least three members of the SIGMOD 2010 Program Committee and after extended discussion the Program Committee accepted 80 papers to be included in the research program of SIGMOD 2010. The call for demonstration proposals resulted in 95 submissions out of which the SIGMOD 2010 Demonstration Track Program Committee selected 35 demonstrations to be presented at SIGMOD 2010. A combination of personal solicitation and the call for submissions resulted in 34 submissions for the Industrial Track. The Industrial Track Program Committee selected 18 papers to be presented in SIGMOD 2010 Industrial Sessions. In addition, the final program includes four tutorials and one panel discussion. Finally, the conference highlights include keynote addresses by two distinguished speakers: Jon Kleinberg from Cornell University and Luiz André Barroso from Google, Inc.

#index 1426501
#* The flow of on-line information in global networks
#@ Jon Kleinberg
#t 2010
#c 5
#% 350859
#% 577220
#% 641137
#% 754107
#% 786841
#% 875959
#% 881460
#% 881498
#% 949164
#% 989650
#% 1083624
#% 1083641
#% 1214671
#% 1425621

#index 1426502
#* Efficiently evaluating complex boolean expressions
#@ Marcus Fontoura;Suhas Sadanandan;Jayavel Shanmugasundaram;Sergei Vassilvitski;Erik Vee;Srihari Venkatesan;Jason Zien
#t 2010
#c 5
#% 158911
#% 297191
#% 333938
#% 397366
#% 480944
#% 511917
#% 567463
#% 631962
#% 640616
#% 646220
#% 660009
#% 726620
#% 730065
#% 742564
#% 760291
#% 978067
#% 1328112
#! The problem of efficiently evaluating a large collection of complex Boolean expressions - beyond simple conjunctions and Disjunctive/Conjunctive Normal Forms (DNF/CNF) - occurs in many emerging online advertising applications such as advertising exchanges and automatic targeting. The simple solution of normalizing complex Boolean expressions to DNF or CNF form, and then using existing methods for evaluating such expressions is not always effective because of the exponential blow-up in the size of expressions due to normalization. We thus propose a novel method for evaluating complex expressions, which leverages existing techniques for evaluating leaf-level conjunctions, and then uses a bottom-up evaluation technique to only process the relevant parts of the complex expressions that contain the matching conjunctions. We develop two such bottom-up evaluation techniques, one based on Dewey IDs and another based on mapping Boolean expressions to one-dimensional intervals. Our experimental evaluation based on data obtained from an online advertising exchange shows that the proposed techniques are efficient and scalable, both with respect to space usage as well as evaluation time.

#index 1426503
#* How to ConQueR why-not questions
#@ Quoc Trung Tran;Chee-Yong Chan
#t 2010
#c 5
#% 442902
#% 465167
#% 479957
#% 480483
#% 765433
#% 893105
#% 960234
#% 1127409
#% 1181286
#% 1217186
#% 1217187
#% 1269490
#% 1328076
#! One useful feature that is missing from today's database systems is an explain capability that enables users to seek clarifications on unexpected query results. There are two types of unexpected query results that are of interest: the presence of unexpected tuples, and the absence of expected tuples (i.e., missing tuples). Clearly, it would be very helpful to users if they could pose follow-up why and why-not questions to seek clarifications on, respectively, unexpected and expected (but missing) tuples in query results. While the why questions can be addressed by applying established data provenance techniques, the problem of explaining the why-not questions has received very little attention. There are currently two explanation models proposed for why-not questions. The first model explains a missing tuple t in terms of modifications to the database such that t appears in the query result wrt the modified database. The second model explains by identifying the data manipulation operator in the query evaluation plan that is responsible for excluding t from the result. In this paper, we propose a new paradigm for explaining a why-not question that is based on automatically generating a refined query whose result includes both the original query's result as well as the user-specified missing tuple(s). In contrast to the existing explanation models, our approach goes beyond merely identifying the "culprit" query operator responsible for the missing tuple(s) and is useful for applications where it is not appropriate to modify the database to obtain missing tuples.

#index 1426504
#* Call to order: a hierarchical browsing approach to eliciting users' preference
#@ Feng Zhao;Gautam Das;Kian-Lee Tan;Anthony K.H. Tung
#t 2010
#c 5
#% 70050
#% 330769
#% 333854
#% 427199
#% 443480
#% 465167
#% 481281
#% 654480
#% 731407
#% 824670
#% 838430
#% 875012
#% 875025
#% 960234
#% 993954
#% 993957
#% 1022225
#% 1022270
#% 1063487
#% 1083667
#% 1133032
#% 1201865
#% 1206819
#% 1217184
#% 1272396
#% 1328160
#% 1400776
#% 1688253
#% 1688273
#! Computing preference queries has received a lot of attention in the database community. It is common that the user is unsure of his/her preference, so care must be taken to elicit the preference of the user correctly. In this paper, we propose to elicit the preferred ordering of a user by utilizing skyline objects as the representatives of the possible ordering. We introduce the notion of order-based representative skylines which selects representatives based on the orderings that they represent. To further facilitate preference exploration, a hierarchical clustering algorithm is applied to compute a denogram on the skyline objects. By coupling the hierarchical clustering with visualization techniques, we allow users to refine their preference weight settings by browsing the hierarchy. Extensive experiments were conducted and the results validate the feasibility and the efficiency of our approach.

#index 1426505
#* Boosting spatial pruning: on optimal pruning of MBRs
#@ Tobias Emrich;Hans-Peter Kriegel;Peer Kröger;Matthias Renz;Andreas Züfle
#t 2010
#c 5
#% 86950
#% 201876
#% 252304
#% 300174
#% 427199
#% 481956
#% 527026
#% 654487
#% 654490
#% 772835
#% 824728
#% 878300
#% 993965
#% 1016191
#% 1127377
#% 1181288
#% 1298880
#% 1426505
#! Fast query processing of complex objects, e.g. spatial or uncertain objects, depends on efficient spatial pruning of objects' approximations, which are typically minimum bounding rectangles (MBRs). In this paper, we propose a novel effective and efficient criterion to determine the spatial topology between multi-dimensional rectangles. Given three rectangles R, A, and B, in a multi-dimensional space, the task is to determine whether A, is definitely closer to R, than B. This domination relation is used in many applications to perform spatial pruning. Traditional techniques apply spatial pruning based on minimal and maximal distance. These techniques however show significant deficiencies in terms of effectivity. We prove that our decision criterion is correct, complete, and efficient to compute even for high dimensional databases. In addition, we tackle the problem of computing the number of objects dominating an object o. The challenge here is to incorporate objects that only partially dominate o. In this work we will show how to detect such partial domination topology by using a modified version of our decision criterion. We propose strategies for conservatively and progressively estimating the total number of objects dominating an object. Our experiments show that the new pruning criterion, albeit very general and widely applicable, significantly outperforms current state-of-the-art pruning criteria.

#index 1426506
#* Leveraging spatio-temporal redundancy for RFID data cleansing
#@ Haiquan Chen;Wei-Shinn Ku;Haixun Wang;Min-Te Sun
#t 2010
#c 5
#% 787130
#% 824747
#% 824764
#% 849543
#% 864417
#% 864470
#% 873104
#% 893102
#% 893103
#% 893189
#% 918685
#% 974946
#% 992830
#% 1016228
#% 1022341
#% 1036083
#% 1063521
#% 1206706
#% 1206747
#% 1206879
#% 1668029
#! Radio Frequency Identification (RFID) technologies are used in many applications for data collection. However, raw RFID readings are usually of low quality and may contain many anomalies. An ideal solution for RFID data cleansing should address the following issues. First, in many applications, duplicate readings (by multiple readers simultaneously or by a single reader over a period of time) of the same object are very common. The solution should take advantage of the resulting data redundancy for data cleaning. Second, prior knowledge about the readers and the environment (e.g., prior data distribution, false negative rates of readers) may help improve data quality and remove data anomalies, and a desired solution must be able to quantify the degree of uncertainty based on such knowledge. Third, the solution should take advantage of given constraints in target applications (e.g., the number of objects in a same location cannot exceed a given value) to elevate the accuracy of data cleansing. There are a number of existing RFID data cleansing techniques. However, none of them support all the aforementioned features. In this paper we propose a Bayesian inference based approach for cleaning RFID raw data. Our approach takes full advantage of data redundancy. To capture the likelihood, we design an n-state detection model and formally prove that the 3-state model can maximize the system performance. Moreover, in order to sample from the posterior, we devise a Metropolis-Hastings sampler with Constraints (MH-C), which incorporates constraint management to clean RFID raw data with high efficiency and accuracy. We validate our solution with a common RFID application and demonstrate the advantages of our approach through extensive simulations.

#index 1426507
#* Sampling dirty data for matching attributes
#@ Henning Köhler;Xiaofang Zhou;Shazia Sadiq;Yanfeng Shu;Kerry Taylor
#t 2010
#c 5
#% 210188
#% 248801
#% 255137
#% 273908
#% 300195
#% 397369
#% 479931
#% 480134
#% 480654
#% 572314
#% 616528
#% 765424
#% 765425
#% 765433
#% 765471
#% 840577
#% 845350
#% 874876
#% 893089
#% 913783
#% 978157
#% 1022227
#% 1206637
#! We investigate the problem of creating and analyzing samples of relational databases to find relationships between string-valued attributes. Our focus is on identifying attribute pairs whose value sets overlap, a pre-condition for typical joins over such attributes. However, real-world data sets are often 'dirty', especially when integrating data from different sources. To deal with this issue, we propose new similarity measures between sets of strings, which not only consider set based similarity, but also similarity between strings instances. To make the measures effective, we develop efficient algorithms for distributed sample creation and similarity computation. Test results show that for dirty data our measures are more accurate for measuring value overlap than existing sample-based methods, but we also observe that there is a clear tradeoff between accuracy and speed. This motivates a two-stage filtering approach, with both measures operating on the same samples.

#index 1426508
#* ERACER: a database approach for statistical inference and data cleaning
#@ Chris Mayfield;Jennifer Neville;Sunil Prabhakar
#t 2010
#c 5
#% 44876
#% 644182
#% 722754
#% 727901
#% 786511
#% 806214
#% 810019
#% 819555
#% 864417
#% 891559
#% 961268
#% 1000502
#% 1022228
#% 1063529
#% 1127604
#% 1328066
#! Real-world databases often contain syntactic and semantic errors, in spite of integrity constraints and other safety measures incorporated into modern DBMSs. We present ERACER, an iterative statistical framework for inferring missing information and correcting such errors automatically. Our approach is based on belief propagation and relational dependency networks, and includes an efficient approximate inference algorithm that is easily implemented in standard DBMSs using SQL and user defined functions. The system performs the inference and cleansing tasks in an integrated manner, using shrinkage techniques to infer correct values accurately even in the presence of dirty data. We evaluate the proposed methods empirically on multiple synthetic and real-world data sets. The results show that our framework achieves accuracy comparable to a baseline statistical method using Bayesian networks with exact inference. However, our framework has wider applicability than the Bayesian network baseline, due to its ability to reason with complex, cyclic relational dependencies.

#index 1426509
#* Recsplorer: recommendation algorithms based on precedence mining
#@ Aditya G. Parameswaran;Georgia Koutrika;Benjamin Bercovitz;Hector Garcia-Molina
#t 2010
#c 5
#% 124010
#% 173879
#% 234992
#% 292169
#% 330687
#% 342870
#% 413550
#% 415107
#% 420063
#% 424021
#% 463903
#% 729886
#% 734590
#% 734591
#% 734592
#% 755395
#% 783708
#% 838504
#% 963898
#% 1001290
#% 1181244
#% 1214666
#% 1287276
#% 1357660
#% 1650399
#! We study recommendations in applications where there are temporal patterns in the way items are consumed or watched. For example, a student who has taken the Advanced Algorithms course is more likely to be interested in Convex Optimization, but a student who has taken Convex Optimization need not be interested in Advanced Algorithms in the future. Similarly, a person who has purchased the Godfather I DVD on Amazon is more likely to purchase Godfather II sometime in the future (though it is not strictly necessary to watch/purchase Godfather I beforehand). We propose a precedence mining model that estimates the probability of future consumption based on past behavior. We then propose Recsplorer: a suite of recommendation algorithms that exploit the precedence information. We evaluate our algorithms, as well as traditional recommendation ones, using a real course planning system. We use existing transcripts to evaluate how well the algorithms perform. In addition, we augment our experiments with a user study on the live system where users rate their recommendations.

#index 1426510
#* TEDI: efficient shortest path query answering on graphs
#@ Fang Wei
#t 2010
#c 5
#% 31482
#% 58365
#% 234905
#% 722530
#% 800534
#% 813718
#% 824692
#% 864462
#% 874889
#% 960259
#% 960304
#% 1063514
#% 1181255
#% 1206910
#% 1217208
#% 1291412
#! Efficient shortest path query answering in large graphs is enjoying a growing number of applications, such as ranked keyword search in databases, social networks, ontology reasoning and bioinformatics. A shortest path query on a graph finds the shortest path for the given source and target vertices in the graph. Current techniques for efficient evaluation of such queries are based on the pre-computation of compressed Breadth First Search trees of the graph. However, they suffer from drawbacks of scalability. To address these problems, we propose TEDI, an indexing and query processing scheme for the shortest path query answering. TEDI is based on the tree decomposition methodology. The graph is first decomposed into a tree in which the node (a.k.a. bag) contains more than one vertex from the graph. The shortest paths are stored in such bags and these local paths together with the tree are the components of the index of the graph. Based on this index, a bottom-up operation can be executed to find the shortest path for any given source and target vertices. Our experimental results show that TEDI offers orders-of-magnitude performance improvement over existing approaches on the index construction time, the index size and the query answering.

#index 1426511
#* GBLENDER: towards blending visual query formulation and query processing in graph databases
#@ Changjiu Jin;Sourav S. Bhowmick;Xiaokui Xiao;James Cheng;Byron Choi
#t 2010
#c 5
#% 268797
#% 288990
#% 601159
#% 629708
#% 727845
#% 765429
#% 810072
#% 832188
#% 864425
#% 864539
#% 905847
#% 960234
#% 960304
#% 960305
#% 1022280
#% 1044450
#% 1063500
#% 1127380
#! Given a graph database D and a query graph g, an exact subgraph matching query asks for the set S of graphs in D that contain g as a subgraph. This type of queries find important applications in several domains such as bioinformatics and chemoinformatics, where users are generally not familiar with complex graph query languages. Consequently, user-friendly visual interfaces which support query graph construction can reduce the burden of data retrieval for these users. Existing techniques for subgraph matching queries built on top of such visual framework are designed to optimize the time required in retrieving the result set S from D, assuming that the whole query graph has been constructed. This leads to sub-optimal system response time as the query processing is initiated only after the user has finished drawing the query graph. In this paper, we take the first step towards exploring a novel graph query processing paradigm, where instead of processing a query graph after its construction, it interleaves visual query construction and processing to improve system response time. To realize this, we present an algorithm called GBLENDER that prunes false results and prefetches partial query results by exploiting the latency offered by the visual query formulation. It employs a novel action-aware indexing scheme that exploits users' interaction characteristics with visual interfaces to support efficient retrieval. Extensive experiments on both real and synthetic datasets demonstrate the effectiveness and efficiency of our solution.

#index 1426512
#* Computing label-constraint reachability in graph databases
#@ Ruoming Jin;Hui Hong;Haixun Wang;Ning Ruan;Yang Xiang
#t 2010
#c 5
#% 58365
#% 70370
#% 94589
#% 197751
#% 237192
#% 864462
#% 956564
#% 1013630
#% 1063514
#% 1073956
#% 1211740
#% 1217208
#% 1669913
#! Our world today is generating huge amounts of graph data such as social networks, biological networks, and the semantic web. Many of these real-world graphs are edge-labeled graphs, i.e., each edge has a label that denotes the relationship between the two vertices connected by the edge. A fundamental research problem on these labeled graphs is how to handle the label-constraint reachability query: Can vertex u reach vertex v through a path whose edge labels are constrained by a set of labels? In this work, we introduce a novel tree-based index framework which utilizes the directed maximal weighted spanning tree algorithm and sampling techniques to maximally compress the generalized transitive closure for the labeled graphs. An extensive experimental evaluation on both real and synthetic datasets demonstrates the efficiency of our approach in answering label-constraint reachability queries.

#index 1426513
#* Pregel: a system for large-scale graph processing
#@ Grzegorz Malewicz;Matthew H. Austern;Aart J.C Bik;James C. Dehnert;Ilan Horn;Naty Leiser;Grzegorz Czajkowski
#t 2010
#c 5
#% 69503
#% 144942
#% 148021
#% 207879
#% 268079
#% 276705
#% 282771
#% 284109
#% 290417
#% 296571
#% 343740
#% 353069
#% 382482
#% 570430
#% 578337
#% 723279
#% 731403
#% 737674
#% 760777
#% 811868
#% 834423
#% 843790
#% 887602
#% 933329
#% 954300
#% 963669
#% 983467
#% 1054227
#% 1063553
#% 1245882
#% 1318636
#% 1346430
#% 1468421
#! Many practical computing problems concern large graphs. Standard examples include the Web graph and various social networks. The scale of these graphs - in some cases billions of vertices, trillions of edges - poses challenges to their efficient processing. In this paper we present a computational model suitable for this task. Programs are expressed as a sequence of iterations, in each of which a vertex can receive messages sent in the previous iteration, send messages to other vertices, and modify its own state and that of its outgoing edges or mutate graph topology. This vertex-centric approach is flexible enough to express a broad set of algorithms. The model has been designed for efficient, scalable and fault-tolerant implementation on clusters of thousands of commodity computers, and its implied synchronicity makes reasoning about programs easier. Distribution-related details are hidden behind an abstract API. The result is a framework for processing large graphs that is expressive and easy to program.

#index 1426514
#* PR-join: a non-blocking join achieving higher early result rate with statistical guarantees
#@ Shimin Chen;Phillip B. Gibbons;Suman Nath
#t 2010
#c 5
#% 136740
#% 227883
#% 273910
#% 397370
#% 420114
#% 427195
#% 745488
#% 810038
#% 824721
#% 912242
#% 951778
#% 993956
#% 1052068
#% 1063551
#% 1092009
#% 1127391
#% 1127428
#% 1217151
#% 1217152
#% 1328052
#% 1328139
#% 1328144
#! Online aggregation is a promising solution to achieving fast early responses for interactive ad-hoc queries that compute aggregates on a large amount of data. Essential to the success of online aggregation is a good non-blocking join algorithm that enables both (i) high early result rates with statistical guarantees and (ii) fast end-to-end query times. We analyze existing non-blocking join algorithms and find that they all provide sub-optimal early result rates, and those with fast end-to-end times achieve them only by further sacrificing their early result rates. We propose a new non-blocking join algorithm, Partitioned expanding Ripple Join (PR-Join), which achieves considerably higher early result rates than previous non-blocking joins, while also delivering fast end-to-end query times. PR-Join performs separate, ripple-like join operations on individual hash partitions, where the width of a ripple expands multiplicatively over time. This contrasts with the non-partitioned, fixed-width ripples of Block Ripple Join. Assuming, as in previous non-blocking join studies, that the input relations are in random order, PR-Join ensures representative early results that are amenable to statistical guarantees. We show both analytically and with real-machine experiments that PR-Join achieves over an order of magnitude higher early result rates than previous non-blocking joins. We also discuss the benefits of using a flash-based SSD for temporary storage, showing that PR-Join can then achieve close to optimal end-to-end performance. Finally, we consider the joining of finite data streams that arrive over time, and find that PR-Join achieves similar or higher result rates than RPJ, the state-of-the-art algorithm specialized for that domain.

#index 1426515
#* PODS: a new model and processing algorithms for uncertain data streams
#@ Thanh T.L. Tran;Liping Peng;Boduo Li;Yanlei Diao;Anna Liu
#t 2010
#c 5
#% 654487
#% 751027
#% 818704
#% 874976
#% 893102
#% 893167
#% 960257
#% 977008
#% 992830
#% 993949
#% 1016178
#% 1063521
#% 1063529
#% 1127378
#% 1127415
#% 1206717
#% 1206735
#% 1206770
#% 1206772
#% 1206786
#% 1206879
#% 1207010
#% 1655707
#! Uncertain data streams, where data is incomplete, imprecise, and even misleading, have been observed in many environments. Feeding such data streams to existing stream systems produces results of unknown quality, which is of paramount concern to monitoring applications. In this paper, we present the PODS system that supports stream processing for uncertain data naturally captured using continuous random variables. PODS employs a unique data model that is flexible and allows efficient computation. Built on this model, we develop evaluation techniques for complex relational operators, i.e., aggregates and joins, by exploring advanced statistical theory and approximation. Evaluation results show that our techniques can achieve high performance while satisfying accuracy requirements, and significantly outperform a state-of-the-art sampling method. A case study further shows that our techniques can enable a tornado detection system (for the first time) to produce detection results at stream speed and with much improved quality.

#index 1426516
#* Fast approximate correlation for massive time-series data
#@ Abdullah Mueen;Suman Nath;Jie Liu
#t 2010
#c 5
#% 172424
#% 172949
#% 227857
#% 261121
#% 290262
#% 328048
#% 333926
#% 333941
#% 460862
#% 464196
#% 605157
#% 765412
#% 810058
#% 823413
#% 824709
#% 993961
#% 1034712
#% 1177867
#% 1328117
#% 1408850
#! We consider the problem of computing all-pair correlations in a warehouse containing a large number (e.g., tens of thousands) of time-series (or, signals). The problem arises in automatic discovery of patterns and anomalies in data intensive applications such as data center management, environmental monitoring, and scientific experiments. However, with existing techniques, solving the problem for a large stream warehouse is extremely expensive, due to the problem's inherent quadratic I/O and CPU complexities. We propose novel algorithms, based on Discrete Fourier Transformation (DFT) and graph partitioning, to reduce the end-to-end response time of an all-pair correlation query. To minimize I/O cost, we partition a massive set of input signals into smaller batches such that caching the signals one batch at a time maximizes data reuse and minimizes disk I/O. To reduce CPU cost, we propose two approximation algorithms. Our first algorithm efficiently computes approximate correlation coefficients of similar signal pairs within a given error bound. The second algorithm efficiently identifies, without any false positives or negatives, all signal pairs with correlations above a given threshold. For many real applications, our approximate solutions are as useful as corresponding exact solutions, due to our strict error guarantees. However, compared to the state-of-the-art exact algorithms, our algorithms are up to 17x faster for several real datasets.

#index 1426517
#* An algorithmic approach to event summarization
#@ Peng Wang;Haixun Wang;Majin Liu;Wei Wang
#t 2010
#c 5
#% 172949
#% 333941
#% 338425
#% 342633
#% 420063
#% 463903
#% 466506
#% 729932
#% 799764
#% 823408
#% 832572
#% 881543
#% 989609
#% 1083670
#% 1083674
#% 1206700
#% 1207013
#! Recently, much study has been directed toward summarizing event data, in the hope that the summary will lead us to a better understanding of the system that generates the events. However, instead of offering a global picture of the system, the summary obtained by most current approaches are piecewise, each describing an isolated snapshot of the system. We argue that the best summary, both in terms of its minimal description length and its interpretability, is the one obtained with the understanding of the internal dynamics of the system. Such understanding includes, for example, what are the internal states of the system, and how the system alternates among these states. In this paper, we adopt an algorithmic approach for event data summarization. More specifically, we use a hidden Markov model to describe the event generation process. We show that summarizing events based on the learned hidden Markov Model achieves short description length and high interpretability. Experiments show that our approach is both efficient and effective.

#index 1426518
#* Spreadsheet as a relational database engine
#@ Jerzy Tyszkiewicz
#t 2010
#c 5
#% 55162
#% 166295
#% 387508
#% 462226
#% 654445
#% 665367
#% 722741
#% 737694
#% 824754
#% 824758
#% 873297
#% 916777
#% 1015364
#% 1206925
#! Spreadsheets are among the most commonly used applications for data management and analysis. Perhaps they are even among the most widely used computer applications of all kinds. However, the spreadsheet paradigm of computation still lacks sufficient analysis. In this paper we demonstrate that a spreadsheet can play the role of a relational database engine, without any use of macros or built-in programming languages, merely by utilizing spreadsheet formulas. We achieve that by implementing all operators of relational algebra by means of spreadsheet functions. Given a definition of a database in SQL, it is therefore possible to construct a spreadsheet workbook with empty worksheets for data tables and worksheets filled with formulas for queries. From then on, when the user enters, alters or deletes data in the data worksheets, the formulas in query worksheets automatically compute the actual results of the queries. Thus, the spreadsheet serves as data storage and executes SQL queries, and therefore acts as a relational database engine. The paper is based on Microsoft Excel (TM), but our constructions work in other spreadsheet systems, too. We present a number of performance tests conducted in the beta version of Excel 2010. Their conclusion is that the performance is sufficient for a desktop database with a couple thousand rows.

#index 1426519
#* Scalable architecture and query optimization fortransaction-time DBs with evolving schemas
#@ Hyun Jin Moon;Carlo A. Curino;Carlo Zaniolo
#t 2010
#c 5
#% 5373
#% 32903
#% 71562
#% 107764
#% 234756
#% 242856
#% 260022
#% 286986
#% 287070
#% 287268
#% 308448
#% 361445
#% 442967
#% 443145
#% 443320
#% 481928
#% 527786
#% 527789
#% 654455
#% 654457
#% 765432
#% 812918
#% 824697
#% 864422
#% 1022236
#% 1052435
#% 1072638
#% 1126566
#% 1127411
#% 1127420
#% 1127421
#% 1206971
#% 1727521
#! The problem of archiving and querying the history of a database is made more complex by the fact that, along with the database content, the database schema also evolves with time. Indeed, archival quality can only be guaranteed by storing past database contents using the schema versions under which they were originally created. This causes major usability and scalability problems in preservation, retrieval and querying of databases with intense evolution histories, i.e., hundreds of schema versions. This scenario is common in web information systems and scientific databases that frequently accumulate that many versions in just a few years. Our system, Archival Information Management System (AIMS), solves this usability issue by letting users write queries against a chosen schema version and then performing for the users the rewriting and execution of queries on all appropriate schema versions. AIMS achieves scalability by using (i) an advanced storage strategy based on relational technology and attribute-level-timestamping of the history of the database content, (ii) suitable temporal indexing and clustering techniques, and (iii) novel temporal query optimizations. In particular, with AIMS we introduce a novel technique called CoalNesT that achieves unprecedented performance when temporal coalescing tuples fragmented by schema changes. Extensive experiments show that the performance and scalability thus achieved greatly exceeds those obtained by previous approaches. The AIMS technology is easily deployed by plugging into existing DBMS replication technologies, leading to very low overhead; moreover, by decoupling logical and physical layers provides multiple query interfaces, from the basic archive&query features considered in the upcoming SQL standards, to the much richer temporal XML/XQuery capabilities proposed by researchers.

#index 1426520
#* Data conflict resolution using trust mappings
#@ Wolfgang Gatterbauer;Dan Suciu
#t 2010
#c 5
#% 663
#% 103704
#% 342829
#% 408396
#% 874971
#% 879041
#% 880394
#% 912245
#% 960246
#% 1022258
#% 1328109
#% 1328125
#% 1405506
#% 1661426
#! In massively collaborative projects such as scientific or community databases, users often need to agree or disagree on the content of individual data items. On the other hand, trust relationships often exist between users, allowing them to accept or reject other users' beliefs by default. As those trust relationships become complex, however, it becomes difficult to define and compute a consistent snapshot of the conflicting information. Previous solutions to a related problem, the update reconciliation problem, are dependent on the order in which the updates are processed and, therefore, do not guarantee a globally consistent snapshot. This paper proposes the first principled solution to the automatic conflict resolution problem in a community database. Our semantics is based on the certain tuples of all stable models of a logic program. While evaluating stable models in general is well known to be hard, even for very simple logic programs, we show that the conflict resolution problem admits a PTIME solution. To the best of our knowledge, ours is the first PTIME algorithm that allows conflict resolution in a principled way. We further discuss extensions to negative beliefs and prove that some of these extensions are hard. This work is done in the context of the BeliefDB project at the University of Washington, which focuses on the efficient management of conflicts in community databases.

#index 1426521
#* Analyzing the energy efficiency of a database server
#@ Dimitris Tsirogiannis;Stavros Harizopoulos;Mehul A. Shah
#t 2010
#c 5
#% 435159
#% 566122
#% 893129
#% 960264
#% 963135
#% 1034471
#% 1039029
#% 1063543
#% 1070440
#% 1247895
#% 1278373
#% 1468291
#% 1468292
#! Rising energy costs in large data centers are driving an agenda for energy-efficient computing. In this paper, we focus on the role of database software in affecting, and, ultimately, improving the energy efficiency of a server. We first characterize the power-use profiles of database operators under different configuration parameters. We find that common database operations can exercise the full dynamic power range of a server, and that the CPU power consumption of different operators, for the same CPU utilization, can differ by as much as 60%. We also find that for these operations CPU power does not vary linearly with CPU utilization. We then experiment with several classes of database systems and storage managers, varying parameters that span from different query plans to compression algorithms and from physical layout to CPU frequency and operating system scheduling. Contrary to what recent work has suggested, we find that within a single node intended for use in scale-out (shared-nothing) architectures, the most energy-efficient configuration is typically the highest performing one. We explain under which circumstances this is not the case, and argue that these circumstances do not warrant a retargeting of database system optimization goals. Further, our results reveal opportunities for cross-node energy optimizations and point out directions for new scale-out architectures.

#index 1426522
#* Processing proximity relations in road networks
#@ Zhengdao Xu;Hans-Arno Jacobsen
#t 2010
#c 5
#% 300162
#% 333938
#% 442615
#% 709882
#% 801679
#% 810061
#% 814343
#% 814385
#% 814646
#% 824723
#% 846190
#% 893092
#% 893162
#% 960282
#% 1015321
#% 1016193
#% 1016252
#% 1022280
#% 1063472
#% 1206898
#% 1527432
#! Applications ranging from location-based services to multi-player online gaming require continuous query support to monitor, track, and detect events of interest among sets of moving objects. Examples are alerting capabilities for detecting whether the distance, the travel cost, or the travel time among a set of moving objects exceeds a threshold. These types of queries are driven by continuous streams of location updates, simultaneously evaluated over many queries. In this paper, we define three types of proximity relations that induce location constraints to model continuous spatio-temporal queries among sets of moving objects in road networks. Our focus lies on evaluating a large number of continuous queries simultaneously. We introduce a novel moving object indexing technique that together with a novel road network partitioning scheme restricts computations within the partial road network. These techniques reduce query processing overhead by more than 95%. Experiments over real-world data sets show that our approach is twenty times faster than a baseline algorithm.

#index 1426523
#* Searching trajectories by locations: an efficiency study
#@ Zaiben Chen;Heng Tao Shen;Xiaofang Zhou;Yu Zheng;Xing Xie
#t 2010
#c 5
#% 172949
#% 201876
#% 287466
#% 318703
#% 427199
#% 443396
#% 460862
#% 462231
#% 464859
#% 480473
#% 481620
#% 631923
#% 659971
#% 765451
#% 810049
#% 960281
#% 976715
#% 1016195
#% 1127422
#% 1127609
#! Trajectory search has long been an attractive and challenging topic which blooms various interesting applications in spatial-temporal databases. In this work, we study a new problem of searching trajectories by locations, in which context the query is only a small set of locations with or without an order specified, while the target is to find the k Best-Connected Trajectories (k-BCT) from a database such that the k-BCT best connect the designated locations geographically. Different from the conventional trajectory search that looks for similar trajectories w.r.t. shape or other criteria by using a sample query trajectory, we focus on the goodness of connection provided by a trajectory to the specified query locations. This new query can benefit users in many novel applications such as trip planning. In our work, we firstly define a new similarity function for measuring how well a trajectory connects the query locations, with both spatial distance and order constraint being considered. Upon the observation that the number of query locations is normally small (e.g. 10 or less) since it is impractical for a user to input too many locations, we analyze the feasibility of using a general-purpose spatial index to achieve efficient k-BCT search, based on a simple Incremental k-NN based Algorithm (IKNN). The IKNN effectively prunes and refines trajectories by using the devised lower bound and upper bound of similarity. Our contributions mainly lie in adapting the best-first and depth-first k-NN algorithms to the basic IKNN properly, and more importantly ensuring the efficiency in both search effort and memory usage. An in-depth study on the adaption and its efficiency is provided. Further optimization is also presented to accelerate the IKNN algorithm. Finally, we verify the efficiency of the algorithm by extensive experiments.

#index 1426524
#* Processing continuous join queries in sensor networks: a filtering approach
#@ Mirco Stern;Klemens Böhm;Erik Buchmann
#t 2010
#c 5
#% 264263
#% 333969
#% 654488
#% 731087
#% 757953
#% 765402
#% 806214
#% 824715
#% 870326
#% 960277
#% 982569
#% 982571
#% 1016178
#% 1022282
#% 1206847
#% 1208229
#% 1327655
#% 1669531
#% 1716961
#! While join processing in wireless sensor networks has received a lot of attention recently, current solutions do not work well for continuous queries. In those networks however, continuous queries are the rule. To minimize the communication costs of join processing, it is important to not ship non-joining tuples. In order to know which tuples do not join, prior work has proposed a precomputation step. For continuous queries however, repeating the precomputation for each execution is unnecessary and leaves aside that data tends to be temporally correlated. In this paper, we present a filtering approach for the processing of continuous join queries. We propose to keep the filters and to maintain them. The problems are determining the sizes of the filters and deciding which filters to update. Simplistic approaches result in bad performance. We show how to compute solutions that are optimal. Experiments on real-world sensor data indicate that our method performs close to a theoretical optimum and consistently outperforms state-of-the-art join approaches.

#index 1426525
#* TACO: tunable approximate computation of outliers in wireless sensor networks
#@ Nikos Giatrakos;Yannis Kotidis;Antonios Deligiannakis;Vasilis Vassalos;Yannis Theodoridis
#t 2010
#c 5
#% 205305
#% 249321
#% 259665
#% 309466
#% 333950
#% 347225
#% 427022
#% 723903
#% 729912
#% 731096
#% 765445
#% 783736
#% 800503
#% 805466
#% 873104
#% 884465
#% 890512
#% 893102
#% 893104
#% 939408
#% 966970
#% 981635
#% 983452
#% 993949
#% 994564
#% 1206849
#! Wireless sensor networks are becoming increasingly popular for a variety of applications. Users are frequently faced with the surprising discovery that readings produced by the sensing elements of their motes are often contaminated with outliers. Outlier readings can severely affect applications that rely on timely and reliable sensory data in order to provide the desired functionality. As a consequence, there is a recent trend to explore how techniques that identify outlier values can be applied to sensory data cleaning. Unfortunately, most of these approaches incur an overwhelming communication overhead, which limits their practicality. In this paper we introduce an in-network outlier detection framework, based on locality sensitive hashing, extended with a novel boosting process as well as efficient load balancing and comparison pruning mechanisms. Our method trades off bandwidth for accuracy in a straightforward manner and supports many intuitive similarity metrics.

#index 1426526
#* GRN model of probabilistic databases: construction, transition and querying
#@ Ruiwen Chen;Yongyi Mao;Iluju Kiringa
#t 2010
#c 5
#% 44876
#% 58608
#% 68244
#% 136358
#% 219474
#% 265692
#% 273683
#% 384978
#% 864394
#% 864417
#% 893168
#% 976984
#% 1016178
#% 1016201
#% 1022206
#% 1022259
#% 1036075
#% 1063521
#% 1063725
#% 1127376
#% 1180008
#% 1650620
#% 1661439
#! Under the tuple-level uncertainty paradigm, we formalize the use of a novel graphical model, Generator-Recognizer Network (GRN), as a model of probabilistic databases. The GRN modeling framework is capable of representing a much wider range of tuple dependency structure. We show that a GRN representation of a probabilistic database may undergo transitions induced by imposing constraints or evaluating queries. We formalize procedures for these two types of transitions such that the resulting graphical models after transitions remain as GRNs. This formalism makes GRN a self-contained modeling framework and a closed representation system for probabilistic databases - a property that is lacking in most existing models. In addition, we show that exploiting the transitional mechanisms allows a systematic approach to constructing GRNs for arbitrary probabilistic data at arbitrary stages. Advantages of GRNs in query evaluation are also demonstrated.

#index 1426527
#* Consistent query answers in inconsistent probabilistic databases
#@ Xiang Lian;Lei Chen;Shaoxu Song
#t 2010
#c 5
#% 86950
#% 213975
#% 273687
#% 420072
#% 442758
#% 479453
#% 654487
#% 778320
#% 810019
#% 810020
#% 810098
#% 824715
#% 824764
#% 833132
#% 864417
#% 893167
#% 941785
#% 992830
#% 1015326
#% 1022228
#% 1063520
#% 1063521
#% 1063725
#% 1127378
#% 1166527
#% 1206646
#% 1206732
#% 1206893
#% 1229786
#% 1290947
#% 1328151
#% 1328155
#% 1328156
#% 1328159
#% 1347336
#! Efficient and effective manipulation of probabilistic data has become increasingly important recently due to many real applications that involve the data uncertainty. This is especially crucial when probabilistic data collected from different sources disagree with each other and incur inconsistencies. In order to accommodate such inconsistencies and enable consistent query answering (CQA), in this paper, we propose the all-possible-repair semantics in the context of inconsistent probabilistic databases, which formalize the repairs on the database as repair worlds via a graph representation. In turn, the CQA problem can be converted into one in the so-called repaired possible worlds (w.r.t. both repair worlds and possible worlds). We investigate a series of consistent queries in inconsistent probabilistic databases, including consistent range queries, join, and top-k queries, which, however, need to deal with an exponential number of the repaired possible worlds at high cost. To tackle the efficiency problem of CQA, in this paper, we propose efficient approaches for retrieving consistent query answers, including effective pruning methods to filter out false positives. Extensive experiments have been conducted to demonstrate the efficiency and effectiveness of our approaches.

#index 1426528
#* Threshold query optimization for uncertain data
#@ Yinian Qi;Rohit Jain;Sarvjeet Singh;Sunil Prabhakar
#t 2010
#c 5
#% 771228
#% 810098
#% 824728
#% 864396
#% 893167
#% 907562
#% 960336
#% 983259
#% 1016201
#% 1016202
#% 1022203
#% 1063485
#% 1063520
#% 1127377
#% 1127378
#% 1206716
#% 1206732
#% 1206735
#% 1206866
#% 1206893
#% 1206987
#% 1217128
#% 1217174
#% 1217175
#% 1217181
#% 1217251
#! The probabilistic threshold query (PTQ) is one of the most common queries in uncertain databases, where all results satisfying the query with probabilities that meet the threshold requirement are returned. PTQ is used widely in nearest-neighbor queries, range queries, ranking queries, etc. In this paper, we investigate the general PTQ for arbitrary SQL queries that involve selections, projections and joins. The uncertain database model that we use is one that combines both attribute and tuple uncertainty as well as correlations between arbitrary attribute sets. We address the PTQ optimization problem that aims at improving the efficiency of PTQ query execution by enabling alternative query plan enumeration for optimization. We propose general optimization rules as well as rules specifically for selections, projections and joins. We introduce a threshold operator (τ-operator) to the query plan and show it is generally desirable to push down the τ-operator as much as possible.

#index 1426529
#* Probabilistic string similarity joins
#@ Jeffrey Jestes;Feifei Li;Zhepeng Yan;Ke Yi
#t 2010
#c 5
#% 359127
#% 480654
#% 547438
#% 654467
#% 810098
#% 864392
#% 864394
#% 893189
#% 907562
#% 1022218
#% 1022227
#% 1022259
#% 1022341
#% 1063521
#% 1063568
#% 1086405
#% 1127425
#% 1190092
#% 1206615
#% 1206665
#% 1206717
#% 1206781
#% 1206972
#% 1291123
#% 1669490
#! Edit distance based string similarity join is a fundamental operator in string databases. Increasingly, many applications in data cleaning, data integration, and scientific computing have to deal with fuzzy information in string attributes. Despite the intensive efforts devoted in processing (deterministic) string joins and managing probabilistic data respectively, modeling and processing probabilistic strings is still a largely unexplored territory. This work studies the string join problem in probabilistic string databases, using the expected edit distance (EED) as the similarity measure. We first discuss two probabilistic string models to capture the fuzziness in string values in real-world applications. The string-level model is complete, but may be expensive to represent and process. The character-level model has a much more succinct representation when uncertainty in strings only exists at certain positions. Since computing the EED between two probabilistic strings is prohibitively expensive, we have designed efficient and effective pruning techniques that can be easily implemented in existing relational database engines for both models. Extensive experiments on real data have demonstrated order-of-magnitude improvements of our approaches over the baseline.

#index 1426530
#* FAST: fast architecture sensitive tree search on modern CPUs and GPUs
#@ Changkyu Kim;Jatin Chhugani;Nadathur Satish;Eric Sedlar;Anthony D. Nguyen;Tim Kaldewey;Victor W. Lee;Scott A. Brandt;Pradeep Dubey
#t 2010
#c 5
#% 111315
#% 191274
#% 287715
#% 300194
#% 317933
#% 333942
#% 333949
#% 397361
#% 397362
#% 464843
#% 464987
#% 479769
#% 479819
#% 481424
#% 580978
#% 864446
#% 875026
#% 960266
#% 1015288
#% 1022230
#% 1050225
#% 1127563
#% 1217168
#% 1222052
#% 1268643
#% 1328057
#% 1328141
#% 1468279
#! In-memory tree structured index search is a fundamental database operation. Modern processors provide tremendous computing power by integrating multiple cores, each with wide vector units. There has been much work to exploit modern processor architectures for database primitives like scan, sort, join and aggregation. However, unlike other primitives, tree search presents significant challenges due to irregular and unpredictable data accesses in tree traversal. In this paper, we present FAST, an extremely fast architecture sensitive layout of the index tree. FAST is a binary tree logically organized to optimize for architecture features like page size, cache line size, and SIMD width of the underlying hardware. FAST eliminates impact of memory latency, and exploits thread-level and datalevel parallelism on both CPUs and GPUs to achieve 50 million (CPU) and 85 million (GPU) queries per second, 5X (CPU) and 1.7X (GPU) faster than the best previously reported performance on the same architectures. FAST supports efficient bulk updates by rebuilding index trees in less than 0.1 seconds for datasets as large as 64Mkeys and naturally integrates compression techniques, overcoming the memory bandwidth bottleneck and achieving a 6X performance improvement over uncompressed index search for large keys on CPUs.

#index 1426531
#* Fast sort on CPUs and GPUs: a case for bandwidth oblivious SIMD sort
#@ Nadathur Satish;Changkyu Kim;Jatin Chhugani;Anthony D. Nguyen;Victor W. Lee;Daehyun Kim;Pradeep Dubey
#t 2010
#c 5
#% 80192
#% 112212
#% 132429
#% 191274
#% 282235
#% 333942
#% 500284
#% 874997
#% 988651
#% 1002570
#% 1050225
#% 1127563
#% 1139191
#% 1209574
#% 1217168
#% 1241839
#% 1328057
#! Sort is a fundamental kernel used in many database operations. In-memory sorts are now feasible; sort performance is limited by compute flops and main memory bandwidth rather than I/O. In this paper, we present a competitive analysis of comparison and non-comparison based sorting algorithms on two modern architectures - the latest CPU and GPU architectures. We propose novel CPU radix sort and GPU merge sort implementations which are 2X faster than previously published results. We perform a fair comparison of the algorithms using these best performing implementations on both architectures. While radix sort is faster on current architectures, the gap narrows from CPU to GPU architectures. Merge sort performs better than radix sort for sorting keys of large sizes - such keys will be required to accommodate the increasing cardinality of future databases. We present analytical models for analyzing the performance of our implementations in terms of architectural features such as core count, SIMD and bandwidth. Our obtained performance results are successfully predicted by our models. Our analysis points to merge sort winning over radix sort on future architectures due to its efficient utilization of SIMD and low bandwidth utilization. We simulate a 64-core platform with varying SIMD widths under constant bandwidth per core constraints, and show that large data sizes of 240 (one trillion records), merge sort performance on large key sizes is up to 3X better than radix sort for large SIMD widths on future architectures. Therefore, merge sort should be the sorting method of choice for future databases.

#index 1426532
#* Page-differential logging: an efficient and DBMS-independent approach for storing data into flash memory
#@ Yi-Reun Kim;Kyu-Young Whang;Il-Yeol Song
#t 2010
#c 5
#% 115617
#% 131555
#% 724148
#% 737385
#% 800613
#% 829901
#% 848746
#% 893215
#% 897788
#% 960238
#% 978505
#% 985755
#% 1085291
#% 1127391
#% 1127428
#% 1447701
#% 1693621
#! Flash memory is widely used as the secondary storage in lightweight computing devices due to its outstanding advantages over magnetic disks. Flash memory has many access characteristics different from those of magnetic disks, and how to take advantage of them is becoming an important research issue. There are two existing approaches to storing data into flash memory: page-based and log-based. The former has good performance for read operations, but poor performance for write operations. In contrast, the latter has good performance for write operations when updates are light, but poor performance for read operations. In this paper, we propose a new method of storing data, called page-differential logging, for flash-based storage systems that solves the drawbacks of the two methods. The primary characteristics of our method are: (1) writing only the difference (which we define as the page-differential) between the original page in flash memory and the up-to-date page in memory; (2) computing and writing the page-differential only once at the time the page needs to be reflected into flash memory. The former contrasts with existing page-based methods that write the whole page including both changed and unchanged parts of data or from log-based ones that keep track of the history of all the changes in a page. Our method allows existing disk-based DBMSs to be reused as flash-based DBMSs just by modifying the flash memory driver, i.e., it is DBMS-independent. Experimental results show that the proposed method is superior in I/O performance, except for some special cases, to existing ones. Specifically, it improves the performance of various mixes of read-only and update operations by 0.5 (the special case when all transactions are read-only on updated pages) ~3.4 times over the page-based method and by 1.6 ~ 3.1 times over the log-based one for synthetic data of approximately 1 Gbytes. The TPC-C benchmark also shows improvement of the I/O time over existing methods by 1.2 ~ 6.1 times. This result indicates the effectiveness of our method under (semi) real workloads. We note that the performance advantage of our method can be further enhanced up to two folds by obviating the need to write to the spare area of the page a second time.

#index 1426533
#* Similarity search and locality sensitive hashing using ternary content addressable memories
#@ Rajendra Shinde;Ashish Goel;Pankaj Gupta;Debojyoti Dutta
#t 2010
#c 5
#% 249321
#% 289316
#% 321455
#% 347225
#% 375017
#% 479649
#% 479973
#% 480156
#% 656800
#% 749529
#% 762054
#% 769927
#% 821931
#% 847166
#% 866738
#% 871357
#% 875957
#% 898308
#% 898309
#% 939408
#% 950253
#% 956507
#% 1022281
#% 1023422
#% 1141462
#% 1215859
#% 1217189
#% 1238632
#% 1811362
#! Similarity search methods are widely used as kernels in various data mining and machine learning applications including those in computational biology, web search/clustering. Nearest neighbor search (NNS) algorithms are often used to retrieve similar entries, given a query. While there exist efficient techniques for exact query lookup using hashing, similarity search using exact nearest neighbors suffers from a "curse of dimensionality", i.e. for high dimensional spaces, best known solutions offer little improvement over brute force search and thus are unsuitable for large scale streaming applications. Fast solutions to the approximate NNS problem include Locality Sensitive Hashing (LSH) based techniques, which need storage polynomial in n with exponent greater than 1, and query time sublinear, but still polynomial in n, where n is the size of the database. In this work we present a new technique of solving the approximate NNS problem in Euclidean space using a Ternary Content Addressable Memory (TCAM), which needs near linear space and has O(1) query time. In fact, this method also works around the best known lower bounds in the cell probe model for the query time using a data structure near linear in the size of the data base. TCAMs are high performance associative memories widely used in networking applications such as address lookups and access control lists. A TCAM can query for a bit vector within a database of ternary vectors, where every bit position represents 0, 1 or *. The * is a wild card representing either a 0 or a 1. We leverage TCAMs to design a variant of LSH, called Ternary Locality Sensitive Hashing (TLSH) wherein we hash database entries represented by vectors in the Euclidean space into {0,1,*}. By using the added functionality of a TLSH scheme with respect to the * character, we solve an instance of the approximate nearest neighbor problem with 1 TCAM access and storage nearly linear in the size of the database. We validate our claims with extensive simulations using both real world (Wikipedia) as well as synthetic (but illustrative) datasets. We observe that using a TCAM of width 288 bits, it is possible to solve the approximate NNS problem on a database of size 1 million points with high accuracy. Finally, we design an experiment with TCAMs within an enterprise ethernet switch (Cisco Catalyst 4500) to validate that TLSH can be used to perform 1.5 million queries per second per 1Gb/s port. We believe that this work can open new avenues in very high speed data mining.

#index 1426534
#* Automatically incorporating new sources in keyword search-based data integration
#@ Partha Pratim Talukdar;Zachary G. Ives;Fernando Pereira
#t 2010
#c 5
#% 248801
#% 268079
#% 333990
#% 480645
#% 572314
#% 577309
#% 660001
#% 660011
#% 763882
#% 765410
#% 810018
#% 824693
#% 845350
#% 879568
#% 957170
#% 960259
#% 961152
#% 993987
#% 1016176
#% 1022259
#% 1055761
#% 1063533
#% 1063547
#% 1065163
#% 1074242
#% 1127393
#% 1127413
#% 1127557
#% 1206817
#% 1217153
#% 1264778
#! Scientific data offers some of the most interesting challenges in data integration today. Scientific fields evolve rapidly and accumulate masses of observational and experimental data that needs to be annotated, revised, interlinked, and made available to other scientists. From the perspective of the user, this can be a major headache as the data they seek may initially be spread across many databases in need of integration. Worse, even if users are given a solution that integrates the current state of the source databases, new data sources appear with new data items of interest to the user. Here we build upon recent ideas for creating integrated views over data sources using keyword search techniques, ranked answers, and user feedback [32] to investigate how to automatically discover when a new data source has content relevant to a user's view - in essence, performing automatic data integration for incoming data sets. The new architecture accommodates a variety of methods to discover related attributes, including label propagation algorithms from the machine learning community [2] and existing schema matchers [11]. The user may provide feedback on the suggested new results, helping the system repair any bad alignments or increase the cost of including a new source that is not useful. We evaluate our approach on actual bioinformatics schemas and data, using state-of-the-art schema matchers as components. We also discuss how our architecture can be adapted to more traditional settings with a mediated schema.

#index 1426535
#* Active knowledge: dynamically enriching RDF knowledge bases by web services
#@ Nicoleta Preda;Gjergji Kasneci;Fabian M. Suchanek;Thomas Neumann;Wenjun Yuan;Gerhard Weikum
#t 2010
#c 5
#% 198466
#% 229827
#% 237190
#% 384978
#% 397356
#% 480149
#% 481923
#% 482116
#% 531458
#% 572311
#% 735875
#% 801675
#% 824702
#% 830000
#% 864418
#% 956564
#% 956573
#% 1055735
#% 1055897
#% 1063559
#% 1072645
#% 1127402
#% 1131140
#% 1131145
#% 1180003
#% 1206702
#% 1217122
#% 1217164
#% 1275182
#% 1328081
#% 1328156
#% 1409954
#% 1499470
#! The proliferation of knowledge-sharing communities and the advances in information extraction have enabled the construction of large knowledge bases using the RDF data model to represent entities and relationships. However, as the Web and its latently embedded facts evolve, a knowledge base can never be complete and up-to-date. On the other hand, a rapidly increasing suite of Web services provide access to timely and high-quality information, but this is encapsulated by the service interface. We propose to leverage the information that could be dynamically obtained from Web services in order to enrich RDF knowledge bases on the fly whenever the knowledge base does not suffice to answer a user query. To this end, we develop a sound framework for appropriately generating queries to encapsulated Web services and efficient algorithms for query execution and result integration. The query generator composes sequences of function calls based on the available service interfaces. As Web service calls are expensive, our method aims to reduce the number of calls in order to retrieve results with sufficient recall. Our approach is fully implemented in a complete prototype system named ANGIE1. The user can query and browse the RDF knowledge base as if it already contained all the facts from the Web services. This data, however, is gathered and integrated on the fly, transparently to the user. We demonstrate the viability and efficiency of our approach in experiments based on real-life data provided by popular Web services.

#index 1426536
#* Schema clustering and retrieval for multi-domain pay-as-you-go data integration systems
#@ Hatem A. Mahmoud;Ashraf Aboulnaga
#t 2010
#c 5
#% 235941
#% 376266
#% 572314
#% 654459
#% 729437
#% 783472
#% 783791
#% 835018
#% 844413
#% 845350
#% 1063534
#% 1127557
#% 1190673
#! A data integration system offers a single interface to multiple structured data sources. Many application contexts (e.g., searching structured data on the web) involve the integration of large numbers of structured data sources. At web scale, it is impractical to use manual or semi-automatic data integration methods, so a pay-as-you-go approach is more appropriate. A pay-as-you-go approach entails using a fully automatic approximate data integration technique to provide an initial data integration system (i.e., an initial mediated schema, and initial mappings from source schemas to the mediated schema), and then refining the system as it gets used. Previous research has investigated automatic approximate data integration techniques, but all existing techniques require the schemas being integrated to belong to the same conceptual domain. At web scale, it is impractical to classify schemas into domains manually or semi-automatically, which limits the applicability of these techniques. In this paper, we present an approach for clustering schemas into domains without any human intervention and based only on the names of attributes in the schemas. Our clustering approach deals with uncertainty in assigning schemas to domains using a probabilistic model. We also propose a query classifier that determines, for a given a keyword query, the most relevant domains to this query. We experimentally demonstrate the effectiveness of our schema clustering and query classification techniques.

#index 1426537
#* Expressive and flexible access to web-extracted data: a keyword-based structured query language
#@ Jeffrey Pound;Ihab F. Ilyas;Grant Weddell
#t 2010
#c 5
#% 411762
#% 452869
#% 465914
#% 577373
#% 665856
#% 748600
#% 777931
#% 805849
#% 913802
#% 945700
#% 956564
#% 987276
#% 993987
#% 997488
#% 1022234
#% 1127393
#% 1206702
#% 1206799
#% 1207026
#! Automated extraction of structured data from Web sources often leads to large heterogeneous knowledge bases (KB), with data and schema items numbering in the hundreds of thousands or millions. Formulating information needs with conventional structured query languages is difficult due to the sheer size of schema information available to the user. We address this challenge by proposing a new query language that blends keyword search with structured query processing over large information graphs with rich semantics. Our formalism for structured queries based on keywords combines the flexibility of keyword search with the expressiveness of structures queries. We propose a solution to the resulting disambiguation problem caused by introducing keywords as primitives in a structured query language. We show how expressions in our proposed language can be rewritten using the vocabulary of the web-extracted KB, and how different possible rewritings can be ranked based on their syntactic relationship to the keywords in the query as well as their semantic coherence in the underlying KB. An extensive experimental study demonstrates the efficiency and effectiveness of our approach. Additionally, we show how our query language fits into QUICK, an end-to-end information system that integrates web-extracted data graphs with full-text search. In this system, the rewritten query describes an arbitrary topic of interest for which corresponding entities, and documents relevant to the entities, are efficiently retrieved.

#index 1426538
#* Multiple feature fusion for social media applications
#@ Bin Cui;Anthony K.H. Tung;Ce Zhang;Zhe Zhao
#t 2010
#c 5
#% 333854
#% 414514
#% 734590
#% 734915
#% 748600
#% 770816
#% 818262
#% 879594
#% 893192
#% 961190
#% 975033
#% 975160
#% 990258
#% 990259
#% 996159
#% 1063484
#% 1063588
#% 1172630
#% 1206681
#% 1206837
#% 1217203
#% 1227625
#% 1279789
#% 1775677
#! The emergence of social media as a crucial paradigm has posed new challenges to the research and industry communities, where media are designed to be disseminated through social interaction. Recent literature has noted the generality of multiple features in the social media environment, such as textual, visual and user information. However, most of the studies employ only a relatively simple mechanism to merge the features rather than fully exploit feature correlation for social media applications. In this paper, we propose a novel approach to fusing multiple features and their correlations for similarity evaluation. Specifically, we first build a Feature Interaction Graph (FIG) by taking features as nodes and the correlations between them as edges. Then, we employ a probabilistic model based on Markov Random Field to describe the graph for similarity measure between multimedia objects. Using that, we design an efficient retrieval algorithm for large social media data. Further, we integrate temporal information into the probabilistic model for social media recommendation. We evaluate our approach using a large real-life corpus collected from Flickr, and the experimental results indicate the superiority of our proposed method over state-of-the-art techniques.

#index 1426539
#* Finding maximal cliques in massive networks by H*-graph
#@ James Cheng;Yiping Ke;Ada Wai-Chee Fu;Jeffrey Xu Yu;Linhong Zhu
#t 2010
#c 5
#% 283833
#% 318330
#% 322619
#% 453518
#% 732477
#% 933558
#% 937814
#% 1048384
#% 1124599
#% 1130969
#% 1179877
#% 1250317
#% 1796812
#! Maximal clique enumeration (MCE) is a fundamental problem in graph theory and has important applications in many areas such as social network analysis and bioinformatics. The problem is extensively studied; however, the best existing algorithms require memory space linear in the size of the input graph. This has become a serious concern in view of the massive volume of today's fast-growing network graphs. Since MCE requires random access to different parts of a large graph, it is difficult to divide the graph into smaller parts and process one part at a time, because either the result may be incorrect and incomplete, or it incurs huge cost on merging the results from different parts. We propose a novel notion, H*-graph, which defines the core of a network and extends to encompass the neighborhood of the core for MCE computation. We propose the first external-memory algorithm for MCE (ExtMCE) that uses the H*-graph to bound the memory usage. We prove both the correctness and completeness of the result computed by ExtMCE. Extensive experiments verify that ExtMCE efficiently processes large networks that cannot be fit in the memory. We also show that the H*-graph captures important properties of the network; thus, updating the maximal cliques in the H*-graph retains the most essential information, with a low update cost, when it is infeasible to perform update on the entire network.

#index 1426540
#* K-isomorphism: privacy preserving network publication against structural attacks
#@ James Cheng;Ada Wai-chee Fu;Jia Liu
#t 2010
#c 5
#% 404719
#% 443463
#% 576761
#% 769940
#% 853532
#% 864412
#% 881460
#% 881480
#% 881496
#% 881523
#% 881546
#% 893100
#% 956511
#% 1063476
#% 1083672
#% 1083675
#% 1127360
#% 1127417
#% 1206763
#% 1207029
#% 1289476
#% 1328188
#% 1415851
#% 1426539
#! Serious concerns on privacy protection in social networks have been raised in recent years; however, research in this area is still in its infancy. The problem is challenging due to the diversity and complexity of graph data, on which an adversary can use many types of background knowledge to conduct an attack. One popular type of attacks as studied by pioneer work [2] is the use of embedding subgraphs. We follow this line of work and identify two realistic targets of attacks, namely, NodeInfo and LinkInfo. Our investigations show that k-isomorphism, or anonymization by forming k pairwise isomorphic subgraphs, is both sufficient and necessary for the protection. The problem is shown to be NP-hard. We devise a number of techniques to enhance the anonymization efficiency while retaining the data utility. A compound vertex ID mechanism is also introduced for privacy preservation over multiple data releases. The satisfactory performance on a number of real datasets, including HEP-Th, EUemail and LiveJournal, illustrates that the high symmetry of social networks is very helpful in mitigating the difficulty of the problem.

#index 1426541
#* Load-balanced query dissemination in privacy-aware online communities
#@ Emiran Curtmola;Alin Deutsch;K. K. Ramakrishnan;Divesh Srivastava
#t 2010
#c 5
#% 23638
#% 307424
#% 322884
#% 340297
#% 342344
#% 481599
#% 496291
#% 576761
#% 612477
#% 636008
#% 654443
#% 661478
#% 745389
#% 770903
#% 800517
#% 800582
#% 824762
#% 839584
#% 849547
#% 874970
#% 878916
#% 960252
#% 963716
#% 963801
#% 1008312
#% 1015327
#% 1016180
#% 1022333
#% 1127584
#% 1722232
#% 1849768
#! We propose a novel privacy-preserving distributed infrastructure in which data resides only with the publishers owning it. The infrastructure disseminates user queries to publishers, who answer them at their own discretion. The infrastructure enforces a publisher k-anonymity guarantee, which prevents leakage of information about which publishers are capable of answering a certain query. Given the virtual nature of the global data collection, we study the challenging problem of efficiently locating publishers in the community that contain data items matching a specified query. We propose a distributed index structure, UQDT, that is organized as a union of Query Dissemination Trees (QDTs), and realized on an overlay (i.e., logical) network infrastructure. Each QDT has data publishers as its leaf nodes, and overlay network nodes as its internal nodes; each internal node routes queries to publishers, based on a summary of the data advertised by publishers in its subtrees. We experimentally evaluate design tradeoffs, and demonstrate that UQDT can maximize throughput by preventing any overlay network node from becoming a bottleneck.

#index 1426542
#* Automatic contention detection and amelioration for data-intensive operations
#@ John Cieslewicz;Kenneth A. Ross;Kyoho Satsumi;Yang Ye
#t 2010
#c 5
#% 115661
#% 148195
#% 172913
#% 214065
#% 295986
#% 464987
#% 479821
#% 480119
#% 765693
#% 824655
#% 873339
#% 896768
#% 963669
#% 1022230
#% 1052064
#% 1052066
#% 1081258
#% 1127399
#% 1129954
#! To take full advantage of the parallelism offered by a multi-core machine, one must write parallel code. Writing parallel code is difficult. Even when one writes correct code, there are numerous performance pitfalls. For example, an unrecognized data hotspot could mean that all threads effectively serialize their access to the hotspot, and throughput is dramatically reduced. Previous work has demonstrated that database operations suffer from such hotspots when naively implemented to run in parallel on a multi-core processor. In this paper, we aim to provide a generic framework for performing certain kinds of concurrent database operations in parallel. The formalism is similar to user-defined aggregates and Google's MapReduce in that users specify certain functions for parts of the computation that need to be performed over large volumes of data. We provide infrastructure that allows multiple threads on a multi-core machine to concurrently perform read and write operations on shared data structures, automatically mitigating hotspots and other performance hazards. Our goal is not to squeeze the last drop of performance out of a particular platform. Rather, we aim to provide a framework within which a programmer can, without detailed knowledge of concurrent and parallel programming, develop code that efficiently utilizes a multi-core machine.

#index 1426543
#* Efficient parallel set-similarity joins using MapReduce
#@ Rares Vernica;Michael J. Carey;Chen Li
#t 2010
#c 5
#% 58352
#% 83134
#% 115661
#% 255137
#% 479973
#% 480654
#% 480774
#% 571725
#% 765463
#% 823403
#% 864392
#% 869500
#% 879600
#% 893164
#% 956506
#% 956518
#% 960326
#% 1023420
#% 1055684
#% 1127425
#% 1217159
#% 1328060
#! In this paper we study how to efficiently perform set-similarity joins in parallel using the popular MapReduce framework. We propose a 3-stage approach for end-to-end set-similarity joins. We take as input a set of records and output a set of joined records based on a set-similarity condition. We efficiently partition the data across nodes in order to balance the workload and minimize the need for replication. We study both self-join and R-S join cases, and show how to carefully control the amount of data kept in main memory on each node. We also propose solutions for the case where, even if we use the most fine-grained partitioning, the data still does not fit in the main memory of a node. We report results from extensive experiments on real datasets, synthetically increased in size, to evaluate the speedup and scaleup properties of the proposed algorithms using Hadoop.

#index 1426544
#* ParaTimer: a progress indicator for MapReduce DAGs
#@ Kristi Morton;Magdalena Balazinska;Dan Grossman
#t 2010
#c 5
#% 227883
#% 393844
#% 765467
#% 765468
#% 800589
#% 810055
#% 810056
#% 960334
#% 963669
#% 983467
#% 1063553
#% 1206984
#% 1468423
#% 1688297
#! Time-oriented progress estimation for parallel queries is a challenging problem that has received only limited attention. In this paper, we present ParaTimer, a new type of time-remaining indicator for parallel queries. Several parallel data processing systems exist. ParaTimer targets environments where declarative queries are translated into ensembles of MapReduce jobs. ParaTimer builds on previous techniques and makes two key contributions. First, it estimates the progress of queries that translate into directed acyclic graphs of MapReduce jobs, where jobs on different paths can execute concurrently (unlike prior work that looked at sequences only). For such queries, we use a new type of critical-path-based progress-estimation approach. Second, ParaTimer handles a variety of real systems challenges such as failures and data skew. To handle unexpected changes in query execution times due to runtime condition changes, ParaTimer provides users with not only one but with a set of time-remaining estimates, each one corresponding to a different carefully selected scenario. We implement our estimator in the Pig system and demonstrate its performance on experiments running on a real, small-scale cluster.

#index 1426545
#* The DataPath system: a data-centric analytic processing engine for large data warehouses
#@ Subi Arumugam;Alin Dobra;Christopher M. Jermaine;Niketan Pansare;Luis Perez
#t 2010
#c 5
#% 13029
#% 36117
#% 54047
#% 136740
#% 172931
#% 300167
#% 300179
#% 442850
#% 480821
#% 481621
#% 726621
#% 810039
#% 873341
#% 983261
#% 993508
#% 1016186
#% 1022262
#% 1328132
#% 1328168
#! Since the 1970's, database systems have been "compute-centric". When a computation needs the data, it requests the data, and the data are pulled through the system. We believe that this is problematic for two reasons. First, requests for data naturally incur high latency as the data are pulled through the memory hierarchy, and second, it makes it difficult or impossible for multiple queries or operations that are interested in the same data to amortize the bandwidth and latency costs associated with their data access. In this paper, we describe a purely-push based, research prototype database system called DataPath. DataPath is "data-centric". In DataPath, queries do not request data. Instead, data are automatically pushed onto processors, where they are then processed by any interested computation. We show experimentally on a multi-terabyte benchmark that this basic design principle makes for a very lean and fast database system.

#index 1426546
#* Variance aware optimization of parameterized queries
#@ Surajit Chaudhuri;Hongrae Lee;Vivek R. Narasayya
#t 2010
#c 5
#% 152585
#% 273694
#% 378414
#% 465167
#% 571088
#% 810017
#% 824756
#% 993946
#% 1015315
#% 1026989
#% 1127399
#% 1127564
#% 1177865
#% 1206624
#% 1217227
#% 1328168
#! Parameterized queries are commonly used in database applications. In a parameterized query, the same SQL statement is potentially executed multiple times with different parameter values. In today's DBMSs the query optimizer typically chooses a single execution plan that is reused for multiple instances of the same query. A key problem is that even if a plan with low average cost across instances is chosen, its variance can be high, which is undesirable in many production settings. In this paper, we describe techniques for selecting a plan that can better address the trade-off between the average and variance of cost across instances of a parameterized query. We show how to efficiently compute the skyline in the average-variance cost space. We have implemented our techniques on top of a commercial DBMS. We present experimental results on benchmark and real-world decision support queries.

#index 1426547
#* Positional update handling in column stores
#@ Sándor Héman;Marcin Zukowski;Niels J. Nes;Lefteris Sidirourgos;Peter Boncz
#t 2010
#c 5
#% 114582
#% 201869
#% 208047
#% 286258
#% 287672
#% 333949
#% 465169
#% 479470
#% 479630
#% 654495
#% 726619
#% 824697
#% 982559
#% 985976
#% 993443
#% 1063524
#% 1206647
#% 1217210
#! In this paper we investigate techniques that allow for on-line updates to columnar databases, leaving intact their high read-only performance. Rather than keeping differential structures organized by the table key values, the core proposition of this paper is that this can better be done by keeping track of the tuple position of the modifications. Not only does this minimize the computational overhead of merging in differences into read-only queries, but this makes the differential structure oblivious of the value of the order keys, allowing it to avoid disk I/O for retrieving the order keys in read-only queries that otherwise do not need them - a crucial advantage for a column-store. We describe a new data structure for maintaining such positional updates, called the Positional Delta Tree (PDT), and describe detailed algorithms for PDT/column merging, updating PDTs, and for using PDTs in transaction management. In experiments with a columnar DBMS, we perform microbenchmarks on PDTs, and show in a TPC-H workload that PDTs allow quick on-line updates, yet significantly reduce their performance impact on read-only queries compared with classical value-based differential methods.

#index 1426548
#* Durable top-k search in document archives
#@ Leong Hou U;Nikos Mamoulis;Klaus Berberich;Srikanta Bedathur
#t 2010
#c 5
#% 169781
#% 218982
#% 262096
#% 427199
#% 643566
#% 654447
#% 730070
#% 805848
#% 879611
#% 907505
#% 956535
#% 967452
#% 983263
#% 987257
#% 1022269
#% 1022338
#% 1058620
#% 1127422
#% 1202765
#% 1292507
#% 1392437
#% 1688264
#! We propose and study a new ranking problem in versioned databases. Consider a database of versioned objects which have different valid instances along a history (e.g., documents in a web archive). Durable top-k search finds the set of objects that are consistently in the top-k results of a query (e.g., a keyword query) throughout a given time interval (e.g., from June 2008 to May 2009). Existing work on temporal top-k queries mainly focuses on finding the most representative top-k elements within a time interval. Such methods are not readily applicable to durable top-k queries. To address this need, we propose two techniques that compute the durable top-k result. The first is adapted from the classic top-k rank aggregation algorithm NRA. The second technique is based on a shared execution paradigm and is more efficient than the first approach. In addition, we propose a special indexing technique for archived data. The index, coupled with a space partitioning technique, improves performance even further. We use data from Wikipedia and the Internet Archive to demonstrate the efficiency and effectiveness of our solutions.

#index 1426549
#* Ajax-based report pages as incrementally rendered views
#@ Yupeng Fu;Keith Kowalczykowski;Kian Win Ong;Yannis Papakonstantinou;Kevin Keliang Zhao
#t 2010
#c 5
#% 13016
#% 83541
#% 152928
#% 201928
#% 201929
#% 210208
#% 210210
#% 227869
#% 227947
#% 300141
#% 309729
#% 462213
#% 479629
#% 571038
#% 733137
#% 810045
#% 864419
#% 874692
#% 956528
#% 993978
#% 1190156
#% 1228017
#! While Ajax-based programming enables faster performance and higher interface quality over pure server-side programming, it is demanding and error prone as each action that partially updates the page requires custom, ad-hoc code. The problem is exacerbated by distributed programming between the browser and server, where the developer uses JavaScript to access the page state and Java/SQL for the database. The FORWARD framework simplifies the development of Ajax pages by treating them as rendered views, where the developer declares a view using an extension of SQL and page units, which map to the view and render the data in the browser. Such a declarative approach leads to significantly less code, as the framework automatically solves performance optimization problems that the developer would otherwise hand-code. Since pages are fueled by views, FORWARD leverages years of database research on incremental view maintenance by creating optimization techniques appropriately extended for the needs of pages (nesting, variability, ordering), thereby achieving performance comparable to hand-coded JavaScript/Java applications.

#index 1426550
#* An evaluation of alternative architectures for transaction processing in the cloud
#@ Donald Kossmann;Tim Kraska;Simon Loesing
#t 2010
#c 5
#% 121
#% 152902
#% 152904
#% 227875
#% 307360
#% 392578
#% 480597
#% 793894
#% 818380
#% 994015
#% 1063488
#% 1111848
#% 1207189
#% 1215805
#% 1217159
#% 1247791
#% 1328130
#! Cloud computing promises a number of advantages for the deployment of data-intensive applications. One important promise is reduced cost with a pay-as-you-go business model. Another promise is (virtually) unlimited throughput by adding servers if the workload increases. This paper lists alternative architectures to effect cloud computing for database applications and reports on the results of a comprehensive evaluation of existing commercial cloud services that have adopted these architectures. The focus of this work is on transaction processing (i.e., read and update workloads), rather than analytics or OLAP workloads, which have recently gained a great deal of attention. The results are surprising in several ways. Most importantly, it seems that all major vendors have adopted a different architecture for their cloud services. As a result, the cost and performance of the services vary significantly depending on the workload.

#index 1426551
#* Indexing multi-dimensional data in a cloud system
#@ Jinbao Wang;Sai Wu;Hong Gao;Jianzhong Li;Beng Chin Ooi
#t 2010
#c 5
#% 312215
#% 340175
#% 340176
#% 382477
#% 505869
#% 723279
#% 723445
#% 772022
#% 772839
#% 824706
#% 874970
#% 960252
#% 960326
#% 963669
#% 978404
#% 978411
#% 998842
#% 998845
#% 1047894
#% 1063527
#% 1127559
#% 1127560
#% 1278124
#% 1313357
#% 1328186
#! Providing scalable database services is an essential requirement for extending many existing applications of the Cloud platform. Due to the diversity of applications, database services on the Cloud must support large-scale data analytical jobs and high concurrent OLTP queries. Most existing work focuses on some specific type of applications. To provide an integrated framework, we are designing a new system, epiC, as our solution to next-generation database systems. In epiC, indexes play an important role in improving overall performance. Different types of indexes are built to provide efficient query processing for different applications. In this paper, we propose RT-CAN, a multi-dimensional indexing scheme in epiC. RT-CAN integrates CAN [23] based routing protocol and the R-tree based indexing scheme to support efficient multi-dimensional query processing in a Cloud system. RT-CAN organizes storage and compute nodes into an overlay structure based on an extended CAN protocol. In our proposal, we make a simple assumption that each compute node uses an R-tree like indexing structure to index the data that are locally stored. We propose a query-conscious cost model that selects beneficial local R-tree nodes for publishing. By keeping the number of persistently connected nodes small and maintaining a global multi-dimensional search index, we can locate the compute nodes that may contain the answer with a few hops, making the scheme scalable in terms of data volume and number of compute nodes. Experiments on Amazon's EC2 show that our proposed routing protocol and indexing scheme are robust, efficient and scalable.

#index 1426552
#* Low overhead concurrency control for partitioned main memory databases
#@ Evan P.C. Jones;Daniel J. Abadi;Samuel Madden
#t 2010
#c 5
#% 1327
#% 4619
#% 25017
#% 27057
#% 54583
#% 201871
#% 227958
#% 442698
#% 442700
#% 442832
#% 571084
#% 708321
#% 733626
#% 765176
#% 765431
#% 983490
#% 993515
#% 1011847
#% 1014224
#% 1022298
#% 1063543
#% 1063554
#% 1523878
#! Database partitioning is a technique for improving the performance of distributed OLTP databases, since "single partition" transactions that access data on one partition do not need coordination with other partitions. For workloads that are amenable to partitioning, some argue that transactions should be executed serially on each partition without any concurrency at all. This strategy makes sense for a main memory database where there are no disk or user stalls, since the CPU can be fully utilized and the overhead of traditional concurrency control, such as two-phase locking, can be avoided. Unfortunately, many OLTP applications have some transactions which access multiple partitions. This introduces network stalls in order to coordinate distributed transactions, which will limit the performance of a database that does not allow concurrency. In this paper, we compare two low overhead concurrency control schemes that allow partitions to work on other transactions during network stalls, yet have little cost in the common case when concurrency is not needed. The first is a light-weight locking scheme, and the second is an even lighter-weight type of speculative concurrency control that avoids the overhead of tracking reads and writes, but sometimes performs work that eventually must be undone. We quantify the range of workloads over which each technique is beneficial, showing that speculative concurrency control generally outperforms locking as long as there are few aborts or few distributed transactions that involve multiple rounds of communication. On a modified TPC-C benchmark, speculative concurrency control can improve throughput relative to the other schemes by up to a factor of two.

#index 1426553
#* Efficient querying and maintenance of network provenance at internet-scale
#@ Wenchao Zhou;Micah Sherr;Tao Tao;Xiaozhou Li;Boon Thau Loo;Yun Mao
#t 2010
#c 5
#% 121397
#% 310905
#% 340175
#% 464724
#% 821939
#% 835186
#% 874978
#% 889644
#% 938093
#% 963514
#% 976987
#% 1017497
#% 1022258
#% 1072073
#% 1206861
#% 1206880
#! Network accountability, forensic analysis, and failure diagnosis are becoming increasingly important for network management and security. Such capabilities often utilize network provenance - the ability to issue queries over network meta-data. For example, network provenance may be used to trace the path a message traverses on the network as well as to determine how message data were derived and which parties were involved in its derivation. This paper presents the design and implementation of ExSPAN, a generic and extensible framework that achieves efficient network provenance in a distributed environment. We utilize the database notion of data provenance to "explain" the existence of any network state, providing a versatile mechanism for network provenance. To achieve such flexibility at Internet-scale, ExSPAN uses declarative networking in which network protocols can be modeled as continuous queries over distributed streams and specified concisely in a declarative query language. We extend existing data models for provenance developed in database literature to enable distribution at Internet-scale, and investigate numerous optimization techniques to maintain and query distributed network provenance efficiently. The ExSPAN prototype is developed using RapidNet, a declarative networking platform based on the emerging ns-3 toolkit. Experiments over a simulated network and an actual deployment in a testbed environment demonstrate that our system supports a wide range of distributed provenance computations efficiently, resulting in significant reductions in bandwidth costs compared to traditional approaches.

#index 1426554
#* Hierarchically organized skew-tolerant histograms for geographic data objects
#@ Yohan J. Roh;Jae Ho Kim;Yon Dohn Chung;Jin Hyun Son;Myoung Ho Kim
#t 2010
#c 5
#% 43163
#% 116084
#% 152902
#% 153260
#% 211087
#% 248822
#% 259995
#% 273887
#% 273903
#% 300193
#% 333947
#% 397385
#% 397386
#% 399762
#% 411355
#% 421050
#% 479816
#% 481775
#% 482092
#% 803119
#% 806212
#% 864426
#% 864451
#% 945911
#% 1015256
#% 1016154
#% 1019108
#! Histograms have been widely used for fast estimation of query result sizes in query optimization. In this paper, we propose a new histogram method, called the Skew-Tolerant Histogram (STHistogram) for two or three dimensional geographic data objects that are used in many real-world applications in practice. The proposed method provides a significantly enhanced accuracy in a robust manner even for the data set that has a highly skewed distribution. Our method detects hotspots present in various parts of a data set and exploits them in organizing histogram buckets. For this purpose, we first define the concept of a hotspot, and provide an algorithm that efficiently extracts hotspots from the given data set. Then, we present our histogram construction method that utilizes hotspot information. We also describe how to estimate query result sizes by using the proposed histogram. We show through extensive performance experiments that the proposed method provides better performance than other existing methods.

#index 1426555
#* Logging every footstep: quantile summaries for the entire history
#@ Yufei Tao;Ke Yi;Cheng Sheng;Jian Pei;Feifei Li
#t 2010
#c 5
#% 56081
#% 58371
#% 248820
#% 273907
#% 287070
#% 333931
#% 453493
#% 479473
#% 480817
#% 482104
#% 571296
#% 763997
#% 800494
#% 801696
#% 816392
#% 993969
#% 1054486
#% 1373450
#! Quantiles are a crucial type of order statistics in databases. Extensive research has been focused on maintaining a space-efficient structure for approximate quantile computation as the underlying dataset is updated. The existing solutions, however, are designed to support only the current, most-updated, snapshot of the dataset. Queries on the past versions of the data cannot be answered. This paper studies the problem of historical quantile search. The objective is to enable ε-approximate quantile retrieval on any snapshot of the dataset in history. The problem is very important in analyzing the evolution of a distribution, monitoring the quality of services, query optimization in temporal databases, and so on. We present the first formal results in the literature. First, we prove a novel theoretical lower bound on the space cost of supporting ε-approximate historical quantile queries. The bound reveals the fundamental difference between answering quantile queries about the past and those about the present time. Second, we propose a structure for finding ε-approximate historical quantiles, and show that it consumes more space than the lower bound by only a square-logarithmic factor. Extensive experiments demonstrate that in practice our technique performs much better than predicted by theory. In particular, the quantiles it returns are remarkably more accurate than the theoretical precision guarantee.

#index 1426556
#* Continuous sampling for online aggregation over multiple queries
#@ Sai Wu;Beng Chin Ooi;Kian-Lee Tan
#t 2010
#c 5
#% 124011
#% 227883
#% 273910
#% 273917
#% 300166
#% 300195
#% 333962
#% 397370
#% 465162
#% 503719
#% 617881
#% 654486
#% 765426
#% 810055
#% 1198372
#% 1328132
#% 1328146
#! In this paper, we propose an online aggregation system called COSMOS (Continuous Sampling for Multiple queries in an Online aggregation System), to process multiple aggregate queries efficiently. In COSMOS, a dataset is first scrambled so that sequentially scanning the dataset gives rise to a stream of random samples for all queries. Moreover, COSMOS organizes queries into a dissemination graph to exploit the dependencies across queries. In this way, aggregates of queries closer to the root (source of data flow) can potentially be used to compute the aggregates of descendent/dependent queries. COSMOS applies some statistical approach to combine answers from ancestor nodes to generate the online aggregates for a node. COSMOS also offers a partitioning strategy to further salvage intermediate answers. We have implemented COSMOS and conducted an extensive experimental study in PostgreSQL. Our results on the TPC-H benchmark show the efficiency and effectiveness of COSMOS.

#index 1426557
#* Histograms reloaded: the merits of bucket diversity
#@ Carl-Christian Kanne;Guido Moerkotte
#t 2010
#c 5
#% 70370
#% 152585
#% 201921
#% 210190
#% 299989
#% 427219
#% 480805
#% 481266
#% 571094
#% 689389
#% 1328191
#! Virtually all histograms store for each bucket the number of distinct values it contains and their average frequency. In this paper, we question this paradigm. We start out by investigating the estimation precision of three commercial database systems which also follow the above paradigm. It turns out that huge errors are quite common. We then introduce new bucket types and investigate their accuracy when building optimal histograms with them. The results are ambiguous. There is no clear winner among the bucket types. At this point, we (1) switch to heterogeneous histograms, where different buckets of the same histogram possibly are of different types, and (2) design more bucket types. The nice consequence of introducing heterogeneous histograms is that we can guarantee decent upper error bounds while at the same time heterogeneous histograms require far less space than homogeneous histograms.

#index 1426558
#* Lineage processing over correlated probabilistic databases
#@ Bhargav Kanagal;Amol Deshpande
#t 2010
#c 5
#% 44876
#% 336306
#% 388024
#% 410276
#% 654487
#% 893168
#% 1016201
#% 1022259
#% 1063523
#% 1127414
#% 1127415
#% 1206717
#% 1206732
#% 1206772
#% 1206877
#% 1207010
#% 1217114
#% 1217154
#% 1217181
#% 1250332
#% 1279377
#! In this paper, we address the problem of scalably evaluating conjunctive queries over correlated probabilistic databases containing tuple or attribute uncertainties. Like previous work, we adopt a two-phase approach where we first compute lineages of the output tuples, and then compute the probabilities of the lineage formulas. However unlike previous work, we allow for arbitrary and complex correlations to be present in the data, captured via a forest of junction trees. We observe that evaluating even read-once (tree structured) lineages (e.g., those generated by hierarchical conjunctive queries), polynomially computable over tuple independent probabilistic databases, is #P-complete for lightly correlated probabilistic databases like Markov sequences. We characterize the complexity of exact computation of the probability of the lineage formula on a correlated database using a parameter called lwidth (analogous to the notion of treewidth). For lineages that result in low lwidth, we compute exact probabilities using a novel message passing algorithm, and for lineages that induce large lwidths, we develop approximate Monte Carlo algorithms to estimate the result probabilities. We scale our algorithms to very large correlated probabilistic databases using the previously proposed INDSEP data structure. To mitigate the complexity of lineage evaluation, we develop optimization techniques to process a batch of lineages by sharing computation across formulas, and to exploit any independence relationships that may exist in the data. Our experimental study illustrates the benefits of using our algorithms for processing lineage formulas over correlated probabilistic databases.

#index 1426559
#* Evaluation of probabilistic threshold queries in MCDB
#@ Luis L. Perez;Subi Arumugam;Christopher M. Jermaine
#t 2010
#c 5
#% 577523
#% 632040
#% 874976
#% 893189
#% 1016201
#% 1063520
#% 1063521
#% 1063529
#% 1068580
#% 1200291
#! MCDB is a prototype database system for managing stochastic models for uncertain data. In this paper, we study the problem of how to use MCDB to answer statistical queries that search for database objects which satisfy some filter condition with greater (or less than) a user-specified probability. For example: "Which packages will arrive late with 5% probability?" "Which regions will see more than a 2% decline in sales with 50% probability?" "What items will be out of stock by Friday with 20% probability?" We consider both the systems aspects and the statistical aspects of the problem.

#index 1426560
#* K-nearest neighbor search for fuzzy objects
#@ Kai Zheng;Pui Cheong Fung;Xiaofang Zhou
#t 2010
#c 5
#% 2115
#% 142639
#% 201876
#% 219719
#% 248797
#% 287466
#% 300162
#% 300174
#% 316937
#% 348590
#% 464859
#% 479649
#% 480632
#% 481947
#% 482109
#% 495433
#% 527167
#% 536348
#% 654487
#% 824728
#% 875013
#% 915262
#% 993955
#% 1015297
#% 1015320
#% 1075368
#% 1206781
#! The K-Nearest Neighbor search (kNN) problem has been investigated extensively in the past due to its broad range of applications. In this paper we study this problem in the context of fuzzy objects that have indeterministic boundaries. Fuzzy objects play an important role in many areas, such as biomedical image databases and GIS. Existing research on fuzzy objects mainly focuses on modelling basic fuzzy object types and operations, leaving the processing of more advanced queries such as kNN query untouched. In this paper, we propose two new kinds of kNN queries for fuzzy objects, Ad-hoc kNN query (AKNN) and Range kNN query (RKNN), to find the k nearest objects qualifying at a probability threshold or within a probability range. For efficient AKNN query processing, we optimize the basic best-first search algorithm by deriving more accurate approximations for the distance function between fuzzy objects and the query object. To improve the performance of RKNN search, effective pruning rules are developed to significantly reduce the search space and further speed up the candidate refinement process. The efficiency of our proposed algorithms as well as the optimization techniques are verified with an extensive set of experiments using both synthetic and real datasets.

#index 1426561
#* An optimal labeling scheme for workflow provenance using skeleton labels
#@ Zhuowei Bao;Susan B. Davidson;Sanjeev Khanna;Sudeepa Roy
#t 2010
#c 5
#% 23614
#% 58365
#% 88051
#% 325384
#% 336509
#% 379482
#% 379483
#% 379484
#% 824692
#% 864462
#% 960304
#% 1063514
#% 1063544
#% 1063545
#% 1174012
#% 1217208
#! We develop a compact and efficient reachability labeling scheme for answering provenance queries on workflow runs that conform to a given specification. Even though a workflow run can be structurally more complex and can be arbitrarily larger than the specification due to fork (parallel) and loop executions, we show that a compact reachability labeling for a run can be efficiently computed using the fact that it originates from a fixed specification. Our labeling scheme is optimal in the sense that it uses labels of logarithmic length, runs in linear time, and answers any reachability query in constant time. Our approach is based on using the reachability labeling for the specification as an effective skeleton for designing the reachability labeling for workflow runs. We also demonstrate empirically the effectiveness of our skeleton-based labeling approach.

#index 1426562
#* SecureBlox: customizable secure distributed data processing
#@ William R. Marczak;Shan Shan Huang;Martin Bravenboer;Micah Sherr;Boon Thau Loo;Molham Aref
#t 2010
#c 5
#% 33190
#% 124016
#% 152928
#% 429214
#% 616956
#% 664539
#% 664709
#% 764971
#% 821939
#% 835186
#% 874978
#% 963801
#% 1015281
#% 1050850
#% 1063724
#% 1127442
#% 1206859
#% 1227859
#% 1287572
#% 1328190
#% 1386046
#% 1468226
#! We present SecureBlox, a declarative system that unifies a distributed query processor with a security policy framework. SecureBlox decouples security concerns from system specification, allowing easy reconfiguration of a system's security properties to suit a given execution environment. Our implementation of SecureBlox is a series of extensions to LogicBlox, an emerging commercial Datalog-based platform for enterprise software systems. SecureBlox enhances LogicBlox to enable distribution and static meta-programmability, and makes novel use of existing LogicBlox features such as integrity constraints. SecureBlox allows meta-programmability via BloxGenerics - a language extension for compile-time code generation based on the security requirements and trust policies of the deployed environment. We present and evaluate detailed use-cases in which SecureBlox enables diverse applications, including an authenticated declarative routing protocol with encrypted advertisements and an authenticated and encrypted parallel hash join operation. Our results demonstrate SecureBlox's abilities to specify and implement a wide range of different security constructs for distributed systems as well as to enable tradeoffs between performance and security.

#index 1426563
#* Differentially private aggregation of distributed time-series with transformation and encryption
#@ Vibhor Rastogi;Suman Nath
#t 2010
#c 5
#% 319849
#% 460862
#% 536484
#% 576111
#% 576761
#% 809245
#% 864412
#% 906944
#% 964080
#% 977011
#% 1019699
#% 1022240
#% 1022246
#% 1068533
#% 1132733
#% 1217125
#% 1225223
#% 1414540
#% 1732708
#% 1740518
#! We propose the first differentially private aggregation algorithm for distributed time-series data that offers good practical utility without any trusted server. This addresses two important challenges in participatory data-mining applications where (i) individual users collect temporally correlated time-series data (such as location traces, web history, personal health data), and (ii) an untrusted third-party aggregator wishes to run aggregate queries on the data. To ensure differential privacy for time-series data despite the presence of temporal correlation, we propose the Fourier Perturbation Algorithm (FPAk). Standard differential privacy techniques perform poorly for time-series data. To answer n queries, such techniques can result in a noise of Θ(n) to each query answer, making the answers practically useless if n is large. Our FPAk algorithm perturbs the Discrete Fourier Transform of the query answers. For answering n queries, FPAk improves the expected error from Θ(n) to roughly Θ(k) where k is the number of Fourier coefficients that can (approximately) reconstruct all the n query answers. Our experiments show that k n for many real-life data-sets resulting in a huge error-improvement for FPAk. To deal with the absence of a trusted central server, we propose the Distributed Laplace Perturbation Algorithm (DLPA) to add noise in a distributed way in order to guarantee differential privacy. To the best of our knowledge, DLPA is the first distributed differentially private algorithm that can scale with a large number of users: DLPA outperforms the only other distributed solution for differential privacy proposed so far, by reducing the computational load per user from O(U) to O(1) where U is the number of users.

#index 1426564
#* Non-homogeneous generalization in privacy preserving data publishing
#@ Wai Kit Wong;Nikos Mamoulis;David Wai Lok Cheung
#t 2010
#c 5
#% 300184
#% 443463
#% 576761
#% 577233
#% 800515
#% 801690
#% 810010
#% 810011
#% 864406
#% 864412
#% 881546
#% 881551
#% 893100
#% 893151
#% 904304
#% 960291
#% 1013611
#% 1022247
#% 1022264
#% 1022265
#% 1206582
#% 1206584
#% 1217156
#! Most previous research on privacy-preserving data publishing, based on the k-anonymity model, has followed the simplistic approach of homogeneously giving the same generalized value in all quasi-identifiers within a partition. We observe that the anonymization error can be reduced if we follow a non-homogeneous generalization approach for groups of size larger than k. Such an approach would allow tuples within a partition to take different generalized quasi-identifier values. Anonymization following this model is not trivial, as its direct application can easily violate k-anonymity. In addition, non-homogeneous generalization allows for additional types of attack, which should be considered in the process. We provide a methodology for verifying whether a non-homogeneous generalization violates k-anonymity. Then, we propose a technique that generates a non-homogeneous generalization for a partition and show that its result satisfies k-anonymity, however by straightforwardly applying it, privacy can be compromised if the attacker knows the anonymization algorithm. Based on this, we propose a randomization method that prevents this type of attack and show that k-anonymity is not compromised by it. Nonhomogeneous generalization can be used on top of any existing partitioning approach to improve its utility. In addition, we show that a new partitioning technique tailored for non-homogeneous generalization can further improve quality. A thorough experimental evaluation demonstrates that our methodology greatly improves the utility of anonymized data in practice.

#index 1426565
#* Preserving privacy and fairness in peer-to-peer data integration
#@ Hazem Elmeleegy;Mourad Ouzzani;Ahmed Elmagarmid;Ahmad Abusalah
#t 2010
#c 5
#% 576110
#% 576761
#% 654448
#% 654468
#% 743280
#% 765446
#% 801690
#% 809245
#% 810011
#% 824769
#% 1015329
#% 1016168
#% 1016250
#% 1400778
#! Peer-to-peer data integration - a.k.a. Peer Data Management Systems (PDMSs) - promises to extend the classical data integration approach to the Internet scale. Unfortunately, some challenges remain before realizing this promise. One of the biggest challenges is preserving the privacy of the exchanged data while passing through several intermediate peers. Another challenge is protecting the mappings used for data translation. Protecting the privacy without being unfair to any of the peers is yet a third challenge. This paper presents a novel query answering protocol in PDMSs to address these challenges. The protocol employs a technique based on noise selection and insertion to protect the query results, and a commutative encryption-based technique to protect the mappings and ensure fairness among peers. An extensive security analysis of the protocol shows that it is resilient to several possible types of attacks. We implemented the protocol within an established PDMS: the Hyperion system. We conducted an experimental study using real data from the healthcare domain. The results show that our protocol manages to achieve its privacy and fairness goals, while maintaining query processing time at the interactive level.

#index 1426566
#* Structured annotations of web queries
#@ Nikos Sarkas;Stelios Paparizos;Panayiotis Tsaparas
#t 2010
#c 5
#% 198058
#% 282232
#% 654442
#% 789961
#% 824693
#% 875017
#% 875061
#% 891559
#% 903014
#% 948374
#% 960259
#% 1015256
#% 1015325
#% 1127393
#% 1127423
#% 1127424
#% 1190070
#% 1217235
#% 1217265
#% 1227648

#index 1426567
#* On active learning of record matching packages
#@ Arvind Arasu;Michaela Götz;Raghav Kaushik
#t 2010
#c 5
#% 136350
#% 170649
#% 201889
#% 236729
#% 249143
#% 310516
#% 314740
#% 314829
#% 342611
#% 350103
#% 376266
#% 387427
#% 420077
#% 480654
#% 577238
#% 579314
#% 722797
#% 729913
#% 765463
#% 810014
#% 823371
#% 844289
#% 864392
#% 893164
#% 913783
#% 915242
#% 960263
#% 983848
#% 991214
#% 993980
#% 1022227
#% 1022229
#% 1206677
#% 1301004
#! We consider the problem of learning a record matching package (classifier) in an active learning setting. In active learning, the learning algorithm picks the set of examples to be labeled, unlike more traditional passive learning setting where a user selects the labeled examples. Active learning is important for record matching since manually identifying a suitable set of labeled examples is difficult. Previous algorithms that use active learning for record matching have serious limitations: The packages that they learn lack quality guarantees and the algorithms do not scale to large input sizes. We present new algorithms for this problem that overcome these limitations. Our algorithms are fundamentally different from traditional active learning approaches, and are designed ground up to exploit problem characteristics specific to record matching. We include a detailed experimental evaluation on realworld data demonstrating the effectiveness of our algorithms.

#index 1426568
#* I4E: interactive investigation of iterative information extraction
#@ Anish Das Sarma;Alpa Jain;Divesh Srivastava
#t 2010
#c 5
#% 283136
#% 283180
#% 301241
#% 410276
#% 462072
#% 504443
#% 577523
#% 754068
#% 756964
#% 810115
#% 864416
#% 875015
#% 893167
#% 893168
#% 939515
#% 939602
#% 976987
#% 997488
#% 1127409
#% 1127414
#% 1206701
#% 1206799
#% 1206986
#% 1250378
#% 1289516
#% 1291356
#! Information extraction systems are increasingly being used to mine structured information from unstructured text documents. A commonly used unsupervised technique is to build iterative information extraction (IIE) systems that learn task-specific rules, called patterns, to generate the desired tuples. Oftentimes, output from an information extraction system may contain unexpected results which may be due to an incorrect pattern, incorrect tuple, or both. In such scenarios, users and developers of the extraction system could greatly benefit from an investigation tool that can quickly help them reason about and repair the output. In this paper, we develop an approach for interactive post-extraction investigation for IIE systems. We formalize three important phases of this investigation, namely, explain the IIE result, diagnose the influential and problematic components, and repair the output from an information extraction system. We show how to characterize the execution of an IIE system and build a suite of algorithms to answer questions pertaining to each of these phases. We experimentally evaluate our proposed approach over several domains over a Web corpus of about 500 million documents. We show that our approach effectively enables post-extraction investigation, while maximizing the gain from user and developer interaction.

#index 1426569
#* ONDUX: on-demand unsupervised learning for information extraction
#@ Eli Cortez;Altigran S. da Silva;Marcos André Gonçalves;Edleno S. de Moura
#t 2010
#c 5
#% 44876
#% 333943
#% 464434
#% 466263
#% 531459
#% 769877
#% 864416
#% 864432
#% 874707
#% 948374
#% 967276
#% 1022260
#% 1166537
#% 1202077
#% 1272286
#! Information extraction by text segmentation (IETS) applies to cases in which data values of interest are organized in implicit semi-structured records available in textual sources (e.g. postal addresses, bibliographic information, ads). It is an important practical problem that has been frequently addressed in the recent literature. In this paper we introduce ONDUX (On Demand Unsupervised Information Extraction), a new unsupervised probabilistic approach for IETS. As other unsupervised IETS approaches, ONDUX relies on information available on pre-existing data to associate segments in the input string with attributes of a given domain. Unlike other approaches, we rely on very effective matching strategies instead of explicit learning strategies. The effectiveness of this matching strategy is also exploited to disambiguate the extraction of certain attributes through a reinforcement step that explores sequencing and positioning of attribute values directly learned on-demand from test data, with no previous human-driven training, a feature unique to ONDUX. This assigns to ONDUX a high degree of flexibility and results in superior effectiveness, as demonstrated by the experimental evaluation we report with textual sources from different domains, in which ONDUX is compared with a state-of-art IETS approach.

#index 1426570
#* Optimizing content freshness of relations extracted from the web using keyword search
#@ Mohan Yang;Haixun Wang;Lipyeow Lim;Min Wang
#t 2010
#c 5
#% 300139
#% 330682
#% 341672
#% 397355
#% 480479
#% 809418
#% 864434
#% 956538
#% 982751
#% 995020
#% 1127557
#% 1140431
#% 1155648
#! An increasing number of applications operate on data obtained from the Web. These applications typically maintain local copies of the web data to avoid network latency in data accesses. As the data on the Web evolves, it is critical that the local copy be kept up-to-date. Data freshness is one of the most important data quality issues, and has been extensively studied for various applications including web crawling. However, web crawling is focused on obtaining as many raw web pages as possible. Our applications, on the other hand, are interested in specific content from specific data sources. Knowing the content or the semantics of the data enables us to differentiate data items based on their importance and volatility, which are key factors that impact the design of the data synchronization strategy. In this work, we formulate the concept of content freshness, and present a novel approach that maintains content freshness with least amount of web communication. Specifically, we assume data is accessible through a general keyword search interface, and we form keyword queries based on their selectivity, as well their contribution to content freshness of the local copy. Experiments show the effectiveness of our approach compared with several naive methods for keeping data fresh.

#index 1426571
#* Feeding frenzy: selectively materializing users' event feeds
#@ Adam Silberstein;Jeff Terrace;Brian F. Cooper;Raghu Ramakrishnan
#t 2010
#c 5
#% 64791
#% 86465
#% 116086
#% 201928
#% 227947
#% 273910
#% 300141
#% 463917
#% 464706
#% 480158
#% 480623
#% 572311
#% 654488
#% 800502
#% 806214
#% 857498
#% 1022206
#% 1022221
#% 1061893
#% 1063490
#% 1127385
#% 1127394
#% 1127560
#% 1217160
#% 1328113
#! Near real-time event streams are becoming a key feature of many popular web applications. Many web sites allow users to create a personalized feed by selecting one or more event streams they wish to follow. Examples include Twitter and Facebook, which a user to follow other users' activity, and iGoogle and My Yahoo, which allow users to follow selected RSS streams. How can we efficiently construct a web page showing the latest events from a user's feed? Constructing such a feed must be fast so the page loads quickly, yet reflects recent updates to the underlying event streams. The wide fanout of popular streams (those with many followers) and high skew (fanout and update rates vary widely) make it difficult to scale such applications. We associate feeds with consumers and event streams with producers. We demonstrate that the best performance results from selectively materializing each consumer's feed: events from high-rate producers are retrieved at query time, while events from lower-rate producers are materialized in advance. A formal analysis of the problem shows the surprising result that we can minimize global cost by making local decisions about each producer/consumer pair, based on the ratio between a given producer's update rate (how often an event is added to the stream) and a given consumer's view rate (how often the feed is viewed). Our experimental results, using Yahoo!'s web-scale database PNUTS, shows that this hybrid strategy results in the lowest system load (and hence improves scalability) under a variety of workloads.

#index 1426572
#* Constructing and exploring composite items
#@ Senjuti Basu Roy;Sihem Amer-Yahia;Ashish Chawla;Gautam Das;Cong Yu
#t 2010
#c 5
#% 190611
#% 248791
#% 333854
#% 408396
#% 464714
#% 481290
#% 643566
#% 832571
#% 893115
#% 1206662
#% 1206698
#% 1328162
#! Nowadays, online shopping has become a daily activity. Web users purchase a variety of items ranging from books to electronics. The large supply of online products calls for sophisticated techniques to help users explore available items. We propose to build composite items which associate a central item with a set of packages, formed by satellite items, and help users explore them. For example, a user shopping for an iPhone (i.e., the central item) with a price budget can be presented with both the iPhone and a package of other items that match well with the iPhone (e.g., {Belkin case, Bose sounddock, Kroo USB cable}) as a composite item, whose total price is within the user's budget. We define and study the problem of effective construction and exploration of large sets of packages associated with a central item, and design and implement efficient algorithms for solving the problem in two stages: summarization, a technique which picks k representative packages for each central item; and visual effect optimization, which helps the user find diverse composite items quickly by minimizing overlap between packages presented to the user in a ranked order. We conduct an extensive set of experiments on Yahoo! Shopping1 data sets to demonstrate the efficiency and effectiveness of our algorithms.

#index 1426573
#* Unbiased estimation of size and other aggregates over hidden web databases
#@ Arjun Dasgupta;Xin Jin;Bradley Jewell;Nan Zhang;Gautam Das
#t 2010
#c 5
#% 8202
#% 172913
#% 268114
#% 340146
#% 397378
#% 413635
#% 480479
#% 654486
#% 783692
#% 809418
#% 879604
#% 907547
#% 943875
#% 956534
#% 960286
#% 993964
#% 1091268
#% 1127356
#% 1131022
#% 1206906
#% 1217158
#% 1423051
#! Many websites provide restrictive form-like interfaces which allow users to execute search queries on the underlying hidden databases. In this paper, we consider the problem of estimating the size of a hidden database through its web interface. We propose novel techniques which use a small number of queries to produce unbiased estimates with small variance. These techniques can also be used for approximate query processing over hidden databases. We present theoretical analysis and extensive experiments to illustrate the effectiveness of our approach.

#index 1426574
#* Towards proximity pattern mining in large graphs
#@ Arijit Khan;Xifeng Yan;Kun-Lung Wu
#t 2010
#c 5
#% 152934
#% 300120
#% 443350
#% 452821
#% 466644
#% 481290
#% 502147
#% 629603
#% 629646
#% 629708
#% 742493
#% 765429
#% 769940
#% 769951
#% 813990
#% 881466
#% 937794
#% 960305
#% 1022280
#% 1063502
#% 1117010
#% 1214624
#% 1214633
#% 1411112
#! Mining graph patterns in large networks is critical to a variety of applications such as malware detection and biological module discovery. However, frequent subgraphs are often ineffective to capture association existing in these applications, due to the complexity of isomorphism testing and the inelastic pattern definition. In this paper, we introduce proximity pattern which is a significant departure from the traditional concept of frequent subgraphs. Defined as a set of labels that co-occur in neighborhoods, proximity pattern blurs the boundary between itemset and structure. It relaxes the rigid structure constraint of frequent subgraphs, while introducing connectivity to frequent itemsets. Therefore, it can benefit from both: efficient mining in itemsets and structure proximity from graphs. We developed two models to define proximity patterns. The second one, called Normalized Probabilistic Association (NmPA), is able to transform a complex graph mining problem to a simplified probabilistic itemset mining problem, which can be solved eficiently by a modified FP-tree algorithm, called pFP. NmPA and pFP are evaluated on real-life social and intrusion networks. Empirical results show that it not only finds interesting patterns that are ignored by the existing approaches, but also achieves high performance for finding proximity patterns in large-scale graphs.

#index 1426575
#* GAIA: graph classification using evolutionary computation
#@ Ning Jin;Calvin Young;Wei Wang
#t 2010
#c 5
#% 288990
#% 466644
#% 629708
#% 727845
#% 769940
#% 813990
#% 1063502
#% 1083688
#% 1207028
#% 1227871
#% 1292523
#! Discriminative subgraphs are widely used to define the feature space for graph classification in large graph databases. Several scalable approaches have been proposed to mine discriminative subgraphs. However, their intensive computation needs prevent them from mining large databases. We propose an efficient method GAIA for mining discriminative subgraphs for graph classification in large databases. Our method employs a novel subgraph encoding approach to support an arbitrary subgraph pattern exploration order and explores the subgraph pattern space in a process resembling biological evolution. In this manner, GAIA is able to find discriminative subgraph patterns much faster than other algorithms. Additionally, we take advantage of parallel computing to further improve the quality of resulting patterns. In the end, we employ sequential coverage to generate association rules as graph classifiers using patterns mined by GAIA. Extensive experiments have been performed to analyze the performance of GAIA and to compare it with two other state-of-the-art approaches. GAIA outperforms the other approaches both in terms of classification accuracy and runtime efficiency.

#index 1426576
#* Finding maximum degrees in hidden bipartite graphs
#@ Yufei Tao;Cheng Sheng;Jianzhong Li
#t 2010
#c 5
#% 256686
#% 261358
#% 303072
#% 333854
#% 410276
#% 415926
#% 460816
#% 480819
#% 737066
#% 743953
#% 796217
#% 800570
#% 806212
#% 875012
#% 993965
#% 1015317
#% 1019798
#% 1051766
#% 1053722
#% 1055737
#% 1063713
#% 1070886
#% 1075132
#% 1086059
#% 1164872
#% 1211644
#% 1379557
#! An (edge) hidden graph is a graph whose edges are not explicitly given. Detecting the presence of an edge requires expensive edge-probing queries. We consider the k most connected vertex problem on hidden bipartite graphs. Specifically, given a bipartite graph G with independent vertex sets B and W, the goal is to find the k vertices in B with the largest degrees using the minimum number of queries. This problem can be regarded as a top-k extension of a semi-join, and is encountered in many applications in practice (e.g., top-k spatial join with arbitrarily complex join predicates). If B and W have n and m vertices respectively, the number of queries needed to solve the problem is nm in the worst case. This, however, is a pessimistic estimate on how many queries are necessary on practical data. In fact, on some easy inputs, the problem can be efficiently settled with only km+ n edges, which is significantly lower than nm for k n. The huge difference between km + n and nm makes it interesting to design an adaptive algorithm that is guaranteed to achieve the best possible performance on every input G. We give such an algorithm, and prove that it is instance optimal among a broad class of solutions. This means that, for any G, our algorithm can perform more queries than the optimal solution (which is currently unknown) by only a constant factor, which can be shown to be at most 2. Extensive experiments demonstrate that, in practice, the number of queries required by our technique is far less than nm, and agrees with our theoretical findings very well.

#index 1426577
#* Connected substructure similarity search
#@ Haichuan Shang;Xuemin Lin;Ying Zhang;Jeffrey Xu Yu;Wei Wang
#t 2010
#c 5
#% 17293
#% 322619
#% 408396
#% 447719
#% 466644
#% 479554
#% 629708
#% 727845
#% 765429
#% 769951
#% 772183
#% 810072
#% 864425
#% 960305
#% 1022279
#% 1022280
#% 1044450
#% 1127380
#% 1328111
#% 1387594
#! Substructure similarity search is to retrieve graphs that approximately contain a given query graph. It has many applications, e.g., detecting similar functions among chemical compounds. The problem is challenging as even testing subgraph containment between two graphs is NP-complete. Hence, existing techniques adopt the filtering-and-verification framework with the focus on developing effective and efficient techniques to remove non-promising graphs. Nevertheless, existing filtering techniques may be still unable to effectively remove many "low" quality candidates. To resolve this, in this paper we propose a novel indexing technique, GrafD-Index, to index graphs according to their "distances" to features. We characterize a tight condition under which the distance-based triangular inequality holds. We then develop lower and upper bounding techniques that exploit the GrafD-Index to (1) prune non-promising graphs and (2) include graphs whose similarities are guaranteed to exceed the given similarity threshold. Considering that the verification phase is not well studied and plays the dominant role in the whole process, we devise efficient algorithms to verify candidates. A comprehensive experiment using real datasets demonstrates that our proposed methods significantly outperform existing methods.

#index 1426578
#* Bed-tree: an all-purpose index structure for string similarity search based on edit distance
#@ Zhenjie Zhang;Marios Hadjieleftheriou;Beng Chin Ooi;Divesh Srivastava
#t 2010
#c 5
#% 288885
#% 317933
#% 410276
#% 480654
#% 654467
#% 765463
#% 864392
#% 893164
#% 910177
#% 956506
#% 991786
#% 1127425
#% 1190092
#% 1198208
#% 1206665
#% 1206677
#% 1206821
#% 1206985
#% 1217179
#% 1217200
#! Strings are ubiquitous in computer systems and hence string processing has attracted extensive research effort from computer scientists in diverse areas. One of the most important problems in string processing is to efficiently evaluate the similarity between two strings based on a specified similarity measure. String similarity search is a fundamental problem in information retrieval, database cleaning, biological sequence analysis, and more. While a large number of dissimilarity measures on strings have been proposed, edit distance is the most popular choice in a wide spectrum of applications. Existing indexing techniques for similarity search queries based on edit distance, e.g., approximate selection and join queries, rely mostly on n-gram signatures coupled with inverted list structures. These techniques are tailored for specific query types only, and their performance remains unsatisfactory especially in scenarios with strict memory constraints or frequent data updates. In this paper we propose the Bed-tree, a B+-tree based index structure for evaluating all types of similarity queries on edit distance and normalized edit distance. We identify the necessary properties of a mapping from the string space to the integer space for supporting searching and pruning for these queries. Three transformations are proposed that capture different aspects of information inherent in strings, enabling efficient pruning during the search process on the tree. Compared to state-of-the-art methods on string similarity search, the Bed-tree is a complete solution that meets the requirements of all applications, providing high scalability and fast response time.

#index 1426579
#* On indexing error-tolerant set containment
#@ Parag Agrawal;Arvind Arasu;Raghav Kaushik
#t 2010
#c 5
#% 152934
#% 273920
#% 387427
#% 479973
#% 480463
#% 480654
#% 481290
#% 569755
#% 579314
#% 616528
#% 654454
#% 765463
#% 765529
#% 844321
#% 864392
#% 875066
#% 893164
#% 913783
#% 987275
#% 1166469
#% 1176957
#% 1190092
#% 1206615
#% 1206677
#% 1206821
#% 1217199
#% 1328106
#% 1328142
#% 1328152
#! Prior work has identified set based comparisons as a useful primitive for supporting a wide variety of similarity functions in record matching. Accordingly, various techniques have been proposed to improve the performance of set similarity lookups. However, this body of work focuses almost exclusively on symmetric notions of set similarity. In this paper, we study the indexing problem for the asymmetric Jaccard containment similarity function that is an error-tolerant variation of set containment. We enhance this similarity function to also account for string transformations that reflect synonyms such as "Bob" and "Robert" referring to the same first name. We propose an index structure that builds inverted lists on carefully chosen token-sets and a lookup algorithm using our index that is sensitive to the output size of the query. Our experiments over real life data sets show the benefits of our techniques. To our knowledge, this is the first paper that studies the indexing problem for Jaccard containment in the presence of string transformations.

#index 1426580
#* Workload-aware storage layout for database systems
#@ Oguzhan Ozmen;Kenneth Salem;Jiri Schindler;Steve Daniel
#t 2010
#c 5
#% 342867
#% 452807
#% 460864
#% 497549
#% 663027
#% 720215
#% 764036
#% 786156
#% 850308
#% 871528
#% 960265
#% 1063551
#% 1085296
#% 1164958
#% 1700130
#! The performance of a database system depends strongly on the layout of database objects, such as indexes or tables, onto the underlying storage devices. A good layout will both balance the I/O workload generated by the database system and avoid the performance-degrading interference that can occur when concurrently accessed objects are stored on the same volume. In current practice, layout is typically guided by heuristics and rules of thumb, such as separating indexes and tables or striping all objects across all of the available storage devices. However, these guidelines may give poor results. In this paper, we address the problem of generating an optimized layout of a given set of database objects. Our layout optimizer goes beyond generic guidelines by making use of a description of the database system's I/O activity. We formulate the layout problem as a non-linear programming (NLP) problem and use the I/O description as input to an NLP solver. Our layout optimization technique, which is incorporated into a database layout advisor, identifies a layout that both balances load and avoids interference. We evaluate experimentally the efficacy of our approach and demonstrate that it can quickly identify non-trivial optimized layouts.

#index 1426581
#* Querying data provenance
#@ Grigoris Karvounarakis;Zachary G. Ives;Val Tannen
#t 2010
#c 5
#% 64902
#% 86954
#% 248819
#% 348181
#% 479465
#% 480158
#% 480656
#% 572311
#% 715288
#% 810115
#% 826032
#% 864469
#% 874971
#% 875015
#% 893095
#% 893117
#% 893167
#% 976987
#% 993981
#% 1016176
#% 1016201
#% 1016204
#% 1022258
#% 1038848
#% 1063544
#% 1063736
#% 1080342
#% 1127413
#% 1206750
#% 1206861
#% 1328125
#% 1328155
#% 1450320
#% 1661440
#! Many advanced data management operations (e.g., incremental maintenance, trust assessment, debugging schema mappings, keyword search over databases, or query answering in probabilistic databases), involve computations that look at how a tuple was produced, e.g., to determine its score or existence. This requires answers to queries such as, "Is this data derivable from trusted tuples?"; "What tuples are derived from this relation?"; or "What score should this answer receive, given initial scores of the base tuples?". Such questions can be answered by consulting the provenance of query results. In recent years there has been significant progress on formal models for provenance. However, the issues of provenance storage, maintenance, and querying have not yet been addressed in an application-independent way. In this paper, we adopt the most general formalism for tuple-based provenance, semiring provenance. We develop a query language for provenance, which can express all of the aforementioned types of queries, as well as many more; we propose storage, processing and indexing schemes for data provenance in support of these queries; and we experimentally validate the feasibility of provenance querying and the benefits of our indexing techniques across a variety of application classes and queries.

#index 1426582
#* Overview of sciDB: large scale array storage, processing and analysis
#@ Paul G. Brown
#t 2010
#c 5
#% 824697
#% 963669
#% 1089604
#% 1328072
#! SciDB [4, 3] is a new open-source data management system intended primarily for use in application domains that involve very large (petabyte) scale array data; for example, scientific applications such as astronomy, remote sensing and climate modeling, bio-science information management, risk management systems in financial applications, and the analysis of web log data. In this talk we will describe our set of motivating examples and use them to explain the features of SciDB. We then briefly give an overview of the project 'in flight', explaining our novel storage manager, array data model, query language, and extensibility frameworks.

#index 1426583
#* Integrating hadoop and parallel DBMs
#@ Yu Xu;Pekka Kostamaa;Like Gao
#t 2010
#c 5
#% 581186
#% 723279
#% 960326
#% 983467
#% 1063553
#% 1127559
#% 1217159
#% 1278123
#% 1328060
#% 1328095
#% 1328186
#! Teradata's parallel DBMS has been successfully deployed in large data warehouses over the last two decades for large scale business analysis in various industries over data sets ranging from a few terabytes to multiple petabytes. However, due to the explosive data volume increase in recent years at some customer sites, some data such as web logs and sensor data are not managed by Teradata EDW (Enterprise Data Warehouse), partially because it is very expensive to load those extreme large volumes of data to a RDBMS, especially when those data are not frequently used to support important business decisions. Recently the MapReduce programming paradigm, started by Google and made popular by the open source Hadoop implementation with major support from Yahoo!, is gaining rapid momentum in both academia and industry as another way of performing large scale data analysis. By now most data warehouse researchers and practitioners agree that both parallel DBMS and MapReduce paradigms have advantages and disadvantages for various business applications and thus both paradigms are going to coexist for a long time [16]. In fact, a large number of Teradata customers, especially those in the e-business and telecom industries have seen increasing needs to perform BI over both data stored in Hadoop and data in Teradata EDW. One common thing between Hadoop and Teradata EDW is that data in both systems are partitioned across multiple nodes for parallel computing, which creates integration optimization opportunities not possible for DBMSs running on a single node. In this paper we describe our three efforts towards tight and efficient integration of Hadoop and Teradata EDW.

#index 1426584
#* A comparison of join algorithms for log processing in MaPreduce
#@ Spyros Blanas;Jignesh M. Patel;Vuk Ercegovac;Jun Rao;Eugene J. Shekita;Yuanyuan Tian
#t 2010
#c 5
#% 58352
#% 114577
#% 115661
#% 136740
#% 286916
#% 723279
#% 960326
#% 963669
#% 1063553
#% 1085307
#% 1127559
#% 1217159
#! The MapReduce framework is increasingly being used to analyze large volumes of data. One important type of data analysis done with MapReduce is log processing, in which a click-stream or an event log is filtered, aggregated, or mined for patterns. As part of this analysis, the log often needs to be joined with reference data such as information about users. Although there have been many studies examining join algorithms in parallel and distributed DBMSs, the MapReduce framework is cumbersome for joins. MapReduce programmers often use simple but inefficient algorithms to perform joins. In this paper, we describe crucial implementation details of a number of well-known join strategies in MapReduce, and present a comprehensive experimental comparison of these join techniques on a 100-node Hadoop cluster. Our results provide insights that are unique to the MapReduce platform and offer guidance on when to use a particular join algorithm on this platform.

#index 1426585
#* Ricardo: integrating R and Hadoop
#@ Sudipto Das;Yannis Sismanis;Kevin S. Beyer;Rainer Gemulla;Peter J. Haas;John McPherson
#t 2010
#c 5
#% 190265
#% 397153
#% 963669
#% 989580
#% 1063553
#% 1063597
#% 1083671
#% 1214666
#% 1260273
#% 1287219
#% 1328066
#% 1328095
#! Many modern enterprises are collecting data at the most detailed level possible, creating data repositories ranging from terabytes to petabytes in size. The ability to apply sophisticated statistical analysis methods to this data is becoming essential for marketplace competitiveness. This need to perform deep analysis over huge data repositories poses a significant challenge to existing statistical software and data management systems. On the one hand, statistical software provides rich functionality for data analysis and modeling, but can handle only limited amounts of data; e.g., popular packages like R and SPSS operate entirely in main memory. On the other hand, data management systems - such as MapReduce-based systems - can scale to petabytes of data, but provide insufficient analytical functionality. We report our experiences in building Ricardo, a scalable platform for deep analytics. Ricardo is part of the eXtreme Analytics Platform (XAP) project at the IBM Almaden Research Center, and rests on a decomposition of data-analysis algorithms into parts executed by the R statistical analysis system and parts handled by the Hadoop data management system. This decomposition attempts to minimize the transfer of data across system boundaries. Ricardo contrasts with previous approaches, which try to get along with only one type of system, and allows analysts to work on huge datasets from within a popular, well supported, and powerful analysis environment. Because our approach avoids the need to re-implement either statistical or data-management functionality, it can be used to solve complex problems right now.

#index 1426586
#* PYMK: friend recommendation at myspace
#@ Michael Moricz;Yerbolat Dosbayev;Mikhail Berlyant
#t 2010
#c 5
#% 268186
#% 452563
#% 961588
#% 1002007
#% 1047390
#! In recent years Social Networking has enjoyed a significant increase in popularity. The main reason behind this surge in popularity is the social experience associated with connecting content to people and also connecting people with other people. Knowing, seeing, hearing what our friends and like-minded people feel or listen to or upload is an unparalleled experience. Similar to real life, finding good friends is not easy without the help of good recommendations. In this Industry Talk paper we present the MySpace friend recommendation algorithm named People You May Know. We will also comment on both the quality and the effectiveness of the algorithms.

#index 1426587
#* Forecasting high-dimensional data
#@ Deepak Agarwal;Datong Chen;Long-ji Lin;Jayavel Shanmugasundaram;Erik Vee
#t 2010
#c 5
#% 227861
#% 235061
#% 829139
#% 866981
#! We propose a method for forecasting high-dimensional data (hundreds of attributes, trillions of attribute combinations) for a duration of several months. Our motivating application is guaranteed display advertising, a multi-billion dollar industry, whereby advertisers can buy targeted (high-dimensional) user visits from publishers many months or even years in advance. Forecasting high-dimensional data is challenging because of the many possible attribute combinations that need to be forecast. To address this issue, we propose a method whereby only a sub-set of attribute combinations are explicitly forecast and stored, while the other combinations are dynamically forecast on-the-fly using high-dimensional attribute correlation models. We evaluate various attribute correlation models, from simple models that assume the independence of attributes to more sophisticated sample-based models that fully capture the correlations in a high-dimensional space. Our evaluation using real-world display advertising data sets shows that fully capturing high-dimensional correlations leads to significant forecast accuracy gains. A variant of the proposed method has been implemented in the context of Yahoo!'s guaranteed display advertising system.

#index 1426588
#* Data warehousing and analytics infrastructure at facebook
#@ Ashish Thusoo;Zheng Shao;Suresh Anthony;Dhruba Borthakur;Namit Jain;Joydeep Sen Sarma;Raghotham Murthy;Hao Liu
#t 2010
#c 5
#% 480821
#% 723279
#% 963669
#% 1350166
#! Scalable analysis on large data sets has been core to the functions of a number of teams at Facebook - both engineering and non-engineering. Apart from ad hoc analysis of data and creation of business intelligence dashboards by analysts across the company, a number of Facebook's site features are also based on analyzing large data sets. These features range from simple reporting applications like Insights for the Facebook Advertisers, to more advanced kinds such as friend recommendations. In order to support this diversity of use cases on the ever increasing amount of data, a flexible infrastructure that scales up in a cost effective manner, is critical. We have leveraged, authored and contributed to a number of open source technologies in order to address these requirements at Facebook. These include Scribe, Hadoop and Hive which together form the cornerstones of the log collection, storage and analytics infrastructure at Facebook. In this paper we will present how these systems have come together and enabled us to implement a data warehouse that stores more than 15PB of data (2.5PB after compression) and loads more than 60TB of new data (10TB after compression) every day. We discuss the motivations behind our design choices, the capabilities of this solution, the challenges that we face in day today operations and future capabilities and improvements that we are working on.

#index 1426589
#* Extreme scale with full SQL language support in microsoft SQL Azure
#@ David G. Campbell;Gopal Kakivaya;Nigel Ellis
#t 2010
#c 5
#% 307360
#% 998845
#% 1054227
#% 1127560
#! Cloud SQL Server is an Internet scale relational database service which is currently used by Microsoft delivered services and also offered directly as a fully relational database service known as "SQL Azure". One of the principle design objectives in Cloud SQL Server was to provide true SQL support with full ACID transactions within controlled scale "consistency domains" and provide a relaxed degree of consistency across consistency domains that would be viable to clusters of 1,000's of nodes. In this paper, we describe the implementation of Cloud SQL Server with an emphasis on this core design principle.

#index 1426590
#* Pay-as-you-go: an adaptive approach to provide full context-aware text search over document content
#@ Zhen Hua Liu;Thomas Baby;Sukhendu Chakraborty;Junyan Ding;Anguel Novoselsky;Vikas Arora
#t 2010
#c 5
#% 136740
#% 172922
#% 287631
#% 339373
#% 385828
#% 397366
#% 406493
#% 480458
#% 480926
#% 481439
#% 577353
#% 632099
#% 654442
#% 783793
#% 810083
#% 867054
#% 881737
#% 960234
#% 1016223
#% 1127567
#% 1190669
#% 1217151
#! RDBMS provides best performance for querying structured data that starts out with a well-defined schema. However, such a 'schema first, data later' approach does not work for unstructured data or data without much structure. Therefore, RDBMS typically stores such data without any schema in LOB columns (for example, Character Large Object (CLOB) or Binary Large Object (BLOB) columns) and provides Information-Retrieval (IR) style, keyword-based search capability over these LOB columns. Lately, XML as a native datatype (XMLType) in RDBMS has been introduced via the SQL/XML standard. Semi-structured data with or without any schema can be stored into such XMLType columns, and XQuery provides query capability over them. In particular, XQuery full text specification provides the capability of searching keywords within document context. Such full context-aware text search capability is more powerful than pure keyword search, since the user can now provide fine-grained context in which the keywords should occur. However, XML with XQuery full text searching requires that the user first convert her text data into XML and store them into XMLType column. Such massive physical data migration with possible loss of document fidelity and its potential impact on existing production environments are often expensive enough that users are reluctant to adopt the XML/XQuery approach. In this paper, we propose a pay-as-you-go architecture to provide XML text view over LOB columns, so that user can take advantage of context-aware full-text search capability adaptively. This adaptive architecture includes a novel XML text index that can be created over the LOB column where the content is stored. The XML text index supports an XML text view over LOB data on top of which XQuery full-text search capability is feasible. Such an adaptive index/view approach provides least intrusion over existing data, as it requires no physical data migration. We describe the design and challenge of building such an adaptive XML text index. Furthermore, we advocate that the pay-as-you-go approach provides the integration bridge between the structured relational world and text oriented document world and fulfills the primary motivation of XML in the database.

#index 1426591
#* Sedna: native XML database management system (internals overview)
#@ Ilya Taranov;Ivan Shcheklein;Alexander Kalinin;Leonid Novak;Sergei Kuznetsov;Roman Pastukhov;Alexander Boldakov;Denis Turdakov;Konstantin Antipin;Andrey Fomichev;Peter Pleshachkov;Pavel Velikhov;Nikolai Zavaritski;Maxim Grinev;Maria Grineva;Dmitry Lizorkin
#t 2010
#c 5
#% 9241
#% 111351
#% 114582
#% 172943
#% 345742
#% 384872
#% 442850
#% 463429
#% 479465
#% 480489
#% 570875
#% 570876
#% 659999
#% 839158
#% 993385
#! We present a native XML database management system, Sedna, which is implemented from scratch as a full-featured database management system for storing large amounts of XML data. We believe that the key contribution of this system is an improved schema-based clustering storage strategy efficient for both XML querying and updating, and powered by a novel memory management technique. We position our approach with respect to state-of-the-art methods.

#index 1426592
#* Optimizing schema-last tuple-store queries in graphd
#@ Scott M. Meyer;Jutta Degener;John Giannandrea;Barak Michener
#t 2010
#c 5
#% 103
#% 111351
#% 261370
#% 286856
#% 303072
#% 326523
#% 387427
#% 519567
#% 824755
#% 978404
#% 1022236
#% 1022298
#% 1063570
#% 1127431
#% 1190676
#! Current relational databases require that a database schema exist prior to data entry and require manual optimization for best performance. We describe the query optimization techniques used by graphd, the schema-last, automatically indexed tuple-store which supports freebase.com, a large world-writable database. Graphd is a log-structured store with a query optimizer based on a functional operator tree over the domain of sorted integer sets which accumulate naturally as tuples are appended to the store. We demonstrate that a set-based optimizer can deliver performance that is roughly comparable to traditional RDBMS query optimization techniques applied to a fixed schema.

#index 1426593
#* OpenII: an open source information integration toolkit
#@ Len Seligman;Peter Mork;Alon Halevy;Ken Smith;Michael J. Carey;Kuang Chen;Chris Wolf;Jayant Madhavan;Akshay Kannan;Doug Burdick
#t 2010
#c 5
#% 824736
#% 864573
#% 993981
#% 1127411
#% 1206886
#% 1217257
#% 1426617
#! OpenII (openintegration.org) is a collaborative effort to create a suite of open-source tools for information integration (II). The project is leveraging the latest developments in II research to create a platform on which integration tools can be built and further research conducted. In addition to a scalable, extensible platform, OpenII includes industrial-strength components developed by MITRE, Google, UC-Irvine, and UC-Berkeley that interoperate through a common repository in order to solve II problems. Components of the toolkit have been successfully applied to several large-scale US government II challenges.

#index 1426594
#* Google fusion tables: web-centered data management and collaboration
#@ Hector Gonzalez;Alon Y. Halevy;Christian S. Jensen;Anno Langen;Jayant Madhavan;Rebecca Shapley;Warren Shen;Jonathan Goldberg-Kidon
#t 2010
#c 5
#% 845350
#% 960234
#% 1063468
#% 1127393
#% 1217146
#% 1426493
#! It has long been observed that database management systems focus on traditional business applications, and that few people use a database management system outside their workplace. Many have wondered what it will take to enable the use of data management technology by a broader class of users and for a much wider range of applications. Google Fusion Tables represents an initial answer to the question of how data management functionality that focused on enabling new users and applications would look in today's computing environment. This paper characterizes such users and applications and highlights the resulting principles, such as seamless Web integration, emphasis on ease of use, and incentives for data sharing, that underlie the design of Fusion Tables. We describe key novel features, such as the support for data acquisition, collaboration, visualization, and web-publishing.

#index 1426595
#* Visual interfaces to data
#@ Christopher Stolte
#t 2010
#c 5
#! Easy-to-use visual interfaces to data can broadly expand the audience for databases. Domain experts rather than database experts can engage in rapid-fire Q&A sessions with the data. Visual interfaces can provide a medium for story-telling, debate, and conversations about the data. They can also put new and challenging demands on the capabilities of traditional relational databases. In this talk, I will describe our formal language-based approach to visual analysis and how the use of a formal language enables us to build user experiences that more effectively support the process of analysis. Tableau's VizQL algebra is a declarative language for succinctly describing visual representations of data and analytics operations on the data. A VizQL statement compiles into the SQL or MDX queries necessary to generate the view and into the graphical commands to render the interactive view of the data. Our easy-to-use drag-and-drop user experiences for analysis and visual interface authoring are built on top of VizQL. In addition to supporting the process of analysis, a formal language-based approach provides a basis for reasoning about the structure of views and the space of possible views. This in turn enables the development of powerful new analytic capabilities, such as automatic presentation of structured data, visual authoring of statistical models, and view-based calculation, which we demonstrate. I will also discuss the challenges we have faced in getting relational databases "in the wild" to effectively support visual analysis for the average business or scientific user. The challenges range from the technical to the political. Traditional relational databases, both for OLTP and OLAP, often require sophisticated data modeling and data management expertise, optimize for performance based on known workloads, and are designed for scaling to large databases sizes (e.g. PB or TB) on clusters of machines rather than reducing analytic latency using limited hardware. I will describe our approaches to building a database focused on providing interactive query performance on tens or hundreds of millions of rows of data with little or no data modeling (physical or logical) and running on a typical knowledge worker desktop machine. Finally, I will discuss the changing landscape of interfaces to databases. The original interface to the database was transactional in focus: Many users read and make atomic changes to a small number of rows in a large database. In recent decades, powerful analytic use cases have emerged focused on the study and analysis of massive amounts of data by relatively small numbers of power users. The emergence of easily authored visual interfaces to public and private data changes will enables a new style of database usage. Millions of users performing analytics on thousands of data sets all hosted in the cloud with usage demonstrating the familiar long-tail distribution. Everyone will become an author and all interfaces will enable analytics.

#index 1426596
#* Graphical XQuery in the aqualogic data services platform
#@ Vinayak Borkar;Michael Carey;Sebu Koleth;Alex Kotopoulis;Kautul Mehta;Joshua Spiegel;Sachin Thatte;Till Westmann
#t 2010
#c 5
#% 801413
#% 810050
#% 814647
#% 875028
#% 1127370
#% 1206614
#% 1206803

#index 1426597
#* Continuous analytics over discontinuous streams
#@ Sailesh Krishnamurthy;Michael J. Franklin;Jeffrey Davis;Daniel Farina;Pasha Golovko;Alan Li;Neil Thombre
#t 2010
#c 5
#% 114582
#% 115661
#% 403195
#% 464215
#% 480113
#% 765470
#% 801694
#% 810008
#% 963669
#% 1127373
#! Continuous analytics systems that enable query processing over steams of data have emerged as key solutions for dealing with massive data volumes and demands for low latency. These systems have been heavily influenced by an assumption that data streams can be viewed as sequences of data that arrived more or less in order. The reality, however, is that streams are not often so well behaved and disruptions of various sorts are endemic. We argue, therefore, that stream processing needs a fundamental rethink and advocate a unified approach toward continuous analytics over discontinuous streaming data. Our approach is based on a simple insight - using techniques inspired by data parallel query processing, queries can be performed over independent sub-streams with arbitrary time ranges in parallel, generating partial results. The consolidation of the partial results over each sub-stream can then be deferred to the time at which the results are actually used on an on-demand basis. In this paper, we describe how the Truviso Continuous Analytics system implements this type of order-independent processing. Not only does the approach provide the first real solution to the problem of processing streaming data that arrives arbitrarily late, it also serves as a critical building block for solutions to a host of hard problems such as parallelism, recovery, transactional consistency, high availability, failover, and replication.

#index 1426598
#* IBM infosphere streams for scalable, real-time, intelligent transportation services
#@ Alain Biem;Eric Bouillet;Hanhua Feng;Anand Ranganathan;Anton Riabov;Olivier Verscheure;Haris Koutsopoulos;Carlos Moran
#t 2010
#c 5
#% 765187
#% 824722
#% 1063555
#% 1164990
#% 1298896
#% 1431827
#% 1445669
#! With the widespread adoption of location tracking technologies like GPS, the domain of intelligent transportation services has seen growing interest in the last few years. Services in this domain make use of real-time location-based data from a variety of sources, combine this data with static location-based data such as maps and points of interest databases, and provide useful information to end-users. Some of the major challenges in this domain include i) scalability, in terms of processing large volumes of real-time and static data; ii) extensibility, in terms of being able to add new kinds of analyses on the data rapidly, and iii) user interaction, in terms of being able to support different kinds of one-time and continuous queries from the end-user. In this paper, we demonstrate the use of IBM InfoSphere Streams, a scalable stream processing platform, for tackling these challenges. We describe a prototype system that generates dynamic, multi-faceted views of transportation information for the city of Stockholm, using real vehicle GPS and road-network data. The system also continuously derives current traffic statistics, and provides useful value-added information such as shortest-time routes from real-time observed and inferred traffic conditions. Our performance experiments illustrate the scalability of the system. For instance, our system can process over 120000 incoming GPS points per second, combine it with a map containing over 600,000 links, continuously generate different kinds of traffic statistics and answer user queries.

#index 1426599
#* SIE-OBI: a streaming information extraction platform for operational business intelligence
#@ Malu Castellanos;Song Wang;Umeshwar Dayal;Chetan Gupta
#t 2010
#c 5
#% 776472
#% 818500
#% 1166537
#% 1181213
#% 1384844
#! Emerging business intelligence (BI) applications aim to provide situational awareness, i.e., information about real-world events that might affect the business operations of an enterprise. For instance, an enterprise might want to know whether customers are posting positive or negative comments about a new product it has just introduced; or whether some natural disaster affects its contracted suppliers. It is difficult to develop such applications today because they require extracting and correlating facts from multiple streaming and stored data sources, typically including unstructured data, which is not well supported by BI platforms today. In this paper, we describe SIE-OBI, a system that we are developing to enable the development and execution of such applications. We describe the novel features of this system, including a declarative interface for rapidly developing such applications, and a platform for optimizing and executing the applications. We illustrate its applicability through two use cases.

#index 1426600
#* HadoopDB in action: building real world applications
#@ Azza Abouzied;Kamil Bajda-Pawlikowski;Jiewen Huang;Daniel J. Abadi;Avi Silberschatz
#t 2010
#c 5
#% 1063553
#% 1190676
#% 1217159
#% 1328186
#! HadoopDB is a hybrid of MapReduce and DBMS technologies, designed to meet the growing demand of analyzing massive datasets on very large clusters of machines. Our previous work has shown that HadoopDB approaches parallel databases in performance and still yields the scalability and fault tolerance of MapReduce-based systems. In this demonstration, we focus on HadoopDB's flexible architecture and versatility with two real world application scenarios: a semantic web data application for protein sequence analysis and a business data warehousing application based on TPC-H. The demonstration offers a thorough walk-through of how to easily build applications on top of HadoopDB.

#index 1426601
#* Online aggregation and continuous query support in MapReduce
#@ Tyson Condie;Neil Conway;Peter Alvaro;Joseph M. Hellerstein;John Gerth;Justin Talbot;Khaled Elmeleegy;Russell Sears
#t 2010
#c 5
#% 227883
#% 300167
#% 438135
#% 954300
#% 963669
#% 1063553
#% 1278381
#% 1286767
#% 1328095
#% 1468411
#! MapReduce is a popular framework for data-intensive distributed computing of batch jobs. To simplify fault tolerance, the output of each MapReduce task and job is materialized to disk before it is consumed. In this demonstration, we describe a modified MapReduce architecture that allows data to be pipelined between operators. This extends the MapReduce programming model beyond batch processing, and can reduce completion times and improve system utilization for batch jobs as well. We demonstrate a modified version of the Hadoop MapReduce framework that supports online aggregation, which allows users to see "early returns" from a job as it is being computed. Our Hadoop Online Prototype (HOP) also supports continuous queries, which enable MapReduce programs to be written for applications such as event monitoring and stream processing. HOP retains the fault tolerance properties of Hadoop, and can run unmodified user-defined MapReduce programs.

#index 1426602
#* MapDupReducer: detecting near duplicates over massive datasets
#@ Chaokun Wang;Jianmin Wang;Xuemin Lin;Wei Wang;Haixun Wang;Hongsong Li;Wanpeng Tian;Jun Xu;Rui Li
#t 2010
#c 5
#% 769944
#% 913783
#% 963669
#% 1055684
#% 1227596
#! Near duplicate detection benefits many applications, e.g., on-line news selection over the Web by keyword search. The purpose of this demo is to show the design and implementation of MapDupReducer, a MapReduce based system capable of detecting near duplicates over massive datasets efficiently.

#index 1426603
#* Large graph processing in the cloud
#@ Rishan Chen;Xuetian Weng;Bingsheng He;Mao Yang
#t 2010
#c 5
#% 258595
#% 328257
#% 813966
#% 963669
#% 983467
#% 1127559
#% 1318636
#% 1468252
#! As the study of graphs, such as web and social graphs, becomes increasingly popular, the requirements of efficiency and programming flexibility of large graph processing tasks challenge existing tools. We propose to demonstrate Surfer, a large graph processing engine designed to execute in the cloud. Surfer provides two basic primitives for programmers - MapReduce and propagation. MapReduce, originally developed by Google, processes different key-value pairs in parallel, and propagation is an iterative computational pattern that transfers information along the edges from a vertex to its neighbors in the graph. These two primitives are complementary in graph processing. MapReduce is suitable for processing flat data structures, such as vertex-oriented tasks, and propagation is optimized for edge-oriented tasks on partitioned graphs. To further improve the programmability of large graph processing, Surfer consists of a small set of high level building blocks that use these two primitives. Developers may also construct custom building blocks. Surfer further provides a GUI (Graphical User Interface) using which developers can visually create large graph processing tasks. Surfer transforms a task into an execution plan composed of MapReduce and propagation operations. It then automatically applies various optimizations to improve the efficiency of distributed execution. Surfer also provides a visualization tool to monitor the detailed execution dynamics of the execution plan to show the interesting tradeoffs between MapReduce and propagation. We demonstrate our system in two ways: first, we demo the ease-of-programming features of the system; second, we show the efficiency of the system with a series of applications on a social network. We find that Surfer is simple to use and is highly efficient for large graph-based tasks.

#index 1426604
#* DCUBE: discrimination discovery in databases
#@ Salvatore Ruggieri;Dino Pedreschi;Franco Turini
#t 2010
#c 5
#% 942743
#% 985041
#% 1083686
#! Discrimination discovery in databases consists in finding unfair practices against minorities which are hidden in a dataset of historical decisions. The DCUBE system implements the approach of [5], which is based on classification rule extraction and analysis, by centering the analysis phase around an Oracle database. The proposed demonstration guides the audience through the legal issues about discrimination hidden in data, and through several legally-grounded analyses to unveil discriminatory situations. The SIGMOD attendees will freely pose complex discrimination analysis queries over the database of extracted classification rules, once they are presented with the database relational schema, a few ad-hoc functions and procedures, and several snippets of SQL queries for discrimination discovery.

#index 1426605
#* S-OLAP: an OLAP system for analyzing sequence data
#@ Chun Kit Chui;Ben Kao;Eric Lo;David Cheung
#t 2010
#c 5
#% 1016173
#% 1063518
#! The Sequence OLAP (S-OLAP) system is a novel online analytical processing system for analyzing sequence data. S-OLAP supports "pattern-based" grouping and aggregation on sequence data - a very powerful concept and capability that is not supported by traditional OLAP systems. It also supports several new OLAP operations that are specific to sequence data analysis. The query processing techniques documented in [1] have been implemented in our S-OLAP engine for efficient query processing. The system also provides users with a friendly graphical interface for query construction and result visualization. Query parameters can be interactively refined and the results are updated in real-time so as to facilitate the exploratory analysis of sequence data.

#index 1426606
#* ProgXe: progressive result generation framework for multi-criteria decision support queries
#@ Venkatesh Raghavan;Elke A. Rundensteiner
#t 2010
#c 5
#% 465167
#% 480671
#% 654480
#! We demonstrate ProgXe, a practical approach to support Multi-Criteria Decision Support (MCDS) applications that need to report results as they are being generated to enable the user to make competitive decisions. ProgXe transforms the execution of MCDS queries involving skyline over joins to be non-blocking by progressively generating results early and often. The demonstration highlights key features of our progressive execution framework that optimizes for early output generation by: (1) evaluating the query at multiple levels of abstraction, (2) exploiting the skyline knowledge gained from both input as well as mapped output spaces. The audience will be able to submit MCDS queries. We provide visualization tools that enable the user to make quick decisions, compare alternative techniques, and provide capability to fine-tune the query predicates based on the early output results.

#index 1426607
#* XTaGe: a flexible XML collection generator
#@ María Pérez;Ismael Sanz;Rafael Berlanga
#t 2010
#c 5
#% 489172
#% 879813
#% 994015
#% 1127389
#! In this demonstration we present XTaGe (XML Tester and Generator), a flexible tool for the creation of complex XML collections. XTaGe focuses on XML collections with complex structural constraints and domain-specific characteristics, which would be very difficult or impossible to replicate using existing XML generators. It addresses the limitations of current XML generators by providing a highly extensible framework to introduce controlled variability in XML structures.

#index 1426608
#* K*SQL: a unifying engine for sequence patterns and XML
#@ Barzan Mozafari;Kai Zeng;Carlo Zaniolo
#t 2010
#c 5
#% 333850
#% 1159250
#% 1206641
#% 1217239
#% 1326584
#% 1328078
#% 1733511
#! A strong interest is emerging in SQL extensions for sequence patterns using Kleene-closure expressions. This burst of interest from both the research community and the commercial world is due to the many database and data stream applications made possible by these extensions, including financial services, RFID-based inventory management, and electronic health systems. In this demo we will present the K*SQL system that represents a major step forward in this area. K*SQL supports a more expressive language that allows for generalized Kleene-closure queries and also achieves the expressive power of the nested word model, which greatly expands the application domain to include XML queries, software trace analysis, and genomics. In this demo, we first introduce the core features of our language in expressing complex pattern queries over both relational and XML data. We overview the architecture of our unifying engine and its user-friendly interfaces. We also present several K*SQL queries from stock market, XML, software trace analysis and genomic applications.

#index 1426609
#* Symbiote: a Reconfigurable Logic Assisted Data Stream Management System (RLADSMS)
#@ Pranav S. Vaidya;Jaehwan John Lee;Francis Bowen;Yingzi Du;Chandima H. Nadungodage;Yuni Xia
#t 2010
#c 5
#% 300179
#% 349151
#% 397414
#% 481943
#% 654508
#% 654510
#% 741290
#% 745434
#% 1081387
#! Numerous monitoring applications such as traffic control systems, border patrol monitoring, and person locater services generate a large number of multimedia data streams that need to be analyzed and processed using image processing and data stream management techniques in order to detect significant events of interest or abnormal conditions. Such multimedia monitoring systems usually have high-bandwidth characteristics, stricter real-time deadlines, and high accuracy requirements. In an attempt to meet all of these requirements, we have designed Symbiote - a distributed Reconfigurable Logic Assisted multimedia Data Stream Management System (RLADSMS) at Indiana University Purdue University, Indianapolis (IUPUI) that provides hardware accelerated data stream processing using Field Programmable Gate Arrays (FPGAs).

#index 1426610
#* Interactive visual exploration of neighbor-based patterns in data streams
#@ Di Yang;Zhenyu Guo;Zaixian Xie;Elke A. Rundensteiner;Matthew O. Ward
#t 2010
#c 5
#% 757720
#% 1015261
#% 1083627
#% 1181258
#% 1328182
#% 1908484
#! We will demonstrate our system, called V iStream, supporting interactive visual exploration of neighbor-based patterns [7] in data streams. V iStream does not only apply innovative multi-query strategies to compute a broad range of popular patterns, such as clusters and outliers, in a highly efficient manner, but it also provides a rich set of visual interfaces and interactions to enable real-time pattern exploration. With ViStream, analysts can easily interact with pattern mining processes by navigating along the time horizons, abstraction levels and parameter spaces, and thus better understand the phenomena of interest.

#index 1426611
#* TwitterMonitor: trend detection over the twitter stream
#@ Michael Mathioudakis;Nick Koudas
#t 2010
#c 5
#% 1214671
#% 1217245
#! We present TwitterMonitor, a system that performs trend detection over the Twitter stream. The system identifies emerging topics (i.e. 'trends') on Twitter in real time and provides meaningful analytics that synthesize an accurate description of each topic. Users interact with the system by ordering the identified trends using different criteria and submitting their own description for each trend. We discuss the motivation for trend detection over social media streams and the challenges that lie therein. We then describe our approach to trend detection, as well as the architecture of TwitterMonitor. Finally, we lay out our demonstration scenario.

#index 1426612
#* Glacier: a query-to-hardware compiler
#@ Rene Mueller;Jens Teubner;Gustavo Alonso
#t 2010
#c 5
#% 794331
#% 1328128
#% 1328185
#! Field-programmable gate arrays (FPGAs) are a promising technology that can be used in database systems. In this demonstration we show Glacier, a library and a compiler that can be employed to implement streaming queries as hardware circuits on FPGAs. Glacier consists of a library of compositional hardware modules that represent stream processing operators. Given a query execution plan, the compiler instantiates the corresponding components and wires them up to a digital circuit. The goal of this demo is to show the flexibility of the compositional approach.

#index 1426613
#* Exploratory keyword search on data graphs
#@ Hilit Achiezra;Konstantin Golenberg;Benny Kimelfeld;Yehoshua Sagiv
#t 2010
#c 5
#% 660011
#% 874894
#% 960261
#% 993987
#% 1063539
#% 1206760
#% 1217198
#% 1217259
#! A system for keyword search on data graphs is demonstrated on two challenging datasets: the large DBLP and Mondial (which is highly cyclic and has a complex schema). The system supports search, exploration and question answering. The demonstration shows how the system copes with the main challenges in keywords search on data graphs. In particular, the system generates answers efficiently and completely (i.e., it does not miss answers). It has an effective ranking mechanism that also takes into account redundancies among answers. Finally, the system uses a novel technique for displaying multi-node subtrees in a compact graphical form that facilitates quick and easy understanding, which is essential for effective browsing of the answers.

#index 1426614
#* Integrating keyword search with multiple dimension tree views over a summary corpus data cube
#@ Mark Sifer;Jian Lin;Yutaka Watanobe;Subhash Bhalla
#t 2010
#c 5
#% 282427
#% 438504
#% 857482
#% 1074236
#! We demonstrate a system that integrates a novel OLAP component with a keyword search engine, to support querying over sparse and ragged corpus data. The key contribution of our system is the integration of dynamically selected point sets such as search results with OLAP querying over aggregated data. During the demonstration, participants will be able to enter a keyword search; observe the returned list of result files; observe distributional features such as outliers and clusters of results in the corpus in multiple dimension views; and select and partition corpus slices in the OLAP component to narrow search results. Participants will be able to experience not just the individual querying features of our system, but the way that they work together to facilitate smooth interaction sequences that combine OLAP and keyword search querying.

#index 1426615
#* Query portals: dynamically generating portals for entity-oriented web queries
#@ Sanjay Agrawal;Kaushik Chakrabarti;Surajit Chaudhuri;Venkatesh Ganti;Arnd Christian König;Dong Xin
#t 2010
#c 5
#% 769884
#% 875001
#% 1063530
#% 1083654
#% 1127426
#% 1190070
#% 1190105
#% 1328142
#! Many web queries seek information about named entities (such as products or people). Web search engines federate such entity-oriented queries to relevant structured databases; the results of those searches are then returned to the user along with web search results. Current federated approaches have two limitations: (i) they often fail to return important results for a broad class of such entity-oriented queries and (ii) the information they return per entity is often inadequate. In this paper, we present the Query Portals system that addresses these limitations. The Query Portals system dynamically generates a portal for an entity-oriented query. It first provides an overview of the relevant entities and further allows users to drill down to gather additional information on these entities. Our architecture uses a judicious combination of pre-processing and query time techniques so that the query portal can be generated efficiently.

#index 1426616
#* Creating and exploring web form repositories
#@ Luciano Barbosa;Hoa Nguyen;Thanh Nguyen;Ramesh Pinnamaneni;Juliana Freire
#t 2010
#c 5
#% 765409
#% 956537
#% 956538
#% 1127405
#% 1127557
#! We present DeepPeep (http://www.deeppeep.org), a new system for discovering, organizing and analyzing Web forms. DeepPeep allows users to explore the entry points to hidden-Web sites whose contents are out of reach for traditional search engines. Besides demonstrating important features of DeepPeep and describing the infrastructure we used to build the system, we will show how this infrastructure can be used to create form collections and form search engines for different domains. We also present the analysis component of DeepPeep which allows users to explore and visualize information in form repositories, helping them not only to better search and understand forms in different domains, but also to refine the form gathering process.

#index 1426617
#* Exploring schema similarity at multiple resolutions
#@ Ken Smith;Craig Bonaceto;Chris Wolf;Beth Yost;Michael Morse;Peter Mork;Doug Burdick
#t 2010
#c 5
#% 438617
#% 729437
#% 864573
#% 1217257
#! Large, dynamic, and ad-hoc organizations must frequently initiate data integration and sharing efforts with insufficient awareness of how organizational data sources are related. Decision makers need to reason about data model interactions much as they do about data instance interactions in OLAP: at multiple levels of granularity. We demonstrate an integrated environment for exploring schema similarity across multiple resolutions. Users visualize and interact with clusters of related schemas using a tool named Affinity. Within any cluster, users may drill-down to examine the extent and content of schema overlap. Further drill down enables users to explore fine-grained element-level correspondences between between two selected schemas.

#index 1426618
#* An automated, yet interactive and portable DB designer
#@ Ioannis Alagiannis;Debabrata Dash;Karl Schnaitter;Anastasia Ailamaki;Neoklis Polyzotis
#t 2010
#c 5
#% 36119
#% 765176
#% 810026
#% 875062
#% 1016220
#% 1022293
#% 1193594
#% 1328212
#! Tuning tools attempt to configure a database to achieve optimal performance for a given workload. Selecting an optimal set of physical structures is computationally hard since it involves searching a vast space of possible configurations. Commercial DBMSs offer tools that can address this problem. The usefulness of such tools, however, is limited by their dependence on greedy heuristics, the need for a-priori (offline) knowledge of the workload, and lack of an optimal materialization schedule to get the best out of suggested design features. Moreover, the open source DBMSs do not provide any automated tuning tools. This demonstration introduces a comprehensive physical designer for the PostgreSQL open source DBMS. The tool suggests design features for both offline and online workloads. It provides close to optimal suggestions for indexes for a given workload by modeling the problem as a combinatorial optimization problem and solving it by sophisticated and mature solvers. It also determines the interaction between indexes to suggest an effective materialization strategy for the selected indexes. The tool is interactive as it allows the database administrator (DBA) to suggest a set of candidate features and shows their benefits and interactions visually. For the demonstration we use large real-world scientific datasets and query workloads.

#index 1426619
#* Midas: integrating public financial data
#@ Sreeram Balakrishnan;Vivian Chu;Mauricio A. Hernández;Howard Ho;Rajasekar Krishnamurthy;Shi Xia Liu;Jan H. Pieper;Jeffrey S. Pierce;Lucian Popa;Christine M. Robson;Lei Shi;Ioana R. Stanoi;Edison L. Ting;Shivakumar Vaithyanathan;Huahai Yang
#t 2010
#c 5
#% 1183368
#% 1217114
#! The primary goal of the Midas project is to build a system that enables easy and scalable integration of unstructured and semi-structured information present across multiple data sources. As a first step in this direction, we have built a system that extracts and integrates information from regulatory filings submitted to the U.S. Securities and Exchange Commission (SEC) and the Federal Deposit Insurance Corporation (FDIC). Midas creates a repository of entities, events, and relationships by extracting, conceptualizing, integrating, and aggregating data from unstructured and semi-structured documents. This repository enables applications to use the extracted and integrated data in a variety of ways including mashups with other public data and complex risk analysis.

#index 1426620
#* Worry-free database upgrades: automated model-driven evolution of schemas and complex mappings
#@ James F. Terwilliger;Philip A. Bernstein;Adi Unnithan
#t 2010
#c 5
#% 488624
#% 777935
#% 824736
#% 945790
#% 1092008
#% 1666129
#! Schema evolution is an unavoidable consequence of the application development lifecycle. The two primary schemas in an application, the client conceptual object model and the persistent database model, must co-evolve or risk quality, stability, and maintainability issues. We present MoDEF, an extension to Visual Studio that supports automatic evolution of object-relational mapping artifacts in the Microsoft Entity Framework. When starting with a valid mapping between client and store, MoDEF translates changes made to a client model into incremental changes to the store as an upgrade script, along with a new valid mapping to the new store. MoDEF mines the existing mapping for mapping patterns which MoDEF reuses for new client artifacts.

#index 1426621
#* US-SQL: managing uncertain schemata
#@ Matteo Magnani;Danilo Montesi
#t 2010
#c 5
#% 810098
#% 824764
#% 1063534
#% 1179162
#% 1425119
#% 1720904
#! In this paper we describe a demo concerning the management of uncertain schemata. Many works have studied the problem of representing uncertainty on attribute values or tuples, like the fact that a value is 10 with probability .3 or 20 with probability .7, leading to the implementation of probabilistic database management systems. In our demo we deal with the representation of uncertainty about the meta-data, i.e., about the meaning of these values. Using our system it is possible to create alternative probabilistic schemata on a database, execute queries over uncertain schemata and verify how this additional information is stored in an underlying relational database and how queries are executed.

#index 1426622
#* PAROS: pareto optimal route selection
#@ Franz Graf;Hans-Peter Kriegel;Matthias Renz;Matthias Schubert
#t 2010
#c 5
#! Modern maps provide a variety of information about roads and their surrounding landscape allowing navigation systems to go beyond simple shortest path computation. In this demo, we show how the concept of skyline queries can be successfully adapted to routing problems considering multiple road attributes. In particular, we demonstrate how to compute several pareto-optimal paths which contain optimal results for a variety of user preferences. The PAROS-system has two main purposes. The first is to calculate the route skyline for a starting point and a destination. Our demonstrator visualizes the result set for up to three road attributes. Therefore, we provide a dual view on the computed skyline paths. The first view displays the result paths on the road map itself. The second view describes the result paths in the property space, displaying the trade-off between the underlying criteria. Thus, a user can browse through the results in order to find the path which fits best to his personal preferences. The second component of our system suits analysis issues. In this component, we illustrate the functionality of the underlying route skyline algorithm. Thus, we provide benchmark information about processing time and the search space visited during route skyline computation.

#index 1426623
#* MoveMine: mining moving object databases
#@ Zhenhui Li;Ming Ji;Jae-Gil Lee;Lu-An Tang;Yintao Yu;Jiawei Han;Roland Kays
#t 2010
#c 5
#% 907380
#% 960283
#% 1206639
#% 1206688
#! With the maturity of GPS, wireless, and Web technologies, increasing amounts of movement data collected from various moving objects, such as animals, vehicles, mobile devices, and climate radars, have become widely available. Analyzing such data has broad applications, e.g., in ecological study, vehicle control, mobile communication management, and climatological forecast. However, few data mining tools are available for flexible and scalable analysis of massive-scale moving object data. Our system, MoveMine, is designed for sophisticated moving object data mining by integrating several attractive functions including moving object pattern mining and trajectory mining. We explore the state-of-the-art and novel techniques at implementation of the selected functions. A user-friendly interface is provided to facilitate interactive exploration of mining results and flexible tuning of the underlying methods. Since MoveMine is tested on multiple kinds of real data sets, it will benefit users to carry out versatile analysis on these kinds of data. At the same time, it will benefit researchers to realize the importance and limitations of current techniques as well as the potential future studies in moving object data mining.

#index 1426624
#* PIQL: a performance insightful query language
#@ Michael Armbrust;Stephen Tu;Armando Fox;Michael J. Franklin;David A. Patterson;Nick Lanham;Beth Trushkowsky;Jesse Trutna
#t 2010
#c 5
#% 1054227
#! Large-scale websites are increasingly moving from relational databases to distributed key-value stores for high request rate, low latency workloads. Often this move is motivated not only by key-value stores' ability to scale simply by adding more hardware, but also by the easy to understand predictable performance they provide for all operations. While this data model works well, lookups are only done by primary key. More complex queries require onerous, explicit index management and imperative data lookups by the developer. We demonstrate PIQL, a Performance Insightful Query Language that allows developers to express many of the queries found on these websites, while still providing strict bounds on the number of I/O operations for any query.

#index 1426625
#* DoCQS: a prototype system for supporting data-oriented content query
#@ Mianwei Zhou;Tao Cheng;Kevin Chen-Chuan Chang
#t 2010
#c 5
#% 730022
#% 754068
#% 805883
#% 854668
#% 869535
#% 1022234
#! Witnessing the richness of data in document content and many ad-hoc efforts for finding such data, we propose a Data-oriented Content Query System(DoCQS), which is oriented towards fine granularity data of all types by searching directly into document content. DoCQS uses the relational model as the underlying data model, and offers a powerful and flexible Content Query Language(CQL) to adapt to diverse query demands. In this demonstration, we show how to model various search tasks by CQL statements, and how the system architecture efficiently supports the CQL execution. Our online demo of the system is available at http://wisdm.cs.uiuc.edu/demos/docqs/.

#index 1426626
#* QRelX: generating meaningful queries that provide cardinality assurance
#@ Manasi Vartak;Venkatesh Raghavan;Elke A. Rundensteiner
#t 2010
#c 5
#% 227894
#% 769900
#% 893105
#% 893172
#% 902467
#% 1063507
#% 1181286
#! In many business and consumer applications, queries have cardinality constraints. However, current database systems provide minimal support for cardinality assurance. Consequently, users must adopt a cumbersome trial-and-error approach to find queries that are close to the original query but also attain the desired cardinality. In this demonstration, we present QRelX a novel framework to automatically generate alternate queries that meet the cardinality and closeness criteria. QRelX employs an innovative query space transformation strategy, proximity-based search and incremental cardinality estimation to efficiently find alternate queries. Our demonstration is an interactive game that allows the audience to compete with QRelX via manual query refinement. We illustrate the importance of cardinality assurance through real-time comparisons between manual refinement and QRelX. We also highlight the novelty of our solution by visualizing the core algorithms of QRelX.

#index 1426627
#* Performing sound flash device measurements: some lessons from uFLIP
#@ Matias Bjørling;Lionel Le Folgoc;Ahmed Mseddi;Philippe Bonnet;Luc Bouganim;Björn Jónsson
#t 2010
#c 5
#% 1222048
#% 1278373
#% 1426521
#! It is amazingly easy to get meaningless results when measuring flash devices, partly because of the peculiarity of flash memory, but primarily because their behavior is determined by layers of complex, proprietary, and undocumented software and hardware. In this demonstration, we share the lessons we learnt developing the uFlip benchmark and conducting experiments with a wide range of flash devices. We illustrate the problems that are actual obstacles to sound performance and energy measurements, and we show how to mitigate the effects of these problems. We also present the uFlip web site and its on-line visualization tool that should help the research community investigate flash device behavior.

#index 1426628
#* GDR: a system for guided data repair
#@ Mohamed Yakout;Ahmed K. Elmagarmid;Jennifer Neville;Mourad Ouzzani
#t 2010
#c 5
#% 301169
#% 400847
#% 480499
#% 810019
#% 903332
#% 994038
#% 1022228
#% 1054480
#% 1127587
#% 1217182
#! Improving data quality is a time-consuming, labor-intensive and often domain specific operation. Existing data repair approaches are either fully automated or not efficient in interactively involving the users. We present a demo of GDR, a Guided Data Repair system that uses a novel approach to efficiently involve the user alongside automatic data repair techniques to reach better data quality as quickly as possible. Specifically, GDR generates data repairs and acquire feedback on them that would be most beneficial in improving the data quality. GDR quantifies the data quality benefit of generated repairs by combining mechanisms from decision theory and active learning. Based on these benefit scores, groups of repairs are ranked and displayed to the user. User feedback is used to train a machine learning component to eventually replace the user in deciding on the validity of a suggested repair. We describe how the generated repairs are ranked and displayed to the user in a "useful-looking" way and demonstrate how data quality can be effectively improved with minimal feedback from the user.

#index 1426629
#* Crescando
#@ Georgios Giannikis;Philipp Unterbrunner;Jeremy Meyer;Gustavo Alonso;Dietmar Fauser;Donald Kossmann
#t 2010
#c 5
#% 857498
#% 1328168
#! This demonstration presents Crescando, an implementation of a distributed relational table that guarantees predictable response time on unpredictable workloads. In Crescando, data is stored in main memory and accessed via full-table scans. By using scans instead of index lookups, Crescando overcomes the read-write contention in index structures and eliminates the scalability issues that exist in traditional index-based systems. Crescando is specifically designed to process a large number of queries in parallel, allowing high query rates. The goal of this demonstration is to show the ability of Crescando to a) quickly answer arbitrary user-generated queries, and b) execute a large number of queries and updates in parallel, while providing strict response time and data freshness guarantees.

#index 1426630
#* iTuned: a tool for configuring and visualizing database parameters
#@ Vamsidhar Thummala;Shivnath Babu
#t 2010
#c 5
#% 1328213
#% 1468266
#! iTuned is a tool that takes a SQL workload as input and recommends good settings for database configuration parameters such as buffer pool sizes, multi-programming level, and number of I/O daemons. iTuned also provides response-surface and sensitivity-analysis plots that help the end-user analyze the impact of each parameter. iTuned has the following novel features: (i) a technique called Adaptive Sampling that proactively brings in appropriate data through planned experiments to find high-impact parameters and high-performance parameter settings, (ii) an executor that supports on-line experiments in production database environments through a cycle-stealing paradigm that places minimal overhead on the production workload, and (iii) portability across different database systems. This demonstration will focus on the interesting use-case scenarios of iTuned, and show the effectiveness of recommended configuration settings on popular database benchmarks.

#index 1426631
#* Pluggable personal data servers
#@ Nicolas Anciaux;Luc Bouganim;Yanli Guo;Philippe Pucheral;Jean-Jacques Vandewalle;Shaoyi Yin
#t 2010
#c 5
#% 960290
#% 993943
#% 1022345
#% 1181263
#% 1381029
#! An increasing amount of personal data is automatically gathered on servers by administrations, hospitals and private companies while several security surveys highlight the failure of database servers to keep confidential data really private. The advent of powerful secure tokens, combining the security of smart card microcontrollers with the storage capacity of NAND Flash chips, introduces a credible alternative to the systematic centralization of personal data. By embedding a full-fledged database server in such device, an individual can now store her personal data in her own secure token, kept under her control, and never disclose in clear her private data to the outside untrusted world. This demonstration shows the benefit of the proposed approach in terms of privacy protection and pervasiveness through a healthcare scenario. This scenario is extracted from a field experiment where medical folders embedded in secure tokens are used to improve the coordination of medical care at home for elderly people. The demonstration also highlights interesting features of the embedded DBMS engine introduced to tackle the secure token's strong hardware constraints.

#index 1426632
#* Mask: a system for privacy-preserving policy-based access to published content
#@ Mohamed Nabeel;Ning Shang;John Zage;Elisa Bertino
#t 2010
#c 5
#% 854831
#! We propose to demonstrate Mask, the first system addressing the seemingly-unsolvable problem of how to selectively share contents among a group of users based on access control policies expressed as conditions against the identity attributes of these users while at the same time assuring the privacy of these identity attributes from the content publisher. Mask consists of three entities: a Content Publisher, Users referred to as Subscribers, and Identity Providers that issue certified identity attributes. The content publisher specifies access control policies against identity attributes of subscribers indicating which conditions the identity attributes of a subscriber must verify in order for this subscriber to access a document or a subdocument. The main novelty of Mask is that, even though the publisher is able to match the identity attributes of the subscribers against its own access control policies, the publisher does not learn the values of the identity attributes of the subscribers; the privacy of the authorized subscribers is thus preserved. Based on the specified access control policies, documents are divided into subdocuments and the subdocuments having different access control policies are encrypted with different keys. Subscribers derive the keys corresponding to the subdocuments they are authorized to access. Key distribution in Mask is supported by a novel group key management protocol by which subscribers can reconstruct the decryption keys from the subscription information they receive from the publisher. The publisher however does not learn which decryption keys each subscriber is able to reconstruct. In this demonstration, we show our system using a healthcare scenario.

#index 1426633
#* SimDB: a similarity-aware database system
#@ Yasin N. Silva;Ahmed M. Aly;Walid G. Aref;Per-Ake Larson
#t 2010
#c 5
#% 333973
#% 864392
#% 1054481
#% 1063484
#% 1063496
#% 1206694
#% 1206820
#! The identification and processing of similarities in the data play a key role in multiple application scenarios. Several types of similarity-aware operations have been studied in the literature. However, in most of the previous work, similarity-aware operations are studied in isolation from other regular or similarity-aware operations. Furthermore, most of the previous research in the area considers a standalone implementation, i.e., without any integration with a database system. In this demonstration we present SimDB, a similarity-aware database management system. SimDB supports multiple similarity-aware operations as first-class database operators. We describe the architectural changes to implement the similarity-aware operators. In particular, we present the way conventional operators' implementation machinery is extended to support similarity-aware operators. We also show how these operators interact with other similarity-aware and regular operators. In particular, we show the effectiveness of multiple equivalence rules that can be used to extend cost-based query optimization to the case of similarity-ware operations.

#index 1426634
#* A demonstration of FlexPref: extensible preference evaluation inside the DBMS engine
#@ Justin J. Levandoski;Mohamed F. Mokbel;Mohamed E. Khalefa;Venkateshwar R. Korukanti
#t 2010
#c 5
#% 465167
#% 479816
#% 875012
#% 1022242
#% 1688273
#! This demonstration presents FlexPref, a framework implemented inside the DBMS query processor that enables efficient and extensible preference query processing. FlexPref provides query processing support inside the database engine for a wide-array of preference evaluation methods (e.g., skyline, top-k, k-dominance, k-frequency) in a single extensible code base. Integration with FlexPref is simple, involving the registration of only three functions that capture the essence of the preference method. Once integrated, the preference method "lives" at the core of the database, enabling the efficient execution of preference queries involving common database operations (e.g, selection, join). Functionality of FlexPref, implemented inside PostgreSQL, is demonstrated through the implementation and use of several state-of-the-art preference methods in a real application scenario.

#index 1426635
#* Mining knowledge from databases: an information network analysis approach
#@ Jiawei Han;Yizhou Sun;Xifeng Yan;Philip S. Yu
#t 2010
#c 5
#% 268079
#% 577273
#% 865734
#% 989654
#% 1002279
#% 1081580
#% 1181261
#% 1214701
#% 1214717
#% 1394202
#! Most people consider a database is merely a data repository that supports data storage and retrieval. Actually, a database contains rich, inter-related, multi-typed data and information, forming one or a set of gigantic, interconnected, heterogeneous information networks. Much knowledge can be derived from such information networks if we systematically develop an effective and scalable database-oriented information network analysis technology. In this tutorial, we introduce database-oriented information network analysis methods and demonstrate how information networks can be used to improve data quality and consistency, facilitate data integration, and generate interesting knowledge. This tutorial presents an organized picture on how to turn a database into one or a set of organized heterogeneous information networks, how information networks can be used for data cleaning, data consolidation, and data qualify improvement, how to discover various kinds of knowledge from information networks, how to perform OLAP in information networks, and how to transform database data into knowledge by information network analysis. Moreover, we present interesting case studies on real datasets, including DBLP and Flickr, and show how interesting and organized knowledge can be generated from database-oriented information networks.

#index 1426636
#* Database systems research on data mining
#@ Carlos Ordonez;Javier García-García
#t 2010
#c 5
#% 818916
#% 845220
#% 1278123
#% 1512993
#! Data mining remains an important research area in database systems. We present a review of processing alternatives, storage mechanisms, algorithms, data structures and optimizations that enable data mining on large data sets. We focus on the computation of well-known multidimensional statistical and machine learning models. We pay particular attention to SQL and MapReduce as two competing technologies for large scale processing. We conclude with a summary of solved major problems and open research issues.

#index 1426637
#* Information theory for data management
#@ Divesh Srivastava;Suresh Venkatasubramanian
#t 2010
#c 5
#% 300711
#% 333876
#% 576092
#% 576111
#% 577233
#% 654458
#% 765462
#% 804840
#% 915256
#% 991154
#% 1181220
#% 1206637
#! We explore the use of information theory as a tool to express and quantify notions of information content and information transfer for representing and analyzing data, using examples from database design, data integration and data anonymization. We also examine the computational challenges associated with information-theoretic primitives, indicating how they might be computed efficiently.

#index 1426638
#* Enterprise information extraction: recent developments and open challenges
#@ Laura Chiticariu;Yunyao Li;Sriram Raghavan;Frederick R. Reiss
#t 2010
#c 5
#% 875064
#% 1183368
#% 1183369
#% 1183370
#% 1183371
#% 1206810
#! Information extraction (IE) - the problem of extracting structured information from unstructured text - has become an increasingly important topic in recent years. A SIGMOD 2006 tutorial [3] outlined challenges and opportunities for the database community to advance the state of the art in information extraction, and posed the following grand challenge: "Can we build a System R for information extraction? Our tutorial gives an overview of progress the database community has made towards meeting this challenge. In particular, we start by discussing design requirements in building an enterprise IE system. We then survey recent technological advances towards addressing these requirements, broadly categorized as: (1) Languages for specifying extraction programs in a declarative way, thus allowing database-style performance optimizations; (2) Infrastructure needed to ensure scalability, and (3) Development support for enterprise IE systems. Finally, we outline several open challenges and opportunities for the database community to further advance the state of the art in enterprise IE systems. The tutorial is intended for students and researchers interested in information extraction and its applications, and assumes no prior knowledge of the area.

#index 1426639
#* Crowds, clouds, and algorithms: exploring the human side of "big data" applications
#@ Sihem Amer-Yahia;AnHai Doan;Jon Kleinberg;Nick Koudas;Michael Franklin
#t 2010
#c 5
#% 835045
#% 885438
#% 924327
#% 1063533
#% 1328172
#% 1355046
#% 1425621
#% 1612964

#index 1429791
#* Proceedings of the Fourth SIGMOD PhD Workshop on Innovative Database Research
#@ Mirella M. Moro;Zografoula Vagena
#t 2010
#c 5
#! The fourth SIGMOD Ph.D. Workshop on Innovative Database Research (IDAR), which is co-located with the 2010 ACM SIGMOD/PODS Conference, was held in Indianapolis, USA on June 11, 2010. The workshop provides a forum for Ph.D. students, who are working on topics related to the SIGMOD conference series, to present, discuss, and receive feedback on their research. This years workshop features 9 paper presentations which were selected from a total of 16 research submissions. In addition, the workshop program includes a keynote address by Divesh Srivastava.

#index 1429792
#* Building a power-aware database management system
#@ Zichen Xu
#t 2010
#c 5
#% 384405
#% 1127556
#% 1134501
#% 1213693
#% 1332763
#% 1426521
#! In today's large-scale data centers, energy costs (i.e., the electricity bill) are projected to outgrow that of hardware. Despite a long history of research in energy-saving techniques, especially low-power hardware, little work has been done to improve the power efficiency of data management software. Power-aware computing research at the application level has been found to be synergistic to that at the hardware and OS levels because it can provide more opportunities for energy reduction in the underlying systems. This paper describes the author's thesis work on creating a power-aware database management (P-DBMS) and initial ideas on the design of such systems, with the focus on a power-aware query optimization module inside the DBMS. We discuss the main technical challenges in designing the optimizer and present our strategies to meet such challenges. We focus our discussions on a power model to accurately measure the energy costs of query executions plans, and a cost evaluation model for plan selection. An important feature of this work is the formal control-theoretic methods we use to model and optimize the database towards the performance and energy saving goals. This rigorous design methodology is in sharp contrast to heuristic-based adaptive solutions that rely on extensive empirical evaluation and manual tuning. Our experiments using a power-aware query optimizer under our initial design show that there exist significant potential in power/energy savings.

#index 1429793
#* Event sequence processing: new models and optimization techniques
#@ Mo Liu;Elke A. Rundensteiner
#t 2010
#c 5
#% 14513
#% 210182
#% 223781
#% 480964
#% 481448
#% 481604
#% 519614
#% 578391
#% 846209
#% 875004
#% 893157
#% 1063518
#% 1217161
#% 1255307
#% 1263930
#! Many modern applications, including online financial feeds, tag-based mass transit systems and RFID-based supply chain management systems transmit real-time data streams. There is a need for a special-purpose event stream processing technology to analyze this vast amount of sequential multi-dimensional data to enable online, operational decision making. Existing techniques such as traditional online analytical processing (OLAP) systems are not designed for real-time pattern-based operations, while state-of-the-art Complex Event Processing (CEP) systems designed for sequence detection do not support OLAP operations. Supporting complex pattern queries at different concept and pattern hierarchies must be devised by providing efficient computation and data sharing. In this dissertation, we propose a novel E-Cube model that combines CEP and OLAP techniques for multi-dimensional event pattern analysis at different abstraction levels. Further, we go beyond the linear sequence pattern queries targeted by ECube core system towards supporting an expressive composite pattern query language (composed of arbitrarily nested sequence, negation, recursion, AND and OR operators) to express powerful pattern matching requests.

#index 1429794
#* Exploiting locality for query processing and compression in scientific databases
#@ Anand Kumar;Yi-Cheng Tu
#t 2010
#c 5
#% 193743
#% 300412
#% 392735
#% 479984
#% 572308
#% 632026
#% 729437
#% 739871
#% 864642
#% 918408
#% 1207017
#% 1346416
#! Improvements in the efficiency of scientific simulations have lead to requirements of large databases. The data captured in such simulations is of large scale and poses challenges in storage, transfer and query processing. However, the data are collected every fraction of a second, storing some redundant information. Thus, the temporal and spatial locality of the data gives us an opportunity to store and transfer over networks efficiently. The data locality also helps in efficiently processing complex analytical queries that are popular in scientific databases. Many scientific data analysis queries involve more than one object/body of interest. Processing such queries pose super-linear computational complexity. In this paper, we propose preliminary solutions to some of these problems along with initial results. Mainly, we try to exploit the spatial and temporal proximity of the data to achieve high levels of compression for efficient storage and analytical query processing.

#index 1429795
#* Improved approaches to mine rare association rules in transactional databases
#@ R. Uday Kiran;P. Krishna Reddy
#t 2010
#c 5
#% 152934
#% 280487
#% 320944
#% 729418
#% 893373
#% 937941
#% 1035815
#% 1697220
#! Rare association rules are the association rules consisting of rare items. It is difficult to mine rare association rules with the single minimum support based approaches such as Apri-ori and FP-growth as they suffer from rare item problem. In the literature, efforts has been made to extract rare association rules with multiple minimum supports. It was observed that the multiple minimum supports-based approach still suffers from performance problems. As a part of proposed work, we have analyzed the multiple minimum supports-based approach and proposed improved approaches for extracting rare association rules. Experimental results show that the proposed approaches are efficient.

#index 1429796
#* Multiple relationship based deduplication
#@ Pei Li
#t 2010
#c 5
#% 406493
#% 577273
#% 768938
#% 769887
#% 810014
#% 819550
#% 850430
#% 913783
#% 915273
#% 915340
#% 937552
#% 1201863
#% 1274906
#! Deduplication refers to the task of finding instances that refer to the same entity in a given table. Several techniques have been presented based on a pairwise comparison and a typical result is the definition of three sets of records i) pairwise records that definitively match, ii) pairwise records that definitively do not match, and iii) pairwise records that possibly match. In this paper we present a general approach for domain independent duplicate problems by means of the knowledge stored in the schema where the analyzed table is included. According to the different kinds of relationships, we propose strategies to build and compare the knowledge networks by means of graph-based similarity. Final similarity decision given different relationship categories is carried out by exploiting two probabilistic logic models.

#index 1429797
#* Put all eggs in one basket: an OLTP and OLAP database approach for traceability data
#@ Veneta Dobreva;Martina-Cezara Albutiu
#t 2010
#c 5
#% 69258
#% 644230
#% 864470
#% 915814
#% 1022298
#% 1063491
#% 1217145
#% 1366460
#! Accurate tracking and tracing of moving objects is an emerging trend in vertical industries like retail, logistics, and manufacturing. In order to monitor objects in business processes, more and more companies are deploying upcoming technologies like Radio Frequency Identification (RFID). Therefore, modern databases have to be able to cope with the challenges originating from the specifics of traceability data: efficient incremental update as well as efficient transactional and analytic ad-hoc querying and efficient storage of the data. Another requirement of business intelligence applications is to provide "real world awareness"[7] by using the latest information in the descision-making process. We therefore present an approach for efficient storing and managing of traceability data (on the example of RFID data), where the OLAP and OLTP components reside in one database and which meets the defined challenges. We discuss and analyze the experimental results and lessons learned and take them as a basis for our future research direction.

#index 1429798
#* Specification and verification of web services transactions
#@ Iman Saleh;Gregory Kulczycki;M. Brian Blake
#t 2010
#c 5
#% 81935
#% 197631
#% 215954
#% 248107
#% 323377
#% 643940
#% 805369
#% 864419
#% 1286358
#% 1700106
#! Research in transactions planning has recognized the evolvement of Web Services as an industry standard to implement transactional business processes. Web transactions are formed by integrating services in an ad-hoc manner. Distributed transaction protocols may be used to ensure data integrity. However, these protocols require some level of coordinated transaction management. Moreover, individual services must be transaction-aware in order to support necessary compensation operations. These assumptions are unrealistic in the case of the Web. We propose a data modeling and contracting framework for Web services that facilitates the verification of data integrity properties in ad-hoc transactions. The proposed framework enables services' integrator to plan their transactions while ensuring data integrity conditions.

#index 1429799
#* Statistical modeling of large distribution sets
#@ Yasuko Matsubara;Yasushi Sakurai;Masatoshi Yoshikawa
#t 2010
#c 5
#% 86950
#% 280819
#% 296738
#% 420077
#% 480133
#% 989629
#% 1273827
#! In this paper we deal with a ubiquitous problem in data management: hierarchical model estimation for large distribution sets. This particular problem arises in many applications. Classification, top-k query processing, clustering and outlier detection are just a few possible applications. Our aim is to continuously and incrementally estimate the model parameters of 'typical' distributions that describe the characteristics of a database. Our approach to model estimation can handle arbitrary types of data (e.g., categorical and numerical data) in databases, incrementally, quickly, and with little resource consumption. Moreover, this paper proposes not only incremental algorithms for model fitting, but also a modeling framework in which the learning approach recognizes hierarchical groups, each of whose distributions has similar characteristics, and separately updates the model parameters of each group without scanning all the distributions in the database. Thus, it can provide a response, i.e., the parameters of typical distribution models, with an arbitrary level of granularity, at any time. Just as importantly, we demonstrate the utility of our approach by showing how it can be applied to two specific problems that arise in the context of data management.

#index 1429800
#* Unsupervised strategies for information extraction by text segmentation
#@ Eli Cortez;Altigran S. da Silva
#t 2010
#c 5
#% 333943
#% 464434
#% 466263
#% 531459
#% 769877
#% 864416
#% 864432
#% 874707
#% 948374
#% 967276
#% 1022260
#% 1166537
#% 1202077
#% 1426569
#! Information extraction by text segmentation (IETS) applies to cases in which data values of interest are organized in implicit semi-structured records available in textual sources (e.g. postal addresses, bibliographic information, ads). It is an important practical problem that has been frequently addressed in the recent literature. We report here partial results from a PhD thesis work in which we introduce ONDUX (On Demand Unsupervised Information Extraction), a new unsupervised probabilistic approach for IETS. As other unsupervised IETS approaches, ONDUX relies on information available on pre-existing data to associate segments in the input string with attributes of a given domain. Unlike other approaches, we rely on very effective matching strategies instead of explicit learning strategies. The effectiveness of this matching strategy is also exploited to disambiguate the extraction of certain attributes through a reinforcement step that explores sequencing and positioning of attribute values directly learned on-demand from test data, with no previous human-driven training, a feature unique to ONDUX. This assigns to ONDUX a high degree of flexibility and results in superior effectiveness, as demonstrated by experimental evaluation we have carried out with textual sources from different domains, in which ONDUX is compared with a state-of-art IETS approach.

#index 1429801
#* Weighted set similarity: queries and updates
#@ Divesh Srivastava
#t 2010
#c 5
#! Consider a universe of items, each of which is associated with a weight, and a database consisting of subsets of these items. Given a query set, a weighted set similarity query identifies either (i) all sets in the database whose similarity to the query set is above a pre-specified threshold, or (ii) the sets in the database with the k highest similarity values to the query set. Weighted set similarity queries are useful in applications like data cleaning and integration for finding approximate matches in the presence of typographical mistakes, multiple formatting conventions, transformation errors, etc. We show that this problem has semantic properties that can be exploited to design index structures that support efficient algorithms for answering queries; these algorithms can achieve arbitrarily stronger pruning than the family of Threshold Algorithms. We describe how these index structures can be efficiently updated using lazy propagation in a way that gives strict guarantees on the quality of subsequent query answers. Finally, we illustrate that our proposed ideas work well in practice for real datasets.

#index 1433965
#* A hybrid model driven development framework for the multidimensional modeling of data warehouses!
#@ Jose-Norberto Mazón;Juan Trujillo
#t 2009
#c 5
#% 308444
#% 614523
#% 630866
#% 720186
#% 881922
#% 896022
#% 907409
#% 945874
#% 998906
#% 1016601
#% 1050406
#% 1050408
#% 1349272
#% 1409368
#% 1409434
#% 1692950
#% 1733295
#! Developing a multidimensional (MD) model of a data warehouse (DW) is a highly complex, prone to fail, and time consuming task, due to the fact that (i) the information needs of decision makers and the available operational data sources that will populate the DW must both be considered in a conceptual MD model, and (ii) complex mappings must be performed to obtain an implementation of this conceptual MD model. However, no significant effort has been made to take these issues into account in a systematic, well structured and comprehensive development process. To overcome the lack of such a process, a framework based on the Model Driven Architecture (MDA) is proposed for the development of a hybrid MD model at the conceptual level and for the automatic derivation of its logical representation. Also, a running example is shown throughout this paper.

#index 1433966
#* Machine models for query processing
#@ Nicole Schweikardt
#t 2009
#c 5
#% 41684
#% 101797
#% 136740
#% 189744
#% 238182
#% 248023
#% 278835
#% 293705
#% 339937
#% 341100
#% 378388
#% 393844
#% 397375
#% 449224
#% 570879
#% 576108
#% 598376
#% 761484
#% 785130
#% 791182
#% 809255
#% 847113
#% 894646
#% 958231
#% 963308
#% 976988
#% 982759
#% 1023420
#% 1039655
#% 1069562
#% 1141470
#% 1181328
#% 1197989
#% 1206666
#% 1376720
#% 1408535
#% 1661446

#index 1433967
#* XML: some papers in a haystack
#@ Mirella M. Moro;Vanessa Braganholo;Carina F. Dorneles;Denio Duarte;Renata Galante;Ronaldo S. Mello
#t 2009
#c 5
#% 116303
#% 248799
#% 269079
#% 273922
#% 333979
#% 333981
#% 333990
#% 378409
#% 378412
#% 397359
#% 397366
#% 397373
#% 397375
#% 411759
#% 428146
#% 479327
#% 479956
#% 480152
#% 480645
#% 480657
#% 481125
#% 487263
#% 488456
#% 488868
#% 536351
#% 551859
#% 570875
#% 574817
#% 577353
#% 584936
#% 654450
#% 654492
#% 654493
#% 745467
#% 766669
#% 772031
#% 791181
#% 810044
#% 810071
#% 810083
#% 810116
#% 810117
#% 810119
#% 824146
#% 824667
#% 824668
#% 824676
#% 830529
#% 836149
#% 844207
#% 848763
#% 866985
#% 866988
#% 875010
#% 881733
#% 894437
#% 894440
#% 902730
#% 956718
#% 993941
#% 993953
#% 1015270
#% 1016134
#% 1016152
#% 1016182
#% 1019089
#% 1053456
#% 1063575
#% 1127605
#% 1215804
#% 1393700
#% 1422705
#% 1688289
#% 1698664
#% 1915875
#% 1915878
#! XML has been explored by both research and industry communities. More than 5500 papers were published on different aspects of XML. With so many publications, it is hard for someone to decide where to start. Hence, this paper presents some of the research topics on XML, namely: XML on relational databases, query processing, views, data matching, and schema evolution. It then summarizes some (some!) of the most relevant or traditional papers on those subjects.

#index 1433968
#* ASSET queries: a declarative alternative to MapReduce
#@ Damianos Chatziantoniou;Elias Tzortzakakis
#t 2009
#c 5
#% 169846
#% 333926
#% 458872
#% 465170
#% 482082
#% 654445
#% 654454
#% 824697
#% 873339
#% 945699
#% 963669
#% 1063553
#% 1206925
#% 1207196
#% 1217159
#% 1328186
#! Today's complex world requires state-of-the-art data analysis over truly massive data sets. These data sets can be stored persistently in databases or flat files, or can be generated in realtime in a continuous manner. An associated set is a collection of data sets, annotated by the values of a domain D. These data sets are populated using a data source according to a condition θ and the annotated value. An ASsociated SET (ASSET) query consists of repeated, successive, interrelated definitions of associated sets, put together in a column-wise fashion, resembling a spreadsheet document. We present DataMingler, a powerful GUI to express and manage ASSET queries, data sources and aggregate functions and the ASSET Query Engine (QE) to efficiently evaluate ASSET queries. We argue that ASSET queries: a) constitute a useful class of OLAP queries, b) are suitable for distributed processing settings, and c) extend the MapReduce paradigm in a declarative way.

#index 1433969
#* Peter Buneman speaks out on phylogeny, the integration of databases and programming languages, curated databases, british plumbing, the value of talking to users, when to ignore the literature, and more
#@ Marianne Winslett
#t 2009
#c 5

#index 1433970
#* Report on the 10th international workshop on web information and data management (WIDM)
#@ Chee-Yong Chan;Neoklis Polyzotis
#t 2009
#c 5

#index 1433971
#* Report on workshop on operating systems support for next generation large scale NVRAM (NVRAMOS 2009)
#@ Sang-Won Lee;Sooyong Kang;Youjip Won;Jongmoo Choi
#t 2009
#c 5
#% 960238
#% 1085291
#% 1134703
#% 1183354
#% 1213668

#index 1433972
#* Workshop on theory and practice of provenance event report
#@ James Cheney
#t 2009
#c 5
#% 263441
#% 1189360
#% 1850591
#! Provenance, or metadata about the creation, influences upon, or other history of objects or data, has attracted attention in a wide variety of contexts in computer science over the last few years. This event report describes a recent workshop on "Theory and Practice of Provenance", intended as a forum for presenting novel ideas about provenance and encouraging interaction among provenance researchers.

#index 1433973
#* Sense the physical, walkthrough the virtual, manage the co (existing) spaces: a database perspective
#@ Beng Chin Ooi;Kian Lee Tan;Anthony Tung
#t 2010
#c 5
#% 249985
#% 480830
#% 571217
#% 640616
#% 742550
#% 778727
#% 788219
#% 793899
#% 806214
#% 863399
#% 874970
#% 889142
#% 925317
#% 960236
#% 963669
#% 975457
#% 981608
#% 1015259
#% 1016193
#% 1021194
#% 1026964
#% 1036083
#% 1063471
#% 1063490
#% 1063542
#% 1063567
#% 1126565
#% 1127354
#% 1206600
#% 1247816
#% 1504736
#! In a co-space environment, the physical space and the virtual space co-exist, and interact simultaneously. While the physical space is virtually enhanced with information, the virtual space is continuously refreshed with real-time, real-world information. To allow users to process and manipulate information seamlessly between the real and digital spaces, novel technologies must be developed. These include smart interfaces, new augmented realities, efficient storage and data management and dissemination techniques. In this paper, we first discuss some promising co-space applications. These applications offer experiences and opportunities that neither of the spaces can realize on its own. We then argue that the database community has much to offer to this field. Finally, we present several challenges that we, as a community, can contribute towards managing the co-space.

#index 1433974
#* A time efficient indexing scheme for complex spatiotemporal retrieval
#@ G. Lagogiannis;N. Lorentzos;S. Sioutas;E. Theodoridis
#t 2010
#c 5
#% 56081
#% 86950
#% 102809
#% 252304
#% 279483
#% 287070
#% 315005
#% 443130
#% 480473
#% 503882
#% 571296
#% 824724
#% 1128078
#! The paper is concerned with the time efficient processing of spatiotemporal predicates, i.e. spatial predicates associated with an exact temporal constraint. A set of such predicates forms a buffer query or a Spatio-temporal Pattern (STP) Query with time. In the more general case of an STP query, the temporal dimension is introduced via the relative order of the spatial predicates (STP queries with order). Therefore, the efficient processing of a spatiotemporal predicate is crucial for the efficient implementation of more complex queries of practical interest. We propose an extension of a known approach, suitable for processing spatial predicates, which has been used for the efficient manipulation of STP queries with order. The extended method is supported by efficient indexing structures. We also provide experimental results that show the efficiency of the technique.

#index 1433975
#* Composition and inversion of schema mappings
#@ Marcelo Arenas;Jorge Pérez;Juan Reutter;Cristian Riveros
#t 2010
#c 5
#% 663
#% 198465
#% 237190
#% 248038
#% 328424
#% 378409
#% 481923
#% 572307
#% 572311
#% 598376
#% 762652
#% 765540
#% 778122
#% 806215
#% 810021
#% 826032
#% 850730
#% 874882
#% 893094
#% 927032
#% 960233
#% 976997
#% 997492
#% 1015302
#% 1036084
#% 1039061
#% 1054485
#% 1063710
#% 1063712
#% 1063723
#% 1092008
#% 1217116
#% 1217117
#% 1270567
#% 1328194
#% 1328195

#index 1433976
#* Database encryption: an overview of contemporary challenges and design considerations
#@ Erez Shmueli;Ronen Vaisenberg;Yuval Elovici;Chanan Glezer
#t 2010
#c 5
#% 226775
#% 286849
#% 287636
#% 379248
#% 397367
#% 408140
#% 495406
#% 664705
#% 725292
#% 726926
#% 765448
#% 963384
#% 993942
#% 994006
#% 1706188
#% 1725666
#! This article describes the major challenges and design considerations pertaining to database encryption. The article first presents an attack model and the main relevant challenges of data security, encryption overhead, key management, and integration footprint. Next, the article reviews related academic work on alternative encryption configurations pertaining to encryption locus; indexing encrypted data; and key management. Finally, the article concludes with a benchmark using the following design criteria: encryption configuration, encryption granularity and keys storage.

#index 1433977
#* Spatio-temporal database research at the University of Melbourne
#@ Egemen Tanin;Rui Zhang;Lars Kulik
#t 2010
#c 5
#% 800518
#% 852295
#% 946433
#% 1127420
#% 1127438
#% 1206668
#% 1206682
#% 1208224
#% 1380973
#% 1399015
#% 1408837
#% 1422720
#% 1441108
#% 1442466
#% 1719090
#! The spatio-temporal database research group at the University of Melbourne focuses on introducing new techniques for distributed systems such as mobile and ubiquitous systems as well as P2P networks. Our approach is to exploit the spatial and temporal nature of data and queries such as motion characteristics. In this article, we discuss four major themes and projects of the group: (i) nearest neighbor queries, (ii) temporal data processing and continuous queries, (iii) P2P spatial data management, and (iv) location privacy. The group is supported by funding from the Australian Research Council and the National ICT Australia Victoria Research Laboratory.

#index 1433978
#* Repeatability & workability evaluation of SIGMOD 2009
#@ S. Manegold;I. Manolescu;L. Afanasiev;J. Feng;G. Gou;M. Hadjieleftheriou;S. Harizopoulos;P. Kalnis;K. Karanasos;D. Laurent;M. Lupu;N. Onose;C. Ré;V. Sans;P. Senellart;T. Wu;D. Shasha
#t 2010
#c 5
#% 1061894
#% 1181320
#! SIGMOD 2008 was the first database conference that offered to test submitters' programs against their data to verify the repeatability of the experiments published [1]. Given the positive feedback concerning the SIGMOD 2008 repeatability initiative, SIGMOD 2009 modified and expanded the initiative with a workability assessment.

#index 1433979
#* Logic in databases: report on the LID 2008 workshop
#@ Andrea Calì;Laks V.S. Lakshmanan;Davide Martinenghi
#t 2010
#c 5
#% 54225
#% 264858
#% 273687
#% 378409
#% 454467
#% 473117
#% 481786
#% 576116
#% 665856
#% 801692
#% 806991
#% 826029
#% 941786
#% 976987
#% 992962
#% 993957
#% 1073322
#% 1269866
#% 1347336
#% 1408524
#% 1409380
#% 1656400
#% 1661435
#% 1666173
#% 1667784
#% 1684011
#% 1700140
#% 1728683
#% 1730008

#index 1433980
#* SIGMOD 2009 best demonstration competition
#@ Björn Pór Jónsson
#t 2010
#c 5
#% 845357
#% 1217257
#% 1217260
#% 1217261
#! This report summarizes the best demonstration competition held during SIGMOD 2009. I first outline the evaluation process and then briefly describe the three best demonstrations. My conclusion is that the competition was a success and I hope that future demonstrations chairs will turn it into an established SIGMOD tradition.

#index 1433981
#* XQJ: XQuery Java API is completed
#@ Marc Van Cappellen;Zhen Hua Liu;Jim Melton;Maxim Orgiyan
#t 2010
#c 5
#% 136740
#% 778480
#% 781453
#% 810036
#% 810083
#% 824751
#% 875028
#% 1127567
#! Just as SQL is a declarative language for querying relational data, XQuery is a declarative language for querying XML. JDBC provides a standard Java API to interact with a variety of SQL engines to declaratively access and manipulate data stored in relational data sources. Similarly, XQJ provides a standard Java API to interact with a variety of XQuery engines to declaratively access and manipulate XML data in variety of XML data sources. XQJ, also known as JSR 225, is designed through the Java Community Process (JCP) [20]. The XQJ specification defines a set of Java interfaces and classes that enable a Java program to submit XQuery expressions to an XQuery engine operating on XML data sources and to consume XQuery results. In this article, we discuss the XQJ APl's technical details with its similarities; and differences from JDBC, the design philosophies and goals for XQJ, the implementations strategies of XQJ in variety of XQuery engines and their operating environments, and the possible future of XQJ.

#index 1433982
#* The ubiquitous DBMS
#@ Kyu-Young Whang;Il-Yeol Song;Taek-Yoon Kim;Ki-Hoon Lee
#t 2010
#c 5
#% 300179
#% 641769
#% 778476
#% 800613
#% 806214
#% 824668
#% 829901
#% 830699
#% 835866
#% 845359
#% 875010
#% 893215
#% 946438
#% 960238
#% 960367
#% 963650
#% 994564
#% 1063551
#% 1087239
#% 1117702
#% 1127391
#% 1181263
#% 1426532
#% 1447701
#% 1669499
#! Advancement in mobile computing technologies has prompted strong needs for database systems that can be used in small devices such as sensors, cellular phones, PDAs, car navigators, and Ultra Mobile PCs (UMPCs). We term the database systems that are customizable for small computing devices as Ubiquitous Database Management Systems (UDBMSs). In this paper, we first review the requirements of the UDBMS. The requirements identified include lightweight DBMSs, selective convergence, flash-optimized storage systems, data synchronization, support of unstructured/semistructured data, complex database operations, selfmanagement, and security. Next, we review existing systems and research prototypes. We review the functionality of UDBMSs including the footprint size, support of standard SQL, transaction management, concurrency control, recovery, indexing, and access control. We then review the supportability of the requirements by those UDBMSs surveyed. We finally present research issues related to the UDBMS.

#index 1433983
#* Relational processing of RDF queries: a survey
#@ Sherif Sakr;Ghazi Al-Naymat
#t 2010
#c 5
#% 286258
#% 434057
#% 480153
#% 519567
#% 572310
#% 783540
#% 823646
#% 824697
#% 824755
#% 851283
#% 864445
#% 960302
#% 1016150
#% 1046418
#% 1063533
#% 1127402
#% 1127431
#% 1127610
#% 1190676
#% 1206875
#% 1217194
#% 1241185
#! The Resource Description Framework (RDF) is a flexible model for representing information about resources in the web. With the increasing amount of RDF data which is becoming available, efficient and scalable management of RDF data has become a fundamental challenge to achieve the SemanticWeb vision. The RDF model has attracted the attention of the database community and many researchers have proposed different solutions to store and query RDF data efficiently. This survey focuses on using relational query processors to store and query RDF data. We provide an overview of the different approaches and classify them according to their storage and query evaluation strategies.

#index 1433984
#* Social sites research through CourseRank
#@ Benjamin Bercovitz;Filip Kaliszan;Georgia Koutrika;Henry Liou;Aditya Parameswaran;Petros Venetis;Zahra Mohammadi Zadeh;Hector Garcia-Molina
#t 2010
#c 5
#% 428272
#% 452563
#% 813966
#% 855601
#% 869525
#% 869608
#% 956516
#% 1055736
#% 1055739
#% 1181246
#% 1217203
#! Social sites such as FaceBook, Orkut, Flickr, MySpace and many others have become immensely popular. At these sites, users share their resources (e.g., photos, profiles, blogs) and learn from each other. On the other hand, higher education applications help students and administrators track and manage academic information such as grades, course evaluations and enrollments. Despite the importance of both these areas, there is relatively little research on the mechanisms that make them effective. Apart from being both a successful social site and an academic planning site, CourseRank provides a live testbed for studying fundamental questions related to social networking, academic planning, and the fusion of these areas. In this paper, we provide a system overview and our main research efforts through CourseRank.

#index 1433985
#* On energy management, load balancing and replication
#@ Willis Lang;Jignesh M. Patel;Jeffrey F. Naughton
#t 2010
#c 5
#% 43172
#% 462779
#% 480281
#% 723288
#% 764396
#% 805469
#% 870120
#% 873244
#% 884945
#% 963628
#% 993444
#% 1034471
#% 1373699
#% 1468291
#! In this paper we investigate some opportunities and challenges that arise in energy-aware computing in a cluster of servers running data-intensive workloads. We leverage the insight that servers in a cluster are often underutilized, which makes it attractive to consider powering down some servers and redistributing their load to others. Of course, powering down servers naively will render data stored only on powered down servers inaccessible. While data replication can be exploited to power down servers without losing access to data, unfortunately, care must be taken in the design of the replication and server power down schemes to avoid creating load imbalances on the remaining "live" servers. Accordingly, in this paper we study the interaction between energy management, load balancing, and replication strategies for data-intensive cluster computing. In particular, we show that Chained Declustering -- a replication strategy proposed more than 20 years ago -- can support very flexible energy management schemes.

#index 1433986
#* Third int'l workshop on "personalized access, profile management, and context awareness in databases" (PersDB 2009)
#@ Sihem Amer-Yahia;Georgia Koutrika
#t 2010
#c 5
#% 800588
#% 993957
#% 1207005
#% 1328172

#index 1433987
#* Second workshop on very large digital libraries: in conjunction with the european conference on digital libraries Corfu, Greece, 2 october 2009
#@ Paolo Manghi;Pasquale Pagano;Yannis Ioannidis
#t 2010
#c 5

#index 1433988
#* 5th international workshop on networking meets databases (NetDB 2009)
#@ Boon Thau Loo;Stefan Saroiu
#t 2010
#c 5

#index 1448943
#* Proceedings of the 1st International Workshop on Workflow Approaches to New Data-centric Science
#@ 
#t 2010
#c 5

#index 1452238
#* Warehouse-scale Computing
#@ Luiz Andre Barroso
#t 2010
#c 5

#index 1452241
#* 2010 SIGMOD Awards Presentations
#@ Ahmed Elmagarmid
#t 2010
#c 5

#index 1453004
#* Proceedings of the Third International Workshop on Testing Database Systems
#@ Shivnath Babu;Glenn Paulley
#t 2010
#c 5

#index 1471585
#* Procceedings of the 13th International Workshop on the Web and Databases
#@ Xin Luna Dong;Felix Naumann
#t 2010
#c 5

#index 1472960
#* The declarative imperative: experiences and conjectures in distributed logic
#@ Joseph M. Hellerstein
#t 2010
#c 5
#% 43162
#% 53388
#% 181027
#% 190745
#% 227883
#% 271908
#% 277328
#% 300169
#% 320187
#% 340635
#% 342377
#% 427287
#% 427290
#% 477353
#% 481101
#% 481597
#% 532638
#% 569762
#% 598376
#% 629097
#% 635132
#% 664709
#% 723295
#% 723446
#% 731499
#% 742697
#% 761220
#% 761222
#% 809267
#% 821939
#% 834458
#% 835186
#% 874978
#% 879080
#% 938773
#% 963875
#% 997709
#% 998845
#% 1001187
#% 1016165
#% 1019705
#% 1022282
#% 1035194
#% 1083582
#% 1127442
#% 1164503
#% 1172464
#% 1206859
#% 1213944
#% 1246527
#% 1254370
#% 1272157
#% 1328129
#% 1350321
#% 1350322
#% 1372690
#% 1426442
#% 1450099
#% 1484141
#% 1523889
#! The rise of multicore processors and cloud computing is putting enormous pressure on the software community to find solutions to the difficulty of parallel and distributed programming. At the same time, there is more--and more varied--interest in data-centric programming languages than at any time in computing history, in part because these languages parallelize naturally. This juxtaposition raises the possibility that the theory of declarative database query languages can provide a foundation for the next generation of parallel and distributed programming languages. In this paper I reflect on my group's experience over seven years using Datalog extensions to build networking protocols and distributed systems. Based on that experience, I present a number of theoretical conjectures that may both interest the database community, and clarify important practical issues in distributed computing. Most importantly, I make a case for database researchers to take a leadership role in addressing the impending programming crisis. This is an extended version of an invited lecture at the ACM PODS 2010 conference [32].

#index 1472961
#* Querying RDF streams with C-SPARQL
#@ Davide Francesco Barbieri;Daniele Braga;Stefano Ceri;Emanuele Della Valle;Michael Grossniklaus
#t 2010
#c 5
#% 300179
#% 443298
#% 725366
#% 788216
#% 878299
#% 907520
#% 930938
#% 956564
#% 1008060
#% 1015296
#% 1016170
#% 1039360
#% 1051379
#% 1060608
#% 1069149
#% 1075756
#% 1320449
#% 1332529
#% 1372719
#% 1413157
#% 1673562
#% 1737582
#% 1737596
#! Continuous SPARQL (C-SPARQL) is a new language for continuous queries over streams of RDF data. CSPARQL queries consider windows, i.e., the most recent triples of such streams, observed while data is continuously flowing. Supporting streams in RDF format guarantees interoperability and opens up important applications, in which reasoners can deal with knowledge evolving over time. Examples of such application domains include real-time reasoning over sensors, urban computing, and social semantic data. In this paper, we present the C-SPARQL language extensions in terms of both syntax and examples. Finally, we discuss existing applications that already use C-SPARQL and give an outlook on future research opportunities.

#index 1472962
#* Beyond isolation: research opportunities in declarative data-driven coordination
#@ Lucja Kot;Nitin Gupta;Sudip Roy;Johannes Gehrke;Christoph Koch
#t 2010
#c 5
#% 50077
#% 207210
#% 214046
#% 274912
#% 284550
#% 295410
#% 300166
#% 394417
#% 754139
#% 960297
#! There are many database applications that require users to coordinate and communicate. Friends want to coordinate travel plans, students want to jointly enroll in the same set of courses, and busy professionals want to coordinate their schedules. These tasks are difficult to program using existing abstractions provided by database systems because in addition to the traditional ACID properties provided by the system they all require some type of coordination between users. This is fundamentally incompatible with isolation in the classical ACID properties. In this position paper, we argue that it is time for the database community to look beyond isolation towards principled and elegant abstractions that allow for communication and coordination between some notion of (suitably generalized) transactions. This new area of declarative data-driven coordination (D3C) is motivated by many novel applications and is full of challenging research problems. We survey existing abstractions in database systems and explain why they are insufficient for D3C, and we outline a plethora of exciting research problems.

#index 1472963
#* Understanding deep web search interfaces: a survey
#@ Ritu Khare;Yuan An;Il-Yeol Song
#t 2010
#c 5
#% 330782
#% 331011
#% 357885
#% 480479
#% 577319
#% 654459
#% 762334
#% 765409
#% 765410
#% 777930
#% 806628
#% 842032
#% 864615
#% 955762
#% 976685
#% 1016163
#% 1093191
#% 1127405
#% 1127557
#% 1131022
#% 1292467
#% 1328136
#% 1396734
#% 1406965
#% 1917687
#! This paper presents a survey on the major approaches to search interface understanding. The Deep Web consists of data that exist on the Web but are inaccessible via text search engines. The traditional way to access these data, i.e., by manually filling-up HTML forms on search interfaces, is not scalable given the growing size of Deep Web. Automatic access to these data requires an automatic understanding of search interfaces. While it is easy for a human to perceive an interface, machine processing of an interface is challenging. During the last decade, several works addressed the automatic interface understanding problem while employing a variety of understanding strategies. This paper presents a survey conducted on the key works. This is the first survey in the field of search interface understanding. Through an exhaustive analysis, we organize the works on a 2-D graph based on the underlying database information extracted and based on the technique employed.

#index 1472964
#* Search result diversification
#@ Marina Drosou;Evaggelia Pitoura
#t 2010
#c 5
#% 177422
#% 262112
#% 397133
#% 800588
#% 805841
#% 879686
#% 1074133
#% 1127465
#% 1166473
#% 1181244
#% 1190093
#% 1206662
#% 1269314
#% 1328120
#% 1328135
#% 1372731
#% 1845364
#! Result diversification has recently attracted much attention as a means of increasing user satisfaction in recommender systems and web search. Many different approaches have been proposed in the related literature for the diversification problem. In this paper, we survey, classify and comparatively study the various definitions, algorithms and metrics for result diversification.

#index 1472965
#* SmartCIS: integrating digital and physical environments
#@ Mengmeng Liu;Svilen R. Mihaylov;Zhuowei Bao;Marie Jacob;Zachary G. Ives;Boon Thau Loo;Sudipto Guha
#t 2010
#c 5
#% 378409
#% 479452
#% 654482
#% 731480
#% 874976
#% 878299
#% 1015281
#% 1016271
#% 1026962
#% 1063587
#% 1083759
#% 1217261
#% 1464044
#% 1523908
#! With the increasing adoption of networked sensors, a new class of applications is emerging that combines data from the "digital world" with real-time sensor readings, in order to intelligently manage physical environments and systems (e.g., "smart" buildings, power grids, data centers). This leads to new challenges in providing programmability, performance, extensibility, and multi-purpose heterogeneous data acquisition. The ASPEN project addresses these challenges by extending data integrationtechniques to the distributed stream world, and adding new abstractions for physical phenomena. We describe the architecture and implementation of our ASPEN system and its showcase intelligent building application, SmartCIS, which was demonstrated at SIGMOD 2009. We summarize the new query processing algorithms we have developed for integrating highly distributed stream data sources, both in low-power sensor devices and traditional PCs and servers; describe query optimization techniques for federations of stream processors; and detail new capabilities such as incremental maintenance of recursive views. Our algorithms and techniques generalize across a wide range of data from RFID and light measurements to real-time machine usage monitoring, energy consumption and recursive query computation.

#index 1472966
#* Report on the EDBT/ICDT 2010 workshop on updates in XML
#@ Michael Benedikt;Daniela Florescu;Philippa Gardner;Giovanna Guerrini;Marco Mesiti;Emmanuel Waller
#t 2010
#c 5
#! The first international workshop on Updates in XML [1] was held in conjunction with the EDBT/ICDT conference in Lausanne (Switzerland) on March 22, 2010, and attracted approximately 25 participants, culminating with about 40 attending the last session. This paper summarizes the main ideas presented in the workshop as well as interesting perspectives identified by the participants.

#index 1472967
#* Report on the first international workshop on cloud data management (CloudDB 2009)
#@ Xiaofeng Meng;Jiaheng Lu;Jie Qiu;Ying Chen;Haixun Wang
#t 2010
#c 5

#index 1479589
#* Proceedings of the 2nd International Workshop on Keyword Search on Structured Data
#@ 
#t 2010
#c 5

#index 1480459
#* Proceedings of the Sixth International Workshop on Data Management on New Hardware
#@ Anastasia Ailamaki;Peter A. Boncz
#t 2010
#c 5
#! Objective The aim of this one-day workshop is to bring together researchers who are interested in optimizing database performance on modern computing infrastructure by designing new data management techniques and tools. Topics of Interest The continued evolution of computing hardware and infrastructure imposes new challenges and bottlenecks to program performance. As a result, traditional database architectures that focus solely on I/O optimization increasingly fail to utilize hardware resources efficiently. CPUs with superscalar out-of-order execution, simultaneous multi-threading, multi-level memory hierarchies, and future storage hardware (such as flash drives) impose a great challenge to optimizing database performance. Consequently, exploiting the characteristics of modern hardware has become an important topic of database systems research. The goal is to make database systems adapt automatically to the sophisticated hardware characteristics, thus maximizing performance transparently to applications. To achieve this goal, the data management community needs interdisciplinary collaboration with computer architecture, compiler and operating systems researchers. This involves rethinking traditional data structures, query processing algorithms, and database software architectures to adapt to the advances in the underlying hardware infrastructure. Paper Selection The seven papers included in the workshop were chosen by the program committee from among sixteen high-quality submissions, following a review process in which each paper received at least three reviews. Based on the reviews, one paper was selected by the workshop chairs as the recipient of the "Best Paper" award. This year, the award goes to "Wimpy Node Clusters: What About Non-Wimpy Workloads?", by Willis Lang (University of Wisconsin); Jignesh M. Patel (University of Wisconsin); Srinath Shankar (Microsoft Corp.) . Workshop Program Eight technical papers were presented at the workshop. The workshop also featured a keynote talk by Evangelos Eleftheriou, IBM Fellow, IBM Zurich Lab. Additionally, the workshop included a panel on current challenges in cloud storage, moderated by Anastasia Ailamaki.

#index 1499763
#* FORUM: a flexible data integration system based on data semantics
#@ Zohra Bellahsene;Salima Benbernou;Helene Jaudoin;Francois Pinet;Olivier Pivert;Farouk Toumani
#t 2010
#c 5
#% 378407
#% 465057
#% 479783
#% 480149
#% 480645
#% 481280
#% 572314
#% 777935
#% 809239
#% 824718
#% 848763
#% 1036084
#% 1063534
#% 1090726
#% 1152978
#% 1332968
#% 1523802
#% 1705177
#! The FORUM project aims at extending existing data integration techniques in order to facilitate the development of mediation systems in large and dynamic environments. It is well known from the literature that a crucial point that hampers the development and wide adoption of mediation systems lies in the high entry and maintenance costs of such systems. To overcome these barriers, the FORUM project investigates three main research issues: (i) automatic discovery of semantic correspondences (ii) consistency maintenance of mappings, and (iii) tolerant rewriting of queries in the presence of approximate mappings.

#index 1499764
#* VSkyline: vectorization for efficient skyline computation
#@ Sung-Ryoung Cho;Jongwuk Lee;Seung-Won Hwang;Hwansoo Han;Sang-Won Lee
#t 2010
#c 5
#% 397361
#% 465167
#% 480119
#% 480671
#% 654480
#% 824670
#% 993954
#% 1022225
#% 1063486
#% 1169282
#% 1206998
#% 1217183
#! A dominance test, which decides the dominance relationship between tuples, is a core operation in skyline computation. Optimizing dominance tests can thus improve the performance of all existing skyline algorithms. Towards this goal, this paper proposes a vectorization of dominance tests in SIMD architectures. Specifically, our vectorization can perform the dominance test of multiple consecutive dimensions in parallel, thereby achieving a speedup of SIMD parallelism degree in theory. However, achieving such performance gain is non-trivial due to complex control dependencies within the dominance test. To address this problem, we devise an efficient vectorization, called VSkyline, which performs the dominance test with SIMD instructions by determining incomparability in a block of four dimensional values. Experimental results using a performance monitor show that VSkyline considerably reduces the numbers of both executed instructions and branch mispredictions.

#index 1499765
#* On models and query languages for probabilistic processes
#@ Daniel Deutch;Tova Milo
#t 2010
#c 5
#% 114677
#% 189742
#% 238399
#% 288764
#% 424283
#% 599777
#% 654524
#% 810053
#% 817690
#% 824806
#% 874885
#% 893117
#% 977012
#% 1022204
#% 1127379
#% 1134141
#% 1180015
#% 1180016
#% 1206929
#% 1291113
#% 1328099
#% 1408536
#% 1424591
#% 1424607
#% 1523864
#% 1523879
#% 1688305
#% 1707157
#% 1707651
#! Probabilistic processes appear naturally in various contexts, with applications to Business Processes, XML data management and more. Many models for specifying and querying such processes exist in the literature; a main goal of research in this area is to design models that are expressive enough to capture real-life processes and analysis tasks, but at the same time allow for efficient query evaluation. We depict the model established in [13, 16, 17, 18], and claim that it achieves a good balance between expressivity and query evaluation complexity. We compare and contrast the model with other common models for probabilistic processes, highlighting the different choices made in models design and their effect on expressivity and incurred complexity.

#index 1499766
#* Business intelligence for small and middle-sized entreprises
#@ Oksana Grabova;Jerome Darmont;Jean-Hugues Chauchat;Iryna Zolotaryova
#t 2010
#c 5
#% 88617
#% 273945
#% 348637
#% 393641
#% 397388
#% 413117
#% 438504
#% 442832
#% 463089
#% 479474
#% 479819
#% 480820
#% 481454
#% 504154
#% 558450
#% 581212
#% 581675
#% 807317
#% 810050
#% 943881
#% 993996
#% 1022298
#% 1095668
#% 1177653
#% 1214969
#% 1232669
#% 1409964
#% 1417954
#% 1728214
#! Data warehouses are the core of decision support systems, which nowadays are used by all kind of enterprises in the entire world. Although many studies have been conducted on the need of decision support systems (DSSs) for small businesses, most of them adopt existing solutions and approaches, which are appropriate for large-scaled enterprises, but are inadequate for small and middle-sized enterprises. Small enterprises require cheap, lightweight architectures and tools (hardware and software) providing online data analysis. In order to ensure these features, we review web-based business intelligence approaches. For real-time analysis, the traditional OLAP architecture is cumbersome and storage-costly; therefore, we also review in-memory processing. Consequently, this paper discusses the existing approaches and tools working in main memory and/or with web interfaces (including freeware tools), relevant for small and middle-sized enterprises in decision making.

#index 1499767
#* Elisa Bertino speaks out: how she accrued 301 coauthors, revitalized a department, cut her commute to three minutes, enhanced our trust in shared data, and more
#@ Marianne Winslett
#t 2010
#c 5

#index 1499768
#* The chair's report on SIGMOD'10 demonstration program
#@ Yufei Tao
#t 2010
#c 5

#index 1499769
#* The SIGMOD 2010 programming contest a distributed query engine
#@ Clément Genzmer;Volker Hudlet;Hyunjung Park;Daniel Schall;Pierre Senellart
#t 2010
#c 5
#% 393844
#% 442850
#! We report on the second annual ACM SIGMOD programming contest, which consisted in building an efficient distributed query engine on top of an in-memory index. This article is co-authored by the organizers of the competition (Clément Genzmer, Pierre Senellart) and the students who built the two leading implementations (Volker Hudlet, Hyunjung Park, Daniel Schall).

#index 1542524
#* TPC-E vs. TPC-C: characterizing the new TPC-E benchmark via an I/O comparison study
#@ Shimin Chen;Anastasia Ailamaki;Manos Athanassoulis;Phillip B. Gibbons;Ryan Johnson;Ippokratis Pandis;Radu Stoica
#t 2011
#c 5
#% 1052068
#% 1063551
#% 1217152
#% 1328052
#! TPC-E is a new OLTP benchmark recently approved by the Transaction Processing Performance Council (TPC). In this paper, we compare TPC-E with the familiar TPCC benchmark in order to understand the behavior of the new TPC-E benchmark. In particular, we compare the I/O access patterns of the two benchmarks by analyzing two OLTP disk traces. We find that (i) TPC-E is more read intensive with a 9.7:1 I/O read to write ratio, while TPC-C sees a 1.9:1 read-to-write ratio; and (ii) although TPC-E uses pseudo-realistic data, TPC-E's I/O access pattern is as random as TPC-C. The latter suggests that like TPC-C, TPC-E can benefit from SSDs, which have superior random I/O support. To verify this, we replay both disk traces on an Intel X25-E SSD and see dramatic improvements for both TPC-C and TPC-E.

#index 1542525
#* Tova Milo speaks out
#@ Marianne Winslett
#t 2011
#c 5

#index 1542526
#* Scientific data management at the Johns Hopkins institute for data intensive engineering and science
#@ Yanif Ahmad;Randal Burns;Michael Kazhdan;Charles Meneveau;Alex Szalay;Andreas Terzis
#t 2011
#c 5
#% 308489
#% 837835
#% 990079
#% 990107
#% 1019700
#% 1034489
#% 1051689
#% 1060259
#% 1127427
#% 1206570
#% 1224000
#% 1335551
#% 1492404

#index 1542527
#* Workshop on semantic data management: a summary report
#@ Reto Krummenacher;Karl Aberer;Atanas Kiryakov;Rajaraman Kanagasabai
#t 2011
#c 5
#! The InternationalWorkshop on Semantic Data Management (SemData) was held in Singapore co-located with the VLDB Conference 2010, with the goal of serving as a platform for the discussion and investigation of various aspects related to semantic databases and data managementin the large. The workshop was a full-day event featuring two research sessions, one industry session and a panel discussion, and attracted over 25 attendees. This report summarizes the key topics presented, interesting ideas discussed and the new perspectives identified during the workshop.

#index 1542528
#* Towards a computational transportation science
#@ Stephan Winter;Monika Sester;Ouri Wolfson;Glenn Geers
#t 2011
#c 5
#% 134187
#% 835045
#% 1248413
#! This workshop report sets out to define Computational Transportation Science as the science behind intelligent transportation systems. In particular it develops a first research agenda for this science, illustrating its unique challenges and putting them to public debate.

#index 1542529
#* Emerging multidisciplinary research across database management systems
#@ Anisoara Nica;Fabian M. Suchanek;Aparna S. Varde
#t 2011
#c 5
#% 1482585
#% 1482586
#% 1482587
#% 1482588
#% 1482589
#% 1482590
#% 1482591
#% 1482592
#% 1482593
#% 1482594
#% 1482595
#% 1482596
#% 1482597
#% 1482598
#% 1482599
#% 1482600
#! The database community is exploring more and more multidisciplinary avenues: Data semantics overlaps with ontology management; reasoning tasks venture into the domain of artificial intelligence; and data stream management and information retrieval shake hands, e.g., when processing Web click-streams. These new research avenues become evident, for example, in the topics that doctoral students choose for their dissertations. This paper surveys the emerging multidisciplinary research by doctoral students in database systems and related areas. It is based on the PIKM 2010, which is the 3rd Ph.D. workshop at the International Conference on Information and Knowledge Management (CIKM). The topics addressed include ontology development, data streams, natural language processing, medical databases, green energy, cloud computing, and exploratory search. In addition to core ideas from the workshop, we list some open research questions in these multidisciplinary areas.

#index 1542530
#* 13th international workshop on the web and databases: WebDB 2010
#@ Xin Luna Dong;Felix Naumann
#t 2011
#c 5
#% 845350
#% 1288161
#% 1471585
#% 1471586

#index 1573339
#* Reverse ontology matching
#@ Jorge Martinez-Gil;Jose F. Aldana-Montes
#t 2011
#c 5
#% 156337
#% 572314
#% 742769
#% 790852
#% 896024
#% 903358
#% 924747
#% 975019
#% 1063710
#% 1090777
#% 1217116
#% 1218637
#% 1246170
#% 1328194
#% 1383548
#% 1537340
#! Ontology Matching aims to find the semantic correspondences between ontologies that belong to a single domain but that have been developed separately. However, there are still some problem areas to be solved, because experts are still needed to supervise the matching processes and an efficient way to reuse the alignments has not yet been found. We propose a novel technique named Reverse Ontology Matching, which aims to find the matching functions that were used in the original process. The use of these functions is very useful for aspects such as modeling behavior from experts, performing matching-by-example, reverse engineering existing ontology matching tools or compressing ontology alignment repositories. Moreover, the results obtained from a widely used benchmark dataset provide evidence of the effectiveness of this approach.

#index 1573340
#* Scalable SQL and NoSQL data stores
#@ Rick Cattell
#t 2011
#c 5
#% 397295
#% 998845
#% 1002142
#% 1426489
#% 1551289
#! In this paper, we examine a number of SQL and socalled "NoSQL" data stores designed to scale simple OLTP-style application loads over many servers. Originally motivated by Web 2.0 applications, these systems are designed to scale to thousands or millions of users doing updates as well as reads, in contrast to traditional DBMSs and data warehouses. We contrast the new systems on their data model, consistency mechanisms, storage mechanisms, durability guarantees, availability, query support, and other dimensions. These systems typically sacrifice some of these dimensions, e.g. database-wide transaction consistency, in order to achieve others, e.g. higher availability and scalability.

#index 1573341
#* Christopher Ré speaks out on his ACM SIGMOD Jim Gray Dissertation Award, what he wishes he had known as a graduate student, and more
#@ Marianne Winslett
#t 2011
#c 5

#index 1573342
#* Paper bricks: an alternative to complete-story peer reviewing
#@ Jens Dittrich
#t 2011
#c 5
#% 1070044
#! The peer review system as used in several computer science communities has several flaws including long review times, overloaded reviewers, as well as fostering of niche topics. These flaws decrease quality, lower impact, slowdown the innovation process, and lead to frustration of authors, readers, and reviewers. In order to fix this, we propose a new peer review system termed paper bricks. Paper bricks has several advantages over the existing system including shorter publications, better competition for new ideas, as well as an accelerated innovation process. Furthermore, paper bricks may be implemented with minimal change to the existing peer review systems.

#index 1581451
#* Proceedings of the Fourth International Workshop on Testing Database Systems
#@ Goetz Graefe;Kenneth Salem
#t 2011
#c 5

#index 1581816
#* Proceedings of the thirtieth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems
#@ Maurizio Lenzerini;Thomas Schwentick
#t 2011
#c 5
#! It is our great pleasure to welcome you to the 2011 ACM Symposium on Principles of Database Systems -- PODS'11. This year's symposium continues its tradition of being the premier international conference on the theoretical aspects of data management. PODS papers are distinguished by a rigorous approach to widely diverse problems in databases, often bringing to bear techniques from a variety of different areas, including computational logic, finite model theory, computational complexity, algorithm design and analysis, programming languages, and artificial intelligence. The first PODS conference was held in Los Angeles (CA) in 1982, with Jeffrey D. Ullman as General Chair. Since that time, virtually all new ideas, methods and techniques for data management have been investigated and presented in subsequent PODS conferences (see http://www09.sigmod.org/sigmod/pods/ for various information on the conference series). To celebrate the 30th anniversary of the Symposium, the PODS Executive Committee has organized the PODS 30th Anniversary Colloquium, a special event held in June 12, 2011, with the goal of providing a retrospective on the role of database theory, and outlining a picture of the future directions of the discipline. The Colloquium featured five invited presentations from distinguished leaders in the field, namely: Moshe Y. Vardi: "The rise, fall, and rise of dependency theory: Part 1, the rise and fall", Ronald Fagin: "The rise, fall, and rise of dependency theory: Part 2, the rise from the ashes", Jeffrey D. Ullman: "Deductive Databases", Serge Abiteboul: "Trees, semistructured data, and other strange ways to go beyond tables", Victor Vianu: "Database Theory: Back to the Future". The PODS Executive Committee is grateful to the speakers for their participation in the event, and to Frank Neven for organizing a lively discussion session ending the Colloquium. This volume contains the proceedings of the Thirtieth ACMSIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems (PODS 2011), held in Athens, Greece, on June 13-15, 2011, in conjunction with the 2011 ACM SIGMOD International Conference on Management of Data. The proceedings include a paper by Daniel Deutch and Tova Milo based on the keynote address by Tova Milo and two papers, the first by Marcelo Arenas and Jorge Pérez, based on the tutorial byMarcelo Arenas, and the second based on the tutorial by S. Muthu Muthukrishnan, and 25 contributed papers that were selected by the Program Committee from 113 submissions. Most of these papers are preliminary reports on work in progress. While they have been read by program committee members, they have not been formally refereed. Many of them will probably appear in more polished and detailed form in scientific journals. The program committee selected the paper Data Exchange beyond Complete Data by Marcelo Arenas, Jorge Pérez and Juan L. Reutter for the PODS 2011 Best Paper Award. In addition, the announcement of the 2011 ACM PODS Alberto O. Mendelzon Test-of-Time Award appears in the proceedings. This year, the award is given to Optimal Aggregation Algorithms for Middleware by Ronald Fagin, Amnon Lotem, and Moni Naor. The paper originally appeared in the proceedings of PODS 2001. Warmest congratulations to the authors of these papers.

#index 1581817
#* A quest for beauty and wealth (or, business processes for database researchers)
#@ Daniel Deutch;Tova Milo
#t 2011
#c 5
#% 29439
#% 101955
#% 114677
#% 145228
#% 248029
#% 262249
#% 288764
#% 289415
#% 384978
#% 404772
#% 425200
#% 464891
#% 492610
#% 538153
#% 589230
#% 630964
#% 733140
#% 749039
#% 801675
#% 810053
#% 817690
#% 823352
#% 824806
#% 824935
#% 848048
#% 874885
#% 956528
#% 976987
#% 994005
#% 1022252
#% 1022253
#% 1022258
#% 1042322
#% 1063736
#% 1092014
#% 1127379
#% 1127409
#% 1134141
#% 1153811
#% 1179996
#% 1180015
#% 1180016
#% 1206929
#% 1270568
#% 1287539
#% 1326580
#% 1426553
#% 1426561
#% 1448946
#% 1468482
#% 1523864
#% 1523879
#% 1538777
#% 1661440
#% 1707651
#% 1709206
#% 1722368
#! While classic data management focuses on the data itself, research on Business Processes considers also the context in which this data is generated and manipulated, namely the processes, the users, and the goals that this data serves. This allows the analysts a better perspective of the organizational needs centered around the data. As such, this research is of fundamental importance. Much of the success of database systems in the last decade is due to the beauty and elegance of the relational model and its declarative query languages, combined with a rich spectrum of underlying evaluation and optimization techniques, and efficient implementations. This, in turn, has lead to an economic wealth for both the users and vendors of database systems. Similar beauty and wealth are sought for in the context of Business Processes. Much like the case for traditional database research, elegant modeling and rich underlying technology are likely to bring economic wealth for the Business Process owners and their users; both can benefit from easy formulation and analysis of the processes. While there have been many important advances in this research in recent years, there is still much to be desired: specifically, there have been many works that focus on the processes behavior (flow), and many that focus on its data, but only very few works have dealt with both. We will discuss here the important advantages of a holistic flow-and-data framework for Business Processes, the progress towards such a framework, and highlight the current gaps and research directions.

#index 1581818
#* Get the most out of your sample: optimal unbiased estimators using partial information
#@ Edith Cohen;Haim Kaplan
#t 2011
#c 5
#% 1331
#% 243166
#% 336610
#% 347225
#% 397369
#% 480805
#% 544011
#% 616528
#% 725373
#% 851889
#% 866696
#% 960250
#% 989512
#% 1014727
#% 1127368
#% 1127369
#% 1164953
#% 1213391
#% 1328163
#! Random sampling is an essential tool in the processing and transmission of data. It is used to summarize data too large to store or manipulate and meet resource constraints on bandwidth or battery power. Estimators that are applied to the sample facilitate fast approximate processing of queries posed over the original data and the value of the sample hinges on the quality of these estimators. Our work targets data sets such as request and traffic logs and sensor measurements, where data is repeatedly collected over multiple instances: time periods, locations, or snapshots. We are interested in operations, like quantiles and range, that span multiple instances. Subset-sums of these operations are used for applications ranging from planning to anomaly and change detection. Unbiased low-variance estimators are particularly effective as the relative error decreases with aggregation. The Horvitz-Thompson estimator, known to minimize variance for subset-sums over a sample of a single instance, is not optimal for multi-instance operations because it fails to exploit samples which provide partial information on the estimated quantity. We present a general principled methodology for the derivation of optimal unbiased estimators over sampled instances and aim to understand its potential. We demonstrate significant improvement in estimate accuracy of fundamental queries for common sampling schemes.

#index 1581819
#* Tight bounds for Lp samplers, finding duplicates in streams, and related problems
#@ Hossein Jowhari;Mert Sağlam;Gábor Tardos
#t 2011
#c 5
#% 54231
#% 88344
#% 203101
#% 379444
#% 590293
#% 751684
#% 805840
#% 808428
#% 816392
#% 824653
#% 1014727
#% 1164862
#% 1164953
#% 1217129
#% 1217130
#% 1379544
#% 1395792
#% 1426447
#% 1426450
#% 1484157
#% 1484158
#% 1484160
#! In this paper, we present near-optimal space bounds for Lp-samplers. Given a stream of updates (additions and subtraction) to the coordinates of an underlying vector x in Rn, a perfect Lp sampler outputs the i-th coordinate with probability xipxpp. In SODA 2010, Monemizadeh and Woodruff showed polylog space upper bounds for approximate Lp-samplers and demonstrated various applications of them. Very recently, Andoni, Krauthgamer and Onak improved the upper bounds and gave a O(ε-plog3n) space ε relative error and constant failure rate Lp-sampler for p є [1,2]. In this work, we give another such algorithm requiring only O(ε-plog2n) space for p є (1,2). For p є (0,1), our space bound is O(ε-1log2n), while for the p=1 case we have an O(log(1/ε)ε-log2n) space algorithm. We also give a O(log2n) bits zero relative error L0-sampler, improving the O(log3n) bits algorithm due to Frahling, Indyk and Sohler. As an application of our samplers, we give better upper bounds for the problem of finding duplicates in data streams. In case the length of the stream is longer than the alphabet size, L1 sampling gives us an O(log2n) space algorithm, thus improving the previous O(log3n) bound due to Gopalan and Radhakrishnan. In the second part of our work, we prove an Ω (log2n) lower bound for sampling from 0, ± 1 vectors (in this special case, the parameter p is not relevant for Lp sampling). This matches the space of our sampling algorithms for constant ε0. We also prove tight space lower bounds for the finding duplicates and heavy hitters problems. We obtain these lower bounds using reductions from the communication complexity problem augmented indexing.

#index 1581820
#* Pan-private algorithms via statistics on sketches
#@ Darakhshan Mir;S. Muthukrishnan;Aleksandar Nikolov;Rebecca N. Wright
#t 2011
#c 5
#% 576110
#% 578389
#% 816392
#% 879397
#% 963242
#% 1029084
#% 1266524
#% 1426323
#% 1426447
#% 1484081
#% 1484155
#% 1521657
#% 1740518
#! Consider fully dynamic data, where we track data as it gets inserted and deleted. There are well developed notions of private data analyses with dynamic data, for example, using differential privacy. We want to go beyond privacy, and consider privacy together with security, formulated recently as pan-privacy by Dwork et al. (ICS 2010). Informally, pan-privacy preserves differential privacy while computing desired statistics on the data, even if the internal memory of the algorithm is compromised (say, by a malicious break-in or insider curiosity or by fiat by the government or law). We study pan-private algorithms for basic analyses, like estimating distinct count, moments, and heavy hitter count, with fully dynamic data. We present the first known pan-private algorithms for these problems in the fully dynamic model. Our algorithms rely on sketching techniques popular in streaming: in some cases, we add suitable noise to a previously known sketch, using a novel approach of calibrating noise to the underlying problem structure and the projection matrix of the sketch; in other cases, we maintain certain statistics on sketches; in yet others, we define novel sketches. We also present the first known lower bounds explicitly for pan privacy, showing our results to be nearly optimal for these problems. Our lower bounds are stronger than those implied by differential privacy or dynamic data streaming alone and hold even if unbounded memory and/or unbounded processing time are allowed. The lower bounds use a noisy decoding argument and exploit a connection between pan-private algorithms and data sanitization.

#index 1581821
#* FIFO indexes for decomposable problems
#@ Cheng Sheng;Yufei Tao
#t 2011
#c 5
#% 1679
#% 41684
#% 91429
#% 126640
#% 227941
#% 282430
#% 287070
#% 300174
#% 300180
#% 321052
#% 378388
#% 464885
#% 565341
#% 630969
#% 733373
#% 760986
#% 783997
#% 801696
#% 810061
#% 1074714
#% 1081215
#% 1164838
#% 1344893
#! This paper studies first-in-first-out (FIFO) indexes, each of which manages a dataset where objects are deleted in the same order as their insertions. We give a technique that converts a static data structure to a FIFO index for all decomposable problems, provided that the static structure can be constructed efficiently. We present FIFO access methods to solve several problems including half-plane search, nearest neighbor search, and extreme-point search. All of our structures consume linear space, and have optimal or near-optimal query cost.

#index 1581822
#* Data exchange beyond complete data
#@ Marcelo Arenas;Jorge Pérez;Juan Reutter
#t 2011
#c 5
#% 663
#% 33547
#% 94459
#% 123102
#% 275032
#% 287339
#% 366807
#% 465053
#% 826032
#% 850730
#% 874882
#% 960233
#% 976996
#% 997492
#% 1063723
#% 1132129
#% 1217116
#% 1270567
#% 1328194
#% 1426463
#% 1562961
#! In the traditional data exchange setting, source instances are restricted to be complete in the sense that every fact is either true or false in these instances. Although natural for a typical database translation scenario, this restriction is gradually becoming an impediment to the development of a wide range of applications that need to exchange objects that admit several interpretations. In particular, we are motivated by two specific applications that go beyond the usual data exchange scenario: exchanging incomplete information and exchanging knowledge bases. In this paper, we propose a general framework for data exchange that can deal with these two applications. More specifically, we address the problem of exchanging information given by representation systems, which are essentially finite descriptions of (possibly infinite) sets of complete instances. We make use of the classical semantics of mappings specified by sets of logical sentences to give a meaningful semantics to the notion of exchanging representatives, from which the standard notions of solution, space of solutions, and universal solution naturally arise. We also introduce the notion of strong representation system for a class of mappings, that resembles the concept of strong representation system for a query language. We show the robustness of our proposal by applying it to the two applications mentioned above: exchanging incomplete information and exchanging knowledge bases, which are both instantiations of the exchanging problem for representation systems. We study these two applications in detail, presenting results regarding expressiveness, query answering and complexity of computing solutions, and also algorithms to materialize solutions.

#index 1581823
#* Incomplete information and certain answers in general data models
#@ Leonid Libkin
#t 2011
#c 5
#% 663
#% 11817
#% 94459
#% 95620
#% 101649
#% 109995
#% 119792
#% 139177
#% 139178
#% 248038
#% 378409
#% 384978
#% 442887
#% 527112
#% 576116
#% 826032
#% 857282
#% 865766
#% 866986
#% 1063724
#% 1065944
#% 1083337
#% 1179997
#% 1291118
#% 1408529
#% 1426460
#% 1481057
#% 1541335
#% 1700126
#! While incomplete information is ubiquitous in all data models - especially in applications involving data translation or integration - our understanding of it is still not completely satisfactory. For example, even such a basic notion as certain answers for XML queries was only introduced recently, and in a way seemingly rather different from relational certain answers. The goal of this paper is to introduce a general approach to handling incompleteness, and to test its applicability in known data models such as relations and documents. The approach is based on representing degrees of incompleteness via semantics-based orderings on database objects. We use it to both obtain new results on incompleteness and to explain some previously observed phenomena. Specifically we show that certain answers for relational and XML queries are two instances of the same general concept; we describe structural properties behind the naive evaluation of queries; answer open questions on the existence of certain answers in the XML setting; and show that previously studied ordering-based approaches were only adequate for SQL's primitive view of nulls. We define a general setting that subsumes relations and documents to help us explain in a uniform way how to compute certain answers, and when good solutions can be found in data exchange. We also look at the complexity of common problems related to incompleteness, and generalize several results from relational and XML contexts.

#index 1581824
#* Determining the currency of data
#@ Wenfei Fan;Floris Geerts;Jef Wijsen
#t 2011
#c 5
#% 663
#% 5191
#% 118359
#% 166497
#% 225004
#% 228800
#% 230142
#% 264858
#% 287268
#% 287333
#% 366807
#% 378409
#% 384978
#% 408396
#% 419926
#% 809239
#% 879041
#% 913783
#% 1063709
#% 1063724
#% 1231247
#% 1310060
#% 1328156
#% 1523816
#% 1523915
#% 1573139
#% 1661426
#! Data in real-life databases become obsolete rapidly. One often finds that multiple values of the same entity reside in a database. While all of these values were once correct, most of them may have become stale and inaccurate. Worse still, the values often do not carry reliable timestamps. With this comes the need for studying data currency, to identify the current value of an entity in a database and to answer queries with the current values, in the absence of timestamps. This paper investigates the currency of data. (1) We propose a model that specifies partial currency orders in terms of simple constraints. The model also allows us to express what values are copied from other data sources, bearing currency orders in those sources, in terms of copy functions defined on correlated attributes. (2) We study fundamental problems for data currency, to determine whether a specification is consistent, whether a value is more current than another, and whether a query answer is certain no matter how partial currency orders are completed. (3) Moreover, we identify several problems associated with copy functions, to decide whether a copy function imports sufficient current data to answer a query, whether such a function copies redundant data, whether a copy function can be extended to import necessary current data for a query while respecting the constraints, and whether it suffices to copy data of a bounded size. (4) We establish upper and lower bounds of these problems, all matching, for combined complexity and data complexity, and for a variety of query languages. We also identify special cases that warrant lower complexity.

#index 1581825
#* New results on two-dimensional orthogonal range aggregation in external memory
#@ Cheng Sheng;Yufei Tao
#t 2011
#c 5
#% 37861
#% 41684
#% 287070
#% 333977
#% 465060
#% 527189
#% 580735
#% 594028
#% 656697
#% 783997
#% 784515
#% 803127
#% 813793
#% 1054486
#% 1074714
#% 1379559
#% 1429693
#! We consider the orthogonal range aggregation problem. The dataset S consists of N axis-parallel rectangles in R2, each of which is associated with an integer weight. Given an axis-parallel rectangle Q and an aggregate function F, a query reports the aggregated result of the weights of the rectangles in S intersecting Q. The goal is to preprocess S into a structure such that all queries can be answered efficiently. We present indexing schemes to solve the problem in external memory when F = max (hence, min) and F = sum (hence, count and average), respectively. Our schemes have linear or near-linear space, and answer a query in O(logBN) or O(logB2/BN) I/Os, where B is the disk block size.

#index 1581826
#* On finding skylines in external memory
#@ Cheng Sheng;Yufei Tao
#t 2011
#c 5
#% 41684
#% 100803
#% 288976
#% 289148
#% 290165
#% 317729
#% 319601
#% 465167
#% 566144
#% 604653
#% 800555
#% 806212
#% 875012
#% 903013
#% 993954
#% 1022224
#% 1052680
#% 1074714
#% 1081215
#% 1196031
#% 1328116
#% 1379557
#! We consider the skyline problem (a.k.a. the maxima problem), which has been extensively studied in the database community. The input is a set P of d-dimensional points. A point dominates another if the former has a lower coordinate than the latter on every dimension. The goal is to find the skyline, which is the set of points p ∈ P such that p is not dominated by any other data point. In the external-memory model, the 2-d version of the problem is known to be solvable in O((N/B)logM/B(N/B)) I/Os, where N is the cardinality of P, B the size of a disk block, and M the capacity of main memory. For fixed d ≥ 3, we present an algorithm with I/O-complexity O((N/B)logd-2/M/B(N/B)). Previously, the best solution was adapted from an in-memory algorithm, and requires O((N/B) logd-2/2(N/M)) I/Os.

#index 1581827
#* Beyond simple aggregates: indexing for summary queries
#@ Zhewei Wei;Ke Yi
#t 2011
#c 5
#% 227883
#% 248822
#% 273902
#% 278835
#% 333926
#% 333931
#% 347226
#% 420053
#% 480465
#% 480628
#% 801696
#% 816392
#% 818938
#% 850727
#% 894443
#% 956457
#% 960250
#% 1016153
#% 1092009
#% 1206819
#% 1565818
#% 1701404
#% 1701436
#! Database queries can be broadly classified into two categories: reporting queries and aggregation queries. The former retrieves a collection of records from the database that match the query's conditions, while the latter returns an aggregate, such as count, sum, average, or max (min), of a particular attribute of these records. Aggregation queries are especially useful in business intelligence and data analysis applications where users are interested not in the actual records, but some statistics of them. They can also be executed much more efficiently than reporting queries, by embedding properly precomputed aggregates into an index. However, reporting and aggregation queries provide only two extremes for exploring the data. Data analysts often need more insight into the data distribution than what those simple aggregates provide, and yet certainly do not want the sheer volume of data returned by reporting queries. In this paper, we design indexing techniques that allow for extracting a statistical summary of all the records in the query. The summaries we support include frequent items, quantiles, various sketches, and wavelets, all of which are of central importance in massive data analysis. Our indexes require linear space and extract a summary with the optimal or near-optimal query cost.

#index 1581828
#* Space-efficient substring occurrence estimation
#@ Alessio Orlandi;Rossano Venturini
#t 2011
#c 5
#% 210189
#% 235941
#% 271801
#% 273705
#% 290703
#% 339936
#% 504208
#% 745489
#% 823464
#% 835496
#% 936965
#% 954624
#% 987259
#% 991192
#% 1128430
#% 1496100
#! We study the problem of estimating the number of occurrences of substrings in textual data: A text T on some alphabet £ of size Ã is preprocessed and an index I is built. The index is used in lieu of the text to answer queries of the form CountH(P), returning an approximated number of the occurrences of an arbitrary pattern P as a substring of T. The problem has its main application in selectivity estimation related to the LIKE predicate in textual databases [15, 14, 5]. Our focus is on obtaining an algorithmic solution with guaranteed error rates and small footprint. To achieve that, we first enrich previous work in the area of compressed text-indexing [8, 11, 6, 17] providing an optimal data structure that requires ?(|T|logÃ/l) bits where l e 1 is the additive error on any answer. We also approach the issue of guaranteeing exact answers for sufficiently frequent patterns, providing a data structure whose size scales with the amount of such patterns. Our theoretical findings are sustained by experiments showing the practical impact of our data structures.

#index 1581829
#* Provenance for aggregate queries
#@ Yael Amsterdamer;Daniel Deutch;Val Tannen
#t 2011
#c 5
#% 663
#% 152928
#% 189868
#% 215225
#% 228817
#% 318704
#% 384978
#% 430773
#% 464891
#% 563512
#% 803609
#% 871765
#% 960352
#% 976987
#% 1022258
#% 1036075
#% 1063736
#% 1092014
#% 1127376
#% 1127409
#% 1153448
#% 1180014
#% 1180021
#% 1189361
#% 1200291
#% 1231247
#% 1287539
#% 1291118
#% 1328151
#% 1426451
#% 1426553
#% 1426581
#% 1488677
#% 1527177
#% 1528613
#% 1661440
#! We study in this paper provenance information for queries with aggregation. Provenance information was studied in the context of various query languages that do not allow for aggregation, and recent work has suggested to capture provenance by annotating the different database tuples with elements of a commutative semiring and propagating the annotations through query evaluation. We show that aggregate queries pose novel challenges rendering this approach inapplicable. Consequently, we propose a new approach, where we annotate with provenance information not just tuples but also the individual values within tuples, using provenance to describe the values computation. We realize this approach in a concrete construction, first for "simple" queries where the aggregation operator is the last one applied, and then for arbitrary (positive) relational algebra queries with aggregation; the latter queries are shown to be more challenging in this context. Finally, we use aggregation to encode queries with difference, and study the semantics obtained for such queries on provenance annotated databases.

#index 1581830
#* On provenance minimization
#@ Yael Amsterdamer;Daniel Deutch;Tova Milo;Val Tannen
#t 2011
#c 5
#% 36181
#% 248026
#% 289266
#% 384978
#% 464727
#% 464891
#% 599549
#% 765446
#% 806215
#% 825661
#% 871765
#% 874883
#% 976987
#% 976997
#% 1016204
#% 1022258
#% 1036075
#% 1063544
#% 1092014
#% 1180021
#% 1287539
#% 1426553
#% 1488677
#% 1541335
#! Provenance information has been proved to be very effective in capturing the computational process performed by queries, and has been used extensively as the input to many advanced data management tools (e.g. view maintenance, trust assessment, or query answering in probabilistic databases). We study here the core of provenance information, namely the part of provenance that appears in the computation of every query equivalent to the given one. This provenance core is informative as it describes the part of the computational process that is inherent to the query. It is also useful as a compact input to the above mentioned data management tools. We study algorithms that, given a query, compute an equivalent query that realizes the core provenance for all tuples in its result. We study these algorithms for queries of varying expressive power. Finally, we observe that, in general, one would not want to require database systems to evaluate a specific query that realizes the core provenance, but instead to be able to find, possibly off-line, the core provenance of a given tuple in the output (computed by an arbitrary equivalent query), without rewriting the query. We provide algorithms for such direct computation of the core provenance.

#index 1581831
#* On the complexity of privacy-preserving complex event processing
#@ Yeye He;Siddharth Barman;Di Wang;Jeffrey F. Naughton
#t 2011
#c 5
#% 1252
#% 256685
#% 875004
#% 977010
#% 1063480
#% 1207016
#% 1217161
#% 1232292
#% 1328078
#% 1379535
#% 1523936
#! Complex Event Processing (CEP) Systems are stream processing systems that monitor incoming event streams in search of userspecified event patterns. While CEP systems have been adopted in a variety of applications, the privacy implications of event pattern reporting mechanisms have yet to be studied - a stark contrast to the significant amount of attention that has been devoted to privacy for relational systems. In this paper we present a privacy problem that arises when the system must support desired patterns (those that should be reported if detected) and private patterns (those that should not be revealed). We formalize this problem, which we term privacy-preserving, utility maximizing CEP (PP-CEP), and analyze its complexity under various assumptions. Our results show that this is a rich problem to study and shed some light on the difficulty of developing algorithms that preserve utility without compromising privacy.

#index 1581832
#* Provenance views for module privacy
#@ Susan B. Davidson;Sanjeev Khanna;Tova Milo;Debmalya Panigrahi;Sudeepa Roy
#t 2011
#c 5
#% 576110
#% 576761
#% 742048
#% 765449
#% 832825
#% 864412
#% 956511
#% 1014464
#% 1074831
#% 1121279
#% 1153476
#% 1164737
#% 1206679
#% 1217125
#% 1414540
#% 1468475
#% 1720925
#% 1728161
#! Scientific workflow systems increasingly store provenance information about the module executions used to produce a data item, as well as the parameter settings and intermediate data items passed between module executions. However, authors/owners of workflows may wish to keep some of this information confidential. In particular, a module may be proprietary, and users should not be able to infer its behavior by seeing mappings between all data inputs and outputs. The problem we address in this paper is the following: Given a workflow, abstractly modeled by a relation R, a privacy requirement ? and costs associated with data. The owner of the workflow decides which data (attributes) to hide, and provides the user with a view R' which is the projection of R over attributes which have not been hidden. The goal is to minimize the cost of hidden data while guaranteeing that individual modules are ?-private. We call this the Secure-View problem. We formally define the problem, study its complexity, and offer algorithmic solutions.

#index 1581833
#* Querying graph patterns
#@ Pablo Barceló;Leonid Libkin;Juan L. Reutter
#t 2011
#c 5
#% 663
#% 32904
#% 212446
#% 268797
#% 291299
#% 378409
#% 442887
#% 562454
#% 587566
#% 632039
#% 644201
#% 801677
#% 826032
#% 865766
#% 905847
#% 989645
#% 1019798
#% 1166490
#% 1206699
#% 1206916
#% 1218646
#% 1223424
#% 1408529
#% 1426443
#% 1481057
#% 1523818
#% 1523898
#% 1538780
#% 1541335
#% 1594585
#% 1700126
#! Graph data appears in a variety of application domains, and many uses of it, such as querying, matching, and transforming data, naturally result in incompletely specified graph data, i.e., graph patterns. While queries need to be posed against such data, techniques for querying patterns are generally lacking, and properties of such queries are not well understood. Our goal is to study the basics of querying graph patterns. We first identify key features of patterns, such as node and label variables and edges specified by regular expressions, and define a classification of patterns based on them. We then study standard graph queries on graph patterns, and give precise characterizations of both data and combined complexity for each class of patterns. If complexity is high, we do further analysis of features that lead to intractability, as well as lower complexity restrictions. We introduce a new automata model for query answering with two modes of acceptance: one captures queries returning nodes, and the other queries returning paths. We study properties of such automata, and the key computational tasks associated with them. Finally, we provide additional restrictions for tractability, and show that some intractable cases can be naturally cast as instances of constraint satisfaction problem.

#index 1581834
#* Maximizing conjunctive views in deletion propagation
#@ Benny Kimelfeld;Jan Vondrák;Ryan Williams
#t 2011
#c 5
#% 664
#% 205305
#% 278831
#% 283180
#% 286901
#% 287000
#% 289424
#% 289425
#% 291869
#% 378401
#% 416007
#% 416036
#% 579315
#% 593801
#% 598376
#% 643566
#% 812092
#% 907551
#% 1098282
#% 1426619
#% 1488677
#% 1523847
#% 1872965
#! In deletion propagation, tuples from the database are deleted in order to reflect the deletion of a tuple from the view. Such an operation may result in the (often necessary) deletion of additional tuples from the view, besides the intentionally deleted one. The complexity of deletion propagation is studied, where the view is defined by a conjunctive query (CQ), and the goal is to maximize the number of tuples that remain in the view. Buneman et al. showed that for some simple CQs, this problem can be solved by a trivial algorithm. This paper identifies additional cases of CQs where the trivial algorithm succeeds, and in contrast, it proves that for some other CQs the problem is NP-hard to approximate better than some constant ratio. In fact, this paper shows that among the CQs without self joins, the hard CQs are exactly the ones that the trivial algorithm fails on. In other words, for every CQ without self joins, deletion propagation is either APX-hard or solvable by the trivial algorithm. The paper then presents approximation algorithms for certain CQs where deletion propagation is APX-hard. Specifically, two constant-ratio (and polynomial-time) approximation algorithms are given for the class of star CQs without self joins. The first algorithm is a greedy algorithm, and the second is based on randomized rounding of a linear program. While the first algorithm is more efficient, the second one has a better approximation ratio. Furthermore, the second algorithm can be extended to a significant generalization of star CQs. Finally, the paper shows that self joins can have a major negative effect on the approximability of the problem.

#index 1581835
#* Determining relevance of accesses at runtime
#@ Michael Benedikt;Georg Gottlob;Pierre Senellart
#t 2011
#c 5
#% 54225
#% 101922
#% 164371
#% 198466
#% 248025
#% 289266
#% 342359
#% 368248
#% 384978
#% 464892
#% 572311
#% 599549
#% 726626
#% 801698
#% 942359
#% 943616
#% 955762
#% 1148409
#% 1206616
#% 1217123
#% 1426535
#! Consider the situation where a query is to be answered using Web sources that restrict the accesses that can be made on backend relational data by requiring some attributes to be given as input of the service. The accesses provide lookups on the collection of attributes values that match the binding. They can differ in whether or not they require arguments to be generated from prior accesses. Prior work has focused on the question of whether a query can be answered using a set of data sources, and in developing static access plans (e.g., Datalog programs) that implement query answering. We are interested in dynamic aspects of the query answering problem: given partial information about the data, which accesses could provide relevant data for answering a given query? We consider immediate and long-term notions of "relevant accesses", and ascertain the complexity of query relevance, for both conjunctive queries and arbitrary positive queries. In the process, we relate dynamic relevance of an access to query containment under access limitations and characterize the complexity of this problem; we produce several complexity results about containment that are of interest by themselves.

#index 1581836
#* Parallel evaluation of conjunctive queries
#@ Paraschos Koutris;Dan Suciu
#t 2011
#c 5
#% 59792
#% 69503
#% 115661
#% 139234
#% 519659
#% 778122
#% 803609
#% 963669
#% 992830
#% 1063548
#% 1086033
#% 1127559
#% 1197989
#% 1328060
#% 1328095
#% 1372690
#% 1468421
#% 1472960
#% 1484141
#% 1523824
#! The availability of large data centers with tens of thousands of servers has led to the popular adoption of massive parallelism for data analysis on large datasets. Several query languages exist for running queries on massively parallel architectures, some based on the MapReduce infrastructure, others using proprietary implementations. Motivated by this trend, this paper analyzes the parallel complexity of conjunctive queries. We propose a very simple model of parallel computation that captures these architectures, in which the complexity parameter is the number of parallel steps requiring synchronization of all servers. We study the complexity of conjunctive queries and give a complete characterization of the queries which can be computed in one parallel step. These form a strict subset of hierarchical queries, and include flat queries like R(x,y), S(x,z), T(x,v), U(x,w), tall queries like R(x), S(x,y), T(x,y,z), U(x,y,z,w), and combinations thereof, which we call tall-flat queries. We describe an algorithm for computing in parallel any tall-flat query, and prove that any query that is not tall-flat cannot be computed in one step in this model. Finally, we present extensions of our results to queries that are not tall-flat.

#index 1581837
#* Querying semantic web data with SPARQL
#@ Marcelo Arenas;Jorge Pérez
#t 2011
#c 5
#% 77943
#% 275928
#% 303887
#% 408396
#% 562454
#% 578570
#% 598376
#% 956573
#% 1019798
#% 1098418
#% 1152440
#% 1201360
#% 1223424
#% 1400160
#% 1424588
#% 1426443
#% 1497253
#% 1500879
#% 1540307
#% 1552657
#% 1567949
#% 1586117
#% 1597489
#% 1603788
#% 1696286
#! The Semantic Web is the initiative of the W3C to make information on the Web readable not only by humans but also by machines. RDF is the data model for Semantic Web data, and SPARQL is the standard query language for this data model. In the last ten years, we have witnessed a constant growth in the amount of RDF data available on the Web, which have motivated the theoretical study of some fundamental aspects of SPARQL and the development of efficient mechanisms for implementing this query language. Some of the distinctive features of RDF have made the study and implementation of SPARQL challenging. First, as opposed to usual database applications, the semantics of RDF is open world, making RDF databases inherently incomplete. Thus, one usually obtains partial answers when querying RDF with SPARQL, and the possibility of adding optional information if present is a crucial feature of SPARQL. Second, RDF databases have a graph structure and are interlinked, thus making graph navigational capabilities a necessary component of SPARQL. Last, but not least, SPARQL has to work at Web scale! RDF and SPARQL have attracted interest from the database community. However, we think that this community has much more to say about these technologies, and, in particular, about the fundamental database problems that need to be solved in order to provide solid foundations for the development of these technologies. In this paper, we survey some of the main results about the theory of RDF and SPARQL putting emphasis on some research opportunities for the database community.

#index 1581838
#* Theory of data stream computing: where to go
#@ S. Muthukrishnan
#t 2011
#c 5
#% 238182
#% 894646
#% 930938
#% 963669
#% 1039655
#% 1039695
#% 1198209
#% 1231059
#% 1232259
#% 1484081
#% 1484141
#% 1581820
#! Computing power has been growing steadily, just as communication rate and memory size. Simultaneously our ability to create data has been growing phenomenally and therefore the need to analyze it. We now have examples of massive data streams that are created in far higher rate than we can capture and store in memory economically, gathered in far more quantity than can be transported to central databases without overwhelming the communication infrastructure, and arrives far faster than we can compute with them in a sophisticated way. This phenomenon has challenged how we store, communicate and compute with data. Theories developed over past 50 years have relied on full capture, storage and communication of data. Instead, what we need for managing modern massive data streams are new methods built around working with less. The past 10 years have seen new theories emerge in computing (data stream algorithms), communication (compressed sensing), databases (data stream management systems) and other areas to address the challenges of massive data streams. Still, lot remains open and new applications of massive data streams have emerged recently. We present an overview of these challenges.

#index 1581839
#* The complexity of text-preserving XML transformations
#@ Timos Antonopoulos;Wim Martens;Frank Neven
#t 2011
#c 5
#% 315302
#% 343978
#% 378394
#% 427874
#% 562313
#% 630965
#% 743615
#% 778122
#% 806858
#% 809259
#% 826034
#% 942356
#% 1071882
#% 1108141
#% 1181329
#% 1224353
#% 1474894
#% 1504036
#% 1661443
#% 1914068
#! While XML is nowadays adopted as the de facto standard for data exchange, historically, its predecessor SGML was invented for describing electronic documents, i.e., marked up text. Actually, today there are still large volumes of such XML texts. We consider simple transformations which can change the internal structure of documents, that is, the mark-up, and can filter out parts of the text but do not disrupt the ordering of the words. Specifically, we focus on XML transformations where the transformed document is a subsequence of the input document when ignoring mark-up. We call the latter text-preserving XML transformations. We characterize such transformations as copy- and rearrange-free transductions. Furthermore, we study the problem of deciding whether a given XML transducer is text-preserving over a given tree language. We consider top-down transducers as well as the abstraction of XSLT called DTL. We show that deciding whether a transformation is text-preserving over an unranked regular tree language is in PTime for top-down transducers, EXPTime-complete for DTL with XPath, and decidable for DTL with MSO patterns. Finally, we obtain that for every transducer in one of the above mentioned classes, the maximal subset of the input schema can be computed on which the transformation is text-preserving.

#index 1581840
#* Efficient evaluation for a temporal logic on changing XML documents
#@ Mikolaj Bojanczyk;Diego Figueira
#t 2011
#c 5
#% 278829
#% 745467
#% 791181
#% 1063733
#% 1180010
#% 1682381
#! We consider a sequence t1,...,tk of XML documents that is produced by a sequence of local edit operations. To describe properties of such a sequence, we use a temporal logic. The logic can navigate both in time and in the document, e.g. a formula can say that every node with label a eventually gets a descendant with label b. For every fixed formula, we provide an evaluation algorithm that works in time O(k ⋅ log(n)), where k is the number of edit operations and n is the maximal size of document that is produced. In the algorithm, we represent formulas of the logic by a kind of automaton, which works on sequences of documents. The algorithm works on XML documents of bounded depth.

#index 1581841
#* Finding a minimal tree pattern under neighborhood constraints
#@ Benny Kimelfeld;Yehoshua Sagiv
#t 2011
#c 5
#% 296877
#% 464863
#% 598376
#% 660011
#% 838492
#% 857282
#% 874894
#% 993987
#% 1016135
#% 1063539
#% 1127413
#% 1206760
#% 1217198
#% 1217259
#% 1223425
#% 1288160
#% 1373433
#% 1426613
#! Tools that automatically generate queries are useful when schemas are hard to understand due to size or complexity. Usually, these tools find minimal tree patterns that contain a given set (or bag) of labels. The labels could be, for example, XML tags or relation names. The only restriction is that, in a tree pattern, adjacent labels must be among some specified pairs. A more expressive framework is developed here, where a schema is a mapping of each label to a collection of bags of labels. A tree pattern conforms to the schema if for all nodes v, the bag comprising the labels of the neighbors is contained in one of the bags to which the label of v is mapped. The problem at hand is to find a minimal tree pattern that conforms to the schema and contains a given bag of labels. This problem is NP-hard even when using the simplest conceivable language for describing schemas. In practice, however, the set of labels is small, so efficiency is realized by means of an algorithm that is fixed-parameter tractable (FPT). Two languages for specifying schemas are discussed. In the first, one expresses pairwise mutual exclusions between labels. Though W[1]-hardness (hence, unlikeliness of an FPT algorithm) is shown, an FPT algorithm is described for the case where the mutual exclusions form a circular-arc graph (e.g., disjoint cliques). The second language is that of regular expressions, and for that another FPT algorithm is described.

#index 1581842
#* A rule-based language for web data management
#@ Serge Abiteboul;Meghyn Bienvenu;Alban Galland;Émilien Antoine
#t 2011
#c 5
#% 11797
#% 77932
#% 95248
#% 101646
#% 384978
#% 452753
#% 532638
#% 801671
#% 809267
#% 821939
#% 835186
#% 874978
#% 1061656
#% 1063731
#% 1072645
#% 1080071
#% 1164503
#% 1172464
#% 1246527
#% 1386046
#% 1472960
#% 1594661
#% 1696824
#! There is a new trend to use Datalog-style rule-based languages to specify modern distributed applications, notably on the Web. We introduce here such a language for a distributed data model where peers exchange messages (i.e. logical facts) as well as rules. The model is formally defined and its interest for distributed data management is illustrated through a variety of examples. A contribution of our work is a study of the impact on expressiveness of "delegations" (the installation of rules by a peer in some other peer) and explicit timestamps. We also validate the semantics of our model by showing that under certain natural conditions, our semantics converges to the same semantics as the centralized system with the same rules. Indeed, we show this is even true when updates are considered.

#index 1581843
#* Relational transducers for declarative networking
#@ Tom J. Ameloot;Frank Neven;Jan Van den Bussche
#t 2011
#c 5
#% 91364
#% 94458
#% 101646
#% 135873
#% 187081
#% 194120
#% 241166
#% 321054
#% 384978
#% 401402
#% 630964
#% 749647
#% 874885
#% 942360
#% 1180017
#% 1246527
#% 1426442
#% 1472960
#! Motivated by a recent conjecture concerning the expressiveness of declarative networking, we propose a formal computation model for "eventually consistent" distributed querying, based on relational transducers. A tight link has been conjectured between coordination-freeness of computations, and monotonicity of the queries expressed by such computations. Indeed, we propose a formal definition of coordination-freeness and confirm that the class of monotone queries is captured by coordination-free transducer networks. Coordination-freeness is a semantic property, but the syntactic class of "oblivious" transducers we define also captures the same class of monotone queries. Transducer networks that are not coordination-free are much more powerful.

#index 1581844
#* Rewrite rules for search database systems
#@ Ronald Fagin;Benny Kimelfeld;Yunyao Li;Sriram Raghavan;Shivakumar Vaithyanathan
#t 2011
#c 5
#% 1002
#% 2602
#% 29446
#% 118021
#% 118032
#% 118040
#% 144029
#% 297306
#% 427027
#% 501276
#% 643485
#% 875061
#% 956543
#% 1105157
#% 1183368
#% 1206687
#% 1426467
#% 1675051
#! The results of a search engine can be improved by consulting auxiliary data. In a search database system, the association between the user query and the auxiliary data is driven by rewrite rules that augment the user query with a set of alternative queries. This paper develops a framework that formalizes the notion of a rewrite program, which is essentially a collection of hedge-rewriting rules. When applied to a search query, the rewrite program produces a set of alternative queries that constitutes a least fixpoint (lfp). The main focus of the paper is on the lfp-convergence of a rewrite program, where a rewrite program is lfp-convergent if the least fixpoint of every search query is finite. Determining whether a given rewrite program is lfp-convergent is undecidable; to accommodate that, the paper proposes a safety condition, and shows that safety guarantees lfp-convergence, and that safety can be decided in polynomial time. The effectiveness of the safety condition in capturing lfp-convergence is illustrated by an application to a rewrite program in an implemented system that is intended for widespread use.

#index 1581845
#* Proceedings of the 2011 ACM SIGMOD International Conference on Management of data
#@ Timos Sellis;Renée J. Miller;Anastasios Kementsietsidis;Yannis Velegrakis
#t 2011
#c 5
#! Welcome to an exciting week in the city of Athens for the 2011 ACM SIGMOD Conference. Athens is a metropolitan and cosmopolitan city, with so many things to do and to see. It is also known as the birth place of Democracy, the city with the world-renown "Acropolis and Parthenon", with the famous Theater of Herodes Atticus and the "marble stadium" where the first modern time Olympic Games took place in 1896, home of Socrates, Plato, Pericles (Golden Age), and home of the very successful 2004 Olympic Games. And now the home of the 2011 ACM SIGMOD Conference! Athens is both an "ancient" and a "modern" city, in which visitors can walk safely and enjoy the rich --- almost 5,000 year old --- history it has to offer. The city offers a lot of sightseeing, museums, shopping and nightlife. We have a program of several social events to complement an excellent technical program. The SIGMOD banquet is in the beautiful Island Restaurant by the sea, the SIGMOD reception on Monday at the Caravel Hotel (conference hotel) Roof Garden, and the PODS 30th Anniversary Colloquium and Reception on Sunday also at the hotel. Through the generous support from our sponsors (Platinum) EMC, Microsoft, and Oracle, (Gold) Google, IBM Research, SAP, Sybase, and Yahoo! Labs, (Silver) AsterData, HP, Intrasoft International, Kosmix, MarkLogic, Twitter, and VirtualTrip, and (Other Supporters) Greenplum and NEC, along with a contribution from ACM SIGMOD, we were able to keep the conference fees to a minimum with an extraordinarily low student registration that will allow many students to participate. We had 375 research papers submitted. A research program committee provided detailed reviews and extensive discussion following SIGMOD's double-blind reviewing policy. The program committee was led by nine group leaders: Sihem Amer-Yahia (Yahoo! Labs), Michael Böhlen (University of Zurich), Bettina Kemme (McGill University), Sam Madden (MIT), Jignesh Patel (University of Wisconsin), Dan Suciu (University of Washington), Wang-Chiew Tan (IBM Research - Almaden and UC Santa Cruz), Nesime Tatbul (ETH Zurich), and Min Wang (HP Labs China). The group leaders ensured that every paper had a champion and received thorough discussion. We accepted a record number of research papers. Nonetheless, it is clear there were still papers that were rejected that would have been valuable contributions to SIGMOD. The group leaders did a great job in keeping the discussions positive and reviewers focused on finding reasons to accept papers, rather than reasons to reject. We hope that SIGMOD continues the trend of accepting more papers to accommodate all the great ideas being produced by the community. All research papers have been invited to participate in SIGMOD's Experimental Repeatability effort, the goal of which is to help to enable SIGMOD papers to stand as reliable, archival work for future research. A demonstration program committee of 35 people reviewed 81 proposals for system demonstrations and accepted 32. We will again be holding a Best Demonstration Award Competition to recognize the most innovative demonstrations. We encourage everyone to participate in the voting. An industrial program committee of 13 people reviewed 36 short presentation proposals, accepting 14. In addition, the industrial program will include two invited talks by Michael Abbot (Twitter) and Don Campbell (IBM). Rounding out the full program, we have a panel on data management issues in health and medical informatics, two invited talks by James Hamilton (Amazon) and Anastasia Ailamaki (EPFL), along with six tutorials on new applications of Datalog, flash memory, copy detection, data privacy, web data management, and statistical relational models. SIGMOD continues its commitment to undergraduate research awarding eight scholarships to undergraduate researchers who will participate in the Undergraduate Research Poster session that will be co-located with a Graduate Research Poster session. In addition, we will host the Third Annual SIGMOD Programming Contest. Student teams from degree granting institutions were invited to compete in this annual contest. This year, the task is to implement a high-throughput main-memory index that is made durable using a flash-based SSD.

#index 1581846
#* LazyFTL: a page-level flash translation layer optimized for NAND flash memory
#@ Dongzhe Ma;Jianhua Feng;Guoliang Li
#t 2011
#c 5
#% 737385
#% 829901
#% 871490
#% 903035
#% 951778
#% 960238
#% 985754
#% 995992
#% 996050
#% 1053489
#% 1063620
#% 1124295
#% 1127302
#% 1150117
#% 1174229
#% 1202494
#% 1328139
#% 1485053
#% 1523901
#% 1820346
#% 1823559
#! Flash is a type of electronically erasable programmable read-only memory (EEPROM), which has many advantages over traditional magnetic disks, such as lower access latency, lower power consumption, lack of noise, and shock resistance. However, due to its special characteristics, flash memory cannot be deployed directly in the place of traditional magnetic disks. The Flash Translation Layer (FTL) is a software layer built on raw flash memory that carries out garbage collection and wear leveling strategies and hides the special characteristics of flash memory from upper file systems by emulating a normal block device like magnetic disks. Most existing FTL schemes are optimized for some specific access patterns or bring about significant overhead of merge operations under certain circumstances. In this paper, we propose a novel FTL scheme named LazyFTL that exhibits low response latency and high scalability, and at the same time, eliminates the overhead of merge operations completely. Experimental results show that LazyFTL outperforms all the typical existing FTL schemes and is very close to the theoretically optimal solution. We also provide a basic design that assists LazyFTL to recover from system failures.

#index 1581847
#* Operation-aware buffer management in flash-based systems
#@ Yanfei Lv;Bin Cui;Bingsheng He;Xuexuan Chen
#t 2011
#c 5
#% 86748
#% 152943
#% 348037
#% 442571
#% 481450
#% 902938
#% 960238
#% 963436
#% 1053488
#% 1063551
#% 1092670
#% 1127391
#% 1183354
#% 1207002
#% 1213385
#% 1217151
#% 1222047
#% 1328139
#% 1422942
#% 1477907
#% 1482312
#% 1821460
#% 1823135
#% 1823137
#% 1823467
#! The inherent asymmetry of read and write speeds of flash memory poses great challenges for buffer management design. Most of existing flash-based buffer management policies adopt disk-oriented strategies by giving a specific priority to dirty pages, while not fully exploiting the characteristics of the flash memory. In this paper, we propose a novel buffer replacement algorithm named FOR, which stands for Flash-based Operation-aware buffer Replacement. The core idea of FOR is based on novel operation-aware page weight determination for buffer replacement. The weight metric not only measures the locality of read/write operations on a page, but also takes the cost difference of read/write operations into account. We further develop an efficient implementation FOR+ with the time complexity of O(1) for each operation. Experiments on synthetic and benchmark traces demonstrate the efficiency of the proposed strategy, which yields better performance compared with some state-of-the-art flash-based buffer management policies.

#index 1581848
#* SkimpyStash: RAM space skimpy key-value store on flash-based storage
#@ Biplob Debnath;Sudipta Sengupta;Jin Li
#t 2011
#c 5
#% 131555
#% 295801
#% 764572
#% 829901
#% 951778
#% 963436
#% 978505
#% 1053490
#% 1085291
#% 1127391
#% 1127428
#% 1174229
#% 1217152
#% 1278373
#% 1328139
#% 1468419
#% 1468535
#% 1523901
#% 1523920
#! We present SkimpyStash, a RAM space skimpy key-value store on flash-based storage, designed for high throughput, low latency server applications. The distinguishing feature of SkimpyStash is the design goal of extremely low RAM footprint at about 1 (± 0.5) byte per key-value pair, which is more aggressive than earlier designs. SkimpyStash uses a hash table directory in RAM to index key-value pairs stored in a log-structured manner on flash. To break the barrier of a flash pointer (say, 4 bytes) worth of RAM overhead per key, it "moves" most of the pointers that locate each key-value pair from RAM to flash itself. This is realized by (i) resolving hash table collisions using linear chaining, where multiple keys that resolve (collide) to the same hash table bucket are chained in a linked list, and (ii) storing the linked lists on flash itself with a pointer in each hash table bucket in RAM pointing to the beginning record of the chain on flash, hence incurring multiple flash reads per lookup. Two further techniques are used to improve performance: (iii) two-choice based load balancing to reduce wide variation in bucket sizes (hence, chain lengths and associated lookup times), and a bloom filter in each hash table directory slot in RAM to disambiguate the choice during lookup, and (iv) compaction procedure to pack bucket chain records contiguously onto flash pages so as to reduce flash reads during lookup. The average bucket size is the critical design parameter that serves as a powerful knob for making a continuum of tradeoffs between low RAM usage and low lookup latencies. Our evaluations on commodity server platforms with real-world data center applications show that SkimpyStash provides throughputs from few 10,000s to upwards of 100,000 get-set operations/sec.

#index 1581849
#* Design and evaluation of main memory hash join algorithms for multi-core CPUs
#@ Spyros Blanas;Yinan Li;Jignesh M. Patel
#t 2011
#c 5
#% 108515
#% 427195
#% 443513
#% 479617
#% 479821
#% 480119
#% 480464
#% 480966
#% 566122
#% 764572
#% 862757
#% 1052064
#% 1129954
#% 1222051
#% 1328057
#! The focus of this paper is on investigating efficient hash join algorithms for modern multi-core processors in main memory environments. This paper dissects each internal phase of a typical hash join algorithm and considers different alternatives for implementing each phase, producing a family of hash join algorithms. Then, we implement these main memory algorithms on two radically different modern multi-processor systems, and carefully examine the factors that impact the performance of each method. Our analysis reveals some interesting results -- a very simple hash join algorithm is very competitive to the other more complex methods. This simple join algorithm builds a shared hash table and does not partition the input relations. Its simplicity implies that it requires fewer parameter settings, thereby making it far easier for query optimizers and execution engines to use it in practice. Furthermore, the performance of this simple algorithm improves dramatically as the skew in the input data increases, and it quickly starts to outperform all other algorithms.

#index 1581850
#* Query optimization techniques for partitioned tables
#@ Herodotos Herodotou;Nedyalko Borisov;Shivnath Babu
#t 2011
#c 5
#% 8288
#% 11797
#% 188719
#% 264263
#% 397397
#% 410276
#% 411554
#% 481293
#% 765431
#% 800505
#% 824714
#% 838534
#% 853025
#% 994007
#% 1328059
#% 1328186
#! Table partitioning splits a table into smaller parts that can be accessed, stored, and maintained independent of one another. From their traditional use in improving query performance, partitioning strategies have evolved into a powerful mechanism to improve the overall manageability of database systems. Table partitioning simplifies administrative tasks like data loading, removal, backup, statistics maintenance, and storage provisioning. Query language extensions now enable applications and user queries to specify how their results should be partitioned for further use. However, query optimization techniques have not kept pace with the rapid advances in usage and user control of table partitioning. We address this gap by developing new techniques to generate efficient plans for SQL queries involving multiway joins over partitioned tables. Our techniques are designed for easy incorporation into bottom-up query optimizers that are in wide use today. We have prototyped these techniques in the PostgreSQL optimizer. An extensive evaluation shows that our partition-aware optimization techniques, with low optimization overhead, generate plans that can be an order of magnitude better than plans produced by current optimizers.

#index 1581851
#* CrowdDB: answering queries with crowdsourcing
#@ Michael J. Franklin;Donald Kossmann;Tim Kraska;Sukriti Ramesh;Reynold Xin
#t 2011
#c 5
#% 210206
#% 227894
#% 479452
#% 616086
#% 1252609
#% 1384503
#% 1426624
#% 1426639
#% 1432722
#% 1452859
#% 1459765
#% 1477589
#% 1526538
#% 1550748
#! Some queries cannot be answered by machines only. Processing such queries requires human input for providing information that is missing from the database, for performing computationally difficult functions, and for matching, ranking, or aggregating results based on fuzzy criteria. CrowdDB uses human input via crowdsourcing to process queries that neither database systems nor search engines can adequately answer. It uses SQL both as a language for posing complex queries and as a way to model data. While CrowdDB leverages many aspects of traditional database systems, there are also important differences. Conceptually, a major change is that the traditional closed-world assumption for query processing does not hold for human input. From an implementation perspective, human-oriented query operators are needed to solicit, integrate and cleanse crowdsourced data. Furthermore, performance and cost depend on a number of new factors including worker affinity, training, fatigue, motivation and location. We describe the design of CrowdDB, report on an initial set of experiments using Amazon Mechanical Turk, and outline important avenues for future work in the development of crowdsourced query processing systems.

#index 1581852
#* Skyline query processing over joins
#@ Akrivi Vlachou;Christos Doulkeridis;Neoklis Polyzotis
#t 2011
#c 5
#% 333854
#% 465167
#% 864452
#% 893105
#% 907527
#% 1063713
#% 1092017
#% 1328184
#! This paper addresses the problem of efficiently computing the skyline set of a relational join. Existing techniques either require to access all tuples of the input relations or demand specialized multi-dimensional access methods to generate the skyline join result. To avoid these inefficiencies, we introduce the novel SFSJ algorithm that fuses the identification of skyline tuples with the computation of the join. SFSJ is able to compute the correct skyline set by accessing only a subset of the input tuples, i.e., it has the property of early termination. SFSJ employs standard access methods for reading the input tuples and is readily implementable in an existing database system. Moreover, it can be used in pipelined execution plans, as it generates the skyline tuples progressively. Additionally, we formally analyze the performance of SFSJ and propose a novel strategy for accessing the input tuples that is proven to be optimal for SFSJ. Finally, we present an extensive experimental study that validates the effectiveness of SFSJ and demonstrates its advantages over existing techniques.

#index 1581853
#* Efficient parallel skyline processing using hyperplane projections
#@ Henning Köhler;Jing Yang;Xiaofang Zhou
#t 2011
#c 5
#% 100803
#% 144423
#% 227894
#% 288976
#% 289148
#% 340176
#% 465167
#% 480671
#% 654480
#% 806212
#% 824670
#% 864453
#% 960326
#% 963669
#% 968270
#% 993954
#% 1017256
#% 1063486
#% 1206767
#% 1206998
#% 1373450
#% 1688253
#% 1727523
#! The skyline of a set of multi-dimensional points (tuples) consists of those points for which no clearly better point exists in the given set, using component-wise comparison on domains of interest. Skyline queries, i.e., queries that involve computation of a skyline, can be computationally expensive, so it is natural to consider parallelized approaches which make good use of multiple processors. We approach this problem by using hyperplane projections to obtain useful partitions of the data set for parallel processing. These partitions not only ensure small local skyline sets, but enable efficient merging of results as well. Our experiments show that our method consistently outperforms similar approaches for parallel skyline computation, regardless of data distribution, and provides insights on the impacts of different optimization strategies.

#index 1581854
#* Scalable query rewriting: a graph-based approach
#@ George Konstantinidis;José Luis Ambite
#t 2011
#c 5
#% 198465
#% 237190
#% 273683
#% 378409
#% 384978
#% 481923
#% 572307
#% 572311
#% 599549
#% 864388
#% 1130756
#% 1250542
#% 1384875
#% 1424604
#% 1426463
#% 1433975
#% 1523891
#! In this paper we consider the problem of answering queries using views, which is important for data integration, query optimization, and data warehouses. We consider its simplest form, conjunctive queries and views, which already is NP-complete. Our context is data integration, so we search for maximally-contained rewritings. By looking at the problem from a graph perspective we are able to gain a better insight and develop an algorithm which compactly represents common patterns in the source descriptions, and (optionally) pushes some computation offline. This together with other optimizations result in an experimental performance about two orders of magnitude faster than current state-of-the-art algorithms, rewriting queries using over 10000 views within seconds.

#index 1581855
#* Automatic discovery of attributes in relational databases
#@ Meihui Zhang;Marios Hadjieleftheriou;Beng Chin Ooi;Cecilia M. Procopiuc;Divesh Srivastava
#t 2011
#c 5
#% 311808
#% 322884
#% 479783
#% 479973
#% 480645
#% 572314
#% 654457
#% 654458
#% 765433
#% 765548
#% 864392
#% 993982
#% 1091267
#% 1181220
#% 1523867
#! In this work we design algorithms for clustering relational columns into attributes, i.e., for identifying strong relationships between columns based on the common properties and characteristics of the values they contain. For example, identifying whether a certain set of columns refers to telephone numbers versus social security numbers, or names of customers versus names of nations. Traditional relational database schema languages use very limited primitive data types and simple foreign key constraints to express relationships between columns. Object oriented schema languages allow the definition of custom data types; still, certain relationships between columns might be unknown at design time or they might appear only in a particular database instance. Nevertheless, these relationships are an invaluable tool for schema matching, and generally for better understanding and working with the data. Here, we introduce data oriented solutions (we do not consider solutions that assume the existence of any external knowledge) that use statistical measures to identify strong relationships between the values of a set of columns. Interpreting the database as a graph where nodes correspond to database columns and edges correspond to column relationships, we decompose the graph into connected components and cluster sets of columns into attributes. To test the quality of our solution, we also provide a comprehensive experimental evaluation using real and synthetic datasets.

#index 1581856
#* Leveraging query logs for schema mapping generation in U-MAP
#@ Hazem Elmeleegy;Ahmed Elmagarmid;Jaewoo Lee
#t 2011
#c 5
#% 287339
#% 328429
#% 341233
#% 479783
#% 480134
#% 572314
#% 591589
#% 762652
#% 765432
#% 765433
#% 806215
#% 824736
#% 824763
#% 893094
#% 893114
#% 993981
#% 1127370
#% 1181235
#% 1206612
#% 1206613
#% 1217196
#% 1232194
#% 1328084
#% 1328193
#% 1328201
#% 1523804
#! In this paper, we introduce U-MAP, a new system for schema mapping generation. U-MAP builds upon and extends existing schema mapping techniques. However, it mitigates some key problems in this area, which have not been previously addressed. The key tenet of U-MAP is to exploit the usage information extracted from the query logs associated with the schemas being mapped. We describe our experience in applying our proposed system to realistic datasets from the retail and life sciences domains. Our results demonstrate the effectiveness and efficiency of U-MAP compared to traditional approaches.

#index 1581857
#* Designing and refining schema mappings via data examples
#@ Bogdan Alexe;Balder ten Cate;Phokion G. Kolaitis;Wang-Chiew Tan
#t 2011
#c 5
#% 333988
#% 378409
#% 384978
#% 599549
#% 806215
#% 809239
#% 810078
#% 824763
#% 826032
#% 1036084
#% 1063712
#% 1127370
#% 1206612
#% 1209667
#% 1215806
#% 1217165
#% 1310057
#% 1334265
#% 1424594
#% 1426466
#% 1493601
#! A schema mapping is a specification of the relationship between a source schema and a target schema. Schema mappings are fundamental building blocks in data integration and data exchange and, as such, obtaining the right schema mapping constitutes a major step towards the integration or exchange of data. Up to now, schema mappings have typically been specified manually or have been derived using mapping-design systems that automatically generate a schema mapping from a visual specification of the relationship between two schemas. We present a novel paradigm and develop a system for the interactive design of schema mappings via data examples. Each data example represents a partial specification of the semantics of the desired schema mapping. At the core of our system lies a sound and complete algorithm that, given a finite set of data examples, decides whether or not there exists a GLAV schema mapping (i.e., a schema mapping specified by Global-and-Local-As-View constraints) that "fits" these data examples. If such a fitting GLAV schema mapping exists, then our system constructs the "most general" one. We give a rigorous computational complexity analysis of the underlying decision problem concerning the existence of a fitting GLAV schema mapping, given a set of data examples. Specifically, we prove that this problem is complete for the second level of the polynomial hierarchy, hence, in a precise sense, harder than NP-complete. This worst-case complexity analysis notwithstanding, we conduct an experimental evaluation of our prototype implementation that demonstrates the feasibility of interactively designing schema mappings using data examples. In particular, our experiments show that our system achieves very good performance in real-life scenarios.

#index 1581858
#* Apples and oranges: a comparison of RDF benchmarks and real RDF datasets
#@ Songyun Duan;Anastasios Kementsietsidis;Kavitha Srinivas;Octavian Udrea
#t 2011
#c 5
#% 378395
#% 384978
#% 519567
#% 1022236
#% 1092530
#% 1127402
#% 1127611
#% 1206875
#% 1288161
#% 1366460
#% 1374374
#! The widespread adoption of the Resource Description Framework (RDF) for the representation of both open web and enterprise data is the driving force behind the increasing research interest in RDF data management. As RDF data management systems proliferate, so are benchmarks to test the scalability and performance of these systems under data and workloads with various characteristics. In this paper, we compare data generated with existing RDF benchmarks and data found in widely used real RDF datasets. The results of our comparison illustrate that existing benchmark data have little in common with real data. Therefore any conclusions drawn from existing benchmark tests might not actually translate to expected behaviours in real settings. In terms of the comparison itself, we show that simple primitive data metrics are inadequate to flesh out the fundamental differences between real and benchmark data. We make two contributions in this paper: (1) To address the limitations of the primitive metrics, we introduce intuitive and novel metrics that can indeed highlight the key differences between distinct datasets; (2) To address the limitations of existing benchmarks, we introduce a new benchmark generator with the following novel characteristics: (a) the generator can use any (real or synthetic) dataset and convert it into a benchmark dataset; (b) the generator can generate data that mimic the characteristics of real datasets with user-specified data properties. On the technical side, we formulate the benchmark generation problem as an integer programming problem whose solution provides us with the desired benchmark datasets. To our knowledge, this is the first methodological study of RDF benchmarks, as well as the first attempt on generating RDF benchmarks in a principled way.

#index 1581859
#* Efficient query answering in probabilistic RDF graphs
#@ Xiang Lian;Lei Chen
#t 2011
#c 5
#% 810098
#% 824697
#% 824764
#% 893167
#% 894441
#% 913798
#% 992830
#% 993985
#% 1022236
#% 1063521
#% 1092530
#% 1127378
#% 1127402
#% 1127431
#% 1127610
#% 1190673
#% 1206910
#% 1229786
#% 1328151
#% 1328155
#% 1332386
#% 1399937
#% 1426423
#% 1523825
#% 1523884
#% 1523889
#% 1702418
#! In this paper, we tackle the problem of efficiently answering queries on probabilistic RDF data graphs. Specifically, we model RDF data by probabilistic graphs, and an RDF query is equivalent to a search over subgraphs of probabilistic graphs that have high probabilities to match with a given query graph. To efficiently processqueries on probabilistic RDF graphs, we propose effective pruning mechanisms, structural and probabilistic pruning. For the structural pruning, we carefully design synopses for vertex/edge labels by considering their distributions and other structural information, in order to improve the pruning power. For the probabilistic pruning, we derive a cost model to guide the pre-computation of probability upper bounds such that the query cost is expected to be low. We construct an index structure that integrates synopses/statistics for structural and robabilistic pruning, and propose an efficient approach to answer queries on probabilistic RDF graph data. The efficiency of our solutions has been verified through extensive experiments.

#index 1581860
#* Facet discovery for structured web search: a query-log mining approach
#@ Jeffrey Pound;Stelios Paparizos;Panayiotis Tsaparas
#t 2011
#c 5
#% 659990
#% 838540
#% 875003
#% 894444
#% 993964
#% 993987
#% 994033
#% 1004294
#% 1015325
#% 1055719
#% 1074093
#% 1130808
#% 1206698
#% 1206746
#% 1217265
#% 1227648
#% 1279761
#% 1399998
#% 1400159
#% 1426467
#% 1426566
#% 1482250
#% 1696323
#! In recent years, there has been a strong trend of incorporating results from structured data sources into keyword-based web search systems such as Bing or Amazon. When presenting structured data, facets are a powerful tool for navigating, refining, and grouping the results. For a given structured data source, a fundamental problem in supporting faceted search is finding an ordered selection of attributes and values that will populate the facets. This creates two sets of challenges. First, because of the limited screen real-estate, it is important that the top facets best match the anticipated user intent. Second, the huge scale of available data to such engines demands an automated unsupervised solution. In this paper, we model the user faceted-search behavior using the intersection of web query-logs with existing structured data. Since web queries are formulated as free-text queries, a challenge in our approach is the inherent ambiguity in mapping keywords to the different possible attributes of a given entity type. We present an automated solution that elicits user preferences on attributes and values, employing different disambiguation techniques ranging from simple keyword matching, to more sophisticated probabilistic models. We demonstrate experimentally the scalability of our solution by running it on over a thousand categories of diverse entity types and measure the facet quality with a real-user study.

#index 1581861
#* Schema-as-you-go: on probabilistic tagging and querying of wide tables
#@ Meiyu Lu;Divyakant Agrawal;Bing Tian Dai;Anthony K.H. Tung
#t 2011
#c 5
#% 333854
#% 333990
#% 481280
#% 488766
#% 572314
#% 915256
#% 916785
#% 960237
#% 960245
#% 960279
#% 960302
#% 1002142
#% 1022259
#% 1063534
#% 1063537
#% 1127393
#% 1127402
#% 1127414
#% 1190119
#% 1206637
#% 1217175
#% 1328110
#% 1426493
#% 1426593
#% 1523833
#! The emergence of Web 2.0 has resulted in a huge amount of heterogeneous data that are contributed by a large number of users, engendering new challenges for data management and query processing. Given that the data are unified from various sources and accessed by numerous users, providing users with a unified mediated schema as data integration is insufficient. On one hand, a deterministic mediated schema restricts users' freedom to express queries in their preferred vocabulary; on the other hand, it is not realistic for users to remember the numerous attribute names that arise from integrating various data sources. As such, a user-oriented data management and query interface is required. In this paper, we propose an out-of-the-box approach that separates users' actions from database operations. This separating layer deals with the challenges from a semantic perspective. It interprets the semantics of each data value through tags that are provided by users, and then inserts the value into the database together with these tags. When querying the database, this layer also serves as a platform for retrieving data by interpreting the semantics of the queried tags from the users. Experiments are conducted to illustrate both the effectiveness and efficiency of our approach.

#index 1581862
#* No free lunch in data privacy
#@ Daniel Kifer;Ashwin Machanavajjhala
#t 2011
#c 5
#% 443463
#% 576110
#% 593994
#% 809245
#% 937549
#% 963241
#% 1029084
#% 1061644
#% 1083653
#% 1200329
#% 1206678
#% 1214684
#% 1217125
#% 1266524
#% 1318624
#% 1318812
#% 1318813
#% 1426322
#% 1426456
#% 1451189
#% 1606359
#% 1670071
#% 1727969
#% 1732708
#% 1740518
#! Differential privacy is a powerful tool for providing privacy-preserving noisy query answers over statistical databases. It guarantees that the distribution of noisy query answers changes very little with the addition or deletion of any tuple. It is frequently accompanied by popularized claims that it provides privacy without any assumptions about the data and that it protects against attackers who know all but one record. In this paper we critically analyze the privacy protections offered by differential privacy. First, we use a no-free-lunch theorem, which defines non-privacy as a game, to argue that it is not possible to provide privacy and utility without making assumptions about how the data are generated. Then we explain where assumptions are needed. We argue that privacy of an individual is preserved when it is possible to limit the inference of an attacker about the participation of the individual in the data generating process. This is different from limiting the inference about the presence of a tuple (for example, Bob's participation in a social network may cause edges to form between pairs of his friends, so that it affects more than just the tuple labeled as "Bob"). The definition of evidence of participation, in turn, depends on how the data are generated -- this is how assumptions enter the picture. We explain these ideas using examples from social network research as well as tabular data for which deterministic statistics have been previously released. In both cases the notion of participation varies, the use of differential privacy can lead to privacy breaches, and differential privacy does not always adequately limit inference about participation.

#index 1581863
#* TrustedDB: a trusted hardware based database with privacy and data confidentiality
#@ Sumeet Bajaj;Radu Sion
#t 2011
#c 5
#% 354287
#% 397367
#% 438481
#% 554365
#% 669839
#% 772109
#% 803731
#% 810042
#% 838424
#% 849461
#% 864413
#% 873336
#% 930426
#% 960290
#% 993942
#% 1022245
#% 1041784
#% 1278123
#% 1386180
#% 1478288
#% 1489744
#% 1523801
#% 1523967
#% 1664117
#% 1706207
#% 1726710
#% 1727993
#! TrustedDB is an outsourced database prototype that allows clients to execute SQL queries with privacy and under regulatory compliance constraints without having to trust the service provider. TrustedDB achieves this by leveraging server-hosted tamper-proof trusted hardware in critical query processing stages. TrustedDB does not limit the query expressiveness of supported queries. And, despite the cost overhead and performance limitations of trusted hardware, the costs per query are orders of magnitude lower than any (existing or) potential future software-only mechanisms. TrustedDB is built and runs on actual hardware, and its performance and costs are evaluated here.

#index 1581864
#* Differentially private data cubes: optimizing noise sources and consistency
#@ Bolin Ding;Marianne Winslett;Jiawei Han;Zhenhui Li
#t 2011
#c 5
#% 67453
#% 248030
#% 757953
#% 810028
#% 864412
#% 963241
#% 977011
#% 1016173
#% 1022247
#% 1061644
#% 1083653
#% 1141473
#% 1190072
#% 1193149
#% 1198224
#% 1198225
#% 1206678
#% 1214684
#% 1217148
#% 1217156
#% 1224602
#% 1381029
#% 1414540
#% 1426454
#% 1426563
#% 1451189
#% 1451190
#% 1523886
#% 1740518
#! Data cubes play an essential role in data analysis and decision support. In a data cube, data from a fact table is aggregated on subsets of the table's dimensions, forming a collection of smaller tables called cuboids. When the fact table includes sensitive data such as salary or diagnosis, publishing even a subset of its cuboids may compromise individuals' privacy. In this paper, we address this problem using differential privacy (DP), which provides provable privacy guarantees for individuals by adding noise to query answers. We choose an initial subset of cuboids to compute directly from the fact table, injecting DP noise as usual; and then compute the remaining cuboids from the initial set. Given a fixed privacy guarantee, we show that it is NP-hard to choose the initial set of cuboids so that the maximal noise over all published cuboids is minimized, or so that the number of cuboids with noise below a given threshold (precise cuboids) is maximized. We provide an efficient procedure with running time polynomial in the number of cuboids to select the initial set of cuboids, such that the maximal noise in all published cuboids will be within a factor (ln|L| + 1)^2 of the optimal, where |L| is the number of cuboids to be published, or the number of precise cuboids will be within a factor (1 - 1/e) of the optimal. We also show how to enforce consistency in the published cuboids while simultaneously improving their utility (reducing error). In an empirical evaluation on real and synthetic data, we report the amounts of error of different publishing algorithms, and show that our approaches outperform baselines significantly.

#index 1581865
#* iReduct: differential privacy with reduced relative errors
#@ Xiaokui Xiao;Gabriel Bender;Michael Hay;Johannes Gehrke
#t 2011
#c 5
#% 273902
#% 850727
#% 963241
#% 977011
#% 1029084
#% 1061644
#% 1141473
#% 1190072
#% 1198224
#% 1198225
#% 1206678
#% 1214684
#% 1328177
#% 1426322
#% 1426323
#% 1426328
#% 1426329
#% 1426454
#% 1426563
#% 1451189
#% 1451190
#% 1523886
#% 1670071
#% 1692264
#% 1697204
#% 1740518
#! Prior work in differential privacy has produced techniques for answering aggregate queries over sensitive data in a privacy-preserving way. These techniques achieve privacy by adding noise to the query answers. Their objective is typically to minimize absolute errors while satisfying differential privacy. Thus, query answers are injected with noise whose scale is independent of whether the answers are large or small. The noisy results for queries whose true answers are small therefore tend to be dominated by noise, which leads to inferior data utility. This paper introduces iReduct, a differentially private algorithm for computing answers with reduced relative error. The basic idea of iReduct is to inject different amounts of noise to different query results, so that smaller (larger) values are more likely to be injected with less (more) noise. The algorithm is based on a novel resampling technique that employs correlated noise to improve data utility. Performance is evaluated on an instantiation of iReduct that generates marginals, i.e., projections of multi-dimensional histograms onto subsets of their attributes. Experiments on real data demonstrate the effectiveness of our solution.

#index 1581866
#* A latency and fault-tolerance optimizer for online parallel query plans
#@ Prasang Upadhyaya;YongChul Kwon;Magdalena Balazinska
#t 2011
#c 5
#% 86476
#% 116040
#% 213976
#% 227883
#% 300127
#% 399766
#% 420114
#% 434178
#% 462497
#% 481289
#% 765470
#% 800583
#% 810008
#% 864486
#% 882833
#% 960280
#% 963669
#% 983467
#% 1022263
#% 1063553
#% 1127396
#% 1194581
#% 1217159
#% 1328214
#% 1426479
#% 1426494
#% 1468411
#% 1468421
#! We address the problem of making online, parallel query plans fault-tolerant: i.e., provide intra-query fault-tolerance without blocking. We develop an approach that not only achieves this goal but does so through the use of different fault-tolerance techniques at different operators within a query plan. Enabling each operator to use a different fault-tolerance strategy leads to a space of fault-tolerance plans amenable to cost-based optimization. We develop FTOpt, a cost-based fault-tolerance optimizer that automatically selects the best strategy for each operator in a query plan in a manner that minimizes the expected processing time with failures for the entire query. We implement our approach in a prototype parallel query-processing engine. Our experiments demonstrate that (1) there is no single best fault-tolerance strategy for all query plans, (2) often hybrid strategies that mix-and-match recovery techniques outperform any uniform strategy, and (3) our optimizer correctly identifies winning fault-tolerance configurations.

#index 1581867
#* ArrayStore: a storage manager for complex parallel array processing
#@ Emad Soroush;Magdalena Balazinska;Daniel Wang
#t 2011
#c 5
#% 86950
#% 115661
#% 246008
#% 248796
#% 248863
#% 321455
#% 397593
#% 411694
#% 427199
#% 438504
#% 443200
#% 461922
#% 463760
#% 481428
#% 503540
#% 563948
#% 572268
#% 631967
#% 765430
#% 775859
#% 874147
#% 1016603
#% 1328066
#% 1426481
#% 1426582
#% 1486236
#% 1563374
#% 1698861
#! We present the design, implementation, and evaluation of ArrayStore, a new storage manager for complex, parallel array processing. ArrayStore builds on prior work in the area of multidimensional data storage, but considers the new problem of supporting a parallel and more varied workload comprising not only range-queries, but also binary operations such as joins and complex user-defined functions. This paper makes two key contributions. First, it examines several existing single-site storage management strategies and array partitioning strategies to identify which combination is best suited for the array-processing workload above. Second, it develops a new and efficient storage-management mechanism that enables parallel processing of operations that must access data from adjacent partitions. We evaluate ArrayStore on over 80GB of real data from two scientific domains and real operators used in these domains. We show that ArrayStore outperforms previously proposed storage management strategies in the context of its diverse target workload.

#index 1581868
#* Fast checkpoint recovery algorithms for frequently consistent applications
#@ Tuan Cao;Marcos Vaz Salles;Benjamin Sowell;Yao Yue;Alan Demers;Johannes Gehrke;Walker White
#t 2011
#c 5
#% 12638
#% 90764
#% 114582
#% 193442
#% 236658
#% 346821
#% 399766
#% 427195
#% 462497
#% 480821
#% 481590
#% 778271
#% 848826
#% 893146
#% 960236
#% 978510
#% 987214
#% 1022298
#% 1207009
#% 1328168
#% 1426552
#% 1523801
#% 1523880
#! Advances in hardware have enabled many long-running applications to execute entirely in main memory. As a result, these applications have increasingly turned to database techniques to ensure durability in the event of a crash. However, many of these applications, such as massively multiplayer online games and main-memory OLTP systems, must sustain extremely high update rates - often hundreds of thousands of updates per second. Providing durability for these applications without introducing excessive overhead or latency spikes remains a challenge for application developers. In this paper, we take advantage of frequent points of consistency in many of these applications to develop novel checkpoint recovery algorithms that trade additional space in main memory for significantly lower overhead and latency. Compared to previous work, our new algorithms do not require any locking or bulk copies of the application state. Our experimental evaluation shows that one of our new algorithms attains nearly constant latency and reduces overhead by more than an order of magnitude for low to medium update rates. Additionally, in a heavily loaded main-memory transaction processing system, it still reduces overhead by more than a factor of two.

#index 1581869
#* Warding off the dangers of data corruption with amulet
#@ Nedyalko Borisov;Shivnath Babu;Nagapramod Mandagere;Sandeep Uttamchandani
#t 2011
#c 5
#% 786078
#% 810017
#% 1053481
#% 1053487
#% 1213386
#% 1328050
#% 1468193
#% 1468430
#% 1475075
#! Occasional corruption of stored data is an unfortunate byproduct of the complexity of modern systems. Hardware errors, software bugs, and mistakes by human administrators can corrupt important sources of data. The dominant practice to deal with data corruption today involves administrators writing ad hoc scripts that run data-integrity tests at the application, database, file-system, and storage levels. This manual approach is tedious, error-prone, and provides no understanding of the potential system unavailability and data loss if a corruption were to occur. We introduce the Amulet system that addresses the problem of verifying the correctness of stored data proactively and continuously. To our knowledge, Amulet is the first system that: (i) gives administrators a declarative language to specify their objectives regarding the detection and repair of data corruption; (ii) contains optimization and execution algorithms to ensure that the administrator's objectives are met robustly and with least cost, e.g., using pay-as-you cloud resources; and (iii) provides timely notification when corruption is detected, allowing proactive repair of corruption before it impacts users and applications. We describe the implementation and a comprehensive evaluation of Amulet for a database software stack deployed on an infrastructure-as-a-service cloud provider.

#index 1581870
#* Schedule optimization for data processing flows on the cloud
#@ Herald Kllapi;Eva Sitaridi;Manolis M. Tsangaris;Yannis Ioannidis
#t 2011
#c 5
#% 32877
#% 304391
#% 330305
#% 333847
#% 479461
#% 590353
#% 610773
#% 798967
#% 914569
#% 963669
#% 983001
#% 983467
#% 1016179
#% 1063553
#% 1092796
#% 1134501
#% 1134559
#% 1164233
#% 1224627
#% 1443257
#% 1460201
#! Scheduling data processing workflows (dataflows) on the cloud is a very complex and challenging task. It is essentially an optimization problem, very similar to query optimization, that is characteristically different from traditional problems in two aspects: Its space of alternative schedules is very rich, due to various optimization opportunities that cloud computing offers; its optimization criterion is at least two-dimensional, with monetary cost of using the cloud being at least as important as query completion time. In this paper, we study scheduling of dataflows that involve arbitrary data processing operators in the context of three different problems: 1) minimize completion time given a fixed budget, 2) minimize monetary cost given a deadline, and 3) find trade-offs between completion time and monetary cost without any a-priori constraints. We formulate these problems and present an approximate optimization framework to address them that uses resource elasticity in the cloud. To investigate the effectiveness of our approach, we incorporate the devised framework into a prototype system for dataflow evaluation and instantiate it with several greedy, probabilistic, and exhaustive search algorithms. Finally, through several experiments that we have conducted with the prototype elastic optimizer on numerous scientific and synthetic dataflows, we identify several interesting general characteristics of the space of alternative schedules as well as the advantages and disadvantages of the various search algorithms. The overall results are quite promising and indicate the effectiveness of our approach.

#index 1581871
#* Zephyr: live migration in shared nothing databases for elastic cloud platforms
#@ Aaron J. Elmore;Sudipto Das;Divyakant Agrawal;Amr El Abbadi
#t 2011
#c 5
#% 31778
#% 114582
#% 201869
#% 320902
#% 340297
#% 531907
#% 963628
#% 966956
#% 990408
#% 1063488
#% 1063541
#% 1210567
#% 1217217
#% 1426489
#% 1468219
#% 1594649
#! Multitenant data infrastructures for large cloud platforms hosting hundreds of thousands of applications face the challenge of serving applications characterized by small data footprint and unpredictable load patterns. When such a platform is built on an elastic pay-per-use infrastructure, an added challenge is to minimize the system's operating cost while guaranteeing the tenants' service level agreements (SLA). Elastic load balancing is therefore an important feature to enable scale-up during high load while scaling down when the load is low. Live migration, a technique to migrate tenants with minimal service interruption and no downtime, is critical to allow lightweight elastic scaling. We focus on the problem of live migration in the database layer. We propose Zephyr, a technique to efficiently migrate a live database in a shared nothing transactional database architecture. Zephyr uses phases of on-demand pull and asynchronous push of data, requires minimal synchronization, results no service unavailability and few or no aborted transactions, minimizes the data transfer overhead, provides ACID guarantees during migration, and ensures correctness in the presence of failures. We outline a prototype implementation using an open source relational database engine and an present a thorough evaluation using various transactional workloads. Zephyr's efficiency is evident from the few tens of failed operations, 10-20% change in average transaction latency, minimal messaging, and no overhead during normal operation when migrating a live database.

#index 1581872
#* Workload-aware database monitoring and consolidation
#@ Carlo Curino;Evan P.C. Jones;Samuel Madden;Hari Balakrishnan
#t 2011
#c 5
#% 38688
#% 149953
#% 480298
#% 753275
#% 805469
#% 820403
#% 960265
#% 1035944
#% 1063561
#% 1189342
#% 1207027
#% 1210203
#% 1296920
#% 1312540
#% 1347758
#% 1408964
#! In most enterprises, databases are deployed on dedicated database servers. Often, these servers are underutilized much of the time. For example, in traces from almost 200 production servers from different organizations, we see an average CPU utilization of less than 4%. This unused capacity can be potentially harnessed to consolidate multiple databases on fewer machines, reducing hardware and operational costs. Virtual machine (VM) technology is one popular way to approach this problem. However, as we demonstrate in this paper, VMs fail to adequately support database consolidation, because databases place a unique and challenging set of demands on hardware resources, which are not well-suited to the assumptions made by VM-based consolidation. Instead, our system for database consolidation, named Kairos, uses novel techniques to measure the hardware requirements of database workloads, as well as models to predict the combined resource utilization of those workloads. We formalize the consolidation problem as a non-linear optimization program, aiming to minimize the number of servers and balance load, while achieving near-zero performance degradation. We compare Kairos against virtual machines, showing up to a factor of 12× higher throughput on a TPC-C-like benchmark. We also tested the effectiveness of our approach on real-world data collected from production servers at Wikia.com, Wikipedia, Second Life, and MIT CSAIL, showing absolute consolidation ratios ranging between 5.5:1 and 17:1.

#index 1581873
#* Predicting cost amortization for query services
#@ (Vasiliki) Verena Kantere;Debabrata Dash;Georgios Gratsias;Anastasia Ailamaki
#t 2011
#c 5
#% 202493
#% 333975
#% 337066
#% 379286
#% 482100
#% 571217
#% 659992
#% 765455
#% 800500
#% 936215
#% 984947
#% 984948
#% 993946
#% 1022293
#% 1034728
#% 1038702
#% 1063540
#% 1206951
#% 1206964
#% 1207188
#% 1232694
#% 1328168
#% 1328211
#% 1332315
#% 1386935
#% 1408797
#! Emerging providers of online services offer access to data collections. Such data service providers need to build data structures, e.g. materialized views and indexes, in order to offer better performance for user query execution. The cost of such structures is charged to the user as part of the overall query service cost. In order to ensure the economic viability of the provider, the building and maintenance cost of new structures has to be amortized to a set of prospective query services that will use them. This work proposes a novel stochastic model that predicts the extent of cost amortization in time and number of services. The model is completed with a novel method that regresses query traffic statistics and provides input to the prediction model. In order to demonstrate the effectiveness of the prediction model, we study its application on an extension of an existing economy model for the management of a cloud DBMS. A thorough experimental study shows that the prediction model ensures the economic viability of the cloud DBMS while enabling the offer of fast and cheap query services.

#index 1581874
#* Performance prediction for concurrent database workloads
#@ Jennie Duggan;Ugur Cetintemel;Olga Papaemmanouil;Eli Upfal
#t 2011
#c 5
#% 119485
#% 251473
#% 251474
#% 251477
#% 765467
#% 765468
#% 810056
#% 1044489
#% 1119400
#% 1130825
#% 1206654
#% 1206984
#% 1426544
#% 1688297

#index 1581875
#* Reverse spatial and textual k nearest neighbor search
#@ Jiaheng Lu;Ying Lu;Gao Cong
#t 2011
#c 5
#% 46803
#% 111315
#% 201876
#% 213975
#% 300163
#% 311027
#% 427199
#% 465009
#% 480661
#% 643566
#% 730019
#% 835018
#% 875013
#% 907572
#% 1016191
#% 1127435
#% 1181288
#% 1206801
#% 1206889
#% 1206997
#% 1328137
#% 1523828
#! Geographic objects associated with descriptive texts are becoming prevalent. This gives prominence to spatial keyword queries that take into account both the locations and textual descriptions of content. Specifically, the relevance of an object to a query is measured by spatial-textual similarity that is based on both spatial proximity and textual similarity. In this paper, we define Reverse Spatial Textual k Nearest Neighbor (RSTkNN) query, i.e., finding objects that take the query object as one of their k most spatial-textual similar objects. Existing works on reverse kNN queries focus solely on spatial locations but ignore text relevance. To answer RSTkNN queries efficiently, we propose a hybrid index tree called IUR-tree (Intersection-Union R-Tree) that effectively combines location proximity with textual similarity. Based on the IUR-tree, we design a branch-and-bound search algorithm. To further accelerate the query processing, we propose an enhanced variant of the IUR-tree called clustered IUR-tree and two corresponding optimization algorithms. Empirical studies show that the proposed algorithms offer scalability and are capable of excellent performance.

#index 1581876
#* Location-aware type ahead search on spatial databases: semantics and efficiency
#@ Senjuti Basu Roy;Kaushik Chakrabarti
#t 2011
#c 5
#% 95353
#% 201876
#% 285932
#% 333854
#% 335381
#% 408396
#% 410276
#% 464876
#% 801684
#% 879610
#% 960360
#% 1014077
#% 1206801
#% 1217199
#% 1217200
#% 1328137
#% 1678564
#! Users often search spatial databases like yellow page data using keywords to find businesses near their current location. Typing the entire query is cumbersome and prone to errors, especially from mobile phones. We address this problem by introducing type-ahead search functionality on spatial databases. Like keyword search on spatial data, type-ahead search needs to be location-aware, i.e., with every letter being typed, it needs to return spatial objects whose names (or descriptions) are valid completions of the query string typed so far, and which rank highest in terms of proximity to the user's location and other static scores. Existing solutions for type-ahead search cannot be used directly as they are not location-aware. We show that a straight-forward combination of existing techniques for performing type-ahead search with those for performing proximity search perform poorly. We propose a formal model for query processing cost and develop novel techniques that optimize that cost. Our empirical evaluations on real and synthetic datasets demonstrate the effectiveness of our techniques. To the best of our knowledge, this is the first work on location-aware type-ahead search.

#index 1581877
#* Collective spatial keyword querying
#@ Xin Cao;Gao Cong;Christian S. Jensen;Beng Chin Ooi
#t 2011
#c 5
#% 330677
#% 427199
#% 480467
#% 766441
#% 867054
#% 874993
#% 1193539
#% 1206801
#% 1206997
#% 1220089
#% 1299876
#% 1328137
#% 1523828
#! With the proliferation of geo-positioning and geo-tagging, spatial web objects that possess both a geographical location and a textual description are gaining in prevalence, and spatial keyword queries that exploit both location and textual description are gaining in prominence. However, the queries studied so far generally focus on finding individual objects that each satisfy a query rather than finding groups of objects where the objects in a group collectively satisfy a query. We define the problem of retrieving a group of spatial web objects such that the group's keywords cover the query's keywords and such that objects are nearest to the query location and have the lowest inter-object distances. Specifically, we study two variants of this problem, both of which are NP-complete. We devise exact solutions as well as approximate solutions with provable approximation bounds to the problems. We present empirical studies that offer insight into the efficiency and accuracy of the solutions.

#index 1581878
#* Finding semantics in time series
#@ Peng Wang;Haixun Wang;Wei Wang
#t 2011
#c 5
#% 172949
#% 222437
#% 394984
#% 466506
#% 641125
#% 729932
#% 881543
#% 893220
#% 915313
#% 926881
#% 960255
#% 968365
#% 992857
#% 1127609
#% 1206700
#% 1206850
#% 1207013
#% 1318667
#% 1328117
#% 1426517
#% 1451081
#% 1451249
#! In order to understand a complex system, we analyze its output or its log data. For example, we track a system's resource consumption (CPU, memory, message queues of different types, etc) to help avert system failures; we examine economic indicators to assess the severity of a recession; we monitor a patient's heart rate or EEG for disease diagnosis. Time series data is involved in many such applications. Much work has been devoted to pattern discovery from time series data, but not much has attempted to use the time series data to unveil a system's internal dynamics. In this paper, we go beyond learning patterns from time series data. We focus on obtaining a better understanding of its data generating mechanism, and we regard patterns and their temporal relations as organic components of the hidden mechanism. Specifically, we propose to model time series data using a novel pattern-based hidden Markov model (pHMM), which aims at revealing a global picture of the system that generates the time series data. We propose an iterative approach to refine pHMMs learned from the data. In each iteration, we use the current pHMM to guide time series segmentation and clustering, which enables us to learn a more accurate pHMM. Furthermore, we propose three pruning strategies to speed up the refinement process. Empirical results on real datasets demonstrate the feasibility and effectiveness of the proposed approach.

#index 1581879
#* Querying contract databases based on temporal behavior
#@ Elio Damaggio;Alin Deutsch;Dayou Zhou
#t 2011
#c 5
#% 1729
#% 31484
#% 215753
#% 234819
#% 268804
#% 326878
#% 458817
#% 491427
#% 542096
#% 600179
#% 671908
#% 799156
#% 837400
#% 893117
#% 900886
#% 1016160
#% 1068329
#% 1068595
#% 1148382
#% 1408536
#% 1702943
#! Considering a broad definition for service contracts (beyond web services and software, e.g. airline tickets and insurance policies), we tackle the challenges of building a high performance broker in which contracts are both specified and queried through their temporal behavior. The temporal dimension, in conjunction with traditional relational attributes, enables our system to better address difficulties arising from the great deal of information regarding the temporal interaction of the various events cited in contracts (e.g. "No refunds are allowed after a reschedule of the flight, which can be requested only before any flight leg has been used"). On the other hand, querying large repositories of temporal specifications poses an interesting indexing challenge. In this paper, we introduce two distinct and complementary indexing techniques that enable our system to scale the evaluation of a novel and theoretically sound notion of permission of a temporal query by a service contract. Our notion of permission is inspired by previous work on model checking but, given the specific characteristic of our problem, does not reduce to it. We evaluate experimentally our implementation, showing that it scales well with both the number and the complexity of the contracts.

#index 1581880
#* Neighborhood-privacy protected shortest distance computing in cloud
#@ Jun Gao;Jeffrey Xu Yu;Ruoming Jin;Jiashuai Zhou;Tengjiao Wang;Dongqing Yang
#t 2011
#c 5
#% 338382
#% 379482
#% 443208
#% 443533
#% 659992
#% 813718
#% 956511
#% 1063476
#% 1127360
#% 1127417
#% 1206763
#% 1217149
#% 1224354
#% 1292553
#% 1318624
#% 1328173
#% 1328183
#% 1328188
#% 1426454
#% 1426540
#% 1740518
#! With the advent of cloud computing, it becomes desirable to utilize cloud computing to efficiently process complex operations on large graphs without compromising their sensitive information. This paper studies shortest distance computing in the cloud, which aims at the following goals: i) preventing outsourced graphs from neighborhood attack, ii) preserving shortest distances in outsourced graphs, iii) minimizing overhead on the client side. The basic idea of this paper is to transform an original graph G into a link graph Gl kept locally and a set of outsourced graphs Go. Each outsourced graph should meet the requirement of a new security model called 1-neighborhood-d-radius. In addition, the shortest distance query can be answered using Gl and Go. Our objective is to minimize the space cost on the client side when both security and utility requirements are satisfied. We devise a greedy method to produce Gl and Go, which can exactly answer the shortest distance queries. We also develop an efficient transformation method to support approximate shortest distance answering under a given additive error bound. The final experimental results illustrate the effectiveness and efficiency of our method.

#index 1581881
#* On k-skip shortest paths
#@ Yufei Tao;Cheng Sheng;Jian Pei
#t 2011
#c 5
#% 31486
#% 324484
#% 410276
#% 442858
#% 443208
#% 443533
#% 450545
#% 813718
#% 839701
#% 841649
#% 985951
#% 1063472
#% 1181255
#% 1211643
#% 1292553
#% 1328210
#% 1412885
#% 1426510
#% 1482228
#% 1484129
#% 1523971
#% 1676469
#! Given two vertices s, t in a graph, let P be the shortest path (SP) from s to t, and P* a subset of the vertices in P. P* is a k-skip shortest path from s to t, if it includes at least a vertex out of every k consecutive vertices in P. In general, P* succinctly describes P by sampling the vertices in P with a rate of at least 1/k. This makes P* a natural substitute in scenarios where reporting every single vertex of P is unnecessary or even undesired. This paper studies k-skip SP computation in the context of spatial network databases (SNDB). Our technique has two properties crucial for real-time query processing in SNDB. First, our solution is able to answer k-skip queries significantly faster than finding the original SPs in their entirety. Second, the previous objective is achieved with a structure that occupies less space than storing the underlying road network. The proposed algorithms are the outcome of a careful theoretical analysis that reveals valuable insight into the characteristics of the k-skip SP problem. Their efficiency has been confirmed by extensive experiments with real data.

#index 1581882
#* Finding shortest path on land surface
#@ Lian Liu;Raymond Chi-Wing Wong
#t 2011
#c 5
#% 28114
#% 86822
#% 119521
#% 206574
#% 213529
#% 232955
#% 236600
#% 236601
#% 415017
#% 594774
#% 818938
#% 824723
#% 864466
#% 1015321
#% 1058620
#% 1063472
#% 1072648
#% 1127432
#% 1328202
#% 1704006
#! Finding shortest paths is a fundamental operator in spatial databases. Recently, terrain datasets have attracted a lot of attention from both industry and academia. There are some interesting issues to be studied in terrain datasets which cannot be found in a traditional two-dimensional space. In this paper, we study one of the issues called a slope constraint which exists in terrain datasets. In this paper, we propose a problem of finding shortest paths with the slope constraint. Then, we show that this new problem is more general than the traditional problem of finding shortest paths without considering the slope constraint. Since finding shortest paths with the slope constraint is costly, we propose a new framework called surface simplification so that we can compute shortest paths with the slope constraint efficiently. Under this framework, the surface is "simplified" such that the complexity of finding shortest paths on this simplified surface is lower. We conducted experiments to show that the surface simplification is very efficient and effective not only for the new problem with the slope constraint but also the traditional problem without the slope constraint.

#index 1581883
#* WHAM: a high-throughput sequence alignment method
#@ Yinan Li;Allison Terrell;Jignesh M. Patel
#t 2011
#c 5
#% 120648
#% 120649
#% 479819
#% 824678
#% 893164
#% 1022219
#% 1022227
#% 1063496
#% 1099017
#% 1328126
#% 1578185
#! Over the last decade the cost of producing genomic sequences has dropped dramatically due to the current so called "next-gen" sequencing methods. However, these next-gen sequencing methods are critically dependent on fast and sophisticated data processing methods for aligning a set of query sequences to a reference genome using rich string matching models. The focus of this work is on the design, development and evaluation of a data processing system for this crucial "short read alignment" problem. Our system, called WHAM, employs novel hash-based indexing methods and bitwise operations for sequence alignments. It allows richer match models than existing methods and it is significantly faster than the existing state-of-the-art method. In addition, its relative speedup over the existing method is poised to increase in the future in which read sequence lengths will increase. The WHAM code is available at http://www.cs.wisc.edu/wham/.

#index 1581884
#* A new approach for processing ranked subsequence matching based on ranked union
#@ Wook-Shin Han;Jinsoo Lee;Yang-Sae Moon;Seung-won Hwang;Hwanjo Yu
#t 2011
#c 5
#% 136740
#% 137711
#% 172949
#% 232122
#% 333854
#% 397381
#% 443369
#% 464994
#% 480146
#% 480330
#% 654456
#% 659255
#% 784537
#% 810018
#% 893220
#% 927033
#% 960242
#% 993965
#% 1022237
#% 1063497
#% 1328178
#% 1581884
#! Ranked subsequence matching finds top-k subsequences most similar to a given query sequence from data sequences. Recently, Han et al. [12] proposed a solution (referred to here as HLMJ) to this problem by using the concept of the minimum distance matching window pair (MDMWP) and a global priority queue. By using the concept of MDMWP, HLMJ can prune many unnecessary accesses to data subsequences using a lower bound distance. However, we notice that HLMJ may incur serious performance overhead for important types of queries. In this paper, we propose a novel systematic framework to solve this problem by viewing ranked subsequence matching as ranked union. Specifically, we propose a notion of the matching subsequence equivalence class (MSEQ) and a novel lower bound called the MSEQ-distance. To completely eliminate the performance problem of HLMJ, we also propose a cost-aware density-based scheduling technique, where we consider both the density and cost of the priority queue. Extensive experimental results with many real datasets show that the proposed algorithm outperforms HLMJ and the adapted PSM [22], a state-of-the-art index-based merge algorithm supporting non-monotonic distance functions, by up to two to three orders of magnitude, respectively.

#index 1581885
#* Interaction between record matching and data repairing
#@ Wenfei Fan;Jianzhong Li;Shuai Ma;Nan Tang;Wenyuan Yu
#t 2011
#c 5
#% 25443
#% 115608
#% 242237
#% 384978
#% 410276
#% 420072
#% 544266
#% 654467
#% 752741
#% 810014
#% 810019
#% 810044
#% 818438
#% 833132
#% 913783
#% 1022228
#% 1054480
#% 1127443
#% 1130461
#% 1196287
#% 1206834
#% 1292496
#% 1292612
#% 1309312
#% 1328143
#% 1328159
#% 1426508
#% 1426628
#% 1426637
#% 1523810
#% 1523832
#% 1523915
#! Central to a data cleaning system are record matching and data repairing. Matching aims to identify tuples that refer to the same real-world object, and repairing is to make a database consistent by fixing errors in the data by using constraints. These are treated as separate processes in current data cleaning systems, based on heuristic solutions. This paper studies a new problem, namely, the interaction between record matching and data repairing. We show that repairing can effectively help us identify matches, and vice versa. To capture the interaction, we propose a uniform framework that seamlessly unifies repairing and matching operations, to clean a database based on integrity constraints, matching rules and master data. We give a full treatment of fundamental problems associated with data cleaning via matching and repairing, including the static analyses of constraints and rules taken together, and the complexity, termination and determinism analyses of data cleaning. We show that these problems are hard, ranging from NP- or coNP-complete, to PSPACE-complete. Nevertheless, we propose efficient algorithms to clean data via both matching and repairing. The algorithms find deterministic fixes and reliable fixes based on confidence and entropy analysis, respectively, which are more accurate than possible fixes generated by heuristics. We experimentally verify that our techniques significantly improve the accuracy of record matching and data repairing taken as separate processes, using real-life data.

#index 1581886
#* We challenge you to certify your updates
#@ Su Chen;Xin Luna Dong;Laks V.S. Lakshmanan;Divesh Srivastava
#t 2011
#c 5
#% 36683
#% 406493
#% 481290
#% 644182
#% 788999
#% 954971
#% 1063725
#% 1190093
#% 1201863
#% 1252626
#% 1328143
#! Correctness of data residing in a database is vital. While integrity constraint enforcement can often ensure data consistency, it is inadequate to protect against updates that involve careless, unintentional errors, e.g., whether a specified update to an employee's record was for the intended employee. We propose a novel approach that is complementary to existing integrity enforcement techniques, to guard against such erroneous updates. Our approach is based on (a) updaters providing an update certificate with each database update, and (b) the database system verifying the correctness of the update certificate provided before performing the update. We formalize a certificate as a (challenge, response) pair, and characterize good certificates as those that are easy for updaters to provide and, when correct, give the system enough confidence that the update was indeed intended. We present algorithms that efficiently enumerate good challenges, without exhaustively exploring the search space of all challenges. We experimentally demonstrate that (i) databases have many good challenges, (ii) these challenges can be efficiently identified, (iii) certificates can be quickly verified for correctness, (iv) under natural models of an updater's knowledge of the database, update certificates catch a high percentage of the erroneous updates without imposing undue burden on the updaters performing correct updates, and (v) our techniques are robust across a wide range of challenge parameter settings.

#index 1581887
#* Labeling recursive workflow executions on-the-fly
#@ Zhuowei Bao;Susan B. Davidson;Tova Milo
#t 2011
#c 5
#% 58365
#% 88051
#% 271250
#% 378412
#% 379482
#% 379483
#% 379484
#% 453549
#% 593971
#% 765129
#% 765488
#% 875046
#% 1063514
#% 1063545
#% 1174012
#% 1217201
#% 1217208
#% 1426561
#% 1484103
#% 1523819
#! This paper presents a compact labeling scheme for answering reachability queries over workflow executions. In contrast to previous work, our scheme allows nodes (processes and data) in the execution graph to be labeled on-the-fly, i.e., in a dynamic fashion. In this way, reachability queries can be answered as soon as the relevant data is produced. We first show that, in general, for workflows that contain recursion, dynamic labeling of executions requires long (linear-size) labels. Fortunately, most real-life scientific workflows are linear recursive, and for this natural class we show that dynamic, yet compact (logarithmic-size) labeling is possible. Moreover, our scheme labels the executions in linear time, and answers any reachability query in constant time. We also show that linear recursive workflows are, in some sense, the largest class of workflows that allow compact, dynamic labeling schemes. Interestingly, the empirical evaluation, performed over both real and synthetic workflows, shows that our proposed dynamic scheme outperforms the state-of-the-art static scheme for large executions, and creates labels that are shorter by a factor of almost 3.

#index 1581888
#* Tracing data errors with view-conditioned causality
#@ Alexandra Meliou;Wolfgang Gatterbauer;Suman Nath;Dan Suciu
#t 2011
#c 5
#% 309095
#% 318704
#% 378401
#% 378409
#% 449788
#% 464891
#% 528334
#% 800563
#% 809239
#% 960233
#% 976987
#% 1063709
#% 1080863
#% 1127409
#% 1209770
#% 1217186
#% 1231247
#% 1272043
#% 1272198
#% 1328076
#% 1426503
#% 1488677
#% 1523811
#% 1692263
#% 1698719
#% 1728042
#! A surprising query result is often an indication of errors in the query or the underlying data. Recent work suggests using causal reasoning to find explanations for the surprising result. In practice, however, one often has multiple queries and/or multiple answers, some of which may be considered correct and others unexpected. In this paper, we focus on determining the causes of a set of unexpected results, possibly conditioned on some prior knowledge of the correctness of another set of results. We call this problem View-Conditioned Causality. We adapt the definitions of causality and responsibility for the case of multiple answers/views and provide a non-trivial algorithm that reduces the problem of finding causes and their responsibility to a satisfiability problem that can be solved with existing tools. We evaluate both the accuracy and effectiveness of our approach on a real dataset of user-generated mobile device tracking data, and demonstrate that it can identify causes of error more effectively than static Boolean influence and alternative notions of causality.

#index 1581889
#* Hybrid in-database inference for declarative information extraction
#@ Daisy Zhe Wang;Michael J. Franklin;Minos Garofalakis;Joseph M. Hellerstein;Michael L. Wick
#t 2011
#c 5
#% 464434
#% 874976
#% 893167
#% 1016201
#% 1022288
#% 1063521
#% 1063547
#% 1127378
#% 1206687
#% 1206717
#% 1217154
#% 1250184
#% 1523866
#% 1523889
#! In the database community, work on information extraction (IE) has centered on two themes: how to effectively manage IE tasks, and how to manage the uncertainties that arise in the IE process in a scalable manner. Recent work has proposed a probabilistic database (PDB) based declarative IE system that supports a leading statistical IE model, and an associated inference algorithm to answer top-k-style queries over the probabilistic IE outcome. Still, the broader problem of effectively supporting general probabilistic inference inside a PDB-based declarative IE system remains open. In this paper, we explore the in-database implementations of a wide variety of inference algorithms suited to IE, including two Markov chain Monte Carlo algorithms, the Viterbi and the sum-product algorithms. We describe the rules for choosing appropriate inference algorithms based on the model, the query and the text, considering the trade-off between accuracy and runtime. Based on these rules, we describe a hybrid approach to optimize the execution of a single probabilistic IE query to employ different inference algorithms appropriate for different records. We show that our techniques can achieve up to 10-fold speedups compared to the non-hybrid solutions proposed in the literature.

#index 1581890
#* Faerie: efficient filtering algorithms for approximate dictionary-based entity extraction
#@ Guoliang Li;Dong Deng;Jianhua Feng
#t 2011
#c 5
#% 480654
#% 654467
#% 765463
#% 800590
#% 824678
#% 864392
#% 864415
#% 893105
#% 893164
#% 956506
#% 1022218
#% 1022227
#% 1055684
#% 1063530
#% 1127368
#% 1127425
#% 1127426
#% 1206665
#% 1206677
#% 1206821
#% 1217179
#% 1217204
#% 1292497
#% 1328142
#% 1328164
#! Dictionary-based entity extraction identifies predefined entities (e.g., person names or locations) from a document. A recent trend for improving extraction recall is to support approximate entity extraction, which finds all substrings in the document that approximately match entities in a given dictionary. Existing methods to address this problem support either token-based similarity (e.g., Jaccard Similarity) or character-based dissimilarity (e.g., Edit Distance). It calls for a unified method to support various similarity/dissimilarity functions, since a unified method can reduce the programming efforts, hardware requirements, and the manpower. In addition, many substrings in the document have overlaps, and we have an opportunity to utilize the shared computation across the overlaps to avoid unnecessary redundant computation. In this paper, we propose a unified framework to support many similarity/dissimilarity functions, such as jaccard similarity, cosine similarity, dice similarity, edit similarity, and edit distance. We devise efficient filtering algorithms to utilize the shared computation and develop effective pruning techniques to improve the performance. The experimental results show that our method achieves high performance and outperforms state-of-the-art studies.

#index 1581891
#* Joint unsupervised structure discovery and information extraction
#@ Eli Cortez;Daniel Oliveira;Altigran S. da Silva;Edleno S. de Moura;Alberto H.F. Laender
#t 2011
#c 5
#% 44876
#% 278109
#% 300288
#% 333943
#% 413199
#% 464434
#% 531459
#% 660272
#% 769877
#% 769884
#% 864416
#% 874707
#% 948374
#% 1166537
#% 1183369
#% 1426569
#% 1471593
#! In this paper we present JUDIE (Joint Unsupervised Structure Discovery and Information Extraction), a new method for automatically extracting semi-structured data records in the form of continuous text (e.g., bibliographic citations, postal addresses, classified ads, etc.) and having no explicit delimiters between them. While in state-of-the-art Information Extraction methods the structure of the data records is manually supplied the by user as a training step, JUDIE is capable of detecting the structure of each individual record being extracted without any user assistance. This is accomplished by a novel Structure Discovery algorithm that, given a sequence of labels representing attributes assigned to potential values, groups these labels into individual records by looking for frequent patterns of label repetitions among the given sequence. We also show how to integrate this algorithm in the information extraction process by means of successive refinement steps that alternate information extraction and structure discovery. Through an extensively experimental evaluation with different datasets in distinct domains, we compare JUDIE with state-of-the-art information extraction methods and conclude that, even without any user intervention, it is able to achieve high quality results on the tasks of discovering the structure of the records and extracting information from them.

#index 1581892
#* Attribute domain discovery for hidden web databases
#@ Xin Jin;Nan Zhang;Gautam Das
#t 2011
#c 5
#% 268114
#% 340146
#% 413635
#% 480479
#% 654459
#% 765410
#% 769890
#% 875064
#% 875067
#% 879604
#% 893144
#% 943875
#% 956534
#% 960286
#% 993964
#% 1127356
#% 1206906
#% 1328136
#% 1372686
#% 1426573
#% 1472963
#! Many web databases are hidden behind restrictive form-like interfaces which may or may not provide domain information for an attribute. When attribute domains are not available, domain discovery becomes a critical challenge facing the application of a broad range of existing techniques on third-party analytical and mash-up applications over hidden databases. In this paper, we consider the problem of domain discovery over a hidden database through its web interface. We prove that for any database schema, an achievability guarantee on domain discovery can be made based solely upon the interface design. We also develop novel techniques which provide effective guarantees on the comprehensiveness of domain discovery. We present theoretical analysis and extensive experiments to illustrate the effectiveness of our approach.

#index 1581893
#* Keyword search over relational databases: a metadata approach
#@ Sonia Bergamaschi;Elton Domnori;Francesco Guerra;Raquel Trillo Lado;Yannis Velegrakis
#t 2011
#c 5
#% 218982
#% 268079
#% 285926
#% 323104
#% 378409
#% 397373
#% 572314
#% 659990
#% 660001
#% 765408
#% 875017
#% 960243
#% 960360
#% 975019
#% 993981
#% 993987
#% 994033
#% 1006336
#% 1016135
#% 1021948
#% 1021954
#% 1022287
#% 1063536
#% 1129527
#% 1174127
#% 1206910
#% 1217198
#% 1288160
#% 1409952
#% 1413199
#% 1467763
#% 1523959
#! Keyword queries offer a convenient alternative to traditional SQL in querying relational databases with large, often unknown, schemas and instances. The challenge in answering such queries is to discover their intended semantics, construct the SQL queries that describe them and used them to retrieve the respective tuples. Existing approaches typically rely on indices built a-priori on the database content. This seriously limits their applicability if a-priori access to the database content is not possible. Examples include the on-line databases accessed through web interface, or the sources in information integration systems that operate behind wrappers with specific query capabilities. Furthermore, existing literature has not studied to its full extend the inter-dependencies across the ways the different keywords are mapped into the database values and schema elements. In this work, we describe a novel technique for translating keyword queries into SQL based on the Munkres (a.k.a. Hungarian) algorithm. Our approach not only tackles the above two limitations, but it offers significant improvements in the identification of the semantically meaningful SQL queries that describe the intended keyword query semantics. We provide details of the technique implementation and an extensive experimental evaluation.

#index 1581894
#* Sharing work in keyword search over databases
#@ Marie Jacob;Zachary Ives
#t 2011
#c 5
#% 36117
#% 220708
#% 248801
#% 300166
#% 300179
#% 397352
#% 442783
#% 565457
#% 572311
#% 577309
#% 643566
#% 660011
#% 733373
#% 763882
#% 810018
#% 818207
#% 824693
#% 912239
#% 960259
#% 960278
#% 993987
#% 1015317
#% 1015325
#% 1022277
#% 1026989
#% 1063713
#% 1083721
#% 1127413
#% 1206702
#% 1426534
#! An important means of allowing non-expert end-users to pose ad hoc queries whether over single databases or data integration systems is through keyword search. Given a set of keywords, the query processor finds matches across different tuples and tables. It computes and executes a set of relational sub-queries whose results are combined to produce the k highest ranking answers. Work on keyword search primarily focuses on single-database, single-query settings: each query is answered in isolation, despite possible overlap between queries posed by different users or at different times; and the number of relevant tables is assumed to be small, meaning that sub-queries can be processed without using cost-based methods to combine work. As we apply keyword search to support ad hoc data integration queries over scientific or other databases on the Web, we must reuse and combine computation. In this paper, we propose an architecture that continuously receives sets of ranked keyword queries, and seeks to reuse work across these queries. We extend multiple query optimization and continuous query techniques, and develop a new query plan scheduling module we call the ATC (based on its analogy to an air traffic controller). The ATC manages the flow of tuples among a multitude of pipelined operators, minimizing the work needed to return the top-k answers for all queries. We also develop techniques to manage the sharing and reuse of state as queries complete and input data streams are exhausted. We show the effectiveness of our techniques in handling queries over real and synthetic data sets.

#index 1581895
#* Nearest keyword search in XML documents
#@ Yufei Tao;Stavros Papadopoulos;Cheng Sheng;Kostas Stefanidis
#t 2011
#c 5
#% 186
#% 121114
#% 137808
#% 321577
#% 397366
#% 397375
#% 397469
#% 580729
#% 654442
#% 660011
#% 726629
#% 745477
#% 765405
#% 765466
#% 810046
#% 810052
#% 824667
#% 824692
#% 824693
#% 863389
#% 866988
#% 874894
#% 893112
#% 960259
#% 960261
#% 1015258
#% 1015277
#% 1016199
#% 1046515
#% 1063472
#% 1206817
#% 1211643
#% 1370256
#% 1523875
#% 1672945
#! This paper studies the nearest keyword (NK) problem on XML documents. In general, the dataset is a tree where each node is associated with one or more keywords. Given a node q and a keyword w, an NK query returns the node that is nearest to q among all the nodes associated with w. NK search is not only useful as a stand-alone operator but also as a building brick for important tasks such as XPath query evaluation and keyword search. We present an indexing scheme that answers NK queries efficiently, in terms of both practical and worst-case performance. The query cost is provably logarithmic to the number of nodes carrying the query keyword. The proposed scheme occupies space linear to the dataset size, and can be constructed by a fast algorithm. Extensive experimentation confirms our theoretical findings, and demonstrates the effectiveness of NK retrieval as a primitive operator in XML databases.

#index 1581896
#* Efficient and generic evaluation of ranked queries
#@ Wen Jin;Jignesh M. Patel
#t 2011
#c 5
#% 300180
#% 333854
#% 397378
#% 480330
#% 480819
#% 643566
#% 659255
#% 733373
#% 763882
#% 765418
#% 772847
#% 800632
#% 818434
#% 893108
#% 893126
#% 893128
#% 927033
#% 941785
#% 975033
#% 982766
#% 1022243
#% 1063474
#% 1075132
#% 1108671
#% 1206766
#% 1231941
#% 1442467
#! An important feature of the existing methods for ranked top-k processing is to avoid searching all the objects in the underlying dataset, and limiting the number of random accesses to the data. However, the performance of these methods degrades rapidly as the number of random accesses increases. In this paper, we propose a novel and general sequential access scheme for top-k query evaluation, which outperforms existing methods. We extend this scheme to efficiently answer top-k queries in subspace and on dynamic data. We also study the "dual" form of top-k queries called "ranking" queries, which returns the rank of a specified record/object, and propose an exact as well as two approximate solutions. An extensive empirical evaluation validates the robustness and efficiency of our techniques.

#index 1581897
#* Changing flights in mid-air: a model for safely modifying continuous queries
#@ Kyumars Sheykh Esmaili;Tahmineh Sanamrad;Peter M. Fischer;Nesime Tatbul
#t 2011
#c 5
#% 162493
#% 300127
#% 578391
#% 765437
#% 800583
#% 893154
#% 960280
#% 975023
#% 1013780
#% 1015280
#% 1016169
#% 1016170
#% 1022208
#% 1022263
#% 1026989
#% 1328078
#% 1375837
#! Continuous queries can run for unpredictably long periods of time. During their lifetime, these queries may need to be adapted either due to changes in application semantics (e.g., the implementation of a new alert detection policy), or due to changes in the system's behavior (e.g., adapting performance to a changing load). While in previous works query modification has been implicitly utilized to serve specific purposes (e.g., load management), to date no research has been done that defines a general-purpose, reliable, and efficiently implementable model for modifying continuous queries at run-time. In this paper, we introduce a punctuation-based framework that can formally express arbitrary lifecycle operations on the basis of input-output mappings and basic control elements such as start or stop of queries. On top of this foundation, we derive all possible query change methods, each providing different levels of correctness guarantees and performance. We further show how these models can be efficiently realized in a state-of-the-art stream processing engine; we also provide experimental results demonstrating the key performance tradeoffs of the change methods.

#index 1581898
#* How soccer players would do stream joins
#@ Jens Teubner;Rene Mueller
#t 2011
#c 5
#% 266837
#% 273911
#% 340635
#% 411680
#% 444341
#% 479654
#% 742564
#% 810033
#% 853339
#% 913788
#% 1015280
#% 1018723
#% 1127569
#% 1190670
#% 1278375
#% 1424010
#% 1426597
#% 1460191
#% 1523815
#% 1712538
#! In spite of the omnipresence of parallel (multi-core) systems, the predominant strategy to evaluate window-based stream joins is still strictly sequential, mostly just straightforward along the definition of the operation semantics. In this work we present handshake join, a way of describing and executing window-based stream joins that is highly amenable to parallelized execution. Handshake join naturally leverages available hardware parallelism, which we demonstrate with an implementation on a modern multi-core system and on top of field-programmable gate arrays (FPGAs), an emerging technology that has shown distinctive advantages for high-throughput data processing. On the practical side, we provide a join implementation that substantially outperforms CellJoin (the fastest published result) and that will directly turn any degree of parallelism into higher throughput or larger supported window sizes. On the semantic side, our work gives a new intuition of window semantics, which we believe could inspire other stream processing algorithms or ongoing standardization efforts for stream query languages.

#index 1581899
#* BE-tree: an index structure to efficiently match boolean expressions over high-dimensional discrete space
#@ Mohammad Sadoghi;Hans-Arno Jacobsen
#t 2011
#c 5
#% 86945
#% 158911
#% 201878
#% 271199
#% 333938
#% 339055
#% 427199
#% 464888
#% 480331
#% 481956
#% 581678
#% 631962
#% 731408
#% 732883
#% 812783
#% 960263
#% 960342
#% 1134501
#% 1217239
#% 1426502
#% 1523814
#% 1523931
#% 1549840
#! BE-Tree is a novel dynamic tree data structure designed to efficiently index Boolean expressions over a high-dimensional discrete space. BE-Tree copes with both high-dimensionality and expressiveness of Boolean expressions by introducing a novel two-phase space-cutting technique that specifically utilizes the discrete and finite domain properties of the space. Furthermore, BE-Tree employs self-adjustment policies to dynamically adapt the tree as the workload changes. We conduct a comprehensive evaluation to demonstrate the superiority of BE-Tree in comparison with state-of-the-art index structures designed for matching Boolean expressions.

#index 1581900
#* TI: an efficient indexing mechanism for real-time search on tweets
#@ Chun Chen;Feng Li;Beng Chin Ooi;Sai Wu
#t 2011
#c 5
#% 64791
#% 397366
#% 463917
#% 479476
#% 480158
#% 480632
#% 482111
#% 504155
#% 766456
#% 824706
#% 879209
#% 910931
#% 1016165
#% 1040837
#% 1055707
#% 1063490
#% 1074159
#% 1292733
#% 1298864
#% 1355042
#% 1384701
#% 1400018
#% 1426571
#! Real-time search dictates that new contents be made available for search immediately following their creation. From the database perspective, this requirement may be quite easily met by creating an up-to-date index for the contents and measuring search quality by the time gap between insertion time and availability of the index. This approach, however, poses new challenges for micro-blogging systems where thousands of concurrent users may upload their micro-blogs or tweets simultaneously. Due to the high update and query loads, conventional approaches would either fail to index the huge amount of newly created contents in real time or fall short of providing a scalable indexing service. In this paper, we propose a tweet index called the TI (Tweet Index), an adaptive indexing scheme for microblogging systems such as Twitter. The intuition of the TI is to index the tweets that may appear as a search result with high probability and delay indexing some other tweets. This strategy significantly reduces the indexing cost without compromising the quality of the search results. In the TI, we also devise a new ranking scheme by combining the relationship between the users and tweets. We group tweets into topics and update the ranking of a topic dynamically. The experiments on a real Twitter dataset confirm the efficiency of the TI.

#index 1581901
#* More efficient datalog queries: subsumptive tabling beats magic sets
#@ K. Tuncay Tekle;Yanhong A. Liu
#t 2011
#c 5
#% 53400
#% 54225
#% 64421
#% 73005
#% 84986
#% 101623
#% 101647
#% 198473
#% 205234
#% 336719
#% 384978
#% 464589
#% 476452
#% 616956
#% 1022288
#% 1063511
#% 1190115
#% 1211091
#% 1246527
#% 1451381
#% 1451382
#% 1672191
#% 1732822
#! Given a set of Datalog rules, facts, and a query, answers to the query can be inferred bottom-up starting with the facts or top-down starting with the query. The dominant strategies to improve the performance of answering queries are reusing answers to subqueries for top-down methods, and transforming rules based on demand from the query, such as the well-known magic sets transformation, for bottom-up methods. However, the performance of these strategies vary drastically, and the most effective method has remained unknown. This paper describes precise time and space complexity analysis for efficient implementation of Datalog queries using subsumptive tabling, a top-down evaluation method with more reuse of answers than the dominant tabling strategy, and shows that subsumptive tabling beats bottom-up evaluation of rules after magic sets transformation in both time and space complexities. It also describes subsumptive demand transformation, a novel method for transforming the rules so that bottom-up evaluation of the transformed rules mimics subsumptive tabling; we show that the time complexity of bottom-up evaluation after this transformation is equal to the the time complexity of top-down evaluation with subsumptive tabling. The paper further describes subsumption optimization, an optimization to increase the use of subsumption in subsumptive methods, and shows its application in the derivation of a well-known demand-driven pointer analysis algorithm. We support our analyses and comparisons through experiments with applications in ontology queries and program analysis.

#index 1581902
#* Entangled queries: enabling declarative data-driven coordination
#@ Nitin Gupta;Lucja Kot;Sudip Roy;Gabriel Bender;Johannes Gehrke;Christoph Koch
#t 2011
#c 5
#% 32897
#% 50077
#% 96223
#% 207210
#% 214046
#% 284550
#% 295410
#% 394417
#% 452556
#% 461899
#% 464836
#% 644201
#% 1472962
#! Many data-driven social and Web applications involve collaboration and coordination. The vision of declarative data-driven coordination (D3C), proposed in [9], is to support coordination in the spirit of data management: to make it data-centric and to specify it using convenient declarative languages. This paper introduces entangled queries, a language that extends SQL by constraints that allow for the coordinated choice of result tuples across queries originating from different users or applications. It is nontrivial to define a declarative coordination formalism without arriving at the general (NP-complete) Constraint Satisfaction Problem from AI. In this paper, we propose an efficiently enforcible syntactic safety condition that we argue is at the sweet spot where interesting declarative power meets applicability in large scale data management systems and applications. The key computational problem of D3C is to match entangled queries to achieve coordination. We present an efficient matching algorithm which statically analyzes query workloads and merges coordinating entangled queries into compound SQL queries. These can be sent to a standard database system and return only coordinated results. We present the overall architecture of an implemented system that contains our evaluation algorithm; we also evaluate the performance of the matching algorithm experimentally on realistic coordination workloads.

#index 1581903
#* Data generation using declarative constraints
#@ Arvind Arasu;Raghav Kaushik;Jian Li
#t 2011
#c 5
#% 451
#% 44876
#% 172913
#% 223781
#% 333947
#% 333986
#% 576761
#% 824744
#% 864426
#% 893212
#% 902467
#% 937550
#% 960262
#% 961197
#% 1127389
#% 1198232
#% 1217165
#% 1221417
#% 1426448
#% 1523871
#% 1670071
#% 1673564
#! We study the problem of generating synthetic databases having declaratively specified characteristics. This problem is motivated by database system and application testing, data masking, and benchmarking. While the data generation problem has been studied before, prior approaches are either non-declarative or have fundamental limitations relating to data characteristics that they can capture and efficiently support. We argue that a natural, expressive, and declarative mechanism for specifying data characteristics is through cardinality constraints; a cardinality constraint specifies that the output of a query over the generated database have a certain cardinality. While the data generation problem is intractable in general, we present efficient algorithms that can handle a large and useful class of constraints. We include a thorough empirical evaluation illustrating that our algorithms handle complex constraints, scale well as the number of constraints increase, and outperform applicable prior techniques.

#index 1581904
#* Efficient auditing for complex SQL queries
#@ Raghav Kaushik;Ravi Ramamurthy
#t 2011
#c 5
#% 334006
#% 442850
#% 462058
#% 572311
#% 765447
#% 824691
#% 874884
#% 874893
#% 942359
#% 993478
#% 993498
#% 1016172
#% 1206679
#% 1523798
#! We address the problem of data auditing that asks for an audit trail of all users and queries that potentially breached information about sensitive data. A lot of the previous work in data auditing has focused on providing strong privacy guarantees and studied the class of queries that can be audited efficiently while retaining the guarantees. In this paper, we approach data auditing from a different perspective. Our goal is to design an auditing system for arbitrary SQL queries containing constructs such as grouping, aggregation and correlated subqueries. Pivoted on the ability to feasibly address arbitrary queries, we study (1)~what privacy guarantees we can expect, and (2)~how we can efficiently perform auditing.

#index 1581905
#* Exact indexing for support vector machines
#@ Hwanjo Yu;Ilhwan Ko;Youngdae Kim;Seungwon Hwang;Wook-Shin Han
#t 2011
#c 5
#% 294634
#% 309208
#% 333854
#% 341269
#% 420077
#% 479462
#% 480330
#% 659993
#% 800511
#% 800553
#% 814646
#% 823360
#% 865733
#% 875000
#% 905157
#% 944095
#% 960242
#% 1271973
#% 1296967
#% 1538183
#! SVM (Support Vector Machine) is a well-established machine learning methodology popularly used for classification, regression, and ranking. Recently SVM has been actively researched for rank learning and applied to various applications including search engines or relevance feedback systems. A query in such systems is the ranking function F learned by SVM. Once learning a function F or formulating the query, processing the query to find top-k results requires evaluating the entire database by F. So far, there exists no exact indexing solution for SVM functions. Existing top-k query processing algorithms are not applicable to the machine-learned ranking functions, as they often make restrictive assumptions on the query, such as linearity or monotonicity of functions. Existing metric-based or reference-based indexing methods are also not applicable, because data points are invisible in the kernel space (SVM feature space) on which the index must be built. Existing kernel indexing methods return approximate results or fix kernel parameters. This paper proposes an exact indexing solution for SVM functions with varying kernel parameters. We first propose key geometric properties of the kernel space -- ranking instability and ordering stability -- which is crucial for building indices in the kernel space. Based on them, we develop an index structure iKernel and processing algorithms. We then present clustering techniques in the kernel space to enhance the pruning effectiveness of the index. According to our experiments, iKernel is highly effective overall producing 1~5% of evaluation ratio on large data sets. According to our best knowledge, iKernel is the first indexing solution that finds exact top-k results of SVM functions without a full scan of data set.

#index 1581906
#* Local graph sparsification for scalable clustering
#@ Venu Satuluri;Srinivasan Parthasarathy;Yiye Ruan
#t 2011
#c 5
#% 176270
#% 214077
#% 249238
#% 274612
#% 313959
#% 338442
#% 420091
#% 765261
#% 824711
#% 881526
#% 961278
#% 1002007
#% 1013696
#% 1035579
#% 1055741
#% 1061639
#% 1083625
#% 1127445
#% 1214695
#% 1300556
#% 1399992
#% 1400003
#% 1549865
#% 1717175
#% 1727910
#! In this paper we look at how to sparsify a graph i.e. how to reduce the edgeset while keeping the nodes intact, so as to enable faster graph clustering without sacrificing quality. The main idea behind our approach is to preferentially retain the edges that are likely to be part of the same cluster. We propose to rank edges using a simple similarity-based heuristic that we efficiently compute by comparing the minhash signatures of the nodes incident to the edge. For each node, we select the top few edges to be retained in the sparsified graph. Extensive empirical results on several real networks and using four state-of-the-art graph clustering and community discovery algorithms reveal that our proposed approach realizes excellent speedups (often in the range 10-50), with little or no deterioration in the quality of the resulting clusters. In fact, for at least two of the four clustering algorithms, our sparsification consistently enables higher clustering accuracies.

#index 1581907
#* Advancing data clustering via projective clustering ensembles
#@ Francesco Gullo;Carlotta Domeniconi;Andrea Tagarelli
#t 2011
#c 5
#% 36672
#% 273891
#% 274612
#% 279456
#% 397384
#% 466083
#% 494396
#% 551737
#% 722902
#% 726725
#% 770836
#% 785335
#% 785355
#% 789010
#% 796202
#% 799743
#% 800529
#% 871026
#% 937551
#% 948088
#% 1000422
#% 1047783
#% 1117068
#% 1133031
#% 1176988
#% 1318596
#% 1393017
#% 1408779
#% 1484090
#% 1663653
#! Projective Clustering Ensembles (PCE) are a very recent advance in data clustering research which combines the two powerful tools of clustering ensembles and projective clustering.Specifically, PCE enables clustering ensemble methods to handle ensembles composed by projective clustering solutions. PCE has been formalized as an optimization problem with either a two-objective or a single-objective function. Two-objective PCE has shown to generally produce more accurate clustering results than its single-objective counterpart, although it can handle the object-based and feature-based cluster representations only independently of one other. Moreover, both the early formulations of PCE do not follow any of the standard approaches of clustering ensembles, namely instance-based, cluster-based, and hybrid. In this paper, we propose an alternative formulation to the PCE problem which overcomes the above issues. We investigate the drawbacks of the early formulations of PCE and define a new single-objective formulation of the problem. This formulation is capable of treating the object- and feature-based cluster representations as a whole, essentially tying them in a distance computation between a projective clustering solution and a given ensemble. We propose two cluster-based algorithms for computing approximations to the proposed PCE formulation, which have the common merit of conforming to one of the standard approaches of clustering ensembles. Experiments on benchmark datasets have shown the significance of our PCE formulation, as both the proposed heuristics outperform existing PCE methods.

#index 1581908
#* Sampling based algorithms for quantile computation in sensor networks
#@ Zengfeng Huang;Lu Wang;Ke Yi;Yunhao Liu
#t 2011
#c 5
#% 333931
#% 401228
#% 745442
#% 783739
#% 783740
#% 783741
#% 801695
#% 805466
#% 810009
#% 810031
#% 816392
#% 864438
#% 874903
#% 874984
#% 879262
#% 993969
#% 1127608
#% 1217131
#% 1290947
#% 1769951
#% 1772138
#! We study the problem of computing approximate quantiles in large-scale sensor networks communication-efficiently, a problem previously studied by Greenwald and Khana [12] and Shrivastava et al [21]. Their algorithms have a total communication cost of O(k log2 n / ε) and O(k log u / ε), respectively, where k is the number of nodes in the network, n is the total size of the data sets held by all the nodes, u is the universe size, and ε is the required approximation error. In this paper, we present a sampling based quantile computation algorithm with O(√kh/ε) total communication (h is the height of the routing tree), which grows sublinearly with the network size except in the pathological case h=Θ(k). In our experiments on both synthetic and real data sets, this improvement translates into a 10 to 100-fold communication reduction for achieving the same accuracy in the computed quantiles. Meanwhile, the maximum individual node communication of our algorithm is no higher than that of the previous two algorithms.

#index 1581909
#* Context-sensitive ranking for document retrieval
#@ Liang Jeff Chen;Yannis Papakonstantinou
#t 2011
#c 5
#% 121466
#% 210182
#% 227866
#% 273902
#% 348173
#% 443350
#% 451768
#% 464215
#% 479476
#% 479646
#% 479822
#% 481290
#% 577329
#% 729418
#% 745519
#% 766430
#% 788094
#% 805785
#% 818207
#% 818259
#% 869536
#% 875002
#% 919706
#% 956551
#% 987224
#% 987275
#% 1019113
#% 1074070
#% 1074200
#% 1077150
#% 1156208
#% 1156209
#% 1206830
#! We study the problem of context-sensitive ranking for document retrieval, where a context is defined as a sub-collection of documents, and is specified by queries provided by domain-interested users. The motivation of context-sensitive search is that the ranking of the same keyword query generally depends on the context. The reason is that the underlying keyword statistics differ significantly from one context to another. The query evaluation challenge is the computation of keyword statistics at runtime, which involves expensive online aggregations. We appropriately leverage and extend materialized view research in order to deliver algorithms and data structures that evaluate context-sensitive queries efficiently. Specifically, a number of views are selected and materialized, each corresponding to one or more large contexts. Materialized views are used at query time to compute statistics which are used to compute ranking scores. Experimental results show that the context-sensitive ranking generally improves the ranking quality, while our materialized view-based technique improves the query efficiency.

#index 1581910
#* Score-consistent algebraic optimization of full-text search queries with GRAFT
#@ Nathan Bales;Alin Deutsch;Vasilis Vassalos
#t 2011
#c 5
#% 194247
#% 210169
#% 275308
#% 287333
#% 384978
#% 387427
#% 481608
#% 598376
#% 654442
#% 750867
#% 753895
#% 777931
#% 874694
#% 875018
#% 881737
#% 987229
#% 1021954
#% 1077150
#% 1124990
#% 1674737
#% 1688266
#% 1715628
#! We address two open problems involving algebraic execution of full-text search queries. First, we show how to correctly apply traditional database rewrite optimizations to full-text algebra plans with integrated scoring, and explain why existing techniques fail. Second, we show how our techniques are applied in a generic scoring framework that supports a wide class of scoring algorithms, including algorithms seen in the literature and user-defined scoring.

#index 1581911
#* Efficient diversity-aware search
#@ Albert Angel;Nick Koudas
#t 2011
#c 5
#% 262112
#% 333854
#% 397133
#% 805841
#% 818266
#% 879618
#% 879686
#% 893128
#% 946521
#% 1063539
#% 1069072
#% 1074105
#% 1074133
#% 1077150
#% 1166473
#% 1181244
#% 1181290
#% 1190093
#% 1206662
#% 1214650
#% 1217245
#% 1292528
#% 1312812
#% 1581911
#! Typical approaches of ranking information in response to a user's query that return the most relevant results ignore important factors contributing to user satisfaction; for instance, the contents of a result document may be redundant given the results already examined. Motivated by emerging applications, in this work we study the problem of Diversity-Aware Search, the essence of which is ranking search results based on both their relevance, as well as their dissimilarity to other results reported. Diversity-Aware Search is generally a hard problem, and even tractable instances thereof cannot be efficiently solved by adapting existing approaches. We propose DIVGEN, an efficient algorithm for diversity-aware search, which achieves significant performance improvements via novel data access primitives. Although selecting the optimal schedule of data accesses is a hard problem, we devise the first low-overhead data access prioritization scheme with theoretical quality guarantees, and good performance in practice. A comprehensive evaluation on real and synthetic large-scale corpora demonstrates the efficiency and effectiveness of our approach.

#index 1581912
#* Mining a search engine's corpus: efficient yet unbiased sampling and aggregate estimation
#@ Mingyang Zhang;Nan Zhang;Gautam Das
#t 2011
#c 5
#% 268114
#% 340146
#% 480810
#% 809418
#% 869499
#% 956534
#% 960286
#% 993964
#% 1077150
#% 1091268
#% 1206906
#% 1372686
#% 1426573
#! Search engines over document corpora typically provide keyword-search interfaces. Examples include search engines over the web as well as those over enterprise and government websites. The corpus of such a search engine forms a rich source of information of analytical interest to third parties, but the only available access is by issuing search queries through its interface. To support data analytics over a search engine's corpus, one needs to address two main problems, the sampling of documents (for offline analytics) and the direct (online) estimation of aggregates, while issuing a small number of queries through the keyword-search interface. Existing work on sampling produces samples with unknown bias and may incur an extremely high query cost. Existing aggregate estimation technique suffers from a similar problem, as the estimation error and query cost can both be large for certain aggregates. We propose novel techniques which produce unbiased samples as well as unbiased aggregate estimates with small variances while incurring a query cost an order of magnitude smaller than the existing techniques. We present theoretical analysis and extensive experiments to illustrate the effectiveness of our approach.

#index 1581913
#* Ranking with uncertain scoring functions: semantics and sensitivity measures
#@ Mohamed A. Soliman;Ihab F. Ilyas;Davide Martinenghi;Marco Tagliasacchi
#t 2011
#c 5
#% 34408
#% 78064
#% 144869
#% 235114
#% 300180
#% 330769
#% 397608
#% 480819
#% 1015317
#% 1075132
#% 1206905
#% 1217141
#% 1456853
#% 1523852
#! Ranking queries report the top-K results according to a user-defined scoring function. A widely used scoring function is the weighted summation of multiple scores. Often times, users cannot precisely specify the weights in such functions in order to produce the preferred order of results. Adopting uncertain/incomplete scoring functions (e.g., using weight ranges and partially-specified weight preferences) can better capture user's preferences in this scenario. In this paper, we study two aspects in uncertain scoring functions. The first aspect is the semantics of ranking queries, and the second aspect is the sensitivity of computed results to refinements made by the user. We formalize and solve multiple problems under both aspects, and present novel techniques that compute query results efficiently to comply with the interactive nature of these problems.

#index 1581914
#* Querying uncertain data with aggregate constraints
#@ Mohan Yang;Haixun Wang;Haiquan Chen;Wei-Shinn Ku
#t 2011
#c 5
#% 508
#% 109572
#% 824764
#% 864417
#% 893189
#% 960270
#% 992830
#% 1022341
#% 1063521
#% 1127376
#% 1206717
#% 1206747
#% 1217154
#% 1227589
#% 1230283
#% 1269495
#% 1269701
#% 1273577
#% 1291116
#% 1426506
#% 1478526
#% 1523884
#% 1592313
#! Data uncertainty arises in many situations. A common approach to query processing uncertain data is to sample many "possible worlds" from the uncertain data and to run queries against the possible worlds. However, sampling is not a trivial task, as a randomly sampled possible world may not satisfy known constraints imposed on the data. In this paper, we focus on an important category of constraints, the aggregate constraints. An aggregate constraint is placed on a set of records instead of on a single record, and a real-life system usually has a large number of aggregate constraints. It is a challenging task to find qualified possible worlds in this scenario, since tuple by tuple sampling is extremely inefficient because it rarely leads to a qualified possible world. In this paper, we introduce two approaches for querying uncertain data with aggregate constraints: constraint aware sampling and MCMC sampling. Our experiments show that the new approaches lead to high quality query results with reasonable cost.

#index 1581915
#* Jigsaw: efficient optimization over uncertain enterprise data
#@ Oliver A. Kennedy;Suman Nath
#t 2011
#c 5
#% 144734
#% 810098
#% 874976
#% 1010488
#% 1022291
#% 1063521
#% 1063529
#% 1063568
#% 1206570
#% 1217180
#% 1328072
#% 1523865
#! Probabilistic databases, in particular ones that allow users to externally define models or probability distributions -- so called VG-Functions -- are an ideal tool for constructing, simulating and analyzing hypothetical business scenarios. Enterprises often use such tools with parameterized models and need to explore a large parameter space in order to discover parameter values that optimize for a given goal. Parameter space is usually very large, making such exploration extremely expensive. We present Jigsaw, a probabilistic database-based simulation framework that addresses this performance problem. In Jigsaw, users define what-if style scenarios as parameterized probabilistic database queries and identify parameter values that achieve desired properties. Jigsaw uses a novel "fingerprinting" technique that efficiently identifies correlations between a query's output distribution for different parameter values. Using fingerprints, Jigsaw is able to reuse work performed for different parameter values, and obtain speedups of as much as 2 orders of magnitude for several real business scenarios.

#index 1581916
#* Sensitivity analysis and explanations for robust query evaluation in probabilistic databases
#@ Bhargav Kanagal;Jian Li;Amol Deshpande
#t 2011
#c 5
#% 136358
#% 336306
#% 464891
#% 528334
#% 654487
#% 864394
#% 893168
#% 976987
#% 991156
#% 1016201
#% 1022259
#% 1058620
#% 1063521
#% 1063711
#% 1127408
#% 1127414
#% 1206717
#% 1231247
#% 1289151
#% 1289557
#% 1289563
#% 1291123
#% 1426558
#% 1471192
#% 1488677
#% 1523890
#% 1674531
#! Probabilistic database systems have successfully established themselves as a tool for managing uncertain data. However, much of the research in this area has focused on efficient query evaluation and has largely ignored two key issues that commonly arise in uncertain data management: First, how to provide explanations for query results, e.g., Why is this tuple in my result? or Why does this output tuple have such high probability?. Second, the problem of determining the sensitive input tuples for the given query, e.g., users are interested to know the input tuples that can substantially alter the output, when their probabilities are modified (since they may be unsure about the input probability values). Existing systems provide the lineage/provenance of each of the output tuples in addition to the output probabilities, which is a boolean formula indicating the dependence of the output tuple on the input tuples. However, lineage does not immediately provide a quantitative relationship and it is not informative when we have multiple output tuples. In this paper, we propose a unified framework that can handle both the issues mentioned above to facilitate robust query processing. We formally define the notions of influence and explanations and provide algorithms to determine the top-l influential set of variables and the top-l set of explanations for a variety of queries, including conjunctive queries, probabilistic threshold queries, top-k queries and aggregation queries. Further, our framework naturally enables highly efficient incremental evaluation when input probabilities are modified (e.g., if uncertainty is resolved). Our preliminary experimental results demonstrate the benefits of our framework for performing robust query processing over probabilistic databases.

#index 1581917
#* Graph cube: on warehousing and OLAP multidimensional networks
#@ Peixiang Zhao;Xiaolei Li;Dong Xin;Jiawei Han
#t 2011
#c 5
#% 210182
#% 223781
#% 273697
#% 420053
#% 481749
#% 824711
#% 846209
#% 860097
#% 881526
#% 994154
#% 1016173
#% 1022205
#% 1063501
#% 1063512
#% 1063518
#% 1063535
#% 1176876
#% 1176884
#% 1328169
#% 1372657
#% 1428692
#% 1441395
#! We consider extending decision support facilities toward large sophisticated networks, upon which multidimensional attributes are associated with network entities, thereby forming the so-called multidimensional networks. Data warehouses and OLAP (Online Analytical Processing) technology have proven to be effective tools for decision support on relational data. However, they are not well-equipped to handle the new yet important multidimensional networks. In this paper, we introduce Graph Cube, a new data warehousing model that supports OLAP queries effectively on large multidimensional networks. By taking account of both attribute aggregation and structure summarization of the networks, Graph Cube goes beyond the traditional data cube model involved solely with numeric value based group-by's, thus resulting in a more insightful and structure-enriched aggregate network within every possible multidimensional space. Besides traditional cuboid queries, a new class of OLAP queries, crossboid, is introduced that is uniquely useful in multidimensional networks and has not been studied before. We implement Graph Cube by combining special characteristics of multidimensional networks with the existing well-studied data cube techniques. We perform extensive experimental studies on a series of real world data sets and Graph Cube is shown to be a powerful and efficient tool for decision support on large multidimensional networks.

#index 1581918
#* MaSM: efficient online updates in data warehouses
#@ Manos Athanassoulis;Shimin Chen;Anastasia Ailamaki;Phillip B. Gibbons;Radu Stoica
#t 2011
#c 5
#% 172968
#% 201869
#% 208047
#% 391842
#% 442850
#% 824697
#% 978404
#% 1022221
#% 1068971
#% 1127391
#% 1217226
#% 1222046
#% 1328052
#% 1426547
#% 1426580
#% 1452238
#% 1523922
#% 1557843
#! Data warehouses have been traditionally optimized for read-only query performance, allowing only offline updates at night, essentially trading off data freshness for performance. The need for 24x7 operations in global markets and the rise of online and other quickly-reacting businesses make concurrent online updates increasingly desirable. Unfortunately, state-of-the-art approaches fall short of supporting fast analysis queries over fresh data. The conventional approach of performing updates in place can dramatically slow down query performance, while prior proposals using differential updates either require large in-memory buffers or may incur significant update migration cost. This paper presents a novel approach for supporting online updates in data warehouses that overcomes the limitations of prior approaches, by making judicious use of available SSDs to cache incoming updates. We model the problem of query processing with differential updates as a type of outer join between the data residing on disks and the updates residing on SSDs. We present MaSM algorithms for performing such joins and periodic migrations, with small memory footprints, low query overhead, low SSD writes, efficient in-place migration of updates, and correct ACID support. Our experiments show that MaSM incurs only up to 7% overhead both on synthetic range scans (varying range size from 100GB to 4KB) and in a TPC-H query replay study, while also increasing the update throughput by orders of magnitude.

#index 1581919
#* Latent OLAP: data cubes over latent variables
#@ Deepak Agarwal;Bee-Chung Chen
#t 2011
#c 5
#% 273916
#% 333925
#% 347881
#% 420053
#% 459025
#% 818916
#% 824734
#% 903016
#% 960292
#% 989572
#% 992830
#% 993958
#% 1015294
#% 1063528
#% 1127378
#% 1189215
#% 1318590
#% 1451160
#! We introduce a novel class of data cube, called latent-variable cube. For many data analysis tasks, data in a database can be represented as points in a multi-dimensional space. Ordinary data cubes compute aggregate functions over these "observed" data points for each cell (i.e., region) in the space, where the cells have different granularities defined by hierarchies. While useful, data cubes do not provide sufficient capability for analyzing "latent variables" that are often of interest but not directly observed in data. For example, when analyzing users' interaction with online advertisements, observed data informs whether a user clicked an ad or not. However, the real interest is often in knowing the click probabilities of ads for different user populations. In this example, click probabilities are latent variables that are not observed but have to be estimated from data. We argue that latent variables are a useful construct for a number of OLAP application scenarios. To facilitate such analyses, we propose cubes that compute aggregate functions over latent variables. Specifically, we discuss the pitfalls of common practice in scenarios where latent variables should, but are not considered; we rigorously define latent-variable cube based on Bayesian hierarchical models and provide efficient algorithms. Through extensive experiments on both simulated and real data, we show that our method is accurate and runs orders of magnitude faster than the baseline.

#index 1581920
#* E-Cube: multi-dimensional event sequence analysis using hierarchical pattern query sharing
#@ Mo Liu;Elke Rundensteiner;Kara Greenfield;Chetan Gupta;Song Wang;Ismail Ari;Abhay Mehta
#t 2011
#c 5
#% 14513
#% 36117
#% 210182
#% 223781
#% 300166
#% 411750
#% 480964
#% 481448
#% 481604
#% 765437
#% 810033
#% 838409
#% 846209
#% 874999
#% 875004
#% 875022
#% 893134
#% 893139
#% 893157
#% 1063518
#% 1181223
#% 1207016
#% 1217161
#% 1255307
#% 1594604
#! Many modern applications, including online financial feeds, tag-based mass transit systems and RFID-based supply chain management systems transmit real-time data streams. There is a need for event stream processing technology to analyze this vast amount of sequential data to enable online operational decision making. Existing techniques such as traditional online analytical processing (OLAP) systems are not designed for real-time pattern-based operations, while state-of-the-art Complex Event Processing (CEP) systems designed for sequence detection do not support OLAP operations. We propose a novel E-Cube model which combines CEP and OLAP techniques for efficient multi-dimensional event pattern analysis at different abstraction levels. Our analysis of the interrelationships in both concept abstraction and pattern refinement among queries facilitates the composition of these queries into an integrated E-Cube hierarchy. Based on this E-Cube hierarchy, strategies of drill-down (refinement from abstract to more specific patterns) and of roll-up (generalization from specific to more abstract patterns) are developed for the efficient workload evaluation. Our proposed execution strategies reuse intermediate results along both the concept and the pattern refinement relationships between queries. Based on this foundation, we design a cost-driven adaptive optimizer called Chase, that exploits the above reuse strategies for optimal E-Cube hierarchy execution. Our experimental studies comparing alternate strategies on a real world financial data stream under different workload conditions demonstrate the superiority of the Chase method. In particular, our Chase execution in many cases performs ten fold faster than the state-of-the art strategy for real stock market query workloads.

#index 1581921
#* Neighborhood based fast graph search in large networks
#@ Arijit Khan;Nan Li;Xifeng Yan;Ziyu Guan;Supriyo Chakraborty;Shu Tao
#t 2011
#c 5
#% 288780
#% 333854
#% 378391
#% 397359
#% 480918
#% 601159
#% 654452
#% 654467
#% 754117
#% 765429
#% 769891
#% 810072
#% 844291
#% 864425
#% 906561
#% 937108
#% 960305
#% 989645
#% 1022280
#% 1044450
#% 1127380
#% 1206703
#% 1318714
#% 1426574
#% 1506217
#% 1523818
#% 1523898
#% 1523900
#! Complex social and information network search becomes important with a variety of applications. In the core of these applications, lies a common and critical problem: Given a labeled network and a query graph, how to efficiently search the query graph in the target network. The presence of noise and the incomplete knowledge about the structure and content of the target network make it unrealistic to find an exact match. Rather, it is more appealing to find the top-k approximate matches. In this paper, we propose a neighborhood-based similarity measure that could avoid costly graph isomorphism and edit distance computation. Under this new measure, we prove that subgraph similarity search is NP hard, while graph similarity match is polynomial. By studying the principles behind this measure, we found an information propagation model that is able to convert a large network into a set of multidimensional vectors, where sophisticated indexing and similarity search algorithms are available. The proposed method, called Ness (Neighborhood Based Similarity Search), is appropriate for graphs with low automorphism and high noise, which are common in many social and information networks. Ness is not only efficient, but also robust against structural noise and information loss. Empirical results show that it can quickly and accurately find high-quality matches in large networks, with negligible cost.

#index 1581922
#* A memory efficient reachability data structure through bit vector compression
#@ Sebastiaan J. van Schaik;Oege de Moor
#t 2011
#c 5
#% 58365
#% 139176
#% 175642
#% 179882
#% 244333
#% 288232
#% 327432
#% 335349
#% 379482
#% 480081
#% 866981
#% 1063514
#% 1194484
#% 1194592
#% 1217208
#! When answering many reachability queries on a large graph, the principal challenge is to represent the transitive closure of the graph compactly, while still allowing fast membership tests on that transitive closure. Recent attempts to address this problem are complex data structures and algorithms such as Path-Tree and 3-HOP. We propose a simple alternative based on a novel form of bit-vector compression. Our starting point is the observation that when computing the transitive closure, reachable vertices tend to cluster together. We adapt the well-known scheme of word-aligned hybrid compression (WAH) to work more efficiently by introducing word partitions. We prove that the resulting scheme leads to a more compact data structure than its closest competitor, namely interval lists. In extensive and detailed experiments, this is confirmed in practice. We also demonstrate that the new technique can handle much larger graphs than alternative algorithms.

#index 1581923
#* Incremental graph pattern matching
#@ Wenfei Fan;Jianzhong Li;Jizhou Luo;Zijing Tan;Xin Wang;Yinghui Wu
#t 2011
#c 5
#% 142231
#% 205419
#% 211658
#% 288990
#% 291299
#% 341672
#% 408396
#% 462213
#% 479629
#% 593696
#% 729936
#% 754058
#% 881523
#% 905886
#% 937549
#% 1023420
#% 1083734
#% 1206924
#% 1217191
#% 1291641
#% 1292553
#% 1407271
#% 1506210
#% 1523818
#! Graph pattern matching has become a routine process in emerging applications such as social networks. In practice a data graph is typically large, and is frequently updated with small changes. It is often prohibitively expensive to recompute matches from scratch via batch algorithms when the graph is updated. With this comes the need for incremental algorithms that compute changes to the matches in response to updates, to minimize unnecessary recomputation. This paper investigates incremental algorithms for graph pattern matching defined in terms of graph simulation, bounded simulation and subgraph isomorphism. (1) For simulation, we provide incremental algorithms for unit updates and certain graph patterns. These algorithms are optimal: in linear time in the size of the changes in the input and output, which characterizes the cost that is inherent to the problem itself. For general patterns we show that the incremental matching problem is unbounded, i.e., its cost is not determined by the size of the changes alone. (2) For bounded simulation, we show that the problem is unbounded even for unit updates and path patterns. (3) For subgraph isomorphism, we show that the problem is intractable and unbounded for unit updates and path patterns. (4) For multiple updates, we develop an incremental algorithm for each of simulation, bounded simulation and subgraph isomorphism. We experimentally verify that these incremental algorithms significantly outperform their batch counterparts in response to small changes, using real-life data and synthetic data.

#index 1581924
#* Assessing and ranking structural correlations in graphs
#@ Ziyu Guan;Jian Wu;Qing Zhang;Ambuj Singh;Xifeng Yan
#t 2011
#c 5
#% 152934
#% 220708
#% 252401
#% 310514
#% 577329
#% 729923
#% 729968
#% 730089
#% 823347
#% 824711
#% 881472
#% 881553
#% 995140
#% 1073984
#% 1083624
#% 1130854
#% 1217126
#% 1328169
#% 1399940
#% 1399993
#% 1446960
#% 1451243
#% 1835483
#! Real-life graphs not only have nodes and edges, but also have events taking place, e.g., product sales in social networks and virus infection in communication networks. Among different events, some exhibit strong correlation with the network structure, while others do not. Such structural correlation will shed light on viral influence existing in the corresponding network. Unfortunately, the traditional association mining concept is not applicable in graphs since it only works on homogeneous datasets like transactions and baskets. We propose a novel measure for assessing such structural correlations in heterogeneous graph datasets with events. The measure applies hitting time to aggregate the proximity among nodes that have the same event. In order to calculate the correlation scores for many events in a large network, we develop a scalable framework, called gScore, using sampling and approximation. By comparing to the situation where events are randomly distributed in the same network, our method is able to discover events that are highly correlated with the graph structure. gScore is scalable and was successfully applied to the co-author DBLP network and social networks extracted from TaoBao.com, the largest online shopping network in China, with many interesting discoveries.

#index 1581925
#* Processing theta-joins using MapReduce
#@ Alper Okcan;Mirek Riedewald
#t 2011
#c 5
#% 115661
#% 136740
#% 156405
#% 480774
#% 480966
#% 954300
#% 960326
#% 963669
#% 983467
#% 1063553
#% 1085307
#% 1217159
#% 1372690
#% 1426481
#% 1426543
#% 1426584
#! Joins are essential for many data analysis tasks, but are not supported directly by the MapReduce paradigm. While there has been progress on equi-joins, implementation of join algorithms in MapReduce in general is not sufficiently understood. We study the problem of how to map arbitrary join conditions to Map and Reduce functions, i.e., a parallel infrastructure that controls data flow based on key-equality only. Our proposed join model simplifies creation of and reasoning about joins in MapReduce. Using this model, we derive a surprisingly simple randomized algorithm, called 1-Bucket-Theta, for implementing arbitrary joins (theta-joins) in a single MapReduce job. This algorithm only requires minimal statistics (input cardinality) and we provide evidence that for a variety of join problems, it is either close to optimal or the best possible option. For some of the problems where 1-Bucket-Theta is not the best choice, we show how to achieve better performance by exploiting additional input statistics. All algorithms can be made 'memory-aware', and they do not require any modifications to the MapReduce environment. Experiments show the effectiveness of our approach.

#index 1581926
#* Llama: leveraging columnar storage for scalable join processing in the MapReduce framework
#@ Yuting Lin;Divyakant Agrawal;Chun Chen;Beng Chin Ooi;Sai Wu
#t 2011
#c 5
#% 286258
#% 287349
#% 480821
#% 571047
#% 824697
#% 875026
#% 963669
#% 978404
#% 1016235
#% 1085307
#% 1328186
#% 1372690
#% 1426494
#% 1426584
#% 1468411
#% 1523824
#% 1523837
#% 1523841
#% 1583573
#% 1602033
#! To achieve high reliability and scalability, most large-scale data warehouse systems have adopted the cluster-based architecture. In this paper, we propose the design of a new cluster-based data warehouse system, LLama, a hybrid data management system which combines the features of row-wise and column-wise database systems. In Llama, columns are formed into correlation groups to provide the basis for the vertical partitioning of tables. Llama employs a distributed file system (DFS) to disseminate data among cluster nodes. Above the DFS, a MapReduce-based query engine is supported. We design a new join algorithm to facilitate fast join processing. We present a performance study on TPC-H dataset and compare Llama with Hive, a data warehouse infrastructure built on top of Hadoop. The experiment is conducted on EC2. The results show that Llama has an excellent load performance and its query performance is significantly better than the traditional MapReduce framework based on row-wise storage.

#index 1581927
#* Fast personalized PageRank on MapReduce
#@ Bahman Bahmani;Kaushik Chakrabarti;Dong Xin
#t 2011
#c 5
#% 309868
#% 340932
#% 348173
#% 577329
#% 730089
#% 805897
#% 869492
#% 960326
#% 963669
#% 983330
#% 983467
#% 987222
#% 1016177
#% 1063553
#% 1063716
#% 1127559
#% 1245882
#% 1318636
#% 1446955
#% 1467704
#! In this paper, we design a fast MapReduce algorithm for Monte Carlo approximation of personalized PageRank vectors of all the nodes in a graph. The basic idea is very efficiently doing single random walks of a given length starting at each node in the graph. More precisely, we design a MapReduce algorithm, which given a graph G and a length », outputs a single random walk of length » starting at each node in G. We will show that the number of MapReduce iterations used by our algorithm is optimal among a broad family of algorithms for the problem, and its I/O efficiency is much better than the existing candidates. We will then show how we can use this algorithm to very efficiently approximate all the personalized PageRank vectors. Our empirical evaluation on real-life graph data and in production MapReduce environment shows that our algorithm is significantly more efficient than all the existing algorithms in the MapReduce setting.

#index 1581928
#* A platform for scalable one-pass analytics using MapReduce
#@ Boduo Li;Edward Mazur;Yanlei Diao;Andrew McGregor;Prashant Shenoy
#t 2011
#c 5
#% 3771
#% 115661
#% 210206
#% 479920
#% 960326
#% 963669
#% 1015282
#% 1063553
#% 1127559
#% 1217130
#% 1217159
#% 1278390
#% 1328095
#% 1354118
#% 1426488
#% 1426544
#% 1468411
#% 1523837
#% 1523918
#% 1535212
#! Today's one-pass analytics applications tend to be data-intensive in nature and require the ability to process high volumes of data efficiently. MapReduce is a popular programming model for processing large datasets using a cluster of machines. However, the traditional MapReduce model is not well-suited for one-pass analytics, since it is geared towards batch processing and requires the data set to be fully loaded into the cluster before running analytical queries. This paper examines, from a systems standpoint, what architectural design changes are necessary to bring the benefits of the MapReduce model to incremental one-pass analytics. Our empirical and theoretical analyses of Hadoop-based MapReduce systems show that the widely-used sort-merge implementation for partitioning and parallel processing poses a fundamental barrier to incremental one-pass analytics, despite various optimizations. To address these limitations, we propose a new data analysis platform that employs hash techniques to enable fast in-memory processing, and a new frequent key based technique to extend such processing to workloads that require a large key-state space. Evaluation of our Hadoop-based prototype using real-world workloads shows that our new platform significantly improves the progress of map tasks, allows the reduce progress to keep up with the map progress, with up to 3 orders of magnitude reduction of internal data spills, and enables results to be returned continuously during the job.

#index 1581929
#* ATLAS: a probabilistic algorithm for high dimensional similarity search
#@ Jiaqi Zhai;Yin Lou;Johannes Gehrke
#t 2011
#c 5
#% 205305
#% 249238
#% 249321
#% 340309
#% 347225
#% 464888
#% 479649
#% 760805
#% 765463
#% 823403
#% 869500
#% 879600
#% 883971
#% 893164
#% 915290
#% 956506
#% 956521
#% 976994
#% 987247
#% 989512
#% 1055684
#% 1219786
#% 1327719
#% 1426543
#% 1502531
#! Given a set of high dimensional binary vectors and a similarity function (such as Jaccard and Cosine), we study the problem of finding all pairs of vectors whose similarity exceeds a given threshold. The solution to this problem is a key component in many applications with feature-rich objects, such as text, images, music, videos, or social networks. In particular, there are many important emerging applications that require the use of relatively low similarity thresholds. We propose ATLAS, a probabilistic similarity search algorithm that in expectation finds a 1 - δ fraction of all similar vector pairs. ATLAS uses truly random permutations both to filter candidate pairs of vectors and to estimate the similarity between vectors. At a 97.5% recall rate, ATLAS consistently outperforms all state-of-the-art approaches and achieves a speed-up of up to two orders of magnitude over both exact and approximate algorithms.

#index 1581930
#* Flexible aggregate similarity search
#@ Yang Li;Feifei Li;Ke Yi;Bin Yao;Min Wang
#t 2011
#c 5
#% 201876
#% 235114
#% 237187
#% 264161
#% 287466
#% 318703
#% 333854
#% 479462
#% 479973
#% 487828
#% 654466
#% 745464
#% 760985
#% 762055
#% 813973
#% 814646
#% 814650
#% 836178
#% 1130826
#% 1217189
#% 1633081

#index 1581931
#* Effective data co-reduction for multimedia similarity search
#@ Zi Huang;Hengtao Shen;Jiajun Liu;Xiaofang Zhou
#t 2011
#c 5
#% 342828
#% 469422
#% 479462
#% 479649
#% 479973
#% 480133
#% 480307
#% 632035
#% 745496
#% 800570
#% 810069
#% 814646
#% 860956
#% 946436
#% 1016195
#% 1022281
#% 1023422
#% 1040539
#% 1063482
#% 1063484
#% 1063729
#% 1131854
#% 1214657
#% 1217189
#% 1328110
#% 1371474
#! Multimedia similarity search has been playing a critical role in many novel applications. Typically, multimedia objects are described by high-dimensional feature vectors (or points) which are organized in databases for retrieval. Although many high-dimensional indexing methods have been proposed to facilitate the search process, efficient retrieval over large, sparse and extremely high-dimensional databases remains challenging due to the continuous increases in data size and feature dimensionality. In this paper, we propose the first framework for Data Co-Reduction (DCR) on both data size and feature dimensionality. By utilizing recently developed co-clustering methods, DCR simultaneously reduces both size and dimensionality of the original data into a compact subspace, where lower bounds of the actual distances in the original space can be efficiently established to achieve fast and lossless similarity search in the filter-and refine approach. Particularly, DCR considers the duality between size and dimensionality, and achieves the optimal coreduction which generates the least number of candidates for actual distance computations. We conduct an extensive experimental study on large and real-life multimedia datasets, with dimensionality ranging from 432 to 1936. Our results demonstrate that DCR outperforms existing methods significantly for lossless retrieval, especially in the presence of extremely high dimensionality.

#index 1581932
#* Efficient exact edit similarity query processing with the asymmetric signature scheme
#@ Jianbin Qin;Wei Wang;Yifei Lu;Chuan Xiao;Xuemin Lin
#t 2011
#c 5
#% 255137
#% 288885
#% 333679
#% 345087
#% 347225
#% 427199
#% 443326
#% 479462
#% 479973
#% 480482
#% 480654
#% 616528
#% 745466
#% 765463
#% 823364
#% 864392
#% 893164
#% 937081
#% 956506
#% 987258
#% 1022227
#% 1063496
#% 1074121
#% 1127425
#% 1206665
#% 1217200
#% 1217204
#% 1267035
#% 1328106
#% 1328110
#% 1387564
#% 1426578
#% 1523863
#% 1523903
#! Given a query string Q, an edit similarity search finds all strings in a database whose edit distance with Q is no more than a given threshold t. Most existing method answering edit similarity queries rely on a signature scheme to generate candidates given the query string. We observe that the number of signatures generated by existing methods is far greater than the lower bound, and this results in high query time and index space complexities. In this paper, we show that the minimum signature size lower bound is t +1. We then propose asymmetric signature schemes that achieve this lower bound. We develop efficient query processing algorithms based on the new scheme. Several dynamic programming-based candidate pruning methods are also developed to further speed up the performance. We have conducted a comprehensive experimental study involving nine state-of-the-art algorithms. The experiment results clearly demonstrate the efficiency of our methods.

#index 1581933
#* Managing scientific data: lessons, challenges, and opportunities
#@ Anastasia Ailamaki
#t 2011
#c 5
#! Today's scientific processes heavily depend on fast and accurate analysis of experimental data. Scientists are routinely overwhelmed by the effort needed to manage the volumes of data produced either by observing phenomena or by sophisticated simulations. As database systems have proven inefficient, inadequate, or insufficient to meet the needs of scientific applications, the scientific community typically uses special-purpose legacy software. When compared to a general-purpose DBMS, however, application-specific systems require more resources to maintain, and in order to achieve acceptable performance they often sacrifice data independence and hinder the reuse of knowledge. Nowadays, scientific datasets are growing at unprecedented rates, a result of increasing complexity of the simulated models and ever-improving instrument precision; consequently, scientists' queries become more sophisticated as they try to interpret the data correctly. Datasets and scientific query complexity are likely to continue to grow indefinitely, rendering legacy systems increasingly inadequate. To respond to the challenge, the data management community aspires to solve scientific data management problems by carefully examining the problems of scientific applications and by developing special- or general-purpose scientific data management techniques and systems. This talk discusses the work of teams around the world in an effort to surface the most critical requirements of such an undertaking, and the technological innovations needed to satisfy them.

#index 1581934
#* Internet scale storage
#@ James Hamilton
#t 2011
#c 5
#! The pace of innovation in data center design has been rapidly accelerating over the last five years, driven by the mega-service operators. I believe we have seen more infrastructure innovation in the last five years than we did in the previous fifteen. Most very large service operators have teams of experts focused on server design, data center power distribution and redundancy, mechanical designs, real estate acquisition, and network hardware and protocols. At low scale, with only a data or center or two, it would be crazy to have all these full time engineers and specialist focused on infrastructural improvements and expansion. But, at high scale with tens of data centers, it would be crazy not to invest deeply in advancing the state of the art. Looking specifically at cloud services, the cost of the infrastructure is the difference between an unsuccessful cloud service and a profitable, self-sustaining business. With continued innovation driving down infrastructure costs, investment capital is available, services can be added and improved, and value can be passed on to customers through price reductions. Amazon Web Services, for example, has had eleven price reductions in four years. I don't recall that happening in my first twenty years working on enterprise software. It really is an exciting time in our industry. I started working on database systems twenty years ago during a period of incredibly rapid change. We improved DB2 performance measured using TPC-A by a factor of ten in a single release. The next release, we made a further four-fold improvement. It's rare to be able to improve a product by forty fold in three years but, admittedly, one of the secrets is to begin from a position where work is truly needed. Back then, the database industry was in its infancy. Customers loved the products and were using them heavily, but we were not anywhere close to delivering on the full promise of the technology. That's exactly where cloud computing is today--just where the database world was twenty years ago. Customers are getting great value from cloud computing but, at the same time, we have much more to do and many of the most interesting problems are yet to be solved. I could easily imagine tenfold improvement across several dimensions in over the next five years. What ties these two problems from different decades together is that some of the biggest problems in cloud computing are problems in persistent state management. What's different is that we now have to tackle these problems in a multi-tenant, high-scale, multi-datacenter environment. It's a new vista for database and storage problems. In this talk, we'll analyze an internet-scale data center looking at the cost of power distribution, servers, storage, networking, and cooling on the belief that understanding what drives cost helps us focus on the most valuable research directions. We'll look at some of the fundamental technology limits approached in cloud database and storage solutions on the belief that, at scale, these limits will constrain practical solutions. And we'll consider existing cloud services since they form the foundation on which future solutions might be built.

#index 1581935
#* LCI: a social channel analysis platform for live customer intelligence
#@ Malu Castellanos;Umeshwar Dayal;Meichun Hsu;Riddhiman Ghosh;Mohamed Dekhil;Yue Lu;Lei Zhang;Mark Schreiman
#t 2011
#c 5
#% 309208
#% 854646
#% 1035591
#% 1190068
#% 1214734
#% 1305481
#% 1560389
#! The rise of Web 2.0 with its increasingly popular social sites like Twitter, Facebook, blogs and review sites has motivated people to express their opinions publicly and more frequently than ever before. This has fueled the emerging field known as sentiment analysis whose goal is to translate the vagaries of human emotion into hard data. LCI is a social channel analysis platform that taps into what is being said to understand the sentiment with the particular ability of doing so in near real-time. LCI integrates novel algorithms for sentiment analysis and a configurable dashboard with different kinds of charts including dynamic ones that change as new data is ingested. LCI has been researched and prototyped at HP Labs in close interaction with the Business Intelligence Solutions (BIS) Division and a few customers. This paper presents an overview of the architecture and some of its key components and algorithms, focusing in particular on how LCI deals with Twitter and illustrating its capabilities with selected use cases.

#index 1581936
#* Bistro data feed management system
#@ Vladislav Shkapenyuk;Theodore Johnson;Divesh Srivastava
#t 2011
#c 5
#% 286536
#% 288821
#% 307470
#% 511917
#% 578391
#% 632893
#% 661478
#% 809123
#% 824742
#% 1063582
#% 1180874
#% 1206944
#% 1217211
#% 1373703
#% 1426597
#! Data feed management is a critical component of many data intensive applications that depend on reliable data delivery to support real-time data collection, correlation and analysis. Data is typically collected from a wide variety of sources and organizations, using a range of mechanisms - some data are streamed in real time, while other data are obtained at regular intervals or collected in an ad hoc fashion. Individual applications are forced to make separate arrangements with feed providers, learn the structure of incoming files, monitor data quality, and trigger any processing necessary. The Bistro data feed manager, designed and implemented at AT&T Labs- Research, simplifies and automates this complex task of data feed management: efficiently handling incoming raw files, identifying data feeds and distributing them to remote subscribers. Bistro supports a flexible specification language to define logical data feeds using the naming structure of physical data files, and to identify feed subscribers. Based on the specification, Bistro matches data files to feeds, performs file normalization and compression, efficiently delivers files, and notifies subscribers using a trigger mechanism. We describe our feed analyzer that discovers the naming structure of incoming data files to detect new feeds, dropped feeds, feed changes, or lost data in an existing feed. Bistro is currently deployed within AT&T Labs and is responsible for the real-time delivery of over 100 different raw feeds, distributing data to several large-scale stream warehouses.

#index 1581937
#* Apache hadoop goes realtime at Facebook
#@ Dhruba Borthakur;Jonathan Gray;Joydeep Sen Sarma;Kannan Muthukkaruppan;Nicolas Spiegelberg;Hairong Kuang;Karthik Ranganathan;Dmytro Molkov;Aravind Menon;Samuel Rash;Rodrigo Schmidt;Amitanand Aiyer
#t 2011
#c 5
#! Facebook recently deployed Facebook Messages, its first ever user-facing application built on the Apache Hadoop platform. Apache HBase is a database-like layer built on Hadoop designed to support billions of messages per day. This paper describes the reasons why Facebook chose Hadoop and HBase over other systems such as Apache Cassandra and Voldemort and discusses the application's requirements for consistency, availability, partition tolerance, data model and scalability. We explore the enhancements made to Hadoop to make it a more effective realtime system, the tradeoffs we made while configuring the system, and how this solution has significant advantages over the sharded MySQL database scheme used in other applications at Facebook and many other web-scale companies. We discuss the motivations behind our design choices, the challenges that we face in day-to-day operations, and future capabilities and improvements still under development. We offer these observations on the deployment as a model for other companies who are contemplating a Hadoop-based solution over traditional sharded RDBMS deployments.

#index 1581938
#* Nova: continuous Pig/Hadoop workflows
#@ Christopher Olston;Greg Chiou;Laukik Chitnis;Francis Liu;Yiping Han;Mattias Larsson;Andreas Neumann;Vellanki B.N. Rao;Vijayanand Sankarasubramanian;Siddharth Seth;Chao Tian;Topher ZiCornell;Xiaodan Wang
#t 2011
#c 5
#% 36117
#% 255137
#% 963669
#% 1054227
#% 1063553
#% 1231247
#% 1291844
#% 1328060
#% 1426479
#% 1426480
#% 1468233
#% 1468411
#% 1468530
#% 1526990
#% 1728161
#! This paper describes a workflow manager developed and deployed at Yahoo called Nova, which pushes continually-arriving data through graphs of Pig programs executing on Hadoop clusters. (Pig is a structured dataflow language and runtime for the Hadoop map-reduce system.) Nova is like data stream managers in its support for stateful incremental processing, but unlike them in that it deals with data in large batches using disk-based processing. Batched incremental processing is a good fit for a large fraction of Yahoo's data processing use-cases, which deal with continually-arriving data and benefit from incremental algorithms, but do not require ultra-low-latency processing.

#index 1581939
#* A Hadoop based distributed loading approach to parallel data warehouses
#@ Yu Xu;Pekka Kostamaa;Yan Qi;Jian Wen;Kevin Keliang Zhao
#t 2011
#c 5
#% 70370
#% 122671
#% 1328060
#% 1426583
#% 1426600
#% 1523841
#% 1523924
#! One critical part of building and running a data warehouse is the ETL (Extraction Transformation Loading) process. In fact, the growing ETL tool market is already a multi-billion-dollar market. Getting data into data warehouses has been a hindering factor to wider potential database applications such as scientific computing, as discussed in recent panels at various database conferences. One particular problem with the current load approaches to data warehouses is that while data are partitioned and replicated across all nodes in data warehouses powered by parallel DBMS(PDBMS), load utilities typically reside on a single node which face the issues of i) data loss/data availability if the node/hard drives crash; ii) file size limit on a single node; iii) load performance. All of these issues are mostly handled manually or only helped to some degree by tools. We notice that one common thing between Hadoop and Teradata Enterprise Data Warehouse (EDW) is that data in both systems are partitioned across multiple nodes for parallel computing, which creates parallel loading opportunities not possible for DBMSs running on a single node. In this paper we describe our approach of using Hadoop as a distributed load strategy to Teradata EDW. We use Hadoop as the intermediate load server to store data to be loaded to Teradata EDW. We gain all the benefits from HDFS (Hadoop Distributed File System): i) significantly increased disk space for the file to be loaded; ii) once the data is written to HDFS, it is not necessary for the data sources to keep the data even before the file is loaded to Teradata EDW; iii) MapReduce programs can be used to transform and add structures to unstructured or semi-structured data; iv) more importantly since a file is distributed in HDFS, the file can be loaded more quickly in parallel to Teradata EDW, which is the main focus in this paper. When both Hadoop and Teradata EDW coexist on the same hardware platform, as being increasingly required by customers because of reduced hardware and system administration costs, we have another optimization opportunity to directly load HDFS data blocks to Teradata parallel units on the same nodes. However, due to the inherent non-uniform data distribution in HDFS, rarely we can avoid transferring HDFS blocks to remote Teradata nodes. We designed a polynomial time optimal algorithm and a polynomial time approximate algorithm to assign HDFS blocks to Teradata parallel units evenly and minimize network traffic. We performed experiments on synthetic and real data sets to compare the performances of the algorithms.

#index 1581940
#* A batch of PNUTS: experiences connecting cloud batch and serving systems
#@ Adam E. Silberstein;Russell Sears;Wenchao Zhou;Brian Frank Cooper
#t 2011
#c 5
#% 963669
#% 978404
#% 983467
#% 998845
#% 1127560
#% 1328186
#% 1426489
#! Cloud data management systems are growing in prominence, particularly at large Internet companies like Google, Yahoo!, and Amazon, which prize them for their scalability and elasticity. Each of these systems trades off between low-latency serving performance and batch processing throughput. In this paper, we discuss our experience running batch-oriented Hadoop on top of Yahoo's serving-oriented PNUTS system instead of the standard HDFS file system. Though PNUTS is optimized for and primarily used for serving, a number of applications at Yahoo! must run batch-oriented jobs that read or write data that is stored in PNUTS. Combining these systems reveals several key areas where the fundamental properties of each system are mismatched. We discuss our approaches to accommodating these mismatches, by either bending the batch and serving abstractions, or inventing new ones. Batch systems like Hadoop provide coarse task-level recovery, while serving systems like PNUTS provide finer record or transaction-level recovery. We combine both types to log record-level errors, while detecting and recovering from large-scale errors. Batch systems optimize for read and write throughput of large requests, while serving systems use indexing to provide low latency access to individual records. To improve latency-insensitive write throughput to PNUTS, we introduce a batch write path. The systems provide conflicting consistency models, and we discuss techniques to isolate them from one another.

#index 1581941
#* Turbocharging DBMS buffer pool using SSDs
#@ Jaeyoung Do;Donghui Zhang;Jignesh M. Patel;David J. DeWitt;Jeffrey F. Naughton;Alan Halverson
#t 2011
#c 5
#% 43171
#% 152905
#% 152943
#% 960238
#% 983476
#% 985755
#% 1052068
#% 1063551
#% 1092670
#% 1127391
#% 1183354
#% 1217151
#% 1222045
#% 1328052
#% 1328139
#% 1450604
#% 1523901
#% 1523922
#! Flash solid-state drives (SSDs) are changing the I/O landscape, which has largely been dominated by traditional hard disk drives (HDDs) for the last 50 years. In this paper we propose and systematically explore designs for using an SSD to improve the performance of a DBMS buffer manager. We propose three alternatives that differ mainly in the way that they deal with the dirty pages evicted from the buffer pool. We implemented these alternatives, as well another recently proposed algorithm for this task (TAC), in SQL Server, and ran experiments using a variety of benchmarks (TPC-C, E and H) at multiple scale factors. Our empirical evaluation shows significant performance improvements of our methods over the default HDD configuration (up to 9.4X), and up to a 6.8X speedup over TAC.

#index 1581942
#* Online reorganization in read optimized MMDBS
#@ Felix Beier;Knut Stolze;Kai-Uwe Sattler
#t 2011
#c 5
#% 13019
#% 43171
#% 210175
#% 287656
#% 317949
#% 333953
#% 480087
#% 480821
#% 772834
#% 824697
#% 1016166
#% 1127400
#% 1202159
#% 1206624
#! Query performance is a critical factor in modern business intelligence and data warehouse systems. An increasing number of companies uses detailed analyses for conducting daily business and supporting management decisions. Thus, several techniques have been developed for achieving near realtime response times - techniques which try to alleviate I/O bottlenecks while increasing the throughputs of available processing units, i.e. by keeping relevant data in compressed main-memory data structures and exploiting the read-only characteristics of analytical workloads. However, update processing and skews in data distribution result in degenerations in these densely packed and highly compressed data structures affecting the memory efficiency and query performance negatively. Reorganization tasks can repair these data structures, but -- since these are usually costly operations -- require a well-considered decision which of several possible strategies should be processed and when, in order to reduce system downtimes. In this paper, we address these problems by presenting an approach for online reorganization in main-memory database systems (MMDBS). Based on a discussion of necessary reorganization strategies in IBM Smart Analytics Optimizer, a read optimized parallel MMDBS, we introduce a framework for executing arbitrary reorganization tasks online, i.e. in the background of normal user workloads without disrupting query results or performance.

#index 1581943
#* Automated partitioning design in parallel database systems
#@ Rimma Nehme;Nicolas Bruno
#t 2011
#c 5
#% 115661
#% 210175
#% 213976
#% 248815
#% 288882
#% 397397
#% 411554
#% 480158
#% 480298
#% 480442
#% 482100
#% 565457
#% 708321
#% 765431
#% 960276
#% 960326
#% 983467
#% 1016220
#% 1022293
#% 1063540
#% 1127559
#% 1278123
#% 1328186
#% 1523799
#! In recent years, Massively Parallel Processors (MPPs) have gained ground enabling vast amounts of data processing. In such environments, data is partitioned across multiple compute nodes, which results in dramatic performance improvements during parallel query execution. To evaluate certain relational operators in a query correctly, data sometimes needs to be re-partitioned (i.e., moved) across compute nodes. Since data movement operations are much more expensive than relational operations, it is crucial to design a suitable data partitioning strategy that minimizes the cost of such expensive data transfers. A good partitioning strategy strongly depends on how the parallel system would be used. In this paper we present a partitioning advisor that recommends the best partitioning design for an expected workload. Our tool recommends which tables should be replicated (i.e., copied into every compute node) and which ones should be distributed according to specific column(s) so that the cost of evaluating similar workloads is minimized. In contrast to previous work, our techniques are deeply integrated with the underlying parallel query optimizer, which results in more accurate recommendations in a shorter amount of time. Our experimental evaluation using a real MPP system, Microsoft SQL Server 2008 Parallel Data Warehouse, with both real and synthetic workloads shows the effectiveness of the proposed techniques and the importance of deep integration of the partitioning advisor with the underlying query optimizer.

#index 1581944
#* Oracle database filesystem
#@ Krishna Kunchithapadam;Wei Zhang;Amit Ganesh;Niloy Mukherjee
#t 2011
#c 5
#% 480831
#% 481919
#% 1127562
#% 1328068
#! Modern enterprise, web, and multimedia applications are generating unstructured content at unforeseen volumes in the form of documents, texts, and media files. Such content is generally associated with relational data such as user names, location tags, and timestamps. Storage of unstructured content in a relational database would guarantee the same robustness, transactional consistency, data integrity, data recoverability and other data management features consolidated across files and relational contents. Although database systems are preferred for relational data management, poor performance of unstructured data storage, limited data transformation functionalities, and lack of interfaces based on filesystem standards may keep more than eighty five percent of non-relational unstructured content out of databases in the coming decades. We introduce Oracle Database Filesystem (DBFS) as a consolidated solution that unifies state-of-the-art network filesystem features with relational database management ones. DBFS is a novel shared-storage network filesystem developed in the RDBMS kernel that allows content management applications to transparently store and organize files using standard filesystem interfaces, in the same database that stores associated relational content. The server component of DBFS is based on Oracle SecureFiles, a novel unstructured data storage engine within the RDBMS that provides filesystem like or better storage performance for files within the database while fully leveraging relational data management features such as transaction atomicity, isolation, read consistency, temporality, and information lifecycle management. We present a preliminary performance evaluation of DBFS that demonstrates more than 10TB/hr throughput of filesystem read and write operations consistently over a period of 12 hours on an Oracle Exadata Database cluster of four server nodes. In terms of file storage, such extreme performance is equivalent to ingestion of more than 2500 million 100KB document files a single day. The set of initial results look very promising for DBFS towards becoming the universal storage solution for both relational and unstructured content.

#index 1581945
#* Emerging trends in the enterprise data analytics: connecting Hadoop and DB2 warehouse
#@ Fatma Özcan;David Hoa;Kevin S. Beyer;Andrey Balmin;Chuan Jie Liu;Yu Li
#t 2011
#c 5
#% 963669
#% 1217159
#% 1278123
#% 1426583
#! Enterprises are dealing with ever increasing volumes of data, reaching into the petabyte scale. With many of our customer engagements, we are observing an emerging trend: They are using Hadoop-based solutions in conjunction with their data warehouses. They are using Hadoop to deal with the data volume, as well as the lack of strict structure in their data to conduct various analyses, including but not limited to Web log analysis, sophisticated data mining, machine learning and model building. This first stage of the analysis is off-line and suitable for Hadoop. But, once their data is summarized or cleansed enough, and their models are built, they are loading the results into a warehouse for interactive querying and report generation. At this later stage, they leverage the wealth of business intelligence tools, which they are accustomed to, that exist for warehouses. In this paper, we outline this use case and discuss the bidirectional connectors we developed between IBM DB2 and IBM InfoSphere BigInsights.

#index 1581946
#* Efficient processing of data warehousing queries in a split execution environment
#@ Kamil Bajda-Pawlikowski;Daniel J. Abadi;Avi Silberschatz;Erik Paulson
#t 2011
#c 5
#% 115661
#% 463837
#% 960326
#% 963669
#% 1063554
#% 1217159
#% 1217169
#% 1328059
#% 1328061
#% 1328066
#% 1328186
#% 1426513
#% 1426543
#% 1426584
#% 1426600
#% 1523924
#! Hadapt is a start-up company currently commercializing the Yale University research project called HadoopDB. The company focuses on building a platform for Big Data analytics in the cloud by introducing a storage layer optimized for structured data and by providing a framework for executing SQL queries efficiently. This work considers processing data warehousing queries over very large datasets. Our goal is to maximize perfor mance while, at the same time, not giving up fault tolerance and scalability. We analyze the complexity of this problem in the split execution environment of HadoopDB. Here, incoming queries are examined; parts of the query are pushed down and executed inside the higher performing database layer; and the rest of the query is processed in a more generic MapReduce framework. In this paper, we discuss in detail performance-oriented query execution strategies for data warehouse queries in split execution environments, with particular focus on join and aggregation operations. The efficiency of our techniques is demonstrated by running experiments using the TPC-H benchmark with 3TB of data. In these experiments we compare our results with a standard commercial parallel database and an open-source MapReduce implementation featuring a SQL interface (Hive). We show that HadoopDB successfully competes with other systems.

#index 1581947
#* SQL server column store indexes
#@ Per-Åke Larson;Cipri Clinciu;Eric N. Hanson;Artem Oks;Susan L. Price;Srikumar Rangarajan;Aleksandras Surna;Qingqing Zhou
#t 2011
#c 5
#% 286258
#% 287349
#% 465169
#% 824697
#% 875026
#% 893129
#% 990389
#% 1063542
#! The SQL Server 11 release (code named "Denali") introduces a new data warehouse query acceleration feature based on a new index type called a column store index. The new index type combined with new query operators processing batches of rows greatly improves data warehouse query performance: in some cases by hundreds of times and routinely a tenfold speedup for a broad range of decision support queries. Column store indexes are fully integrated with the rest of the system, including query processing and optimization. This paper gives an overview of the design and implementation of column store indexes including enhancements to query processing and query optimization to take full advantage of the new indexes. The resulting performance improvements are illustrated by a number of example queries.

#index 1581948
#* An analytic data engine for visualization in tableau
#@ Richard Wesley;Matthew Eldridge;Pawel T. Terlecki
#t 2011
#c 5
#% 172930
#% 765468
#% 824697
#% 873341
#% 1022298
#% 1063542
#% 1082202
#% 1217210
#! Efficient data processing is critical for interactive visualization of analytic data sets. Inspired by the large amount of recent research on column-oriented stores, we have developed a new specialized analytic data engine tightly-coupled with the Tableau data visualization system. The Tableau Data Engine ships as an integral part of Tableau 6.0 and is intended for the desktop and server environments. This paper covers the main requirements of our project, system architecture and query-processing pipeline. We use real-life visualization scenarios to illustrate basic concepts and provide experimental evaluation.

#index 1581949
#* Learning statistical models from relational data
#@ Lise Getoor;Lilyana Mihalkova
#t 2011
#c 5
#% 89958
#% 266230
#% 396021
#% 676365
#% 722914
#% 850430
#% 1000452
#% 1000502
#% 1108071
#% 1127415
#% 1250334
#% 1270256
#% 1270261
#% 1273915
#% 1279353
#% 1289560
#% 1372657
#% 1417084
#% 1417109
#% 1467732
#% 1718473
#! Statistical Relational Learning (SRL) is a subarea of machine learning which combines elements from statistical and probabilistic modeling with languages which support structured data representations. In this survey, we will: 1) provide an introduction to SRL, 2) describe some of the distinguishing characteristics of SRL systems, including relational feature construction and collective classification, 3) describe three SRL systems in detail, 4) discuss applications of SRL techniques to important data management problems such as entity resolution, selectivity estimation, and information integration, and 5) discuss connections between SRL methods and existing database research such as probabilistic databases.

#index 1581950
#* Web data management
#@ Michael J. Cafarella;Alon Y. Halevy
#t 2011
#c 5
#% 955762
#% 956564
#% 1013546
#% 1019061
#% 1063570
#% 1127393
#% 1275182
#% 1288161
#% 1291356
#% 1426493
#% 1523913
#! Web Data Management (or WDM) refers to a body of work concerned with leveraging the large collections of structured data that can be extracted from the Web. Over the past few years, several research and commercial efforts have explored these collections of data with the goal of improving Web search and developing mechanisms for surfacing different kinds of search answers. This work has leveraged (1) collections of structured data such as HTML tables, lists and forms, (2) recent ontologies and knowledge bases created by crowd-sourcing, such as Wikipedia and its derivatives, DBPedia, YAGO and Freebase, and (3) the collection of text documents from the Web, from which facts could be extracted in a domain-independent fashion. The promise of this line of work is based on the observation that new kinds of results can be obtained by leveraging a huge collection of independently created fragments of data, and typically in ways that are wholly unrelated to the authors' original intent. For example, we might use many database schemas to compute a schema thesaurus. Or we might examine many spreadsheets of scientific data that reveal the aggregate practice of an entire scientific field. As such, WDM is tightly linked to Web-enabled collaboration, even (or especially) if the collaborators are unwitting ones. We will cover the key techniques, principles and insights obtained so far in the area of Web Data Management.

#index 1581951
#* Privacy-aware data management in information networks
#@ Michael Hay;Kun Liu;Gerome Miklau;Jian Pei;Evimaria Terzi
#t 2011
#c 5
#% 956511
#% 963241
#% 1063476
#% 1127417
#% 1190107
#% 1190108
#% 1190207
#% 1195950
#% 1200862
#% 1206763
#% 1217125
#% 1259854
#% 1318624
#% 1318653
#% 1328188
#% 1366214
#% 1399968
#% 1400043
#% 1415851
#% 1426540
#% 1475160
#% 1478165
#% 1523886
#% 1524388
#% 1535288
#% 1581409
#% 1581862
#% 1740518
#! The proliferation of information networks, as a means of sharing information, has raised privacy concerns for enterprises who manage such networks and for individual users that participate in such networks. For enterprises, the main challenge is to satisfy two competing goals: releasing network data for useful data analysis and also preserving the identities or sensitive relationships of the individuals participating in the network. Individual users, on the other hand, require personalized methods that increase their awareness of the visibility of their private information. This tutorial provides a systematic survey of the problems and state-of-the-art methods related to both enterprise and personalized privacy in information networks. The tutorial discusses privacy threats, privacy attacks, and privacy-preserving mechanisms tailored specifically to network data.

#index 1581952
#* Large-scale copy detection
#@ Xin Luna Dong;Divesh Srivastava
#t 2011
#c 5
#% 201935
#% 340294
#% 654447
#% 745490
#% 800180
#% 879600
#% 961367
#% 990305
#% 1014293
#% 1074122
#% 1137895
#% 1166531
#% 1190061
#% 1193647
#% 1328155
#% 1328156
#% 1405655
#% 1491640
#% 1496347
#% 1523915
#% 1547484
#% 1688291
#% 1815177
#% 1858257
#! The Web has enabled the availability of a vast amount of useful information in recent years. However, the web technologies that have enabled sources to share their information have also made it easy for sources to copy from each other and often publish without proper attribution. Understanding the copying relationships between sources has many benefits, including helping data providers protect their own rights, improving various aspects of data integration, and facilitating in-depth analysis of information flow. The importance of copy detection has led to a substantial amount of research in many disciplines of Computer Science, based on the type of information considered, such as text, images, videos, software code, and structured data. This tutorial explores the similarities and differences between the techniques proposed for copy detection across the different types of information. We also examine the computational challenges associated with large-scale copy detection, indicating how they could be detected efficiently, and identify a range of open problems for the community.

#index 1581953
#* Data management over flash memory
#@ Ioannis Koltsidas;Stratis D. Viglas
#t 2011
#c 5
#% 451767
#% 729852
#% 829901
#% 902938
#% 951778
#% 957221
#% 960238
#% 963436
#% 985754
#% 985755
#% 1053457
#% 1053488
#% 1063551
#% 1085291
#% 1121345
#% 1127391
#% 1127428
#% 1129952
#% 1183354
#% 1196583
#% 1207002
#% 1217151
#% 1217152
#% 1217213
#% 1222045
#% 1222046
#% 1222047
#% 1328052
#% 1412877
#% 1450604
#% 1468198
#% 1480460
#% 1511675
#% 1523920
#% 1523922
#% 1581846
#% 1670337
#% 1820346
#! Flash SSDs are quickly becoming mainstream and emerge as alternatives to magnetic disks. It is therefore imperative to incorporate them seamlessly into the enterprise. We present the salient results of research in the area, touching all aspects of the data management stack: from the fundamentals of flash technology, through storage for database systems and the manipulation of SSD-resident data, to query processing.

#index 1581954
#* Datalog and emerging applications: an interactive tutorial
#@ Shan Shan Huang;Todd Jeffrey Green;Boon Thau Loo
#t 2011
#c 5
#% 583
#% 13014
#% 181027
#% 271908
#% 368248
#% 378409
#% 384978
#% 435130
#% 442661
#% 465053
#% 553959
#% 664709
#% 752760
#% 801668
#% 809267
#% 821939
#% 826032
#% 835186
#% 874978
#% 874997
#% 1022258
#% 1022288
#% 1063738
#% 1206880
#% 1270566
#% 1287572
#% 1328185
#% 1386046
#% 1426562
#% 1426612
#% 1456130
#% 1732822
#! We are witnessing an exciting revival of interest in recursive Datalog queries in a variety of emerging application domains such as data integration, information extraction, networking, program analysis, security, and cloud computing. This tutorial briefly reviews the Datalog language and recursive query processing and optimization techniques, then discusses applications of Datalog in three application domains: data integration, declarative networking, and program analysis. Throughout the tutorial, we use LogicBlox, a commercial Datalog engine for enterprise software systems, to allow the audience to walk through code examples presented in the tutorial.

#index 1581955
#* One-pass data mining algorithms in a DBMS with UDFs
#@ Carlos Ordonez;Sasi K. Pitchaimalai
#t 2011
#c 5
#% 960324
#% 1376243
#% 1512993
#% 1535426
#! Data mining research is extensive, but most work has proposed efficient algorithms, data structures and optimizations that work outside a DBMS, mostly on flat files. In contrast, we present a data mining system that can work on top of a relational DBMS based on a combination of SQL queries and User-Defined Functions (UDFs), debuking the common perception that SQL is inefficient or inadequate for data mining. We show our system can analyze large data sets significantly faster than external data mining tools. Moreover, our UDF-based algorithms can process a data set in one pass and have linear scalability.

#index 1581956
#* Inspector gadget: a framework for custom monitoring and debugging of distributed dataflows
#@ Christopher Olston;Benjamin Reed
#t 2011
#c 5
#% 481102
#% 726621
#% 1231247
#% 1328060
#! We demonstrate a novel dataflow introspection framework called Inspector Gadget, which makes it easy to create custom monitoring and debugging add-ons to an existing dataflow engine such as Pig. The framework is motivated by a series of informal user interviews, which revealed that dataflow monitoring and debugging needs are both pressing and diverse. Of the 14 monitoring/debugging behaviors requested by users, we were able to implement 12 in Inspector Gadget, in just a few hundred lines of (Java) code each.

#index 1581957
#* RAFT at work: speeding-up mapreduce applications under task and node failures
#@ Jorge-Arnulfo Quiané-Ruiz;Christoph Pinkel;Jörg Schad;Jens Dittrich
#t 2011
#c 5
#% 399766
#% 462497
#% 864486
#% 869394
#% 963669
#% 1026962
#% 1278391
#% 1354118
#% 1386049
#% 1523836
#% 1523841
#% 1594684
#! The MapReduce framework is typically deployed on very large computing clusters where task and node failures are no longer an exception but the rule. Thus, fault-tolerance is an important aspect for the efficient operation of MapReduce jobs. However, currently MapReduce implementations fully recompute failed tasks (subparts of a job) from the beginning. This can significantly decrease the runtime performance of MapReduce applications. We present an alternative system that implements RAFT ideas. RAFT is a family of powerful and inexpensive Recovery Algorithms for Fast-Tracking MapReduce jobs under task and node failures. To recover from task failures, RAFT exploits the intermediate results persisted by MapReduce at several points in time. RAFT piggybacks checkpoints on the task progress computation. To recover from node failures, RAFT maintains a per-map task list of all input key-value pairs producing intermediate results and pushes intermediate results to reducers. In this demo, we demonstrate that RAFT recovers efficiently from both task and node failures. Further, the audience can compare RAFT with Hadoop via an easy-to-use web interface.

#index 1581958
#* WattDB: an energy-proportional cluster of wimpy nodes
#@ Daniel Schall;Volker Hudlet
#t 2011
#c 5
#% 1034471
#% 1278373
#% 1373701
#% 1426521
#! The constant growth of data in all businesses leads to bigger database servers. While peak load times require fast and heavyweight hardware to guarantee performance, idle times are a waste of energy and money. Todays DBMSs have the ability to cluster several servers for performance and fault tolerance. Nevertheless, they do not support dynamic powering of the cluster's nodes based on the current workload. In this demo, we propose a newly developed DBMS running on clustered commodity hardware, which is able to dynamically power nodes. The demo allows the user to interact with the DBMS and adjust workloads, while the cluster's reaction is shown in real-time.

#index 1581959
#* BRRL: a recovery library for main-memory applications in the cloud
#@ Tuan Cao;Benjamin Sowell;Marcos Vaz Salles;Alan Demers;Johannes Gehrke
#t 2011
#c 5
#% 581183
#% 594217
#% 778271
#% 978510
#% 1022298
#% 1350336
#% 1523880
#% 1581868
#! In this demonstration we present BRRL, a library for making distributed main-memory applications fault tolerant. BRRL is optimized for cloud applications with frequent points of consistency that use data-parallelism to avoid complex concurrency control mechanisms. BRRL differs from existing recovery libraries by providing a simple table abstraction and using schema information to optimize checkpointing. We will demonstrate the utility of BRRL using a distributed transaction processing system and a platform for scientific behavioral simulations.

#index 1581960
#* A data-oriented transaction execution engine and supporting tools
#@ Ippokratis Pandis;Pinar Tözün;Miguel Branco;Dimitris Karampinas;Danica Porobic;Ryan Johnson;Anastasia Ailamaki
#t 2011
#c 5
#% 864548
#% 1022298
#% 1181215
#% 1523799
#% 1523878
#! Conventional OLTP systems assign each transaction to a worker thread and that thread accesses data, depending on what the transaction dictates. This thread-to-transaction work assignment policy leads to unpredictable accesses. The unpredictability forces each thread to enter a large number of critical sections for the completion of even the simplest of the transactions; leading to poor performance and scalability on modern manycore hardware. This demonstration highlights the chaotic access patterns of conventional OLTP designs which are the source of scalability problems. Then, it presents a working prototype of a transaction processing engine that follows a non-conventional architecture, called data-oriented or DORA. DORA is designed around the thread-to-data work assignment policy. It distributes the transaction execution to multiple threads and offers predictable accesses. By design, DORA can decentralize the lock management service, and thereby eliminate the critical sections executed inside the lock manager. We explain the design of the system and show that it more efficiently utilizes the abundant processing power of modern hardware, always contrasting it against the conventional execution. In addition, we present different components of the system, such as a dynamic load balancer. Finally, we present a set of tools that enable the development of applications that use DORA.

#index 1581961
#* iGraph in action: performance analysis of disk-based graph indexing techniques
#@ Wook-Shin Han;Minh-Duc Pham;Jinsoo Lee;Romans Kasperovics;Jeffrey Xu Yu
#t 2011
#c 5
#% 765429
#% 864425
#% 960305
#% 1022280
#% 1044450
#% 1127380
#% 1523835
#! Graphs provide a powerful way to model complex structures such as chemical compounds, proteins, images, and program dependence. The previous practice for experiments in graph indexing techniques is that the author of a newly proposed technique does not implement existing indexes on his own code base, but instead uses the original authors' binary executables and reports only the wall clock time. However, we observed that this practice may result in several problems [6]. In order to address these problems, we have implemented all representative graph indexing techniques on a common framework called iGraph [6]. In this demonstration we showcase iGraph and its visual tools using several real datasets and their workloads. For selected queries of the workloads, we show several unique features including visual performance analysis.

#index 1581962
#* StreamRec: a real-time recommender system
#@ Badrish Chandramouli;Justin J. Levandoski;Ahmed Eldawy;Mohamed F. Mokbel
#t 2011
#c 5
#% 378388
#% 452563
#% 801694
#% 813966
#% 1328078
#% 1426639

#index 1581963
#* SkylineSearch: semantic ranking and result visualization for pubmed
#@ Julia Stoyanovich;Mayur Lodha;William Mee;Kenneth A. Ross
#t 2011
#c 5
#! Life sciences researchers perform scientific literature search as part of their daily activities. Many such searches are executed against PubMed, a central repository of life sciences articles, and often return hundreds, or even thousands, of results, pointing to the need for data exploration tools. In this demonstration we present SkylineSearch, a semantic ranking and result visualization system designed specifically for PubMed, and available to the scientific community at skyline.cs.columbia.edu. Our system leverages semantic annotations of articles with terms from the MeSH controlled vocabulary, and presents results as a two-dimensional skyline, plotting relevance against publication date. We demonstrate that SkylineSearch supports a richer data exploration experience than does the search functionality of PubMed, allowing users to find relevant references more easily. We also show that SkylineSearch executes queries and presents results in interactive time.

#index 1581964
#* A cross-service travel engine for trip planning
#@ Gang Chen;Chen Liu;Meiyu Lu;Beng Chin Ooi;Shanshan Ying;Anthony Tung;Dongxiang Zhang;Meihui Zhang
#t 2011
#c 5
#% 281251
#% 480467
#% 1181318
#! The online travel services and resources are far from well organized and integrated. Trip planning is still a laborious job requiring interaction with a combination of services such as travel guides, personal travel blogs, map services and public transportation to piece together an itinerary. To facilitate this process, we have designed a cross-service travel engine for trip planners. Our system seamlessly and semantically integrates various types of travel services and resources based on a geographical ontology. We also built a user-friendly visualization tool for travellers to conveniently browse and design personal itineraries on Google Maps.

#index 1581965
#* WINACS: construction and analysis of web-based computer science information networks
#@ Tim Weninger;Marina Danilevsky;Fabio Fumarola;Joshua Hailpern;Jiawei Han;Thomas J. Johnston;Surya Kallumadi;Hyungsul Kim;Zhijin Li;David McCloskey;Yizhou Sun;Nathan E. TeGrotenhuis;Chi Wang;Xiao Yu
#t 2011
#c 5
#% 1176884
#% 1214701
#% 1328118
#% 1426635
#% 1451159
#% 1482398
#% 1560218
#% 1561591
#% 1610177
#! WINACS (Web-based Information Network Analysis for Computer Science) is a project that incorporates many recent, exciting developments in data sciences to construct a Web-based computer science information network and to discover, retrieve, rank, cluster, and analyze such an information network. With the rapid development of the Web, huge amounts of information are available in the form of Web documents, structures, and links. It has been a dream of the database and Web communities to harvest such information and reconcile the unstructured nature of the Web with the neat, semi-structured schemas of the database paradigm. Taking computer science as a dedicated domain, WINACS first discovers related Web entity structures, and then constructs a heterogeneous computer science information network in order to rank, cluster and analyze this network and support intelligent and analytical queries.

#index 1581966
#* Tweets as data: demonstration of TweeQL and Twitinfo
#@ Adam Marcus;Michael S. Bernstein;Osama Badar;David R. Karger;Samuel Madden;Robert C. Miller
#t 2011
#c 5
#% 300167
#% 300169
#% 334053
#% 397372
#% 1384210
#% 1384224
#% 1573368
#! Microblogs such as Twitter are a tremendous repository of user-generated content. Increasingly, we see tweets used as data sources for novel applications such as disaster mapping, brand sentiment analysis, and real-time visualizations. In each scenario, the workflow for processing tweets is ad-hoc, and a lot of unnecessary work goes into repeating common data processing patterns. We introduce TweeQL, a stream query processing language that presents a SQL-like query interface for unstructured tweets to generate structured data for downstream applications. We have built several tools on top of TweeQL, most notably TwitInfo, an event timeline generation and exploration interface that summarizes events as they are discussed on Twitter. Our demonstration will allow the audience to interact with both TweeQL and TwitInfo to convey the value of data embedded in tweets.

#index 1581967
#* MOBIES: mobile-interface enhancement service for hidden web database
#@ Xin Jin;Aditya Mone;Nan Zhang;Gautam Das
#t 2011
#c 5
#% 875003
#% 875067
#% 1426573
#% 1581892
#! Many web databases are hidden behind form-based interfaces which are not always easy-to-use on mobile devices because of limitations such as small screen sizes, trickier text entry, etc. In this demonstration, we have developed MOBIES, a third-party system that generates mobile-user-friendly interfaces by exploiting data analytics specific to the hidden web databases. Our user studies show the effectiveness of MOBIES on improving user experience over a hidden web database.

#index 1581968
#* Search computing: multi-domain search on ranked data
#@ Alessandro Bozzon;Daniele Braga;Marco Brambilla;Stefano Ceri;Francesco Corcoglioniti;Piero Fraternali;Salvatore Vadacca
#t 2011
#c 5
#% 1127395
#% 1399949
#% 1491897
#% 1523826
#% 1733680
#! We demonstrate the Search Computing framework for multi-domain queries upon ranked data collected from Web sources. Search Computing answers to queries like "Find a good Jazz concert close to a specified location, a good restaurant and a hotel at walking distance" and fills the gap between generic and domain-specific search engines, by proposing new methods, techniques, interfaces, and tools for building search-based applications spanning multiple data services. The main enabling technology is an execution engine supporting methods for rank-join execution upon ranked data sources, abstracted and wrapped by means of a unifying service model. The demo walks through the interface for formulating multi-domain queries and follows the steps of the query engine that builds the result, with the help of run-time monitors that clearly explain the system's behavior. Once results are extracted, the demonstration shows several approaches for visualizing results and exploring the information space.

#index 1581969
#* EnBlogue: emergent topic detection in web 2.0 streams
#@ Foteini Alvanaki;Michel Sebastian;Krithi Ramamritham;Gerhard Weikum
#t 2011
#c 5
#% 967452
#% 1206890
#% 1355045
#% 1355046
#% 1426611
#% 1451249
#% 1482181
#! Emergent topics are newly arising themes in news, blogs, or tweets, often implied by interesting and unexpected correlations of tags or entities. We present the enBlogue system for emergent topic detection. The name enBlogue reflects the analogy with emerging trends in fashion often referred to as en Vogue. EnBlogue continuously monitors Web 2.0 streams and keeps track of sudden changes in tag correlations which can be adjusted using personalization to reflect particular user interests. We demonstrate enBlogue with several real-time monitoring scenarios as well as with time lapse on archived data.

#index 1581970
#* NOAM: news outlets analysis and monitoring system
#@ Ilias Flaounas;Omar Ali;Marco Turchi;Tristan Snowsill;Florent Nicart;Tijl De Bie;Nello Cristianini
#t 2011
#c 5
#% 309208
#% 1267809
#% 1268055
#! We present NOAM, an integrated platform for the monitoring and analysis of news media content. NOAM is the data management system behind various applications and scientific studies aiming at modelling the mediasphere. The system is also intended to address the need in the AI community for platforms where various AI technologies are integrated and deployed in the real world. It combines a relational database (DB) with state of the art AI technologies, including data mining, machine learning and natural language processing. These technologies are organised in a robust, distributed architecture of collaborating modules, that are used to populate and annotate the DB. NOAM manages tens of millions of news items in multiple languages, automatically annotating them in order to enable queries based on their semantic properties. The system also includes a unified user interface for interacting with its various modules.

#index 1581971
#* Pay-as-you-go mapping selection in dataspaces
#@ Cornelia Hedeler;Khalid Belhajjame;Norman W. Paton;Alvaro A.A. Fernandes;Suzanne M. Embury;Lu Mao;Chenjuan Guo
#t 2011
#c 5
#% 874876
#% 960233
#% 1022257
#% 1063534
#% 1127413
#% 1133483
#% 1230781
#% 1328084
#% 1372730
#! The vision of dataspaces proposes an alternative to classical data integration approaches with reduced up-front costs followed by incremental improvement on a pay-as-you-go basis. In this paper, we demonstrate DSToolkit, a system that allows users to provide feedback on results of queries posed over an integration schema. Such feedback is then used to annotate the mappings with their respective precision and recall. The system then allows a user to state the expected levels of precision (or recall) that the query results should exhibit and, in order to produce those results, the system selects those mappings that are predicted to meet the stated constraints.

#index 1581972
#* Exelixis: evolving ontology-based data integration system
#@ Haridimos Kondylakis;Dimitris Plexousakis
#t 2011
#c 5
#% 777935
#% 1071630
#% 1077467
#% 1127411
#% 1333458
#% 1333610
#% 1416180
#! The evolution of ontologies is an undisputed necessity in ontology-based data integration. Yet, few research efforts have focused on addressing the need to reflect ontology evolution onto the underlying data integration systems. We present Exelixis, a web platform that enables query answering over evolving ontologies without mapping redefinition. This is achieved by rewriting queries among ontology versions. First, changes between ontologies are automatically detected and described using a high level language of changes. Those changes are interpreted as sound global-as-view (GAV) mappings. Then query expansion is applied in order to consider constraints from the ontology and unfolding to apply the GAV mappings. Whenever equivalent rewritings cannot be produced we a) guide query redefinition and/or b) provide the best "over-approximations", i.e. the minimally-containing and minimally-generalized rewritings. For the demonstration we will use four versions of the CIDOC-CRM ontology and real user queries to show the functionality of the system. Then we will allow conference participants to directly interact with the system to test its capabilities.

#index 1581973
#* U-MAP: a system for usage-based schema matching and mapping
#@ Hazem Elmeleegy;Jaewoo Lee;El Kindi Rezig;Mourad Ouzzani;Ahmed Elmagarmid
#t 2011
#c 5
#% 572314
#% 660001
#% 1206613
#% 1232194
#% 1581856
#% 1705177
#! This demo shows how usage information buried in query logs can play a central role in data integration and data exchange. More specifically, our system U-Map uses query logs to generate correspondences between the attributes of two different schemas and the complex mapping rules to transform and restructure data records from one of these schemas to another. We introduce several novel features showing the benefit of incorporating query log analysis into these key components of data integration and data exchange systems.

#index 1581974
#* The SystemT IDE: an integrated development environment for information extraction rules
#@ Laura Chiticariu;Vivian Chu;Sajib Dasgupta;Thilo W. Goetz;Howard Ho;Rajasekar Krishnamurthy;Alexander Lang;Yunyao Li;Bin Liu;Sriram Raghavan;Frederick R. Reiss;Shivakumar Vaithyanathan;Huaiyu Zhu
#t 2011
#c 5
#% 1183368
#% 1206687
#% 1231247
#% 1264720
#% 1426638
#% 1471192
#% 1481633
#% 1523847
#! Information Extraction (IE)-the problem of extracting structured information from unstructured text - has become the key enabler for many enterprise applications such as semantic search, business analytics and regulatory compliance. While rule-based IE systems are widely used in practice due to their well-known "explainability," developing high-quality information extraction rules is known to be a labor-intensive and time-consuming iterative process. Our demonstration showcases SystemT IDE, the integrated development environment for SystemT, a state-of-the-art rule-based IE system from IBMResearch that has been successfully embedded in multiple IBM enterprise products. SystemT IDE facilitates the development, test and analysis of high-quality IE rules by means of sophisticated techniques, ranging from data management to machine learning. We show how to build high-quality IE annotators using a suite of tools provided by SystemT IDE, including computing data provenance, learning basic features such as regular expressions and dictionaries, and automatically refining rules based on labeled examples.

#index 1581975
#* ProApproX: a lightweight approximation query processor over probabilistic trees
#@ Pierre Senellart;Asma Souihli
#t 2011
#c 5
#% 977012
#% 1291113
#% 1291114
#% 1291120
#! We demonstrate a system for querying probabilistic XML documents with simple XPath queries. A user chooses between a variety of query answering techniques, both exact and approximate, and observes the running behavior, pros, and cons, of each method, in terms of efficiency, precision of the result, and data model and query language supported.

#index 1581976
#* SPROUT2: a squared query engine for uncertain web data
#@ Robert Fink;Andrew Hogue;Dan Olteanu;Swaroop Rath
#t 2011
#c 5
#% 1206717
#% 1538785
#% 1594634
#! SPROUT² is a query answering system that allows users to ask structured queries over tables embedded in Web pages, over Google Fusion tables, and over uncertain tables that can be extracted from answers to Google Squared. At the core of this service lies SPROUT, a query engine for probabilistic databases. This demonstration allows users to compose and ask ad-hoc queries of their choice and also to take a tour through the system's capabilities along pre-arranged scenarios on, e.g., movie actors and directors, biomass facilities, or leveraging corporate databases.

#index 1581977
#* Fuzzy prophet: parameter exploration in uncertain enterprise scenarios
#@ Oliver A. Kennedy;Steve Lee;Charles Loboz;Slawek Smyl;Suman Nath
#t 2011
#c 5
#% 810098
#% 1063521
#% 1063568
#% 1581915
#! We present Fuzzy Prophet, a probabilistic database tool for constructing, simulating and analyzing business scenarios with uncertain data. Fuzzy Prophet takes externally defined probability distribution (so called VG-Functions) and a declarative description of a target scenario, and performs Monte Carlo simulation to compute probability distribution of the scenario's outcomes. In addition, Fuzzy Prophet supports parameter optimization,where probabilistic models are parameterized and a large parameter space must be explored to find parameters that optimize or achieve a desired goal. Fuzzy Prophet's key innovation is to use 'fingerprints' that can identify parameter values producing correlated outputs of a user-provided stochastic function and to reuse computations across such values. Fingerprints significantly expedite the process of parameter exploration in offline optimization and interactive what-if exploration tasks.

#index 1581978
#* LinkDB: a probabilistic linkage database system
#@ Ekaterini Ioannou;Wolfgang Nejdl;Claudia Niederée;Yannis Velegrakis
#t 2011
#c 5
#% 864417
#% 874876
#% 893189
#% 913783
#% 1103296
#% 1523833
#! Entity linkage deals with the problem of identifying whether two pieces of information represent the same real world object. The traditional methodology computes the similarity among the entities, and then merges those with similarity above some specific threshold. We demonstrate LinkDB, an original entity storage and querying system that deals with the entity linkage problem in a novel way. LinkDB is a probabilistic linkage database that uses existing linkage techniques to generate linkages among entities, but instead of performing the merges based on these linkages, it stores them alongside the data and performs only the required merges at run-time, by effectively taking into consideration the query specifications. We explain the technical challenges behind this kind of query answering, and we show how this new mechanism is able to provide answers that traditional entity linkage mechanisms cannot.

#index 1581979
#* CONFLuEnCE: CONtinuous workFLow ExeCution Engine
#@ Panayiotis Neophytou;Panos K. Chrysanthis;Alexandros Labrinidis
#t 2011
#c 5
#% 391696
#% 451429
#% 664071
#% 832825
#% 879809
#% 950481
#% 993949
#! Traditional workflow enactment systems view a workflow as a one-time interaction with various data sources, executing a series of steps once, whenever the workflow results are requested. The fundamental underlying assumption has been that data sources are passive and all interactions are structured along the request/reply (query) model. Hence, traditional Workflow Management Systems cannot effectively support business or scientific reactive applications that require the processing of continuous data streams. In this demo, we will present our prototype which transforms workflow execution from the traditional step-wise workflow execution model to a continuous execution model, in order to handle data streams published and delivered asynchronously from multiple sources. We will demonstrate a supply chain management scenario which takes advantage of our continuous execution model to enable on-line interaction between different user roles as well as streaming data coming from various sources.

#index 1581980
#* Demonstration of Qurk: a query processor for humanoperators
#@ Adam Marcus;Eugene Wu;David R. Karger;Samuel Madden;Robert C. Miller
#t 2011
#c 5
#% 442850
#% 1477559
#% 1477589
#% 1581851
#! Crowdsourcing technologies such as Amazon's Mechanical Turk ("MTurk") service have exploded in popularity in recent years. These services are increasingly used for complex human-reliant data processing tasks, such as labelling a collection of images, combining two sets of images to identify people that appear in both, or extracting sentiment from a corpus of text snippets. There are several challenges in designing a workflow that filters, aggregates, sorts and joins human-generated data sources. Currently, crowdsourcing-based workflows are hand-built, resulting in increasingly complex programs. Additionally, developers must hand-optimize tradeoffs among monetary cost, accuracy, and time to completion of results. These challenges are well-suited to a declarative query interface that allows developers to describe their worflow at a high level and automatically optimizes workflow and tuning parameters. In this demonstration, we will present Qurk, a novel query system that allows human-based processing for relational databases. The audience will interact with the system to build queries and monitor their progress. The audience will also see Qurk from an MTurk user's perspective, and complete several tasks to better understand how a query is processed.

#index 1581981
#* Automatic example queries for ad hoc databases
#@ Bill Howe;Garret Cole;Nodira Khoussainova;Leilani Battle
#t 2011
#c 5
#% 466240
#% 480645
#% 845350
#% 960233
#% 1328162
#% 1328200
#% 1523949
#! Motivated by eScience applications, we explore automatic generation of example "starter" queries over unstructured collections of tables without relying on a schema, a query log, or prior input from users. Such example queries are demonstrably sufficient to have non-experts self-train and become productive using SQL, helping to increase the uptake of database technology among scientists. Our method is to learn a model for each relational operator based on example queries from public databases, then assemble queries syntactically operator-by-operator. For example, the likelihood that a pair of attributes will be used as a join condition in an example query depends on the cardinality of their intersection, among other features. Our demonstration illustrates that datasets with different statistical properties lead to different sets of example queries with different properties.

#index 1581982
#* NetTrails: a declarative platform for maintaining and querying provenance in distributed systems
#@ Wenchao Zhou;Qiong Fei;Shengzhi Sun;Tao Tao;Andreas Haeberlen;Zachary Ives;Boon Thau Loo;Micah Sherr
#t 2011
#c 5
#% 1246527
#% 1426553
#% 1426581
#! We demonstrate NetTrails, a declarative platform for maintaining and interactively querying network provenance in a distributed system. Network provenance describes the history and derivations of network state that result from the execution of a distributed protocol. It has broad applicability in the management, diagnosis, and security analysis of networks. Our demonstration shows the use of NetTrails for maintaining and querying network provenance in a variety of distributed settings, ranging from declarative networks to unmodified legacy distributed systems. We conclude our demonstration with a discussion of our ongoing research on enhancing the query language and security guarantees.

#index 1581983
#* GBLENDER: visual subgraph query formulation meets query processing
#@ Changjiu Jin;Sourav S. Bhowmick;Xiaokui Xiao;Byron Choi;Shuigeng Zhou
#t 2011
#c 5
#% 629708
#% 832188
#% 1426511
#% 1426577
#! Due to the complexity of graph query languages, the need for visual query interfaces that can reduce the burden of query formulation is fundamental to the spreading of graph data management tools to wider community. We present a novel HCI (human-computer interaction)-aware graph query processing paradigm, where instead of processing a query graph after its construction, it interleaves visual query construction and processing to improve system response time. We demonstrate a system called GBLENDER that exploits GUI latency to prune false results and prefetch candidate data graphs by employing a novel action-aware indexing scheme and a data structure called spindle-shaped graphs (SPIG). We demonstrate various innovative features of GBLENDER and its promising performance in evaluating subgraph containment and similarity queries.

#index 1581984
#* Coordination through querying in the youtopia system
#@ Nitin Gupta;Lucja Kot;Gabriel Bender;Sudip Roy;Johannes Gehrke;Christoph Koch
#t 2011
#c 5
#% 32897
#% 50077
#% 1472962
#% 1581902
#! In a previous paper, we laid out the vision of declarative data-driven coordination (D3C) where users are provided with novel abstractions that enable them to communicate and coordinate through declarative specifications [3]. In this demo, we will show Youtopia, a novel database system which is our first attempt at implementing this vision. Youtopia provides coordination abstractions within the DBMS. Users submit queries that come with explicit coordination constraints to be met by other queries in the system. Such queries are evaluated together; the system ensures that their joint execution results in the satisfaction of all coordination constraints. That is, the queries coordinate their answers in the manner specified by the users. We show how Youtopia and its abstractions simplify the implementation of a three-tier flight reservation application that allows users to coordinate travel arrangements with their friends.

#index 1581985
#* DBWiki: a structured wiki for curated data and collaborative data management
#@ Peter Buneman;James Cheney;Sam Lindley;Heiko Müller
#t 2011
#c 5
#% 875015
#% 960234
#% 1063581
#% 1063709
#% 1404050
#! Wikis have proved enormously successful as a means to collaborate in the creation and publication of textual information. At the same time, a large number of curated databases have been developed through collaboration for the dissemination of structured data in specific domains, particularly bioinformatics. We demonstrate a general-purpose platform for collaborative data management, DBWiki, designed to achieve the best of both worlds. Our system not only facilitates the collaborative creation of a database; it also provides features not usually provided by database technology such as versioning, provenance tracking, citability, and annotation. In our demonstration we will show how DBWiki makes it easy to create, correct, discuss and query structured data, placing more power in the hands of users while managing tedious details of data curation automatically.

#index 1581986
#* Rapid development of web-based query interfacesfor XML datasets with QURSED
#@ Abhijith Kashyap;Michalis Petropoulos
#t 2011
#c 5
#% 570877
#% 808601
#! We present QURSED, a system that automates the development of web-based query forms and reports (QFRs) for semi-structured XML data. Whereas many tools for automated development of QFRs have been proposed for relational datasets, QURSED- to the best of our knowledge, is the first tool that facilitates development of web-based QFRs over XML data. The QURSED system is available online at http://db.cse.buffalo.edu/qursed.

#index 1586196
#* Proceedings of the Seventh International Workshop on Data Management on New Hardware
#@ Stavros Harizopoulos;Qiong Luo
#t 2011
#c 5
#! The aim of this one-day workshop is to bring together researchers who are interested in optimizing database performance on modern computing infrastructure by designing new data management techniques and tools.

#index 1586955
#* Databases and Social Networks
#@ Denilson Barbosa;Gerome Miklau;Cong Yu
#t 2011
#c 5
#! The ACM SIGMOD Workshop on Databases and Social Networks (DBSocial) aims to disseminate results founded on database research and practice that advance the state-of-the-art in the observation, management, and analysis of inherently networked data that results primarily from social phenomena. In particular, DBSocial is intended to foster a discussion about the role that the database community should play in the area of social network research. As such, DBSocial welcomes papers whose approaches are fundamentally centered on theoretical foundations and best practices in databases and very closely related areas such as data mining and information retrieval

#index 1589311
#* Proceedings of the International Workshop on Semantic Web Information Management
#@ Roberto De Virgilio;Fausto Giunchiglia;Letizia Tanca
#t 2011
#c 5
#! The ceaseless expansion of the World Wide Web is making more and more complex for humans to efficiently find the needed information. The underlying idea of having a description of the data on the Web, organized in such a way as to be used by machines for automation, integration and reuse across various applications, has been exploited in several research fields. As in the previous editions, the International Workshop on "Semantic Web Information Management" (SWIM) aims at reviewing the most recent data-centered solutions for the Semantic Web. In particular, its ambition is to present and analyze the techniques for semantic information management, by taking advantage of the synergisms between the logical basis of the semantic web and the logical foundations of conceptual modeling. Indeed, the leitmotif of these researches is the proposal of models and methods conceived to represent and manage the so-called "semantic data", that is, data appropriately structured to be easily machine-processable on the Web, according to semantic models (e.g. RDF, RDF(S), OWL). The long-standing experience of the information modeling community can provide a priceless contribution to the substantial problems arising in semantic data management.

#index 1589321
#* Proceedings of the 10th ACM International Workshop on Data Engineering for Wireless and Mobile Access
#@ Georgios Kollios;Yufei Tao
#t 2011
#c 5

#index 1596017
#* Improving the performance of identifying contributors for XML keyword search
#@ Rung-Ren Lin;Ya-Hui Chang;Kun-Mao Chao
#t 2011
#c 5
#% 397366
#% 424332
#% 654442
#% 810052
#% 1015258
#% 1016135
#% 1019060
#% 1044480
#% 1127424
#% 1224353
#% 1372728
#% 1399984
#% 1465391
#% 1490131
#! Keyword search is a friendly mechanism for users to identify desired information in XML databases, and LCA is a popular concept for locating the meaningful subtrees corresponding to query keywords. Among all the LCA-based approaches, MaxMatch [9] is the only one which could achieve the property of monotonicity and consistency, by outputting only contributors instead of the whole subtree. Although the MaxMatch algorithm performs efficiently in some cases, there is still room for improvement. In this paper, we first propose to improve its performance by avoiding unnecessary index accesses. We then speed up the process of subset detection, which is a core procedure for determining contributors. The resultant algorithm is called MinMap and MinMap+, respectively. At last, we analytically and empirically demonstrate the efficiency of our methods. According to our experiments, our two algorithms work better than the existing one, and MinMap+ is particularly helpful when the breadth of the tree is large and the number of keywords grows.

#index 1596018
#* Exploring schema repositories with schemr
#@ Kuang Chen;Akshay Kannan;Jayant Madhavan;Alon Halevy
#t 2011
#c 5
#% 431103
#% 572314
#% 800497
#% 801413
#% 845350
#% 893115
#% 1127393
#% 1217257
#% 1426593
#! Schemr is a search engine for users to search for and visualize schemas in a metadata repository. Users may search by keywords and by example, using schema fragments as query terms. Schemr uses a novel search algorithm, based on a combination of text search and schema matching techniques, coupled with a structurally-aware scoring metric. Schemr presents search results in a GUI that allows users to explore which elements match and how well they do. The GUI supports interactions, including panning, zooming, layout and drilling-in. This paper introduces Schemr as a new component of the information integration toolbox and discusses its benefits in several applications.

#index 1596019
#* Dennis Shasha speaks out: on how puzzles helped his career, what drives him to write, how we can help biologists, the principles underlying database tuning, why he wears shorts all year, and more
#@ Marianne Winslett
#t 2011
#c 5

#index 1596020
#* Affiliation analysis of database publications
#@ David Aumüller;Erhard Rahm
#t 2011
#c 5
#% 292514
#% 723439
#% 845353
#% 1127741
#! We analyze the author affiliations of database publications to determine the main institutions contributing research results in our field. We consider the publications of the last decade (2000'2009) that appeared in the top conferences SIGMOD and VLDB and in the VLDBJ and TODS journals. We determine the top affiliations in terms of number of papers and aggregate the numbers at the levels of entire countries and continents. Further, we analyze to which degree authors from different affiliations and countries cooperate on jointly authored papers, and study the development over time. We also consider the number and size of affiliations of different countries.

#index 1596021
#* Report on data-intensive software management and mining
#@ Seung-won Hwang
#t 2011
#c 5
#% 906062
#% 1384146
#% 1384377
#% 1490465

#index 1596022
#* Dagstuhl seminar on bidirectional transformations (BX)
#@ Zhenjiang Hu;Andy Schurr;Perdita Stevens;James F. Terwilliger
#t 2011
#c 5
#% 286901
#% 539527
#% 790328
#% 874911
#% 1024193
#% 1092008
#% 1105428
#% 1230599
#% 1475508
#% 1523822
#% 1529797
#% 1541335
#% 1578656
#% 1578672
#% 1728141

#index 1596023
#* Report on DEIS'10: advanced school on data exchange, information, and streams (A GI-Dagstuhl Seminar)
#@ Phokion G. Kolaitis;Maurizio Lenzerini;Nicole Schweikardt
#t 2011
#c 5

#index 1617916
#* Search, adapt, and reuse: the future of scientific workflows
#@ Sarah Cohen-Boulakia;Ulf Leser
#t 2011
#c 5
#% 235941
#% 268797
#% 765409
#% 793374
#% 810103
#% 853032
#% 879803
#% 879809
#% 879810
#% 906520
#% 929605
#% 954295
#% 961135
#% 1042656
#% 1063500
#% 1063571
#% 1063593
#% 1065709
#% 1072370
#% 1079538
#% 1090730
#% 1111653
#% 1155147
#% 1174009
#% 1174012
#% 1206750
#% 1207025
#% 1358747
#% 1372706
#% 1372707
#% 1375919
#% 1426581
#% 1426593
#% 1442582
#% 1448950
#% 1489256
#% 1523877
#% 1720925
#! Over the last years, a number of scientific workflow management systems (SciWFM) have been brought to a state of maturity that should permit their usage in a production-style environment. This is especially true for the Life Sciences, but SciWFM also attract considerable attention in fields like geophysics or climate research. These developments, accompanied by the growing availability of analytical tools wrapped as (web) services, were driven by a series of very interesting promises: End users will be empowered to develop their own pipelines; reuse of services will be enhanced by easier integration into custom workflows; time necessary for developing analysis pipelines will decrease; etc. But despite all efforts, SciWFM have not yet found widespread acceptance in their intended audience. In this paper, we argue that a wider adoption of SciWFM will only be achieved if the focus of research and development is shifted from methods for developing and running workflows to searching, adapting, and reusing existing workflows. Only by this shift can SciWFM outreach to the mass of domain scientists actually performing scientific analysis - and with little interest in developing them themselves. To this end, SciWFM need to be combined with communitywide workflow repositories allowing users to find solutions for their scientific needs (coded as a workflow). In this vision paper, we show how and where such developments have already started and highlight new research questions arising.

#index 1617917
#* A survey on energy-efficient data management
#@ Jun Wang;Ling Feng;Wenwei Xue;Zhanjiang Song
#t 2011
#c 5
#% 303994
#% 340221
#% 657650
#% 765402
#% 828584
#% 896764
#% 960264
#% 963135
#% 1008996
#% 1034471
#% 1034473
#% 1037583
#% 1065042
#% 1127556
#% 1137284
#% 1174227
#% 1180960
#% 1213723
#% 1213726
#% 1263928
#% 1309515
#% 1327638
#% 1414199
#% 1426521
#% 1429792
#% 1432737
#% 1433982
#% 1464311
#% 1468292
#% 1523806
#! Energy management has now become a critical and urgent issue in green computing. A lot of efforts have been made on energy-efficiency computing at various levels from individual hardware components, system software, to applications. In this paper, we describe the energyefficiency computing problem, as well as possible strategies to tackle the problem. We survey some recently developed energy-saving data management techniques. Benchmarks and power models are described in the end for the evaluation of energy-efficiency solutions.

#index 1617918
#* Laura Haas speaks out on managing teams versus children, research versus product development, and much more
#@ Marianne Winslett;Vanessa Braganholo
#t 2011
#c 5

#index 1617919
#* Alberto Laender speaks out on why Google bought his startup, how to evaluate graduate program quality, how to do high-impact research in a developing country, how hyperinflation nurtured Brazil's software industry, and more
#@ Marianne Winslett
#t 2011
#c 5

#index 1617920
#* Report on the first international workshop on flash-based database systems (FlashDB 2011)
#@ Xiaofeng Meng;Peiquan Jin;Wei Cao;Lihua Yue
#t 2011
#c 5
#% 960238
#% 1052793
#% 1092670
#% 1130947
#% 1217151
#% 1245061
#% 1482212
#% 1587192
#% 1823467

#index 1617921
#* Repeatability and workability evaluation of SIGMOD 2011
#@ Philippe Bonnet;Stefan Manegold;Matias Bjørling;Wei Cao;Javier Gonzalez;Joel Granados;Nancy Hall;Stratos Idreos;Milena Ivanova;Ryan Johnson;David Koop;Tim Kraska;René Müller;Dan Olteanu;Paolo Papotti;Christine Reilly;Dimitris Tsirogiannis;Cong Yu;Juliana Freire;Dennis Shasha
#t 2011
#c 5
#% 1061894
#% 1433978
#! SIGMOD has offered, since 2008, to verify the experiments published in the papers accepted at the conference. This year, we have been in charge of reproducing the experiments provided by the authors (repeatability), and exploring changes to experiment parameters (workability). In this paper, we assess the SIGMOD repeatability process in terms of participation, review process and results. While the participation is stable in terms of number of submissions, we find this year a sharp contrast between the high participation from Asian authors and the low participation from American authors. We also find that most experiments are distributed as Linux packages accompanied by instructions on how to setup and run the experiments. We are still far from the vision of executable papers.

#index 1647976
#* A logical toolbox for ontological reasoning
#@ Andrea Calì;Georg Gottlob;Thomas Lukasiewicz;Andreas Pieris
#t 2011
#c 5
#% 191611
#% 265104
#% 287339
#% 384978
#% 480648
#% 490909
#% 576116
#% 591778
#% 598376
#% 599549
#% 733595
#% 736407
#% 826032
#% 992962
#% 1020896
#% 1063724
#% 1217115
#% 1217122
#% 1347304
#% 1409909
#% 1413141
#% 1416180
#% 1511857
#% 1523844
#% 1552660
#% 1585244
#% 1732822
#! In ontology-enhanced database systems, an ontology on top of the extensional database expresses intensional knowledge that enhances the database schema. Queries posed to such systems are to be evaluated considering all the knowledge inferred from the data by means of the ontology; in other words, queries are to be evaluated against the logical theory constituted by the data and the ontology. In this context, tractability of query answering is a central issue, given that the data size is normally very large. This paper surveys results on a recently introduced family of Datalog-based languages, called Datalog+/-, which is a useful logical toolbox for ontology modeling and for ontology-based query answering. We present different Datalog+/- languages and related complexity results, showing that Datalog+/- can be successfully adopted due to its clarity, expressiveness and its good computational properties.

#index 1647977
#* The database Wiki project: a general-purpose platform for data curation and collaboration
#@ Peter Buneman;James Cheney;Sam Lindley;Heiko Mueller
#t 2011
#c 5
#% 330627
#% 742561
#% 742563
#% 765450
#% 875015
#% 885368
#% 960234
#% 1063581
#% 1063709
#% 1404050
#% 1581985
#! Databases and wikis have complementary strengths and weaknesses for use in collaborative data management and data curation. Relational databases, for example, offer advantages such as scalability, query optimization and concurrency control, but are not easy to use and lack other features needed for collaboration. Wikis have proved enormously successful as a means to collaborate because they are easy to use, encourage sharing, and provide built-in support for archiving, history-tracking and annotation. However, wikis lack support for structured data, efficiently querying data at scale, and localized provenance and annotation. To achieve the best of both worlds, we are developing a general-purpose platform for collaborative data management, called DBWIKI. Our system not only facilitates the collaborative creation of structured data; it also provides features not usually provided by database technology such as annotation, citability, versioning, and provenance tracking. This paper describes the technical details behind DBWIKI that make it easy to create, correct, discuss, and query structured data, placing more power in the hands of users while managing tedious details of data curation automatically.

#index 1647978
#* Meral Özsoyoğlu speaks out: on genealogical data management, searching ontologies, and more
#@ Marianne Winslett;Vanessa Braganholo
#t 2011
#c 5

#index 1647979
#* Divesh Srivastava speaks out: on the importance of looking at real data, abstracting problems and more
#@ Marianne Winslett;Vanessa Braganholo
#t 2011
#c 5

#index 1647980
#* Data management research at NEC labs
#@  Data Management Research Group
#t 2011
#c 5
#% 130338
#% 287352
#% 754085
#% 1022748
#% 1063554
#% 1463413
#% 1592314
#% 1594596
#% 1621139
#% 1643315

#index 1647981
#* Integration of vectorwise with ingres
#@ Doug Inkster;Marcin Zukowski;Peter Boncz
#t 2011
#c 5
#% 1977
#% 689389
#% 864446
#% 1022262
#% 1426547
#% 1586201
#! Actian Corporation recently entered into a cooperative relationship with VectorWise BV to integrate its Vector-Wise technology into the Ingres RDBMS server. The resulting commercial product has already achieved phenomenal performance results with the TPC-H industry standard benchmark, and has been well received in the analytical RDBMS market. This paper describes the integration of the VectorWise technology with Ingres, some of the design decisions made as part of the integration project, and the problems that had to be solved in the process.

#index 1647982
#* 30 Years of PODS in facts and figures
#@ Tom J. Ameloot;Maarten Marx;Wim Martens;Frank Neven;Justin Van Wees
#t 2011
#c 5
#% 1425621

#index 1647983
#* A call to arms: revisiting database design
#@ Antonio Badia;Daniel Lemire
#t 2011
#c 5
#% 158908
#% 275367
#% 287631
#% 301171
#% 322880
#% 389068
#% 393844
#% 397295
#% 427305
#% 431103
#% 654470
#% 654482
#% 765455
#% 765462
#% 805821
#% 810111
#% 810117
#% 824660
#% 825661
#% 873186
#% 874876
#% 875029
#% 960271
#% 960369
#% 1021601
#% 1022202
#% 1044447
#% 1117704
#% 1134501
#% 1189164
#% 1200291
#% 1215465
#% 1217228
#% 1312538
#% 1328066
#% 1366249
#% 1399992
#% 1416121
#% 1425119
#% 1468027
#% 1473009
#% 1523960
#% 1529327
#% 1531193
#% 1551289
#% 1567483
#% 1573139
#% 1573340
#% 1705177
#% 1728313
#% 1841633

#index 1667308
#* Optimizing index scans on flash memory SSDs
#@ Eun-Mi Lee;Sang-Won Lee;Sangwon Park
#t 2012
#c 5
#% 18614
#% 393844
#% 411554
#% 462941
#% 1063551
#% 1085291
#% 1217213
#! Unlike harddisks, flash memory SSDs have very fast latency in random reads and thus the relative bandwidth gap between sequential and random read is quite small, though not negligible. For this reason, it has been believed that index scan would become more attractive access method in flash memory storage devices. In reality, however, the existing index scan can outperform the full table scan only in very selective predicates. In this paper, we investigate how to optimize the index scan on flash memory SSDs. First, we empirically show that the index scan underperforms the full table scan even when the selectivity of selection predicate is less than 5% and explain its reason. Second, we revisit the idea of sorted index scan and demonstrate that it can outperform the full table scan even when the selectivity is larger than 30%. However, one drawback of the sorted index scan is that it loses the sortedness of the retrieved records. Third, in order to efficiently resort the result from the sorted index scan, we propose a new external index-based sort algorithm, partitioned sort, which exploits the information of key value distribution in the index leaf nodes. It can sort data in one pass regardless of the available sort memory size.

#index 1667309
#* Parallel data processing with MapReduce: a survey
#@ Kyong-Ha Lee;Yoon-Joon Lee;Hyunsik Choi;Yon Dohn Chung;Bongki Moon
#t 2012
#c 5
#% 480821
#% 723279
#% 800491
#% 954300
#% 960326
#% 963669
#% 983467
#% 1017256
#% 1023419
#% 1023420
#% 1054227
#% 1063553
#% 1127354
#% 1127550
#% 1127559
#% 1127590
#% 1157449
#% 1157462
#% 1208113
#% 1215805
#% 1217159
#% 1217232
#% 1278123
#% 1278124
#% 1292903
#% 1328059
#% 1328060
#% 1328095
#% 1328186
#% 1372690
#% 1373695
#% 1373699
#% 1426486
#% 1426488
#% 1426513
#% 1426543
#% 1426544
#% 1426583
#% 1426584
#% 1426585
#% 1459261
#% 1459262
#% 1464195
#% 1464709
#% 1464950
#% 1467704
#% 1468411
#% 1468421
#% 1468423
#% 1482627
#% 1523806
#% 1523820
#% 1523824
#% 1523837
#% 1523839
#% 1523841
#% 1523924
#% 1542029
#% 1573238
#% 1581407
#% 1581925
#% 1581926
#% 1581928
#% 1594639
#% 1602033
#! A prominent parallel data processing tool MapReduce is gaining significant momentum from both industry and academia as the volume of data to analyze grows rapidly. While MapReduce is used in many areas where massive data analysis is required, there are still debates on its performance, efficiency per node, and simple abstraction. This survey intends to assist the database and open source communities in understanding various technical aspects of the MapReduce framework. In this survey, we characterize the MapReduce framework and discuss its inherent pros and cons. We then introduce its optimization strategies reported in the recent literature. We also discuss the open issues and challenges raised on parallel data analysis with MapReduce.

#index 1667310
#* Processing and visualizing the data in tweets
#@ Adam Marcus;Michael S. Bernstein;Osama Badar;David R. Karger;Samuel Madden;Robert C. Miller
#t 2012
#c 5
#% 300167
#% 300169
#% 397372
#% 726621
#% 1063553
#% 1573368
#% 1573369
#! Microblogs such as Twitter provide a valuable stream of diverse user-generated data. While the data extracted from Twitter is generally timely and accurate, the process by which developers extract structured data from the tweet stream is ad-hoc and requires reimplementation of common data manipulation primitives. In this paper, we present two systems for querying and extracting structure from Twitter-embedded data. The first, TweeQL, provides a streaming SQL-like interface to the Twitter API, making common tweet processing tasks simpler. The second, TwitInfo, shows how end-users can interact with and understand aggregated data from the tweet stream, in addition to showcasing the power of the TweeQL language. Together these systems show the richness of content that can be extracted from Twitter.

#index 1667311
#* Jiawei Han speaks out: on data mining, privacy issues and managing students
#@ Marianne Winslett;Vanessa Braganholo
#t 2012
#c 5

#index 1667312
#* The database architectures research group at CWI
#@ Martin Kersten;Stefan Manegold;Sjoerd Mullender
#t 2012
#c 5
#% 443513
#% 479821
#% 875010
#% 960268
#% 993947
#% 1016186
#% 1052066
#% 1129957
#% 1181240
#% 1217169
#% 1217170
#% 1217193
#% 1328102
#% 1372688
#% 1592316

#index 1667313
#* SAP HANA database: data management for modern business applications
#@ Franz Färber;Sang Kyun Cha;Jürgen Primsch;Christof Bornhövd;Stefan Sigg;Wolfgang Lehner
#t 2012
#c 5
#% 1159178
#% 1372742
#% 1594650
#% 1614904
#% 1615897
#% 1697286
#! The SAP HANA database is positioned as the core of the SAP HANA Appliance to support complex business analytical processes in combination with transactionally consistent operational workloads. Within this paper, we outline the basic characteristics of the SAP HANA database, emphasizing the distinctive features that differentiate the SAP HANA database from other classical relational database management systems. On the technical side, the SAP HANA database consists of multiple data processing engines with a distributed query processing environment to provide the full spectrum of data processing -- from classical relational data supporting both row- and column-oriented physical representations in a hybrid engine, to graph and text processing for semi- and unstructured data management within the same system. From a more application-oriented perspective, we outline the specific support provided by the SAP HANA database of multiple domain-specific languages with a built-in set of natively implemented business functions. SQL -- as the lingua franca for relational database systems -- can no longer be considered to meet all requirements of modern applications, which demand the tight interaction with the data management layer. Therefore, the SAP HANA database permits the exchange of application semantics with the underlying data management platform that can be exploited to increase query expressiveness and to reduce the number of individual application-to-database round trips.

#index 1667314
#* Report on the 8th international workshop on quality in databases (QDB10)
#@ Andrea Maurino;Cinzia Cappiello;Panos Vassiliadis;Kai-Uwe Sattler
#t 2012
#c 5
#% 960365
#% 1100631
#% 1475101
#% 1497998

#index 1667315
#* The meaningful use of big data: four perspectives -- four challenges
#@ Christian Bizer;Peter Boncz;Michael L. Brodie;Orri Erling
#t 2012
#c 5
#! Twenty-five Semantic Web and Database researchers met at the 2011 STI Semantic Summit in Riga, Latvia July 6-8, 2011[1] to discuss the opportunities and challenges posed by Big Data for the Semantic Web, Semantic Technologies, and Database communities. The unanimous conclusion was that the greatest shared challenge was not only engineering Big Data, but also doing so meaningfully. The following are four expressions of that challenge from different perspectives.

#index 1667316
#* Fourth workshop on very large digital libraries: on the marriage between very large digital libraries and very large data archives
#@ Leonardo Candela;Paolo Manghi;Yannis Ioannidis
#t 2012
#c 5
#% 1183385
#% 1433987
#% 1550142
#% 1624229

#index 1764503
#* Optimizing XML twig queries with full-text predicates
#@ Ya-Hui Chang
#t 2012
#c 5
#% 397375
#% 765466
#% 824667
#% 824703
#% 875018
#% 893112
#% 1127424
#% 1217192
#% 1372696
#! Efficient query processing has been a critical issue for XML repositories. In this paper, we consider the XML query which can be represented as a query tree with twig patterns, and also consists of full-text constraints. Previously, the structure-first approach and the keyword-first approach have been proposed to process such kind of queries. The main focus of this paper is constructing an integrated system to support these two approaches and find the best execution plan. To achieve this goal, we first analyze the components of these two approaches and design a set of operators. We then derive the corresponding cost model and rewriting rules to perform costbased optimization. We also propose several heuristic rules by observing the behaviors of the two approaches. Via an extensive experimental study, we demonstrate that our cost-based system and heuristic system are both effective.

#index 1764504
#* Quality-aware service-oriented data integration: requirements, state of the art and open challenges
#@ Schahram Dustdar;Reinhard Pichler;Vadim Savenkov;Hong-Linh Truong
#t 2012
#c 5
#% 136740
#% 184695
#% 248038
#% 330305
#% 344898
#% 378409
#% 447907
#% 464891
#% 572307
#% 572311
#% 588533
#% 629276
#% 719298
#% 770146
#% 778321
#% 801692
#% 811656
#% 826032
#% 863398
#% 864417
#% 874876
#% 976987
#% 976997
#% 1022258
#% 1022259
#% 1036075
#% 1037015
#% 1039063
#% 1063712
#% 1063737
#% 1065125
#% 1111108
#% 1129527
#% 1153810
#% 1163517
#% 1190113
#% 1197728
#% 1202161
#% 1215807
#% 1245999
#% 1335448
#% 1414317
#% 1459820
#% 1493449
#% 1512791
#% 1554587
#% 1611811
#% 1661428
#% 1661432
#% 1661789
#% 1661791
#! With a multitude of data sources available online, data consumers might find it hard to select the best combination of sources for their needs. Aspects such as price, licensing, service and data quality play a major role in selecting data sources. We therefore advocate qualityaware data services as a natural data source model for complex data integration tasks and mash-ups. This paper focuses on requirements, state of the art, and the main research challenges on the way to the realization of such services.

#index 1764505
#* A survey of view selection methods
#@ Imene Mami;Zohra Bellahsene
#t 2012
#c 5
#% 51676
#% 152928
#% 199537
#% 201928
#% 210182
#% 210208
#% 248806
#% 273697
#% 273917
#% 286991
#% 300166
#% 330305
#% 333962
#% 335726
#% 340301
#% 369236
#% 397351
#% 397432
#% 424925
#% 451768
#% 462079
#% 464706
#% 464878
#% 479792
#% 480158
#% 482110
#% 482111
#% 487530
#% 488275
#% 566126
#% 568186
#% 572311
#% 589226
#% 635798
#% 837649
#% 853008
#% 875062
#% 895186
#% 1014206
#% 1101772
#% 1181305
#% 1384975
#% 1388097
#% 1529012
#% 1546764
#% 1616878
#% 1654043
#% 1676093
#% 1776382
#% 1776509
#! Materialized view selection is a critical problem in many applications such as query processing, data warehousing, distributed and semantic web databases, etc. We refer to the problem of selecting an appropriate set of materialized views as the view selection problem. Many different view selection methods have been proposed in the literature to address this issue. The present paper provides a survey of view selection methods. It defines a framework for highlighting the view selection problem by identifying the main dimensions that are the basis in the classification of view selection methods. Based on this classification, this study reviews most of the view selection methods by identifying respective potentials and limits.

#index 1764506
#* David Lomet speaks out: on database recovery, logs, versions and more...
#@ Marianne Winslett;Vanessa Braganholo
#t 2012
#c 5

#index 1764507
#* Catriel Beeri speaks out: on his favorite pieces of work and on the importance of Sabbaticals
#@ Marianne Winslett;Vanessa Braganholo
#t 2012
#c 5

#index 1764508
#* Query languages for graph databases
#@ Peter T. Wood
#t 2012
#c 5
#% 28120
#% 32904
#% 36309
#% 56639
#% 64902
#% 101925
#% 120649
#% 145182
#% 151432
#% 154334
#% 157133
#% 197751
#% 235941
#% 237191
#% 248025
#% 261370
#% 265104
#% 268797
#% 268799
#% 333845
#% 415004
#% 442887
#% 442960
#% 463919
#% 465062
#% 481434
#% 481935
#% 571038
#% 571040
#% 577372
#% 862101
#% 905847
#% 1019798
#% 1063500
#% 1098424
#% 1101707
#% 1152437
#% 1166490
#% 1181231
#% 1206702
#% 1206916
#% 1218644
#% 1292514
#% 1372706
#% 1396751
#% 1426443
#% 1426462
#% 1538787
#% 1540331
#% 1594585
#% 1674366
#% 1818411
#! Query languages for graph databases started to be investigated some 25 years ago. With much current data, such as linked data on the Web and social network data, being graph-structured, there has been a recent resurgence in interest in graph query languages. We provide a brief survey of many of the graph query languages that have been proposed, focussing on the core functionality provided in these languages. We also consider issues such as expressive power and the computational complexity of query evaluation.

#index 1764509
#* Institute for the management of information systems Athena research center
#@  IMIS research team
#t 2012
#c 5
#% 1080161
#% 1127361
#% 1181289
#% 1207004
#% 1218741
#% 1230826
#% 1298901
#% 1364945
#% 1431356
#% 1480851
#% 1480862
#% 1495138
#% 1512785
#% 1538422
#% 1546751
#% 1549848
#% 1549855
#% 1562909
#% 1601216
#% 1614884
#% 1618257
#% 1618260
#% 1618279
#% 1624262
#% 1641936
#% 1646598
#% 1667266
#% 1700738
#% 1847979
#% 1940422

#index 1764510
#* What.s new in SQL:2011
#@ Fred Zemke
#t 2012
#c 5
#% 278619
#% 742057
#% 783793
#! SQL:2011 was published in December 2011, replacing the former version (SQL:2008) as the most recent update to the SQL standard for relational databases. This paper surveys the new non-temporal features of SQL:2011.

#index 1764511
#* The curriculum forecast for Portland: cloudy with a chance of data
#@ Michael Grossniklaus;David Maier
#t 2012
#c 5
#% 963669
#% 1180960
#% 1573340
#! With the advent of cloud computing, new data management technologies and systems have emerged that differ from existing databases in important ways. As a consequence, universities are currently facing the challenge of integrating these topics into their curriculum in order to prepare students for the changed IT landscape. In this report, we describe the approach we have taken at Portland State University to teach data management in the cloud. We also present our experiences with this effort and give an outlook on how it could be adapted to suit the requirements of other universities.

#index 1770114
#* Proceedings of the 31st symposium on Principles of Database Systems
#@ Markus Krötzsch;Maurizio Lenzerini;Michael Benedikt
#t 2012
#c 5
#! First, a brief overview of the contents of this volume, the proceedings of the thirty-first ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems (PODS 2012). The proceedings includes an overview of the keynote address by Surajit Chaudhuri along with two papers based on invited tutorials, one by Michael Mahoney and one by Benjamin Pierce. There are 26 research papers that were selected by the Program Committee, out of 101 submissions with authors from over 25 countries across the world. Out of the 26 accepted papers, the program committee selected the paper Worst Case Optimal Join Algorithms by Hung Q. Ngo, Ely Porat, Christopher Ré and Atri Rudra for the PODS 2012 Best Paper Award. In addition, the announcement of the 2012 ACM PODS Alberto O. Mendelzon Test-of-Time Award appears in the proceedings, given to Containment and Equivalence for an XPath Fragment by Gerome Miklau and Dan Suciu. The latter paper originally appeared in the proceedings of PODS 2002. Congratulations to the authors of these papers. The review process was grueling, and involved enormous effort from a large group of researchers. This year PODS experimented with the use of an External Review Committee, consisting of distinguished experts in areas of particular interest to PODS, in addition to our core Program Committee. We relied heavily on the Easychair system for management of all aspects of the review process, and we are extremely grateful to Andrei Voronkov for his help in adding and modifying new features to support the External Review Committee. All members of the Program Committee, External Review Committee, and the additional external referees deserve thanks for their work -- both for producing the selection of papers that appear here in the proceedings, and for providing high-quality feedback to all authors of submitted papers.

#index 1770115
#* What next?: a half-dozen data management research goals for big data and the cloud
#@ Surajit Chaudhuri
#t 2012
#c 5
#% 227883
#% 273908
#% 273909
#% 273910
#% 893178
#% 994014
#% 1063553
#% 1127559
#% 1214667
#% 1217148
#% 1217227
#% 1246515
#% 1278123
#% 1278124
#% 1328095
#% 1426594
#% 1573184
#% 1670071
#% 1711796
#% 1740518
#% 1746870
#! In this short paper, I describe six data management research challenges relevant for Big Data and the Cloud. Although some of these problems are not new, their importance is amplified by Big Data and Cloud Computing.

#index 1770116
#* Graph sketches: sparsification, spanners, and subgraphs
#@ Kook Jin Ahn;Sudipto Guha;Andrew McGregor
#t 2012
#c 5
#% 176270
#% 214077
#% 278835
#% 379443
#% 751684
#% 805744
#% 808428
#% 816392
#% 824653
#% 866773
#% 866990
#% 874902
#% 879397
#% 967414
#% 1192853
#% 1198209
#% 1231059
#% 1299097
#% 1426447
#% 1426450
#% 1524287
#% 1581819
#% 1584827
#% 1584896
#% 1611386
#% 1668207
#% 1682599
#% 1699388
#% 1708848
#! When processing massive data sets, a core task is to construct synopses of the data. To be useful, a synopsis data structure should be easy to construct while also yielding good approximations of the relevant properties of the data set. A particularly useful class of synopses are sketches, i.e., those based on linear projections of the data. These are applicable in many models including various parallel, stream, and compressed sensing settings. A rich body of analytic and empirical work exists for sketching numerical data such as the frequencies of a set of entities. Our work investigates graph sketching where the graphs of interest encode the relationships between these entities. The main challenge is to capture this richer structure and build the necessary synopses with only linear measurements. In this paper we consider properties of graphs including the size of the cuts, the distances between nodes, and the prevalence of dense sub-graphs. Our main result is a sketch-based sparsifier construction: we show that Õ(nε-2) random linear projections of a graph on n nodes suffice to (1+ε) approximate all cut values. Similarly, we show that Õ(ε-2) linear projections suffice for (additively) approximating the fraction of induced sub-graphs that match a given pattern such as a small clique. Finally, for distance estimation we present sketch-based spanner constructions. In this last result the sketches are adaptive, i.e., the linear projections are performed in a small number of batches where each projection may be chosen dependent on the outcome of earlier sketches. All of the above results immediately give rise to data stream algorithms that also apply to dynamic graph streams where edges are both inserted and deleted. The non-adaptive sketches, such as those for sparsification and subgraphs, give us single-pass algorithms for distributed data streams with insertion and deletions. The adaptive sketches can be used to analyze MapReduce algorithms that use a small number of rounds.

#index 1770117
#* Approximating and testing k-histogram distributions in sub-linear time
#@ Piotr Indyk;Reut Levi;Ronitt Rubinfeld
#t 2012
#c 5
#% 248821
#% 256686
#% 347226
#% 397385
#% 414770
#% 479648
#% 482123
#% 593972
#% 656750
#% 765292
#% 833351
#% 866990
#% 963287
#% 1015256
#% 1061620
#% 1166536
#% 1300144
#% 1584890
#% 1817390
#! A discrete distribution p, over [n], is a k histogram if its probability distribution function can be represented as a piece-wise constant function with k pieces. Such a function is represented by a list of k intervals and k corresponding values. We consider the following problem: given a collection of samples from a distribution p, find a k-histogram that (approximately) minimizes the l 2 distance to the distribution p. We give time and sample efficient algorithms for this problem. We further provide algorithms that distinguish distributions that have the property of being a k-histogram from distributions that are ε-far from any k-histogram in the l 1 distance and l 2 distance respectively.

#index 1770118
#* Mergeable summaries
#@ Pankaj K. Agarwal;Graham Cormode;Zengfeng Huang;Jeff Phillips;Zhewei Wei;Ke Yi
#t 2012
#c 5
#% 91390
#% 221326
#% 248820
#% 278835
#% 329791
#% 333931
#% 335411
#% 345611
#% 419377
#% 519953
#% 594012
#% 762055
#% 766228
#% 783740
#% 800582
#% 801695
#% 805466
#% 810031
#% 813786
#% 816392
#% 894443
#% 903219
#% 959933
#% 993969
#% 1039655
#% 1064269
#% 1105370
#% 1108067
#% 1127608
#% 1180018
#% 1292985
#% 1426447
#% 1426452
#% 1474891
#% 1496105
#% 1521643
#% 1701497
#% 1920318
#! We study the mergeability of data summaries. Informally speaking, mergeability requires that, given two summaries on two data sets, there is a way to merge the two summaries into a single summary on the union of the two data sets, while preserving the error and size guarantees. This property means that the summaries can be merged in a way like other algebraic operators such as sum and max, which is especially useful for computing summaries on massive distributed data. Several data summaries are trivially mergeable by construction, most notably all the sketches that are linear functions of the data sets. But some other fundamental ones like those for heavy hitters and quantiles, are not (known to be) mergeable. In this paper, we demonstrate that these summaries are indeed mergeable or can be made mergeable after appropriate modifications. Specifically, we show that for ε-approximate heavy hitters, there is a deterministic mergeable summary of size O(1/ε) for ε-approximate quantiles, there is a deterministic summary of size O(1 over ε log(εn))that has a restricted form of mergeability, and a randomized one of size O(1 over ε log 3/21 over ε) with full mergeability. We also extend our results to geometric summaries such as ε-approximations and εkernels. We also achieve two results of independent interest: (1) we provide the best known randomized streaming bound for ε-approximate quantiles that depends only on ε, of size O(1 over ε log 3/21 over ε, and (2) we demonstrate that the MG and the SpaceSaving summaries for heavy hitters are isomorphic.

#index 1770119
#* The ACM PODS Alberto O. Mendelzon test-of-time award 2012
#@ Richard Hull;Phokion G. Kolaitis;Dirk Van Gucht
#t 2012
#c 5

#index 1770120
#* Worst-case optimal join algorithms: [extended abstract]
#@ Hung Q. Ngo;Ely Porat;Christopher Ré;Atri Rudra
#t 2012
#c 5
#% 5758
#% 17802
#% 102784
#% 136740
#% 205156
#% 210190
#% 273682
#% 300167
#% 384978
#% 427161
#% 476682
#% 479648
#% 480125
#% 787408
#% 810016
#% 813756
#% 824682
#% 847068
#% 864426
#% 874899
#% 953714
#% 956456
#% 976985
#% 1015256
#% 1063548
#% 1141493
#% 1217119
#% 1426325
#% 1482709
#! Efficient join processing is one of the most fundamental and well-studied tasks in database research. In this work, we examine algorithms for natural join queries over many relations and describe a novel algorithm to process these queries optimally in terms of worst-case data complexity. Our result builds on recent work by Atserias, Grohe, and Marx, who gave bounds on the size of a full conjunctive query in terms of the sizes of the individual relations in the body of the query. These bounds, however, are not constructive: they rely on Shearer's entropy inequality which is information-theoretic. Thus, the previous results leave open the question of whether there exist algorithms whose running time achieve these optimal bounds. An answer to this question may be interesting to database practice, as we show in this paper that any project-join plan is polynomially slower than the optimal bound for some queries. We construct an algorithm whose running time is worst-case optimal for all natural join queries. Our result may be of independent interest, as our algorithm also yields a constructive proof of the general fractional cover bound by Atserias, Grohe, and Marx without using Shearer's inequality. In addition, we show that this bound is equivalent to a geometric inequality by Bollobás and Thomason, one of whose special cases is the famous Loomis-Whitney inequality. Hence, our results algorithmically prove these inequalities as well. Finally, we discuss how our algorithm can be used to compute a relaxed notion of joins.

#index 1770121
#* Deterministic regular expressions in linear time
#@ Benotît Groz;Sebastian Maneth;Slawek Staworko
#t 2012
#c 5
#% 186
#% 33611
#% 114570
#% 152835
#% 194366
#% 229119
#% 281842
#% 378392
#% 390964
#% 475997
#% 526925
#% 772031
#% 894435
#% 957534
#% 1232255
#% 1349599
#% 1370257
#% 1456293
#% 1543544
#% 1582162
#% 1661446
#% 1669572
#% 1818408
#! Deterministic regular expressions are widely used in XML processing. For instance, all regular expressions in DTDs and XML Schemas are required to be deterministic. In this paper we show that determinism of a regular expression e can be tested in linear time. The best known algorithms, based on the Glushkov automaton, require O(σ|e|) time, where σ is the number of distinct symbols in e. We further show that matching a word w against an expression e can be achieved in combined linear time O(|e|+|w|), for a wide range of deterministic regular expressions: (i) star-free (for multiple input words), (ii) bounded-occurrence, i.e., expressions in which each symbol appears a bounded number of times, and (iii) bounded plus-depth, i.e., expressions in which the nesting depth of alternating plus (union) and concatenation symbols is bounded. Our algorithms use a new structural decomposition of the parse tree of e. For matching arbitrary deterministic regular expressions we present an O(|e| + |w|log log|e|) time algorithm.

#index 1770122
#* Linguistic foundations for bidirectional transformations: invited tutorial
#@ Benjamin C. Pierce
#t 2012
#c 5
#% 43031
#% 286901
#% 287000
#% 378401
#% 641844
#% 769848
#% 782777
#% 809123
#% 815501
#% 828725
#% 874911
#% 889571
#% 948929
#% 957975
#% 997006
#% 1019218
#% 1024193
#% 1066609
#% 1091055
#% 1108966
#% 1111235
#% 1134530
#% 1150508
#% 1192243
#% 1221129
#% 1230599
#% 1255692
#% 1475506
#% 1475507
#% 1475508
#% 1527175
#% 1528297
#% 1596022
#% 1617884
#% 1673658
#% 1675648
#% 1706847
#% 1804788
#% 1870335
#% 1916164
#! Computing is full of situations where two different structures must be "connected" in such a way that updates to each can be propagated to the other. This is a generalization of the classical view update problem, which has been studied for decades in the database community [11, 2, 22]; more recently, related problems have attracted considerable interest in other areas, including programming languages [42, 28, 34, 39, 4, 7, 33, 16, 1, 37, 35, 47, 49] software model transformation [43, 50, 44, 45, 12, 13, 14, 24, 25, 10, 51], user interfaces [38] and system configuration [36]. See [18, 17, 10, 30] for recent surveys. Among the fruits of this cross-pollination has been the development of a linguistic perspective on the problem. Rather than taking some view definition language as fixed (e.g., choosing some subset of relational algebra) and looking for tractable ways of "inverting" view definitions to propagate updates from view to source [9], we can directly design new bidirectional programming languages in which every expression defines a pair of functions mapping updates on one structure to updates on the other. Such structures are often called lenses [18]. The foundational theory of lenses has been studied extensively [20, 47, 26, 32, 48, 40, 15, 31, 46, 41, 21, 27], and lens-based language designs have been developed in several domains, including strings [5, 19, 3, 36], trees [18, 28, 39, 35, 29], relations [6], graphs [23], and software models [43, 50, 44, 12, 13, 14, 24, 25, 8]. These languages share some common elements with modern functional languages---in particular, they come with very expressive type systems. In other respects, they are rather novel and surprising. This tutorial surveys recent developments in the theory of lenses and the practice of bidirectional programming languages.

#index 1770123
#* The power of the dinur-nissim algorithm: breaking privacy of statistical and graph databases
#@ Krzysztof Choromanski;Tal Malkin
#t 2012
#c 5
#% 300184
#% 576110
#% 809244
#% 893101
#% 963242
#% 1106392
#% 1193149
#% 1214684
#% 1357695
#% 1419417
#% 1426322
#% 1426323
#% 1426329
#% 1464628
#% 1484065
#% 1484081
#% 1484155
#% 1661427
#% 1670071
#% 1732708
#% 1740518
#! A few years ago, Dinur and Nissim (PODS, 2003) proposed an algorithm for breaking database privacy when statistical queries are answered with a perturbation error of magnitude o(√n) for a database of size n. This negative result is very strong in the sense that it completely reconstructs Ω(n) data bits with an algorithm that is simple, uses random queries, and does not put any restriction on the perturbation other than its magnitude. Their algorithm works for a model where the database consists of bits, and the statistical queries asked by the adversary are sum queries for a subset of locations. In this paper we extend the attack to work for much more general settings in terms of the type of statistical query allowed, the database domain, and the general tradeoff between perturbation and privacy. Specifically, we prove: For queries of the type ∑in=1 φixi; where φ_{i} are i.i.d. and with a finite third moment and positive variance (this includes as a special case the sum queries of Dinur-Nissim and several subsequent extensions), we prove that the quadratic relation between the perturbation and what the adversary can reconstruct holds even for smaller perturbations, and even for a larger data domain. If φi is Gaussian, Poissonian, or bounded and of positive variance, this holds for arbitrary data domains and perturbation; for other φi this holds as long as the domain is not too large and the perturbation is not too small. A positive result showing that for a sum query the negative result mentioned above is tight. Specifically, we build a distribution on bit databases and an answering algorithm such that any adversary who wants to recover a little more than the negative result above allows, will not succeed except with negligible probability. We consider a richer class of summation queries, focusing on databases representing graphs, where each entry is an edge, and the query is a structural function of a subgraph. We show an attack that recovers a big portion of the graph edges, as long as the graph and the function satisfy certain properties. The attacking algorithms in both our negative results are straight-forward extensions of the Dinur-Nissim attack, based on asking φ-weighted queries or queries choosing a subgraph uniformly at random. The novelty of our work is in the analysis, showing that this simple attack is much more powerful than was previously known, as well as pointing to possible limits of this approach and putting forth new application domains such as graph problems (which may occur in social networks, Internet graphs, etc). These results may find applications not only for breaking privacy, but also in the positive direction, for recovering complicated structure information using inaccurate estimates about its substructures.

#index 1770124
#* A rigorous and customizable framework for privacy
#@ Daniel Kifer;Ashwin Machanavajjhala
#t 2012
#c 5
#% 67453
#% 329858
#% 443463
#% 740764
#% 742048
#% 844340
#% 864406
#% 881507
#% 882473
#% 893100
#% 963241
#% 1040082
#% 1053876
#% 1061644
#% 1083653
#% 1206678
#% 1217125
#% 1217148
#% 1266524
#% 1287870
#% 1292636
#% 1340046
#% 1381029
#% 1401185
#% 1426456
#% 1580183
#% 1581862
#% 1670071
#% 1727969
#% 1740518
#! In this paper we introduce a new and general privacy framework called Pufferfish. The Pufferfish framework can be used to create new privacy definitions that are customized to the needs of a given application. The goal of Pufferfish is to allow experts in an application domain, who frequently do not have expertise in privacy, to develop rigorous privacy definitions for their data sharing needs. In addition to this, the Pufferfish framework can also be used to study existing privacy definitions. We illustrate the benefits with several applications of this privacy framework: we use it to formalize and prove the statement that differential privacy assumes independence between records, we use it to define and study the notion of composition in a broader context than before, we show how to apply it to protect unbounded continuous attributes and aggregate information, and we show how to use it to rigorously account for prior data releases.

#index 1770125
#* Static analysis and optimization of semantic web queries
#@ Andrés Letelier;Jorge Pérez;Reinhard Pichler;Sebastian Skritek
#t 2012
#c 5
#% 183738
#% 303886
#% 331899
#% 427161
#% 464717
#% 599549
#% 756494
#% 824688
#% 893149
#% 917695
#% 956573
#% 993437
#% 1022236
#% 1055731
#% 1127431
#% 1127610
#% 1152438
#% 1152440
#% 1206875
#% 1223424
#% 1366460
#% 1424588
#% 1426472
#% 1552657
#% 1581837
#% 1589318
#% 1655424
#% 1675275
#% 1696286
#% 1914832
#! Static analysis is a fundamental task in query optimization. In this paper we study static analysis and optimization techniques for SPARQL, which is the standard language for querying Semantic Web data. Of particular interest for us is the optionality feature in SPARQL. It is crucial in Semantic Web data management, where data sources are inherently incomplete and the user is usually interested in partial answers to queries. This feature is one of the most complicated constructors in SPARQL and also the one that makes this language depart from classical query languages such as relational conjunctive queries. We focus on the class of well-designed SPARQL queries, which has been proposed in the literature as a fragment of the language with good properties regarding query evaluation. We first propose a tree representation for SPARQL queries, called pattern trees, which captures the class of well-designed SPARQL graph patterns and which can be considered as a query execution plan. Among other results, we propose several transformation rules for pattern trees, a simple normal form, and study equivalence and containment. We also study the enumeration and counting problems for this class of queries.

#index 1770126
#* The complexity of evaluating path expressions in SPARQL
#@ Katja Losemann;Wim Martens
#t 2012
#c 5
#% 32904
#% 139556
#% 197751
#% 210214
#% 248025
#% 268782
#% 268797
#% 281764
#% 292677
#% 299967
#% 404772
#% 555093
#% 562454
#% 571038
#% 836006
#% 957534
#% 1010654
#% 1180011
#% 1201360
#% 1223424
#% 1224938
#% 1299109
#% 1370257
#% 1424588
#% 1497253
#% 1504036
#% 1581837
#% 1746861
#% 1818411
#% 1872906
#! The World Wide Web Consortium (W3C) recently introduced property paths in SPARQL 1.1, a query language for RDF data. Property paths allow SPARQL queries to evaluate regular expressions over graph data. However, they differ from standard regular expressions in several notable aspects. For example, they have a limited form of negation, they have numerical occurrence indicators as syntactic sugar, and their semantics on graphs is defined in a non-standard manner. We formalize the W3C semantics of property paths and investigate various query evaluation problems on graphs. More specifically, let x and y be two nodes in an edge-labeled graph and r be an expression. We study the complexities of (1) deciding whether there exists a path from x to y that matches r and (2) counting how many paths from x to y match r. Our main results show that, compared to an alternative semantics of regular expressions on graphs, the complexity of (1) and (2) under W3C semantics is significantly higher. Whereas the alternative semantics remains in polynomial time for large fragments of expressions, the W3C semantics makes problems (1) and (2) intractable almost immediately. As a side-result, we prove that the membership problem for regular expressions with numerical occurrence indicators and negation is in polynomial time.

#index 1770127
#* Space-efficient range reporting for categorical data
#@ Yakov Nekrich
#t 2012
#c 5
#% 37861
#% 41684
#% 56081
#% 164362
#% 189846
#% 205818
#% 273714
#% 281731
#% 319601
#% 379448
#% 443130
#% 493690
#% 548646
#% 571296
#% 604653
#% 866705
#% 1407149
#% 1529934
#% 1668216
#% 1668251
#% 1701405
#% 1747483
#! In the colored (or categorical) range reporting problem the set of input points is partitioned into categories and stored in a data structure; a query asks for categories of points that belong to the query range. In this paper we study two-dimensional colored range reporting in the external memory model and present I/O-efficient data structures for this problem. In particular, we describe data structures that answer three-sided colored reporting queries in O(K/B) I/Os and two-dimensional colored reporting queries in(log2logB N + K/B) I/Os when points lie on an N x N grid, K is the number of reported colors, and B is the block size. The space usage of both data structures is close to optimal.

#index 1770128
#* Dynamic top-k range reporting in external memory
#@ Cheng Sheng;Yufei Tao
#t 2012
#c 5
#% 41684
#% 158408
#% 273714
#% 563629
#% 656697
#% 726629
#% 1013634
#% 1336220
#% 1701404
#! In the top-K range reporting problem, the dataset contains N points in the real domain ℜ, each of which is associated with a real-valued score. Given an interval x1,x2 in ℜ and an integer K≤ N, a query returns the K points in x1,x2 having the smallest scores. We want to store the dataset in a structure so that queries can be answered efficiently. In the external memory model, the state of the art is a static structure that consumes O(N/B) space, answers a query in O(logB N + K/B) time, and can be constructed in O(N + (N log N / B) log M/B (N/B)) time, where B is the size of a disk block, and M the size of memory. We present a fully-dynamic structure that retains the same space and query bounds, and can be updated in O(log2B N) amortized time per insertion and deletion. Our structure can be constructed in O((N/B) log M/B (N/B)) time.

#index 1770129
#* Indexability of 2D range search revisited: constant redundancy and weak indivisibility
#@ Yufei Tao
#t 2012
#c 5
#% 11274
#% 41684
#% 86950
#% 237204
#% 248015
#% 248016
#% 273714
#% 281731
#% 287479
#% 321455
#% 344424
#% 370597
#% 411694
#% 427199
#% 464886
#% 723366
#% 1426295
#% 1537413
#% 1668215
#! In the 2D orthogonal range search problem, we want to preprocess a set of 2D points so that, given any axis-parallel query rectangle, we can report all the data points in the rectangle efficiently. This paper presents a lower bound on the query time that can be achieved by any external memory structure that stores a point at most r times, where r is a constant integer. Previous research has resolved the bound at two extremes: r = 1, and r being arbitrarily large. We, on the other hand, derive the explicit tradeoff at every specific r. A premise that lingers in existing studies is the so-called indivisibility assumption: all the information bits of a point are treated as an atom, i.e., they are always stored together in the same block. We partially remove this assumption by allowing a data structure to freely divide a point into individual bits stored in different blocks. The only assumption is that, those bits must be retrieved for reporting, as opposed to being computed -- we refer to this requirement as the weak indivisibility assumption. We also describe structures to show that our lower bound is tight up to only a small factor.

#index 1770130
#* Approximate computation and implicit regularization for very large-scale data analysis
#@ Michael W. Mahoney
#t 2012
#c 5
#% 73831
#% 265086
#% 270134
#% 296756
#% 341672
#% 577329
#% 577991
#% 765261
#% 869485
#% 898311
#% 963669
#% 1039649
#% 1055741
#% 1063716
#% 1082173
#% 1134501
#% 1232034
#% 1328066
#% 1399996
#% 1531275
#% 1581927
#% 1724716
#! Database theory and database practice are typically the domain of computer scientists who adopt what may be termed an algorithmic perspective on their data. This perspective is very different than the more statistical perspective adopted by statisticians, scientific computers, machine learners, and other who work on what may be broadly termed statistical data analysis. In this article, I will address fundamental aspects of this algorithmic-statistical disconnect, with an eye to bridging the gap between these two very different approaches. A concept that lies at the heart of this disconnect is that of statistical regularization, a notion that has to do with how robust is the output of an algorithm to the noise properties of the input data. Although it is nearly completely absent from computer science, which historically has taken the input data as given and modeled algorithms discretely, regularization in one form or another is central to nearly every application domain that applies algorithms to noisy data. By using several case studies, I will illustrate, both theoretically and empirically, the nonobvious fact that approximate computation, in and of itself, can implicitly lead to statistical regularization. This and other recent work suggests that, by exploiting in a more principled way the statistical properties implicit in worst-case algorithms, one can in many cases satisfy the bicriteria of having algorithms that are scalable to very large-scale databases and that also have good inferential or predictive properties.

#index 1770131
#* Max-Sum diversification, monotone submodular functions and dynamic updates
#@ Allan Borodin;Hyun Chul Lee;Yuli Ye
#t 2012
#c 5
#% 42369
#% 262112
#% 281656
#% 332979
#% 564425
#% 642975
#% 729923
#% 879618
#% 1073970
#% 1074025
#% 1081290
#% 1166473
#% 1181244
#% 1190093
#% 1328135
#% 1400011
#% 1450870
#% 1470696
#% 1472964
#% 1489397
#% 1536529
#% 1536552
#% 1591980
#% 1594636
#% 1598392
#% 1598393
#% 1845364
#% 1872965
#! Result diversification has many important applications in databases, operations research, information retrieval, and finance. In this paper, we study and extend a particular version of result diversification, known as max-sum diversification. More specifically, we consider the setting where we are given a set of elements in a metric space and a set valuation function f defined on every subset. For any given subset S, the overall objective is a linear combination of f(S) and the sum of the distances induced by S. The goal is to find a subset S satisfying some constraints that maximizes the overall objective. This problem is first studied by Gollapudi and Sharma in [17] for modular set functions and for sets satisfying a cardinality constraint (uniform matroids). In their paper, they give a 2-approximation algorithm by reducing to an earlier result in [20]. The first part of this paper considers an extension of the modular case to the monotone submodular case, for which the algorithm in [17] no longer applies. Interestingly, we are able to maintain the same 2-approximation using a natural, but different greedy algorithm. We then further extend the problem by considering any matroid constraint and show that a natural single swap local search algorithm provides a 2-approximation in this more general setting. This extends the Nemhauser, Wolsey and Fisher approximation result [20] for the problem of submodular function maximization subject to a matroid constraint (without the distance function component). The second part of the paper focuses on dynamic updates for the modular case. Suppose we have a good initial approximate solution and then there is a single weight-perturbation either on the valuation of an element or on the distance between two elements. Given that users expect some stability in the results they see, we ask how easy is it to maintain a good approximation without significantly changing the initial set. We measure this by the number of updates, where each update is a swap of a single element in the current solution with a single element outside the current solution. We show that we can maintain an approximation ratio of 3 by just a single update if the perturbation is not too large.

#index 1770132
#* Query-based data pricing
#@ Paraschos Koutris;Prasang Upadhyaya;Magdalena Balazinska;Bill Howe;Dan Suciu
#t 2012
#c 5
#% 248038
#% 341704
#% 353190
#% 378410
#% 384978
#% 571217
#% 630967
#% 737720
#% 778122
#% 809238
#% 976986
#% 1206951
#% 1426418
#% 1478165
#% 1488677
#% 1661430
#% 1700135
#% 1914701
#! Data is increasingly being bought and sold online, and Web-based marketplace services have emerged to facilitate these activities. However, current mechanisms for pricing data are very simple: buyers can choose only from a set of explicit views, each with a specific price. In this paper, we propose a framework for pricing data on the Internet that, given the price of a few views, allows the price of any query to be derived automatically. We call this capability "query-based pricing." We first identify two important properties that the pricing function must satisfy, called arbitrage-free and discount-free. Then, we prove that there exists a unique function that satisfies these properties and extends the seller's explicit prices to all queries. When both the views and the query are Unions of Conjunctive Queries, the complexity of computing the price is high. To ensure tractability, we restrict the explicit prices to be defined only on selection views (which is the common practice today). We give an algorithm with polynomial time data complexity for computing the price of any chain query by reducing the problem to network flow. Furthermore, we completely characterize the class of Conjunctive Queries without self-joins that have PTIME data complexity (this class is slightly larger than chain queries), and prove that pricing all other queries is NP-complete, thus establishing a dichotomy on the complexity of the pricing problem when all views are selection queries.

#index 1770133
#* Local transformations and conjunctive-query equivalence
#@ Ronald Fagin;Phokion G. Kolaitis
#t 2012
#c 5
#% 129217
#% 188350
#% 287316
#% 287339
#% 378409
#% 599549
#% 801691
#% 809239
#% 826032
#% 850730
#% 997492
#% 1015302
#% 1063712
#% 1065944
#% 1270567
#% 1328194
#% 1424604
#% 1538778
#% 1541335
#! Over the past several decades, the study of conjunctive queries has occupied a central place in the theory and practice of database systems. In recent years, conjunctive queries have played a prominent role in the design and use of schema mappings for data integration and data exchange tasks. In this paper, we investigate several different aspects of conjunctive-query equivalence in the context of schema mappings and data exchange. In the first part of the paper, we introduce and study a notion of a local transformation between database instances that is based on conjunctive-query equivalence. We show that the chase procedure for GLAV mappings (that is, schema mappings specified by source-to-target tuple-generating dependencies) is a local transformation with respect to conjunctive-query equivalence. This means that the chase procedure preserves bounded conjunctive-query equivalence, that is, if two source instances are indistinguishable using conjunctive queries of a sufficiently large size, then the target instances obtained by chasing these two source instances are also indistinguishable using conjunctive queries of a given size. Moreover, we obtain polynomial bounds on the level of indistinguishability between source instances needed to guarantee indistinguishability between the target instances produced by the chase. The locality of the chase extends to schema mappings specified by a second-order tuple-generating dependency (SO tgd), but does not hold for schema mappings whose specification includes target constraints. In the second part of the paper, we take a closer look at the composition of two GLAV mappings. In particular, we break GLAV mappings into a small number of well-studied classes (including LAV and GAV), and complete the picture as to when the composition of schema mappings from these various classes can be guaranteed to be a GLAV mapping, and when they can be guaranteed to be conjunctive-query equivalent to a GLAV mapping. We also show that the following problem is decidable: given a schema mapping specified by an SO tgd and a GLAV mapping, are they conjunctive-query equivalent? In contrast, the following problem is known to be undecidable: given a schema mapping specified by an SO tgd and a GLAV mapping, are they logically equivalent?

#index 1770134
#* A dichotomy in the complexity of deletion propagation with functional dependencies
#@ Benny Kimelfeld
#t 2012
#c 5
#% 583
#% 664
#% 286901
#% 287000
#% 291869
#% 378401
#% 416007
#% 907551
#% 992830
#% 1426461
#% 1488677
#% 1562962
#% 1581834
#% 1692263
#! A classical variant of the view-update problem is deletion propagation, where tuples from the database are deleted in order to realize a desired deletion of a tuple from the view. This operation may cause a (sometimes necessary) side effect---deletion of additional tuples from the view, besides the intentionally deleted one. The goal is to propagate deletion so as to maximize the number of tuples that remain in the view. In this paper, a view is defined by a self-join-free conjunctive query (sjf-CQ) over a schema with functional dependencies. A condition is formulated on the schema and view definition at hand, and the following dichotomy in complexity is established. If the condition is met, then deletion propagation is solvable in polynomial time by an extremely simple algorithm (very similar to the one observed by Buneman et al.). If the condition is violated, then the problem is NP-hard, and it is even hard to realize an approximation ratio that is better than some constant; moreover, deciding whether there is a side-effect-free solution is NP-complete. This result generalizes a recent result by Kimelfeld et al., who ignore functional dependencies. For the class of sjf-CQs, it also generalizes a result by Cong et al., stating that deletion propagation is in polynomial time if keys are preserved by the view.

#index 1770135
#* The wavelet trie: maintaining an indexed sequence of strings in compressed space
#@ Roberto Grossi;Giuseppe Ottaviano
#t 2012
#c 5
#% 158790
#% 238060
#% 288578
#% 370597
#% 453572
#% 846094
#% 919830
#% 996633
#% 1055308
#% 1063727
#% 1080906
#% 1151107
#% 1153123
#% 1210023
#% 1264407
#% 1264695
#% 1723778
#% 1740659
#% 1812718
#! An indexed sequence of strings is a data structure for storing a string sequence that supports random access, searching, range counting and analytics operations, both for exact matches and prefix search. String sequences lie at the core of column-oriented databases, log processing, and other storage and query tasks. In these applications each string can appear several times and the order of the strings in the sequence is relevant. The prefix structure of the strings is relevant as well: common prefixes are sought in strings to extract interesting features from the sequence. Moreover, space-efficiency is highly desirable as it translates directly into higher performance, since more data can fit in fast memory. We introduce and study the problem of compressed indexed sequence of strings, representing indexed sequences of strings in nearly-optimal compressed space, both in the static and dynamic settings, while preserving provably good performance for the supported operations. We present a new data structure for this problem, the Wavelet Trie, which combines the classical Patricia Trie with the Wavelet Tree, a succinct data structure for storing a compressed sequence. The resulting Wavelet Trie smoothly adapts to a sequence of strings that changes over time. It improves on the state-of-the-art compressed data structures by supporting a dynamic alphabet (i.e. the set of distinct strings) and prefix queries, both crucial requirements in the aforementioned applications, and on traditional indexes by reducing space occupancy to close to the entropy of the sequence.

#index 1770136
#* On the optimality of clustering properties of space filling curves
#@ Pan Xu;Srikanta Tirthapura
#t 2012
#c 5
#% 13032
#% 45766
#% 227706
#% 235402
#% 415957
#% 443397
#! Space filling curves have for long been used in the design of data structures for multidimensional data. A fundamental quality metric of a space filling curve is its "clustering number" with respect to a class of queries, which is the average number of contiguous segments on the space filling curve that a query region can be partitioned into. We present a characterization of the clustering number of a general class of space filling curves, as well as the first non-trivial lower bounds on the clustering number for any space filling curve. Our results also answer an open problem that was posed by Jagadish in 1997.

#index 1770137
#* Nearest-neighbor searching under uncertainty
#@ Pankaj K. Agarwal;Alon Efrat;Swaminathan Sankararaman;Wuzhou Zhang
#t 2012
#c 5
#% 3623
#% 145154
#% 211801
#% 235114
#% 427199
#% 579203
#% 745464
#% 813973
#% 836178
#% 957391
#% 1023422
#% 1063520
#% 1081581
#% 1127377
#% 1179162
#% 1181270
#% 1181287
#% 1200291
#% 1200827
#% 1206716
#% 1217128
#% 1264692
#% 1314442
#% 1408794
#% 1415670
#% 1442072
#% 1446816
#% 1523904
#% 1555743
#% 1581930
#% 1614964
#% 1616585
#% 1633081
#! Nearest-neighbor queries, which ask for returning the nearest neighbor of a query point in a set of points, are important and widely studied in many fields because of a wide range of applications. In many of these applications, such as sensor databases, location based services, face recognition, and mobile data, the location of data is imprecise. We therefore study nearest neighbor queries in a probabilistic framework in which the location of each input point and/or query point is specified as a probability density function and the goal is to return the point that minimizes the expected distance, which we refer to as the expected nearest neighbor (ENN). We present methods for computing an exact ENN or an ε-approximate ENN, for a given error parameter 0

#index 1770138
#* Classification of annotation semirings over query containment
#@ Egor V. Kostylev;Juan L. Reutter;András Z. Salamon
#t 2012
#c 5
#% 663
#% 137867
#% 190638
#% 215225
#% 228817
#% 289266
#% 318704
#% 464722
#% 464891
#% 599549
#% 874884
#% 938789
#% 976987
#% 1054485
#% 1063709
#% 1206732
#% 1383775
#% 1592794
#% 1716777
#% 1818425
#% 1818427
#! We study the problem of query containment of (unions of) conjunctive queries over annotated databases. Annotations are typically attached to tuples and represent metadata such as probability, multiplicity, comments, or provenance. It is usually assumed that annotations are drawn from a commutative semiring. Such databases pose new challenges in query optimization, since many related fundamental tasks, such as query containment, have to be reconsidered in the presence of propagation of annotations. We axiomatize several classes of semirings for each of which containment of conjunctive queries is equivalent to existence of a particular type of homomorphism. For each of these types we also specify all semirings for which existence of a corresponding homomorphism is a sufficient (or necessary) condition for the containment. We exploit these techniques to develop new decision procedures for containment of unions of conjunctive queries and axiomatize corresponding classes of semirings. This generalizes previous approaches and allows us to improve known complexity bounds.

#index 1770139
#* Efficient approximations of conjunctive queries
#@ Pablo Barceló;Leonid Libkin;Miguel Romero
#t 2012
#c 5
#% 122392
#% 191611
#% 219474
#% 248038
#% 303886
#% 317646
#% 321058
#% 338450
#% 339937
#% 378409
#% 384978
#% 411380
#% 427161
#% 465064
#% 480810
#% 572311
#% 599549
#% 643572
#% 806215
#% 857282
#% 993437
#% 1198377
#% 1224352
#% 1493601
#% 1523818
#% 1538785
#% 1581823
#! When finding exact answers to a query over a large database is infeasible, it is natural to approximate the query by a more efficient one that comes from a class with good bounds on the complexity of query evaluation. In this paper we study such approximations for conjunctive queries. These queries are of special importance in databases, and we have a very good understanding of the classes that admit fast query evaluation, such as acyclic, or bounded (hyper)treewidth queries. We define approximations of a given query Q as queries from one of those classes that disagree with Q as little as possible. We mostly concentrate on approximations that are guaranteed to return correct answers. We prove that for the above classes of tractable conjunctive queries, approximations always exist, and are at most polynomial in the size of the original query. This follows from general results we establish that relate closure properties of classes of conjunctive queries to the existence of approximations. We also show that in many cases, the size of approximations is bounded by the size of the query they approximate. We establish a number of results showing how combinatorial properties of queries affect properties of their approximations, study bounds on the number of approximations, as well as the complexity of finding and identifying approximations. We also look at approximations that return all correct answers and study their properties.

#index 1770140
#* On the complexity of package recommendation problems
#@ Ting Deng;Wenfei Fan;Floris Geerts
#t 2012
#c 5
#% 66645
#% 118359
#% 183738
#% 246258
#% 384978
#% 462772
#% 480819
#% 522882
#% 598376
#% 643566
#% 782011
#% 813966
#% 824782
#% 893105
#% 917695
#% 949368
#% 1063713
#% 1075132
#% 1127465
#% 1127471
#% 1181290
#% 1214668
#% 1217203
#% 1328172
#% 1476463
#% 1482270
#% 1590539
#% 1620192
#! Recommendation systems aim to recommend items that are likely to be of interest to users. This paper investigates several issues fundamental to such systems. We model recommendation systems for packages of items. We use queries to specify multi-criteria for item selections and express compatibility constraints on items in a package, and use functions to compute the cost and usefulness of items to a user. We study recommendations of points of interest, to suggest top-k packages. We also investigate recommendations of top-k items, as a special case. In addition, when sensible suggestions cannot be found, we propose query relaxation recommendations to help users revise their selection criteria, or adjustment recommendations to guide vendors to modify their item collections. We identify several problems, to decide whether a set of packages makes a top-k recommendation, whether a rating bound is maximum for selecting top-k packages, whether we can relax the selection query to find packages that users want, and whether we can update a bounded number of items such that the users' requirements can be satisfied. We also study function problems for computing top-k packages, and counting problems to find how many packages meet the user's criteria. We establish the upper and lower bounds of these problems, all matching, for combined and data complexity. These results reveal the impact of variable sizes of packages, the presence of compatibility constraints, as well as a variety of query languages for specifying selection criteria and compatibility constraints, on the analyses of these problems.

#index 1770141
#* Space-efficient estimation of statistics over sub-sampled streams
#@ Andrew McGregor;A. Pavan;Srikanta Tirthapura;David Woodruff
#t 2012
#c 5
#% 278835
#% 299989
#% 449086
#% 580702
#% 646233
#% 805744
#% 816392
#% 859116
#% 960257
#% 1092012
#% 1141469
#% 1206923
#% 1207124
#% 1232282
#% 1484158
#! In many stream monitoring situations, the data arrival rate is so high that it is not even possible to observe each element of the stream. The most common solution is to sample a small fraction of the data stream and use the sample to infer properties and estimate aggregates of the original stream. However, the quantities that need to be computed on the sampled stream are often different from the original quantities of interest and their estimation requires new algorithms. We present upper and lower bounds (often matching) for estimating frequency moments, support size, entropy, and heavy hitters of the original stream from the data observed in the sampled stream.

#index 1770142
#* Rectangle-efficient aggregation in spatial data streams
#@ Srikanta Tirthapura;David Woodruff
#t 2012
#c 5
#% 109451
#% 171554
#% 273691
#% 278835
#% 320179
#% 333977
#% 379443
#% 397385
#% 435137
#% 458765
#% 480628
#% 492912
#% 520632
#% 527189
#% 578390
#% 745442
#% 765291
#% 765459
#% 771386
#% 783741
#% 784515
#% 805744
#% 813797
#% 816392
#% 847112
#% 864525
#% 874986
#% 879397
#% 894646
#% 948471
#% 1024400
#% 1054486
#% 1064279
#% 1173980
#% 1426278
#% 1484080
#% 1581825
#% 1657953
#% 1663950
#! We consider the estimation of aggregates over a data stream of multidimensional axis-aligned rectangles. Rectangles are a basic primitive object in spatial databases, and efficient aggregation of rectangles is a fundamental task. The data stream model has emerged as a de facto model for processing massive databases in which the data resides in external memory or the cloud and is streamed through main memory. For a point p, let n(p) denote the sum of the weights of all rectangles in the stream that contain p. We give near-optimal solutions for basic problems, including (1) the k-th frequency moment Fk = ∑ points p|n(p)|k, (2)~the counting version of stabbing queries, which seeks an estimate of n(p) given p, and (3) identification of heavy-hitters, i.e., points p for which n(p) is large. An important special case of Fk is F0, which corresponds to the volume of the union of the rectangles. This is a celebrated problem in computational geometry known as "Klee's measure problem", and our work yields the first solution in the streaming model for dimensions greater than one.

#index 1770143
#* Randomized algorithms for tracking distributed count, frequencies, and ranks
#@ Zengfeng Huang;Ke Yi;Qin Zhang
#t 2012
#c 5
#% 333931
#% 336610
#% 654443
#% 800582
#% 810009
#% 859116
#% 874994
#% 894443
#% 903219
#% 993960
#% 1056443
#% 1068351
#% 1127608
#% 1217131
#% 1232249
#% 1524288
#% 1581908
#% 1651455
#% 1711934
#% 1723621
#% 1770118
#% 1770497
#! We show that randomization can lead to significant improvements for a few fundamental problems in distributed tracking. Our basis is the count-tracking problem, where there are k players, each holding a counter ni that gets incremented over time, and the goal is to track an ∑-approximation of their sum n=∑ini continuously at all times, using minimum communication. While the deterministic communication complexity of the problem is θ(k/ε • log N), where N is the final value of n when the tracking finishes, we show that with randomization, the communication cost can be reduced to θ(√k/ε • log N). Our algorithm is simple and uses only O(1) space at each player, while the lower bound holds even assuming each player has infinite computing power. Then, we extend our techniques to two related distributed tracking problems: frequency-tracking and rank-tracking, and obtain similar improvements over previous deterministic algorithms. Both problems are of central importance in large data monitoring and analysis, and have been extensively studied in the literature.

#index 1770144
#* Continuous distributed counting for non-monotonic streams
#@ Zhenming Liu;Bozidar Radunović;Milan Vojnović
#t 2012
#c 5
#% 160390
#% 333931
#% 654488
#% 801695
#% 810009
#% 824652
#% 864444
#% 891559
#% 894646
#% 1039695
#% 1217131
#% 1232249
#% 1426450
#% 1651455
#% 1876163
#! We consider the continual count tracking problem in a distributed environment where the input is an aggregate stream that originates from k distinct sites and the updates are allowed to be non-monotonic, i.e. both increments and decrements are allowed. The goal is to continually track the count within a prescribed relative accuracy ε at the lowest possible communication cost. Specifically, we consider an adversarial setting where the input values are selected and assigned to sites by an adversary but the order is according to a random permutation or is a random i.i.d process. The input stream of values is allowed to be non-monotonic with an unknown drift -1≤μ=1 where the case μ = 1 corresponds to the special case of a monotonic stream of only non-negative updates. We show that a randomized algorithm guarantees to track the count accurately with high probability and has the expected communication cost Õ(min√k/(|#956;|ε), √k n/ε, n}), for an input stream of length n, and establish matching lower bounds. This improves upon previously best known algorithm whose expected communication cost is Θ(min√k/ε,n]) that applies only to an important but more restrictive class of monotonic input streams, and our results are substantially more positive than the communication complexity of Ω(n) under fully adversarial input. We also show how our framework can also accommodate other types of random input streams, including fractional Brownian motion that has been widely used to model temporal long-range dependencies observed in many natural phenomena. Last but not least, we show how our non-monotonic counter can be applied to track the second frequency moment and to a Bayesian linear regression problem.

#index 1770145
#* Proceedings of the on SIGMOD/PODS 2012 PhD Symposium
#@ Xin Luna Dong;M. Tamer Özsu
#t 2012
#c 5
#! Welcome to the SIGMOD/PODS 2012 PhD Symposium, which is co-located with SIGMOD/PODS 2012 and takes place on Sunday, May 20th, 2012. This is a forum where PhD students have an opportunity of presenting their research ideas, receiving feedback from and interacting with senior members of the community. The focus of the Symposium this year is on mentorship and providing constructive feedback to the students. Many members of the Program Committee attend the Symposium, providing ample opportunity for discussions and feedback. Each student whose submission is accepted for presentation is paired with a member of the PC for mentorship and one-on-one discussions. This year we received 20 papers in response to call for papers. Each of these was reviewed by three members of the PC with a focus on selecting submissions that were ready and could benefit from feedback. In the end, we selected twelve submissions for discussions at the Symposium. The proceedings include short descriptions of these PhD theses.

#index 1770146
#* Getting your acceptance rate to 80%: a checklist for publishing
#@ Eamonn Keogh
#t 2012
#c 5
#! SIGMOD acceptance rates have generally been in the narrow range of between 14 to 18 percent during the past decade. However, for given individuals the range is much wider. Some people have a zero percent acceptance rate, after five or six frustratingly unsuccessful attempts they set their sights lower (or, more pessimistically, they fail to get tenure and stop trying). Many people have acceptance rates that reflect the SIGMOD average of about 20%. Are there people that have perfect acceptance rates? In this talk I argue that while a perfect acceptance rate is essentially impossible to achieve year after year, an 80% acceptance rate is possible for top conferences. I will show how ten simple "tricks" allow you to significantly increase your odds of acceptance. As proof of utility I note that in the last ten years these ideas have allowed me to achieve 80%+ acceptance rates for many competitive conferences, including ICDM (22 papers), SIGKDD (19 papers), SDM (16 papers), VLDB (6) papers etc.

#index 1770147
#* Towards an extensible efficient event processing kernel
#@ Mohammad Sadoghi
#t 2012
#c 5
#% 252304
#% 271199
#% 333938
#% 339055
#% 480296
#% 960342
#% 1022274
#% 1075132
#% 1114714
#% 1127386
#% 1134501
#% 1302863
#% 1426502
#% 1433973
#% 1523792
#% 1523931
#% 1549840
#% 1580599
#% 1581899
#% 1586200
#% 1591782
#% 1591807
#% 1846732
#% 1846797
#! The efficient processing of large collections of patterns (Boolean expressions, XPath queries, or continuous SQL queries) over data streams plays a central role in major data intensive applications ranging from user-centric processing and personalization to real-time data analysis. On the one hand, emerging user-centric applications, including computational advertising and selective information dissemination, demand determining and presenting to an end-user only the most relevant content that is both user-consumable and suitable for limited screen real estate of target (mobile) devices. We achieve these user-centric requirements through novel high-dimensional indexing structures and (parallel) algorithms. On the other hand, applications in real-time data analysis, including computational finance and intrusion detection, demand meeting stringent subsecond processing requirements and providing high-frequency and low-latency event processing over data streams. We achieve real-time data analysis requirements by leveraging reconfigurable hardware -- FPGAs -- to sustain line-rate processing by exploiting unprecedented degrees of parallelism and potential for pipelining, only available through custom-built, application-specific, and low-level logic design. Finally, we conduct a comprehensive evaluation to demonstrate the superiority of our proposed techniques in comparison with state-of-the-art algorithms designed for event processing.

#index 1770148
#* High performance spatial query processing for large scale scientific data
#@ Ablimit Aji;Fusheng Wang
#t 2012
#c 5
#% 86950
#% 210187
#% 1063553
#% 1217159
#% 1218736
#% 1375919
#% 1601116
#! Analyzing and querying large volumes of spatially derived data from scientific experiments has posed major challenges in the past decade. For example, the systematic analysis of imaged pathology specimens result in rich spatially derived information with GIS characteristics at cellular and sub-cellular scales, with nearly a million derived markups and hundred million features per image. This provides critical information for evaluation of experimental results, support of biomedical studies and pathology image based diagnosis. However, the vast amount of spatially oriented morphological information poses major challenges for analytical medical imaging. The major challenges I attack include: i) How can we provide cost effective, scalable spatial query support for medical imaging GIS? ii) How can we provide fast response queries on analytical imaging data to support biomedical research and clinical diagnosis? and iii) How can we provide expressive queries to support spatial queries and spatial pattern discoveries for end users? In my thesis, I work towards developing a MapReduce based framework MIGIS to support expressive, cost effective and high performance spatial queries. The framework includes a real-time spatial query engine RESQUE consisting of a variety of optimized access methods, boundary and density aware spatial data partitioning, a declarative query language interface, a query translator which automates translation of the spatial queries into MapReduce programs and an execution engine which parallelizes and executes queries on Hadoop. Our preliminary experiments demonstrate that MIGIS is a cost effective architecture which achieves high performance spatial query execution. MIGIS is extensible and can be adapted to support similar complex spatial queries for large scale spatial data in other scientific domains.

#index 1770149
#* Holistic indexing: offline, online and adaptive indexing in the same kernel
#@ Eleni Petraki
#t 2012
#c 5
#% 480158
#% 482100
#% 810026
#% 810111
#% 875062
#% 960268
#% 1016220
#% 1016221
#% 1207102
#% 1217169
#% 1372713
#% 1545227
#% 1592316
#% 1730730
#% 1741032
#! Proper physical design is a momentous issue for the performance of modern database systems and applications. Nowadays, a growing amount of applications require the execution of dynamic and exploratory workloads with unpredictable characteristics that change over time, e.g., social networks, scientific databases and multimedia databases. In addition, as most modern applications move to the big data era, investing time and resources in building the wrong set of indexes over large collections of data can severely affect performance. Offline, online and adaptive indexing are three distinct approaches to the problem of automating the physical design choices. Offline indexing is best in static environments with stable workloads. Online indexing is best in relatively dynamic environments where the query workload can be monitored. Adaptive indexing is best in fully dynamic environments where no idle time or workload knowledge may be assumed. We observe that these three approaches are complementary, while none of them can satisfy the needs of modern applications in isolation. We envision a new index selection approach, holistic indexing that excels its predecessors by combining the best features of offline, online and adaptive indexing while overcoming their weaknesses. The main goal is the creation of a database kernel that can autonomously create partial indexes which are continuously refined during query processing as in adaptive indexing but at the same time the system continuously detects any opportunity to improve the physical design offline; whenever any idle time occurs it tries to exploit knowledge gathered during query processing to refine existing indexes further or create new ones. We sketch the research space and the new challenges such a direction brings.

#index 1770150
#* Data quality and integration in collaborative environments
#@ Gregor Endler
#t 2012
#c 5
#% 22948
#% 85086
#% 189781
#% 223781
#% 231963
#% 242234
#% 489819
#% 572314
#% 576214
#% 845350
#% 903332
#% 1063533
#% 1063534
#! The trend to merge medical practices into cooperatively operating networks and organizational units like Medical Supply Centers generates new challenges for an adequate IT support. In particular, new use cases for common economic planning, controlling and treatment coordination arise. This requires consolidation of data originating from heterogeneous and autonomous software systems. Heterogeneity and autonomy are core reasons for low data quality. The intuitive approach of initially integrating heterogeneous systems into a federated system creates a very high upfront effort before the system can become operable and does not adequately consider the fact that data quality requirements might change over time. To remedy this, we propose an approach for continuous data quality improvement which enables a demand driven step by step system integration. By adapting the generic Total Data Quality Management process to healthcare specific use cases, we are developing an extended model for continuous data quality management in cooperative healthcare settings. The IT tools which are needed to provide the information that drives this process are currently in development within a government supported project involving both industry and academia.

#index 1770151
#* Clustering techniques for open relation extraction
#@ Filipe Mesquita
#t 2012
#c 5
#% 118771
#% 301241
#% 458630
#% 722926
#% 938705
#% 938706
#% 939384
#% 939944
#% 1077150
#% 1190065
#% 1291356
#% 1330550
#% 1451013
#% 1711865
#! This work investigates clustering techniques for Relation Extraction (RE). Relation Extraction is the task of extracting relationships among named entities (e.g., people, organizations and geo-political entities) from natural language text. We are particularly interested in the open RE scenario, where the number of target relations is too large or even unknown. Our contributions are in two aspects of the clustering process: (1) extraction and weighting of features and (2) scalability. In order to evaluate our techniques in large scale, we propose an automatic evaluation method based on pointwise mutual information. Our preliminary results show that our clustering techniques as well as our evaluation method are promising.

#index 1770152
#* RecDB: towards DBMS support for online recommender systems
#@ Mohamed Sarwat
#t 2012
#c 5
#% 173879
#% 220706
#% 220711
#% 268785
#% 330687
#% 342687
#% 428272
#% 452563
#% 767656
#% 813966
#% 956521
#% 1217203
#% 1625360
#% 1650569
#% 1770385
#% 1798382
#! Recommender systems have become popular in both commercial and academic settings. The main purpose of recommender systems is to suggest to users useful and interesting items or content (data) from a considerably large set of items. Traditional recommender systems do not take into account system issues (i.e., scalability and query efficiency). In an age of staggering web use growth and everpopular social media applications (e.g., Facebook, Google Reader), users are expressing their opinions over a diverse set of data (e.g., news stories, Facebook posts, retail purchases) faster than ever. In this paper, we propose RecDB; a fully fledged database system that provides online recommendation to users. We implement RecDB using existing open source database system Apache Derby, and we use showcase the effectiveness of RecDB by adopting inside Sindbad; a Location-Based Social Networking system developed at University of Minnesota.

#index 1770153
#* Foundations of regular expressions in XML schema languages and SPARQL
#@ Katja Losemann
#t 2012
#c 5
#% 262724
#% 335151
#% 462235
#% 894435
#% 916831
#% 949370
#% 1019798
#% 1021195
#% 1022285
#% 1055754
#% 1105410
#% 1113937
#% 1127694
#% 1129529
#% 1201360
#% 1217202
#% 1223424
#% 1299109
#% 1370257
#% 1373479
#% 1424588
#% 1426464
#% 1426465
#% 1428486
#% 1497253
#% 1504036
#% 1615661
#% 1746861
#% 1770126
#% 1872906
#! Regular expressions can be found in a wide array of technology for data processing on the web. We are motivated by two such technologies: schema languages for XML and query languages for graph-structured or linked data. Our focus is on theoretical aspects of regular expressions in these contexts.

#index 1770154
#* Foundational aspects of semantic web optimization
#@ Sebastian Skritek
#t 2012
#c 5
#% 956573
#% 1022236
#% 1127431
#% 1127610
#% 1152349
#% 1152438
#% 1152440
#% 1223424
#% 1310057
#% 1366460
#% 1424588
#% 1500887
#% 1538791
#% 1540304
#% 1552657
#% 1581837
#% 1655424
#% 1702404
#% 1770125
#! The goal of the semantic web is to make the information available on the web easier accessible. Its idea is to provide machine readable meta-data to enable the development of tools that support users in finding the relevant data. The goal of the thesis is to shed some light onto different foundational aspects of optimization tasks occurring in the field of the Semantic Web. Examples include the redundancy elimination in RDF data or static query analysis of (well-designed) SPARQL queries. Towards this goal, we already contributed several results.

#index 1770155
#* Linking records in dynamic world
#@ Pei Li
#t 2012
#c 5
#% 420072
#% 819550
#% 875066
#% 913783
#% 993980
#% 1022659
#% 1053052
#% 1194593
#% 1201863
#% 1328216
#% 1480787
#% 1523834
#! In real-world, entities change dynamically and the changes are capture in two dimensions: time and space. For data sets that contain temporal records, where each record is associated with a time stamp and describes some aspects of a real-world entity at that particular time, we often wish to identify records that describe the same entity over time and so be able to enable interesting longitudinal data analysis. For data sets that contain geographically referenced data describing real-world entities at different locations (i.e., location entities), we wish to link those entities that belong to the same organization or network. However, existing record linkage techniques ignore additional evidence in temporal and spatial data and can fall short for these cases. This proposal studies linking temporal and spatial records. For temporal record linkage, we apply time decay to capture the effect of elapsed time on entity value evolution, and propose clustering methods that consider time order of records in clustering. For linking location records, we distinguish between strong and weak evidence; for the former, we study core generation in presence of erroneous data, and then leverage the discovered strong evidence to make remaining decisions.

#index 1770156
#* An adaptive event stream processing environment
#@ Samujjwal Bhandari
#t 2012
#c 5
#% 177755
#% 481290
#% 850430
#% 878299
#% 1063523
#% 1070843
#% 1269318
#% 1281501
#% 1396542
#% 1444427
#% 1591778
#% 1591821
#! With the increasing application of Event Stream Processing (ESP) for event pattern detection, it has become important to enhance the extant ESP capabilities to deal with applications having dynamic behavior. This dissertation research explores the limitations of current ESP systems due to fixed pattern detection mechanism and discusses the motivational ideas that demand enhancements in ESP. We propose a solution called adaptive ESP that explores, learns, and updates evolving patterns in dynamic applications. Development of adaptive ESP requires several research issues to be addressed: such as handling input data streams, enhancing event languages with probabilistic information, using machine learning algorithms, and processing feedback from experts. We discuss these issues with the proposed architecture for the system and explore research issues and some of the initial work for developing adaptive ESP.

#index 1770157
#* Dynamic management of resources and workloads for RDBMS in cloud: a control-theoretic approach
#@ Pengcheng Xiong
#t 2012
#c 5
#% 754085
#% 805473
#% 843801
#% 864447
#% 1063541
#% 1063561
#% 1130825
#% 1183344
#% 1206984
#% 1328213
#% 1594596
#% 1621139
#! As cloud computing environments become explosively popular, dealing with unpredictable changes, uncertainties, and disturbances in both systems and environments turns out to be one of the major challenges facing the concurrent computing industry. My research goal is to dynamically manage resources and workloads for RDBMS in cloud computing environments in order to achieve ``better performance but lower cost", i.e., better service level compliance but lower consumption of virtualized computing resource(s). Nowadays, although control theory offers a principled way to deal with the challenge based on feedback mechanisms, a controller is typically designed based on the system designer's domain knowledge and intuition instead of the behavior of the system being controlled. My research approach is based on the essence of control theory but transcends state-of-the-art control-theoretic approaches by leveraging interdisciplinary areas, especially from machine learning. While machine learning is often viewed merely as a toolbox that can be deployed for many data-centric problems, my research makes efforts to incorporate machine learning as a full-fledged engineering discipline into control-theoretic approaches for realizing my research goal. My PhD thesis work implements two solid systems by leveraging machine learning techniques, namely, ActiveSLA and SmartSLA. ActiveSLA is an automatic controller featuring risk assessment admission control to obtain the most profitable service-level compliance. SmartSLA is an automatic controller featuring cost-sensitive adaptation to achieve the lowest total cost. The experimental results show that both of the two systems outperform the state-of-the-art methods.

#index 1770158
#* Efficient optimization and processing for distributed monitoring and control applications
#@ Mengmeng Liu
#t 2012
#c 5
#% 152928
#% 300167
#% 322880
#% 411554
#% 480602
#% 565457
#% 654482
#% 765434
#% 765435
#% 824715
#% 878299
#% 1015324
#% 1016169
#% 1016208
#% 1016269
#% 1022258
#% 1026962
#% 1026989
#% 1083759
#% 1127442
#% 1206880
#% 1217261
#% 1464044
#% 1523908
#! In recent years, we have seen an increasing number of applications in networking, sensor networks, cloud computing, and environmental monitoring, that aim to monitor, control, and make decisions over large volumes of dynamic data. In my dissertation, we aim to enable a generic framework for these distributed monitoring and control applications, and address the limitations of prior work such as data stream management systems and adaptive query processing systems. In particular, we make the following contributions: 1) supporting the maintenance of recursive queries over distributed data streams, 2) enabling full-fledged cost-based incremental query re-optimization, and 3) as ongoing work, incorporating the cost estimation of plan switching during query re-optimization. Our solutions are implemented and evaluated using our prototype system Aspen, over a variety of workloads and benchmarks. In addition, our prototype system Aspen enables an end-to-end framework to support control and decision-making over integrated data streams from both the physical world (e.g., sensor streams) and the digital world (e.g., web, streams, databases).

#index 1770318
#* Proceedings of the 2012 ACM SIGMOD International Conference on Management of Data
#@ K. Selçuk Candan;Yi Chen;Richard Snodgrass;Luis Gravano;Ariel Fuxman
#t 2012
#c 5
#! We are delighted to welcome you to SIGMOD 2012, the 2012 edition of the ACM SIGMOD International Conference on Management of Data, in Scottsdale, Arizona, in the Southwest of the United States. Scottsdale is in the heart of the Sonoran Desert and offers stunning desert vistas and a breathtaking setting for the conference. At the same time, Scottsdale is adjacent to Phoenix, one of the largest and fastest-growing cities in the United States. SIGMOD 2012 hosts an exciting technical program, with two keynote talks, by Pat Hanrahan (Stanford University and Tableau Software) and Amin Vahdat (University of California, San Diego and Google); a plenary session with "Perspectives on Big Data," by Donald Kossmann (ETHZ), Kristen LeFevre (Google Research and University of Michigan), Sam Madden (MIT), and Anand Rajaraman (@WalmartLabs); 48 research paper presentations; six tutorials; 30 demonstrations; and 18 industrial presentations. In addition to having full 30-minute presentation slots, research papers are included in one of two Research Plenary Poster Sessions. One of these sessions is jointly for PODS and SIGMOD research papers, to deepen the ties between the two conferences. Another new plenary poster session, for papers from the 11 workshops co-located with SIGMOD 2012, is an effort to strengthen the link and synergy between the workshops and the conference. SIGMOD 2012 includes several technical and social events designed specifically for student attendees. The SIGMOD/PODS 2012 Ph.D. Symposium, the Database Mentoring Workshop, the Undergraduate Research Poster Competition, and the New Researcher Symposium are all established components of the SIGMOD program and are all part of SIGMOD 2012. The conference also hosts a session dedicated to highlighting the finalists of the SIGMOD Programming Contest. (This year's task is to implement a multidimensional, high-throughput, in-memory indexing system.) In addition, the conference includes a new Information Session on Careers in Industry, aimed at bringing student attendees together with our Gold, Platinum, and Diamond sponsors, as well as "vis-à-vis" meetings aimed at helping Ph.D. students meet internationally recognized researchers in their research areas, to exchange ideas and receive guidance in a relaxed social setting.

#index 1770319
#* Calvin: fast distributed transactions for partitioned database systems
#@ Alexander Thomson;Thaddeus Diamond;Shu-Chun Weng;Kun Ren;Philip Shao;Daniel J. Abadi
#t 2012
#c 5
#% 4619
#% 251359
#% 397295
#% 978404
#% 998845
#% 1022298
#% 1127560
#% 1237169
#% 1328131
#% 1426552
#% 1426589
#% 1441622
#% 1468530
#% 1480906
#% 1523801
#% 1538766
#% 1581868
#! Many distributed storage systems achieve high data access throughput via partitioning and replication, each system with its own advantages and tradeoffs. In order to achieve high scalability, however, today's systems generally reduce transactional support, disallowing single transactions from spanning multiple partitions. Calvin is a practical transaction scheduling and data replication layer that uses a deterministic ordering guarantee to significantly reduce the normally prohibitive contention costs associated with distributed transactions. Unlike previous deterministic database system prototypes, Calvin supports disk-based storage, scales near-linearly on a cluster of commodity machines, and has no single point of failure. By replicating transaction inputs rather than effects, Calvin is also able to support multiple consistency levels---including Paxos-based strong consistency across geographically distant replicas---at no cost to transactional throughput.

#index 1770320
#* Advanced partitioning techniques for massively distributed computation
#@ Jingren Zhou;Nicolas Bruno;Wei Lin
#t 2012
#c 5
#% 86929
#% 115661
#% 210169
#% 394617
#% 411554
#% 442918
#% 565457
#% 723279
#% 745443
#% 963669
#% 983467
#% 1015323
#% 1016209
#% 1063553
#% 1127559
#% 1468421
#% 1523824
#! An increasing number of companies rely on distributed data storage and processing over large clusters of commodity machines for critical business decisions. Although plain MapReduce systems provide several benefits, they carry certain limitations that impact developer productivity and optimization opportunities. Higher level programming languages plus conceptual data models have recently emerged to address such limitations. These languages offer a single machine programming abstraction and are able to perform sophisticated query optimization and apply efficient execution strategies. In massively distributed computation, data shuffling is typically the most expensive operation and can lead to serious performance bottlenecks if not done properly. An important optimization opportunity in this environment is that of judicious placement of repartitioning operators and choice of alternative implementations. In this paper we discuss advanced partitioning strategies, their implementation, and how they are integrated in the Microsoft Scope system. We show experimentally that our approach significantly improves performance for a large class of real-world jobs.

#index 1770321
#* SkewTune: mitigating skew in mapreduce applications
#@ YongChul Kwon;Magdalena Balazinska;Bill Howe;Jerome Rolia
#t 2012
#c 5
#% 201883
#% 227883
#% 268079
#% 480761
#% 480966
#% 481775
#% 765470
#% 963669
#% 983467
#% 1127354
#% 1208113
#% 1328058
#% 1328060
#% 1426481
#% 1426486
#% 1468411
#% 1468423
#% 1486236
#% 1523820
#% 1523841
#% 1526991
#% 1532861
#% 1581836
#% 1581866
#% 1581928
#! We present an automatic skew mitigation approach for user-defined MapReduce programs and present SkewTune, a system that implements this approach as a drop-in replacement for an existing MapReduce implementation. There are three key challenges: (a) require no extra input from the user yet work for all MapReduce applications, (b) be completely transparent, and (c) impose minimal overhead if there is no skew. The SkewTune approach addresses these challenges and works as follows: When a node in the cluster becomes idle, SkewTune identifies the task with the greatest expected remaining processing time. The unprocessed input data of this straggling task is then proactively repartitioned in a way that fully utilizes the nodes in the cluster and preserves the ordering of the input data so that the original output can be reconstructed by concatenation. We implement SkewTune as an extension to Hadoop and evaluate its effectiveness using several real applications. The results show that SkewTune can significantly reduce job runtime in the presence of skew and adds little to no overhead in the absence of skew.

#index 1770322
#* Parallel main-memory indexing for moving-object query and update workloads
#@ Darius Šidlauskas;Simonas Šaltenis;Christian S. Jensen
#t 2012
#c 5
#% 5387
#% 70815
#% 227864
#% 273888
#% 286929
#% 333940
#% 403195
#% 421124
#% 479474
#% 480829
#% 481759
#% 526870
#% 729619
#% 765454
#% 800186
#% 990408
#% 1015305
#% 1015320
#% 1016193
#% 1022298
#% 1046510
#% 1127612
#% 1230827
#% 1285685
#% 1298884
#% 1618261
#! We are witnessing a proliferation of Internet-worked, geo-positioned mobile devices such as smartphones and personal navigation devices. Likewise, location-related services that target the users of such devices are proliferating. Consequently, server-side infrastructures are needed that are capable of supporting the location-related query and update workloads generated by very large populations of such moving objects. This paper presents a main-memory indexing technique that aims to support such workloads. The technique, called PGrid, uses a grid structure that is capable of exploiting the parallelism offered by modern processors. Unlike earlier proposals that maintain separate structures for updates and queries, PGrid allows both long-running queries and rapid updates to operate on a single data structure and thus offers up-to-date query results. Because PGrid does not rely on creating snapshots, it avoids the stop-the-world problem that occurs when workload processing is interrupted to perform such snapshotting. Its concurrency control mechanism relies instead on hardware-assisted atomic updates as well as object-level copying, and it treats updates as non-divisible operations rather than as combinations of deletions and insertions; thus, the query semantics guarantee that no objects are missed in query results. Empirical studies demonstrate that PGrid scales near-linearly with the number of hardware threads on four modern multi-core processors. Since both updates and queries are processed on the same current data-store state, PGrid outperforms snapshot-based techniques in terms of both query freshness and CPU cycle-wise efficiency.

#index 1770323
#* Divergent physical design tuning for replicated databases
#@ Mariano P. Consens;Kleoni Ioannidou;Jeff LeFevre;Neoklis Polyzotis
#t 2012
#c 5
#% 248815
#% 340629
#% 397390
#% 482100
#% 726619
#% 778724
#% 1016220
#% 1016221
#% 1016264
#% 1127352
#% 1523893
#% 1573236
#% 1594612
#% 1594649
#! We introduce divergent designs as a novel tuning paradigm for database systems that employ replication. A divergent design installs a different physical configuration (e.g., indexes and materialized views) with each database replica, specializing replicas for different subsets of the workload. At runtime, queries are routed to the subset of the replicas configured to yield the most efficient execution plans. When compared to uniformly designed replicas, divergent replicas can potentially execute their subset of the queries significantly faster, and their physical configurations could be initialized and maintained(updated) in less time. However, the specialization of divergent replicas limits the ability to load-balance the workload at runtime. We formalize the divergent design problem, characterize the properties of good designs, and analyze the complexity of identifying the optimal divergent design. Our paradigm captures the trade-off between load balancing among all n replicas vs. load balancing among m ≤ n specialized replicas. We develop an effective algorithm (leveraging single-node-tuning functionality) to compute good divergent designs for all the points of this trade-off. Experimental results validate the effectiveness of the algorithm and demonstrate that divergent designs can substantially improve workload performance.

#index 1770324
#* Skew-aware automatic database partitioning in shared-nothing, parallel OLTP systems
#@ Andrew Pavlo;Carlo Curino;Stanley Zdonik
#t 2012
#c 5
#% 25017
#% 36119
#% 43171
#% 48043
#% 116042
#% 129891
#% 152707
#% 229164
#% 248815
#% 397390
#% 397397
#% 463089
#% 480158
#% 480761
#% 481591
#% 482100
#% 571066
#% 571084
#% 708321
#% 765176
#% 765431
#% 993947
#% 1014224
#% 1016220
#% 1022298
#% 1063543
#% 1127596
#% 1328213
#% 1426552
#% 1523799
#% 1551289
#% 1573340
#% 1581943
#% 1628175
#% 1654042
#! The advent of affordable, shared-nothing computing systems portends a new class of parallel database management systems (DBMS) for on-line transaction processing (OLTP) applications that scale without sacrificing ACID guarantees [7, 9]. The performance of these DBMSs is predicated on the existence of an optimal database design that is tailored for the unique characteristics of OLTP workloads. Deriving such designs for modern DBMSs is difficult, especially for enterprise-class OLTP systems, since they impose extra challenges: the use of stored procedures, the need for load balancing in the presence of time-varying skew, complex schemas, and deployments with larger number of partitions. To this purpose, we present a novel approach to automatically partitioning databases for enterprise-class OLTP systems that significantly extends the state of the art by: (1) minimizing the number distributed transactions, while concurrently mitigating the effects of temporal skew in both the data distribution and accesses, (2) extending the design space to include replicated secondary indexes, (4) organically handling stored procedure routing, and (3) scaling of schema complexity, data size, and number of partitions. This effort builds on two key technical contributions: an analytical cost model that can be used to quickly estimate the relative coordination cost and skew for a given workload and a candidate database design, and an informed exploration of the huge solution space based on large neighborhood search. To evaluate our methods, we integrated our database design tool with a high-performance parallel, main memory DBMS and compared our methods against both popular heuristics and a state-of-the-art research prototype [17]. Using a diverse set of benchmarks, we show that our approach improves throughput by up to a factor of 16x over these other approaches.

#index 1770325
#* Sample-driven schema mapping
#@ Li Qian;Michael J. Cafarella;H. V. Jagadish
#t 2012
#c 5
#% 333988
#% 333990
#% 378409
#% 480645
#% 654458
#% 659990
#% 660001
#% 660011
#% 800497
#% 809239
#% 824693
#% 893196
#% 960234
#% 993981
#% 993982
#% 993987
#% 1019068
#% 1127413
#% 1167472
#% 1206612
#% 1206613
#% 1215806
#% 1328124
#% 1328200
#% 1426466
#% 1426534
#% 1567959
#% 1581857
#! End-users increasingly find the need to perform light-weight, customized schema mapping. State-of-the-art tools provide powerful functions to generate schema mappings, but they usually require an in-depth understanding of the semantics of multiple schemas and their correspondences, and are thus not suitable for users who are technically unsophisticated or when a large number of mappings must be performed. We propose a system for sample-driven schema mapping. It automatically constructs schema mappings, in real time, from user-input sample target instances. Because the user does not have to provide any explicit attribute-level match information, she is isolated from the possibly complex structure and semantics of both the source schemas and the mappings. In addition, the user never has to master any operations specific to schema mappings: she simply types data values into a spreadsheet-style interface. As a result, the user can construct mappings with a much lower cognitive burden. In this paper we present Mweaver, a prototype sample-driven schema mapping system. It employs novel algorithms that enable the system to obtain desired mapping results while meeting interactive response performance requirements. We show the results of a user study that compares Mweaver with two state-of-the-art mapping tools across several mapping tasks, both real and synthetic. These suggest that the Mweaver system enables users to perform practical mapping tasks in about 1/5th the time needed by the state-of-the-art tools.

#index 1770326
#* Can we beat the prefix filtering?: an adaptive framework for similarity join and search
#@ Jiannan Wang;Guoliang Li;Jianhua Feng
#t 2012
#c 5
#% 333679
#% 480654
#% 654467
#% 765463
#% 824678
#% 824684
#% 864392
#% 893164
#% 956458
#% 956506
#% 1022227
#% 1054481
#% 1055684
#% 1127368
#% 1127425
#% 1206665
#% 1206677
#% 1206821
#% 1328164
#% 1426543
#% 1426578
#% 1523903
#% 1573234
#% 1581890
#% 1581929
#% 1581932
#% 1594614
#% 1654056
#! As two important operations in data cleaning, similarity join and similarity search have attracted much attention recently. Existing methods to support similarity join usually adopt a prefix-filtering-based framework. They select a prefix of each object and prune object pairs whose prefixes have no overlap. We have an observation that prefix lengths have significant effect on the performance. Different prefix lengths lead to significantly different performance, and prefix filtering does not always achieve high performance. To address this problem, in this paper we propose an adaptive framework to support similarity join. We propose a cost model to judiciously select an appropriate prefix for each object. To efficiently select prefixes, we devise effective indexes. We extend our method to support similarity search. Experimental results show that our framework beats the prefix-filtering-based framework and achieves high efficiency.

#index 1770327
#* InfoGather: entity augmentation and attribute discovery by holistic matching with web tables
#@ Mohamed Yakout;Kris Ganjam;Kaushik Chakrabarti;Surajit Chaudhuri
#t 2012
#c 5
#% 333990
#% 348173
#% 480645
#% 572314
#% 654459
#% 654467
#% 800497
#% 1127393
#% 1215321
#% 1328133
#% 1328200
#% 1523913
#% 1560397
#% 1567959
#% 1581927
#% 1592311
#! The Web contains a vast corpus of HTML tables, specifically entity attribute tables. We present three core operations, namely entity augmentation by attribute name, entity augmentation by example and attribute discovery, that are useful for "information gathering" tasks (e.g., researching for products or stocks). We propose to use web table corpus to perform them automatically. We require the operations to have high precision and coverage, have fast (ideally interactive) response times and be applicable to any arbitrary domain of entities. The naive approach that attempts to directly match the user input with the web tables suffers from poor precision and coverage. Our key insight is that we can achieve much higher precision and coverage by considering indirectly matching tables in addition to the directly matching ones. The challenge is to be robust to spuriously matched tables: we address it by developing a holistic matching framework based on topic sensitive pagerank and an augmentation framework that aggregates predictions from multiple matched tables. We propose a novel architecture that leverages preprocessing in MapReduce to achieve extremely fast response times at query time. Our experiments on real-life datasets and 573M web tables show that our approach has (i) significantly higher precision and coverage and (ii) four orders of magnitude faster response times compared with the state-of-the-art approach.

#index 1770328
#* Interactive regret minimization
#@ Danupon Nanongkai;Ashwin Lall;Atish Das Sarma;Kazuhisa Makino
#t 2012
#c 5
#% 300180
#% 333951
#% 875012
#% 903013
#% 1075132
#% 1083667
#% 1114539
#% 1131307
#% 1181269
#% 1206643
#% 1206819
#% 1211644
#% 1328160
#% 1450862
#% 1523894
#% 1594651
#% 1604467
#% 1671751
#% 1688273
#% 1914457
#! We study the notion of regret ratio proposed in [19] Nanongkai et al. [VLDB10] to deal with multi-criteria decision making in database systems. The regret minimization query proposed in [19] Nanongkai et al. was shown to have features of both skyline and top-k: it does not need information from the user but still controls the output size. While this approach is suitable for obtaining a reasonably small regret ratio, it is still open whether one can make the regret ratio arbitrarily small. Moreover, it remains open whether reasonable questions can be asked to the users in order to improve efficiency of the process. In this paper, we study the problem of minimizing regret ratio when the system is enhanced with interaction. We assume that when presented with a set of tuples the user can tell which tuple is most preferred. Under this assumption, we develop the problem of interactive regret minimization where we fix the number of questions and tuples per question that we can display, and aim at minimizing the regret ratio. We try to answer two questions in this paper: (1) How much does interaction help? That is, how much can we improve the regret ratio when there are interactions? (2) How efficient can interaction be? In particular, we measure how many questions we have to ask the user in order to make her regret ratio small enough. We answer both questions from both theoretical and practical standpoints. For the first question, we show that interaction can reduce the regret ratio almost exponentially. To do this, we prove a lower bound for the previous approach (thereby resolving an open problem from [19] Nanongkai et al.), and develop an almost-optimal upper bound that makes the regret ratio exponentially smaller. Our experiments also confirm that, in practice, interactions help in improving the regret ratio by many orders of magnitude. For the second question, we prove that when our algorithm shows a reasonable number of points per question, it only needs a few questions to make the regret ratio small. Thus, interactive regret minimization seems to be a necessary and sufficient way to deal with multi-criteria decision making in database systems.

#index 1770329
#* MCJoin: a memory-constrained join for column-store main-memory databases
#@ Steven Keith Begley;Zhen He;Yi-Ping Phoebe Chen
#t 2012
#c 5
#% 245771
#% 286258
#% 322880
#% 443513
#% 464177
#% 479821
#% 480119
#% 566122
#% 824697
#% 875026
#% 1063542
#% 1089604
#% 1127390
#% 1270566
#% 1328057
#! There exists a need for high performance, read-only main-memory database systems for OLAP-style application scenarios. Most of the existing works in this area are centered around the domain of column-store databases, which are particularly well suited to OLAP-style scenarios and have been shown to overcome the memory bottleneck issues that have been found to hinder the more traditional row-store database systems. One of the main database operations these systems are focused on optimizing is the JOIN operation. However, all these existing systems use join algorithms that are designed with the unrealistic assumption that there is unlimited temporary memory available to perform the join. In contrast, we propose a Memory Constrained Join algorithm (MCJoin) which is both high performing and also performs all of its operations within a tight given memory constraint. Extensive experimental results show that MCJoin outperforms a naive memory constrained version of the state-of-the-art Radix-Clustered Hash Join algorithm in all of the situations tested, with margins of up to almost 500%.

#index 1770330
#* Holistic optimization by prefetching query results
#@ Karthik Ramachandra;S. Sudarshan
#t 2012
#c 5
#% 97771
#% 257642
#% 287483
#% 461897
#% 480128
#% 480780
#% 617086
#% 1127439
#% 1206914
#% 1246682
#% 1594644
#% 1594658
#% 1732824
#! In this paper we address the problem of optimizing performance of database/web-service backed applications by means of automatically prefetching query results. Prefetching has been performed in earlier work based on predicting query access patterns; however such prediction is often of limited value, and can perform unnecessary prefetches. There has been some earlier work on program analysis and rewriting to automatically insert prefetch requests; however, such work has been restricted to rewriting of single procedures. In many cases, the query is in a procedure which does not offer much scope for prefetching within the procedure; in contrast, our approach can perform prefetching in a calling procedure, even when the actual query is in a called procedure, thereby greatly improving the benefits due to prefetching. Our approach does not perform any intrusive changes to the source code, and places prefetch instructions at the earliest possible points while avoiding wasteful prefetches. We have incorporated our techniques into a tool for holistic optimization called DBridge, to prefetch query results in Java programs that use JDBC. Our tool can be easily extended to handle Hibernate API calls as well as Web service requests. Our experiments on several real world applications demonstrate the applicability and significant performance gains due to our techniques.

#index 1770331
#* Managing large dynamic graphs efficiently
#@ Jayanta Mondal;Amol Deshpande
#t 2012
#c 5
#% 91077
#% 151432
#% 183248
#% 225006
#% 268799
#% 274612
#% 481434
#% 660011
#% 711970
#% 824693
#% 881523
#% 1055741
#% 1063500
#% 1063514
#% 1063539
#% 1206910
#% 1232289
#% 1237170
#% 1291617
#% 1318636
#% 1426571
#% 1464649
#% 1506217
#% 1523818
#% 1523819
#! There is an increasing need to ingest, manage, and query large volumes of graph-structured data arising in applications like social networks, communication networks, biological networks, and so on. Graph databases that can explicitly reason about the graphical nature of the data, that can support flexible schemas and node-centric or edge-centric analysis and querying, are ideal for storing such data. However, although there is much work on single-site graph databases and on efficiently executing different types of queries over large graphs, to date there is little work on understanding the challenges in distributed graph databases, needed to handle the large scale of such data. In this paper, we propose the design of an in-memory, distributed graph data management system aimed at managing a large-scale dynamically changing graph, and supporting low-latency query processing over it. The key challenge in a distributed graph database is that, partitioning a graph across a set of machines inherently results in a large number of distributed traversals across partitions to answer even simple queries. We propose aggressive replication of the nodes in the graph for supporting low-latency querying, and investigate three novel techniques to minimize the communication bandwidth and the storage requirements. First, we develop a hybrid replication policy that monitors node read-write frequencies to dynamically decide what data to replicate, and whether to do eager or lazy replication. Second, we propose a clustering-based approach to amortize the costs of making these replication decisions. Finally, we propose using a fairness criterion to dictate how replication decisions should be made. We provide both theoretical analysis and efficient algorithms for the optimization problems that arise. We have implemented our framework as a middleware on top of the open-source CouchDB key-value store. We evaluate our system on a social graph, and show that our system is able to handle very large graphs efficiently, and that it reduces the network bandwidth consumption significantly.

#index 1770332
#* Query preserving graph compression
#@ Wenfei Fan;Jianzhong Li;Xin Wang;Yinghui Wu
#t 2012
#c 5
#% 31484
#% 194127
#% 205419
#% 288628
#% 464883
#% 542102
#% 593696
#% 654452
#% 656242
#% 722530
#% 754058
#% 754117
#% 771914
#% 838518
#% 881523
#% 937549
#% 1002007
#% 1015266
#% 1063501
#% 1063514
#% 1083734
#% 1214643
#% 1217208
#% 1246431
#% 1407271
#% 1451193
#% 1523818
#% 1523819
#% 1560413
#% 1581922
#% 1587732
#! It is common to find graphs with millions of nodes and billions of edges in, e.g., social networks. Queries on such graphs are often prohibitively expensive. These motivate us to propose query preserving graph compression, to compress graphs relative to a class Λ of queries of users' choice. We compute a small Gr from a graph G such that (a) for any query Q Ε Λ Q, Q(G) = Q'(Gr), where Q' Ε Λ can be efficiently computed from Q; and (b) any algorithm for computing Q(G) can be directly applied to evaluating Q' on Gr as is. That is, while we cannot lower the complexity of evaluating graph queries, we reduce data graphs while preserving the answers to all the queries in Λ. To verify the effectiveness of this approach, (1) we develop compression strategies for two classes of queries: reachability and graph pattern queries via (bounded) simulation. We show that graphs can be efficiently compressed via a reachability equivalence relation and graph bisimulation, respectively, while reserving query answers. (2) We provide techniques for aintaining compressed graph Gr in response to changes ΔG to the original graph G. We show that the incremental maintenance problems are unbounded for the two lasses of queries, i.e., their costs are not a function of the size of ΔG and changes in Gr. Nevertheless, we develop incremental algorithms that depend only on ΔG and Gr, independent of G, i.e., we do not have to decompress Gr to propagate the changes. (3) Using real-life data, we experimentally verify that our compression techniques could reduce graphs in average by 95% for reachability and 57% for graph pattern matching, and that our incremental maintenance algorithms are efficient.

#index 1770333
#* SCARAB: scaling reachability computation on large graphs
#@ Ruoming Jin;Ning Ruan;Saikat Dey;Jeffrey Yu Xu
#t 2012
#c 5
#% 58365
#% 88051
#% 256685
#% 722530
#% 824692
#% 864462
#% 960304
#% 1044451
#% 1063514
#% 1206685
#% 1217208
#% 1482190
#% 1482226
#% 1523819
#% 1531325
#% 1581922
#% 1594607
#% 1676469
#% 1688299
#! Most of the existing reachability indices perform well on small- to medium- size graphs, but reach a scalability bottleneck around one million vertices/edges. As graphs become increasingly large, scalability is quickly becoming the major research challenge for the reachability computation today. Can we construct indices which scale to graphs with tens of millions of vertices and edges? Can the existing reachability indices which perform well on moderate-size graphs be scaled to very large graphs? In this paper, we propose SCARAB (standing for SCAlable ReachABility), a unified reachability computation framework: it not only can scale the existing state-of-the-art reachability indices, which otherwise could only be constructed and work on moderate size graphs, but also can help speed up the online query answering approaches. Our experimental results demonstrate that SCARAB can perform on graphs with millions of vertices/edges and is also much faster then GRAIL, the state-of-the-art scalability index approach.

#index 1770334
#* Skimmer: rapid scrolling of relational query results
#@ Manish Singh;Arnab Nandi;H. V. Jagadish
#t 2012
#c 5
#% 145196
#% 316179
#% 345271
#% 443531
#% 479931
#% 654486
#% 818916
#% 838540
#% 956649
#% 960244
#% 960287
#% 1016203
#% 1022314
#% 1130808
#% 1166473
#% 1181246
#% 1328120
#! A relational database often yields a large set of tuples as the result of a query. Users browse this result set to find the information they require. If the result set is large, there may be many pages of data to browse. Since results comprise tuples of alphanumeric values that have few visual markers, it is hard to browse the data quickly, even if it is sorted. In this paper, we describe the design of a system for browsing relational data by scrolling through it at a high speed. Rather than showing the user a fast changing blur, the system presents the user with a small number of representative tuples. Representative tuples are selected to provide a "good impression" of the query result. We show that the information loss to the user is limited, even at high scrolling speeds, and that our algorithms can pick good representatives fast enough to provide for real-time, high-speed scrolling over large datasets.

#index 1770335
#* Efficient spatial sampling of large geographical tables
#@ Anish Das Sarma;Hongrae Lee;Hector Gonzalez;Jayant Madhavan;Alon Halevy
#t 2012
#c 5
#% 68091
#% 227934
#% 248802
#% 248804
#% 316709
#% 408396
#% 427199
#% 527007
#% 725483
#% 1075132
#% 1426594
#! Large-scale map visualization systems play an increasingly important role in presenting geographic datasets to end users. Since these datasets can be extremely large, a map rendering system often needs to select a small fraction of the data to visualize them in a limited space. This paper addresses the fundamental challenge of thinning: determining appropriate samples of data to be shown on specific geographical regions and zoom levels. Other than the sheer scale of the data, the thinning problem is challenging because of a number of other reasons: (1) data can consist of complex geographical shapes, (2) rendering of data needs to satisfy certain constraints, such as data being preserved across zoom levels and adjacent regions, and (3) after satisfying the constraints, an optimal solution needs to be chosen based on objectives such as maximality, fairness, and importance of data. This paper formally defines and presents a complete solution to the thinning problem. First, we express the problem as a integer programming formulation that efficiently solves thinning for desired objectives. Second, we present more efficient solutions for maximality, based on DFS traversal of a spatial tree. Third, we consider the common special case of point datasets, and present an even more efficient randomized algorithm. Finally, we have implemented all techniques from this paper in Google Maps visualizations of Fusion Tables, and we describe a set of experiments that demonstrate the tradeoffs among the algorithms.

#index 1770336
#* Declarative error management for robust data-intensive applications
#@ Carl-Christian Kanne;Vuk Ercegovac
#t 2012
#c 5
#% 2655
#% 136740
#% 320773
#% 321643
#% 416017
#% 479623
#% 541980
#% 954300
#% 963669
#% 1063553
#% 1127559
#% 1328095
#% 1468421
#! We present an approach to declaratively manage run-time errors in data-intensive applications. When large volumes of raw data meet complex third-party libraries, deterministic run-time errors become likely, and existing query processors typically stop without returning a result when a run-time error occurs. The ability to degrade gracefully in the presence of run-time errors, and partially execute jobs, is typically limited to specific operators such as bulkloading. We generalize this concept to all operators of a query processing system, introducing a novel data type "partial result with errors" and corresponding operators. We show how to extend existing error-unaware operators to support this type, and as an added benefit, eliminate side-effect based error reporting. We use declarative specifications of acceptable results to control the semantics of error-aware operators. We have incorporated our approach into a declarative query processing system, which compiles the language constructs into instrumented execution plans for clusters of machines. We experimentally validate that the instrumentation overhead is below 20% in microbenchmarks, and not detectable when running I/O-intensive workloads.

#index 1770337
#* bLSM: a general purpose log structured merge tree
#@ Russell Sears;Raghu Ramakrishnan
#t 2012
#c 5
#% 107692
#% 208047
#% 244119
#% 570884
#% 723279
#% 951778
#% 961010
#% 963436
#% 978392
#% 978404
#% 978513
#% 985922
#% 992831
#% 1092672
#% 1127392
#% 1127560
#% 1328150
#% 1400975
#% 1426489
#% 1523901
#% 1581937
#% 1591735
#% 1621125
#% 1625031
#% 1770407
#! Data management workloads are increasingly write-intensive and subject to strict latency SLAs. This presents a dilemma: Update in place systems have unmatched latency but poor write throughput. In contrast, existing log structured techniques improve write throughput but sacrifice read performance and exhibit unacceptable latency spikes. We begin by presenting a new performance metric: read fanout, and argue that, with read and write amplification, it better characterizes real-world indexes than approaches such as asymptotic analysis and price/performance. We then present bLSM, a Log Structured Merge (LSM) tree with the advantages of B-Trees and log structured approaches: (1) Unlike existing log structured trees, bLSM has near-optimal read and scan performance, and (2) its new "spring and gear" merge scheduler bounds write latency without impacting throughput or allowing merges to block writes for extended periods of time. It does this by ensuring merges at each level of the tree make steady progress without resorting to techniques that degrade read performance. We use Bloom filters to improve index performance, and find a number of subtleties arise. First, we ensure reads can stop after finding one version of a record. Otherwise, frequently written items would incur multiple B-Tree lookups. Second, many applications check for existing values at insert. Avoiding the seek performed by the check is crucial.

#index 1770338
#* Skeleton automata for FPGAs: reconfiguring without reconstructing
#@ Jens Teubner;Louis Woods;Chongling Nie
#t 2012
#c 5
#% 266837
#% 320454
#% 480296
#% 490588
#% 730031
#% 731408
#% 804256
#% 893111
#% 994015
#% 1015272
#% 1148075
#% 1166682
#% 1206798
#% 1328185
#% 1523854
#% 1523931
#% 1541400
#% 1586200
#% 1594599
#% 1735824
#% 1840082
#% 1846756
#! While the performance opportunities of field-programmable gate arrays field (FPGAs)field for high-volume query processing are well-known, system makers still have to compromise between desired query expressiveness and high compilation effort. The cost of the latter is the primary limitation in building efficient FPGA/CPU hybrids. In this work we report on an FPGA-based stream processing engine that does not have this limitation. We provide a hardware implementation of XML projection [14] that can be reconfigured in less than a micro-second, yet supports a rich and expressive dialect of XPath. By performing XML projection in the network, we can fully leverage its filtering effect and improve XQuery performance by several factors. These improvements are made possible by a new design approach for FPGA acceleration, called skeleton automata. Skeleton automata separate the structure of finite-state automata from their semantics. Since individual queries only affect the latter, with our approach query workload changes can be accommodated fast and with high expressiveness.

#index 1770339
#* NoDB: efficient query execution on raw data files
#@ Ioannis Alagiannis;Renata Borovica;Miguel Branco;Stratos Idreos;Anastasia Ailamaki
#t 2012
#c 5
#% 479449
#% 480158
#% 482100
#% 765176
#% 765431
#% 810026
#% 845351
#% 875062
#% 893130
#% 960234
#% 960268
#% 1016220
#% 1016221
#% 1206799
#% 1217169
#% 1328066
#% 1372713
#% 1375919
#% 1545227
#% 1573236
#% 1592316
#! As data collections become larger and larger, data loading evolves to a major bottleneck. Many applications already avoid using database systems, e.g., scientific data analysis and social networks, due to the complexity and the increased data-to-query time. For such applications data collections keep growing fast, even on a daily basis, and we are already in the era of data deluge where we have much more data than what we can move, store, let alone analyze. Our contribution in this paper is the design and roadmap of a new paradigm in database systems, called NoDB, which do not require data loading while still maintaining the whole feature set of a modern database system. In particular, we show how to make raw data files a first-class citizen, fully integrated with the query engine. Through our design and lessons learned by implementing the NoDB philosophy over a modern DBMS, we discuss the fundamental limitations as well as the strong opportunities that such a research path brings. We identify performance bottlenecks specific for in situ processing, namely the repeated parsing and tokenizing overhead and the expensive data type conversion costs. To address these problems, we introduce an adaptive indexing mechanism that maintains positional information to provide efficient access to raw data files, together with a flexible caching structure. Our implementation over PostgreSQL, called PostgresRaw, is able to avoid the loading cost completely, while matching the query performance of plain PostgreSQL and even outperforming it in many cases. We conclude that NoDB systems are feasible to design and implement over modern database architectures, bringing an unprecedented positive effect in usability and performance.

#index 1770340
#* High-performance complex event processing over XML streams
#@ Barzan Mozafari;Kai Zeng;Carlo Zaniolo
#t 2012
#c 5
#% 351041
#% 654477
#% 731408
#% 765274
#% 803121
#% 850728
#% 864465
#% 874910
#% 875004
#% 875010
#% 994015
#% 1015338
#% 1024476
#% 1063734
#% 1224353
#% 1266913
#% 1269311
#% 1328054
#% 1426608
#% 1523808
#% 1661435
#% 1733511
#! Much research attention has been given to delivering high-performance systems that are capable of complex event processing (CEP) in a wide range of applications. However, many current CEP systems focus on processing efficiently data having a simple structure, and are otherwise limited in their ability to support efficiently complex continuous queries on structured or semi-structured information. However, XML streams represent a very popular form of data exchange, comprising large portions of social network and RSS feeds, financial records, configuration files, and similar applications requiring advanced CEP queries. In this paper, we present the XSeq language and system that support CEP on XML streams, via an extension of XPath that is both powerful and amenable to an efficient implementation. Specifically, the XSeq language extends XPath with natural operators to express sequential and Kleene-* patterns over XML streams, while remaining highly amenable to efficient implementation. XSeq is designed to take full advantage of recent advances in the field of automata on Visibly Pushdown Automata (VPA), where higher expressive power can be achieved without compromising efficiency (whereas the amenability to efficient implementation was not demonstrated in XPath extensions previously proposed). We illustrate XSeq's power for CEP applications through examples from different domains, and provide formal results on its expressiveness and complexity. Finally, we present several optimization techniques for XSeq queries. Our extensive experiments indicate that XSeq brings outstanding performance to CEP applications: two orders of magnitude improvement are obtained over the same queries executed in general-purpose XML engines.

#index 1770341
#* Prediction-based geometric monitoring over distributed data streams
#@ Nikos Giatrakos;Antonios Deligiannakis;Minos Garofalakis;Izchak Sharfman;Assaf Schuster
#t 2012
#c 5
#% 654443
#% 654488
#% 763708
#% 765445
#% 806214
#% 810009
#% 824652
#% 874994
#% 874995
#% 891929
#% 960368
#% 997490
#% 1016155
#% 1054483
#% 1063739
#% 1206769
#% 1217131
#% 1523969
#% 1524288
#! Many modern streaming applications, such as online analysis of financial, network, sensor and other forms of data are inherently distributed in nature. An important query type that is the focal point in such application scenarios regards actuation queries, where proper action is dictated based on a trigger condition placed upon the current value that a monitored function receives. Recent work studies the problem of (non-linear) sophisticated function tracking in a distributed manner. The main concept behind the geometric monitoring approach proposed there, is for each distributed site to perform the function monitoring over an appropriate subset of the input domain. In the current work, we examine whether the distributed monitoring mechanism can become more efficient, in terms of the number of communicated messages, by extending the geometric monitoring framework to utilize prediction models. We initially describe a number of local estimators (predictors) that are useful for the applications that we consider and which have already been shown particularly useful in past work. We then demonstrate the feasibility of incorporating predictors in the geometric monitoring framework and show that prediction-based geometric monitoring in fact generalizes the original geometric monitoring framework. We propose a large variety of different prediction-based monitoring models for the distributed threshold monitoring of complex functions. Our extensive experimentation with a variety of real data sets, functions and parameter settings indicates that our approaches can provide significant communication savings ranging between two times and up to three orders of magnitude, compared to the transmission cost of the original monitoring framework.

#index 1770342
#* Online windowed subsequence matching over probabilistic sequences
#@ Zheng Li;Tingjian Ge
#t 2012
#c 5
#% 93113
#% 172950
#% 229127
#% 273712
#% 376266
#% 464712
#% 480654
#% 480938
#% 654510
#% 662750
#% 810065
#% 864474
#% 875004
#% 993949
#% 1063523
#% 1290946
#% 1451177
#% 1451534
#% 1480884
#% 1633148
#! Windowed subsequence matching over deterministic strings has been studied in previous work in the contexts of knowledge discovery, data mining, and molecular biology. However, we observe that in these applications, as well as in data stream monitoring, complex event processing, and time series data processing in which streams can be mapped to strings, the strings are often noisy and probabilistic. We study this problem in the online setting where efficiency is paramount. We first formulate the query semantics, and propose an exact algorithm. Then we propose a randomized approximation algorithm that is faster and, in the mean time, provably accurate. Moreover, we devise a filtering algorithm to further enhance the efficiency with an optimization technique that is adaptive to sequence stream contents. Finally, we propose algorithms for patterns with negations. In order to verify the algorithms, we conduct a systematic empirical study using three real datasets and some synthetic datasets.

#index 1770343
#* MaskIt: privately releasing user context streams for personalized mobile applications
#@ Michaela Götz;Suman Nath;Johannes Gehrke
#t 2012
#c 5
#% 268079
#% 279755
#% 577329
#% 864406
#% 893101
#% 911803
#% 937550
#% 956551
#% 1000502
#% 1022247
#% 1060475
#% 1063478
#% 1112971
#% 1217156
#% 1253676
#% 1265776
#% 1272359
#% 1292624
#% 1298885
#% 1364890
#% 1370254
#% 1426567
#% 1432717
#% 1476141
#% 1527000
#% 1581831
#% 1584349
#% 1643078
#% 1729021
#! The rise of smartphones equipped with various sensors has enabled personalization of various applications based on user contexts extracted from sensor readings. At the same time it has raised serious concerns about the privacy of user contexts. In this paper, we present MASKIT, a technique to filter a user context stream that provably preserves privacy. The filtered context stream can be released to applications or be used to answer their queries. Privacy is defined with respect to a set of sensitive contexts specified by the user. MASKIT limits what adversaries can learn from the filtered stream about the user being in a sensitive context - even if the adversaries are powerful and have knowledge about the filtering system and temporal correlations in the context stream. At the heart of MASKIT is a privacy check deciding whether to release or suppress the current user context. We present two novel privacy checks and explain how to choose the one with the higher utility for a user. Our experiments on real smartphone context traces of 91 users demonstrate the high utility of MASKIT.

#index 1770344
#* Authenticating location-based services without compromising location privacy
#@ Haibo Hu;Jianliang Xu;Qian Chen;Ziwei Yang
#t 2012
#c 5
#% 443327
#% 452685
#% 513367
#% 743280
#% 745532
#% 810042
#% 874980
#% 1013611
#% 1022214
#% 1034732
#% 1055695
#% 1063478
#% 1127362
#% 1181949
#% 1206751
#% 1211647
#% 1217157
#% 1270569
#% 1394513
#% 1400777
#% 1405945
#% 1426415
#% 1488562
#% 1523850
#% 1594676
#% 1594685
#! The popularity of mobile social networking services (mSNSs) is propelling more and more businesses, especially those in retailing and marketing, into mobile and location-based forms. To address the trust issue, the service providers are expected to deliver their location-based services in an authenticatable manner, so that the correctness of the service results can be verified by the client. However, existing works on query authentication cannot preserve the privacy of the data being queried, which are sensitive user locations when it comes to location-based services and mSNSs. In this paper, we address this challenging problem by proposing a comprehensive solution that preserves unconditional location privacy when authenticating range queries. Three authentication schemes for $R$-tree and grid-file index, together with two optimization techniques, are developed. Cost models, security analysis, and experimental results consistently show the effectiveness, reliability and robustness of the proposed schemes under various system settings and query workloads.

#index 1770345
#* Effective caching of shortest paths for location-based services
#@ Jeppe Rishede Thomsen;Man Lung Yiu;Christian S. Jensen
#t 2012
#c 5
#% 210182
#% 443533
#% 481916
#% 527191
#% 800540
#% 805864
#% 893162
#% 987215
#% 1055849
#% 1063472
#% 1102989
#% 1190098
#% 1190134
#% 1195885
#% 1265149
#% 1292553
#% 1426510
#% 1688302
#% 1834787
#% 1870668
#! Web search is ubiquitous in our daily lives. Caching has been extensively used to reduce the computation time of the search engine and reduce the network traffic beyond a proxy server. Another form of web search, known as online shortest path search, is popular due to advances in geo-positioning. However, existing caching techniques are ineffective for shortest path queries. This is due to several crucial differences between web search results and shortest path results, in relation to query matching, cache item overlapping, and query cost variation. Motivated by this, we identify several properties that are essential to the success of effective caching for shortest path search. Our cache exploits the optimal subpath property, which allows a cached shortest path to answer any query with source and target nodes on the path. We utilize statistics from query logs to estimate the benefit of caching a specific shortest path, and we employ a greedy algorithm for placing beneficial paths in the cache. Also, we design a compact cache structure that supports efficient query matching at runtime. Empirical results on real datasets confirm the effectiveness of our proposed techniques.

#index 1770346
#* Towards a unified architecture for in-RDBMS analytics
#@ Xixuan Feng;Arun Kumar;Benjamin Recht;Christopher Ré
#t 2012
#c 5
#% 1331
#% 97427
#% 384978
#% 416555
#% 416668
#% 481290
#% 751777
#% 757953
#% 824749
#% 874976
#% 893168
#% 1061607
#% 1063521
#% 1127378
#% 1127415
#% 1193365
#% 1232034
#% 1299294
#% 1318710
#% 1328066
#% 1523866
#% 1523889
#% 1605920
#% 1770346
#! The increasing use of statistical data analysis in enterprise applications has created an arms race among database vendors to offer ever more sophisticated in-database analytics. One challenge in this race is that each new statistical technique must be implemented from scratch in the RDBMS, which leads to a lengthy and complex development process. We argue that the root cause for this overhead is the lack of a unified architecture for in-database analytics. Our main contribution in this work is to take a step towards such a unified architecture. A key benefit of our unified architecture is that performance optimizations for analytics techniques can be studied generically instead of an ad hoc, per-technique fashion. In particular, our technical contributions are theoretical and empirical studies of two key factors that we found impact performance: the order data is stored, and parallelization of computations on a single-node multicore RDBMS. We demonstrate the feasibility of our architecture by integrating several popular analytics techniques into two commercial and one open-source RDBMS. Our architecture requires changes to only a few dozen lines of code to integrate a new statistical technique. We then compare our approach with the native analytics tools offered by the commercial RDBMSes on various analytics tasks, and validate that our approach achieves competitive or higher performance, while still achieving the same quality.

#index 1770347
#* Tiresias: the database oracle for how-to queries
#@ Alexandra Meliou;Dan Suciu
#t 2012
#c 5
#% 663
#% 36683
#% 318704
#% 368248
#% 465190
#% 874911
#% 880394
#% 893189
#% 960293
#% 976987
#% 992830
#% 1063521
#% 1206717
#% 1470497
#% 1581829
#% 1581903
#% 1651554
#! How-To queries answer fundamental data analysis questions of the form: "How should the input change in order to achieve the desired output". As a Reverse Data Management problem, the evaluation of how-to queries is harder than their "forward" counterpart: hypothetical, or what-if queries. In this paper, we present Tiresias, the first system that provides support for how-to queries, allowing the definition and integrated evaluation of a large set of constrained optimization problems, specifically Mixed Integer Programming problems, on top of a relational database system. Tiresias generates the problem variables, constraints and objectives by issuing standard SQL statements, allowing for its integration with any RDBMS. The contributions of this work are the following: (a) we define how-to queries using possible world semantics, and propose the specification language TiQL (for Tiresias Query Language) based on simple extensions to standard Datalog. (b) We define translation rules that generate a Mixed Integer Program (MIP) from TiQL specifications, which can be solved using existing tools. (c) Tiresias implements powerful "data-aware" optimizations that are beyond the capabilities of modern MIP solvers, dramatically improving the system performance. (d) Finally, an extensive performance evaluation on the TPC-H dataset demonstrates the effectiveness of these optimizations, particularly highlighting the ability to apply divide-and-conquer methods to break MIP problems into smaller instances.

#index 1770348
#* GUPT: privacy preserving data analysis made easy
#@ Prashanth Mohan;Abhradeep Thakurta;Elaine Shi;Dawn Song;David Culler
#t 2012
#c 5
#% 13014
#% 528360
#% 576761
#% 864412
#% 963241
#% 963669
#% 1029084
#% 1080356
#% 1083653
#% 1130981
#% 1217148
#% 1217156
#% 1393823
#% 1426323
#% 1426454
#% 1426563
#% 1468410
#% 1523886
#% 1584793
#% 1584903
#% 1612101
#% 1740518
#! It is often highly valuable for organizations to have their data analyzed by external agents. However, any program that computes on potentially sensitive data risks leaking information through its output. Differential privacy provides a theoretical framework for processing data while protecting the privacy of individual records in a dataset. Unfortunately, it has seen limited adoption because of the loss in output accuracy, the difficulty in making programs differentially private, lack of mechanisms to describe the privacy budget in a programmer's utilitarian terms, and the challenging requirement that data owners and data analysts manually distribute the limited privacy budget between queries. This paper presents the design and evaluation of a new system, GUPT, that overcomes these challenges. Unlike existing differentially private systems such as PINQ and Airavat, it guarantees differential privacy to programs not developed with privacy in mind, makes no trust assumptions about the analysis program, and is secure to all known classes of side-channel attacks. GUPT uses a new model of data sensitivity that degrades privacy of data over time. This enables efficient allocation of different levels of privacy for different user applications while guaranteeing an overall constant level of privacy and maximizing the utility of each application. GUPT also introduces techniques that improve the accuracy of output while achieving the same level of privacy. These approaches enable GUPT to easily execute a wide variety of data analysis programs while providing both utility and privacy.

#index 1770349
#* CrowdScreen: algorithms for filtering data with humans
#@ Aditya G. Parameswaran;Hector Garcia-Molina;Hyunjung Park;Neoklis Polyzotis;Aditya Ramesh;Jennifer Widom
#t 2012
#c 5
#% 1083692
#% 1150163
#% 1206636
#% 1211801
#% 1214647
#% 1252609
#% 1264744
#% 1338533
#% 1410150
#% 1526538
#% 1536509
#% 1550748
#% 1573506
#% 1581851
#% 1581980
#% 1628171
#! Given a large set of data items, we consider the problem of filtering them based on a set of properties that can be verified by humans. This problem is commonplace in crowdsourcing applications, and yet, to our knowledge, no one has considered the formal optimization of this problem. (Typical solutions use heuristics to solve the problem.) We formally state a few different variants of this problem. We develop deterministic and probabilistic algorithms to optimize the expected cost (i.e., number of questions) and expected error. We experimentally show that our algorithms provide definite gains with respect to other strategies. Our algorithms can be applied in a variety of crowdsourcing scenarios and can form an integral part of any query processor that uses human computation.

#index 1770350
#* Local structure and determinism in probabilistic databases
#@ Theodoros Rekatsinas;Amol Deshpande;Lise Getoor
#t 2012
#c 5
#% 121397
#% 212157
#% 233849
#% 235023
#% 571102
#% 1016178
#% 1016201
#% 1022259
#% 1036395
#% 1063719
#% 1111133
#% 1127378
#% 1196918
#% 1206987
#% 1217181
#% 1273913
#% 1275146
#% 1289558
#% 1289570
#% 1291123
#% 1349569
#% 1372709
#% 1426461
#% 1426558
#% 1495931
#% 1523866
#% 1523890
#% 1538784
#% 1615075
#% 1650767
#% 1650778
#% 1668637
#% 1728680
#! While extensive work has been done on evaluating queries over tuple-independent probabilistic databases, query evaluation over correlated data has received much less attention even though the support for correlations is essential for many natural applications of probabilistic databases, e.g., information extraction, data integration, computer vision, etc. In this paper, we develop a novel approach for efficiently evaluating probabilistic queries over correlated databases where correlations are represented using a factor graph, a class of graphical models widely used for capturing correlations and performing statistical inference. Our approach exploits the specific values of the factor parameters and the determinism in the correlations, collectively called local structure, to reduce the complexity of query evaluation. Our framework is based on arithmetic circuits, factorized representations of probability distributions that can exploit such local structure. Traditionally, arithmetic circuits are generated following a compilation process and can not be updated directly. We introduce a generalization of arithmetic circuits, called annotated arithmetic circuits, and a novel algorithm for updating them, which enables us to answer probabilistic queries efficiently. We present a comprehensive experimental analysis and show speed-ups of at least one order of magnitude in many cases.

#index 1770351
#* So who won?: dynamic max discovery with the crowd
#@ Stephen Guo;Aditya Parameswaran;Hector Garcia-Molina
#t 2012
#c 5
#% 177541
#% 805895
#% 847120
#% 963243
#% 1047347
#% 1091267
#% 1096055
#% 1150163
#% 1232244
#% 1250605
#% 1272396
#% 1407358
#% 1452857
#% 1477559
#% 1550748
#% 1560402
#% 1581851
#% 1628171
#! We consider a crowdsourcing database system that may cleanse, populate, or filter its data by using human workers. Just like a conventional DB system, such a crowdsourcing DB system requires data manipulation functions such as select, aggregate, maximum, average, and so on, except that now it must rely on human operators (that for example compare two objects) with very different latency, cost and accuracy characteristics. In this paper, we focus on one such function, maximum, that finds the highest ranked object or tuple in a set. In particularm we study two problems: given a set of votes (pairwise comparisons among objects), how do we select the maximum? And how do we improve our estimate by requesting additional votes? We show that in a crowdsourcing DB system, the optimal solution to both problems is NP-Hard. We then provide heuristic functions to select the maximum given evidence, and to select additional votes. We experimentally evaluate our functions to highlight their strengths and weaknesses.

#index 1770352
#* Processing a large number of continuous preference top-k queries
#@ Albert Yu;Pankaj K. Agarwal;Jun Yang
#t 2012
#c 5
#% 1679
#% 144126
#% 264161
#% 282441
#% 300180
#% 321052
#% 333951
#% 410958
#% 465167
#% 762055
#% 766228
#% 847057
#% 875019
#% 875023
#% 893126
#% 1022217
#% 1075132
#% 1164838
#% 1496105
#% 1581894
#% 1581913
#% 1588453
#% 1595894
#! Given a set of objects, each with multiple numeric attributes, a (preference) top-k query retrieves the k objects with the highest scores according to a user preference, defined as a linear combination of attribute values. We consider the problem of processing a large number of continuous top-k queries, each with its own preference. When objects or user preferences change, the query results must be updated. We present a dynamic index that supports the reverse top k query, which is of independent interest. Combining this index with another one for top-k queries, we develop a scalable solution for processing many continuous top-k queries that exploits the clusteredness in user preferences. We also define an approximate version of the problem and present a solution significantly more efficient than the exact one with little loss in accuracy.

#index 1770353
#* Optimal top-k generation of attribute combinations based on ranked lists
#@ Jiaheng Lu;Pierre Senellart;Chunbin Lin;Xiaoyong Du;Shan Wang;Xinxing Chen
#t 2012
#c 5
#% 333854
#% 397378
#% 480330
#% 631988
#% 643057
#% 643566
#% 745820
#% 800508
#% 818232
#% 824681
#% 874975
#% 960242
#% 983263
#% 994013
#% 1069655
#% 1075132
#% 1206957
#% 1464039
#% 1581896
#% 1594557
#% 1594558
#! In this work, we study a novel query type, called top-k,m queries. Suppose we are given a set of groups and each group contains a set of attributes, each of which is associated with a ranked list of tuples, with ID and score. All lists are ranked in decreasing order of the scores of tuples. We are interested in finding the best combinations of attributes, each combination involving one attribute from each group. More specifically, we want the top-k combinations of attributes according to the corresponding top-m tuples with matching IDs. This problem has a wide range of applications from databases to search engines on traditional and non-traditional types of data (relational data, XML, text, etc.). We show that a straightforward extension of an optimal top-k algorithm, the Threshold Algorithm (TA), has shortcomings in solving the km problem, as it needs to compute a large number of intermediate results for each combination and reads moreinputs than needed. To overcome this weakness, we provide here, for the first time, a provably instance-optimal algorithm and further develop optimizations for efficient query evaluation to reduce computational and memory costs and the number of accesses. We demonstrate experimentally the scalability and efficiency of our algorithms over three real applications.

#index 1770354
#* Top-k bounded diversification
#@ Piero Fraternali;Davide Martinenghi;Marco Tagliasacchi
#t 2012
#c 5
#% 262112
#% 462239
#% 763882
#% 794881
#% 874993
#% 1058620
#% 1063713
#% 1075132
#% 1166473
#% 1190093
#% 1206662
#% 1328135
#% 1328137
#% 1400011
#% 1450870
#% 1472964
#% 1489397
#% 1523826
#% 1536552
#% 1581349
#% 1581410
#% 1581911
#% 1581913
#% 1594636
#% 1697429
#! This paper investigates diversity queries over objects embedded in a low-dimensional vector space. An interesting case is provided by spatial Web objects, which are produced in great quantity by location-based services that let users attach content to places, and arise also in trip planning, news analysis, and real estate scenarios. The targeted queries aim at retrieving the best set of objects relevant to given user criteria and well distributed over a region of interest. Such queries are a particular case of diversified top-k queries, for which existing methods are too costly, as they evaluate diversity by accessing and scanning all relevant objects, even if only a small subset is needed. We therefore introduce Space Partitioning and Probing (SPP), an algorithm that minimizes the number of accessed objects while finding exactly the same result as MMR, the most popular diversification algorithm. SPP belongs to a family of algorithms that rely only on score-based and distance-based access methods, which are available in most geo-referenced Web data sources, and do not require retrieving all the relevant objects. Experiments show that SPP significantly reduces the number of accessed objects while incurring a very low computational overhead.

#index 1770355
#* Temporal alignment
#@ Anton Dignös;Michael H. Böhlen;Johann Gamper
#t 2012
#c 5
#% 178813
#% 213957
#% 213959
#% 287268
#% 318704
#% 333874
#% 335715
#% 361445
#% 443136
#% 463751
#% 464854
#% 465002
#% 503697
#% 527790
#% 565462
#% 578404
#% 645159
#% 659975
#% 726629
#% 789017
#% 800003
#% 1424357
#% 1688261
#! In order to process interval timestamped data, the sequenced semantics has been proposed. This paper presents a relational algebra solution that provides native support for the three properties of the sequenced semantics: snapshot reducibility, extended snapshot reducibility, and change preservation. We introduce two temporal primitives, temporal splitter and temporal aligner, and define rules that use these primitives to reduce the operators of a temporal algebra to their nontemporal counterparts. Our solution supports the three properties of the sequenced semantics through interval adjustment and timestamp propagation. We have implemented the temporal primitives and reduction rules in the kernel of PostgreSQL to get native database support for processing interval timestamped data. The support is comprehensive and includes outer joins, antijoins, and aggregations with predicates and functions over the time intervals of argument relations. The implementation and empirical evaluation confirms effectiveness and scalability of our solution that leverages existing database query optimization techniques.

#index 1770356
#* A highway-centric labeling approach for answering distance queries on large sparse graphs
#@ Ruoming Jin;Ning Ruan;Yang Xiang;Victor Lee
#t 2012
#c 5
#% 70370
#% 256685
#% 442858
#% 443208
#% 443533
#% 453549
#% 479803
#% 527029
#% 577372
#% 722530
#% 785120
#% 787540
#% 793252
#% 813718
#% 1013630
#% 1019117
#% 1063472
#% 1065980
#% 1080074
#% 1102989
#% 1181254
#% 1217208
#% 1230568
#% 1292553
#% 1314064
#% 1328210
#% 1355056
#% 1404186
#% 1412885
#% 1426510
#% 1482228
#% 1484129
#% 1581881
#% 1597258
#% 1676469
#% 1702975
#! The distance query, which asks the length of the shortest path from a vertex $u$ to another vertex v, has applications ranging from link analysis, semantic web and other ontology processing, to social network operations. Here, we propose a novel labeling scheme, referred to as Highway-Centric Labeling, for answering distance queries in a large sparse graph. It empowers the distance labeling with a highway structure and leverages a novel bipartite set cover framework/algorithm. Highway-centric labeling provides better labeling size than the state-of-the-art $2$-hop labeling, theoretically and empirically. It also offers both exact distance and approximate distance with bounded accuracy. A detailed experimental evaluation on both synthetic and real datasets demonstrates that highway-centric labeling can outperform the state-of-the-art distance computation approaches in terms of both index size and query time.

#index 1770357
#* Efficient processing of distance queries in large graphs: a vertex cover approach
#@ James Cheng;Yiping Ke;Shumo Chu;Carter Cheng
#t 2012
#c 5
#% 41684
#% 60237
#% 280394
#% 282771
#% 283833
#% 288780
#% 303087
#% 325316
#% 330609
#% 341704
#% 548494
#% 617131
#% 847101
#% 985929
#% 1068448
#% 1181255
#% 1263166
#% 1426510
#% 1426539
#% 1484129
#! We propose a novel disk-based index for processing single-source shortest path or distance queries. The index is useful in a wide range of important applications (e.g., network analysis, routing planning, etc.). Our index is a tree-structured index constructed based on the concept of vertex cover. We propose an I/O-efficient algorithm to construct the index when the input graph is too large to fit in main memory. We give detailed analysis of I/O and CPU complexity for both index construction and query processing, and verify the efficiency of our index for query processing in massive real-world graphs.

#index 1770358
#* Aggregate suppression for enterprise search engines
#@ Mingyang Zhang;Nan Zhang;Gautam Das
#t 2012
#c 5
#% 268114
#% 300184
#% 576761
#% 586838
#% 740764
#% 809244
#% 810028
#% 869499
#% 893101
#% 907563
#% 937550
#% 956534
#% 1019163
#% 1190059
#% 1217158
#% 1292623
#% 1399990
#% 1415847
#% 1581912
#% 1740518
#! Many enterprise websites provide search engines to facilitate customer access to their underlying documents or data. With the web interface of such a search engine, a customer can specify one or a few keywords that he/she is interested in; and the search engine returns a list of documents/tuples matching the user-specified keywords, sorted by an often-proprietary scoring function. It was traditionally believed that, because of its highly-restrictive interface (i.e., keyword search only, no SQL-style queries), such a search engine serves its purpose of answering individual keyword-search queries without disclosing big-picture aggregates over the data which, as we shall show in the paper, may incur significant privacy concerns to the enterprise. Nonetheless, recent work on sampling and aggregate estimation over a search engine's corpus through its keyword-search interface transcends this traditional belief. In this paper, we consider a novel problem of suppressing sensitive aggregates for enterprise search engines while maintaining the quality of answers provided to individual keyword-search queries. We demonstrate the effectiveness and efficiency of our novel techniques through theoretical analysis and extensive experimental studies.

#index 1770359
#* Probase: a probabilistic taxonomy for text understanding
#@ Wentao Wu;Hongsong Li;Haixun Wang;Kenny Q. Zhu
#t 2012
#c 5
#% 405391
#% 509695
#% 754068
#% 756964
#% 786523
#% 815283
#% 939601
#% 956503
#% 956564
#% 1063570
#% 1131827
#% 1155587
#% 1269587
#% 1269899
#% 1275182
#% 1275192
#% 1289516
#% 1409954
#% 1770360
#% 1770371
#% 1826433
#! Knowledge is indispensable to understanding. The ongoing information explosion highlights the need to enable machines to better understand electronic text in human language. Much work has been devoted to creating universal ontologies or taxonomies for this purpose. However, none of the existing ontologies has the needed depth and breadth for universal understanding. In this paper, we present a universal, probabilistic taxonomy that is more comprehensive than any existing ones. It contains 2.7 million concepts harnessed automatically from a corpus of 1.68 billion web pages. Unlike traditional taxonomies that treat knowledge as black and white, it uses probabilities to model inconsistent, ambiguous and uncertain information it contains. We present details of how the taxonomy is constructed, its probabilistic modeling, and its potential applications in text understanding.

#index 1770360
#* Optimizing index for taxonomy keyword search
#@ Bolin Ding;Haixun Wang;Ruoming Jin;Jiawei Han;Zhongyuan Wang
#t 2012
#c 5
#% 198055
#% 273697
#% 576114
#% 756964
#% 869501
#% 869535
#% 931292
#% 939601
#% 956564
#% 1074101
#% 1083721
#% 1130854
#% 1166527
#% 1181293
#% 1269899
#% 1392432
#% 1538767
#% 1567485
#% 1641950
#% 1702425
#% 1770359
#% 1826433
#% 1845813
#! Query substitution is an important problem in information retrieval. Much work focuses on how to find substitutes for any given query. In this paper, we study how to efficiently process a keyword query whose substitutes are defined by a given taxonomy. This problem is challenging because each term in a query can have a large number of substitutes, and the original query can be rewritten into any of their combinations. We propose to build an additional index (besides inverted index) to efficiently process queries. For a query workload, we formulate an optimization problem which chooses the additional index structure, aiming at minimizing the query evaluation cost, under given index space constraints. We show the NP-hardness of the problem, and propose a pseudo-polynomial time algorithm using dynamic programming, as well as an 1 over 4(1-1/e)-approximation algorithm to solve the problem. Experimental results show that, with only 10% additional index space, our approach can greatly reduce the query evaluation cost.

#index 1770361
#* A model-based approach to attributed graph clustering
#@ Zhiqiang Xu;Yiping Ke;Yi Wang;Hong Cheng;James Cheng
#t 2012
#c 5
#% 44876
#% 115608
#% 280819
#% 303620
#% 765551
#% 915344
#% 1328169
#% 1387604
#% 1401774
#% 1524268
#% 1535346
#! Graph clustering, also known as community detection, is a long-standing problem in data mining. However, with the proliferation of rich attribute information available for objects in real-world graphs, how to leverage structural and attribute information for clustering attributed graphs becomes a new challenge. Most existing works take a distance-based approach. They proposed various distance measures to combine structural and attribute information. In this paper, we consider an alternative view and propose a model-based approach to attributed graph clustering. We develop a Bayesian probabilistic model for attributed graphs. The model provides a principled and natural framework for capturing both structural and attribute aspects of a graph, while avoiding the artificial design of a distance measure. Clustering with the proposed model can be transformed into a probabilistic inference problem, for which we devise an efficient variational algorithm. Experimental results on large real-world datasets demonstrate that our method significantly outperforms the state-of-art distance-based attributed graph clustering method.

#index 1770362
#* Towards effective partition management for large graphs
#@ Shengqi Yang;Xifeng Yan;Bo Zong;Arijit Khan
#t 2012
#c 5
#% 124743
#% 202286
#% 243166
#% 253560
#% 273374
#% 274612
#% 322846
#% 330305
#% 340175
#% 345693
#% 467185
#% 479973
#% 519567
#% 577329
#% 754117
#% 871315
#% 893106
#% 1022236
#% 1206875
#% 1215445
#% 1318636
#% 1426479
#% 1426513
#% 1464649
#% 1506217
#% 1523799
#% 1581837
#% 1581871
#% 1586117
#! Searching and mining large graphs today is critical to a variety of application domains, ranging from community detection in social networks, to de novo genome sequence assembly. Scalable processing of large graphs requires careful partitioning and distribution of graphs across clusters. In this paper, we investigate the problem of managing large-scale graphs in clusters and study access characteristics of local graph queries such as breadth-first search, random walk, and SPARQL queries, which are popular in real applications. These queries exhibit strong access locality, and therefore require specific data partitioning strategies. In this work, we propose a Self Evolving Distributed Graph Management Environment (Sedge), to minimize inter-machine communication during graph query processing in multiple machines. In order to improve query response time and throughput, Sedge introduces a two-level partition management architecture with complimentary primary partitions and dynamic secondary partitions. These two kinds of partitions are able to adapt in real time to changes in query workload. (Sedge) also includes a set of workload analyzing algorithms whose time complexity is linear or sublinear to graph size. Empirical results show that it significantly improves distributed graph processing on today's commodity clusters.

#index 1770363
#* TreeSpan: efficiently computing similarity all-matching
#@ Gaoping Zhu;Xuemin Lin;Ke Zhu;Wenjie Zhang;Jeffrey Xu Yu
#t 2012
#c 5
#% 378391
#% 407822
#% 408396
#% 765429
#% 769951
#% 810072
#% 864425
#% 960305
#% 1022279
#% 1022280
#% 1127380
#% 1181229
#% 1181230
#% 1206703
#% 1426577
#% 1523825
#% 1523900
#% 1523970
#% 1592340
#% 1594628
#! Given a query graph $q$ and a data graph G, computing all occurrences of q in G, namely exact all-matching, is fundamental in graph data analysis with a wide spectrum of real applications. It is challenging since even finding one occurrence of q in G (subgraph isomorphism test) is NP-Complete. Consider that in many real applications, exploratory queries from users are often inaccurate to express their real demands. In this paper, we study the problem of efficiently computing all approximate occurrences of q in G. Particularly, we study the problem of efficiently retrieving all matches of q in G with the number of possible missing edges bounded by a given threshold θ, namely similarity all-matching. The problem of similarity all-matching is harder than the problem of exact all-matching since it covers the problem of exact all-matching as a special case with θ = 0. In this paper, we develop a novel paradigm to conduct similarity all-matching. Specifically, we propose to use a minimal set QT of spanning trees in q to cover all connected subgraphs q' of q missing at most θ edges; that is, each q' is spanned by a spanning tree in QT. Then, we conduct exact all-matching for each spanning tree in QT to induce all similarity matches. A rigid theoretic analysis shows that our new search paradigm significantly reduces the times of conducting exact all-matching against the existing techniques. To further speed-up the computation, we develop new filtering, computation sharing, and search ordering techniques. Our comprehensive experiments on both real and synthetic datasets demonstrate that our techniques outperform the state of the art technique by 7 orders of magnitude.

#index 1770364
#* Locality-sensitive hashing scheme based on dynamic collision counting
#@ Junhao Gan;Jianlin Feng;Qiong Fang;Wilfred Ng
#t 2012
#c 5
#% 86786
#% 232764
#% 249321
#% 395135
#% 427199
#% 479973
#% 654466
#% 762054
#% 814646
#% 818938
#% 1022281
#% 1426417
#% 1581929
#% 1594627
#! Locality-Sensitive Hashing (LSH) and its variants are well-known methods for solving the c-approximate NN Search problem in high-dimensional space. Traditionally, several LSH functions are concatenated to form a "static" compound hash function for building a hash table. In this paper, we propose to use a base of m single LSH functions to construct "dynamic" compound hash functions, and define a new LSH scheme called Collision Counting LSH (C2LSH). If the number of LSH functions under which a data object o collides with a query object q is greater than a pre-specified collision threhold l, then o can be regarded as a good candidate of c-approximate NN of q. This is the basic idea of C2LSH. Our theoretical studies show that, by appropriately choosing the size of LSH function base m and the collision threshold l, C2LSH can have a guarantee on query quality. Notably, the parameter m is not affected by dimensionality of data objects, which makes C2LSH especially good for high dimensional NN search. The experimental studies based on synthetic datasets and four real datasets have shown that C2LSH outperforms the state of the art method LSB-forest in high dimensional space.

#index 1770365
#* Efficient external-memory bisimulation on DAGs
#@ Jelle Hellings;George H.L. Fletcher;Herman Haverkort
#t 2012
#c 5
#% 31484
#% 41684
#% 232758
#% 281655
#% 291299
#% 397360
#% 464883
#% 660000
#% 747091
#% 824663
#% 993951
#% 1015266
#% 1051088
#% 1181325
#% 1224939
#% 1350319
#% 1376720
#% 1407271
#% 1495309
#% 1526272
#% 1581817
#% 1587732
#! In this paper we introduce the first efficient external-memory algorithm to compute the bisimilarity equivalence classes of a directed acyclic graph (DAG). DAGs are commonly used to model data in a wide variety of practical applications, ranging from XML documents and data provenance models, to web taxonomies and scientific workflows. In the study of efficient reasoning over massive graphs, the notion of node bisimilarity plays a central role. For example, grouping together bisimilar nodes in an XML data set is the first step in many sophisticated approaches to building indexing data structures for efficient XPath query evaluation. To date, however, only internal-memory bisimulation algorithms have been investigated. As the size of real-world DAG data sets often exceeds available main memory, storage in external memory becomes necessary. Hence, there is a practical need for an efficient approach to computing bisimulation in external memory. Our general algorithm has a worst-case IO-complexity of O(Sort(|N| + |E|)), where |N| and |E| are the numbers of nodes and edges, resp., in the data graph and Sort(n) is the number of accesses to external memory needed to sort an input of size n. We also study specializations of this algorithm to common variations of bisimulation for tree-structured XML data sets. We empirically verify efficient performance of the algorithms on graphs and XML documents having billions of nodes and edges, and find that the algorithms can process such graphs efficiently even when very limited internal memory is available. The proposed algorithms are simple enough for practical implementation and use, and open the door for further study of external-memory bisimulation algorithms. To this end, the full open-source C++ implementation has been made freely available.

#index 1770366
#* Materialized view selection for XQuery workloads
#@ Asterios Katsifodimos;Ioana Manolescu;Vasilis Vassalos
#t 2012
#c 5
#% 210182
#% 397360
#% 397375
#% 464878
#% 479465
#% 479629
#% 480152
#% 480158
#% 570877
#% 576214
#% 654452
#% 733593
#% 765423
#% 824661
#% 824669
#% 824690
#% 864465
#% 875007
#% 1015260
#% 1015271
#% 1016134
#% 1022209
#% 1063494
#% 1194635
#% 1206683
#% 1224936
#% 1266955
#% 1540433
#% 1594601
#! The efficient processing of XQuery still poses significant challenges. A particularly effective technique to improve XQuery processing performance consists of using materialized views to answer queries. In this work, we consider the problem of choosing the best views to materialize within a given space budget in order to improve the performance of a query workload. The paper is the first to address the view selection problem for queries and views with value joins and multiple return nodes. The challenges we face stem from the expressive power and features of both the query and view languages and from the size of the search space of candidate views to materialize. While the general problem has prohibitive complexity, we propose and study a heuristic algorithm and demonstrate its superior performance compared to the state of the art.

#index 1770367
#* Analytic database technologies for a new kind of user: the data enthusiast
#@ Pat Hanrahan
#t 2012
#c 5
#! Analytics enables businesses to increase the efficiency of their activities and ultimately increase their profitability. As a result, it is one of the fastest growing segments of the database industry. There are two usages of the word analytics. The first refers to a set of algorithms and technologies, inspired by data mining, computational statistics, and machine learning, for supporting statistical inference and prediction. The second is equally important: analytical thinking. Analytical thinking is a structured approach to reasoning and decision making based on facts and data. Most of the recent work in the database community has focused on the first, the algorithmic and systems problems. The people behind these advances comprise a new generation of data scientists who have either the mathematical skills to develop advanced statistical models, or the computer skills to develop or implement scalable systems for processing large, complex datasets. The second aspect of analytics -- supporting the analytical thinker -- although equally important and challenging, has received much less attention. In this talk, I will describe recent advances in in making both forms of analytics accessible to a broader range of people, who I call data enthusiasts. A data enthusiast is an educated person who believes that data can be used to answer a question or solve a problem. These people are not mathematicians or programmers, and only know a bit of statistics. I'll review recent work on building easy-to-use, yet powerful, visual interfaces for working with data; and the analytical database technology needed to support these interfaces.

#index 1770368
#* Symbiosis in scale out networking and data management
#@ Amin Vahdat
#t 2012
#c 5
#! This talk highlights the symbiotic relationship between data management and networking through a study of two seemingly independent trends in the traditionally separate communities: large-scale data processing and software defined networking. First, data processing at scale increasingly runs across hundreds or thousands of servers. We show that balancing network performance with computation and storage is a prerequisite to both efficient and scalable data processing. We illustrate the need for scale out networking in support of data management through a case study of TritonSort, currently the record holder for several sorting benchmarks, including GraySort and JouleSort. Our TritonSort experience shows that disk-bound workloads require 10 Gb/s provisioned bandwidth to keep up with modern processors while emerging flash workloads require 40 Gb/s fabrics at scale. We next argue for the need to apply data management techniques to enable Software Defined Networking (SDN) and Scale Out Networking. SDN promises the abstraction of a single logical network fabric rather than a collection of thousands of individual boxes. In turn, scale out networking allows network capacity (ports, bandwidth) to be expanded incrementally, rather than by wholesale fabric replacement. However, SDN requires an extensible model of both static and dynamic network properties and the ability to deliver dynamic updates to a range of network applications in a fault tolerant and low latency manner. Doing so in networking environments where updates are typically performed by timer-based broadcasts and models are specified as comma-separated text files processed by one-off scripts presents interesting challenges. For example, consider an environment where applications from routing to traffic engineering to monitoring to intrusion/anomaly detection all essentially boil down to inserting, triggering and retrieving updates to/from a shared, extensible data store.

#index 1770369
#* Mob data sourcing
#@ Daniel Deutch;Tova Milo
#t 2012
#c 5
#% 384978
#% 464891
#% 577309
#% 976987
#% 1022258
#% 1063521
#% 1065099
#% 1072645
#% 1092014
#% 1206717
#% 1287539
#% 1292493
#% 1355029
#% 1426462
#% 1550748
#% 1581842
#% 1581851
#% 1594633
#! Crowdsourcing is an emerging paradigm that harnesses a mass of users to perform various types of tasks. We focus in this tutorial on a particular form of crowdsourcing, namely crowd (or mob) datasourcing whose goal is to obtain, aggregate or process data. We overview crowd datasourcing solutions in various contexts, explain the need for a principled solution, describe advances towards achieving such a solution, and highlight remaining gaps.

#index 1770370
#* Managing and mining large graphs: patterns and algorithms
#@ Christos Faloutsos;U. Kang
#t 2012
#c 5
#% 283833
#% 309749
#% 720278
#% 823342
#% 963669
#% 1083682
#% 1176970
#% 1318636
#% 1524264
#% 1566936
#% 1606050
#% 1607936
#% 1635120
#% 1663625
#% 1688472
#% 1710595
#! Graphs are everywhere: social networks, the World Wide Web, biological networks, and many more. The sizes of graphs are growing at unprecedented rate, spanning millions and billions of nodes and edges. What are the patterns in large graphs, spanning Giga, Tera, and heading toward Peta bytes? What are the best tools, and how can they help us solve graph mining problems? How do we scale up algorithms for handling graphs with billions of nodes and edges? These are exactly the goals of this tutorial. We start with the patterns in real-world static, weighted, and dynamic graphs. Then we describe important tools for large graph mining, including singular value decomposition, and Hadoop. Finally, we conclude with the design and the implementation of scalable graph mining algorithms on Hadoop. This tutorial is complementary to the related tutorial "Managing and Mining Large Graphs: Systems and Implementations".

#index 1770371
#* Managing and mining large graphs: systems and implementations
#@ Bin Shao;Haixun Wang;Yanghua Xiao
#t 2012
#c 5
#% 1023420
#% 1206699
#% 1245882
#% 1318636
#% 1350336
#% 1372657
#% 1426513
#% 1529319
#% 1770359
#% 1848107
#! We are facing challenges at all levels ranging from infrastructures to programming models for managing and mining large graphs. A lot of algorithms on graphs are ad-hoc in the sense that each of them assumes that the underlying graph data can be organized in a certain way that maximizes the performance of the algorithm. In other words, there is no standard graph systems based on which graph algorithms are developed and optimized. In response to this situation, a lot of graph systems have been proposed recently. In this tutorial, we discuss several representative systems. Still, we focus on providing perspectives from a variety of standpoints on the goals and the means for developing a general purpose graph system. We highlight the challenges posed by the graph data, the constraints of architectural design, the different types of application needs, and the power of different programming models that support such needs. This tutorial is complementary to the related tutorial "Managing and Mining Large Graphs: Patterns and Algorithms".

#index 1770372
#* Computational reproducibility: state-of-the-art, challenges, and database research opportunities
#@ Juliana Freire;Philippe Bonnet;Dennis Shasha
#t 2012
#c 5
#% 251720
#% 845350
#% 1061894
#% 1164019
#% 1164020
#% 1164023
#% 1286704
#% 1433978
#% 1615891
#% 1617921
#% 1776080
#% 1889740
#! Computational experiments have become an integral part of the scientific method, but reproducing, archiving, and querying them is still a challenge. The first barrier to a wider adoption is the fact that it is hard both for authors to derive a compendium that encapsulates all the components needed to reproduce a result and for reviewers to verify the results. In this tutorial, we will present a series of guidelines and, through hands-on examples, review existing tools to help authors create of reproducible results. We will also outline open problems and new directions for database-related research having to do with querying computational experiments.

#index 1770373
#* Database techniques for linked data management
#@ Andreas Harth;Katja Hose;Ralf Schenkel
#t 2012
#c 5
#% 769356
#% 845350
#% 851283
#% 1127431
#% 1127610
#% 1190676
#% 1241185
#% 1333435
#% 1366460
#% 1399937
#% 1399974
#% 1413091
#% 1413160
#% 1413162
#% 1433983
#% 1471588
#% 1597474
#% 1603788
#% 1616664
#% 1619380
#% 1641515
#! Linked Data refers to data published in accordance with a number of principles rooted in web standards. In the past few years we have witnessed a tremendous growth in Linked Data publishing on the web, leading to tens of billions of data items published online. Querying the data is a key functionality required to make use of the wealth of rich interlinked data. The goal of the tutorial is to introduce, motivate, and detail techniques for querying heterogeneous structured data from across the web. Our tutorial aims to introduce database researchers and practitioners to the new publishing paradigm on the web, and show how the abundance of data published as Linked Data can serve as fertile ground for database research and experimentation. As such, the tutorial focuses on applying database techniques to processing Linked Data, such as optimized indexing and query processing methods in the centralized setting as well as distributed approaches for querying. At the same time, we make the connection from Linked Data best practices to established technologies in distributed databases and the concept of Dataspaces and show differences as well as commonalities between the fields.

#index 1770374
#* Differential privacy in data publication and analysis
#@ Yin Yang;Zhenjie Zhang;Gerome Miklau;Marianne Winslett;Xiaokui Xiao
#t 2012
#c 5
#% 248030
#% 864412
#% 956511
#% 963241
#% 977011
#% 1029084
#% 1080356
#% 1190072
#% 1198224
#% 1214684
#% 1298824
#% 1426323
#% 1426454
#% 1426563
#% 1451189
#% 1451190
#% 1521653
#% 1521654
#% 1523886
#% 1581409
#% 1581862
#% 1581864
#% 1581865
#% 1595893
#% 1605968
#% 1606359
#% 1627567
#% 1670071
#% 1689683
#% 1730731
#% 1740518
#% 1770426
#% 1818428
#% 1846816
#% 1846817
#! Data privacy has been an important research topic in the security, theory and database communities in the last few decades. However, many existing studies have restrictive assumptions regarding the adversary's prior knowledge, meaning that they preserve individuals' privacy only when the adversary has rather limited background information about the sensitive data, or only uses certain kinds of attacks. Recently, differential privacy has emerged as a new paradigm for privacy protection with very conservative assumptions about the adversary's prior knowledge. Since its proposal, differential privacy had been gaining attention in many fields of computer science, and is considered among the most promising paradigms for privacy-preserving data publication and analysis. In this tutorial, we will motivate its introduction as a replacement for other paradigms, present the basics of the differential privacy model from a database perspective, describe the state of the art in differential privacy research, explain the limitations and shortcomings of differential privacy, and discuss open problems for future research.

#index 1770375
#* Automatic web-scale information extraction
#@ Philip Bohannon;Nilesh Dalvi;Yuval Filmus;Nori Jacoby;Sathiya Keerthi;Alok Kirpal
#t 2012
#c 5
#% 275915
#% 428148
#% 479807
#% 754068
#% 1022235
#% 1127393
#% 1131145
#% 1217114
#% 1275182
#% 1328133
#% 1328199
#% 1538764
#% 1560398
#! In this demonstration, we showcase the technologies that we are building at Yahoo! for Web-scale Information Extraction. Given any new Website, containing semi-structured information about a pre-specified set of schemas, we show how to populate objects in the corresponding schema by automatically extracting information from the Website.

#index 1770376
#* Just-in-time information extraction using extraction views
#@ Amr El-Helw;Mina H. Farid;Ihab F. Ilyas
#t 2012
#c 5
#% 572311
#% 782759
#% 815884
#% 1022288
#% 1183371
#% 1206687
#% 1206799
#% 1206862

#index 1770377
#* ColumbuScout: towards building local search engines over large databases
#@ Cody Hansen;Feifei Li
#t 2012
#c 5
#% 397418
#% 1015325
#% 1206665
#% 1206760
#% 1217235
#% 1328106
#% 1523941
#% 1594563
#% 1651606
#! In many database applications, search is still executed via form based query interfaces, which are then translated into SQL statements to find matching records. Ranking is usually not implemented unless users have explicitly indicated how to rank the matching records, e.g., in the ascending order of year. Often, this approach is neither intuitive nor user friendly (especially with many search fields in a query form). It also requires application developers to design schema-specific query forms and develop specific programs that understand these forms. In this work, we propose to demonstrate the ColumbuScout system that aims at quickly building and deploying a local search engine over one or more large databases. The ColumbuScout system adopts a search-engine-style approach for searches over local databases. It introduces its own indexing structures and storage designs, to improve its overall efficiency and scalability. We will demonstrate that it is simple for application developers to deploy ColumbuScout over any databases, and ColumbuScout is able to support search engine-like types of search over large databases efficiently and effectively.

#index 1770378
#* SOFIA SEARCH: a tool for automating related-work search
#@ Behzad Golshan;Theodoros Lappas;Evimaria Terzi
#t 2012
#c 5
#% 722904
#% 1418196
#! When working on a new project, researchers need to devote a significant amount of time and effort to surveying the relevant literature. This is required in order to gain expertise, evaluate the significance of their work and gain useful insights about a particular scientific domain. While necessary, relevant-work search is also a time-consuming and arduous process, requiring the continuous participation of the user. In this work, we introduce Sofia Search, a tool that fully automates the search and retrieval of the literature related to a topic. Given a seed of papers submitted by the user, Sofia Search searches the Web for candidate related papers, evaluates their relevance to the seed and downloads them for the user. The tool also provides modules for the evaluation and ranking of authors and papers, in the context of the retrieved papers. In the demo, we will demonstrate the functionality of our tool, by allowing users to use it via a simple and intuitive interface.

#index 1770379
#* RACE: real-time applications over cloud-edge
#@ Badrish Chandramouli;Joris Claessens;Suman Nath;Ivo Santos;Wenchao Zhou
#t 2012
#c 5
#% 330305
#% 1207015
#% 1594581
#! The Cloud-Edge topology - where multiple smart edge devices such as phones are connected to one another via the Cloud - is becoming ubiquitous. We demonstrate RACE, a novel framework and system for specifying and efficiently executing distributed real-time applications in the Cloud-Edge topology. RACE uses LINQ for StreamInsight to succinctly express a diverse suite of useful real-time applications. Further, it exploits the processing power of edge devices and the Cloud to partition and execute such queries in a distributed manner. RACE features a novel cost-based optimizer that efficiently finds the optimal placement, minimizing global communication cost while handling multi-level join queries and asymmetric network links.

#index 1770380
#* Partiqle: an elastic SQL engine over key-value stores
#@ Junichi Tatemura;Oliver Po;Wang-Pin Hsiung;Hakan Hacigümüş
#t 2012
#c 5
#% 273912
#% 287352
#% 1002142
#% 1063488
#% 1063554
#% 1426487
#% 1463413
#% 1712496
#! The demo features Partiqle, a SQL engine over key-value stores as a relational alternative for the recent procedural approaches to support OLTP workloads elastically. Based on our microsharding framework [12], it employs a declarative specification, called transaction classes, of constraints applied on the transactions in a workload. We demonstrate use of a transaction class in design and analysis of OLTP workloads. We then demonstrate live-scaling of our fully functioning system on a server cluster.

#index 1770381
#* JustMyFriends: full SQL, full transactional amenities, and access privacy
#@ Arthur Meacham;Dennis Shasha
#t 2012
#c 5
#% 271185
#% 325359
#% 397367
#% 593711
#% 593800
#% 963668
#% 1016189
#% 1198205
#% 1298793
#% 1386206
#% 1526996
#% 1627614
#% 1627627
#! A major obstacle to using Cloud services for many enterprises is the fear that the data will be stolen. Bringing the Cloud in-house is an incomplete solution to the problem because that implies that data center personnel as well as myriad repair personnel must be trusted. An ideal security solution would be to share data among precisely the people who should see it ("my friends") and nobody else. Encryption might seem to be an easy answer. Each friend could download the data, update it perhaps, and return it to a shared untrusted repository. But such a solution permits no concurrency and therefore no real sharing. JustMyFriends ensures sharing among friends without revealing unencrypted data to anyone outside of a circle of trust. In fact, non-friends (such as system administrators) see only encrypted blobs being added to a persistent store. JustMyFriends allows data sharing and full transactions. It supports the use of all SQL including stored procedures, updates, and arbitrary queries. Additionally, it provides full access privacy, preventing the host from discovering patterns or correlations in the user's data access behavior. The demonstration will show how friends in an unnamed government agency can coordinate the management of a spy network in a transactional fashion. Demo visitors will be able to play the roles of station chiefs and/or of troublemakers. As station chiefs, they will write their own transactions and queries, logout, login. As troublemakers, visitors will be able to play the role of a curious observer, kill client processes, and in general try to disrupt the system.

#index 1770382
#* Dynamic optimization of generalized SQL queries with horizontal aggregations
#@ Carlos Ordonez;Javier García-García;Zhibo Chen
#t 2012
#c 5
#% 907537
#% 1016212
#% 1019972
#% 1512993
#% 1755380
#! SQL presents limitations to return aggregations as tables with a horizontal layout. A user generally needs to write separate queries and data definition statements to combine transposition with aggregation. With that motivation in mind, we introduce horizontal aggregations, a complementary class of aggregations to traditional (vertical) SQL aggregations. The SQL syntax extension is minimal and it significantly enhances the expressive power and ease of use of SQL. Our proposed SQL extension blurs the boundary between row values and column names. We present a prototype query optimizer that can evaluate arbitrary nested queries combining filtering, joins and both classes of aggregations. Horizontal aggregations have many applications in ad-hoc querying, OLAP cube processing and data mining. We demonstrate query optimization of horizontal aggregations introduces new research challenges.

#index 1770383
#* ConsAD: a real-time consistency anomalies detector
#@ Kamal Zellag;Bettina Kemme
#t 2012
#c 5
#% 201869
#% 341704
#% 632091
#% 632092
#% 814649
#% 1594686
#! In this demonstration, we present ConsAD, a tool that detects consistency anomalies for arbitrary multi-tier applications that use lower levels of isolation than serializability. As the application is running, ConsAD detects and quantifies anomalies indicating exactly the transactions and data items involved. Furthermore, it classifies the detected anomalies into patterns showing the business methods involved as well as their occurrence frequency. ConsAD can guide designers to either choose an isolation level for which their application shows few anomalies or change their transaction design to avoid the anomalies. Its graphical interface shows detailed information about detected anomalies as they occur and analyzes their patterns as well as their distribution.

#index 1770384
#* Interactive performance monitoring of a composite OLTP and OLAP workload
#@ Anja Bog;Kai Sachs;Hasso Plattner
#t 2012
#c 5
#% 893175
#% 1217145
#% 1545223
#% 1556421
#% 1593673
#% 1891269
#! Online transaction processing (OLTP) and online analytical processing (OLAP) are thought of as two separate domains, despite sharing the same business data to operate on. This is the result of performance impairments encountered in the past when running on the same system, the workloads becoming ever more sophisticated, leading to contradictory optimization in database design. Recent developments in hardware and database systems are bringing forth research prototypes supporting mixed OLTP and OLAP workloads, challenging this separation. At the same time new benchmarks are proposed to assess these mixed workload systems. In the demonstration, we show an interactive performance monitor and benchmark driver developed for the Composite Benchmark for Transaction Processing and Reporting. The performance monitor allows us to directly determine the impact of changing shares within the workload and to interactively assess behavioral characteristics of different database systems under changing mixed workload conditions.

#index 1770385
#* Sindbad: a location-based social networking system
#@ Mohamed Sarwat;Jie Bao;Ahmed Eldawy;Justin J. Levandoski;Amr Magdy;Mohamed F. Mokbel
#t 2012
#c 5
#% 301777
#% 813966
#% 1426571
#% 1478994
#% 1846747
#% 1846818
#! This demo presents Sindbad; a location-based social networking system. Sindbad supports three new services beyond traditional social networking services, namely, location-aware news feed, location-aware recommender, and location-aware ranking. These new services not only consider social relevance for its users, but they also consider spatial relevance. Since location-aware social networking systems have to deal with large number of users, large number of messages, and user mobility, efficiency and scalability are important issues. To this end, Sindbad encapsulates its three main services inside the query processing engine of PostgreSQL. Usage and internal functionality of Sindbad, implemented with PostgreSQL and Google Maps API, are demonstrated through user (i.e., web/phone) and system analyzer GUI interfaces, respectively.

#index 1770386
#* MAQSA: a system for social analytics on news
#@ Sihem Amer-Yahia;Samreen Anjum;Amira Ghenai;Aysha Siddique;Sofiane Abbar;Sam Madden;Adam Marcus;Mohammed El-Haddad
#t 2012
#c 5
#% 46803
#% 722904
#% 1477588
#% 1560381
#% 1573368
#! We present MAQSA, a system for social analytics on news. MAQSA provides an interactive topic-centric dashboard that summarizes news articles and social activity (e.g., comments and tweets) around them. MAQSA helps editors and publishers in newsrooms understand user engagement and audience sentiment evolution on various topics of interest. It also helps news consumers explore public reaction on articles relevant to a topic and refine their exploration via related entities, topics, articles and tweets. Given a topic, e.g., "Gulf Oil Spill," or "The Arab Spring", MAQSA combines three key dimensions: time, geographic location, and topic to generate a detailed activity dashboard around relevant articles. The dashboard contains an annotated comment timeline and a social graph of comments. It utilizes commenters' locations to build maps of comment sentiment and topics by region of the world. Finally, to facilitate exploration, MAQSA provides listings of related entities, articles, and tweets. It algorithmically processes large collections of articles and tweets, and enables the dynamic specification of topics and dates for exploration. In this demo, participants will be invited to explore the social dynamics around articles on oil spills, the Libyan revolution, and the Arab Spring. In addition, participants will be able to define and explore their own topics dynamically.

#index 1770387
#* Surfacing time-critical insights from social media
#@ Bogdan Alexe;Mauricio A. Hernandez;Kirsten W. Hildrum;Rajasekar Krishnamurthy;Georgia Koutrika;Meenakshi Nagarajan;Haggai Roitman;Michal Shmueli-Scheuer;Ioana R. Stanoi;Chitra Venkatramani;Rohit Wagle
#t 2012
#c 5
#% 1384224
#% 1399966
#% 1400018
#% 1426611
#% 1581966
#% 1581974
#! We propose to demonstrate an end-to-end framework for leveraging time-sensitive and critical social media information for businesses. More specifically, we focus on identifying, structuring, integrating, and exposing timely insights that are essential to marketing services and monitoring reputation over social media. Our system includes components for information extraction from text, entity resolution and integration, analytics, and a user interface.

#index 1770388
#* Taagle: efficient, personalized search in collaborative tagging networks
#@ Silviu Maniu;Bogdan Cautis
#t 2012
#c 5
#% 333854
#% 956552
#% 1035588
#% 1074070
#% 1074116
#% 1127592
#% 1217225
#% 1227601
#% 1292590
#% 1594584
#% 1667787
#! We demonstrate the Taagle system for top-k retrieval in social tagging systems (also known as folksonomies). The general setting is the following: users form a weighted social network, which may reflect friendship, similarity, or trust; items from a public pool of items (e.g., URLs, blogs, photos, documents) are tagged by users with keywords; users search for the top-k items having certain tags. Going beyond a classic search paradigm where data is decoupled from the users querying it, users can now act both as producers and seekers of information. Hence finding the most relevant items in response to a query should be done in a network-aware manner: items tagged by users who are closer (more similar) to the seeker should be given more weight than items tagged by distant users. We illustrate with Taagle novel algorithms and a general approach that has the potential to scale to current applications, in an online context where the social network, the tagging data and even the seekers' search ingredients can change at any moment. We also illustrate possible design choices for providing users a fully-personalized and customizable search interface. By this interface, they can calibrate how social proximity is computed (for example, with respect to similarity in tagging actions), how much weight the social score of tagging actions should have in the result build-up, or the criteria by which the user network should be explored. In order to further reduce running time, seekers are given the possibility to chose between exact or approximate answers, and can benefit from cached results of previous queries (materialized views).

#index 1770389
#* PrefDB: bringing preferences closer to the DBMS
#@ Anastasios Arvanitis;Georgia Koutrika
#t 2012
#c 5
#% 731407
#% 745519
#% 810018
#% 994017
#% 1846739
#! In this demonstration we present a preference-aware relational query answering system, termed PrefDB. The key novelty of PrefDB is the use of an extended relational data model and algebra that allow expressing different flavors of preferential queries. Furthermore, unlike existing approaches that either treat the DBMS as a black box or require modifications of the database core, PrefDB's hybrid implementation enables operator-level query optimizations without being obtrusive to the database engine. We showcase the flexibility and efficiency of PrefDB using PrefDBAdmin, a graphical tool that we have built aiming at assisting application designers in the task of building, testing and tuning queries with preferences.

#index 1770390
#* Auto-completion learning for XML
#@ Serge Abiteboul;Yael Amsterdamer;Tova Milo;Pierre Senellart
#t 2012
#c 5
#% 1022220
#% 1083734
#% 1127389
#% 1488676
#% 1538772
#% 1538773
#% 1818415
#! Editing an XML document manually is a complicated task. While many XML editors exist in the market, we argue that some important functionalities are missing in all of them. Our goal is to makes the editing task simpler and faster. We present ALEX (Auto-completion Learning Editor for XML), an editor that assists the users by providing intelligent auto-completion suggestions. These suggestions are adapted to the user needs, simply by feeding ALEX with a set of example XML documents to learn from. The suggestions are also guaranteed to be compliant with a given XML schema, possibly including integrity constraints. To fulfill this challenging goal, we rely on novel, theoretical foundations by us and others, which are combined here in a system for the first time.

#index 1770391
#* Logos: a system for translating queries into narratives
#@ Andreas Kokkalis;Panagiotis Vagenas;Alexandros Zervakis;Alkis Simitsis;Georgia Koutrika;Yannis Ioannidis
#t 2012
#c 5
#% 443465
#% 1044471
#% 1549891
#% 1698603
#! This paper presents Logos, a system that provides natural language translations for relational queries expressed in SQL. Our translation mechanism is based on a graph-based approach to the query translation problem. We represent various forms of structured queries as directed graphs and we annotate the graph edges with template labels using an extensible template mechanism. Logos uses different graph traversal strategies for efficiently exploring these graphs and composing textual query descriptions. The audience may interactively explore Logos using various database schemata and issuing either sample or ad hoc queries.

#index 1770392
#* PAnG: finding patterns in annotation graphs
#@ Philip Anderson;Andreas Thor;Joseph Benik;Louiqa Raschid;Maria Esther Vidal
#t 2012
#c 5
#% 1063501
#% 1641479
#% 1641522
#% 1715022
#! Annotation graph datasets are a natural representation of scientific knowledge. They are common in the life sciences and health sciences, where concepts such as genes, proteins or clinical trials are annotated with controlled vocabulary terms from ontologies. We present a tool, PAnG (Patterns in Annotation Graphs), that is based on a complementary methodology of graph summarization and dense subgraphs. The elements of a graph summary correspond to a pattern and its visualization can provide an explanation of the underlying knowledge. Scientists can use PAnG to develop hypotheses and for exploration.

#index 1770393
#* VizDeck: self-organizing dashboards for visual analytics
#@ Alicia Key;Bill Howe;Daniel Perry;Cecilia Aragon
#t 2012
#c 5
#% 18610
#% 85700
#% 180947
#% 270637
#% 434617
#% 772616
#% 960347
#% 1145301
#% 1147592
#% 1169596
#% 1206840
#% 1615884
#! We present VizDeck, a web-based tool for exploratory visual analytics of unorganized relational data. Motivated by collaborations with domain scientists who search for complex patterns in hundreds of data sources simultaneously, VizDeck automatically recommends appropriate visualizations based on the statistical properties of the data and adopts a card game metaphor to help organize the recommended visualizations into interactive visual dashboard applications in seconds with zero programming. The demonstration allows users to derive, share, and permanently store their own dashboard from hundreds of real science datasets using a production system deployed at the University of Washington.

#index 1770394
#* Kaizen: a semi-automatic index advisor
#@ Ivo Jimenez;Huascar Sanchez;Quoc Trung Tran;Neoklis Polyzotis
#t 2012
#c 5
#% 261358
#% 810111
#% 875027
#% 1016220
#% 1016221
#% 1127352
#% 1206953
#% 1207103
#% 1328212

#index 1770395
#* Shark: fast data analysis using coarse-grained distributed memory
#@ Cliff Engle;Antonio Lupher;Reynold Xin;Matei Zaharia;Michael J. Franklin;Scott Shenker;Ion Stoica
#t 2012
#c 5
#% 1217159
#% 1583709
#% 1783374
#! Shark is a research data analysis system built on a novel coarse-grained distributed shared-memory abstraction. Shark marries query processing with deep data analysis, providing a unified system for easy data manipulation using SQL and pushing sophisticated analysis closer to data. It scales to thousands of nodes in a fault-tolerant manner. Shark can answer queries 40X faster than Apache Hive and run machine learning programs 25X faster than MapReduce programs in Apache Hadoop on large datasets.

#index 1770396
#* Exploiting MapReduce-based similarity joins
#@ Yasin N. Silva;Jason M. Reed
#t 2012
#c 5
#% 731409
#% 864392
#% 960326
#% 963669
#% 1054481
#% 1426543
#! Cloud enabled systems have become a crucial component to efficiently process and analyze massive amounts of data. One of the key data processing and analysis operations is the Similarity Join, which retrieves all data pairs whose distances are smaller than a pre-defined threshold ∈. Even though multiple algorithms and implementation techniques have been proposed for Similarity Joins, very little work has addressed the study of Similarity Joins for cloud systems. This paper presents MRSimJoin, a multi-round MapReduce based algorithm to efficiently solve the Similarity Join problem. MRSimJoin efficiently partitions and distributes the data until the subsets are small enough to be processed in a single node. The proposed algorithm is general enough to be used with data that lies in any metric space. We have implemented MRSimJoin in Hadoop, a highly used open-source cloud system. We show how this operation can be used in multiple real-world data analysis scenarios with multiple data types and distance functions. Particularly, we show the use of MRSimJoin to identify similar images represented as feature vectors, and similar publications in a bibliographic database. We also show how MRSimJoin scales in each scenario when important parameters, e.g., ∈, data size and number of cluster nodes, increase. We demonstrate the execution of MRSimJoin queries using an Amazon Elastic Compute Cloud (EC2) cluster.

#index 1770397
#* GLADE: big data analytics made easy
#@ Yu Cheng;Chengjie Qin;Florin Rusu
#t 2012
#c 5
#% 963669
#% 1063553
#% 1217159
#% 1217232
#% 1328066
#% 1426545
#! We present GLADE, a scalable distributed system for large scale data analytics. GLADE takes analytical functions expressed through the User-Defined Aggregate (UDA) interface and executes them efficiently on the input data. The entire computation is encapsulated in a single class which requires the definition of four methods. The runtime takes the user code and executes it right near the data by taking full advantage of the parallelism available inside a single machine as well as across a cluster of computing nodes. The demonstration has two goals. First, it presents the architecture of GLADE and how processing is done by using a series of analytical functions. Second, it compares GLADE with two different classes of systems for data analytics: a relational database (PostgreSQL) enhanced with UDAs and Map-Reduce (Hadoop). We show how the analytical functions are coded into each of these systems (for Map-Reduce, we use both Java code as well as Pig Latin) and compare their expressiveness, scalability, and running time efficiency.

#index 1770398
#* ReStore: reusing results of MapReduce jobs in pig
#@ Iman Elghandour;Ashraf Aboulnaga
#t 2012
#c 5
#% 963669
#% 1063553
#% 1730737
#! Analyzing large scale data has become an important activity for many organizations, and is now facilitated by the MapReduce programming and execution model and its implementations, most notably Hadoop. Query languages such as Pig Latin, Hive, and Jaql make it simpler for users to express complex analysis tasks, and the compilers of these languages translate these complex tasks into workflows of MapReduce jobs. Each job in these workflows reads its input from the distributed file system used by the MapReduce system (e.g., HDFS in the case of Hadoop) and produces output that is stored in this distributed file system. This output is then read as input by the next job in the workflow. The current practice is to delete these intermediate results from the distributed file system at the end of executing the workflow. It would be more useful if these intermediate results can be stored and reused in future workflows. We demonstrate ReStore, an extension to Pig that enables it to manage storage and reuse of intermediate results of the MapReduce workflows executed in the Pig data analysis system. ReStore matches input workflows of MapReduce jobs with previously executed jobs and rewrites these workflows to reuse the stored results of the matched jobs. ReStore also creates additional reuse opportunities by materializing and reserving the output of query execution operators that are executed within a MapReduce job. In this demonstration we showcase the MapReduce jobs and sub-jobs recommended by ReStore for a given Pig query, the rewriting of input queries to reuse stored intermediate results, and a what-if analysis of the effectiveness of reusing stored outputs of previously executed jobs.

#index 1770399
#* Clydesdale: structured data processing on hadoop
#@ Andrey Balmin;Tim Kaldewey;Sandeep Tata
#t 2012
#c 5
#% 479821
#% 824697
#% 1022230
#% 1023420
#% 1217159
#% 1328186
#% 1523841
#% 1581407
#% 1581926
#% 1798376
#% 1798411
#! There have been several recent proposals modifying Hadoop, radically changing the storage organization or query processing techniques to obtain good performance for structured data processing. We will showcase Clydesdale, a research prototype for structured data processing on Hadoop that can achieve dramatic performance improvements over existing solutions, without any changes to the underlying MapReduce implementation. Clydesdale achieves this through a novel synthesis of several techniques from the database literature and carefully adapting them to the Hadoop environment. On the star schema benchmark, we show that Clydesdale is on average 38x faster than Hive, the dominant approach for structured data processing on Hadoop today. To the best of our knowledge, Clydesdale is the fastest solution for processing workloads on structured data sets that fit a star schema on Hadoop. Attendees will be able to run queries on the data from the star schema benchmark on a remote Hadoop cluster with Clydesdale and Hive installed, and get a breakdown of the time taken to execute the query. Attendees will also be able to pose their own queries using ClyQL -- a novel embedded DSL in Scala that can be used to rapidly prototype star join queries. With this demonstration, we hope to convince the attendees that unlike previously thought, Hadoop can indeed efficiently support structured data processing.

#index 1770400
#* Tiresias: a demonstration of how-to queries
#@ Alexandra Meliou;Yisong Song;Dan Suciu
#t 2012
#c 5
#% 960293
#% 1063521
#% 1770347
#! In this demo, we will present Tiresias, the first how-to query engine. How-to queries represent fundamental data analysis questions of the form: "How should the input change in order to achieve the desired output". They exemplify an important Reverse Data Management problem: solving constrained optimization problems over data residing in a DBMS. Tiresias, named after the mythical oracle of Thebes, has complex under-workings, but includes a simple interface that allows users to load datasets and interactively design optimization problems by simply selecting actions, key performance indicators, and objectives. The user choices are translated into a declarative query, which is then processed by Tiresias and translated into a Mixed Integer Program: we then use an MIP solver to find a solution. The solution is then presented to the user as an interactive data instance. The user can provide feedback by rejecting certain tuples and/or values. Then, based on the user feedback, Tiresias automatically refines the how-to query and presents a new set of results.

#index 1770401
#* AstroShelf: understanding the universe through scalable navigation of a galaxy of annotations
#@ Panayiotis Neophytou;Roxana Gheorghiu;Rebecca Hachey;Timothy Luciani;Di Bao;Alexandros Labrinidis;Elisabeta G. Marai;Panos K. Chrysanthis
#t 2012
#c 5
#% 879809
#% 1102997
#% 1581979
#! This demo presents AstroShelf, our on-going effort to enable astrophysicists to collaboratively investigate celestial objects using data originating from multiple sky surveys, hosted at different sites. The AstroShelf platform combines database and data stream, workflow and visualization technologies to provide a means for querying and displaying telescope images (in a Google Sky manner), visualizations of spectrum data, and for managing annotations. In addition to the user interface, AstroShelf supports a programmatic interface (available as a web service), which allows astrophysicists to incorporate functionality from AstroShelf in their own programs. A key feature is Live Annotations which is the detection and delivery of events or annotations to users in real-time, based on their profiles. We demonstrate the capabilities of AstroShelf through real end-user exploration scenarios (with participation from "stargazers" in the audience), in the presence of simulated annotation workloads executed through web services.

#index 1770402
#* OPAvion: mining and visualization in large graphs
#@ Leman Akoglu;Duen Horng Chau;U. Kang;Danai Koutra;Christos Faloutsos
#t 2012
#c 5
#% 1318636
#% 1524264
#% 1573362
#% 1607936
#% 1710593
#% 1710595
#! Given a large graph with millions or billions of nodes and edges, like a who-follows-whom Twitter graph, how do we scalably compute its statistics, summarize its patterns, spot anomalies, visualize and make sense of it? We present OPAvion, a graph mining system that provides a scalable, interactive workflow to accomplish these analysis tasks. OPAvion consists of three modules: (1) The Summarization module (Pegasus) operates off-line on massive, disk-resident graphs and computes graph statistics, like PageRank scores, connected components, degree distribution, triangles, etc.; (2) The Anomaly Detection module (OddBall) uses graph statistics to mine patterns and spot anomalies, such as nodes with many contacts but few interactions with them (possibly telemarketers); (3) The Interactive Visualization module (Apolo) lets users incrementally explore the graph, starting with their chosen nodes or the flagged anomalous nodes; then users can expand to the nodes' vicinities, label them into categories, and thus interactively navigate the interesting parts of the graph. In our demonstration, we invite our audience to interact with OPAvion and try out its core capabilities on the Stack Overflow Q&A graph that describes over 6 million questions and answers among 650K users.

#index 1770403
#* CloudAlloc: a monitoring and reservation system for compute clusters
#@ Enrico Iori;Alkis Simitsis;Themis Palpanas;Kevin Wilkinson;Stavros Harizopoulos
#t 2012
#c 5
#% 808595
#% 1120545
#% 1203972
#! Cloud computing has emerged as a promising environment capable of providing flexibility, scalability, elasticity, fail-over mechanisms, high availability, and other important features to applications. Compute clusters are relatively easy to create and use, but tools to effectively share cluster resources are lacking. CloudAlloc addresses this problem and schedules workloads to cluster resources using allocation algorithms that can be easily changed according to the objectives of the enterprise. It also monitors resource utilization and thus, provides accountability for actual usage. CloudAlloc is a lightweight, flexible, easy-to-use tool for cluster resource allocation that has also proved useful as a research platform. We demonstrate its features and also discuss its allocation algorithms that minimize power usage. CloudAlloc was implemented and is in use at HP Labs.

#index 1770404
#* TIRAMOLA: elastic nosql provisioning through a cloud management platform
#@ Ioannis Konstantinou;Evangelos Angelou;Dimitrios Tsoumakos;Christina Boumpouka;Nectarios Koziris;Spyros Sioutas
#t 2012
#c 5
#% 384911
#% 1217159
#% 1232669
#% 1246360
#% 1400975
#% 1426489
#% 1426550
#% 1428142
#% 1459252
#% 1594595
#% 1594596
#% 1642260
#! NoSQL databases focus on analytical processing of large scale datasets, offering increased scalability over commodity hardware. One of their strongest features is elasticity, which allows for fairly portioned premiums and high-quality performance. Yet, the process of adaptive expansion and contraction of resources usually involves a lot of manual effort, often requiring the definition of the conditions for scaling up or down to be provided by the users. To date, there exists no open-source system for automatic resizing of NoSQL clusters. In this demonstration, we present TIRAMOLA, a modular, cloud-enabled framework for monitoring and adaptively resizing NoSQL clusters. Our system incorporates a decision-making module which allows for optimal cluster resize actions in order to maximize any quantifiable reward function provided together with life-long adaptation to workload or infrastructural changes. The audience will be able to initiate HBase clusters of various sizes and apply varying workloads through multiple YCSB clients. The attendees will be able to watch, in real-time, the system perform automatic VM additions and removals as well as how cluster performance metrics change relative to the optimization parameters of their choice.

#index 1770405
#* Amazon dynamoDB: a seamlessly scalable non-relational database service
#@ Swaminathan Sivasubramanian
#t 2012
#c 5
#! Reliability and scalability of an application is dependent on how its application state is managed. To run applications at massive scale requires one to operate datastores that can scale to operate seamlessly across thousands of servers and can deal with various failure modes such as server failures, datacenter failures and network partitions. The goal of Amazon DynamoDB is to eliminate this complexity and operational overhead for our customers by offering a seamlessly scalable database service. In this talk, I will talk about how developers can build applications on DynamoDB without having to deal with the complexity of operating a large scale database.

#index 1770406
#* Efficient transaction processing in SAP HANA database: the end of a column store myth
#@ Vishal Sikka;Franz Färber;Wolfgang Lehner;Sang Kyun Cha;Thomas Peh;Christof Bornhövd
#t 2012
#c 5
#% 136740
#% 824804
#% 893176
#% 1016215
#% 1022298
#% 1211646
#% 1217145
#% 1328141
#% 1372742
#% 1477935
#% 1486653
#% 1490163
#% 1614904
#% 1615897
#% 1667313
#! The SAP HANA database is the core of SAP's new data management platform. The overall goal of the SAP HANA database is to provide a generic but powerful system for different query scenarios, both transactional and analytical, on the same data representation within a highly scalable execution environment. Within this paper, we highlight the main features that differentiate the SAP HANA database from classical relational database engines. Therefore, we outline the general architecture and design criteria of the SAP HANA in a first step. In a second step, we challenge the common belief that column store data structures are only superior in analytical workloads and not well suited for transactional workloads. We outline the concept of record life cycle management to use different storage formats for the different stages of a record. We not only discuss the general concept but also dive into some of the details of how to efficiently propagate records through their life cycle and moving database entries from write-optimized to read-optimized storage formats. In summary, the paper aims at illustrating how the SAP HANA database is able to efficiently work in analytical as well as transactional workload environments.

#index 1770407
#* Walnut: a unified cloud object store
#@ Jianjun Chen;Chris Douglas;Michi Mutsuzaki;Patrick Quaid;Raghu Ramakrishnan;Sriram Rao;Russell Sears
#t 2012
#c 5
#% 29590
#% 36103
#% 723279
#% 963666
#% 963669
#% 978404
#% 998845
#% 1061758
#% 1127560
#% 1217213
#% 1400975
#% 1426489
#% 1426589
#% 1468530
#% 1518201
#% 1538766
#% 1625041
#% 1770337
#! Walnut is an object-store being developed at Yahoo! with the goal of serving as a common low-level storage layer for a variety of cloud data management systems including Hadoop (a MapReduce system), MObStor (a multimedia serving system), and PNUTS (an extended key-value serving system). Thus, a key performance challenge is to meet the latency and throughput requirements of the wide range of workloads commonly observed across these diverse systems. The motivation for Walnut is to leverage a carefully optimized low-level storage system, with support for elasticity and high-availability, across all of Yahoo!'s data clouds. This would enable sharing of hardware resources across hitherto siloed clouds of different types, offering greater potential for intelligent load balancing and efficient elastic operation, and simplify the operational tasks related to data storage. In this paper, we discuss the motivation for unifying different storage clouds, describe the requirements of a common storage layer, and present the Walnut design, which uses a quorum-based replication protocol and one-hop direct client access to the data in most regular operations. A unique contribution of Walnut is its hybrid object strategy, which efficiently supports both small and large objects. We present experiments based on both synthetic and real data traces, showing that Walnut works well over a wide range of workloads, and can indeed serve as a common low-level storage layer across a range of cloud systems.

#index 1770408
#* The value of social media data in enterprise applications
#@ Shivakumar Vaithyanathan
#t 2012
#c 5
#! Social media is an interactive vehicle for communication accessed on a daily basis by hundreds of millions of people. Unlike conventional media, which is a one-way street for information exchange, social media enables people to write content as well as provide feedback and recommend content to other users. There are multiple enterprise applications, such as customer retention, new customer acquisition, campaign management and lead generation that can significantly benefit from the consumer insights hidden in the massive amounts of social media content. Defining, extracting and representing entities such as people, organization and products, and their inter-relationships enables the building of comprehensive consumer profiles that can be leveraged in enterprise applications. Building these social media profiles requires a combination of text and entity analytics, while the utilization of such profiles makes heavy use of statistical models and machine learning. In this talk I will briefly describe the work in progress at IBM Research - Almaden on how such consumer insights, both at the level of an individual and at the level of appropriate micro-segments, can be used in enterprise applications in companies ranging from movie studios to financial services and insurance companies. I will also provide a brief overview of text, entity and statistical modeling tools that can operate in a distributed fashion over very large amounts of data.

#index 1770409
#* Anatomy of a gift recommendation engine powered by social media
#@ Yannis Pavlidis;Madhusudan Mathihalli;Indrani Chakravarty;Arvind Batra;Ron Benson;Ravi Raj;Robert Yau;Mike McKiernan;Venky Harinarayan;Anand Rajaraman
#t 2012
#c 5
#! More and more people conduct their shopping online [1], especially during the holiday season [2]. Shopping online offers a lot of convenience, including the luxury of shopping from home, the ease of research, better prices, and in many cases access to unique products not available in stores. One of the facets of shopping is gifting. Gifting may be the act of giving a present to somebody because of an event (e.g., birthday) or occasion (e.g., house warming party). People may also treat themselves or loved ones to a gift. Regardless of the occasion or the reason for gifting, there is often one common denominator: delight the receiver. The pursuit of delight can cause a great deal of stress and also be extremely time consuming as many people today either already have everything, or have easy access to everything. The @WalmartLabs Gift Recommendation Engine and its first application, Shopycat, which is a gift finder application on Facebook, aim to find the right and "wow" gifts much easier and quicker than ever before, by taking into account social media interactions. In this paper we will begin by describing the Shopycat Social Gift Finder Facebook application. Next, we describe the components of the engine. Finally, we discuss the metrics used to evaluate the engine. Building such a gift recommendation engine raises many challenges, in inferring user interests, computing the giftability of a product and an interest, and processing the big and fast data associated with social media. We briefly discuss our solutions to these challenges. Overall, our gift recommendation engine is an example that illustrates social commerce, a powerful emerging trend in e-commerce, and a major focus of @WalmartLabs.

#index 1770410
#* Designing a scalable crowdsourcing platform
#@ Chris Van Pelt;Alex Sorokin
#t 2012
#c 5
#! Computers are extremely efficient at crawling, storing and processing huge volumes of structured data. They are great at exploiting link structures to generate valuable knowledge. Yet there are plenty of data processing tasks that are difficult today. Labeling sentiment, moderating images, and mining structured content from the web are still too hard for computers. Automated techniques can get us a long way in some of those, but human inteligence is required when an accurate decision is ultimately important. In many cases that decision is easy for people and can be made quickly - in a few seconds to few minutes. By creating millions of simple online tasks we create a distributed computing machine. By shipping the tasks to millions of contributers around the globe, we make this human computer available 24/7 to make important decisions about your data. In this talk, I will describe our approach to designing CrowdFlower - a scalable crowdsourcing platform - as it evolved over the last 4 years. We think about crowdsourcing in terms of Quality, Cost and Speed. They are the ultimate design objectives of a human computer. Unfortunately, we can't have all 3. A general price-constrained task requiring 99.9% accuracy and 10 minute turnaround is not possible today. I will discuss design decisions behind CrowdFlower that allow us to pursue any two of these objectives. I will briefly present examples of common crowdsourced tasks and tools built into the platform to make the design of complex tasks easy, tools such as CrowdFlower Markup Language(CML). Quality control is the single most important challenge in Crowdsourcing. To enable an unidentified crowd of people to produce meaningful work, we must be certain that we can filter out bad contributors and produce high quality output. Initially we only used consensus. As the diversity and size of our crowd grew, so did the number of people attempting fraud. CrowdFlower developed "Gold standard" to block attempts of fraud. The use of gold allowed us to train contributors for the details of specific domains. By defining expected responses for a subset of the work and providing explanations of why a given response was expected, we are able distribute tasks to an ever-expanding anonymous workforce without sacrificing quality.

#index 1770411
#* Query optimization in microsoft SQL server PDW
#@ Srinath Shankar;Rimma Nehme;Josep Aguilar-Saborit;Andrew Chung;Mostafa Elhemali;Alan Halverson;Eric Robinson;Mahadevan Sankara Subramanian;David DeWitt;César Galindo-Legaria
#t 2012
#c 5
#% 287647
#% 565457
#% 673778
#% 1070308
#% 1523929
#% 1581943
#! In recent years, Massively Parallel Processors have increasingly been used to manage and query vast amounts of data. Dramatic performance improvements are achieved through distributed execution of queries across many nodes. Query optimization for such system is a challenging and important problem. In this paper we describe the Query Optimizer inside the SQL Server Parallel Data Warehouse product (PDW QO). We leverage existing QO technology in Microsoft SQL Server to implement a cost-based optimizer for distributed query execution. By properly abstracting metadata we can readily reuse existing logic for query simplification, space exploration and cardinality estimation. Unlike earlier approaches that simply parallelize the best serial plan, our optimizer considers a rich space of execution alternatives, and picks one based on a cost-model for the distributed execution environment. The result is a high-quality, effective query optimizer for distributed query processing in an MPP.

#index 1770412
#* F1: the fault-tolerant distributed RDBMS supporting google's ad business
#@ Jeff Shute;Mircea Oancea;Stephan Ellner;Ben Handy;Eric Rollins;Bart Samwel;Radek Vingralek;Chad Whipkey;Xin Chen;Beat Jegerlehner;Kyle Littlefield;Phoenix Tong
#t 2012
#c 5
#! Many of the services that are critical to Google's ad business have historically been backed by MySQL. We have recently migrated several of these services to F1, a new RDBMS developed at Google. F1 implements rich relational database features, including a strictly enforced schema, a powerful parallel SQL query engine, general transactions, change tracking and notification, and indexing, and is built on top of a highly distributed storage system that scales on standard hardware in Google data centers. The store is dynamically sharded, supports transactionally-consistent replication across data centers, and is able to handle data center outages without data loss. The strong consistency properties of F1 and its storage system come at the cost of higher write latencies compared to MySQL. Having successfully migrated a rich customer-facing application suite at the heart of Google's ad business to F1, with no downtime, we will describe how we restructured schema and applications to largely hide this increased latency from external users. The distributed nature of F1 also allows it to scale easily and to support significantly higher throughput for batch workloads than a traditional RDBMS. With F1, we have built a novel hybrid system that combines the scalability, fault tolerance, transparent sharding, and cost benefits so far available only in "NoSQL" systems with the usability, familiarity, and transactional guarantees expected from an RDBMS.

#index 1770413
#* Oracle in-database hadoop: when mapreduce meets RDBMS
#@ Xueyuan Su;Garret Swart
#t 2012
#c 5
#% 645385
#% 963669
#% 1063553
#% 1328059
#% 1328095
#% 1328186
#% 1429285
#% 1581944
#! Big data is the tar sands of the data world: vast reserves of raw gritty data whose valuable information content can only be extracted at great cost. MapReduce is a popular parallel programming paradigm well suited to the programmatic extraction and analysis of information from these unstructured Big Data reserves. The Apache Hadoop implementation of MapReduce has become an important player in this market due to its ability to exploit large networks of inexpensive servers. The increasing importance of unstructured data has led to the interest in MapReduce and its Apache Hadoop implementation, which has led to the interest of data processing vendors in supporting this programming style. Oracle RDBMS has had support for the MapReduce paradigm for many years through the mechanism of user defined pipelined table functions and aggregation objects. However, such support has not been Hadoop source compatible. Native Hadoop programs needed to be rewritten before becoming usable in this framework. The ability to run Hadoop programs inside the Oracle database provides a versatile solution to database users, allowing them use programming skills they may already possess and to exploit the growing Hadoop eco-system. In this paper, we describe a prototype of Oracle In-Database Hadoop that supports the running of native Hadoop applications written in Java. This implementation executes Hadoop applications using the efficient parallel capabilities of the Oracle database and a subset of the Apache Hadoop infrastructure. This system's target audience includes both SQL and Hadoop users. We discuss the architecture and design, and in particular, demonstrate how MapReduce functionalities are seamlessly integrated within SQL queries. We also share our experience in building such a system within Oracle database and follow-on topics that we think are promising areas for exploration.

#index 1770414
#* TAO: how facebook serves the social graph
#@ Venkateshwaran Venkataramani;Zach Amsden;Nathan Bronson;George Cabrera III;Prasad Chakka;Peter Dimov;Hui Ding;Jack Ferris;Anthony Giardullo;Jeremy Hoon;Sachin Kulkarni;Nathan Lawrence;Mark Marchukov;Dmitri Petrov;Lovro Puzar
#t 2012
#c 5
#! Over 800 million people around the world share their social interactions with friends on Facebook, providing a rich body of information referred to as the social graph. In this talk, I describe how we model and serve this graph. Our model uses typed nodes (fbobjects) and edges (associations) to express the relationships and actions that happen on Facebook. We access the graph via a simple API that provides queries over the set of same-typed associations leaving an object. We have found this API to be both sufficiently expressive and amenable to a scalable implementation. In the last segment of the talk I describe the design of TAO, our graph data store. TAO is a distributed implementation of the fbobject and association API that has been serving production traffic at Facebook for more than 2 years.

#index 1770415
#* Large-scale machine learning at twitter
#@ Jimmy Lin;Alek Kolcz
#t 2012
#c 5
#% 209021
#% 400847
#% 742990
#% 815796
#% 891559
#% 946521
#% 963669
#% 978404
#% 1063553
#% 1127964
#% 1189164
#% 1264744
#% 1275693
#% 1328060
#% 1328066
#% 1328186
#% 1467704
#% 1470631
#% 1523858
#% 1581926
#% 1581946
#% 1586685
#% 1594588
#% 1594623
#% 1604467
#% 1605938
#% 1605943
#% 1700911
#! The success of data-driven solutions to difficult problems, along with the dropping costs of storing and processing massive amounts of data, has led to growing interest in large-scale machine learning. This paper presents a case study of Twitter's integration of machine learning tools into its existing Hadoop-based, Pig-centric analytics platform. We begin with an overview of this platform, which handles "traditional" data warehousing and business intelligence tasks for the organization. The core of this work lies in recent Pig extensions to provide predictive analytics capabilities that incorporate machine learning, focused specifically on supervised classification. In particular, we have identified stochastic gradient descent techniques for online learning and ensemble methods as being highly amenable to scaling out to large amounts of data. In our deployed solution, common machine learning tasks such as data sampling, feature generation, training, and testing can be accomplished directly in Pig, via carefully crafted loaders, storage functions, and user-defined functions. This means that machine learning is just another Pig script, which allows seamless integration with existing infrastructure for data management, scheduling, and monitoring in a production environment, as well as access to rich libraries of user-defined functions and the materialized output of other scripts.

#index 1770416
#* Recurring job optimization in scope
#@ Nicolas Bruno;Sameer Agarwal;Srikanth Kandula;Bing Shi;Ming-Chuan Wu;Jingren Zhou
#t 2012
#c 5
#% 963669
#% 1127559

#index 1770417
#* Dynamic workload driven data integration in tableau
#@ Kristi Morton;Ross Bunker;Jock Mackinlay;Robert Morton;Chris Stolte
#t 2012
#c 5
#% 270633
#% 434617
#% 845350
#% 1013546
#% 1063534
#% 1426493
#% 1426594
#% 1523955
#! Tableau is a commercial business intelligence (BI) software tool that supports interactive, visual analysis of data. Armed with a visual interface to data and a focus on usability, Tableau enables a wide audience of end-users to gain insight into their datasets. The user experience is a fluid process of interaction in which exploring and visualizing data takes just a few simple drag-and-drop operations (no programming or DB experience necessary). In this context of exploratory, ad-hoc visual analysis, we describe a novel approach to integrating large, heterogeneous data sources. We present a new feature in Tableau called data blending, which gives users the ability to create data visualization mashups from structured, heterogeneous data sources dynamically without any upfront integration effort. Users can author visualizations that automatically integrate data from a variety of sources, including data warehouses, data marts, text files, spreadsheets, and data cubes. Because our data blending system is workload driven, we are able to bypass many of the pain-points and uncertainty in creating mediated schemas and schema-mappings in current pay-as-you-go integration systems.

#index 1770418
#* Finding related tables
#@ Anish Das Sarma;Lujun Fang;Nitin Gupta;Alon Halevy;Hongrae Lee;Fei Wu;Reynold Xin;Cong Yu
#t 2012
#c 5
#% 201889
#% 468530
#% 572314
#% 765435
#% 855119
#% 874896
#% 893118
#% 938708
#% 1117028
#% 1127393
#% 1176941
#% 1250364
#% 1291356
#% 1328133
#% 1328199
#% 1328200
#% 1338626
#% 1426543
#% 1426594
#% 1523913
#% 1567959
#% 1592311
#% 1846750
#! We consider the problem of finding related tables in a large corpus of heterogenous tables. Detecting related tables provides users a powerful tool for enhancing their tables with additional data and enables effective reuse of available public data. Our first contribution is a framework that captures several types of relatedness, including tables that are candidates for joins and tables that are candidates for union. Our second contribution is a set of algorithms for detecting related tables that can be either unioned or joined. We describe a set of experiments that demonstrate that our algorithms produce highly related tables. We also show that we can often improve the results of table search by pulling up tables that are ranked much lower based on their relatedness to top-ranked tables. Finally, we describe how to scale up our algorithms and show the results of running it on a corpus of over a million tables extracted from Wikipedia.

#index 1770419
#* Optimizing analytic data flows for multiple execution engines
#@ Alkis Simitsis;Kevin Wilkinson;Malu Castellanos;Umeshwar Dayal
#t 2012
#c 5
#% 36117
#% 252374
#% 480788
#% 482067
#% 800563
#% 983467
#% 1063553
#% 1119400
#% 1127367
#% 1217226
#% 1328186
#% 1426486
#% 1523837
#% 1523841
#% 1567910
#% 1581870
#% 1588764
#% 1747332
#! Next generation business intelligence involves data flows that span different execution engines, contain complex functionality like data/text analytics, machine learning operations, and need to be optimized against various objectives. Creating correct analytic data flows in such an environment is a challenging task and is both labor-intensive and time-consuming. Optimizing these flows is currently an ad-hoc process where the result is largely dependent on the abilities and experience of the flow designer. Our previous work addressed analytic flow optimization for multiple objectives over a single execution engine. This paper focuses on optimizing flows for a single objective, namely performance, over multiple execution engines. We consider flows that span a DBMS, a Map-Reduce engine, and an orchestration engine (e.g., an ETL tool or scripting language). This configuration is emerging as a common paradigm used to combine analysis of unstructured data with analysis of structured data (e.g., NoSQL plus SQL). We present flow transformations that model data shipping, function shipping, and operation decomposition and we describe how flow graphs are generated for multiple engines. Performance results for various configurations demonstrate the benefit of optimization.

#index 1770420
#* CloudRAMSort: fast and efficient large-scale distributed RAM sort on shared-nothing cluster
#@ Changkyu Kim;Jongsoo Park;Nadathur Satish;Hongrae Lee;Pradeep Dubey;Jatin Chhugani
#t 2012
#c 5
#% 2236
#% 32910
#% 77937
#% 100554
#% 121528
#% 172913
#% 227914
#% 340670
#% 442700
#% 444285
#% 458615
#% 776773
#% 813307
#% 963145
#% 963669
#% 972818
#% 983467
#% 1022230
#% 1050225
#% 1068005
#% 1118618
#% 1127563
#% 1217159
#% 1278123
#% 1278382
#% 1328057
#% 1328141
#% 1350336
#% 1373695
#% 1410152
#% 1426530
#% 1426531
#% 1426594
#% 1434021
#% 1507613
#% 1535762
#% 1536976
#% 1548997
#% 1567904
#! Sorting is a fundamental kernel used in many database operations. The total memory available across cloud computers is now sufficient to store even hundreds of terabytes of data in-memory. Applications requiring high-speed data analysis typically use in-memory sorting. The two most important factors in designing a high-speed in-memory sorting system are the single-node sorting performance and inter-node communication. In this paper, we present CloudRAMSort, a fast and efficient system for large-scale distributed sorting on shared-nothing clusters. CloudRAMSort performs multi-node optimizations by carefully overlapping computation with inter-node communication. The system uses a dynamic multi-stage random sampling approach for improved load-balancing between nodes. CloudRAMSort maximizes per-node efficiency by exploiting modern architectural features such as multiple cores and SIMD (Single-Instruction Multiple Data) units. This holistic combination results in the highest performing sorting performance on distributed shared-nothing platforms. CloudRAMSort sorts 1 Terabyte (TB) of data in 4.6 seconds on a 256-node Xeon X5680 cluster called the Intel Endeavor system. CloudRAMSort also performs well on heavily skewed input distributions, sorting 1 TB of data generated using Zipf distribution in less than 5 seconds. We also provide a detailed analytical model that accurately projects (within avg. 7%) the performance of CloudRAMSort with varying tuple sizes and interconnect bandwidths. Our analytical model serves as a useful tool to analyze performance bottlenecks on current systems and project performance with future architectural advances. With architectural trends of increasing number of cores, bandwidth, SIMD width, cache-sizes, and interconnect bandwidth, we believe CloudRAMSort would be the system of choice for distributed sorting of large-scale in-memory data of current and future systems

#index 1770421
#* Adaptive optimizations of recursive queries in teradata
#@ Ahmad Ghazal;Dawit Seid;Alain Crolotte;Mohammed Al-Kateb
#t 2012
#c 5
#% 36117
#% 77940
#% 152585
#% 152996
#% 172902
#% 190330
#% 234756
#% 243166
#% 248793
#% 300166
#% 333848
#% 578087
#% 765456
#% 810016
#% 810084
#% 960304
#% 1063514
#% 1217208
#% 1328059
#% 1328183
#% 1426510
#% 1482228
#% 1506217
#% 1510442
#% 1523825
#% 1524264
#% 1727489
#% 1736254
#! Recursive queries were introduced as part of ANSI SQL 99 to support processing of hierarchical data typical of air flight schedules, bill-of-materials, data cube dimension hierarchies, and ancestor-descendant information (e.g. XML data stored in relations). Recently, recursive queries have also found extensive use in web data analysis such as social network and click stream data. Teradata implemented recursive queries in V2R6 using static plans whereby a query is executed in multiple iterations, each iteration corresponding to one level of the recursion. Such a static planning strategy may not be optimal since the demographics of intermediate results from recursive iterations often vary to a great extent. Gathering feedback at each iteration could address this problem by providing size estimates to the optimizer which, in turn, can produce an execution plan for the next iteration. However, such a full feedback scheme suffers from lack of pipelining and the inability to exploit global optimizations across the different recursion iterations. In this paper, we propose adaptive optimization techniques that avoid the issues with static as well as full feedback optimization approaches. Our approach employs a mix of multi-iteration pre-planning and dynamic feedback techniques which are generally applicable to any recursive query implementation in an RDBMS. We also validated the effectiveness of our proposed techniques by conducting experiments on a prototype implementation using a real-life social network data from the FriendFeed online blogging service.

#index 1770422
#* From x100 to vectorwise: opportunities, challenges and things most researchers do not think about
#@ Marcin Zukowski;Peter Boncz
#t 2012
#c 5
#% 1977
#% 864446
#% 1022262
#% 1426547
#% 1647981
#! In 2008 a group of researchers behind the X100 database kernel created Vectorwise: a spin-off which together with the Actian corporation (previously Ingres) worked on bringing this technology to the market. Today, Vectorwise is a popular product and one of the examples of conversion of a research prototype into successful commercial software. We describe here some of the interesting aspects of the work performed by the Vectorwise development team in the process, and discuss the opportunities and challenges resulting from the decision of integrating a prototype-quality kernel with Ingres, an established commercial product. We also discuss how requirements coming from reallife scenarios sometimes clashed with design choices and simplifications often found in research projects, and how Vectorwise team addressed some of of them.

#index 1770423
#* Declarative web application development: encapsulating dynamic JavaScript widgets (abstract only)
#@ Robert Bolton;David Ing;Christopher Rebert;Kristina Lam Thai
#t 2012
#c 5
#% 1426549
#! The development of modern, highly interactive AJAX Web applications that enable dynamic visualization of data requires writing a great deal of tedious "plumbing code" to interface data between browser-based DOM and AJAX components, the application server, and the SQL database. Worse, each of these layers utilizes a different language. Further, much code is needed to keep the page and application states in sync using an imperative paradigm, which hurts simplicity. These factors result in a frustrating experience for today's Web developer. The FORWARD Project aims to alleviate this frustration by enabling pages that are "rendered views", in the SQL sense of "view". Our work in the project has led to a highly declarative approach whereby JavaScript/AJAX UI widgets automatically render views over the application state (database + session data + page data) without requiring the developer to tediously code how changes to the application state lead to invocation of the components' update methods. In contrast to conventional Web application development approaches, a FORWARD application involves only two languages, both declarative: an extended version of SQL, and an XML-based language for configuration and orchestration. The framework automatically handles efficient exchange of user input and changes to the underlying data, and updates the application state accordingly. The developer does not need to write any JavaScript or explicit updating code themselves. On the client side, FORWARD "units" wrap widgets using JavaScript to collect user input, directly display data, and reflect server-side updates to the data. On the server side, units contain Java code necessary to expose their functionality to the FORWARD framework and define their XML configuration representation. Our demo consists of a dynamically rendered webpage which internally uses AJAX to update a Google Maps widget that shows location markers for current Groupon deals in a specified area. It will illustrate that our SQL-driven approach makes this kind of rich dynamic webpage easy to write, with significant improvements in simplicity, brevity, and development time, while still providing the quality experience expected from top AJAX components. The amount of "plumbing code" is significantly reduced, enhancing the experience of AJAX Web application developers.

#index 1770424
#* Towards scalable summarization and visualization of large text corpora (abstract only)
#@ Tyler Sliwkanich;Douglas Schneider;Aaron Yong;Mitchell Home;Denilson Barbosa
#t 2012
#c 5
#! Society is awash with problems requiring the analysis of vast quantities of text and data. From detecting flu trends out of twitter conversations to finding scholarly works answering specific questions, we rely more and more on computers to process text for us. Text analytics is the application of computational, mathematical, and statistical models to derive information from large quantities of data coming primarily as text. Our project provides fast and effective text-analytics tools for large document collections, such as the blogosphere. We use natural language processing and database techniques to extract, collect, analyze, visualize, and archive information extracted from text. We focus on discovering relationships between entities (people, places, organizations, etc.) mentioned in one or more sources (blog posts or news articles). We built a custom solution using mostly off-the-shelf, open-source tools to provide a scalable platform for users to search and analyze large text corpora. Currently, we provide two main outlets for users to discover these relations: (1) full-text search over the documents and (2) graph visualizations of the entities and their relationships. This provides the user with succinct and easily digestible information gleaned from the corpus as a whole. For example, we can easily pose queries like which companies were bought by Google? as entity:google relation:bought. The extracted data is stored on a combination of the noSQL database CouchDB and Apache's Lucene. This combination is justified as our work-flow consists of offline batch insertions with almost no updates. Because we support specialized queries, we can forgo the flexibility of traditional SQL solutions and materialize all necessary indices, which are used to quickly query large amounts of de-normalized data using MapReduce. Lucene provides a flexible and powerful query syntax to yield relevant ranked results to the user. Moreover, its indices are synchronized by a process subscribed to the list of database changes published by CouchDB. The graph visualizations rely on CouchDB's ability to export the data in any format: we currently use a customized graph visualization relying on XML data. Finally, we use memcached to further improve the performance, especially for queries involving popular entities.

#index 1770425
#* Reducing cache misses in hash join probing phase by pre-sorting strategy (abstract only)
#@ Gi-Hwan Oh;Jae-Myung Kim;Woon-Hak Kang;Sang-Won Lee
#t 2012
#c 5
#% 172911
#% 1328057
#% 1581849
#! Recently, several studies on multi-core cache-aware hash join have been carried out [Kim09VLDB, Blanas11SIGMOD]. In particular, the work of Blanas has shown that rather simple no-partitioning hash join can outperform the work of Kim. Meanwhile, the simple but best performing hash join of Blanas still experiences severe cache misses in probing phase. Because the key values of tuples in outer relation are not sorted or clustered, each outer record has different hashed key value and thus accesses the different hash bucket. Since the size of hash table of inner table is usually much larger than that of the CPU cache, it is highly probable that the reference to hash bucket of inner table by each outer record would encounter cache miss. To reduce the cache misses in hash join probing phase, we propose a new join algorithm, Sorted Probing (in short, SP), which pre-sorts the hashed key values of outer table of hash join so that the access to the hash bucket of inner table has strong temporal locality, thus minimizing the cache misses during the probing phase. As an optimization technique of sorting, we used the cache-aware AlphaSort technique, which extracts the key from each record of data set to be sorted and its pointer, and then sorts the pairs of (key, rec_ptr). For performance evaluation, we used two hash join algorithms from Blanas' work, no partitioning(NP) and independent partitioning(IP) in a standard C++ program, provided by Blanas. Also, we implemented the AlphaSort and added it before each probing phase of NP and IP, and we call each algorithm as NP+SP and IP+SP. For syntactic workload, IP+SP outperforms all other algorithms: IP+SP is faster than other altorithms up to 30%.

#index 1770426
#* DP-tree: indexing multi-dimensional data under differential privacy (abstract only)
#@ Shangfu Peng;Yin Yang;Zhenjie Zhang;Marianne Winslett;Yong Yu
#t 2012
#c 5
#% 1846816
#! e-differential privacy (e-DP) is a strong and rigorous scheme for protecting individuals' privacy while releasing useful statistical information. The main idea is to inject random noise into the results of statistical queries, such that the existence of any single record has negligible impact on the distributions of query results. The accuracy of such randomized results depends heavily upon the query processing technique, which has been an active research topic in recent years. So far, most existing methods focus on 1-dimensional queries. The only work that handles multi-dimensional query processing under e-DP is [1], which indexes the sensitive data using variants of the quad-tree and the k-d-tree. As we point out in this paper, these structures are inherently suboptimal for answering queries under e-DP. Consequently, the solutions in [1] suffer from several serious drawbacks, including limited and unstable query accuracy, as well as bias towards certain types of queries. Motivated by this, we propose the DP-tree, a novel index structure for multi-dimensional query processing under e-DP that eliminates the problems encountered by the methods in [1]. Further, we show that the effectiveness of the DP-tree can be improved using statistical information about the query workload. Extensive experiments using real and synthetic datasets confirm that the DP-tree achieves significantly higher query accuracy than existing methods. Interestingly, an adaptation of the DP-tree also outperforms previous 1D solutions in their restricted scope, by large margins.

#index 1770427
#* Temporal provenance discovery in micro-blog message streams (abstract only)
#@ Zijun Xue;Junjie Yao;Bin Cui
#t 2012
#c 5
#! Recent years have witnessed the flourishing increases of micro-blog message applications. Prominent examples include Twitter, Facebook's status, and Sina Weibo in China. Messages in these applications are short (140 characters in a message) and easy to create. The subscription and re-sharing features also make it fairly intuitive to propagate. Micro-blog applications provide abundant information to present world scale user interests and social pulse in an unexpected way. But the precious corpus also brings out the noise and fast changing fragments to prohibit effective understanding and management. In this work, we propose a micro-blog provenance model to capture temporal connections within micro-blog messages. Here, provenance refers to data origin identification and transformation logging, demonstrating of great value in recent database and workflow systems. The provenance model is used to represent the message development trail and changes explicitly. We select various types of connections in micro-blog applications to identify the provenance. To cope with the real time micro-message deluge, we discuss a novel message grouping approach to encode and maintain the provenance information. A summary index structure is utilized to enable efficient provenance updating. We collect in-coming messages and compare them with an in-memory index to associate them with related ones. The closely related messages form some virtual provenance representation in a coarse granularity. We periodically dump memory values onto disks. In the actual implementation, we also introduce several adaptive pruning strategies to extend the potential of provenance discovery efficiency. We use the temporal decaying and granularity levels to filter out low chance messages. In the demonstration, we reveal the usefulness of provenance information for rich query retrieval and dynamic message tracking for effective message organization. The real-time collection approach shows advantages over some baselines. Experiments conducted on a real dataset verify the effectiveness and efficiency of our provenance approach. Results show that the partial-indexing strategy and other restriction ones can maintenance the accuracy at 90% and returning rate at 60% with a reasonable low memory usage. This is the first work towards provenance-based indexing support for micro-blog platforms.

#index 1770428
#* SigSpot: mining significant anomalous regions from time-evolving networks (abstract only)
#@ Misael Mongiovì;Petko Bogdanov;Razvan Ranca;Ambuj K. Singh;Evangelos E. Papalexakis;Christos Faloutsos
#t 2012
#c 5
#% 303075
#% 469401
#% 575673
#% 727932
#% 729983
#% 769901
#% 823346
#% 848218
#% 848219
#% 853537
#% 1030876
#% 1192439
#% 1216045
#% 1332155
#% 1535306
#% 1535361
#% 1594680
#% 1642054
#% 1646746
#% 1688444
#% 1710593
#% 1871394
#! Anomaly detection in dynamic networks has a rich gamut of application domains, such as road networks, communication networks and water distribution networks. An anomalous event, such as a traffic accident, denial of service attack or a chemical spill, can cause a local shift from normal behavior in the network state that persists over an interval of time. Detecting such anomalous regions of network and time extent in large real-world networks is a challenging task. Existing anomaly detection techniques focus on either the time series associated with individual network edges or on global anomalies that affect the entire network. In order to detect anomalous regions, one needs to consider both the time and the affected network substructure jointly, which brings forth computational challenges due to the combinatorial nature of possible solutions. We propose the problem of mining all Significant Anomalous Regions (SAR) in time-evolving networks that asks for the discovery of connected temporal subgraphs comprised of edges that significantly deviate from normal in a persistent manner. We propose an optimal Baseline algorithm for the problem and an efficient approximation, called S IG S POT. Compared to Baseline, SIGSPOT is up to one order of magnitude faster in real data, while achieving less than 10% average relative error rate. In synthetic datasets it is more than 30 times faster than Baseline with 94% accuracy and solves efficiently large instances that are infeasible (more than 10 hours running time) for Baseline. We demonstrate the utility of SIGSPOT for inferring accidents on road networks and study its scalability when detecting anomalies in social, transportation and synthetic evolving networks, spanning up to 1GB.

#index 1770429
#* VRRC: web based tool for visualization and recommendation on co-authorship network (abstract only)
#@ Eduardo M. Barbosa;Mirella M. Moro;Giseli Rabello Lopes;J. Palazzo M. de Oliveira
#t 2012
#c 5
#% 1529651
#! Scientific studies are usually developed by contributions from different researchers. Analyzing such collaborations is often necessary, for example, when evaluating the quality of a research group. Also, identifying new partnership possibilities within a set of researchers is frequently desired, for example, when looking for partners in foreign countries. Both analysis and identification are not easy tasks, and are usually done manually. This work presents VRRC, a new approach for visualizing recommendations of people within a co-authorship network (i.e., a graph in which nodes represent researchers and edges represent their co-authorships). VRRC input is a publication list from which it extracts the co-authorships. VRRC then recommends which relations could be created or intensified based on metrics designed for evaluating co-authorship networks. Finally, VRRC provides brand new ways to visualize not only the final recommendations but also the intermediate interactions within the network, including: a complete representation of the co-authorship network; an overview of the collaborations evolution over time; and the recommendations for each researcher to initiate or intensify cooperation. Some visualizations are interactive, allowing to filter data by time frame and highlighting specific collaborations. The contributions of our work, compared to the state-of-art, can be summarized as follows: (i) VRRC can be applied to any co-authorship network, it provides both net and recommendation visualizations, it is a Web-based tool and it allows easy sharing of the created visualizations (existing tools do not offer all these features together); (ii) VRRC establishes graphical representations to ease the visualization of its results (traditional approaches present the recommendation results through simple lists or charts); and (iii) with VRRC, the user can identify not only new possible collaborations but also existing cooperation that can be intensified (current recommendation approaches only indicate new collaborations). This work was partially supported by CNPq, Brazil.

#index 1770430
#* Fast sampling word correlations of high dimensional text data (abstract only)
#@ Frank Rosner;Alexander Hinneburg;Martin Gleditzsch;Mathias Priebe;Andreas Both
#t 2012
#c 5
#% 342617
#% 347225
#% 1693954
#! Finding correlated words in large document collections is an important ingredient for text analytics. The naïve approach computes the correlations of each word against all other words and filters for highly correlated word pairs. Clearly, this quadratic method cannot be applied to real world scenarios with millions of documents and words. Our main contribution is to transform the task of finding highly correlated word pairs into a word clustering problem that is efficiently solved by locality sensitive hashing (LSH). A key insight of our new method is to note that the empirical Pearson correlation between two words is the cosine of the angle between the centered versions of their word vectors. The angle can be approximated by an LSH scheme. Although centered word vectors are not sparse, the computation of the LSH hash functions can exploit the inherent sparsity of the word data. This leads to an efficient way to detect collisions between centered word vectors having a small angle and therefore provides a fast algorithm to sample highly correlated word pairs. Our new method based on LSH improves run time complexity of the enhanced naïve algorithm. This algorithm reduces the dimensionality of the word vectors using random projection and approximates correlations by computing cosine similarity on the reduced and centered word vectors. However, this method still has quadratic run time. Our new method replaces the filtering for high correlations in the naïve algorithm with finding hash collisions, which can be done by sorting the hash values of the word vectors. We evaluate the scalability of our new algorithm to large text collections.

#index 1775949
#* PODS 30th Anniversary Colloquium
#@ Maurizio Lenzerini;Moshe Y. Vardi;Ronald Fagin;Jeffrey D. Ullman;Serge Abiteboul;Victor Vianu;Frank Neven
#t 2011
#c 5

#index 1789645
#* Proceedings of the Eighth International Workshop on Data Management on New Hardware
#@ Shimin Chen;Stavros Harizopoulos
#t 2012
#c 5
#! The aim of this one-day workshop is to bring together researchers who are interested in optimizing database performance on modern computing infrastructure by designing new data management techniques and tools.

#index 1790790
#* Proceedings of the 4th International Workshop on Semantic Web Information Management
#@ Roberto De Virgilio;Fausto Giunchiglia;Letizia Tanca
#t 2012
#c 5
#! The ceaseless expansion of the World Wide Web is making more and more complex for humans to efficiently find the needed information. The underlying idea of having a description of the data on the Web, organized in such a way as to be used by machines for automation, integration and reuse across various applications, has been exploited in several research fields. As in the previous editions, the International Workshop on "Semantic Web Information Management" (SWIM) aims at reviewing the most recent data-centered solutions for the Semantic Web. In particular, its ambition is to present and analyze the techniques for semantic information management, by taking advantage of the synergisms between the logical basis of the semantic web and the logical foundations of conceptual modeling. Indeed, the leitmotif of these researches is the proposal of models and methods conceived to represent and manage the so-called "semantic data", that is, data appropriately structured to be easily machine-processable on the Web, according to semantic models (e.g. RDF, RDF(S), OWL). The long-standing experience of the information modeling community can provide a priceless contribution to the substantial problems arising in semantic data management.

#index 1800658
#* Proceedings of the Third International Workshop on Keyword Search on Structured Data
#@ Ling Tok Wang;Ge Yu;Jiaheng Lu;Wei Wang
#t 2012
#c 5
#! Information search is an indispensable component of our lives. Web search engines are widely used for searching textual documents, images, and video. However, there are also vast collections of structured and semi-structured data both on the Web and in enterprises, such as relational databases, XML data, etc. Traditionally, to access these resources, a user must learn structured or semi-structured query languages, and must be able to access data schemas, which are most likely heterogeneous, complex, and fast-evolving. To relieve web and scientific users from the learning curve and enable them to easily access structured and semi-structured data, there is a growing research interest to support keyword search on these data sources. The third International Workshop on Keyword Search on Structured Data (KEYS 2012) is held in Scottsdale, Arizona, USA on 20th May, 2012, in conjunction with SIGMOD 2012 conference, and aims to encourage researchers from both academia and industry communities to discuss the opportunities and challenges in keyword search on (semi-)structured data, and to present the key issues and novel techniques in this area. In response to the call for papers, KEYS 2012 has attracted highly diversified submissions, coming from USA, China, UK, Greece, Singapore, Mexico and Japan, resulting in an international final program. All submissions were peer reviewed by three program committee members. The program committee selected 8 full research papers for inclusion in the proceeding. The accepted papers covered a wide range of research topics and novel applications on keyword search on structured data.

#index 1803696
#* Proceedings of the Eleventh ACM International Workshop on Data Engineering for Wireless and Mobile Access
#@ George Kollios;Yufei Tao;Mario A. Nascimento;Mohamed A. Sharaf;Man Lung Yiu
#t 2012
#c 5
#! It is our great pleasure to welcome you all to the Eleventh ACM International Workshop on Data Engineering for Wireless and Mobile Access (MobiDE'12), held in conjunction with SIGMOD 2012. MobiDE continues its tradition of bringing together researchers and practitioners in databases, mobile computing, and networking, and providing a full day of exciting presentations and discussions. As in previous years, the workshop serves as a forum to present latest research and engineering results and contributions, and set future directions in wireless and mobile data management. MobiDE'12 is the eleventh of a successful series of workshops that aims to act as a bridge between the data management, wireless networking, and mobile computing communities. The 1st MobiDE workshop took place in Seattle, USA (August 1999), in conjunction with MobiCom 1999. The 2nd MobiDE workshop was held in Santa Barbara, USA (May 2001), together with SIGMOD 2001. The 3rd MobiDE workshop was organized in San Diego, USA (September 2003), co-located with MobiCom 2003. The 4th, 5th, 6th, 7th, 8th, 9th, and 10th MobiDE workshops took place in Baltimore, USA (June 2005), Chicago, USA (June 2006), Beijing, China (June 2007), Vancouver, Canada (June 2008), Providence, USA (June 2009), Indianapolis, USA (June 2010), and Athens, Greece (June 2011) respectively, co-located with SIGMOD. The program of MobiDE'12 covers a range of topics such as big mobile data, mobile services and middleware, query processing, energy-aware mobile data management etc. It features 8 high-quality papers, with authors from 8 countries. In addition, the program includes two invited keynote speeches: one by Prof. Shashi Shekhar from University of Minnesota, Minneapolis, USA, with title "Spatial Big-Data Challenges Intersecting Mobility and Cloud Computing", another by Dr. Kyriakos Mouratidis from Singapore Management University, Singapore, with title "Spatial Queries in Wireless Broadcast Environments". These proceedings will serve as a valuable reference point for the latest results on mobile and wireless data engineering.

#index 1842346
#* Proceedings of the Fifth International Workshop on Testing Database Systems
#@ 
#t 2012
#c 5

#index 1842359
#* Proceedings of the 2nd ACM SIGMOD Workshop on Databases and Social Networks
#@ Denilson Barbosa;Kristen LeFevre;Evimaria Terzi
#t 2012
#c 5
#! The ACM SIGMOD Workshop on Databases and Social Networks (DBSocial) aims at disseminating results founded on database research and practice that advance the state-of-the-art in the observation, management, and analysis of inherently networked data that results primarily from social phenomena. In particular, DBSocial is intended to foster a discussion about the role that the database community should play in the area of social network research. As such, DBSocial welcomes papers whose approaches are fundamentally centered on theoretical foundations and best practices in databases and very closely-related areas such as data mining and information retrieval. In its second edition, DBSocial features three invited talks and five peer-reviewed papers (selected out of twelve submissions). The invited talks cover challenges and advances in building the infrastructure necessary for storing, accessing, and analyzing large social networks, as well as supporting applications that use them. Amol Desphande from the University of Maryland talks about challenges in building a scalable data management and analytics tool capable of handling large networks, and some advances in his group towards that end. Pankaj Gupta and Jimmy Lin discuss components of the Twitter infrastructure for handling large and sparse networks of users. Finally, Adam Silberstein gives the perspective of LinkedIn, detailing components in their infrastructure, and how they are used to back their diverse set of social applications. The papers cover a wide range of topics related to DBSocial. A technique for solving race conditions in a multi-tiered database system similar to those used in social networking sites is proposed by Ghandeharizadeh and Yap. Next, Ahmed and Guha present an exploratory analysis of how linguistic features correlate with geographical information about users, based on a dataset from Flickr. The paper by Jouili, O-Doherty and Van Roy discusses the problem of trust inference between users based on common preferences, which remains a challenging problem as users seldom explicitly indicate trust. Xiao, Tan, and Aung consider the problem of managing "circles" of connections in social networks, which is an emerging problem arising from the popularity of social networking sites. Closing the proceedings, the paper by Ribeiro and Silva proposes an algorithm for finding the occurrences of a set of graphs within in one graph. Their approach is to use indexing structures called G-Tries, which are shown to lead to substantial efficiency gains.

#index 1842360
#* Gumball: a race condition prevention technique for cache augmented SQL database management systems
#@ Shahram Ghandeharizadeh;Jason Yap
#t 2012
#c 5
#% 287230
#% 302760
#% 333995
#% 397357
#% 480474
#% 480818
#% 610649
#% 763884
#% 777934
#% 800516
#% 828601
#% 978365
#% 1053458
#% 1291617
#% 1426489
#% 1526992
#% 1573340
#% 1621133
#% 1747340
#! Query intensive applications augment a Relational Database Management System (RDBMS) with a middle-tier cache to enhance performance. An example is memcached in use by very large well known sites such as Facebook. In the presence of updates to the normalized tables of the RDBMS, invalidation based consistency techniques delete the impacted key-value pairs residing in the cache. A subsequent reference for these key-value pairs observes a cache miss, recomputes the new values from the RDBMS, and inserts the new key-value pairs in the cache. These techniques suffer from race conditions that result in cache states that produce stale data. The Gumball Technique (GT) addresses this limitation by preventing race conditions. Experimental results show GT enhances the accuracy of an application hundreds of folds and, in some cases, may reduce system performance slightly.

#index 1842361
#* Distance matters: an exploratory analysis of the linguistic features of Flickr photo tag metadata in relation to impression management
#@ Syed Ishtiaque Ahmed;Shion Guha
#t 2012
#c 5
#% 855601
#% 881054
#% 905320
#% 955010
#% 959694
#% 1047423
#% 1560221
#! Tags are words that users add to shared multimedia contents as metadata to facilitate better categorization and improved sharing experiences. With the burgeoning growth of shared images and videos over online social networks, a huge number of tags is being populated everyday in public or shared databases. While one major reason for tagging a photo or a video incorporates the functional needs for the organization of that shared object, people also use tags as a medium of communication for conveying their emotions to their family, friends, and other contacts. The diversity in the linguistic features of these tags demonstrates some interesting patterns that reflect different facets of human nature in managing their online impression to their social peers. This paper investigates how some linguistic features of tags associated with the Flickr photos change with the distance between the user's home location and the location where the photo is taken. In our exploratory analysis "affective" and "relativ" words and their multiplicative interaction show correlations with this distance. These initial findings help us to have a better understanding of online social phenomena related to the expression of emotions and sharing information. At the same time, this might have some indirect implications to understand the insight of impression management in online communities.

#index 1842362
#* Towards trust inference from bipartite social networks
#@ Daire O'Doherty;Salim Jouili;Peter Van Roy
#t 2012
#c 5
#% 173879
#% 268079
#% 577367
#% 754098
#% 790459
#% 813966
#% 842605
#% 874419
#% 943767
#% 1026499
#% 1247796
#% 1268028
#% 1280753
#% 1386131
#% 1491563
#% 1555156
#% 1668087
#! The emergence of trust as a key link between users in social networks has provided an effective means of enhancing the personalization of on-line user content. However, the availability of such trust information remains a challenge to the algorithms that use it, as the majority of social networks do not provide a means of explicit trust feedback. This paper presents an investigation into the inference of trust relations between actor pairs of a social network, based solely on the structural information of the bipartite graph typical of most on-line social networks. Using intuition inspired from real life observations, we argue that the popularity of an item in a social graph is inversely related to the level of trust between actor pairs who have rated it. From an existing bipartite social graph, this method computes a new social graph, linking actors together by means of symmetric weighted trust relations. Through a set of experiments performed on a real social network dataset, our method produces statistically significant results, showing strong trust prediction accuracy.

#index 1842363
#* Towards ad-hoc circles in social networking sites
#@ Qian Xiao;Htoo Htet Aung;Kian-Lee Tan
#t 2012
#c 5
#% 1190107
#% 1299956
#% 1399968
#% 1451162
#% 1586961
#% 1714618
#% 1746976
#! Social Networking Sites (SNSs) allow users to publish posts to certain user-defined circles (sets of users). However, existing SNS models are limited in several ways. First, it is not practical to predefine all circles a user will ever need for disseminating her posts. Second, existing SNSs do not currently have an effecitive mechanism for a user to create and/or customize dynamic (ad-hoc) circles for each publishing session. Third, SNSs do not have features to assist users to manage and use the circles in an easy way by considering the user's ever-changing habits accordingly. In this paper, we propose a novel model for creating ad-hoc circles as needs arise. We present a recommendation framework -- the Circle OpeRation RECommendaTion (CORRECT) framework -- to assist users in easily utilizing our proposed model. Contrary to current SNS offerings, our proposed model does not require a user to create an extensive list of predefined circles; instead, ad-hoc circles are recommended based on a few building-block circles the user has defined and historical ad-hoc circles the user has created.

#index 1842364
#* Querying subgraph sets with g-tries
#@ Pedro Ribeiro;Fernando Silva
#t 2012
#c 5
#% 466644
#% 601159
#% 629708
#% 864425
#% 867050
#% 906507
#% 907288
#% 937071
#% 1227871
#% 1360432
#% 1387896
#% 1401363
#% 1493390
#% 1495182
#% 1506492
#% 1643159
#! In this paper we present an universal methodology for finding all the occurrences of a given set of subgraphs in one single larger graph. Past approaches would either enumerate all possible subgraphs of a certain size or query a single subgraph. We use g-tries, a data structure specialized in dealing with subgraph sets. G-Tries store the topological information on a tree that exposes common substructure. Using a specialized canonical form and symmetry breaking conditions, a single non-redundant search of the entire set of subgraphs is possible. We give results of applying g-tries querying to different social networks, showing that we can efficiently find the occurrences of a set containing subgraphs of multiple sizes, outperforming previous methods.

#index 1880312
#* Algorithms for regular languages that use algebra
#@ Mikołaj Bojańczyk
#t 2012
#c 5
#% 69625
#% 345071
#% 427027
#% 791181
#% 1106512
#% 1230865
#% 1328532
#% 1489355
#! This paper argues that an algebraic approach to regular languages, such as using monoids, can yield efficient algorithms on strings and trees.

#index 1880313
#* A brief survey of automatic methods for author name disambiguation
#@ Anderson A. Ferreira;Marcos André Gonçalves;Alberto H.F. Laender
#t 2012
#c 5
#% 296738
#% 302390
#% 310546
#% 337235
#% 376266
#% 722903
#% 722904
#% 760866
#% 804877
#% 805885
#% 809459
#% 809460
#% 810635
#% 818916
#% 915273
#% 937552
#% 957814
#% 967295
#% 1015618
#% 1020797
#% 1090229
#% 1107051
#% 1133176
#% 1206818
#% 1211086
#% 1213413
#% 1213414
#% 1274820
#% 1434125
#% 1450830
#% 1467901
#% 1498542
#% 1663664
#% 1755323
#% 1777687
#! Name ambiguity in the context of bibliographic citation records is a hard problem that affects the quality of services and content in digital libraries and similar systems. The challenges of dealing with author name ambiguity have led to a myriad of disambiguation methods. Generally speaking, the proposed methods usually attempt to group citation records of a same author by finding some similarity among them or try to directly assign them to their respective authors. Both approaches may either exploit supervised or unsupervised techniques. In this article, we propose a taxonomy for characterizing the current author name disambiguation methods described in the literature, present a brief survey of the most representative ones and discuss several open challenges.

#index 1880314
#* Tamer Özsu speaks out: on journals, conferences, encyclopedias and technology
#@ Marianne Winslett;Vanessa Braganholo
#t 2012
#c 5

#index 1880315
#* Erich Neuhold speaks out: on industry research versus academic research, funding projects, and more
#@ Marianne Winslett;Vanessa Braganholo
#t 2012
#c 5

#index 1880316
#* A call for surveys
#@ Philip A. Bernstein;Christian S. Jensen;Kian-Lee Tan
#t 2012
#c 5

#index 1897685
#* Edgar F. Codd Innovations Award Talk
#@ Bruce Lindsay
#t 2012
#c 5

#index 1897766
#* SIGMOD Contributions Award Talk
#@ Marianne Winslett
#t 2012
#c 5

#index 1897767
#* Test Of Time Award Talk: Executing SQL over Encrypted Data in the Database-Service-Provider Model
#@ Hakan Hacigumus;Balakrishna (Bala) Iyer;Chen Li;Sharad Mehrotra
#t 2012
#c 5

#index 1897768
#* SIGMOD Jim Gray Doctoral Dissertation Award Talk
#@ Ryan Johnson
#t 2012
#c 5

#index 1905957
#* Semiring-annotated data: queries and provenance?
#@ Grigoris Karvounarakis;Todd J. Green
#t 2012
#c 5
#% 663
#% 137867
#% 190638
#% 215225
#% 228817
#% 318704
#% 384978
#% 464891
#% 864394
#% 893095
#% 893167
#% 976987
#% 1022258
#% 1063736
#% 1231247
#% 1426581
#% 1450320
#% 1536933
#% 1581829
#% 1581830
#% 1592792
#% 1592794
#% 1728680
#% 1770138
#% 1818427
#% 1846720
#! We present an overview of the literature on querying semiring-annotated data, a notion we introduced five years ago in a paper with Val Tannen. First, we show that positive relational algebra calculations for various forms of annotated relations, as well as provenance models for such queries, are particular cases of the same general algorithm involving commutative semirings. For this reason, we present a formal framework for answering queries on data with annotations from commutative semirings, and propose a comprehensive provenance representation based on semirings of polynomials. We extend these considerations to XQuery views over annotated, unordered XML data, and show that the semiring framework suffices for a large positive fragment of XQuery applied to such data. Finally, we conclude with a brief overview of the large body of work that builds upon these results, including both extensions to the theoretical foundations and uses in practical applications.

#index 1905958
#* Handling temporal information in web search engines
#@ Edimar Manica;Carina F. Dorneles;Renata Renata Galante
#t 2012
#c 5
#% 319244
#% 751830
#% 815884
#% 845350
#% 877569
#% 1024551
#% 1052713
#% 1107850
#% 1161152
#% 1267029
#% 1316424
#% 1483642
#% 1700558
#% 1783036
#! TheWeb can be considered a vast repository of temporal information, as it daily receives a huge amount of new pages. Generally, users are interested in information related to a specific temporal interval. In the information retrieval area, researches have newly incorporated the temporal dimension to the search engines. This paper presents a comprehensive study that describes the evolution of search engines on the exploitation of temporal information. Research directions and future perspectives are also presented, considering the authors' point of view.

#index 1905959
#* Ryan Johnson: recipient of the 2012 ACM SIGMOD Jim Gray dissertation award
#@ Marianne Winslett;Vanessa Braganholo
#t 2012
#c 5

#index 1905960
#* dbTrento: the data and information management group at the University of Trento
#@ Themis Palpanas;Yannis Velegrakis
#t 2012
#c 5
#% 864389
#% 893104
#% 916370
#% 960267
#% 1068972
#% 1102993
#% 1106435
#% 1112739
#% 1127370
#% 1127589
#% 1179657
#% 1232194
#% 1329680
#% 1333839
#% 1333840
#% 1337104
#% 1400073
#% 1400111
#% 1491657
#% 1507078
#% 1523833
#% 1523959
#% 1528081
#% 1535372
#% 1535385
#% 1549883
#% 1563386
#% 1581893
#% 1581978
#% 1588360
#% 1595888
#% 1624280
#% 1642110
#% 1651549
#% 1651606
#% 1688421
#% 1689686
#% 1693866
#% 1756063
#% 1770403
#% 1846717
#% 1874816
#% 1874876
#% 1880478
#% 1897999

#index 1905961
#* Temporal features in SQL:2011
#@ Krishna Kulkarni;Jan-Eike Michels
#t 2012
#c 5
#% 287268
#% 319244
#% 335715
#% 361445
#% 645159
#% 1764510
#! SQL:2011 was published in December of 2011, replacing SQL:2008 as the most recent revision of the SQL standard. This paper covers the most important new functionality that is part of SQL:2011: the ability to create and manipulate temporal tables.

#index 1905962
#* A high-throughput in-memory index, durable on flash-based SSD: insights into the winning solution of the SIGMOD programming contest 2011
#@ Thomas Kissinger;Benjamin Schlegel;Matthias Boehm;Dirk Habich;Wolfgang Lehner
#t 2012
#c 5
#% 286929
#% 300194
#% 722570
#% 1217152
#% 1523920
#% 1581848
#! Growing memory capacities and the increasing number of cores on modern hardware enforces the design of new in-memory indexing structures that reduce the number of memory transfers and minimizes the need for locking to allow massive parallel access. However, most applications depend on hard durability constraints requiring a persistent medium like SSDs, which shorten the latency and throughput gap between main memory and hard disks. In this paper, we present our winning solution of the SIGMOD Programming Contest 2011. It consists of an in-memory indexing structure that provides a balanced read/write performance as well as non-blocking reads and single-lock writes. Complementary to this index, we describe an SSD-optimized logging approach to fit hard durability requirements at a high throughput rate.

#index 1905963
#* Report of the international workshop on business intelligence and the web: BEWEB 2011
#@ Jose-Norberto Mazón;Irene Garrigós;Florian Daniel;Malu Castellanos
#t 2012
#c 5
#% 1384843
#! The 2nd International Workshop on Business intelligencE and the WEB (BEWEB) was co-located with the EDBT/ICDT 2011 Joint Conference in Uppsala (Sweden) on March 25, 2011. BEWEB intends to be an international forum for researchers and practitioners to exchange ideas on how to leverage the huge amount of data that is available on the Web in BI applications and on how to apply Web engineering methods and techniques to the design of BI applications. This report summarizes the 2011 edition of BEWEB.

#index 1945681
#* Querying Semantic Data on the Web?
#@ Marcelo Arenas;Claudio Gutierrez;Daniel P. Miranker;Jorge Pérez;Juan F. Sequeda
#t 2013
#c 5
#% 261741
#% 275928
#% 303887
#% 408396
#% 598376
#% 956574
#% 1098424
#% 1201360
#% 1218627
#% 1223424
#% 1333447
#% 1399974
#% 1424588
#% 1497253
#% 1540320
#% 1586117
#% 1597489
#% 1603788
#% 1641504
#% 1696286
#% 1746826
#% 1746861
#% 1746863
#% 1770125
#% 1770126
#% 1846460
#% 1876854
#% 1942758

#index 1945682
#* Introducing an annotated bibliography on temporal and evolution aspects in the semantic web
#@ Fabio Grandi
#t 2013
#c 5
#% 108499
#% 126704
#% 140617
#% 149632
#% 481542
#% 630974
#% 750935
#% 778477

#index 1945683
#* An overview of the deco system: data model and query language; query processing and optimization
#@ Hyunjung Park;Richard Pang;Aditya Parameswaran;Hector Garcia-Molina;Neoklis Polyzotis;Jennifer Widom
#t 2013
#c 5
#% 13016
#% 300169
#% 1526538
#% 1581851
#% 1628028
#% 1628171
#% 1746876
#% 1746898
#% 1895096
#% 1919728
#! Deco is a comprehensive system for answering declarative queries posed over stored relational data together with data obtained on-demand from the crowd. In this overview paper, we describe Deco's data model, query language, and system prototype, summarizing material from earlier papers. Deco's data model was designed to be general, flexible, and principled. Deco's query language extends SQL with simple constructs necessary for crowdsourcing, and has a precise semantics for arbitrary queries. Deco's query execution engine and cost-based query optimizer incorporate many novel techniques to address the limitations of traditional query processing techniques in the crowdsourcing setting. Query processing is guided by the objective of minimizing monetary cost and reducing latency.

#index 1945684
#* Daniel Abadi speaks out
#@ Marianne Winslett;Vanessa Braganholo
#t 2013
#c 5

#index 1945685
#* The data analytics group at the qatar computing research institute
#@ George Beskales;Gautam Das;Ahmed K. Elmagarmid;Ihab F. Ilyas;Felix Naumann;Mourad Ouzzani;Paolo Papotti;Jorge Quiane-Ruiz;Nan Tang
#t 2013
#c 5
#% 480496
#% 480499
#% 659991
#% 752741
#% 810019
#% 833132
#% 913783
#% 1022228
#% 1054480
#% 1328143
#% 1328159
#% 1414317
#% 1426628
#% 1523804
#% 1523834
#% 1550749
#% 1581851
#% 1581885
#% 1642098
#% 1763276
#% 1770376
#% 1846719
#% 1880470
#% 1895095

#index 1945686
#* Daisy: the center for data-intensive systems at Aalborg University
#@ Hua Lu;Torben Bach Pedersen;Simonas Šaltenis;Bent Thomsen;Lone Leth Thomsen;Kristian Torp
#t 2013
#c 5
#% 287268
#% 443257
#% 893212
#% 907395
#% 915842
#% 1022251
#% 1044493
#% 1049781
#% 1050410
#% 1114757
#% 1127612
#% 1135178
#% 1156046
#% 1164014
#% 1206712
#% 1206731
#% 1208212
#% 1208337
#% 1230828
#% 1230842
#% 1245052
#% 1245080
#% 1246151
#% 1248443
#% 1267410
#% 1292533
#% 1296946
#% 1298884
#% 1310325
#% 1328208
#% 1372701
#% 1372710
#% 1384843
#% 1390324
#% 1400776
#% 1410453
#% 1445732
#% 1480773
#% 1482209
#% 1486243
#% 1490118
#% 1494930
#% 1494935
#% 1523828
#% 1523885
#% 1523957
#% 1537100
#% 1537153
#% 1557474
#% 1573155
#% 1583281
#% 1594579
#% 1594638
#% 1594674
#% 1616905
#% 1618261
#% 1642882
#% 1643290
#% 1643324
#% 1667234
#% 1733625
#% 1770322
#% 1789646
#% 1846740
#% 1846830
#% 1855859
#% 1878475
#% 1882096
#% 1882101
#% 1895069
#% 1895092
#% 1905342
#% 1922701
#% 1941055

#index 1945687
#* On the equivalence of PLSI and projected clustering
#@ Charu C. Aggarwal
#t 2013
#c 5
#% 248792
#% 273891
#% 280819
#% 300131
#% 332094
#% 342621
#% 464888
#% 465031
#% 480132
#% 643008
#% 722904
#% 778215
#% 915305
#% 1016200
#% 1108903
#% 1865111
#! The problem of projected clustering was first proposed in the ACMSIGMOD Conference in 1999, and the Probabilistic Latent Semantic Indexing (PLSI) technique was independently proposed in the ACMSIGIR Conference in the same year. Since then, more than two thousand papers have been written on these problems by the database, data mining and information retrieval communities, along completely independent lines of work. In this paper, we show that these two problems are essentially equivalent, under a probabilistic interpretation to the projected clustering problem. We will show that the EM-algorithm, when applied to the probabilistic version of the projected clustering problem, can be almost identically interpreted as the PLSI technique. The implications of this equivalence are significant, in that they imply the cross-usability of many of the techniques which have been developed for these problems over the last decade. We hope that our observations about the equivalence of these problems will stimulate further research which can significantly improve the currently available solutions for either of these problems.

#index 1945688
#* Challenges and communities of medical informatics research
#@ Vagelis Hristidis
#t 2013
#c 5
#% 1207026
#% 1467763
#! This article discusses experiences and lessons learned from working on health informatics research as a computer scientist. In particular, I present challenges faced when conducting research on medical informatics, and explain some of the aspects that make medical data and systems unique. Then, I present the two broad research communities studying medical informatics problems. Finally, I offer advice on how to bridge the gap between these communities and increase their research productivity.

#index 1945689
#* 10th international workshop on quality in databases: QDB 2012
#@ Xin Luna Dong;Eduard Constantin Dragut
#t 2013
#c 5
#% 810014
#% 937552
#% 960365
#% 1063709
#% 1542530
#% 1581886
#% 1673578
#% 1846778
#% 1880463
#% 1895068
#% 1895100
#% 1959784

#index 1945690
#* Report from the first workshop on scalable workflow enactment engines and technology (SWEET'12)
#@ Jan Hidders;Jacek Sroka;Paolo Missier
#t 2013
#c 5
#% 954295
#% 1063553
#% 1291844
#% 1292903
#% 1328186
#% 1426486
#! This report summarizes the presentations and discussions of SWEET 2012, the First InternationalWorkshop on ScalableWorkflow Enactment Engines and Technologies. SWEET was held in conjunction with the 2012 SIGMOD conference in Scottsdale, Arizona, USA on May 20th, 2012. The goal of the workshop was to bring together researchers and practitioners to explore the state of the art in workflow-based programming for data-intensive applications, and the potential of cloud-based computing in this area. The program featured two very well attended invited talks by Pawel Garbacki from Google and Jimmy Lin from the University of Maryland, on leave at Twitter at the time, as well as a tutorial on Oozie, Yahoo's workflow engine based on Hadoop, by Mohammad Islam from Yahoo/Cloudera.

#index 1945691
#* XLDB Asia 2012: the first extremely large databases conference at Asia
#@ Xiaofeng Meng;Fusheng Wang
#t 2013
#c 5
#! The Extremely Large Databases (XLDB) series of conferences/ workshops have been held successfully six times in recent years. The First XLDB Conference at Asia (XLDB Asia) was held at Beijing, China on June 22-23, 2012. The conference attracted nearly 200 participants. XLDB takes a fresh format on the organization through invited talks, lightning talks and open discussions. Most invited speakers are also owners of real extremely large data from industries and scientific research, practitioners who are handling the real data, or DBMS researchers who are researching new solutions. Based on the enthusiastic embrace and positive feedbacks from participants, we believe the conference series will continue as a venue for the discussion on the management and analysis of extremely large data sets with increasing popularity.

#index 1955649
#* Makeflow: a portable abstraction for data intensive computing on clusters, clouds, and grids
#@ Michael Albrecht;Patrick Donnelly;Peter Bui;Douglas Thain
#t 2012
#c 5
#% 590372
#% 723279
#% 879803
#% 954295
#% 963669
#% 979304
#% 983467
#% 1017256
#% 1063553
#% 1127550
#% 1456819
#% 1464901
#% 1533925
#% 1641740
#! In recent years, there has been a renewed interest in languages and systems for large scale distributed computing. Unfortunately, most systems available to the end user use a custom description language tightly coupled to a specific runtime implementation, making it difficult to transfer applications between systems. To address this problem we introduce Makeflow, a simple system for expressing and running a data-intensive workflow across multiple execution engines without requiring changes to the application or workflow description. Makeflow allows any user familiar with basic Unix Make syntax to generate a workflow and run it on one of many supported execution systems. Furthermore, in order to assess the performance characteristics of the various execution engines available to users and assist them in selecting one for use we introduce Workbench, a suite of benchmarks designed for analyzing common workflow patterns. We evaluate Workbench on two physical architectures -- the first a storage cluster with local disks and a slower network and the second a high performance computing cluster with a central parallel filesystem and fast network -- using a variety of execution engines. We conclude by demonstrating three applications that use Makeflow to execute data intensive applications consisting of thousands of jobs.

#index 1955650
#* Evaluating parameter sweep workflows in high performance computing
#@ Fernando Chirigati;Vítor Silva;Eduardo Ogasawara;Daniel de Oliveira;Jonas Dias;Fábio Porto;Patrick Valduriez;Marta Mattoso
#t 2012
#c 5
#% 261139
#% 346653
#% 451429
#% 481784
#% 511920
#% 610845
#% 765129
#% 810563
#% 875046
#% 983749
#% 1092770
#% 1159972
#% 1164233
#% 1174009
#% 1270147
#% 1278124
#% 1292897
#% 1426489
#% 1497610
#% 1567949
#% 1606217
#% 1641742
#% 1655405
#! Scientific experiments based on computer simulations can be defined, executed and monitored using Scientific Workflow Management Systems (SWfMS). Several SWfMS are available, each with a different goal and a different engine. Due to the exploratory analysis, scientists need to run parameter sweep (PS) workflows, which are workflows that are invoked repeatedly using different input data. These workflows generate a large amount of tasks that are submitted to High Performance Computing (HPC) environments. Different execution models for a workflow may have significant differences in performance in HPC. However, selecting the best execution model for a given workflow is difficult due to the existence of many characteristics of the workflow that may affect the parallel execution. We developed a study to show performance impacts of using different execution models in running PS workflows in HPC. Our study contributes by presenting a characterization of PS workflow patterns (the basis for many existing scientific workflows) and its behavior under different execution models in HPC. We evaluated four execution models to run workflows in parallel. Our study measures the performance behavior of small, large and complex workflows among the evaluated execution models. The results can be used as a guideline to select the best model for a given scientific workflow execution in HPC. Our evaluation may also serve as a basis for workflow designers to analyze the expected behavior of an HPC workflow engine based on the characteristics of PS workflows.

#index 1955651
#* DAGwoman: enabling DAGMan-like workflows on non-Condor platforms
#@ Thomas Tschager;Heiko A. Schmidt
#t 2012
#c 5
#% 419721
#% 590372
#% 767436
#% 808595
#% 879809
#% 905680
#% 954302
#% 983708
#% 1035119
#% 1174009
#% 1251539
#% 1681403
#! Scientific analyses have grown more and more complex. Thus, scientific workflows gained much interest and importance to automate and handle complex analyses. Tools abound to ease generation, handling and enactment of scientific workflows on distributed compute resources. Among the different workflow engines DAGMan seems to be widely available and supported by a number of tools. Unfortunately, if Condor is not installed users lack the possibility to use DAGMan. A new workflow engine, DAGwoman, is presented which can be run in user-space and allows to run DAGMan-formatted workflows. Using an artificial and two bioinformatics workflows DAGwoman is compared to GridWay's GWDAG engine and to DAGMan based on Condor-G. Showing good results with respect to workflow engine delay and features richness DAGwoman offers a complementary tool to efficiently run DAGMan-workflows if Condor is not available.

#index 1955652
#* Oozie: towards a scalable workflow management system for Hadoop
#@ Mohammad Islam;Angelo K. Huang;Mohamed Battisha;Michelle Chiang;Santhosh Srinivasan;Craig Peters;Andreas Neumann;Alejandro Abdelnur
#t 2012
#c 5
#% 1035750
#% 1174009
#% 1291844
#% 1328060
#% 1328095
#% 1468530
#% 1681403
#% 1713622
#! Hadoop is a massively scalable parallel computation platform capable of running hundreds of jobs concurrently, and many thousands of jobs per day. Managing all these computations demands for a workflow and scheduling system. In this paper, we identify four indispensable qualities that a Hadoop workflow management system must fulfill namely Scalability, Security, Multi-tenancy, and Operability. We find that conventional workflow management tools lack at least one of these qualities, and therefore present Apache Oozie, a workflow management system specialized for Hadoop. We discuss the architecture of Oozie, share our production experience over the last few years at Yahoo, and evaluate Oozie's scalability and performance.

#index 1955653
#* Turbine: a distributed-memory dataflow engine for extreme-scale many-task applications
#@ Justin M. Wozniak;Timothy G. Armstrong;Ketan Maheshwari;Ewing L. Lusk;Daniel S. Katz;Michael Wilde;Ian T. Foster
#t 2012
#c 5
#% 645990
#% 769155
#% 831260
#% 879794
#% 954300
#% 979683
#% 983467
#% 998845
#% 1023420
#% 1063553
#% 1092768
#% 1117387
#% 1291843
#% 1316333
#% 1328095
#% 1400975
#% 1464950
#% 1468421
#% 1475055
#% 1475079
#% 1561032
#% 1567910
#% 1637932
#% 1828537
#! Efficiently utilizing the rapidly increasing concurrency of multi-petaflop computing systems is a significant programming challenge. One approach is to structure applications with an upper layer of many loosely-coupled coarse-grained tasks, each comprising a tightly-coupled parallel function or program. "Many-task" programming models such as functional parallel dataflow may be used at the upper layer to generate massive numbers of tasks, each of which generates significant tighly-coupled parallelism at the lower level via multithreading, message passing, and/or partitioned global address spaces. At large scales, however, the management of task distribution, data dependencies, and inter-task data movement is a significant performance challenge. In this work, we describe Turbine, a new highly scalable and distributed many-task dataflow engine. Turbine executes a generalized many-task intermediate representation with automated self-distribution, and is scalable to multi-petaflop infrastructures. We present here the architecture of Turbine and its performance on highly concurrent systems.

#index 1971477
#* Proceedings of the 32nd symposium on Principles of database systems
#@ Richard Hull;Wenfei Fan
#t 2013
#c 5
#! This volume contains the proceedings of the thirty-second ACM SIGMOD-SIGACT-SIGART Symposium on Principles of Database Systems (PODS 2013), held in New York City, USA, on June 23--26, 2013, in conjunction with the 2013 ACM SIGMOD International Conference on Management of Data. The proceedings include papers based on the keynote address by Diego Calvanese (co-authored with Giuseppe De Giacomo and Marco Montali), and two invited tutorials by Piotr Indyk and Pablo Barceló, and 24 contributed papers that were selected by the Program Committee from 97 submissions. Submissions were unusually strong this year and the conference was very competitive. The program committee worked hard to select the best from the submissions, but many papers with high scores and good reviews could not make it into the final program. Most of the 24 accepted papers are extended abstracts. While the submissions have been read by program committee members, they have not been formally refereed. It is anticipated that many of these papers will appear in more polished and detailed form in scientific journals. The program committee selected the paper Verification of Database-Driven Systems via Amalgamation by Mikołaj Bojańczyk, Luc Segoufin and Szymon Toruńczyk for the PODS 2013 Best Paper Award. In addition, the ACM PODS Alberto O. Mendelzon Test-of-Time Award 2013 appears in the proceedings. This year, the award is given to Revealing Information while Preserving Privacy by Irit Dinur and Kobbi Nissim. The paper originally appeared in the proceedings of PODS 2003. Warmest congratulations to the authors of these papers!

#index 1971478
#* Spanners: a formal framework for information extraction
#@ Ronald Fagin;Benny Kimelfeld;Frederick Reiss;Stijn Vansummeren
#t 2013
#c 5
#% 32904
#% 101925
#% 248025
#% 257855
#% 268797
#% 273687
#% 275304
#% 296540
#% 319244
#% 344425
#% 352294
#% 401124
#% 464434
#% 466892
#% 562454
#% 587566
#% 657754
#% 747945
#% 757350
#% 855020
#% 929963
#% 1183368
#% 1195396
#% 1200842
#% 1206687
#% 1290067
#% 1307976
#% 1471192
#% 1478714
#% 1523847
#% 1729156
#% 1853692
#% 1888487
#% 1951753
#! An intrinsic part of information extraction is the creation and manipulation of relations extracted from text. In this paper, we develop a foundational framework where the central construct is what we call a spanner. A spanner maps an input string into relations over the spans (intervals specified by bounding indices) of the string. The focus of this paper is on the representation of spanners. Conceptually, there are two kinds of such representations. Spanners defined in a primitive representation extract relations directly from the input string; those defined in an algebra apply algebraic operations to the primitively represented spanners. This framework is driven by SystemT, an IBM commercial product for text analysis, where the primitive representation is that of regular expressions with capture variables. We define additional types of primitive spanner representations by means of two kinds of automata that assign spans to variables. We prove that the first kind has the same expressive power as regular expressions with capture variables; the second kind expresses precisely the algebra of the regular spanners---the closure of the first kind under standard relational operators. The core spanners extend the regular ones by string-equality selection (an extension used in SystemT). We give some fundamental results on the expressiveness of regular and core spanners. As an example, we prove that regular spanners are closed under difference (and complement), but core spanners are not. Finally, we establish connections with related notions in the literature.

#index 1971479
#* Charting the tractability frontier of certain conjunctive query answering
#@ Jef Wijsen
#t 2013
#c 5
#% 36683
#% 273687
#% 288946
#% 289424
#% 838543
#% 949372
#% 1200291
#% 1224935
#% 1347336
#% 1426459
#% 1464322
#% 1552647
#% 1615075
#% 1679459
#% 1700140
#% 1747244
#% 1898008
#% 1950073
#% 1990737
#! An uncertain database is defined as a relational database in which primary keys need not be satisfied. A repair (or possible world) of such database is obtained by selecting a maximal number of tuples without ever selecting two distinct tuples with the same primary key value. For a Boolean query q, the decision problem CERTAINTY(q) takes as input an uncertain database db and asks whether q is satisfied by every repair of db. Our main focus is on acyclic Boolean conjunctive queries without self-join. Previous work has introduced the notion of (directed) attack graph of such queries, and has proved that CERTAINTY(q) is first-order expressible if and only if the attack graph of q is acyclic. The current paper investigates the boundary between tractability and intractability of CERTAINTY(q). We first classify cycles in attack graphs as either weak or strong, and then prove among others the following. If the attack graph of a query q contains a strong cycle, then CERTAINTY(q) is coNP-complete. If the attack graph of q contains no strong cycle and every weak cycle is terminal (i.e., no edge leads from a vertex in the cycle to a vertex outside the cycle), then CERTAINTY(q) is in P. We then partially address the only remaining open case, i.e., when the attack graph contains some nonterminal cycle and no strong cycle. Finally, we establish a relationship between the complexities of CERTAINTY(q) and evaluating q on probabilistic databases.

#index 1971480
#* Enumeration of first-order queries on classes of structures with bounded expansion
#@ Wojciech Kazana;Luc Segoufin
#t 2013
#c 5
#% 93660
#% 101944
#% 189741
#% 292675
#% 384978
#% 986480
#% 1042142
#% 1042143
#% 1521663
#% 1552186
#% 1682389
#% 1914832
#! We consider the evaluation of first-order queries over classes of databases with bounded expansion. The notion of bounded expansion is fairly broad and generalizes bounded degree, bounded treewidth and exclusion of at least one minor. It was known that over a class of databases with bounded expansion, first-order sentences could be evaluated in time linear in the size of the database. We first give a different proof of this result. Moreover, we show that answers to first-order queries can be enumerated with constant delay after a linear time preprocessing. We also show that counting the number of answers to a query can be done in time linear in the size of the database.

#index 1971481
#* On the BDD/FC conjecture
#@ Tomasz Gogacz;Jerzy Marcinkowski
#t 2013
#c 5
#% 874914
#% 1217122
#% 1500877
#% 1511857
#% 1523844
#% 1552660
#% 1808579
#! Bounded Derivation Depth property (BDD) and Finite Controllability (FC) are two properties of sets of datalog rules and tuple generating dependencies (known as Datalog3 programs), which recently attracted some attention. We conjecture that the first of these properties implies the second, and support this conjecture by some evidence proving, among other results, that it holds true for all theories over binary signature.

#index 1971482
#* The fine classification of conjunctive queries and parameterized logarithmic space complexity
#@ Hubie Chen;Moritz Müller
#t 2013
#c 5
#% 248033
#% 263371
#% 268708
#% 292675
#% 335852
#% 338450
#% 339937
#% 384978
#% 427161
#% 599549
#% 735461
#% 756494
#% 809166
#% 857282
#% 880261
#% 927017
#% 1172178
#% 1426325
#% 1464591
#% 1490844
#% 1926690
#% 1972413
#! We perform a fundamental investigation of the complexity of conjunctive query evaluation from the perspective of parameterized complexity. We classify sets of boolean conjunctive queries according to the complexity of this problem. Previous work showed that a set of conjunctive queries is fixed-parameter tractable precisely when the set is equivalent to a set of queries having bounded treewidth. We present a fine classification of query sets up to parameterized logarithmic space reduction. We show that, in the bounded treewidth regime, there are three complexity degrees and that the properties that determine the degree of a query set are bounded pathwidth and bounded tree depth. We also engage in a study of the two higher degrees via logarithmic space machine characterizations and complete problems. Our work yields a significantly richer perspective on the complexity of conjunctive queries and, at the same time, suggests new avenues of research in parameterized complexity.

#index 1971483
#* A dichotomy in the intensional expressive power of nested relational calculi augmented with aggregate functions and a powerset operator
#@ Limsoon Wong
#t 2013
#c 5
#% 91364
#% 99017
#% 159506
#% 163444
#% 189868
#% 210349
#% 245655
#% 245658
#% 303891
#% 332909
#% 342387
#% 435157
#% 464690
#% 587434
#% 766571
#! The extensional aspect of expressive power---i.e., what queries can or cannot be expressed---has been the subject of many studies of query languages. Paradoxically, although efficiency is of primary concern in computer science, the intensional aspect of expressive power---i.e., what queries can or cannot be implemented efficiently---has been much neglected. Here, we discuss the intensional expressive power of NRC(Q, +, ·, ‏, ÷, Σ, powerset), a nested relational calculus augmented with aggregate functions and a powerset operation. We show that queries on structures such as long chains, deep trees, etc. have a dichotomous behaviour: Either they are already expressible in the calculus without using the powerset operation or they require at least exponential space. This result generalizes in three significant ways several old dichotomy-like results, such as that of Suciu and Paredaens that the complex object algebra of Abiteboul and Beeri needs exponential space to implement the transitive closure of a long chain. Firstly, a more expressive query language---in particular, one that captures SQL---is considered here. Secondly, queries on a more general class of structures than a long chain are considered here. Lastly, our proof is more general and holds for all query languages exhibiting a certain normal form and possessing a locality property.

#index 1971484
#* Semantic acyclicity on graph databases
#@ Pablo Barceló Baeza;Miguel Romero;Moshe Y. Vardi
#t 2013
#c 5
#% 451
#% 32904
#% 44930
#% 237180
#% 248025
#% 268708
#% 268797
#% 289266
#% 289425
#% 291299
#% 321058
#% 339937
#% 390685
#% 535150
#% 599549
#% 993437
#% 1019798
#% 1172184
#% 1523818
#% 1538787
#% 1594585
#% 1703568
#% 1770139
#% 1912326
#% 1959480
#! It is known that unions of acyclic conjunctive queries (CQs) can be evaluated in linear time, as opposed to arbitrary CQs, for which the evaluation problem is NP-complete. It follows from techniques in the area of constraint-satisfaction problems that "semantically acyclic" unions of CQs -- i.e., unions of CQs that are equivalent to a union of acyclic ones -- can be evaluated in polynomial time, though testing membership in the class of semantically acyclic CQs is NP-complete. We study here the fundamental notion of semantic acyclicity in the context of graph databases and unions of conjunctive regular path queries with inverse (UC2RPQs). It is known that unions of acyclic C2RPQs can be evaluated efficiently, but it is by no means obvious whether the same holds for the class of UC2RPQs that are semantically acyclic. We prove that checking whether a UC2RPQ is semantically acyclic is decidable in 2EXPSPACE, and that it is EXPSPACE-hard even in the absence of inverses. Furthermore, we show that evaluation of semantically acyclic UC2RPQs is fixed-parameter tractable. In addition, our tools yield a strong theory of approximations for UC2RPQs when no equivalent acyclic UC2RPQ exists.

#index 1971485
#* Collaborative data-driven workflows: think global, act local
#@ Serge Abiteboul;Victor Vianu
#t 2013
#c 5
#% 663
#% 101646
#% 101955
#% 188086
#% 215675
#% 289415
#% 384978
#% 723448
#% 723449
#% 770373
#% 824806
#% 874885
#% 960352
#% 1072645
#% 1134504
#% 1472960
#% 1581842
#% 1745223
#% 1747245
#% 1846805
#! We introduce and study a model of collaborative data-driven workflows. In a local-as-view style, each peer has a partial view of a global instance that remains purely virtual. Local updates have side effects on other peers' data, defined via the global instance. We also assume that the peers provide (an abstraction of) their specifications, so that each peer can actually see and reason on the specification of the entire system. We study the ability of a peer to carry out runtime reasoning about the global run of the system, and in particular about actions of other peers, based on its own local observations. A main contribution is to show that, under a reasonable restriction (namely, key-visibility), one can construct a finite symbolic representation of the infinite set of global runs consistent with given local observations. Using the symbolic representation, we show that we can evaluate in PSPACE a large class of properties over global runs, expressed in an extension of first-order logic with past linear-time temporal operators, PLTL-FO. We also provide a variant of the algorithm allowing to incrementally monitor a statically defined property, and then develop an extension allowing to monitor an infinite class of properties sharing the same temporal structure, defined dynamically as the run unfolds. Finally, we consider an extension of the language, that permits workflow control with PLTL-FO formulas. We prove that this does not increase the power of the workflow specification language, thereby showing that the language is closed under such introspective reasoning.

#index 1971486
#* Deciding monotone duality and identifying frequent itemsets in quadratic logspace
#@ Georg Gottlob
#t 2013
#c 5
#% 2027
#% 21137
#% 65347
#% 101922
#% 109626
#% 197754
#% 212252
#% 221328
#% 237200
#% 251196
#% 264624
#% 307477
#% 415868
#% 420062
#% 444303
#% 448482
#% 450697
#% 474888
#% 476682
#% 496126
#% 496601
#% 504526
#% 543184
#% 543747
#% 563323
#% 579314
#% 641694
#% 711960
#% 791382
#% 1000772
#% 1076675
#% 1076680
#% 1232256
#% 1272324
#% 1394282
#! The monotone duality problem is defined as follows: Given two monotone formulas f and g in irredundant DNF, decide whether f and g are dual. This problem is the same as duality testing for hypergraphs, that is, checking whether a hypergraph H consists of precisely all minimal transversals of a hypergraph G. By exploiting a recent problem-decomposition method by Boros and Makino (ICALP 2009), we show that duality testing for hypergraphs, and thus for monotone DNFs, is feasible in DSPACE(log2 n), i.e., in quadratic logspace. As the monotone duality problem is equivalent to a number of problems in the areas of databases, data mining, and knowledge discovery, the results presented here yield new complexity results for those problems, too. For example, it follows from our results that whenever, for a Boolean-valued relation (whose attributes represent items), a number of maximal frequent itemsets and a number of minimal infrequent itemsets are known, then it can be decided in quadratic logspace whether there exist additional frequent or infrequent itemsets.

#index 1971487
#* When is naive evaluation possible?
#@ Amélie Gheerbrant;Leonid Libkin;Cristina Sirangelo
#t 2013
#c 5
#% 663
#% 11817
#% 26735
#% 94459
#% 95620
#% 109995
#% 119792
#% 289236
#% 378409
#% 384978
#% 527112
#% 556918
#% 576116
#% 587326
#% 806215
#% 865765
#% 866986
#% 1065944
#% 1481057
#% 1541335
#% 1581822
#% 1581823
#% 1818418
#! The term naive evaluation refers to evaluating queries over incomplete databases as if nulls were usual data values, i.e., to using the standard database query evaluation engine. Since the semantics of query answering over incomplete databases is that of certain answers, we would like to know when naive evaluation computes them: i.e., when certain answers can be found without inventing new specialized algorithms. For relational databases it is well known that unions of conjunctive queries possess this desirable property, and results on preservation of formulae under homomorphisms tell us that within relational calculus, this class cannot be extended under the open-world assumption. Our goal here is twofold. First, we develop a general framework that allows us to determine, for a given semantics of incompleteness, classes of queries for which naive evaluation computes certain answers. Second, we apply this approach to a variety of semantics, showing that for many classes of queries beyond unions of conjunctive queries, naive evaluation makes perfect sense under assumptions different from open-world. Our key observations are: (1) naive evaluation is equivalent to monotonicity of queries with respect to a semantics-induced ordering, and (2) for most reasonable semantics, such monotonicity is captured by preservation under various types of homomorphisms. Using these results we find classes of queries for which naive evaluation works, e.g., positive first-order formulae for the closed-world semantics. Even more, we introduce a general relation-based framework for defining semantics of incompleteness, show how it can be used to capture many known semantics and to introduce new ones, and describe classes of first-order queries for which naive evaluation works under such semantics.

#index 1971488
#* On XPath with transitive axes and data tests
#@ Diego Figueira
#t 2013
#c 5
#% 175464
#% 765450
#% 769518
#% 814648
#% 942356
#% 1039062
#% 1181329
#% 1217135
#% 1266894
#% 1424606
#% 1425586
#% 1511885
#% 1531336
#% 1638638
#% 1673664
#% 1871524
#% 1888534
#% 1890839
#! We study the satisfiability problem for XPath with data equality tests. XPath is a node selecting language for XML documents whose satisfiability problem is known to be undecidable, even for very simple fragments. However, we show that the satisfiability for XPath with the rightward, leftward and downward reflexive-transitive axes (namely following-sibling-or-self, preceding-sibling-or-self, descendant-or-self) is decidable. Our algorithm yields a complexity of 3EXPSPACE, and we also identify an expressive-equivalent normal form for the logic for which the satisfiability problem is in 2EXPSPACE. These results are in contrast with the undecidability of the satisfiability problem as soon as we replace the reflexive-transitive axes with just transitive (non-reflexive) ones.

#index 1971489
#* Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data
#@ Kenneth Ross;Divesh Srivastava;Dimitris Papadias
#t 2013
#c 5
#! It is our great pleasure to welcome you to the 2013 ACM SIGMOD Conference on Management of Data, SIGMOD'13. This year the conference is being held in New York City, at the Millennium Broadway Hotel in the Times Square theater district. New York City provides a wide range of attractions, including world-class restaurants, theater, museums, and monuments, with a distinctive architecture and skyline. Many of the larger technical sessions will take place in the Hudson Theater, a real Broadway-style theater, which will give a local flavor to the conference. SIGMOD 2013 hosts an exciting technical program, with two keynote talks that reflect New York City's status as a major financial center, "Big Data in Capital Markets" by Alex Nazaruk and Michael Rauchman (Middle Lake Partners LLC), and "Managing Database Technology at Enterprise Scale" by Paul Yaron (J.P. Morgan Chase); a panel session on "We are Drowning in a Sea of Least Publishable Units (LPUs)" by Michael Stonebraker (MIT), David DeWitt (Microsoft), Jeff Naughton (University of Wisconsin) and Ihab Ilyas (QCRI and University of Waterloo); 76 research paper presentations; 6 tutorials; 43 demonstrations; and 15 industrial presentations. In addition to having full 30-minute presentation slots, SIGMOD research papers, SIGMOD industrial papers, and PODS papers will also be presented in a Research Plenary Poster Session on Wednesday morning. SIGMOD 2013 has a broad workshop program, with 11 co-located workshops preceding the conference. The New Researcher symposium will take place on Tuesday evening, as will the session highlighting the finalists of the SIGMOD programming contest (this year's task is to implement a streaming document filtering system), the Microsoft reception and the Undergraduate Research Poster presentations. On Wednesday evening, SIGMOD will hold an entertainment event in the Hudson Theater, including a comedy performance and a world-class jazz ensemble. We received 372 research paper submissions. For the first time in its history, SIGMOD 2013 involved a reviewing process of two rounds, which replaced the feedback and shepherding mechanisms of previous conferences. 30 papers were accepted during the first round, while 46 were accepted after revisions, for a total of 76. Although the total acceptance rate has increased to 20%, we hope that the revision stage has improved the quality of the technical program. We are thankful to Microsoft CMT and its technical support team for their help during the reviewing process.

#index 1971490
#* Data-driven neuroscience: enabling breakthroughs via innovative data management
#@ Alexandros Stougiannis;Mirjana Pavlovic;Farhan Tauheed;Thomas Heinis;Anastasia Ailamaki
#t 2013
#c 5
#% 24108
#% 114577
#% 252304
#% 330333
#% 462059
#% 480093
#% 485312
#% 797419
#% 927035
#% 1062886
#% 1846821
#% 1880467
#! Scientists in all disciplines increasingly rely on simulations to develop a better understanding of the subject they are studying. For example the neuroscientists we collaborate with in the Blue Brain project have started to simulate the brain on a supercomputer. The level of detail of their models is unprecedented as they model details on the subcellular level (e.g., the neurotransmitter). This level of detail, however, also leads to a true data deluge and the neuroscientists have only few tools to efficiently analyze the data. This demonstration showcases three innovative spatial management techniques that have substantial impact on computational neuroscience and other disciplines in that they allow to build, analyze and simulate bigger and more detailed models. More particularly, we demonstrate a tool that integrates three spatial data management techniques that have enabled breakthroughs in neuroscience: FLAT that enables efficient querying of spatial data, SCOUT that allows for fast exploration of spatial data and HiDOP that makes efficient data discovery possible.

#index 1971491
#* Workload optimization using SharedDB
#@ Georgios Giannikis;Darko Makreshanski;Gustavo Alonso;Donald Kossmann
#t 2013
#c 5
#% 1328168
#% 1563004
#% 1730732
#! This demonstration presents SharedDB, an implementation of a relational database system capable of executing all SQL operators by sharing computation and resources across all running queries. SharedDB sidesteps the traditional query-at-a-time approach and executes queries in batches. Unlike proposed multi-query optimization ideas, in SharedDB queries do not have to contain common subexpressions in order to be part of the same batch, which allows for a higher degree of sharing. By sharing as much as possible, SharedDB avoids repeating parts of computation that is common across all running queries. The goal of this demonstration is to show the ability of shared query execution to a) answer complex and diverse workloads, and b) reduce the interaction among concurrently executed queries that is observed in traditional systems and leads to performance deterioration and instabilities.

#index 1971492
#* Peckalytics: analyzing experts and interests on Twitter
#@ Alex Cheng;Nilesh Bansal;Nick Koudas
#t 2013
#c 5
#! We provide a description of Peckalytics, its technology and functionality. Peckalytics processes the entire Twitter data stream in real time and provides a flexible search interface to identify experts in any topic area as well as users with interests in any topic. It provides flexible analytics around sets of experts, their followers as well as sets of users with specific interests. The system is implemented to scale for large data sizes. At the time of this writing it operates on an archive of 30 billion tweets, with 220,000 new tweets crawled every minute. In addition to raw tweets, the social graph of users, and profile information, Peckalytics makes novel use of Twitter lists to assess the expertise of different users. Our aim is to facilitate targeting and optimization of advertising campaigns on the Twitter platform.

#index 1971493
#* GeoDeepDive: statistical inference using familiar data-processing languages
#@ Ce Zhang;Vidhya Govindaraju;Jackson Borchardt;Tim Foltz;Christopher Ré;Shanan Peters
#t 2013
#c 5
#% 1573237
#% 1895052
#% 1913330
#% 1971514
#! We describe our proposed demonstration of GeoDeepDive, a system that helps geoscientists discover information and knowledge buried in the text, tables, and figures of geology journal articles. This requires solving a host of classical data management challenges including data acquisition (e.g., from scanned documents), data extraction, and data integration. SIGMOD attendees will see demonstrations of three aspects of our system: (1) an end-to-end system that is of a high enough quality to perform novel geological science, but is written by a small enough team so that each aspect can be manageably explained; (2) a simple feature engineering system that allows a user to write in familiar SQL or Python; and (3) the effect of different sources of feedback on result quality including expert labeling, distant supervision, traditional rules, and crowd-sourced data. Our prototype builds on our work integrating statistical inference and learning tools into traditional database systems. If successful, our demonstration will allow attendees to see that data processing systems that use machine learning contain many familiar data processing problems such as efficient querying, indexing, and supporting tools for database-backed websites, none of which are machine-learning problems, per se.

#index 1971494
#* QUBLE: blending visual subgraph query formulation with query processing on large networks
#@ Ho Hoang Hung;Sourav S Bhowmick;Ba Quan Truong;Byron Choi;Shuigeng Zhou
#t 2013
#c 5
#% 274612
#% 629708
#% 727845
#% 1426511
#% 1581983
#% 1770363
#! In a previous paper, we laid out the vision of a novel graph query processing paradigm where instead of processing a visual query graph after its construction, it interleaves visual query formulation and processing by exploiting the latency offered by the GUI [4]. Our recent attempts at implementing this vision [4,6], show significant improvement in the system response time (SRT) for subgraph queries. However, these efforts are designed specifically for graph databases containing a large collection of small or medium-sized graphs. Consequently, its frequent fragment-based action-aware indexing schemes and query processing strategy are unsuitable for supporting subgraph queries on large networks containing thousands of nodes and edges. In this demonstration, we present a novel system called QUBLE (QUery Blender for Large nEtworks) to realize this novel paradigm on large networks. We demonstrate various innovative features of QUBLE and its promising performance.

#index 1971495
#* Query processing on prefix trees live
#@ Thomas Kissinger;Benjamin Schlegel;Dirk Habich;Wolfgang Lehner
#t 2013
#c 5
#% 442850
#% 1089604
#% 1592312
#% 1789648
#! Modern database systems have to process huge amounts of data and should provide results with low latency at the same time. To achieve this, data is nowadays typically hold completely in main memory, to benefit of its high bandwidth and low access latency that could never be reached with disks. Current in-memory databases are usually column-stores that exchange columns or vectors between operators and suffer from a high tuple reconstruction overhead. In this demonstration proposal, we present DexterDB, which implements our novel prefix tree-based processing model that makes indexes the first-class citizen of the database system. The core idea is that each operator takes a set of indexes as input and builds a new index as output that is indexed on the attribute requested by the successive operator. With that, we are able to build composed operators, like the multi-way-select-join-group. Such operators speed up the processing of complex OLAP queries so that DexterDB outperforms state-of-the-art in-memory databases. Our demonstration focuses on the different optimization options for such query plans. Hence, we built an interactive GUI that connects to a DexterDB instance and allows the manipulation of query optimization parameters. The generated query plans and important execution statistics are visualized to help the visitor to understand our processing model.

#index 1971496
#* Stat!: an interactive analytics environment for big data
#@ Mike Barnett;Badrish Chandramouli;Robert DeLine;Steven Drucker;Danyel Fisher;Jonathan Goldstein;Patrick Morrison;John Platt
#t 2013
#c 5
#% 378388
#% 438135
#% 960294
#% 1468411
#% 1523824
#% 1590902
#% 1765681
#% 1846826
#% 1880459
#! Exploratory analysis on big data requires us to rethink data management across the entire stack -- from the underlying data processing techniques to the user experience. We demonstrate Stat! -- a visualization and analytics environment that allows users to rapidly experiment with exploratory queries over big data. Data scientists can use Stat! to quickly refine to the correct query, while getting immediate feedback after processing a fraction of the data. Stat! can work with multiple processing engines in the backend; in this demo, we use Stat! with the Microsoft StreamInsight streaming engine. StreamInsight is used to generate incremental early results to queries and refine these results as more data is processed. Stat! allows data scientists to explore data, dynamically compose multiple queries to generate streams of partial results, and display partial results in both textual and visual form.

#index 1971497
#* SciQL: array data processing inside an RDBMS
#@ Ying Zhang;Martin Kersten;Stefan Manegold
#t 2013
#c 5
#% 125595
#% 248863
#% 427030
#% 482093
#% 845351
#% 1426582
#% 1652704
#% 1882103
#! Scientific discoveries increasingly rely on the ability to efficiently grind massive amounts of experimental data using database technologies. To bridge the gap between the needs of the Data-Intensive Research fields and the current DBMS technologies, we have introduced SciQL (pronounced as 'cycle'). SciQL is the first SQL-based declarative query language for scientific applications with both tables and arrays as first class citizens. It provides a seamless symbiosis of array-, set- and sequence- interpretations. A key innovation is the extension of value-based grouping of SQL:2003 with structural grouping, i.e., group array elements based on their positions. This leads to a generalisation of window-based query processing with wide applicability in science domains. In this demo, we showcase a proof of concept implementation of SciQL in the relational database system MonetDB. First, with the Conway's Game of Life application implemented purely in SciQL queries, we demonstrate the storage of arrays in the MonetDB as first class citizens, and the execution of a comprehensive set of basic operations on arrays. Then, to show the usefulness of SciQL for real-world array data processing use cases, we demonstrate how various common image processing and remote sensing operations are executed as SciQL queries. The audience is invited to challenge SciQL with their use cases.

#index 1971498
#* Less watts, more performance: an intelligent storage engine for data appliances
#@ Louis Woods;Jens Teubner;Gustavo Alonso
#t 2013
#c 5
#% 442850
#% 1444819
#% 1549031
#% 1594599
#% 1770338
#% 1846732
#% 1846841
#! In this demonstration, we present Ibex, a novel storage engine featuring hybrid, FPGA-accelerated query processing. In Ibex, an FPGA is inserted along the path between the storage devices and the database engine. The FPGA acts as an intelligent storage engine supporting query off-loading from the query engine. Apart from significant performance improvements for many common SQL queries, the demo will show how Ibex reduces data movement, CPU usage, and overall energy consumption in database appliances.

#index 1971499
#* A demonstration of SQLVM: performance isolation in multi-tenant relational database-as-a-service
#@ Vivek Narasayya;Sudipto Das;Manoj Syamala;Surajit Chaudhuri;Feng Li;Hyunjung Park
#t 2013
#c 5
#% 1581872
#% 1594596
#% 1846790
#! Sharing resources of a single database server among multiple tenants is common in multi-tenant Database-as-a-Service providers, such as Microsoft SQL Azure. Multi-tenancy enables cost reduction for the cloud service provider which it can pass on as savings to the tenants. However, resource sharing can adversely affect a tenant's performance due to other tenants' workloads contending for shared resources. Service providers today do not provide any assurances to a tenant in terms of isolating its performance from other co-located tenants. SQLVM, a project at Microsoft Research, is an abstraction for performance isolation which is built on a promise of reserving key database server resources, such as CPU, I/O and memory, for each tenant. The key challenge is in supporting this abstraction within a RDBMS without statically allocating resources to tenants, while ensuring low overheads and scaling to large numbers of tenants. This demonstration will show how SQLVM can effectively isolate a tenant's performance from other tenant workloads co-located at the same database server. Our demonstration will use various scripted scenarios and a data collection and visualization framework to illustrate performance isolation using SQLVM.

#index 1971500
#* A query answering system for data with evolution relationships
#@ Siarhei Bykau;Flavio Rizzolo;Yannis Velegrakis
#t 2013
#c 5
#% 992830
#% 1127601
#% 1333840
#% 1642110
#% 1651549
#! Evolving data has attracted considerable research attention. Researchers have focused on modeling and querying of schema/instance-level structural changes, such as, insertion, deletion and modification of attributes. Databases with such a functionality are known as temporal databases. A limitation of the temporal databases is that they treat changes as independent events, while often the appearance (or elimination) of some structure in the database is the result of an evolution of some existing structure. We claim that maintaining the causal relationship between the two structures is of major importance since it allows additional reasoning to be performed and answers to be generated for queries that previously had no answers. We present the TrenDS, a system for exploiting the evolution relationships between the structures in the database. In particular, our system combines different structures that are associated through evolution relationships into virtual structures to be used during query answering. The virtual structures define ``possible'' database instances, in a fashion similar to the possible worlds in the probabilistic databases. TrenDS uses a query answering mechanism that allows queries to be answered over these possible databases without materializing them. Evaluation of such queries raises many technical challenges, since it requires the discovery of Steiner forests on the evolution graphs.

#index 1971501
#* STEM: a spatio-temporal miner for bursty activity
#@ Theodoros Lappas;Marcos R. Vieira;Dimitrios Gunopulos;Vassilis J. Tsotras
#t 2013
#c 5
#% 210347
#% 577220
#% 729943
#% 765412
#% 1022338
#% 1214669
#% 1848111
#! Burst identification has been extensively studied in the context of document streams, where a burst is generally exhibited when an unusually high frequency is observed for a term t. Previous works have focused exclusively on either temporal or spatial burstiness patterns. The former represents bursty timeframes within a single stream, while the latter characterizes sets of streams that simultaneously exhibited a bursty behavior for a user-specified timeframe. Our previous work was the first to study the spatiotemporal burstiness of terms. In this context, a burstiness pattern consists of both a timeframe and a set of streams, both of which need to be identified automatically. In this paper we describe STEM (Spatio-TEmporal Miner), a system for finding spatiotemporal burstiness patterns in a collection of spatially distributed frequency streams. STEM implements the full functionality required to mine spatiotemporal burstiness patterns from virtually any collection of geostamped streams. Examples of such collections include document streams (e.g. online newspapers), geo-aware microblogging platforms (e.g. Twitter). This paper describes the STEM system and discusses how its features can be accessed via a user-friendly interface.

#index 1971502
#* Research-insight: providing insight on research by publication network analysis
#@ Fangbo Tao;Xiao Yu;Kin Hou Lei;George Brova;Xiao Cheng;Jiawei Han;Rucha Kanade;Yizhou Sun;Chi Wang;Lidan Wang;Tim Weninger
#t 2013
#c 5
#% 1181261
#% 1214701
#% 1451159
#% 1495579
#% 1560218
#% 1606073
#% 1635098
#% 1693927
#% 1898007
#! A database contains rich, inter-related, multi-typed data and information, forming one or a set of gigantic, intercon- nected, heterogeneous information networks. Much knowl- edge can be derived from such information networks if we systematically develop an effective and scalable database-oriented information network analysis technology. In this system demo, we take a computer science research publica- tion network as an example, which is an information net- work derived from an integration of DBLP, other web-based information about researchers, and partially available cita- tion data, and construct a Research-Insight system in order to demonstrate the power of database-oriented information network analysis. We show that nontrivial research insight can be obtained from such analysis, including (1) ranking, clustering, classification and similarity search of researchers, terms and venues for research subfields and themes, (2) recommending good researchers and good research papers to read or cite when conducting research on certain topics (3) predicting potential collaborators for certain theme-oriented research, and (4) predicting advisor-advisee rela- tionships and affiliation history based on historical research publications. Although some of these functions have been studied in recent research, effective and scalable realization of such functions in large networks still poses challenging research problems. Moreover, some function are our on- going research tasks. By integrating these functionalities, Research-Insight may not only provide with us insightful rec- ommendations in CS research but also help us gain insight on how to perform effective data mining in large databases.

#index 1971503
#* TsingNUS: a location-based service system towards live city
#@ Guoliang Li;Nan Zhang;Ruicheng Zhong;Sitong Liu;Weihuang Huang;Ju Fan;Kian-Lee Tan;Lizhu Zhou;Jianhua Feng
#t 2013
#c 5
#% 1328137
#% 1581876
#% 1594674
#% 1846749
#% 1848110
#% 1918373
#% 1918428
#% 1919899
#! We present our system towards live city, called TsingNUS, aiming to provide users with more user-friendly location-aware search experiences. TsingNUS crawls location-based user-generated content from the Web (e.g., Foursquare and Twitter), cleans and integrates them to provide users with rich well-structured data. TsingNUS provides three user-friendly search paradigms: location-aware instant search, location-aware similarity search and direction-aware search. Instant search returns relevant answers instantly as users type in queries letter by letter, which can help users to save typing efforts significantly. Location-aware similarity search enables fuzzy matching between queries and the underlying data, which can tolerate typing errors. The two features boost the search performance and improve the experiences for mobile users who often misspell the keywords due to the limitation of the mobile phone's keyboard. In addition, users have direction-aware search requirements in many applications. For example, a driver on the highway wants to find the nearest gas station or restaurant. She has a search requirement that the answers should be in front of her driving direction. TsingNUS enables direction-aware search to address this problem and allows users to search in specific directions. Moreover, TsingNUS incorporates continuous search to efficiently support continuously moving queries in a client-server system which can reduce the number of queries submitted to the server and communication cost between the client and server. We have implemented and deployed a system which has been commonly used and widely accepted.

#index 1971504
#* Continuous outlier detection in data streams: an extensible framework and state-of-the-art algorithms
#@ Dimitrios Georgiadis;Maria Kontaki;Anastasios Gounaris;Apostolos N. Papadopoulos;Kostas Tsichlas;Yannis Manolopoulos
#t 2013
#c 5
#% 479462
#% 479791
#% 918001
#% 1181258
#% 1375640
#% 1472282
#% 1594605
#! Anomaly detection is an important data mining task, aiming at the discovery of elements that show significant diversion from the expected behavior; such elements are termed as outliers. One of the most widely employed criteria for determining whether an element is an outlier is based on the number of neighboring elements within a fixed distance (R), against a fixed threshold (k). Such outliers are referred to as distance-based outliers and are the focus of this work. In this demo, we show both an extendible framework for outlier detection algorithms and specific outlier detection algorithms for the demanding case where outlier detection is continuously performed over a data stream. More specifically: i) first we demonstrate a novel flavor of an open-source publicly available tool for Massive Online Analysis (MOA) that is endowed with capabilities to encapsulate algorithms that continuously detect outliers and ii) second, we present four online outlier detection algorithms. Two of these algorithms have been designed by the authors of this demo, with a view to improving on key aspects related to outlier mining, such as running time, flexibility and space requirements.

#index 1971505
#* Fact checking and analyzing the web
#@ François Goasdoué;Konstantinos Karanasos;Yannis Katsis;Julien Leblay;Ioana Manolescu;Stamatis Zampetakis
#t 2013
#c 5
#% 1399967
#! Fact checking and data journalism are currently strong trends. The sheer amount of data at hand makes it difficult even for trained professionals to spot biased, outdated or simply incorrect information. We propose to demonstrate FactMinder, a fact checking and analysis assistance application. SIGMOD attendees will be able to analyze documents using FactMinder and experience how background knowledge and open data repositories help build insightful overviews of current topics.

#index 1971506
#* Iterative parallel data processing with stratosphere: an inside look
#@ Stephan Ewen;Sebastian Schelter;Kostas Tzoumas;Daniel Warneke;Volker Markl
#t 2013
#c 5
#% 69503
#% 963669
#% 1328059
#% 1426486
#% 1426513
#% 1880444
#% 1880445
#! Iterative algorithms occur in many domains of data analysis, such as machine learning or graph analysis. With increasing interest to run those algorithms on very large data sets, we see a need for new techniques to execute iterations in a massively parallel fashion. In prior work, we have shown how to extend and use a parallel data flow system to efficiently run iterative algorithms in a shared-nothing environment. Our approach supports the incremental processing nature of many of those algorithms. In this demonstration proposal we illustrate the process of implementing, compiling, optimizing, and executing iterative algorithms on Stratosphere using examples from graph analysis and machine learning. For the first step, we show the algorithm's code and a visualization of the produced data flow programs. The second step shows the optimizer's execution plan choices, while the last phase monitors the execution of the program, visualizing the state of the operators and additional metrics, such as per-iteration runtime and number of updates. To show that the data flow abstraction supports easy creation of custom programming APIs, we also present programs written against a lightweight Pregel API that is layered on top of our system with a small programming effort.

#index 1971507
#* SONDY: an open source platform for social dynamics mining and analysis
#@ Adrien Guille;Cécile Favre;Hakim Hacid;Djamel A. Zighed
#t 2013
#c 5
#% 1176853
#% 1432574
#% 1477588
#% 1556479
#% 1573368
#% 1592948
#! This paper describes SONDY, a tool for analysis of trends and dynamics in online social network data. SONDY addresses two audiences: (i) end-users who want to explore social activity and (ii) researchers who want to experiment and compare mining techniques on social data. SONDY helps end-users like media analysts or journalists understand social network users interests and activity by providing emerging topics and events detection as well as network analysis functionalities. To this end, the application proposes visualizations such as interactive time-lines that summarize information and colored user graphs that reflect the structure of the network. SONDY also provides researchers an easy way to compare and evaluate recent techniques to mine social data, implement new algorithms and extend the application without being concerned with how to make it accessible. In the demo, participants will be invited to explore information from several datasets of various sizes and origins (such as a dataset consisting of 7,874,772 messages published by 1,697,759 Twitter users during a period of 7 days) and apply the different functionalities of the platform in real-time.

#index 1971508
#* Noah: a dynamic ridesharing system
#@ Charls Tian;Yan Huang;Zhi Liu;Favyen Bastani;Ruoming Jin
#t 2013
#c 5
#% 548623
#% 1044493
#% 1230568
#% 1484129
#% 1597258
#! This demo presents Noah: a dynamic ridesharing system. Noah supports large scale real-time ridesharing with service guarantee on road networks. Taxis and trip requests are dynamically matched. Different from traditional systems, a taxi can have more than one customer on board given that all waiting time and service time constraints of trips are satisfied. Noah's real-time response relies on three main components: (1) a fast shortest path algorithm with caching on road networks; (2) fast dynamic matching algorithms to schedule ridesharing on the fly; (3) a spatial indexing method for fast retrieving moving taxis. Users will be able to submit requests from a smartphone, choose specific parameters such as number of taxis in the system, service constraints, and matching algorithms, to explore the internal functionalities and implementations of Noah. The system analyzer will show the system performance including average waiting time, average detour percentage, average response time, and average level of sharing. Taxis, routes, and requests will be animated and visualized through Google Maps API. The demo is based on trips of 17,000 Shanghai taxis for one day (May 29, 2009); the dataset contains 432,327 trips. Each trip includes the starting and destination coordinates and the start time. An iPhone application is implemented to allow users to submit a trip request to the Noah system during the demonstration.

#index 1971509
#* Interactive data mining with 3D-parallel-coordinate-trees
#@ Elke Achtert;Hans-Peter Kriegel;Erich Schubert;Arthur Zimek
#t 2013
#c 5
#% 240337
#% 273891
#% 300131
#% 300183
#% 333929
#% 529288
#% 574319
#% 726032
#% 749202
#% 765439
#% 778279
#% 785332
#% 785335
#% 798816
#% 844527
#% 880918
#% 1137067
#% 1165480
#% 1196030
#% 1560072
#% 1618290
#% 1663653
#% 1846695
#% 1846754
#% 1852892
#% 1912090
#% 1945139
#% 1978716
#% 1984073
#! Parallel coordinates are an established technique to visualize high-dimensional data, in particular for data mining purposes. A major challenge is the ordering of axes, as any axis can have at most two neighbors when placed in parallel on a 2D plane. By extending this concept to a 3D visualization space we can place several axes next to each other. However, finding a good arrangement often does not necessarily become easier, as still not all axes can be arranged pairwise adjacently to each other. Here, we provide a tool to explore complex data sets using 3D-parallel-coordinate-trees, along with a number of approaches to arrange the axes.

#index 1971510
#* StreamWorks: a system for dynamic graph search
#@ Sutanay Choudhury;Lawrence Holder;George Chin;Abhik Ray;Sherman Beus;John Feo
#t 2013
#c 5
#% 300179
#% 654510
#% 726621
#% 1464040
#% 1581923
#% 1848107
#! Acting on time-critical events by processing ever growing social media, news or cyber data streams is a major technical challenge. Many of these data sources can be modeled as multi-relational graphs. Mining and searching for subgraph patterns in a continuous setting requires an efficient approach to incremental graph search. The goal of our work is to enable real-time search capabilities for graph databases. This demonstration will present a dynamic graph query system that leverages the structural and semantic characteristics of the underlying multi-relational graph.

#index 1971511
#* MESSIAH: missing element-conscious SLCA nodes search in XML data
#@ Ba Quan Truong;Sourav S Bhowmick;Curtis Dyreson;Aixin Sun
#t 2013
#c 5
#% 333981
#% 397366
#% 479465
#% 745463
#% 810052
#% 956599
#% 960261
#% 994015
#% 1015258
#% 1016135
#% 1127424
#% 1181282
#% 1292476
#% 1370256
#% 1372729
#% 1456849
#% 1464039
#% 1846814
#! Keyword search for smallest lowest common ancestors (SLCAs) in XML data has been widely accepted as a meaningful way to identify matching nodes where their subtrees contain an input set of keywords. Although SLCA and its variants (e.g.,MLCA) perform admirably in identifying matching nodes, surprisingly, they perform poorly for searches on irregular schemas that have missing elements, that is, (sub)elements that are optional, or appear in some instances of an element type but not all (e.g., a "population" subelement in a "city" element might be optional, appearing when the population is known and absent when the population is unknown). In this paper, we generalize the SLCA search paradigm to support queries involving missing elements. Specifically, we propose a novel property called optionality resilience that specifies the desired behaviors of an XML keyword search (XKS) approach for queries involving missing elements. We present two variants of a novel algorithm called MESSIAH (Missing Element-conSciouS hIgh-quality SLCA searcH), which are optionality resilient to irregular documents. MESSIAH logically transforms an XML document to a minimal full document where all missing elements are represented as empty elements, i.e., the irregular schema is made "regular", and then employs efficient strategies to identify partial and complete full SLCA nodes (SLCA nodes in the full document) from it. Specifically, it generates the same SLCA nodes as any state-of-the-art approach when the query does not involve missing elements but avoids irrelevant results when missing elements are involved. Our experimental study demonstrates the ability of MESSIAH to produce superior quality search results.

#index 1971512
#* TOUCH: in-memory spatial join by hierarchical data-oriented partitioning
#@ Sadegh Nobari;Farhan Tauheed;Thomas Heinis;Panagiotis Karras;Stéphane Bressan;Anastasia Ailamaki
#t 2013
#c 5
#% 2115
#% 86950
#% 86952
#% 114577
#% 152937
#% 172379
#% 172909
#% 172962
#% 210186
#% 210187
#% 227932
#% 232456
#% 388000
#% 427199
#% 452852
#% 462059
#% 479797
#% 480093
#% 481455
#% 659919
#% 765430
#% 927035
#% 1062886
#% 1712062
#! Efficient spatial joins are pivotal for many applications and particularly important for geographical information systems or for the simulation sciences where scientists work with spatial models. Past research has primarily focused on disk-based spatial joins; efficient in-memory approaches, however, are important for two reasons: a) main memory has grown so large that many datasets fit in it and b) the in-memory join is a very time-consuming part of all disk-based spatial joins. In this paper we develop TOUCH, a novel in-memory spatial join algorithm that uses hierarchical data-oriented space partitioning, thereby keeping both its memory footprint and the number of comparisons low. Our results show that TOUCH outperforms known in-memory spatial-join algorithms as well as in-memory implementations of disk-based join approaches. In particular, it has a one order of magnitude advantage over the memory-demanding state of the art in terms of number of comparisons (i.e., pairwise object comparisons), as well as execution time, while it is two orders of magnitude faster when compared to approaches with a similar memory footprint. Furthermore, TOUCH is more scalable than competing approaches as data density grows.

#index 1971513
#* CS2: a new database synopsis for query estimation
#@ Feng Yu;Wen-Chi Hou;Cheng Luo;Dunren Che;Mengxia Zhu
#t 2013
#c 5
#% 82346
#% 102765
#% 116084
#% 137885
#% 210173
#% 214073
#% 248822
#% 273903
#% 273909
#% 277347
#% 285924
#% 287667
#% 321468
#% 333946
#% 333983
#% 393907
#% 411554
#% 480465
#% 482092
#% 482123
#% 689389
#% 765456
#% 816392
#% 874987
#% 956455
#% 1174744
#! Fast and accurate estimations for complex queries are profoundly beneficial for large databases with heavy workloads. In this research, we propose a statistical summary for a database, called CS2 (Correlated Sample Synopsis), to provide rapid and accurate result size estimations for all queries with joins and arbitrary selections. Unlike the state-of-the-art techniques, CS2 does not completely rely on simple random samples, but mainly consists of correlated sample tuples that retain join relationships with less storage. We introduce a statistical technique, called reverse sample, and design a powerful estimator, called reverse estimator, to fully utilize correlated sample tuples for query estimation. We prove both theoretically and empirically that the reverse estimator is unbiased and accurate using CS2. Extensive experiments on multiple datasets show that CS2 is fast to construct and derives more accurate estimations than existing methods with the same space budget.

#index 1971514
#* Towards high-throughput gibbs sampling at scale: a study across storage managers
#@ Ce Zhang;Christopher Ré
#t 2013
#c 5
#% 663
#% 44876
#% 390132
#% 464434
#% 722904
#% 765682
#% 798509
#% 893189
#% 939376
#% 1022341
#% 1063521
#% 1083687
#% 1127378
#% 1166535
#% 1190065
#% 1206987
#% 1291115
#% 1291123
#% 1306948
#% 1417383
#% 1426585
#% 1467732
#% 1523858
#% 1523866
#% 1552647
#% 1558463
#% 1560203
#% 1573237
#% 1581889
#% 1590538
#% 1765219
#% 1846811
#% 1895052
#! Factor graphs and Gibbs sampling are a popular combination for Bayesian statistical methods that are used to solve diverse problems including insurance risk models, pricing models, and information extraction. Given a fixed sampling method and a fixed amount of time, an implementation of a sampler that achieves a higher throughput of samples will achieve a higher quality than a lower-throughput sampler. We study how (and whether) traditional data processing choices about materialization, page layout, and buffer-replacement policy need to be changed to achieve high-throughput Gibbs sampling for factor graphs that are larger than main memory. We find that both new theoretical and new algorithmic techniques are required to understand the tradeoff space for each choice. On both real and synthetic data, we demonstrate that traditional baseline approaches may achieve two orders of magnitude lower throughput than an optimal approach. For a handful of popular tasks across several storage backends, including HBase and traditional unix files, we show that our simple prototype achieves competitive (and sometimes better) throughput compared to specialized state-of-the-art approaches on factor graphs that are larger than main memory.

#index 1971515
#* I/O efficient: computing SCCs in massive graphs
#@ Zhiwei Zhang;Jeffrey Xu Yu;Lu Qin;Lijun Chang;Xuemin Lin
#t 2013
#c 5
#% 41684
#% 281655
#% 303087
#% 341100
#% 397472
#% 407822
#% 410276
#% 548494
#% 617131
#% 847101
#% 985929
#% 1224046
#% 1230562
#% 1523819
#% 1523898
#% 1770365
#! A strongly connected component (SCC) is a maximal subgraph of a directed graph G in which every pair of nodes are reachable from each other in the SCC. With such a property, a general directed graph can be represented by a directed acyclic graph DAG by contracting an SCC of G to a node in DAG. In many real applications that need graph pattern matching, topological sorting, or reachability query processing, the best way to deal with a general directed graph is to deal with its DAG representation. Therefore, finding all SCCs in a directed graph G is a critical operation. The existing in-memory algorithms based on depth first search (DFS) can find all SCCs in linear time w.r.t. the size of a graph. However, when a graph cannot resident entirely in the main memory, the existing external or semi-external algorithms to find all SCCs have limitation to achieve high I/O efficiency. In this paper, we study new I/O efficient semi-external algorithms to find all SCCs for a massive directed graph G that cannot reside in main memory entirely. To overcome the deficiency of the existing DFS based semi-external algorithm that heavily relies on a total order, we explore a weak order based on which we investigate new algorithms. We propose a new two phase algorithm, namely, tree construction and tree search. In the tree construction phase, a spanning tree of G can be constructed in bounded sequential scans of G. In the tree search phase, it needs to sequentially scan the graph once to find all SCCs. In addition, we propose a new single phase algorithm, which combines the tree construction and tree search phases into a single phase, with three new optimization techniques. They are early acceptance, early rejection, and batch processing. By the single phase algorithm with the new optimization techniques, we can significantly reduce the number of I/Os and CPU cost. We conduct extensive experimental studies using 4 real datasets including a massive real dataset, and several synthetic datasets to confirm the I/O efficiency of our approaches.

#index 1971516
#* Massive graph triangulation
#@ Xiaocheng Hu;Yufei Tao;Chin-Wan Chung
#t 2013
#c 5
#% 1604
#% 41684
#% 1124590
#% 1245882
#% 1256460
#% 1523970
#% 1605988
#% 1606069
#% 1625106
#% 1654051
#% 1719564
#% 1723782
#% 1770365
#% 1848109
#% 1907284
#! This paper studies I/O-efficient algorithms for settling the classic triangle listing problem, whose solution is a basic operator in dealing with many other graph problems. Specifically, given an undirected graph G, the objective of triangle listing is to find all the cliques involving 3 vertices in G. The problem has been well studied in internal memory, but remains an urgent difficult challenge when G does not fit in memory, rendering any algorithm to entail frequent I/O accesses. Although previous research has attempted to tackle the challenge, the state-of-the-art solutions rely on a set of crippling assumptions to guarantee good performance. Motivated by this, we develop a new algorithm that is provably I/O and CPU efficient at the same time, without making any assumption on the input G at all. The algorithm uses ideas drastically different from all the previous approaches, and outperformed the existing competitors by a factor over an order of magnitude in our extensive experimentation.

#index 1971517
#* Discovering XSD keys from XML data
#@ Marcelo Arenas;Jonny Daenen;Frank Neven;Martin Ugarte;Jan Van den Bussche;Stijn Vansummeren
#t 2013
#c 5
#% 64419
#% 70235
#% 125557
#% 169370
#% 257873
#% 384978
#% 393844
#% 398752
#% 413602
#% 420062
#% 431034
#% 462472
#% 563958
#% 733268
#% 848763
#% 894435
#% 1021195
#% 1022285
#% 1036080
#% 1063573
#% 1192271
#% 1200330
#% 1217202
#% 1370257
#% 1456293
#% 1818415
#! A great deal of research into the learning of schemas from XML data has been conducted in recent years to enable the automatic discovery of XML Schemas from XML documents when no schema, or only a low-quality one is available. Unfortunately, and in strong contrast to, for instance, the relational model, the automatic discovery of even the simplest of XML constraints, namely XML keys, has been left largely unexplored in this context. A major obstacle here is the unavailability of a theory on reasoning about XML keys in the presence of XML schemas, which is needed to validate the quality of candidate keys. The present paper embarks on a fundamental study of such a theory and classifies the complexity of several crucial properties concerning XML keys in the presence of an XSD, like, for instance, testing for consistency, boundedness, satisfiability, universality, and equivalence. Of independent interest, novel results are obtained related to cardinality estimation of XPath result sets. A mining algorithm is then developed within the framework of levelwise search. The algorithm leverages known discovery algorithms for functional dependencies in the relational model, but incorporates the above mentioned properties to assess and refine the quality of derived keys. An experimental study on an extensive body of real world XML data evaluating the effectiveness of the proposed algorithm is provided.

#index 1971518
#* Don't be SCAREd: use SCalable Automatic REpairing with maximal likelihood and bounded changes
#@ Mohamed Yakout;Laure Berti-Équille;Ahmed K. Elmagarmid
#t 2013
#c 5
#% 92286
#% 203148
#% 303305
#% 551723
#% 722754
#% 855904
#% 913783
#% 961190
#% 1022228
#% 1042787
#% 1063725
#% 1180000
#% 1206961
#% 1408780
#% 1426508
#% 1434292
#% 1523810
#% 1550749
#% 1581885
#! Various computational procedures or constraint-based methods for data repairing have been proposed over the last decades to identify errors and, when possible, correct them. However, these approaches have several limitations including the scalability and quality of the values to be used in replacement of the errors. In this paper, we propose a new data repairing approach that is based on maximizing the likelihood of replacement data given the data distribution, which can be modeled using statistical machine learning techniques. This is a novel approach combining machine learning and likelihood methods for cleaning dirty databases by value modification. We develop a quality measure of the repairing updates based on the likelihood benefit and the amount of changes applied to the database. We propose SCARE (SCalable Automatic REpairing), a systematic scalable framework that follows our approach. SCARE relies on a robust mechanism for horizontal data partitioning and a combination of machine learning techniques to predict the set of possible updates. Due to data partitioning, several updates can be predicted for a single record based on local views on each data partition. Therefore, we propose a mechanism to combine the local predictions and obtain accurate final predictions. Finally, we experimentally demonstrate the effectiveness, efficiency, and scalability of our approach on real-world datasets in comparison to recent data cleaning approaches.

#index 1971519
#* The big data ecosystem at LinkedIn
#@ Roshan Sumbaly;Jay Kreps;Sam Shah
#t 2013
#c 5
#% 152934
#% 452563
#% 640616
#% 730089
#% 923029
#% 963669
#% 978404
#% 998845
#% 1063527
#% 1063553
#% 1183091
#% 1217199
#% 1405658
#% 1426588
#% 1468530
#% 1471641
#% 1475757
#% 1476492
#% 1518201
#% 1527023
#% 1542029
#% 1581937
#% 1581940
#% 1594588
#% 1701031
#% 1765838
#% 1770415
#% 1895058
#% 1895067
#% 1918401
#! The use of large-scale data mining and machine learning has proliferated through the adoption of technologies such as Hadoop, with its simple programming semantics and rich and active ecosystem. This paper presents LinkedIn's Hadoop-based analytics stack, which allows data scientists and machine learning researchers to extract insights and build product features from massive amounts of data. In particular, we present our solutions to the ``last mile'' issues in providing a rich developer ecosystem. This includes easy ingress from and egress to online systems, and managing workflows as production processes. A key characteristic of our solution is that these distributed system concerns are completely abstracted away from researchers. For example, deploying data back into the online system is simply a 1-line Pig command that a data scientist can add to the end of their script. We also present case studies on how this ecosystem is used to solve problems ranging from recommendations to news feed updates to email digesting to descriptive analytical dashboards for our members.

#index 1971520
#* Enhancements to SQL server column stores
#@ Per-Ake Larson;Cipri Clinciu;Campbell Fraser;Eric N. Hanson;Mostafa Mokhtar;Michal Nowakiewicz;Vassilis Papadimos;Susan L. Price;Srikumar Rangarajan;Remus Rusanu;Mayukh Saubhasik
#t 2013
#c 5
#% 286258
#% 287349
#% 824697
#% 893129
#% 990389
#% 1426547
#% 1581947
#% 1647981
#! SQL Server 2012 introduced two innovations targeted for data warehousing workloads: column store indexes and batch (vectorized) processing mode. Together they greatly improve performance of typical data warehouse queries, routinely by 10X and in some cases by a 100X or more. The main limitations of the initial version are addressed in the upcoming release. Column store indexes are updatable and can be used as the base storage for a table. The repertoire of batch mode operators has been expanded, existing operators have been improved, and query optimization has been enhanced. This paper gives an overview of SQL Server's column stores and batch processing, in particular the enhancements introduced in the upcoming release.

#index 1971521
#* Split query processing in polybase
#@ David J. DeWitt;Alan Halverson;Rimma Nehme;Srinath Shankar;Josep Aguilar-Saborit;Artin Avanes;Miro Flasza;Jim Gramling
#t 2013
#c 5
#% 1426583
#% 1581939
#% 1581946
#% 1621136
#% 1770411
#! This paper presents Polybase, a feature of SQL Server PDW V2 that allows users to manage and query data stored in a Hadoop cluster using the standard SQL query language. Unlike other database systems that provide only a relational view over HDFS-resident data through the use of an external table mechanism, Polybase employs a split query processing paradigm in which SQL operators on HDFS-resident data are translated into MapReduce jobs by the PDW query optimizer and then executed on the Hadoop cluster. The paper describes the design and implementation of Polybase along with a thorough performance evaluation that explores the benefits of employing a split query processing paradigm for executing queries that involve both structured data in a relational DBMS and unstructured data in Hadoop. Our results demonstrate that while the use of a split-based query execution paradigm can improve the performance of some queries by as much as 10X, one must employ a cost-based query optimizer that considers a broad set of factors when deciding whether or not it is advantageous to push a SQL operator to Hadoop. These factors include the selectivity factor of the predicate, the relative sizes of the two clusters, and whether or not their nodes are co-located. In addition, differences in the semantics of the Java and SQL languages must be carefully considered in order to avoid altering the expected results of a query.

#index 1971522
#* Hekaton: SQL server's memory-optimized OLTP engine
#@ Cristian Diaconu;Craig Freedman;Erik Ismert;Per-Ake Larson;Pravin Mittal;Ryan Stonecipher;Nitin Verma;Mike Zwilling
#t 2013
#c 5
#% 397444
#% 753278
#% 1063543
#% 1063818
#% 1127596
#% 1181215
#% 1523878
#% 1523974
#% 1592312
#% 1594617
#% 1668635
#! Hekaton is a new database engine optimized for memory resident data and OLTP workloads. Hekaton is fully integrated into SQL Server; it is not a separate system. To take advantage of Hekaton, a user simply declares a table memory optimized. Hekaton tables are fully transactional and durable and accessed using T-SQL in the same way as regular SQL Server tables. A query can reference both Hekaton tables and regular tables and a transaction can update data in both types of tables. T-SQL stored procedures that reference only Hekaton tables can be compiled into machine code for further performance improvements. The engine is designed for high con-currency. To achieve this it uses only latch-free data structures and a new optimistic, multiversion concurrency control technique. This paper gives an overview of the design of the Hekaton engine and reports some experimental results.

#index 1971523
#* Query containment in entity SQL
#@ Guillem Rull;Philip A. Bernstein;Ivo Garcia dos Santos;Yannis Katsis;Sergey Melnik;Ernest Teniente
#t 2013
#c 5
#% 36181
#% 123085
#% 289266
#% 443173
#% 465050
#% 599549
#% 824148
#% 960309
#% 1092008
#% 1426453
#% 1661452
#% 1669512
#! We describe a software architecture we have developed for a constructive containment checker of Entity SQL queries defined over extended ER schemas expressed in Microsoft's Entity Data Model. Our application of interest is compilation of object-to-relational mappings for Microsoft's ADO.NET Entity Framework, which has been shipping since 2007. The supported language includes several features which have been individually addressed in the past but, to the best of our knowledge, they have not been addressed all at once before. Moreover, when embarking on an implementation, we found no guidance in the literature on how to modularize the software or apply published algorithms to a commercially-supported language. This paper reports on our experience in addressing these real-world challenges.

#index 1971524
#* BigBench: towards an industry standard benchmark for big data analytics
#@ Ahmad Ghazal;Tilmann Rabl;Minqing Hu;Francois Raab;Meikel Poess;Alain Crolotte;Hans-Arno Jacobsen
#t 2013
#c 5
#% 152904
#% 227875
#% 285555
#% 893175
#% 954300
#% 994015
#% 1022297
#% 1023420
#% 1063553
#% 1217159
#% 1328059
#% 1332777
#% 1426210
#% 1426489
#% 1545218
#% 1545222
#% 1621133
#% 1747213
#% 1895054
#! There is a tremendous interest in big data by academia, industry and a large user base. Several commercial and open source providers unleashed a variety of products to support big data storage and processing. As these products mature, there is a need to evaluate and compare the performance of these systems. In this paper, we present BigBench, an end-to-end big data benchmark proposal. The underlying business model of BigBench is a product retailer. The proposal covers a data model and synthetic data generator that addresses the variety, velocity and volume aspects of big data systems containing structured, semi-structured and unstructured data. The structured part of the BigBench data model is adopted from the TPC-DS benchmark, which is enriched with semi-structured and unstructured data components. The semi-structured part captures registered and guest user clicks on the retailer's website. The unstructured data captures product reviews submitted online. The data generator designed for BigBench provides scalable volumes of raw data based on a scale factor. The BigBench workload is designed around a set of queries against the data model. From a business prospective, the queries cover the different categories of big data analytics proposed by McKinsey. From a technical prospective, the queries are designed to span three different dimensions based on data sources, query processing types and analytic techniques. We illustrate the feasibility of BigBench by implementing it on the Teradata Aster Database. The test includes generating and loading a 200 Gigabyte BigBench data set and testing the workload by executing the BigBench queries (written using Teradata Aster SQL-MR) and reporting their response times.

#index 1971525
#* Petabyte scale databases and storage systems at Facebook
#@ Dhruba Borthakur
#t 2013
#c 5
#! At Facebook, we use various types of databases and storage system to satisfy the needs of different applications. The solutions built around these data store systems have a common set of requirements: they have to be highly scalable, maintenance costs should be low and they have to perform efficiently. We use a sharded mySQL+memcache solution to support real-time access of tens of petabytes of data and we use TAO to provide consistency of this web-scale database across geographical distances. We use Haystack data store for storing the 3 billion new photos we host every week. We use Apache Hadoop to mine intelligence from 100 petabytes of click logs and combine it with the power of Apache HBase to store all Facebook Messages. This paper describes the reasons why each of these databases is appropriate for that workload and the design decisions and tradeoffs that were made while implementing these solutions. We touch upon the consistency, availability and partitioning tolerance of each of these solutions. We touch upon the reasons why some of these systems need ACID semantics and other systems do not. We describe the techniques we have used to map the Facebook Graph Database into a set of relational tables. We speak of how we plan to do big-data deployments across geographical locations and our requirements for a new breed of pure-memory and pure-SSD based transactional database. Esteemed researchers in the Database Management community have benchmarked query latencies on Hive/Hadoop to be less performant than a traditional Parallel DBMS. We describe why these benchmarks are insufficient for Big Data deployments and why we continue to use Hadoop/Hive. We present an alternate set of benchmark techniques that measure capacity of a database, the value/byte in that database and the efficiency of inbuilt crowd-sourcing techniques to reduce administration costs of that database.

#index 1971526
#* Parallel analytics as a service
#@ Petrie Wong;Zhian He;Eric Lo
#t 2013
#c 5
#% 172913
#% 1063561
#% 1207027
#% 1312540
#% 1549846
#% 1549874
#% 1581872
#% 1581874
#% 1581943
#% 1594649
#% 1594650
#% 1621129
#% 1688297
#% 1770323
#% 1770324
#% 1846730
#% 1895061
#! Recently, massively parallel processing relational database systems (MPPDBs) have gained much momentum in the big data analytic market. With the advent of hosted cloud computing, we envision that the offering of MPPDB-as-a-Service (MPPDBaaS) will become attractive for companies having analytical tasks on only hundreds gigabytes to some ten terabytes of data because they can enjoy high-end parallel analytics at a cheap cost. This paper presents Thrifty, a prototype implementation of MPPDB-as-a-service. The major research issue is how to achieve a lower total cost of ownership by consolidating thousands of MPPDB tenants on to a shared hardware infrastructure, with a performance SLA that guarantees the tenants can obtain the query results as if they are executing their queries on dedicated machines. Thrifty achieves the goal by using a tenant-driven design that includes (1) a cluster design that carefully arranges the nodes in the cluster into groups and creates an MPPDB for each group of nodes, (2) a tenant placement that assigns each tenant to several MPPDBs (for high availability service through replication), and (3) a query routing algorithm that routes a tenant's query to the proper MPPDB at run-time. Experiments show that in a MPPDBaaS with 5000 tenants, where each tenant requests 2 to 32 nodes MPPDB to query against 200GB to 3.2TB of data, Thrifty can serve all the tenants with a 99.9% performance SLA guarantee and a high availability replication factor of 3, using only 18.7% of the nodes requested by the tenants.

#index 1971527
#* ε-Matching: event processing over noisy sequences in real time
#@ Zheng Li;Tingjian Ge;Cindy X. Chen
#t 2013
#c 5
#% 3888
#% 190611
#% 328185
#% 1063480
#% 1063523
#% 1190134
#% 1290946
#% 1523854
#% 1567921
#% 1770342
#% 1880478
#% 1931643
#! Regular expression matching over sequences in real time is a crucial task in complex event processing on data streams. Given that such data sequences are often noisy and errors have temporal and spatial correlations, performing regular expression matching effectively and efficiently is a challenging task. Instead of the traditional approach of learning a distribution of the stream first and then processing queries, we propose a new approach that efficiently does the matching based on an error model. In particular, our algorithms are based on the realistic Markov chain error model, and report all matching paths to trace relevant basic events that trigger the matching. This is much more informative than a single matching path. We also devise algorithms to efficiently return only top-k matching paths, and to handle negations in an extended regular expression. Finally, we conduct a comprehensive experimental study to evaluate our algorithms using real datasets.

#index 1971528
#* Indexing for subtree similarity-search using edit distance
#@ Sara Cohen
#t 2013
#c 5
#% 66654
#% 289193
#% 303066
#% 574583
#% 576105
#% 654442
#% 725192
#% 754108
#% 826007
#% 931227
#% 938982
#% 994015
#% 1065412
#% 1290919
#% 1312537
#% 1386500
#% 1595888
#% 1668638
#% 1672486
#% 1693200
#! Given a tree Q and a large set of trees T = {T1,...,Tn}, the subtree similarity-search problem is that of finding the subtrees of trees among T that are most similar to Q, using the tree edit distance metric. Determining similarity using tree edit distance has been proven useful in a variety of application areas. While subtree similarity-search has been studied in the past, solutions required traversal of all of T, which poses a severe bottleneck in processing time, as T grows larger. This paper proposes the first index structure for subtree similarity-search, provided that the unit cost function is used. Extensive experimentation and comparison to previous work shows the huge improvement gained when using the proposed index structure and processing algorithm.

#index 1971529
#* Efficient ad-hoc search for personalized PageRank
#@ Yasuhiro Fujiwara;Makoto Nakatsuji;Hiroaki Shiokawa;Takeshi Mishima;Makoto Onizuka
#t 2013
#c 5
#% 577329
#% 766447
#% 769952
#% 823342
#% 844298
#% 878224
#% 879598
#% 915344
#% 939905
#% 1016176
#% 1022233
#% 1026860
#% 1038781
#% 1055877
#% 1131827
#% 1260668
#% 1425316
#% 1531275
#% 1581927
#% 1588228
#% 1594624
#% 1607321
#% 1707460
#% 1770333
#% 1826418
#% 1826429
#% 1872230
#% 1942743
#! Personalized PageRank (PPR) has been successfully applied to various applications. In real applications, it is important to set PPR parameters in an ad-hoc manner when finding similar nodes because of dynamically changing nature of graphs. Through interactive actions, interactive similarity search supports users to enhance the efficacy of applications. Unfortunately, if the graph is large, interactive similarity search is infeasible due to its high computation cost. Previous PPR approaches cannot effectively handle interactive similarity search since they need precomputation or approximate computation of similarities. The goal of this paper is to efficiently find the top-k nodes with exact node ranking so as to effectively support interactive similarity search based on PPR. Our solution is Castanet. The key Castanet operations are (1) estimate upper/lower bounding similarities iteratively, and (2) prune unnecessary nodes dynamically to obtain top-k nodes in each iteration. Experiments show that our approach is much faster than existing approaches.

#index 1971530
#* Building an efficient RDF store over a relational database
#@ Mihaela A. Bornea;Julian Dolby;Anastasios Kementsietsidis;Kavitha Srinivas;Patrick Dantressangle;Octavian Udrea;Bishwaranjan Bhattacharjee
#t 2013
#c 5
#% 956664
#% 1022236
#% 1055731
#% 1098453
#% 1127431
#% 1206875
#% 1269903
#% 1366460
#% 1374374
#% 1581858
#% 1641506
#% 1798403
#% 1846717
#% 1846779
#! Efficient storage and querying of RDF data is of increasing importance, due to the increased popularity and widespread acceptance of RDF on the web and in the enterprise. In this paper, we describe a novel storage and query mechanism for RDF which works on top of existing relational representations. Reliance on relational representations of RDF means that one can take advantage of 35+ years of research on efficient storage and querying, industrial-strength transaction support, locking, security, etc. However, there are significant challenges in storing RDF in relational, which include data sparsity and schema variability. We describe novel mechanisms to shred RDF into relational, and novel query translation techniques to maximize the advantages of this shredded representation. We show that these mechanisms result in consistently good performance across multiple RDF benchmarks, even when compared with current state-of-the-art stores. This work provides the basis for RDF support in DB2 v.10.1.

#index 1971531
#* Minimal MapReduce algorithms
#@ Yufei Tao;Wenqing Lin;Xiaokui Xiao
#t 2013
#c 5
#% 69503
#% 144423
#% 288976
#% 341704
#% 465167
#% 956521
#% 963669
#% 1063553
#% 1127559
#% 1214705
#% 1328061
#% 1328186
#% 1399956
#% 1426543
#% 1426544
#% 1426584
#% 1484141
#% 1523806
#% 1523824
#% 1523839
#% 1523841
#% 1523880
#% 1523924
#% 1560415
#% 1573238
#% 1581407
#% 1581411
#% 1581836
#% 1581925
#% 1581926
#% 1581927
#% 1581996
#% 1587209
#% 1592315
#% 1594623
#% 1594639
#% 1602032
#% 1605950
#% 1605989
#% 1605990
#% 1654044
#% 1707461
#% 1730737
#% 1741027
#% 1769264
#% 1770321
#% 1798411
#% 1846687
#% 1846750
#% 1846759
#% 1846778
#% 1869836
#% 1869837
#% 1880438
#% 1880439
#% 1895055
#! MapReduce has become a dominant parallel computing paradigm for big data, i.e., colossal datasets at the scale of tera-bytes or higher. Ideally, a MapReduce system should achieve a high degree of load balancing among the participating machines, and minimize the space usage, CPU and I/O time, and network transfer at each machine. Although these principles have guided the development of MapReduce algorithms, limited emphasis has been placed on enforcing serious constraints on the aforementioned metrics simultaneously. This paper presents the notion of minimal algorithm, that is, an algorithm that guarantees the best parallelization in multiple aspects at the same time, up to a small constant factor. We show the existence of elegant minimal algorithms for a set of fundamental database problems, and demonstrate their excellent performance with extensive experiments.

#index 1971532
#* Latch-free data structures for DBMS: design, implementation, and evaluation
#@ Takashi Horikawa
#t 2013
#c 5
#% 84047
#% 148296
#% 234819
#% 397444
#% 484514
#% 753278
#% 916748
#% 1063543
#% 1082195
#% 1129955
#% 1217152
#% 1328149
#% 1770319
#! The fact that multi-core CPUs have become so common and that the number of CPU cores in one chip has continued to rise means that a server machine can easily contain an extremely high number of CPU cores. The CPU scalability of IT systems is thus attracting a considerable amount of research attention. Some systems, such as ACID-compliant DBMSs, are said to be difficult to scale, probably due to the mutual exclusion required to ensure data consistency. Possible countermeasures include latch-free (LF) data structures, an elemental technology to improve the CPU scalability by eliminating the need for mutual exclusion. This paper investigates these LF data structures with a particular focus on their applicability and effectiveness. Some existing LF data structures (such as LF hash tables) have been adapted to PostgreSQL, one of the most popular open-source DBMSs. The performance improvement was evaluated with a benchmark program simulating real-world transactions. Measurement results obtained from state-of-the-art 80-core machines demonstrated that the LF data structures were effective for performance improvement in a many-core situation in which DBT-1 throughput increased by about 2.5 times. Although the poor performance of the original DBMS was due to a severe latch-related bottleneck and can be improved by parameter tuning, it is of practical importance that LF data structures provided performance improvement without deep understanding of the target system behavior that is necessary for the parameter tuning.

#index 1971533
#* Information preservation in statistical privacy and bayesian estimation of unattributed histograms
#@ Bing-Rong Lin;Daniel Kifer
#t 2013
#c 5
#% 67453
#% 209373
#% 576110
#% 577233
#% 577239
#% 757953
#% 809245
#% 891559
#% 893100
#% 977011
#% 1022246
#% 1022247
#% 1029084
#% 1061644
#% 1198224
#% 1198227
#% 1206714
#% 1206938
#% 1217148
#% 1287870
#% 1381029
#% 1426454
#% 1426455
#% 1426456
#% 1523886
#% 1581864
#% 1584793
#% 1611348
#% 1701728
#% 1740518
#% 1874769
#% 1959472
#! In statistical privacy, utility refers to two concepts: information preservation -- how much statistical information is retained by a sanitizing algorithm, and usability -- how (and with how much difficulty) does one extract this information to build statistical models, answer queries, etc. Some scenarios incentivize a separation between information preservation and usability, so that the data owner first chooses a sanitizing algorithm to maximize a measure of information preservation and, afterward, the data consumers process the sanitized output according to their needs [22, 46]. We analyze a variety of utility measures and show that the average (over possible outputs of the sanitizer) error of Bayesian decision makers forms the unique class of utility measures that satisfy three axioms related to information preservation. The axioms are agnostic to Bayesian concepts such as subjective probabilities and hence strengthen support for Bayesian views in privacy research. In particular, this result connects information preservation to aspects of usability -- if the information preservation of a sanitizing algorithm should be measured as the average error of a Bayesian decision maker, shouldn't Bayesian decision theory be a good choice when it comes to using the sanitized outputs for various purposes? We put this idea to the test in the unattributed histogram problem where our decision- theoretic post-processing algorithm empirically outperforms previously proposed approaches.

#index 1971534
#* Online search of overlapping communities
#@ Wanyun Cui;Yanghua Xiao;Haixun Wang;Yiqi Lu;Wei Wang
#t 2013
#c 5
#% 382908
#% 760866
#% 809459
#% 824711
#% 905901
#% 989654
#% 1100134
#% 1108861
#% 1137865
#% 1172001
#% 1272097
#% 1426539
#% 1451234
#% 1523970
#% 1689531
#% 1707461
#% 1719412
#! A great deal of research has been conducted on modeling and discovering communities in complex networks. In most real life networks, an object often participates in multiple overlapping communities. In view of this, recent research has focused on mining overlapping communities in complex networks. The algorithms essentially materialize a snapshot of the overlapping communities in the network. This approach has three drawbacks, however. First, the mining algorithm uses the same global criterion to decide whether a subgraph qualifies as a community. In other words, the criterion is fixed and predetermined. But in reality, communities for different vertices may have very different characteristics. Second, it is costly, time consuming, and often unnecessary to find communities for an entire network. Third, the approach does not support dynamically evolving networks. In this paper, we focus on online search of overlapping communities, that is, given a query vertex, we find meaningful overlapping communities the vertex belongs to in an online manner. In doing so, each search can use community criterion tailored for the vertex in the search. To support this approach, we introduce a novel model for overlapping communities, and we provide theoretical guidelines for tuning the model. We present several algorithms for online overlapping community search and we conduct comprehensive experiments to demonstrate the effectiveness of the model and the algorithms. We also suggest many potential applications of our model and algorithms.

#index 1971535
#* A direct mining approach to efficient constrained graph pattern discovery
#@ Feida Zhu;Zequn Zhang;Qiang Qu
#t 2013
#c 5
#% 399793
#% 431105
#% 464989
#% 478274
#% 543962
#% 629603
#% 629708
#% 727845
#% 765429
#% 769940
#% 772830
#% 785402
#% 813990
#% 915301
#% 960305
#% 1022280
#% 1063502
#% 1083649
#% 1117006
#% 1393168
#% 1587722
#% 1918359
#! Despite the wealth of research on frequent graph pattern mining, how to efficiently mine the complete set of those with constraints still poses a huge challenge to the existing algorithms mainly due to the inherent bottleneck in the mining paradigm. In essence, mining requests with explicitly-specified constraints cannot be handled in a way that is direct and precise. In this paper, we propose a direct mining framework to solve the problem and illustrate our ideas in the context of a particular type of constrained frequent patterns --- the "skinny" patterns, which are graph patterns with a long backbone from which short twigs branch out. These patterns, which we formally define as l-long δ-skinny patterns, are able to reveal insightful spatial and temporal trajectory patterns in mobile data mining, information diffusion, adoption propagation, and many others. Based on the key concept of a canonical diameter, we develop SkinnyMine, an efficient algorithm to mine all the l-long δ-skinny patterns guaranteeing both the completeness of our mining result as well as the unique generation of each target pattern. We also present a general direct mining framework together with two properties of reducibility and continuity for qualified constraints. Our experiments on both synthetic and real data demonstrate the effectiveness and scalability of our approach.

#index 1971536
#* Knowledge harvesting in the big-data era
#@ Fabian Suchanek;Gerhard Weikum
#t 2013
#c 5
#% 198055
#% 301241
#% 504443
#% 660001
#% 756964
#% 801668
#% 875064
#% 913783
#% 915340
#% 937552
#% 939376
#% 943834
#% 956501
#% 956564
#% 1000502
#% 1055735
#% 1063570
#% 1083658
#% 1117028
#% 1130858
#% 1131827
#% 1166537
#% 1190065
#% 1190118
#% 1214667
#% 1246170
#% 1269587
#% 1269899
#% 1275182
#% 1291356
#% 1292517
#% 1300591
#% 1314445
#% 1355059
#% 1372745
#% 1399948
#% 1409954
#% 1417383
#% 1426449
#% 1441630
#% 1450836
#% 1455643
#% 1467732
#% 1471192
#% 1471201
#% 1471208
#% 1471327
#% 1481643
#% 1482288
#% 1482395
#% 1484272
#% 1496780
#% 1523838
#% 1523913
#% 1536526
#% 1536527
#% 1536549
#% 1536587
#% 1538763
#% 1573237
#% 1585243
#% 1586117
#% 1592008
#% 1592010
#% 1592066
#% 1592089
#% 1592311
#% 1598410
#% 1641935
#% 1642010
#% 1654046
#% 1654048
#% 1693868
#% 1693886
#% 1708912
#% 1711773
#% 1711796
#% 1711855
#% 1711857
#% 1711865
#% 1730995
#% 1746825
#% 1746844
#% 1746947
#% 1770359
#% 1770375
#% 1846712
#% 1880463
#% 1913255
#% 1913318
#% 1913401
#% 1913617
#% 1913651
#% 1913673
#% 1918389
#% 1918434
#% 1925700
#% 1925702
#% 1942737
#% 1992003
#% 1992101
#% 1992411
#% 1992420
#% 1992425
#! The proliferation of knowledge-sharing communities such as Wikipedia and the progress in scalable information extraction from Web and text sources have enabled the automatic construction of very large knowledge bases. Endeavors of this kind include projects such as DBpedia, Freebase, KnowItAll, ReadTheWeb, and YAGO. These projects provide automatically constructed knowledge bases of facts about named entities, their semantic classes, and their mutual relationships. They contain millions of entities and hundreds of millions of facts about them. Such world knowledge in turn enables cognitive applications and knowledge-centric services like disambiguating natural-language text, semantic search for entities and relations in Web and enterprise data, and entity-oriented analytics over unstructured contents. Prominent examples of how knowledge bases can be harnessed include the Google Knowledge Graph and the IBM Watson question answering system. This tutorial presents state-of-the-art methods, recent advances, research opportunities, and open challenges along this avenue of knowledge harvesting and its applications. Particular emphasis will be on the twofold role of knowledge bases for big-data analytics: using scalable distributed algorithms for harvesting knowledge from Web and text sources, and leveraging entity-centric knowledge for deeper interpretation of and better intelligence with Big Data.

#index 1971537
#* Natural language question answering over RDF data
#@ Ruizhe Huang;Lei Zou
#t 2013
#c 5
#% 1913604
#% 1913673
#! As more and more RDF data becomes available, such as DBpedia, Yago and Freebase, it is desired to provide users with simple interfaces to access the datasets. Although the SPARQL query language is a standard way to query RDF data, it remains tedious and difficult even for expert users because of the formality of the language and the complexity of the underlying schema of RDF data. An ideal system should allow users to express queries in their own languages. In this work, we propose a methodology to translate natural language questions into SPARQL queries, which can be answered by existing RDF engines and fulfill users? information need.

#index 1972710
#* Querying graph databases
#@ Pablo Barceló Baeza
#t 2013
#c 5
#% 32904
#% 64902
#% 99340
#% 145182
#% 175464
#% 183411
#% 186984
#% 191611
#% 197751
#% 235941
#% 237180
#% 237191
#% 248025
#% 268797
#% 291299
#% 292677
#% 321051
#% 384978
#% 390685
#% 415004
#% 442887
#% 442960
#% 462235
#% 481434
#% 547286
#% 562454
#% 571038
#% 571040
#% 576097
#% 576102
#% 577372
#% 587563
#% 598376
#% 599549
#% 600179
#% 731485
#% 769518
#% 956574
#% 993437
#% 1019118
#% 1019798
#% 1098424
#% 1166490
#% 1181329
#% 1206916
#% 1292514
#% 1297471
#% 1307976
#% 1372706
#% 1407276
#% 1497253
#% 1538787
#% 1581833
#% 1581837
#% 1746861
#% 1770126
#% 1770139
#% 1818406
#% 1818411
#% 1888487
#% 1912326
#% 1951753
#% 1959479
#% 1959480
#% 1971484
#! Graph databases have gained renewed interest in the last years, due to its applications in areas such as the Semantic Web and Social Networks Analysis. We study the problem of querying graph databases, and, in particular, the expressiveness and complexity of evaluation for several general-purpose query languages, such as the regular path queries and its extensions with conjunctions and inverses. We distinguish between two semantics for these languages. The first one, based on simple paths, easily leads to intractability, while the second one, based on arbitrary paths, allows tractable evaluation for an expressive family of languages. We also study two recent extensions of these languages that have been motivated by modern applications of graph databases. The first one allows to treat paths as first-class citizens, while the second one permits to express queries that combine the topology of the graph with its underlying data.

#index 1972711
#* Sketching via hashing: from heavy hitters to compressed sensing to sparse fourier transform
#@ Piotr Indyk
#t 2013
#c 5
#% 63781
#% 91386
#% 256883
#% 347200
#% 347226
#% 446438
#% 479795
#% 492912
#% 493188
#% 576119
#% 654461
#% 723939
#% 951770
#% 963258
#% 1039581
#% 1062478
#% 1141523
#% 1211829
#% 1292821
#% 1312122
#% 1385997
#% 1400948
#% 1426284
#% 1426298
#% 1438459
#% 1442716
#% 1484160
#% 1654694
#% 1668260
#% 1668261
#% 1670160
#% 1701390
#% 1757501
#% 1770471
#% 1815965
#% 1816485
#% 1992591
#% 1992592

#index 1972712
#* On the expressive power of update primitives
#@ Tom J. Ameloot;Jan Van den Bussche;Emmanuel Waller
#t 2013
#c 5
#% 319
#% 36179
#% 47955
#% 98462
#% 101646
#% 102547
#% 230556
#% 261370
#% 287793
#% 333979
#% 340698
#% 384978
#% 399235
#% 463580
#% 481128
#% 738953
#% 1092015
#% 1266686
#% 1472966
#% 1523876
#% 1718234
#% 1725653
#% 1764510
#% 1818423
#! The SQL standard offers three primitive operations (insert, delete, and update which is here called modify) to update a relation based on a generic query. This paper compares the expressiveness of programs composed of these three operations, with the general notion of update that simply replaces the content of the relation by the result of a query. It turns out that replacing cannot be expressed in terms of insertions, deletions, and modifications, and neither can modifications be expressed in terms of insertions and deletions. The expressive power gained by if-then-else control flow in programs is investigated as well. Different ways to perform replacing are discussed: using a temporary variable; using the new SQL merge operation; using SQL's data change delta tables; or using queries involving object creation or arithmetic. Finally the paper investigates the power of alternating the different primitives. For example, an insertion followed by a modification cannot always be expressed as a modification followed by an insertion.

#index 1972713
#* Nearest neighbor searching under uncertainty II
#@ Pankaj K. Agarwal;Boris Aronov;Sariel Har-Peled;Jeff M. Phillips;Ke Yi;Wuzhou Zhang
#t 2013
#c 5
#% 56081
#% 97043
#% 190611
#% 203962
#% 211801
#% 256627
#% 302321
#% 335411
#% 772835
#% 1058620
#% 1164838
#% 1179162
#% 1181270
#% 1200291
#% 1206716
#% 1408794
#% 1446816
#% 1594641
#% 1614964
#% 1616585
#% 1770137
#! Nearest-neighbor (NN) search, which returns the nearest neighbor of a query point in a set of points, is an important and widely studied problem in many fields, and it has wide range of applications. In many of them, such as sensor databases, location-based services, face recognition, and mobile data, the location of data is imprecise. We therefore study nearest neighbor queries in a probabilistic framework in which the location of each input point is specified as a probability distribution function. We present efficient algorithms for (i) computing all points that are nearest neighbors of a query point with nonzero probability; (ii) estimating, within a specified additive error, the probability of a point being the nearest neighbor of a query point; (iii) using it to return the point that maximizes the probability being the nearest neighbor, or all the points with probabilities greater than some threshold to be the NN. We also present some experimental results to demonstrate the effectiveness of our approach.

#index 1972714
#* Learning and verifying quantified boolean queries by example
#@ Azza Abouzied;Dana Angluin;Christos Papadimitriou;Joseph M. Hellerstein;Avi Silberschatz
#t 2013
#c 5
#% 697
#% 81510
#% 92778
#% 101913
#% 116168
#% 123266
#% 180945
#% 252025
#% 449515
#% 450951
#% 451056
#% 916791
#% 1014125
#% 1217187
#% 1424594
#% 1818416
#% 1818419
#% 1905566
#! To help a user specify and verify quantified queries --- a class of database queries known to be very challenging for all but the most expert users --- one can question the user on whether certain data objects are answers or non-answers to her intended query. In this paper, we analyze the number of questions needed to learn or verify qhorn queries, a special class of Boolean quantified queries whose underlying form is conjunctions of quantified Horn expressions. We provide optimal polynomial-question and polynomial-time learning and verification algorithms for two subclasses of the class qhorn with upper constant limits on a query's causal density.

#index 1972715
#* Verification of relational data-centric dynamic systems with external services
#@ Babak Bagheri Hariri;Diego Calvanese;Giuseppe De Giacomo;Alin Deutsch;Marco Montali
#t 2013
#c 5
#% 297770
#% 341697
#% 384978
#% 570649
#% 581901
#% 769518
#% 770373
#% 810053
#% 824702
#% 826032
#% 888014
#% 942360
#% 982402
#% 982526
#% 1060757
#% 1062932
#% 1099727
#% 1100590
#% 1153042
#% 1173493
#% 1180017
#% 1260125
#% 1326580
#% 1335442
#% 1373496
#% 1396853
#% 1415590
#% 1426453
#% 1538776
#% 1622330
#% 1622331
#! Data-centric dynamic systems are systems where both the process controlling the dynamics and the manipulation of data are equally central. We study verification of (first-order) mu-calculus variants over relational data-centric dynamic systems, where data are maintained in a relational database, and the process is described in terms of atomic actions that evolve the database. Action execution may involve calls to external services, thus inserting fresh data into the system. As a result such systems are infinite-state. We show that verification is undecidable in general, and we isolate notable cases where decidability is achieved. Specifically we start by considering service calls that return values deterministically (depending only on passed parameters). We show that in a mu-calculus variant that preserves knowledge of objects appeared along a run we get decidability under the assumption that the fresh data introduced along a run are bounded, though they might not be bounded in the overall system. In fact we tie such a result to a notion related to weak acyclicity studied in data exchange. Then, we move to nondeterministic services and we investigate decidability under the assumption that knowledge of objects is preserved only if they are continuously present. We show that if infinitely many values occur in a run but do not accumulate in the same state, then we get again decidability. We give syntactic conditions to avoid this accumulation through the novel notion of "generate-recall acyclicity", which ensures that every service call activation generates new values that cannot be accumulated indefinitely.

#index 1972716
#* The complexity of mining maximal frequent subgraphs
#@ Benny Kimelfeld;Phokion G. Kolaitis
#t 2013
#c 5
#% 39702
#% 115964
#% 129187
#% 212030
#% 431105
#% 466644
#% 478274
#% 579314
#% 604724
#% 629708
#% 641694
#% 729933
#% 729938
#% 769910
#% 769940
#% 772830
#% 796214
#% 813990
#% 874894
#% 902448
#% 953601
#% 977014
#% 993437
#% 1041761
#% 1053860
#% 1086059
#% 1281961
#% 1454139
#% 1603850
#% 1972413
#! A frequent subgraph of a given collection of graphs is a graph that is isomorphic to a subgraph of at least as many graphs in the collection as a given threshold. Frequent subgraphs generalize frequent itemsets and arise in various contexts, from bioinformatics to the Web. Since the space of frequent subgraphs is typically extremely large, research in graph mining has focused on special types of frequent subgraphs that can be orders of magnitude smaller in number, yet encapsulate the space of all frequent subgraphs. Maximal frequent subgraphs (i.e., the ones not properly contained in any frequent subgraph) constitute the most useful such type. In this paper, we embark on a comprehensive investigation of the computational complexity of mining maximal frequent subgraphs. Our study is carried out by considering the effect of three different parameters: possible restrictions on the class of graphs; a fixed bound on the threshold; and a fixed bound on the number of desired answers. We focus on specific classes of connected graphs: general graphs, planar graphs, graphs of bounded degree, and graphs of bounded tree-width (trees being a special case). Moreover, each class has two variants: the one in which the nodes are unlabeled, and the one in which they are uniquely labeled. We delineate the complexity of the enumeration problem for each of these variants by determining when it is solvable in (total or incremental) polynomial time and when it is NP-hard. Specifically, for the labeled classes, we show that bounding the threshold yields tractability but, in most cases, bounding the number of answers does not, unless P=NP; an exception is the case of labeled trees, where bounding either of these two parameters yields tractability. The state of affairs turns out to be quite different for the unlabeled classes. The main (and most challenging to prove) result concerns unlabeled trees: we show NP-hardness, even if the input consists of two trees, and both the threshold and the number of desired answers are equal to just two. In other words, we establish that the following problem is NP-complete: given two unlabeled trees, do they have more than one maximal subtree in common?

#index 1972717
#* Ontology-based data access: a study through disjunctive datalog, CSP, and MMSNP
#@ Meghyn Bienvenu;Balder ten Cate;Carsten Lutz;Frank Wolter
#t 2013
#c 5
#% 235018
#% 248026
#% 268708
#% 338753
#% 343623
#% 751703
#% 821569
#% 992962
#% 992964
#% 1024387
#% 1051197
#% 1051201
#% 1065944
#% 1068400
#% 1217122
#% 1379602
#% 1405521
#% 1416180
#% 1603860
#% 1625108
#% 1826177
#% 1872683
#% 1880450
#% 1888869
#% 1919039
#! Ontology-based data access is concerned with querying incomplete data sources in the presence of domain-specific knowledge provided by an ontology. A central notion in this setting is that of an ontology-mediated query, which is a database query coupled with an ontology. In this paper, we study several classes of ontology-mediated queries, where the database queries are given as some form of conjunctive query and the ontologies are formulated in description logics or other relevant fragments of first-order logic, such as the guarded fragment and the unary-negation fragment. The contributions of the paper are three-fold. First, we characterize the expressive power of ontology-mediated queries in terms of fragments of disjunctive datalog. Second, we establish intimate connections between ontology-mediated queries and constraint satisfaction problems (CSPs) and their logical generalization, MMSNP formulas. Third, we exploit these connections to obtain new results regarding (i) first-order rewritability and datalog-rewritability of ontology-mediated queries, (ii) P/NP dichotomies for ontology-mediated queries, and (iii) the query containment problem for ontology-mediated queries.

#index 1972718
#* Communication steps for parallel query processing
#@ Paul Beame;Paraschos Koutris;Dan Suciu
#t 2013
#c 5
#% 27762
#% 50176
#% 130279
#% 238182
#% 278835
#% 847068
#% 963669
#% 1029102
#% 1063553
#% 1068625
#% 1141493
#% 1232282
#% 1328095
#% 1372690
#% 1442067
#% 1484141
#% 1523824
#% 1560415
#% 1581836
#% 1770115
#% 1770120
#% 1864656
#! We consider the problem of computing a relational query q on a large input database of size n, using a large number p of servers. The computation is performed in rounds, and each server can receive only O(n/p1-ε) bits of data, where ε ∈[0,1] is a parameter that controls replication. We examine how many global communication steps are needed to compute q. We establish both lower and upper bounds, in two settings. For a single round of communication, we give lower bounds in the strongest possible model, where arbitrary bits may be exchanged; we show that any algorithm requires ε ≥ 1--1/τ*, where τ* is the fractional vertex cover of the hypergraph of q. We also give an algorithm that matches the lower bound for a specific class of databases. For multiple rounds of communication, we present lower bounds in a model where routing decisions for a tuple are tuple-based. We show that for the class of tree-like queries there exists a tradeoff between the number of rounds and the space exponent ε. The lower bounds for multiple rounds are the first of their kind. Our results also imply that transitive closure cannot be computed in O(1) rounds of communication.

#index 1972719
#* I/O-efficient planar range skyline and attrition priority queues
#@ Casper Kejlberg-Rasmussen;Yufei Tao;Konstantinos Tsakalidis;Kostas Tsichlas;Jeonghun Yoon
#t 2013
#c 5
#% 41684
#% 62393
#% 75633
#% 158790
#% 273714
#% 290829
#% 317729
#% 344424
#% 427199
#% 465167
#% 479473
#% 481934
#% 560668
#% 571296
#% 751769
#% 806212
#% 849816
#% 902462
#% 993954
#% 1022224
#% 1022266
#% 1074714
#% 1092017
#% 1223422
#% 1328116
#% 1484079
#% 1581826
#% 1611295
#% 1668216
#% 1746482
#! We study the static and dynamic planar range skyline reporting problem in the external memory model with block size B, under a linear space budget. The problem asks for an O(n/B) space data structure that stores n points in the plane, and supports reporting the k maximal input points (a.k.a.skyline) among the points that lie within a given query rectangle Q = [α1[α2] × [β1β2. When Q is 3-sided, i.e. one of its edges is grounded, two variants arise: top-open for β2 = ∞ and left-open for α1 = - ∞ (symmetrically bottom-open and right-open) queries. We present optimal static data structures for top-open queries, for the cases where the universe is R2, a U × U grid, and rank space [O(n)]2. We also show that left-open queries are harder, as they require Ω((n/B)ε + k/B) I/Os for ε 0, when only linear space is allowed. We show that the lower bound is tight, by a structure that supports 4-sided queries in matching complexities. Interestingly, these lower and upper bounds coincide with those of the planar orthogonal range reporting problem, i.e., the skyline requirement does not alter the problem difficulty at all! Finally, we present the first dynamic linear space data structure that supports top-open queries in O(log2Bε n + k/B1 ε and updates in O(log2Bε n) worst case I/Os, for ε ∈ [0, 1]. This also yields a linear space data structure for 4-sided queries with optimal query I/Os and O(log(n/B)) amortized update I/Os. We consider of independent interest the main component of our dynamic structures, a new real-time I/O-efficient and catenable variant of the fundamental structure priority queue with attrition by Sundar.

#index 1972720
#* Trial for RDF: adapting graph query languages for RDF data
#@ Leonid Libkin;Juan Reutter;Domagoj Vrgoč
#t 2013
#c 5
#% 32904
#% 63354
#% 175464
#% 191611
#% 268797
#% 343623
#% 384978
#% 390685
#% 577372
#% 733595
#% 778122
#% 822573
#% 874910
#% 1019798
#% 1223424
#% 1497253
#% 1523818
#% 1538787
#% 1581837
#% 1594585
#% 1764508
#% 1770126
#% 1791183
#% 1818411
#% 1888487
#% 1901455
#% 1912326
#% 1959480
#% 1959485
#! Querying RDF data is viewed as one of the main applications of graph query languages, and yet the standard model of graph databases -- essentially labeled graphs -- is different from the triples-based model of RDF. While encodings of RDF databases into graph data exist, we show that even the most natural ones are bound to lose some functionality when used in conjunction with graph query languages. The solution is to work directly with triples, but then many properties taken for granted in the graph database context (e.g., reachability) lose their natural meaning. Our goal is to introduce languages that work directly over triples and are closed, i.e., they produce sets of triples, rather than graphs. Our basic language is called TriAL, or Triple Algebra: it guarantees closure properties by replacing the product with a family of join operations. We extend TriAL with recursion, and explain why such an extension is more intricate for triples than for graphs. We present a declarative language, namely a fragment of datalog, capturing the recursive algebra. For both languages, the combined complexity of query evaluation is given by low-degree polynomials. We compare our languages with relational languages, such as finite-variable logics, and previously studied graph query languages such as adaptations of XPath, regular path queries, and nested regular expressions; many of these languages are subsumed by the recursive triple algebra. We also provide examples of the usefulness of TriAL in querying graph and RDF data.

#index 1972721
#* Flag & check: data access with monadically defined queries
#@ Sebastian Rudolph;Markus Krötzsch
#t 2013
#c 5
#% 28120
#% 94461
#% 140410
#% 248025
#% 342829
#% 384978
#% 465053
#% 490909
#% 591778
#% 598376
#% 598678
#% 599549
#% 731485
#% 733595
#% 778122
#% 826032
#% 992962
#% 1063724
#% 1217113
#% 1217115
#% 1328190
#% 1409925
#% 1500877
#% 1523803
#% 1523844
#% 1585244
#% 1619617
#% 1737460
#% 1826064
#% 1826177
#% 1826229
#% 1888838
#% 1932717
#% 1933359
#! We introduce monadically defined queries (MODEQs) and nested monadically defined queries (NEMODEQs), two querying formalisms that extend conjunctive queries, conjunctive two-way regular path queries, and monadic Datalog queries. Both can be expressed as Datalog queries and in monadic second-order logic, yet they have a decidable query containment problem and favorable query answering complexities: a data complexity of P, and a combined complexity of NP (MODEQs) and PSpace (NEMODEQs). We show that (NE)MODEQ answering remains decidable in the presence of a well-known generic class of tuple-generating dependencies. In addition, techniques to rewrite queries under dependencies into (NE)MODEQs are introduced. Rewriting can be applied partially, and (NE)MODEQ answering is still decidable if the non-rewritable part of the TGDs permits decidable (NE)MODEQ answering on other grounds.

#index 1972722
#* Verification of database-driven systems via amalgamation
#@ Mikołaj Bojańczyk;Luc Segoufin;Szymon Toruńczyk
#t 2013
#c 5
#% 236025
#% 445446
#% 875055
#% 888015
#% 942360
#% 1179996
#% 1180017
#% 1638637
#% 1675640
#! We describe a general framework for static verification of systems that base their decisions upon queries to databases. The database is specified using constraints, typically a schema, and is not modified during a run of the system. The system is equipped with a finite number of registers for storing intermediate information from the database and the specification consists of a transition table described using quantifier-free formulas that can query either the database or the registers. Our main result concerns systems querying XML databases -- modeled as data trees -- using quantifier-free formulas with predicates such as the descendant axis or comparison of data values. In this scenario we show an ExpSpace algorithm for deciding reachability. Our technique is based on the notion of amalgamation and is quite general. For instance it also applies to relational databases (with an optimal PSpace algorithm). We also show that minor extensions of the model lead to undecidability.

#index 1972723
#* Well-founded semantics for extended datalog and ontological reasoning
#@ André Hernich;Clemens Kupke;Thomas Lukasiewicz;Georg Gottlob
#t 2013
#c 5
#% 103705
#% 190336
#% 587508
#% 992962
#% 1416180
#% 1511884
#% 1808579
#! The Datalog± family of expressive extensions of Datalog has recently been introduced as a new paradigm for query answering over ontologies, which captures and extends several common description logics. It extends plain Datalog by features such as existentially quantified rule heads and, at the same time, restricts the rule syntax so as to achieve decidability and tractability. In this paper, we continue the research on Datalog±. More precisely, we generalize the well-founded semantics (WFS), as the standard semantics for nonmonotonic normal programs in the database context, to Datalog± programs with negation under the unique name assumption (UNA). We prove that for guarded Datalog± with negation under the standard WFS, answering normal Boolean conjunctive queries is decidable, and we provide precise complexity results for this problem, namely, in particular, completeness for PTIME (resp., 2-EXPTIME) in the data (resp., combined) complexity.

#index 1972724
#* SQUIN: a traversal based query execution system for the web of linked data
#@ Olaf Hartig
#t 2013
#c 5
#% 1152440
#% 1333447
#% 1586117
#% 1597474
#% 1846460
#% 1876854
#! The World Wide Web (WWW) currently evolves into a Web of Linked Data where content providers publish and link their data as they have done with hypertext for the last 20 years. We understand this emerging dataspace as a huge, distributed database which is -at best- partially known to query execution systems. To tap the full potential of the Web, such a system must be able to answer a query using data from initially unknown data sources. For this purpose, traditional query execution paradigms are unsuitable because those assume a fixed set of potentially relevant data sources beforehand. We demonstrate the query execution system SQUIN which implements a novel query execution approach. The main idea is to integrate the traversal of data links into the result construction process. This approach allows the execution engine to discover potentially relevant data during the query execution. In our demonstration, attendees can query the Web of Linked Data using SQUIN and, thus, learn about the new query execution approach. Furthermore, attendees can experience the suitability of the approach for Web applications by using a simple, Linked Data based mash-up implemented on top of SQUIN.

#index 1972725
#* DBalancer: distributed load balancing for NoSQL data-stores
#@ Ioannis Konstantinou;Dimitrios Tsoumakos;Ioannis Mytilinis;Nectarios Koziris
#t 2013
#c 5
#% 232771
#% 340175
#% 348152
#% 925571
#% 947144
#% 998845
#% 1400975
#% 1426481
#% 1426489
#% 1563022
#% 1595816
#% 1765838
#! Unanticipated load spikes or skewed data access patterns may lead to severe performance degradation in data serving applications, a typical problem of distributed NoSQL data-stores. In these cases, load balancing is a necessary operation. In this demonstration, we present the DBalancer, a generic distributed module that can be installed on top of a typical NoSQL data-store and provide an efficient and highly configurable load balancing mechanism. Balancing is performed by simple message exchanges and typical data movement operations supported by most modern NoSQL data-stores. We present the system's architecture, we describe in detail its modules and their interaction and we implement a suite of different algorithms on top of it. Through a web-based interactive GUI we allow the users to launch NoSQL clusters of various sizes, to apply numerous skewed and dynamic workloads and to compare the implemented load balancing algorithms. Videos and graphs showcasing each algorithm's effect on a number of indicative performance and cost metrics will be created on the fly for every setup. By browsing the results of different executions users will be able to grasp each algorithm's balancing mechanisms and performance impact in a number of representative setups.

#index 1972726
#* COCCUS: self-configured cost-based query services in the cloud
#@ Ioannis Konstantinou;Verena Kantere;Dimitrios Tsoumakos;Nectarios Koziris
#t 2013
#c 5
#% 1426489
#% 1581873
#% 1596141
#% 1730735
#% 1770132
#% 1855851
#! Recently, a large number of pay-as-you-go data services are offered over cloud infrastructures. Data service providers need appropriate and flexible query charging mechanisms and query optimization that take into consideration cloud operational expenses, pricing strategies and user preferences. Yet, existing solutions are static and non-configurable. We demonstrate COCCUS, a modular system for cost-aware query execution, adaptive query charge and optimization of cloud data services. The audience can set their queries along with their execution preferences and budget constraints, while COCCUS adaptively determines query charge and manages secondary data structures according to various economic policies. We demonstrate COCCUS's operation over centralized and shared nothing CloudDBMS architectures on top of public and private IaaS clouds. The audience is enabled to set economic policies and execute various workloads through a comprehensive GUI. COCCUS's adaptability is showcased using real-time graphs depicting a number of key performance metrics.

#index 1972727
#* Data mining algorithms as a service in the cloud exploiting relational database systems
#@ Carlos Ordonez;Javier García-García;Carlos Garcia-Alvarado;Wellington Cabrera;Veerabhadran Baladandayuthapani;Mohammed S. Quraishi
#t 2013
#c 5
#% 1278123
#% 1357693
#% 1376243
#% 1512993
#% 1535426
#! We present a novel cloud system based on DBMS technology, where data mining algorithms are offered as a service. A local DBMS connects to the cloud and the cloud system returns computed data mining models as small relational tables that are archived and which can be easily transferred, queried and integrated with the client database. Unlike other analytic systems, our solution is not based on MapReduce. Our system avoids exporting large tables outside the local DBMS and thus it avoids transmitting large volumes of data to the cloud. The system offers three processing modes: local, cloud and hybrid, where a linear cost model is used to choose processing mode. In hybrid mode processing is split between the local DBMS and the cloud DBMS. Our system has a job scheduler with FIFO, SJF and RR policies to enhance response time and get partial results early. The cloud DBMS performs dynamic job scheduling, model computation and model archive management. Our system incorporates several optimizations: local data set summarization with sufficient statistics, sampling, caching matrices in RAM and selectively transmitting small matrices, back and forth. We show that in general the most efficient computing mechanism is hybrid processing: summarizing or sampling the data set in the local DBMS, transferring small matrices back and forth, leaving mathematically complex methods as a task for the cloud DBMS.

#index 1972728
#* PARAS: interactive parameter space exploration for association rule mining
#@ Abhishek Mukherji;Xika Lin;Christopher R. Botaish;Jason Whitehouse;Elke A. Rundensteiner;Matthew O. Ward;Carolina Ruiz
#t 2013
#c 5
#% 300120
#% 443427
#% 481290
#% 1301004
#% 1846823
#% 1959797
#! We demonstrate our PARAS technology for supporting interactive association mining at near real-time speeds. Key technical innovations of PARAS, in particular, stable region abstractions and rule redundancy management supporting novel parameter space-centric exploratory queries will be showcased. The audience will be able to interactively explore the parameter space view of rules. They will experience near real-time speeds achieved by PARAS for operations, such as comparing rule sets mined using different parameter values, that would otherwise take hours of computation and much manual investigation. Overall, we will demonstrate that the PARAS system provides a rich experience to data analysts through parameter tuning recommendations while significantly reducing the trial-and-error interactions.

#index 1972729
#* xPAD: a platform for analytic data flows
#@ Alkis Simitsis;Kevin Wilkinson;Petar Jovanovic
#t 2013
#c 5
#% 273908
#% 874987
#% 1217226
#% 1770419
#! As enterprises become more automated, real-time, and data-driven, they need to integrate new data sources and specialized processing engines. The traditional business intelligence architecture of Extract-Transform-Load (ETL) flows, followed by querying, reporting, and analytic operations, is being generalized to analytic data flows that utilize a variety of data types and operations. These complicated flows are difficult to design, implement and maintain since they span a variety of systems. Additionally, new design requirements may be imposed such as design for fault-tolerance, freshness, maintainability, sampling, etc. To reduce development time and maintenance costs, automation is needed. We present xPAD, our platform to manage analytic data flows. xPAD enables flow design. We show how these designs can be optimized, not just for performance, but for other objectives as well. xPAD is engine-agnostic. We show how it can generate executable code for a number of execution engines. It can also import existing flows from other engines and optimize those flows. In that way, it can transform a flow written for one engine into an optimized flow for a different engine. In our demonstration, we will also use various example flows to show optimization for different objectives and comparison of flow execution on different engines.

#index 1972730
#* Execution and optimization of continuous queries with cyclops
#@ Harold Lim;Shivnath Babu
#t 2013
#c 5
#% 300179
#% 428155
#% 1217261
#% 1468411
#% 1535212
#% 1581937
#% 1846826
#% 1895062
#! As the data collected by enterprises grows in scale, there is a growing trend of performing data analytics on large datasets. Batch processing systems that can handle petabyte scale of data, such as Hadoop, have flourished and gained traction in the industry. As the results of batch analytics have been used to continuously improve front-facing user experience, there is a growing interest in pushing the processing latency down. This trend has fueled a resurgence in the development and usage of execution engines that can process continuous queries. An important class of continuous queries is windowed aggregation queries. Such queries arise in a wide range of applications such as generating personalized content and results. Today, considerable manual effort goes into finding the most suitable execution engine for these queries and on tuning query performance on these engines. An ecosystem composed of multiple execution engines may be needed in order to run the overall query workload efficiently given the diverse set of requirements that arise in practice. Cyclops is a continuous query processing platform that manages and orchestrates windowed aggregation queries in an ecosystem composed of multiple continuous query execution engines. Cyclops employs a cost-based approach for picking the most suitable engine and plan for executing a given query. This demonstration first presents an interactive visualization of the rich execution plan space of windowed aggregation queries, which allows users to analyze and understand the differences among plans. The next part of the demonstration will drill down into the design of Cyclops. For a given query, we show the cost spectrum of query execution plans across three different execution engines---Esper, Storm, and Hadoop---as estimated by Cyclops.

#index 1972731
#* The farm: where pig scripts are bred and raised
#@ Craig P. Sayers;Alkis Simitsis;Georgia Koutrika;Alejandro Guerrero Gonzalez;David Tamez Cantu;Meichun Hsu
#t 2013
#c 5
#% 726702
#% 1019222
#% 1770419
#! Even though scripting languages like Pig allow for simpler coding, performing analytics over Big Data using Map-Reduce engines remains challenging. To further assist developers, and support novice users, we offer "The Farm", a catalog of scriptable services supporting creation, discovery, composition, and optimized execution. Each Pig script added to The Farm becomes an executable service, with inputs and outputs defined by relation schemas. Those services are discoverable using natural language search, and composable using a drag-and-drop interface. To support efficient execution, composed services are automatically merged to a single executable script, which can then be run by a growing selection of platform-specific optimizers and interpreters.

#index 1972732
#* Rule-based application development using Webdamlog
#@ Serge Abiteboul;Émilien Antoine;Gerome Miklau;Julia Stoyanovich;Jules Testard
#t 2013
#c 5
#% 1072645
#% 1472960
#% 1581842
#% 1594661
#% 1846805
#% 1895078
#! We present the WebdamLog system for managing distributed data on the Web in a peer-to-peer manner. We demonstrate the main features of the system through an application called Wepic for sharing pictures between attendees of the sigmod conference. Using Wepic, the attendees will be able to share, download, rate and annotate pictures in a highly decentralized manner. We show how WebdamLog handles heterogeneity of the devices and services used to share data in such a Web setting. We exhibit the simple rules that define the Wepic application and show how to easily modify the Wepic application.

#index 1972733
#* FAST: differentially private real-time aggregate monitor with filtering and adaptive sampling
#@ Liyue Fan;Li Xiong;Vaidy Sunderam
#t 2013
#c 5
#% 913731
#% 1061644
#% 1217148
#% 1426563
#% 1614887
#% 1740518
#% 1760888
#% 1919894
#! Sharing aggregate statistics of private data can be of great value when data mining can be performed in real-time to understand important phenomena such as influenza outbreaks or traffic congestion. However, to this date there have been no tools for releasing real-time aggregated data with differential privacy, a strong and provable privacy guarantee. We propose FAST, a real-time system that allows differentially private aggregate sharing and time-series analytics. FAST employs a set of novel, adaptive strategies to improve the utility of shared/released data while guaranteeing the user-specified level of differential privacy. We will demonstrate the challenges and our solutions in the context of prepared data sets as well as live participation data dynamically collected among the SIGMOD'13 attendees.

#index 1972734
#* GRDB: a system for declarative and interactive analysis of noisy information networks
#@ Walaa Eldin Moustafa;Hui Miao;Amol Deshpande;Lise Getoor
#t 2013
#c 5
#% 729913
#% 730089
#% 769942
#% 853536
#% 937552
#% 993980
#% 1206834
#% 1601221
#% 1605922
#! There is a growing interest in methods for analyzing data describing networks of all types, including biological, physical, social, and scientific collaboration networks. Typically the data describing these networks is observational, and thus noisy and incomplete; it is often at the wrong level of fidelity and abstraction for meaningful data analysis. This demonstration presents GrDB, a system that enables data analysts to write declarative programs to specify and combine different network data cleaning tasks, visualize the output, and engage in the process of decision review and correction if necessary. The declarative interface of GrDB makes it very easy to quickly write analysis tasks and execute them over data, while the visual component facilitates debugging the program and performing fine grained corrections.

#index 1972735
#* CARTILAGE: adding flexibility to the Hadoop skeleton
#@ Alekh Jindal;Jorge Quiané-Ruiz;Samuel Madden
#t 2013
#c 5
#% 1217159
#% 1328186
#% 1523841
#% 1523924
#% 1581407
#% 1581926
#% 1586685
#% 1594639
#% 1621131
#% 1621145
#% 1880472
#! Modern enterprises have to deal with a variety of analytical queries over very large datasets. In this respect, Hadoop has gained much popularity since it scales to thousand of nodes and terabytes of data. However, Hadoop suffers from poor performance, especially in I/O performance. Several works have proposed alternate data storage for Hadoop in order to improve the query performance. However, many of these works end up making deep changes in Hadoop or HDFS. As a result, they are (i) difficult to adopt by several users, and (ii) not compatible with future Hadoop releases. In this paper, we present CARTILAGE, a comprehensive data storage framework built on top of HDFS. CARTILAGE allows users full control over their data storage, including data partitioning, data replication, data layouts, and data placement. Furthermore, CARTILAGE can be layered on top of an existing HDFS installation. This means that Hadoop, as well as other query engines, can readily make use of CARTILAGE. We describe several use-cases of CARTILAGE and propose to demonstrate the flexibility and efficiency of CARTILAGE through a set of novel scenarios.

#index 1972736
#* LinkIT: privacy preserving record linkage and integration via transformations
#@ Luca Bonomi;Li Xiong;James J. Lu
#t 2013
#c 5
#% 819551
#% 913783
#% 960288
#% 1063496
#% 1068712
#% 1372692
#% 1670071
#% 1740518
#% 1919782
#% 1962328
#! We propose to demonstrate an open-source tool, LinkIT, for privacy preserving record Linkage and Integration via data Transformations. LinkIT implements novel algorithms that support data transformations for linking sensitive attributes, and is designed to work with our previously developed tool, FRIL (Fine-grained Record Integration and Linkage), to provide a complete record linkage solution. LinkIT can be also used as a stand-alone secure transformation tool to link string records. The system uses a novel embedding technique based on frequent variable length grams mined from original records with differential privacy, and utilizes a personalized threshold for performing linkage in the embedded space. Compared to the state-of-the-art secure transformation method [16], LinkIT guarantees stronger privacy with better scalability while achieving comparable utility results.

#index 1972737
#* PBS at work: advancing data management with consistency metrics
#@ Peter Bailis;Shivaram Venkataraman;Michael J. Franklin;Joseph M. Hellerstein;Ion Stoica
#t 2013
#c 5
#% 3083
#% 398237
#% 1111848
#% 1470582
#% 1584976
#% 1769270
#% 1889760
#% 1911294
#! A large body of recent work has proposed analytical and empirical techniques for quantifying the data consistency properties of distributed data stores. In this demonstration, we begin to explore the wide range of new database functionality they enable, including dynamic query tuning, consistency SLAs, monitoring, and administration. Our demonstration will exhibit how both application programmers and database administrators can leverage these features. We describe three major application scenarios and present a system architecture for supporting them. We also describe our experience in integrating Probabilistically Bounded Staleness (PBS) predictions into Cassandra, a popular NoSQL store and sketch a demo platform that will allow SIGMOD attendees to experience the importance and applicability of real-time consistency metrics.

#index 1972738
#* HiNGE: enabling temporal network analytics at scale
#@ Udayan Khurana;Amol Deshpande
#t 2013
#c 5
#% 1038323
#% 1480911
#! However, much of the prior work on those topics has been restricted to static networks, a primary reason being the lack of efficient temporal data management systems to store and query large dynamic network datasets. In this demonstration proposal, we present HiNGE (Historical Network/Graph Explorer), a system that enables interactive exploration and analytics over large evolving networks through visualization and node-centric metric computations. HiNGE is built on top of a distributed graph database system that stores the entire history of a network, and enables efficiently retrieving and analyzing multiple graph snapshots from arbitrary time points in the past. The cornerstone of our system is a novel hierarchical parallelizable index structure, called DeltaGraph, that enables compact recording of the historical trace of a network on disk, and supports efficient retrieval of historical snapshots for single-site or parallel processing. The other key component of our system is an in-memory graph data structure, called GraphPool, that can maintain hundreds of historical graph snapshots in main memory in a non-redundant manner. We demonstrate the efficient and usability of our system at performing temporal analytics over large-scale dynamic networks.

#index 1972739
#* The power of data use management in action
#@ Prasang Upadhyaya;Nick Anderson;Magdalena Balazinska;Bill Howe;Raghav Kaushik;Ravi Ramamurthy;Dan Suciu
#t 2013
#c 5
#% 993943
#% 1217148
#% 1467794
#% 1478165
#% 1488676
#% 1563411
#% 1581904
#% 1628170
#! In this demonstration, we show-case a database management system extended with a new type of component that we call a Data Use Manager (DUM). The DUM enables DBAs to attach policies to data loaded into the DBMS. It then monitors how users query the data, flags potential policy violations, recommends possible fixes, and supports offline analysis of user activities related to data policies. The demonstration uses real healthcare data.

#index 1972740
#* Speeding up database applications with Pyxis
#@ Alvin Cheung;Owen Arden;Samuel Madden;Andrew C. Myers
#t 2013
#c 5
#% 19622
#% 1528269
#% 1880462
#! We propose to demonstrate Pyxis, a system that optimizes database applications by pushing computation to the database server. Our system applies program analysis techniques to the application source code to determine pieces of application logic that should be moved to the database server to improve performance. This frees the developer from the need to understand the intricacies of database operations or learn a new programming language for stored procedures. In addition, by dynamically monitoring resource utilization on the database server, Pyxis can migrate computation between application and database in response to workload changes. Our previous experiments have shown that Pyxis can decrease latency up to 3x for transactional applications, and improve throughput up to 1.7x when compared to a standard implementation using embedded SQL statements in application logic. We will demonstrate these capabilities via a visualization of real-time performance as well as an interactive code partitioning tool we have developed.

#index 1972741
#* WOW: what the world of (data) warehousing can learn from the World of Warcraft
#@ Rene Mueller;Tim Kaldewey;Guy M. Lohman;John McPherson
#t 2013
#c 5
#% 843768
#% 850735
#% 1063508
#% 1219779
#% 1789653
#! Although originally designed to accelerate pixel monsters, graphics Processing Units (GPUs) have been used for some time as accelerators for selected data base operations. However, to the best of our knowledge, no one has yet reported building a complete system that allows executing complex analytics queries, much less an entire data warehouse benchmark at realistic scale. In this demo, we showcase such a complete system prototype running on a high-end GPU paired with an IBM storage system that achieves 90% hardware efficiency. Our solution delivers sustainable high throughput for business analytics queries in a realistic scenario, i.e., the Star Schema Benchmark at scale factor 1,000. Attendees can interact with our system through a graphical user interface on a tablet PC. They will be able to experience first hand how queries that require processing more than six billion rows, or 100 GB of data, are answered in less than 20 seconds. The user interface allows submitting queries, live performance monitoring of the current query all the way down to the operator level, and viewing the result once the query completes.

#index 1972742
#* CTrace: semantic comparison of multi-granularity process traces
#@ Qing Liu;Kerry Taylor;Xiang Zhao;Geoffrey Squire;Xuemin Lin;Corne Kloppers;Richard Miller
#t 2013
#c 5
#% 1564179
#% 1846801
#! A process trace describes the processes taken in a workflow to generate a particular result. Given many process traces, each with a large amount of very low level information, it is a challenge to make process traces meaningful to different users. It is more challenging to compare two complex process traces generated by heterogenous systems and have different levels of granularity. We present CTrace, a system that (1) lets users explore the conceptual abstraction of large process traces with different levels of granularity, and (2) provides semantic comparison among traces in which both the structural and the semantic similarity are considered. The above functions are underpinned by a novel notion of multi-granularity process trace and efficient multi-granularity similarity comparison algorithms.

#index 1972743
#* Packing experiments for sharing and publication
#@ Fernando Chirigati;Dennis Shasha;Juliana Freire
#t 2013
#c 5
#% 1063593
#% 1164020
#% 1164021
#% 1875071
#% 1889740
#% 1889742
#% 1889745
#! Reproducibility is a core component of the scientific process. Revisiting and reusing past results allow science to move forward - "standing on the shoulders of giants", as Newton once said. An impediment to the adoption of computational reproducibility is that authors find it difficult to generate a compendium that encompasses all the required components to correctly reproduce their experiments. Even when a compendium is available, reviewers and readers may have difficulties in verifying the results on platforms different from the ones where the experiments were originally run. As a step towards simplifying the process of creating reproducible experiments, we have developed ReproZip, a tool that automatically captures the provenance of experiments and packs all the necessary files, library dependencies and variables to reproduce the results. Reviewers can then unpack and run the experiments without having to install any additional software. We will demonstrate real use cases for ReproZip, how packages are created, and how reviewers can validate and explore experiments.

#index 1972744
#* CHIC: a combination-based recommendation system
#@ Manasi Vartak;Samuel Madden
#t 2013
#c 5
#% 152934
#% 301590
#% 479816
#% 810018
#% 936962
#% 1015317
#% 1070886
#% 1489210
#% 1541748
#% 1826422
#! Current recommender systems are focused largely on recommending items based on similarity. For instance, Netflix can recommend movies similar to previously viewed movies, and Amazon can recommend items based on ratings of similar users. Although similarity-based recommendation works well for books and movies, it provides an incomplete solution for items such as clothing or furniture which are inherently used in combination with other items of the same type, e.g., shirt with pants, and desk with a chair. As a result, the decision to buy a clothing or furniture item depends not only on the item itself, but also on how well it works with other items of that type. Recommending such items therefore requires a combination-based recommendation system that given an item, can suggest interesting and diverse combinations containing that item. This problem is challenging because features affecting combination quality are often difficult to identify; quality, being a function of all items in the combination, cannot be computed independently; and there are an exponential number of combinations to explore. In this demonstration, we present CHIC, a first-of-its-kind, combination-based recommendation system for clothing. The audience will interact with our system through the CHIC mobile app which allows the user to take a picture of a clothing item and search for interesting combinations containing the item instantly. The audience can also compete with CHIC to create alternate ensembles and compare quality. Finally, we highlight via visualizations the core modules of CHIC including model building and our novel search and classification algorithm, C-Search.

#index 1972745
#* A scalable lock manager for multicores
#@ Hyungsoo Jung;Hyuck Han;Alan D. Fekete;Gernot Heiser;Heon Y. Yeom
#t 2013
#c 5
#% 9241
#% 91631
#% 158109
#% 336201
#% 403195
#% 1181215
#% 1203477
#% 1368900
#% 1426413
#% 1523878
#% 1526973
#% 1528307
#% 1563004
#% 1606343
#% 1716261
#% 1730860
#% 1763280
#% 1871522
#! Modern implementations of DBMS software are intended to take advantage of high core counts that are becoming common in high-end servers. However, we have observed that several database platforms, including MySQL, Shore-MT, and a commercial system, exhibit throughput collapse as load increases, even for a workload with little or no logical contention for locks. Our analysis of MySQL identifies latch contention within the lock manager as the bottleneck responsible for this collapse. We design a lock manager with reduced latching, implement it in MySQL, and show that it avoids the collapse and generally improves performance. Our efficient implementation of a lock manager is enabled by a staged allocation and de-allocation of locks. Locks are pre-allocated in bulk, so that the lock manager only has to perform simple list-manipulation operations during the acquire and release phases of a transaction. De-allocation of the lock data-structures is also performed in bulk, which enables the use of fast implementations of lock acquisition and release, as well as concurrent deadlock checking.

#index 1972746
#* Photon: fault-tolerant and scalable joining of continuous data streams
#@ Rajagopal Ananthanarayanan;Venkatesh Basker;Sumit Das;Ashish Gupta;Haifeng Jiang;Tianhao Qiu;Alexey Reznichenko;Deomid Ryabkov;Manpreet Singh;Shivakumar Venkataraman
#t 2013
#c 5
#% 86476
#% 114577
#% 232771
#% 251359
#% 322880
#% 654444
#% 723279
#% 769155
#% 778482
#% 963669
#% 989488
#% 993948
#% 998845
#% 1022232
#% 1054227
#% 1426584
#% 1535212
#% 1538766
#% 1581898
#% 1874962
#% 1911326
#! Web-based enterprises process events generated by millions of users interacting with their websites. Rich statistical data distilled from combining such interactions in near real-time generates enormous business value. In this paper, we describe the architecture of Photon, a geographically distributed system for joining multiple continuously flowing streams of data in real-time with high scalability and low latency, where the streams may be unordered or delayed. The system fully tolerates infrastructure degradation and datacenter-level outages without any manual intervention. Photon guarantees that there will be no duplicates in the joined output (at-most-once semantics) at any point in time, that most joinable events will be present in the output in real-time (near-exact semantics), and exactly-once semantics eventually. Photon is deployed within Google Advertising System to join data streams such as web search queries and user clicks on advertisements. It produces joined logs that are used to derive key business metrics, including billing for advertisers. Our production deployment processes millions of events per minute at peak with an average end-to-end latency of less than 10 seconds. We also present challenges and solutions in maintaining large persistent state across geographically distant locations, and highlight the design principles that emerged from our experience.

#index 1972747
#* Cumulon: optimizing statistical data analysis in the cloud
#@ Botong Huang;Shivnath Babu;Jun Yang
#t 2013
#c 5
#% 69503
#% 280819
#% 293727
#% 963669
#% 1063553
#% 1318636
#% 1328066
#% 1328095
#% 1355170
#% 1426582
#% 1426585
#% 1468234
#% 1523598
#% 1523820
#% 1532872
#% 1573238
#% 1594623
#% 1621142
#% 1641841
#% 1695451
#% 1730777
#% 1895052
#! We present Cumulon, a system designed to help users rapidly develop and intelligently deploy matrix-based big-data analysis programs in the cloud. Cumulon features a flexible execution model and new operators especially suited for such workloads. We show how to implement Cumulon on top of Hadoop/HDFS while avoiding limitations of MapReduce, and demonstrate Cumulon's performance advantages over existing Hadoop-based systems for statistical data analysis. To support intelligent deployment in the cloud according to time/budget constraints, Cumulon goes beyond database-style optimization to make choices automatically on not only physical operators and their parameters, but also hardware provisioning and configuration settings. We apply a suite of benchmarking, simulation, modeling, and search techniques to support effective cost-based optimization over this rich space of deployment plans.

#index 1972748
#* Inter-media hashing for large-scale retrieval from heterogeneous data sources
#@ Jingkuan Song;Yang Yang;Yi Yang;Zi Huang;Heng Tao Shen
#t 2013
#c 5
#% 227937
#% 479462
#% 479649
#% 722904
#% 762054
#% 810069
#% 814646
#% 839873
#% 855563
#% 898309
#% 1022281
#% 1040539
#% 1279785
#% 1292880
#% 1415758
#% 1426417
#% 1432313
#% 1450831
#% 1474964
#% 1581931
#% 1649056
#% 1661257
#% 1750268
#% 1750621
#% 1770364
#% 1775721
#% 1775731
#% 1826280
#% 1878997
#% 1884017
#! In this paper, we present a new multimedia retrieval paradigm to innovate large-scale search of heterogenous multimedia data. It is able to return results of different media types from heterogeneous data sources, e.g., using a query image to retrieve relevant text documents or images from different data sources. This utilizes the widely available data from different sources and caters for the current users' demand of receiving a result list simultaneously containing multiple types of data to obtain a comprehensive understanding of the query's results. To enable large-scale inter-media retrieval, we propose a novel inter-media hashing (IMH) model to explore the correlations among multiple media types from different data sources and tackle the scalability issue. To this end, multimedia data from heterogeneous data sources are transformed into a common Hamming space, in which fast search can be easily implemented by XOR and bit-count operations. Furthermore, we integrate a linear regression model to learn hashing functions so that the hash codes for new data points can be efficiently generated. Experiments conducted on real-world large-scale multimedia datasets demonstrate the superiority of our proposed method compared with state-of-the-art techniques.

#index 1972749
#* Collective spatial keyword queries: a distance owner-driven approach
#@ Cheng Long;Raymond Chi-Wing Wong;Ke Wang;Ada Wai-Chee Fu
#t 2013
#c 5
#% 93664
#% 287466
#% 838407
#% 847167
#% 1058620
#% 1206801
#% 1206997
#% 1214668
#% 1328137
#% 1486233
#% 1523828
#% 1555383
#% 1581875
#% 1581877
#% 1594674
#% 1618262
#% 1641963
#% 1720754
#% 1798388
#% 1798389
#% 1806261
#% 1806273
#% 1972749
#! Recently, spatial keyword queries become a hot topic in the literature. One example of these queries is the collective spatial keyword query (CoSKQ) which is to find a set of objects in the database such that it covers a set of given keywords collectively and has the smallest cost. Unfortunately, existing exact algorithms have severe scalability problems and existing approximate algorithms, though scalable, cannot guarantee near-to-optimal solutions. In this paper, we study the CoSKQ problem and address the above issues. Firstly, we consider the CoSKQ problem using an existing cost measurement called the maximum sum cost. This problem is called MaxSum-CoSKQ and is known to be NP-hard. We observe that the maximum sum cost of a set of objects is dominated by at most three objects which we call the distance owners of the set. Motivated by this, we propose a distance owner-driven approach which involves two algorithms: one is an exact algorithm which runs faster than the best-known existing algorithm by several orders of magnitude and the other is an approximate algorithm which improves the best-known constant approximation factor from 2 to 1.375. Secondly, we propose a new cost measurement called diameter cost and CoSKQ with this measurement is called Dia-CoSKQ. We prove that Dia-CoSKQ is NP-hard. With the same distance owner-driven approach, we design two algorithms for Dia-CoSKQ: one is an exact algorithm which is efficient and scalable and the other is an approximate algorithm which gives a √3-factor approximation. We conducted extensive experiments on real datasets which verified that the proposed exact algorithms are scalable and the proposed approximate algorithms return near-to-optimal solutions.

#index 1972750
#* InfoGather+: semantic matching and annotation of numeric and time-varying attributes in web tables
#@ Meihui Zhang;Kaushik Chakrabarti
#t 2013
#c 5
#% 413869
#% 572314
#% 765433
#% 830529
#% 1127393
#% 1328200
#% 1417383
#% 1426513
#% 1523913
#% 1592311
#% 1642010
#% 1693868
#% 1770327
#% 1770418
#% 1869827
#! Users often need to gather information about "entities" of interest. Recent efforts try to automate this task by leveraging the vast corpus of HTML tables; this is referred to as "entity augmentation". The accuracy of entity augmentation critically depends on semantic relationships between web tables as well as semantic labels of those tables. Current techniques work well for string-valued and static attributes but perform poorly for numeric and time-varying attributes. In this paper, we first build a semantic graph that (i) labels columns with unit, scale and timestamp information and (ii) computes semantic matches between columns even when the same numeric attribute is expressed in different units or scales. Second, we develop a novel entity augmentation API suited for numeric and time-varying attributes that leverages the semantic graph. Building the graph is challenging as such label information is often missing from the column headers. Our key insight is to leverage the wealth of tables on the web and infer label information from semantically matching columns of other web tables; this complements "local" extraction from column headers. However, this creates an interdependence between labels and semantic matches; we address this challenge by representing the task as a probabilistic graphical model that jointly discovers labels and semantic matches over all columns. Our experiments on real-life datasets show that (i) our semantic graph contains higher quality labels and semantic matches and (ii) entity augmentation based on the above graph has significantly higher precision and recall compared with the state-of-the-art.

#index 1972751
#* Shortest path and distance queries on road networks: towards bridging theory and practice
#@ Andy Diwen Zhu;Hui Ma;Xiaokui Xiao;Siqiang Luo;Youze Tang;Shuigeng Zhou
#t 2013
#c 5
#% 410276
#% 443208
#% 443533
#% 801679
#% 813718
#% 881686
#% 1063472
#% 1230568
#% 1328210
#% 1357841
#% 1412885
#% 1464046
#% 1484129
#% 1523971
#% 1581881
#% 1668186
#% 1707457
#% 1972751
#! Given two locations s and t in a road network, a distance query returns the minimum network distance from s to t, while a shortest path query computes the actual route that achieves the minimum distance. These two types of queries find important applications in practice, and a plethora of solutions have been proposed in past few decades. The existing solutions, however, are optimized for either practical or asymptotic performance, but not both. In particular, the techniques with enhanced practical efficiency are mostly heuristic-based, and they offer unattractive worst-case guarantees in terms of space and time. On the other hand, the methods that are worst-case efficient often entail prohibitive preprocessing or space overheads, which render them inapplicable for the large road networks (with millions of nodes) commonly used in modern map applications. This paper presents Arterial Hierarchy (AH), an index structure that narrows the gap between theory and practice in answering shortest path and distance queries on road networks. On the theoretical side, we show that, under a realistic assumption, AH answers any distance query in Õ(log α) time, where α = dmax/dmin, and dmax (resp. dmin) is the largest (resp. smallest) L∞ distance between any two nodes in the road network. In addition, any shortest path query can be answered in Õ(k + log α) time, where k is the number of nodes on the shortest path. On the practical side, we experimentally evaluate AH on a large set of real road networks with up to twenty million nodes, and we demonstrate that (i) AH outperforms the state of the art in terms of query time, and (ii) its space and pre-computation overheads are moderate.

#index 1972752
#* Branch-and-bound algorithm for reverse top-k queries
#@ Akrivi Vlachou;Christos Doulkeridis;Kjetil Nørvåg;Yannis Kotidis
#t 2013
#c 5
#% 300163
#% 300180
#% 333854
#% 333951
#% 479816
#% 480330
#% 527189
#% 763882
#% 864452
#% 875025
#% 893108
#% 941785
#% 1022226
#% 1022243
#% 1063485
#% 1206698
#% 1206995
#% 1328184
#% 1523827
#% 1589324
#% 1595894
#% 1618269
#% 1770352
#% 1919727
#% 1984410
#! Top-k queries return to the user only the k best objects based on the individual user preferences and comprise an essential tool for rank-aware query processing. Assuming a stored data set of user preferences, reverse top-k queries have been introduced for retrieving the users that deem a given database object as one of their top-k results. Reverse top-k queries have already attracted significant interest in research, due to numerous real-life applications such as market analysis and product placement. Currently, the most efficient algorithm for computing the reverse top-k set is RTA. RTA has two main drawbacks when processing a reverse top-k query: (i) it needs to access all stored user preferences, and (ii) it cannot avoid executing a top-k query for each user preference that belongs to the result set. To address these limitations, in this paper, we identify useful properties for processing reverse top-k queries without accessing each user's individual preferences nor executing the top-k query. We propose an intuitive branch-and-bound algorithm for processing reverse top-k queries efficiently and discuss novel optimizations to boost its performance. Our experimental evaluation demonstrates the efficiency of the proposed algorithm that outperforms RTA by a large margin.

#index 1972753
#* Bolt-on causal consistency
#@ Peter Bailis;Ali Ghodsi;Joseph M. Hellerstein;Ion Stoica
#t 2013
#c 5
#% 3083
#% 114559
#% 124019
#% 147896
#% 151528
#% 240016
#% 316868
#% 318428
#% 320187
#% 340608
#% 397295
#% 709864
#% 823076
#% 850309
#% 978766
#% 998845
#% 1014037
#% 1063488
#% 1111848
#% 1127560
#% 1190127
#% 1426489
#% 1426492
#% 1426589
#% 1470582
#% 1530857
#% 1625057
#% 1625058
#% 1769270
#% 1889760
#% 1913814
#% 1970423
#% 1987867
#! We consider the problem of separating consistency-related safety properties from availability and durability in distributed data stores via the application of a "bolt-on" shim layer that upgrades the safety of an underlying general-purpose data store. This shim provides the same consistency guarantees atop a wide range of widely deployed but often inflexible stores. As causal consistency is one of the strongest consistency models that remain available during system partitions, we develop a shim layer that upgrades eventually consistent stores to provide convergent causal consistency. Accordingly, we leverage widely deployed eventually consistent infrastructure as a common substrate for providing causal guarantees. We describe algorithms and shim implementations that are suitable for a large class of application-level causality relationships and evaluate our techniques using an existing, production-ready data store and with real-world explicit causality relationships.

#index 1972754
#* Leveraging transitive relations for crowdsourced joins
#@ Jiannan Wang;Guoliang Li;Tim Kraska;Michael J. Franklin;Jianhua Feng
#t 2013
#c 5
#% 288952
#% 765548
#% 1063533
#% 1217163
#% 1452857
#% 1452858
#% 1477559
#% 1550748
#% 1581851
#% 1628027
#% 1628171
#% 1746845
#% 1746896
#% 1770349
#% 1770351
#% 1869838
#% 1880463
#! The development of crowdsourced query processing systems has recently attracted a significant attention in the database community. A variety of crowdsourced queries have been investigated. In this paper, we focus on the crowdsourced join query which aims to utilize humans to find all pairs of matching objects from two collections. As a human-only solution is expensive, we adopt a hybrid human-machine approach which first uses machines to generate a candidate set of matching pairs, and then asks humans to label the pairs in the candidate set as either matching or non-matching. Given the candidate pairs, existing approaches will publish all pairs for verification to a crowdsourcing platform. However, they neglect the fact that the pairs satisfy transitive relations. As an example, if o1 matches with o2, and o2 matches with o3, then we can deduce that o1 matches with o3 without needing to crowdsource (o1, o3). To this end, we study how to leverage transitive relations for crowdsourced joins. We propose a hybrid transitive-relations and crowdsourcing labeling framework which aims to crowdsource the minimum number of pairs to label all the candidate pairs. We prove the optimal labeling order and devise a parallel labeling algorithm to efficiently crowdsource the pairs following the order. We evaluate our approaches in both simulated environment and a real crowdsourcing platform. Experimental results show that our approaches with transitive relations can save much more money and time than existing methods, with a little loss in the result quality.

#index 1972755
#* Lightweight authentication of linear algebraic queries on data streams
#@ Stavros Papadopoulos;Graham Cormode;Antonios Deligiannakis;Minos Garofalakis
#t 2013
#c 5
#% 381870
#% 654444
#% 654497
#% 654507
#% 654508
#% 657774
#% 745532
#% 805466
#% 810042
#% 874980
#% 927452
#% 1015278
#% 1022213
#% 1022214
#% 1061593
#% 1194358
#% 1217147
#% 1217149
#% 1217735
#% 1223423
#% 1232259
#% 1358822
#% 1385029
#% 1594672
#% 1597302
#% 1628172
#! We consider a stream outsourcing setting, where a data owner delegates the management of a set of disjoint data streams to an untrusted server. The owner authenticates his streams via signatures. The server processes continuous queries on the union of the streams for clients trusted by the owner. Along with the results, the server sends proofs of result correctness derived from the owner's signatures, which are easily verifiable by the clients. We design novel constructions for a collection of fundamental problems over streams represented as linear algebraic queries. In particular, our basic schemes authenticate dynamic vector sums and dot products, as well as dynamic matrix products. These techniques can be adapted for authenticating a wide range of important operations in streaming environments, including group by queries, joins, in-network aggregation, similarity matching, and event processing. All our schemes are very lightweight, and offer strong cryptographic guarantees derived from formal definitions and proofs. We experimentally confirm the practicality of our schemes.

#index 1972756
#* Integrating scale out and fault tolerance in stream processing using operator state management
#@ Raul Castro Fernandez;Matteo Migliavacca;Evangelia Kalyvianaki;Peter Pietzuch
#t 2013
#c 5
#% 874999
#% 875006
#% 995806
#% 1016169
#% 1083732
#% 1164236
#% 1241604
#% 1301624
#% 1426584
#% 1460208
#% 1535212
#% 1549884
#% 1591798
#% 1604253
#% 1633106
#% 1634994
#% 1635782
#% 1660161
#% 1730987
#% 1747337
#% 1868748
#% 1869832
#% 1874962
#% 1931525
#! As users of "big data" applications expect fresh results, we witness a new breed of stream processing systems (SPS) that are designed to scale to large numbers of cloud-hosted machines. Such systems face new challenges: (i) to benefit from the "pay-as-you-go" model of cloud computing, they must scale out on demand, acquiring additional virtual machines (VMs) and parallelising operators when the workload increases; (ii) failures are common with deployments on hundreds of VMs-systems must be fault-tolerant with fast recovery times, yet low per-machine overheads. An open question is how to achieve these two goals when stream queries include stateful operators, which must be scaled out and recovered without affecting query results. Our key idea is to expose internal operator state explicitly to the SPS through a set of state management primitives. Based on them, we describe an integrated approach for dynamic scale out and recovery of stateful operators. Externalised operator state is checkpointed periodically by the SPS and backed up to upstream VMs. The SPS identifies individual operator bottlenecks and automatically scales them out by allocating new VMs and partitioning the checkpointed state. At any point, failed operators are recovered by restoring checkpointed state on a new VM and replaying unprocessed tuples. We evaluate this approach with the Linear Road Benchmark on the Amazon EC2 cloud platform and show that it can scale automatically to a load factor of L=350 with 50 VMs, while recovering quickly from failures.

#index 1972757
#* Simulation of database-valued markov chains using SimSQL
#@ Zhuhua Cai;Zografoula Vagena;Luis Perez;Subramanian Arumugam;Peter J. Haas;Christopher Jermaine
#t 2013
#c 5
#% 152585
#% 333965
#% 410276
#% 722904
#% 891559
#% 1083687
#% 1127559
#% 1176961
#% 1183378
#% 1318636
#% 1328061
#% 1328095
#% 1328186
#% 1426210
#% 1426462
#% 1426585
#% 1468421
#% 1486236
#% 1523820
#% 1523858
#% 1523866
#% 1523880
#% 1550026
#% 1558463
#% 1581889
#% 1581915
#% 1581927
#% 1590538
#% 1592312
#% 1594623
#! This paper describes the SimSQL system, which allows for SQLbased specification, simulation, and querying of database-valued Markov chains, i.e., chains whose value at any time step comprises the contents of an entire database. SimSQL extends the earlier Monte Carlo database system (MCDB), which permitted Monte Carlo simulation of static database-valued random variables. Like MCDB, SimSQL uses user-specified "VG functions" to generate the simulated data values that are the building blocks of a simulated database. The enhanced functionality of SimSQL is enabled by the ability to parametrize VG functions using stochastic tables, so that one stochastic database can be used to parametrize the generation of another stochastic database, which can parametrize another, and so on. Other key extensions include the ability to explicitly define recursive versions of a stochastic table and the ability to execute the simulation in a MapReduce environment. We focus on applying SimSQL to Bayesian machine learning.

#index 1972758
#* Provenance-based dictionary refinement in information extraction
#@ Sudeepa Roy;Laura Chiticariu;Vitaly Feldman;Frederick R. Reiss;Huaiyu Zhu
#t 2013
#c 5
#% 232532
#% 301241
#% 375017
#% 378401
#% 408396
#% 740916
#% 744539
#% 769884
#% 855108
#% 935763
#% 976987
#% 1022288
#% 1063547
#% 1183368
#% 1206687
#% 1217153
#% 1231247
#% 1234486
#% 1250181
#% 1260639
#% 1265135
#% 1328199
#% 1426461
#% 1481633
#% 1523847
#% 1581834
#% 1581888
#% 1711796
#% 1732738
#! Dictionaries of terms and phrases (e.g. common person or organization names) are integral to information extraction systems that extract structured information from unstructured text. Using noisy or unrefined dictionaries may lead to many incorrect results even when highly precise and sophisticated extraction rules are used. In general, the results of the system are dependent on dictionary entries in arbitrary complex ways, and removal of a set of entries can remove both correct and incorrect results. Further, any such refinement critically requires laborious manual labeling of the results. In this paper, we study the dictionary refinement problem and address the above challenges. Using provenance of the outputs in terms of the dictionary entries, we formalize an optimization problem of maximizing the quality of the system with respect to the refined dictionaries, study complexity of this problem, and give efficient algorithms. We also propose solutions to address incomplete labeling of the results where we estimate the missing labels assuming a statistical model. We conclude with a detailed experimental evaluation using several real-world extractors and competition datasets to validate our solutions. Beyond information extraction, our provenance-based techniques and solutions may find applications in view-maintenance in general relational settings.

#index 1972759
#* Mind the gap: large-scale frequent sequence mining
#@ Iris Miliaraki;Klaus Berberich;Rainer Gemulla;Spyros Zoupanos
#t 2013
#c 5
#% 152934
#% 290703
#% 310559
#% 329537
#% 329598
#% 329600
#% 379325
#% 420063
#% 459006
#% 630984
#% 729418
#% 745515
#% 769620
#% 778732
#% 823384
#% 946709
#% 963669
#% 985041
#% 1127463
#% 1166534
#% 1355060
#% 1536517
#% 1536527
#% 1693879
#% 1962321
#! Frequent sequence mining is one of the fundamental building blocks in data mining. While the problem has been extensively studied, few of the available techniques are sufficiently scalable to handle datasets with billions of sequences; such large-scale datasets arise, for instance, in text mining and session analysis. In this paper, we propose MG-FSM, a scalable algorithm for frequent sequence mining on MapReduce. MG-FSM can handle so-called "gap constraints", which can be used to limit the output to a controlled set of frequent sequences. At its heart, MG-FSM partitions the input database in a way that allows us to mine each partition independently using any existing frequent sequence mining algorithm. We introduce the notion of w-equivalency, which is a generalization of the notion of a "projected database" used by many frequent pattern mining algorithms. We also present a number of optimization techniques that minimize partition size, and therefore computational and communication costs, while still maintaining correctness. Our experimental study in the context of text mining suggests that MG-FSM is significantly more efficient and scalable than alternative approaches.

#index 1972760
#* TF-Label: a topological-folding labeling scheme for reachability querying in a large graph
#@ James Cheng;Silu Huang;Huanhuan Wu;Ada Wai-Chee Fu
#t 2013
#c 5
#% 47573
#% 58365
#% 88051
#% 341704
#% 379482
#% 824692
#% 864462
#% 960304
#% 1044451
#% 1055756
#% 1206685
#% 1217208
#% 1426539
#% 1482190
#% 1523819
#% 1531325
#% 1581922
#% 1594607
#% 1625106
#% 1688299
#% 1770333
#% 1770357
#% 1848109
#% 1880447
#% 1907284
#! Reachability querying is a basic graph operation with numerous important applications in databases, network analysis, computational biology, software engineering, etc. Although many indexes have been proposed to answer reachability queries, most of them are only efficient for handling relatively small graphs. We propose TF-label, an efficient and scalable labeling scheme for processing reachability queries. TF-label is constructed based on a novel topological folding (TF) that recursively folds an input graph into half so as to reduce the label size, thus improving query efficiency. We show that TF-label is efficient to construct and propose efficient algorithms and optimization schemes. Our experiments verify that TF-label is significantly more scalable and efficient than the state-of-the-art methods in both index construction and query processing.

#index 1972761
#* Finding time period-based most frequent path in big trajectory data
#@ Wuman Luo;Haoyu Tan;Lei Chen;Lionel M. Ni
#t 2013
#c 5
#% 193110
#% 280416
#% 296090
#% 427199
#% 480473
#% 480817
#% 769899
#% 794880
#% 864397
#% 960283
#% 989604
#% 1022268
#% 1044452
#% 1044468
#% 1127437
#% 1180064
#% 1190134
#% 1298896
#% 1409360
#% 1480783
#% 1523860
#% 1594591
#% 1605948
#% 1613884
#% 1613886
#% 1872250
#! The rise of GPS-equipped mobile devices has led to the emergence of big trajectory data. In this paper, we study a new path finding query which finds the most frequent path (MFP) during user-specified time periods in large-scale historical trajectory data. We refer to this query as time period-based MFP (TPMFP). Specifically, given a time period T, a source v_s and a destination v_d, TPMFP searches the MFP from v_s to v_d during T. Though there exist several proposals on defining MFP, they only consider a fixed time period. Most importantly, we find that none of them can well reflect people's common sense notion which can be described by three key properties, namely suffix-optimal (i.e., any suffix of an MFP is also an MFP), length-insensitive (i.e., MFP should not favor shorter or longer paths), and bottleneck-free (i.e., MFP should not contain infrequent edges). The TPMFP with the above properties will reveal not only common routing preferences of the past travelers, but also take the time effectiveness into consideration. Therefore, our first task is to give a TPMFP definition that satisfies the above three properties. Then, given the comprehensive TPMFP definition, our next task is to find TPMFP over huge amount of trajectory data efficiently. Particularly, we propose efficient search algorithms together with novel indexes to speed up the processing of TPMFP. To demonstrate both the effectiveness and the efficiency of our approach, we conduct extensive experiments using a real dataset containing over 11 million trajectories.

#index 1972762
#* Shark: SQL and rich analytics at scale
#@ Reynold S. Xin;Josh Rosen;Matei Zaharia;Michael J. Franklin;Scott Shenker;Ion Stoica
#t 2013
#c 5
#% 248793
#% 248795
#% 300167
#% 824697
#% 963669
#% 983467
#% 1127559
#% 1217159
#% 1217232
#% 1278123
#% 1278391
#% 1328066
#% 1328186
#% 1386049
#% 1426488
#% 1426513
#% 1523820
#% 1523824
#% 1523924
#% 1566972
#% 1567923
#% 1594630
#% 1769265
#% 1770321
#% 1770346
#% 1783374
#% 1783392
#% 1783393
#% 1880459
#% 1994165
#! Shark is a new data analysis system that marries query processing with complex analytics on large clusters. It leverages a novel distributed memory abstraction to provide a unified engine that can run SQL queries and sophisticated analytics functions (e.g. iterative machine learning) at scale, and efficiently recovers from failures mid-query. This allows Shark to run SQL queries up to 100X faster than Apache Hive, and machine learning programs more than 100X faster than Hadoop. Unlike previous systems, Shark shows that it is possible to achieve these speedups while retaining a MapReduce-like execution engine, and the fine-grained fault tolerance properties that such engine provides. It extends such an engine in several ways, including column-oriented in-memory storage and dynamic mid-query replanning, to effectively execute SQL. The result is a system that matches the speedups reported for MPP analytic databases over MapReduce, while offering fault tolerance properties and complex analytics capabilities that they lack.

#index 1972763
#* Improving regular-expression matching on strings using negative factors
#@ Xiaochun Yang;Bin Wang;Tao Qiu;Yaoshu Wang;Chen Li
#t 2013
#c 5
#% 120649
#% 212287
#% 341144
#% 347995
#% 404772
#% 528867
#% 547609
#% 1044404
#! The problem of finding matches of a regular expression (RE) on a string exists in many applications such as text editing, biosequence search, and shell commands. Existing techniques first identify candidates using substrings in the RE, then verify each of them using an automaton. These techniques become inefficient when there are many candidate occurrences that need to be verified. In this paper we propose a novel technique that prunes false negatives by utilizing negative factors, which are substrings that cannot appear in an answer. A main advantage of the technique is that it can be integrated with many existing algorithms to improve their efficiency significantly. We give a full specification of this technique. We develop an efficient algorithm that utilizes negative factors to prune candidates, then improve it by using bit operations to process negative factors in parallel. We show that negative factors, when used together with necessary factors (substrings that must appear in each answer), can achieve much better pruning power. We analyze the large number of negative factors, and develop an algorithm for finding a small number of high-quality negative factors. We conducted a thorough experimental study of this technique on real data sets, including DNA sequences, proteins, and text documents, and show the significant performance improvement when applying the technique in existing algorithms. For instance, it improved the search speed of the popular Gnu Grep tool by 11 to 74 times for text documents.

#index 1972764
#* Fast data in the era of big data: Twitter's real-time related query suggestion architecture
#@ Gilad Mishne;Jeff Dalton;Zhenghua Li;Aneesh Sharma;Jimmy Lin
#t 2013
#c 5
#% 214709
#% 279755
#% 298183
#% 340901
#% 641976
#% 730070
#% 765412
#% 869501
#% 918001
#% 960414
#% 978404
#% 993949
#% 1063553
#% 1063555
#% 1083721
#% 1130854
#% 1328060
#% 1338638
#% 1426588
#% 1426597
#% 1468530
#% 1526990
#% 1526991
#% 1535212
#% 1537502
#% 1581937
#% 1586685
#% 1598342
#% 1598383
#% 1598486
#% 1604467
#% 1621151
#% 1692327
#% 1746858
#% 1770321
#% 1770415
#% 1846784
#% 1846826
#% 1879052
#% 1895058
#% 1895062
#% 1986661
#! We present the architecture behind Twitter's real-time related query suggestion and spelling correction service. Although these tasks have received much attention in the web search literature, the Twitter context introduces a real-time "twist": after significant breaking news events, we aim to provide relevant results within minutes. This paper provides a case study illustrating the challenges of real-time data processing in the era of "big data". We tell the story of how our system was built twice: our first implementation was built on a typical Hadoop-based analytics stack, but was later replaced because it did not meet the latency requirements necessary to generate meaningful real-time results. The second implementation, which is the system deployed in production today, is a custom in-memory processing engine specifically designed for the task. This experience taught us that the current typical usage of Hadoop as a "big data" platform, while great for experimentation, is not well suited to low-latency processing, and points the way to future work on data analytics platforms that can handle "big" as well as "fast" data.

#index 1972765
#* Micro adaptivity in Vectorwise
#@ Bogdan Răducanu;Peter Boncz;Marcin Zukowski
#t 2013
#c 5
#% 11948
#% 397353
#% 425053
#% 465169
#% 742564
#% 983261
#% 1026989
#% 1092750
#% 1586201
#% 1699611
#! Performance of query processing functions in a DBMS can be affected by many factors, including the hardware platform, data distributions, predicate parameters, compilation method, algorithmic variations and the interactions between these. Given that there are often different function implementations possible, there is a latent performance diversity which represents both a threat to performance robustness if ignored (as is usual now) and an opportunity to increase the performance if one would be able to use the best performing implementation in each situation. Micro Adaptivity, proposed here, is a framework that keeps many alternative function implementations (flavors) in a system. It uses a learning algorithm to choose the most promising flavor potentially at each function call, guided by the actual costs observed so far. We argue that Micro Adaptivity both increases performance robustness, and saves development time spent in finding and tuning heuristics and cost model thresholds in query optimization. In this paper, we (i) characterize a number of factors that cause performance diversity between primitive flavors, (ii) describe an e-greedy learning algorithm that casts the flavor selection into a multi-armed bandit problem, and (iii) describe the software framework for Micro Adaptivity that we implemented in the Vectorwise system. We provide micro-benchmarks, and an overall evaluation on TPC-H, showing consistent improvements.

#index 1972766
#* Timeline index: a unified data structure for processing queries on temporal data in SAP HANA
#@ Martin Kaufmann;Amin Amiri Manjili;Panagiotis Vagenas;Peter Michael Fischer;Donald Kossmann;Franz Färber;Norman May
#t 2013
#c 5
#% 58371
#% 163442
#% 252608
#% 287070
#% 340175
#% 463751
#% 464856
#% 480096
#% 480422
#% 565462
#% 570884
#% 571296
#% 726629
#% 800003
#% 864422
#% 1054486
#% 1891266
#% 1905961
#! Managing temporal data is becoming increasingly important for many applications. Several database systems already support the time dimension, but provide only few temporal operators, which also often exhibit poor performance characteristics. On the academic side, a large number of algorithms and data structures have been proposed, but they often address a subset of these temporal operators only. In this paper, we develop the Timeline Index as a novel, unified data structure that efficiently supports temporal operators such as temporal aggregation, time travel, and temporal joins. As the Timeline Index is independent of the physical order of the data, it provides flexibility in physical design; e.g., it supports any kind of compression scheme, which is crucial for main memory column stores. Our experiments show that the Timeline Index has predictable performance and beats state-of-the-art approaches significantly, sometimes by orders of magnitude.

#index 1972767
#* Incremental mapping compilation in an object-to-relational mapping system
#@ Philip A. Bernstein;Marie Jacob;Jorge Pérez;Guillem Rull;James F. Terwilliger
#t 2013
#c 5
#% 286901
#% 342845
#% 443101
#% 674636
#% 790328
#% 824148
#% 945790
#% 960309
#% 1092008
#% 1531204
#% 1630968
#% 1666129
#% 1675648
#% 1728354
#! In an object-to-relational mapping system (ORM), mapping expressions explain how to expose relational data as objects and how to store objects in tables. If mappings are sufficiently expressive, then it is possible to define lossy mappings. If a user updates an object, stores it in the database based on a lossy mapping, and then retrieves the object from the database, the user might get a different result than the updated state of the object; that is, the mapping might not "roundtrip." To avoid this, the ORM should validate that user-defined mappings roundtrip the data. However, this problem is NP-hard, so mapping validation can be very slow for large or complex mappings. We circumvent this problem by developing an incremental compiler for OR mappings. Given a validated mapping, a modification to the object schema is compiled into incremental modifications of the mapping. We define the problem formally, present algorithms to solve it for Microsoft's Entity Framework, and report on an implementation. For some mappings, incremental compilation is over 100 times faster than a full mapping compilation, in one case dropping from 8 hours to 50 seconds.

#index 1972768
#* Query processing on smart SSDs: opportunities and challenges
#@ Jaeyoung Do;Yang-Suk Kee;Jignesh M. Patel;Chanik Park;Kwanghyun Park;David J. DeWitt
#t 2013
#c 5
#% 261738
#% 479802
#% 480821
#% 960238
#% 985755
#% 1217151
#% 1217234
#% 1222045
#% 1328139
#% 1328185
#% 1523901
#% 1523922
#% 1581941
#% 1581953
#% 1586198
#% 1623573
#! Data storage devices are getting "smarter." Smart Flash storage devices (a.k.a. "Smart SSD") are on the horizon and will package CPU processing and DRAM storage inside a Smart SSD, and make that available to run user programs inside a Smart SSD. The focus of this paper is on exploring the opportunities and challenges associated with exploiting this functionality of Smart SSDs for relational analytic query processing. We have implemented an initial prototype of Microsoft SQL Server running on a Samsung Smart SSD. Our results demonstrate that significant performance and energy gains can be achieved by pushing selected query processing components inside the Smart SSDs. We also identify various changes that SSD device manufacturers can make to increase the benefits of using Smart SSDs for data processing applications, and also suggest possible research opportunities for the database community.

#index 1972769
#* LinkBench: a database benchmark based on the Facebook social graph
#@ Timothy G. Armstrong;Vamsi Ponnekanti;Dhruba Borthakur;Mark Callaghan
#t 2013
#c 5
#% 172220
#% 172913
#% 609968
#% 1002007
#% 1127560
#% 1426489
#% 1523920
#% 1529320
#% 1545217
#% 1581937
#% 1625031
#% 1800676
#! Database benchmarks are an important tool for database researchers and practitioners that ease the process of making informed comparisons between different database hardware, software and configurations. Large scale web services such as social networks are a major and growing database application area, but currently there are few benchmarks that accurately model web service workloads. In this paper we present a new synthetic benchmark called LinkBench. LinkBench is based on traces from production databases that store "social graph" data at Facebook, a major social network. We characterize the data and query workload in many dimensions, and use the insights gained to construct a realistic synthetic benchmark. LinkBench provides a realistic and challenging test for persistent storage of social and web service data, filling a gap in the available tools for researchers, developers and administrators.

#index 1972770
#* Building, maintaining, and using knowledge bases: a report from the trenches
#@ Omkar Deshpande;Digvijay S. Lamba;Michel Tourn;Sanjib Das;Sri Subramaniam;Anand Rajaraman;Venky Harinarayan;AnHai Doan
#t 2013
#c 5
#% 198055
#% 198058
#% 956564
#% 1022235
#% 1063570
#% 1092530
#% 1190065
#% 1206800
#% 1217153
#% 1288161
#% 1409954
#% 1770409
#! A knowledge base (KB) contains a set of concepts, instances, and relationships. Over the past decade, numerous KBs have been built, and used to power a growing array of applications. Despite this flurry of activities, however, surprisingly little has been published about the end-to-end process of building, maintaining, and using such KBs in industry. In this paper we describe such a process. In particular, we describe how we build, update, and curate a large KB at Kosmix, a Bay Area startup, and later at WalmartLabs, a development and research lab of Walmart. We discuss how we use this KB to power a range of applications, including query understanding, Deep Web search, in-context advertising, event monitoring in social media, product search, social gifting, and social mining. Finally, we discuss how the KB team is organized, and the lessons learned. Our goal with this paper is to provide a real-world case study, and to contribute to the emerging direction of building, maintaining, and using knowledge bases for data management applications.

#index 1972771
#* On brewing fresh espresso: LinkedIn's distributed data serving platform
#@ Lin Qiao;Kapil Surlaker;Shirshanka Das;Tom Quiggle;Bob Schulman;Bhaskar Ghosh;Antony Curtis;Oliver Seeliger;Zhen Zhang;Aditya Auradar;Chris Beaver;Gregory Brandt;Mihir Gandhi;Kishore Gopalakrishna;Wai Ip;Swaroop Jgadish;Shi Lu;Alexander Pachev;Aditya Ramesh;Abraham Sebastian;Rupa Shanbhag;Subbu Subramaniam;Yun Sun;Sajid Topiwala;Cuong Tran;Jemiah Westerman;David Zhang
#t 2013
#c 5
#% 307360
#% 978404
#% 998845
#% 1127560
#% 1237169
#% 1770412
#% 1846785
#% 1911326
#% 1913810
#% 1913811
#! Espresso is a document-oriented distributed data serving platform that has been built to address LinkedIn's requirements for a scalable, performant, source-of-truth primary store. It provides a hierarchical document model, transactional support for modifications to related documents, real-time secondary indexing, on-the-fly schema evolution and provides a timeline consistent change capture stream. This paper describes the motivation and design principles involved in building Espresso, the data model and capabilities exposed to clients, details of the replication and secondary indexing implementation and presents a set of experimental results that characterize the performance of the system along various dimensions. When we set out to build Espresso, we chose to apply best practices in industry, already published works in research and our own internal experience with different consistency models. Along the way, we built a novel generic distributed cluster management framework, a partition-aware change- capture pipeline and a high-performance inverted index implementation.

#index 1972772
#* Turboiso: towards ultrafast and robust subgraph isomorphism search in large graph databases
#@ Wook-Shin Han;Jinsoo Lee;Jeong-Hoon Lee
#t 2013
#c 5
#% 147607
#% 288990
#% 617223
#% 765429
#% 772884
#% 813990
#% 850729
#% 960305
#% 1022280
#% 1044450
#% 1063500
#% 1127380
#% 1181229
#% 1206703
#% 1523825
#% 1523835
#% 1581921
#% 1848107
#% 1959792
#! Given a query graph q and a data graph g, the subgraph isomorphism search finds all occurrences of q in g and is considered one of the most fundamental query types for many real applications. While this problem belongs to NP-hard, many algorithms have been proposed to solve it in a reasonable time for real datasets. However, a recent study has shown, through an extensive benchmark with various real datasets, that all existing algorithms have serious problems in their matching order selection. Furthermore, all algorithms blindly permutate all possible mappings for query vertices, often leading to useless computations. In this paper, we present an efficient and robust subgraph search solution, called TurboISO, which is turbo-charged with two novel concepts, candidate region exploration and the combine and permute strategy (in short, Comb/Perm). The candidate region exploration identifies on-the-fly candidate subgraphs (i.e, candidate regions), which contain embeddings, and computes a robust matching order for each candidate region explored. The Comb/Perm strategy exploits the novel concept of the neighborhood equivalence class (NEC). Each query vertex in the same NEC has identically matching data vertices. During subgraph isomorphism search, Comb/Perm generates only combinations for each NEC instead of permutating all possible enumerations. Thus, if a chosen combination is determined to not contribute to a complete solution, all possible permutations for that combination will be safely pruned. Extensive experiments with many real datasets show that TurboISO consistently and significantly outperforms all competitors by up to several orders of magnitude.

#index 1972773
#* EBM: an entropy-based model to infer social strength from spatiotemporal data
#@ Huy Pham;Cyrus Shahabi;Yan Liu
#t 2013
#c 5
#% 318051
#% 321455
#% 660007
#% 722904
#% 891559
#% 955712
#% 1023420
#% 1135166
#% 1476153
#% 1606049
#% 1667252
#! The ubiquity of mobile devices and the popularity of location-based-services have generated, for the first time, rich datasets of people's location information at a very high fidelity. These location datasets can be used to study people's behavior - for example, social studies have shown that people, who are seen together frequently at the same place and at the same time, are most probably socially related. In this paper, we are interested in inferring these social connections by analyzing people's location information, which is useful in a variety of application domains from sales and marketing to intelligence analysis. In particular, we propose an entropy-based model (EBM) that not only infers social connections but also estimates the strength of social connections by analyzing people's co-occurrences in space and time. We examine two independent ways: diversity and weighted frequency, through which co-occurrences contribute to social strength. In addition, we take the characteristics of each location into consideration in order to compensate for cases where only limited location information is available. We conducted extensive sets of experiments with real-world datasets including both people's location data and their social connections, where we used the latter as the ground-truth to verify the results of applying our approach to the former. We show that our approach outperforms the competitors.

#index 1972774
#* RTP: robust tenant placement for elastic in-memory database clusters
#@ Jan Schaffner;Tim Januschowski;Megan Kercher;Tim Kraska;Hasso Plattner;Michael J. Franklin;Dean Jacobs
#t 2013
#c 5
#% 37868
#% 113718
#% 282460
#% 408396
#% 462779
#% 571084
#% 793895
#% 800587
#% 1062541
#% 1142433
#% 1581871
#% 1581872
#% 1581874
#% 1592341
#% 1594649
#% 1594650
#% 1598314
#% 1628175
#% 1846790
#! In the cloud services industry, a key issue for cloud operators is to minimize operational costs. In this paper, we consider algorithms that elastically contract and expand a cluster of in-memory databases depending on tenants' behavior over time while maintaining response time guarantees. We evaluate our tenant placement algorithms using traces obtained from one of SAP's production on-demand applications. Our experiments reveal that our approach lowers operating costs for the database cluster of this application by a factor of 2.2 to 10, measured in Amazon EC2 hourly rates, in comparison to the state of the art. In addition, we carefully study the trade-off between cost savings obtained by continuously migrating tenants and the robustness of servers towards load spikes and failures.

#index 1972775
#* Calibrating trajectory data for similarity-based analysis
#@ Han Su;Kai Zheng;Haozhou Wang;Jiamin Huang;Xiaofang Zhou
#t 2013
#c 5
#% 295512
#% 410855
#% 435148
#% 464847
#% 480473
#% 527176
#% 659971
#% 765451
#% 769899
#% 771228
#% 772835
#% 810049
#% 960283
#% 975042
#% 989604
#% 1016195
#% 1022268
#% 1044468
#% 1127436
#% 1147432
#% 1181287
#% 1190134
#% 1206688
#% 1328209
#% 1409360
#% 1523860
#% 1549860
#% 1594591
#% 1720760
#% 1846708
#! Due to the prevalence of GPS-enabled devices and wireless communications technologies, spatial trajectories that describe the movement history of moving objects are being generated and accumulated at an unprecedented pace. Trajectory data in a database are intrinsically heterogeneous, as they represent discrete approximations of original continuous paths derived using different sampling strategies and different sampling rates. Such heterogeneity can have a negative impact on the effectiveness of trajectory similarity measures, which are the basis of many crucial trajectory processing tasks. In this paper, we pioneer a systematic approach to trajectory calibration that is a process to transform a heterogeneous trajectory dataset to one with (almost) unified sampling strategies. Specifically, we propose an anchor-based calibration system that aligns trajectories to a set of anchor points, which are fixed locations independent of trajectory data. After examining four different types of anchor points for the purpose of building a stable reference system, we propose a geometry-based calibration approach that considers the spatial relationship between anchor points and trajectories. Then a more advanced model-based calibration method is presented, which exploits the power of machine learning techniques to train inference models from historical trajectory data to improve calibration effectiveness. Finally, we conduct extensive experiments using real trajectory datasets to demonstrate the effectiveness and efficiency of the proposed calibration system.

#index 1972776
#* Recursive mechanism: towards node differential privacy and unrestricted joins
#@ Shixi Chen;Shuigeng Zhou
#t 2013
#c 5
#% 663
#% 963241
#% 976987
#% 1217125
#% 1217148
#% 1318624
#% 1581862
#% 1740518
#% 1939569
#% 1960708
#! Existing differential privacy (DP) studies mainly consider aggregation on data sets where each entry corresponds to a particular participant to be protected. In many situations, a user may pose a relational algebra query on a database with sensitive data, and desire differentially private aggregation on the result of the query. However, no existing work is able to release such aggregation when the query contains unrestricted join operations. This severely limits the applications of existing DP techniques because many data analysis tasks require unrestricted joins. One example is subgraph counting on a graph. Furthermore, existing methods for differentially private subgraph counting support only edge DP and are subject to very simple subgraphs. Until recent, whether any nontrivial graph statistics can be released with reasonable accuracy for arbitrary kind of input graphs under node DP was still an open problem. In this paper, we propose a novel differentially private mechanism that supports unrestricted joins, to release an approximation of a linear statistic of the result of some positive relational algebra calculation over a sensitive database. The error bound of the approximate answer is roughly proportional to the empirical sensitivity of the query --- a new notion that measures the maximum possible change to the query answer when a participant withdraws its data from the sensitive database. For subgraph counting, our mechanism provides a solution to achieve node DP, for any kind of subgraphs.

#index 1972777
#* Utility-maximizing event stream suppression
#@ Di Wang;Yeye He;Elke Rundensteiner;Jeffrey F. Naughton
#t 2013
#c 5
#% 109757
#% 576761
#% 654462
#% 730044
#% 810011
#% 864412
#% 875004
#% 915266
#% 1063480
#% 1181268
#% 1206785
#% 1217161
#% 1255307
#% 1414564
#% 1426450
#% 1426563
#% 1512988
#% 1520165
#% 1523936
#% 1581831
#% 1606075
#% 1770343
#% 1848755
#! Complex Event Processing (CEP) has emerged as a technology for monitoring event streams in search of user specified event patterns. When a CEP system is deployed in sensitive environments the user may wish to mitigate leaks of private information while ensuring that useful nonsensitive patterns are still reported. In this paper we consider how to suppress events in a stream to reduce the disclosure of sensitive patterns while maximizing the detection of nonsensitive patterns. We first formally define the problem of utility-maximizing event suppression with privacy preferences, and analyze its computational hardness. We then design a suite of real-time solutions to solve this problem. Our first solution optimally solves the problem at the event-type level. The second solution, at the event-instance level, further optimizes the event-type level solution by exploiting runtime event distributions using advanced pattern match cardinality estimation techniques. Our user study and experimental evaluation over both real-world and synthetic event streams show that our algorithms are effective in maximizing utility yet still efficient enough to offer near real-time system responsiveness.

#index 1972778
#* Column imprints: a secondary index structure
#@ Lefteris Sidirourgos;Martin Kersten
#t 2013
#c 5
#% 86950
#% 191154
#% 227861
#% 273904
#% 316523
#% 322884
#% 466953
#% 481599
#% 504155
#% 824697
#% 866981
#% 982559
#% 983260
#% 993415
#% 1016131
#% 1181935
#% 1218721
#% 1312535
#% 1328102
#% 1426547
#% 1477922
#! Large scale data warehouses rely heavily on secondary indexes, such as bitmaps and b-trees, to limit access to slow IO devices. However, with the advent of large main memory systems, cache conscious secondary indexes are needed to improve also the transfer bandwidth between memory and cpu. In this paper, we introduce column imprint, a simple but efficient cache conscious secondary index. A column imprint is a collection of many small bit vectors, each indexing the data points of a single cacheline. An imprint is used during query evaluation to limit data access and thus minimize memory traffic. The compression for imprints is cpu friendly and exploits the empirical observation that data often exhibits local clustering or partial ordering as a side-effect of the construction process. Most importantly, column imprint compression remains effective and robust even in the case of unclustered data, while other state-of-the-art solutions fail. We conducted an extensive experimental evaluation to assess the applicability and the performance impact of the column imprints. The storage overhead, when experimenting with real world datasets, is just a few percent over the size of the columns being indexed. The evaluation time for over 40000 range queries of varying selectivity revealed the efficiency of the proposed index compared to zonemaps and bitmaps with WAH compression.

#index 1972779
#* An online cost sensitive decision-making method in crowdsourcing systems
#@ Jinyang Gao;Xuan Liu;Beng Chin Ooi;Haixun Wang;Gang Chen
#t 2013
#c 5
#% 1047347
#% 1150163
#% 1432722
#% 1452857
#% 1472273
#% 1550748
#% 1581851
#% 1581980
#% 1598354
#% 1730733
#% 1770349
#% 1770351
#% 1869838
#% 1880463
#! Crowdsourcing has created a variety of opportunities for many challenging problems by leveraging human intelligence. For example, applications such as image tagging, natural language processing, and semantic-based information retrieval can exploit crowd-based human computation to supplement existing computational algorithms. Naturally, human workers in crowdsourcing solve problems based on their knowledge, experience, and perception. It is therefore not clear which problems can be better solved by crowdsourcing than solving solely using traditional machine-based methods. Therefore, a cost sensitive quantitative analysis method is needed. In this paper, we design and implement a cost sensitive method for crowdsourcing. We online estimate the profit of the crowdsourcing job so that those questions with no future profit from crowdsourcing can be terminated. Two models are proposed to estimate the profit of crowdsourcing job, namely the linear value model and the generalized non-linear model. Using these models, the expected profit of obtaining new answers for a specific question is computed based on the answers already received. A question is terminated in real time if the marginal expected profit of obtaining more answers is not positive. We extends the method to publish a batch of questions in a HIT. We evaluate the effectiveness of our proposed method using two real world jobs on AMT. The experimental results show that our proposed method outperforms all the state-of-art methods.

#index 1972780
#* Characterizing tenant behavior for placement and crisis mitigation in multitenant DBMSs
#@ Aaron J. Elmore;Sudipto Das;Alexander Pucher;Divyakant Agrawal;Amr El Abbadi;Xifeng Yan
#t 2013
#c 5
#% 805473
#% 926881
#% 960265
#% 963614
#% 981520
#% 1009952
#% 1189342
#% 1217217
#% 1313373
#% 1426489
#% 1426499
#% 1581457
#% 1581871
#% 1581872
#% 1581874
#% 1588734
#% 1594596
#% 1594649
#% 1621143
#% 1747329
#% 1846790
#% 1913813
#! A multitenant database management system (DBMS) in the cloud must continuously monitor the trade-off between efficient resource sharing among multiple application databases (tenants) and their performance. Considering the scale of \attn{hundreds to} thousands of tenants in such multitenant DBMSs, manual approaches for continuous monitoring are not tenable. A self-managing controller of a multitenant DBMS faces several challenges. For instance, how to characterize a tenant given its variety of workloads, how to reduce the impact of tenant colocation, and how to detect and mitigate a performance crisis where one or more tenants' desired service level objective (SLO) is not achieved. We present Delphi, a self-managing system controller for a multitenant DBMS, and Pythia, a technique to learn behavior through observation and supervision using DBMS-agnostic database level performance measures. Pythia accurately learns tenant behavior even when multiple tenants share a database process, learns good and bad tenant consolidation plans (or packings), and maintains a pertenant history to detect behavior changes. Delphi detects performance crises, and leverages Pythia to suggests remedial actions using a hill-climbing search algorithm to identify a new tenant placement strategy to mitigate violating SLOs. Our evaluation using a variety of tenant types and workloads shows that Pythia can learn a tenant's behavior with more than 92% accuracy and learn the quality of packings with more than 86% accuracy. During a performance crisis, Delphi is able to reduce 99th percentile latencies by 80%, and can consolidate 45% more tenants than a greedy baseline, which balances tenant load without modeling tenant behavior.

#index 1972781
#* Determining the relative accuracy of attributes
#@ Yang Cao;Wenfei Fan;Wenyuan Yu
#t 2013
#c 5
#% 157889
#% 281843
#% 329541
#% 384978
#% 643566
#% 777931
#% 903332
#% 913783
#% 925294
#% 1054480
#% 1063713
#% 1075132
#% 1081580
#% 1127443
#% 1129527
#% 1303500
#% 1328156
#% 1355029
#% 1355642
#% 1373120
#% 1455643
#% 1491640
#% 1497992
#% 1538793
#% 1581885
#% 1730734
#% 1897972
#! The relative accuracy problem is to determine, given tuples t1 and t2 that refer to the same entity e, whether t1[A] is more accurate than t2A, i.e., t1A is closer to the true value of the A attribute of e than t2A. This has been a longstanding issue for data quality, and is challenging when the true values of e are unknown. This paper proposes a model for determining relative accuracy. (1) We introduce a class of accuracy rules and an inference system with a chase procedure, to deduce relative accuracy. (2) We identify and study several fundamental problems for relative accuracy. Given a set Ie of tuples pertaining to the same entity e and a set of accuracy rules, these problems are to decide whether the chase process terminates, is Church-Rosser, and leads to a unique target tuple te composed of the most accurate values from Ie for all the attributes of e. (3) We propose a framework for inferring accurate values with user interaction. (4) We provide algorithms underlying the framework, to find the unique target tuple te whenever possible; when there is no enough information to decide a complete te, we compute top-k candidate targets based on a preference model. (5) Using real-life and synthetic data, we experimentally verify the effectiveness and efficiency of our method.

#index 1972782
#* Optimal splitters for temporal and multi-version databases
#@ Wangchao Le;Feifei Li;Yufei Tao;Robert Christensen
#t 2013
#c 5
#% 41684
#% 210190
#% 227866
#% 299982
#% 340825
#% 466506
#% 479648
#% 480299
#% 480817
#% 494333
#% 548489
#% 571296
#% 723366
#% 726629
#% 745513
#% 802243
#% 813793
#% 881459
#% 1022238
#% 1127420
#% 1180004
#% 1241889
#! Temporal and multi-version databases are ideal candidates for a distributed store, which offers large storage space, and parallel and distributed processing power from a cluster of (commodity) machines. A key challenge is to achieve a good load balancing algorithm for storage and processing of these data, which is done by partitioning the database. We introduce the concept of optimal splitters for temporal and multi-version databases, which induce a partition of the input data set, and guarantee that the size of the maximum bucket be minimized among all possible configurations, given a budget for the desired number of buckets. We design efficient methods for memory- and disk resident data respectively, and show that they significantly outperform competing baseline methods both theoretically and empirically on large real data sets.

#index 1972783
#* Value invention in data exchange
#@ Patricia C. Arocena;Boris Glavic;Renee J. Miller
#t 2013
#c 5
#% 286998
#% 328429
#% 378409
#% 480429
#% 481935
#% 824736
#% 826032
#% 850730
#% 893094
#% 912245
#% 927032
#% 960352
#% 993981
#% 1036084
#% 1127370
#% 1180001
#% 1328194
#% 1426593
#% 1434929
#% 1523804
#% 1538791
#% 1552654
#% 1581857
#% 1763277
#! The creation of values to represent incomplete information, often referred to as value invention, is central in data exchange. Within schema mappings, Skolem functions have long been used for value invention as they permit a precise representation of missing information. Recent work on a powerful mapping language called second-order tuple generating dependencies (SO tgds), has drawn attention to the fact that the use of arbitrary Skolem functions can have negative computational and programmatic properties in data exchange. In this paper, we present two techniques for understanding when the Skolem functions needed to represent the correct semantics of incomplete information are computationally well-behaved. Specifically, we consider when the Skolem functions in second-order (SO) mappings have a first-order (FO) semantics and are therefore programmatically and computationally more desirable for use in practice. Our first technique, linearization, significantly extends the Nash, Bernstein and Melnik unskolemization algorithm, by understanding when the sets of arguments of the Skolem functions in a mapping are related by set inclusion. We show that such a linear relationship leads to mappings that have FO semantics and are expressible in popular mapping languages including source-to-target tgds and nested tgds. Our second technique uses source semantics, specifically functional dependencies (including keys), to transform SO mappings into equivalent FO mappings. We show that our algorithms are applicable to a strictly larger class of mappings than previous approaches, but more importantly we present an extensive experimental evaluation that quantifies this difference (about 78% improvement) over an extensive schema mapping benchmark and illustrates the applicability of our results on real mappings.

#index 1972784
#* Quantiles over data streams: an experimental study
#@ Lu Wang;Ge Luo;Ke Yi;Graham Cormode
#t 2013
#c 5
#% 248820
#% 273907
#% 333931
#% 492912
#% 765404
#% 783740
#% 801695
#% 801696
#% 810009
#% 810059
#% 816392
#% 874903
#% 903219
#% 954300
#% 993969
#% 1127608
#% 1217131
#% 1373450
#% 1489548
#% 1581908
#% 1770118
#% 1920318
#% 1938415
#! A fundamental problem in data management and analysis is to generate descriptions of the distribution of data. It is most common to give such descriptions in terms of the cumulative distribution, which is characterized by the quantiles of the data. The design and engineering of efficient methods to find these quantiles has attracted much study, especially in the case where the data is described incrementally, and we must compute the quantiles in an online, streaming fashion. Yet while such algorithms have proved to be tremendously useful in practice, there has been limited formal comparison of the competing methods, and no comprehensive study of their performance. In this paper, we remedy this deficit by providing a taxonomy of different methods, and describe efficient implementations. In doing so, we propose and analyze variations that have not been explicitly studied before, yet which turn out to perform the best. To illustrate this, we provide detailed experimental comparisons demonstrating the tradeoffs between space, time, and accuracy for quantile computation.

#index 1972785
#* String similarity measures and joins with synonyms
#@ Jiaheng Lu;Chunbin Lin;Wei Wang;Chen Li;Haiyong Wang
#t 2013
#c 5
#% 2833
#% 46803
#% 214073
#% 255137
#% 280850
#% 480654
#% 519953
#% 593764
#% 729913
#% 749468
#% 765463
#% 788218
#% 864392
#% 893164
#% 956506
#% 1041329
#% 1206615
#% 1206665
#% 1217200
#% 1217204
#% 1328164
#% 1523903
#% 1573234
#% 1581932
#% 1590535
#% 1654056
#% 1739421
#% 1770326
#! A string similarity measure quantifies the similarity between two text strings for approximate string matching or comparison. For example, the strings "Sam" and "Samuel" can be considered similar. Most existing work that computes the similarity of two strings only considers syntactic similarities, e.g., number of common words or q-grams. While these are indeed indicators of similarity, there are many important cases where syntactically different strings can represent the same real-world object. For example, "Bill" is a short form of "William". Given a collection of predefined synonyms, the purpose of the paper is to explore such existing knowledge to evaluate string similarity measures more effectively and efficiently, thereby boosting the quality of string matching. In particular, we first present an expansion-based framework to measure string similarities efficiently while considering synonyms. Because using synonyms in similarity measures is, while expressive, computationally expensive (NP-hard), we propose an efficient algorithm, called selective-expansion, which guarantees the optimality in many real scenarios. We then study a novel indexing structure called SI-tree, which combines both signature and length filtering strategies, for efficient string similarity joins with synonyms. We develop an estimator to approximate the size of candidates to enable an online selection of signature filters to further improve the efficiency. This estimator provides strong low-error, high-confidence guarantees while requiring only logarithmic space and time costs, thus making our method attractive both in theory and in practice. Finally, the results from an empirical study of the algorithms verify the effectiveness and efficiency of our approach.

#index 1972786
#* On the correct and complete enumeration of the core search space
#@ Guido Moerkotte;Pit Fender;Marius Eich
#t 2013
#c 5
#% 86947
#% 201927
#% 220425
#% 267270
#% 334006
#% 463276
#% 463735
#% 465165
#% 480091
#% 481608
#% 482115
#% 511345
#% 562135
#% 565457
#% 651607
#% 765457
#% 893165
#% 960323
#% 1063510
#% 1211646
#% 1594583
#! Reordering more than traditional joins (e.g. outerjoins, antijoins) requires some care, since not all reorderings are valid. To prevent invalid plans, two approaches have been described in the literature. We show that both approaches still produce invalid plans. We present three conflict detectors. All of them are (1) correct, i.e., prevent invalid plans, (2) easier to understand and implement than the previous (buggy) approaches, (3) more flexible in the sense that the restriction that all predicates must reject nulls is no longer required, and (4) extensible in the sense that it is easy to add new operators. Further, the last of our three approaches is complete, i.e., it allows for the generation of all valid plans within the core search space.

#index 1972787
#* Fast exact shortest-path distance queries on large networks by pruned landmark labeling
#@ Takuya Akiba;Yoichi Iwata;Yuichi Yoshida
#t 2013
#c 5
#% 379482
#% 433981
#% 725363
#% 729923
#% 754117
#% 818476
#% 823342
#% 833631
#% 881460
#% 906306
#% 960259
#% 1002007
#% 1019117
#% 1127407
#% 1128428
#% 1130956
#% 1181254
#% 1206910
#% 1292553
#% 1384246
#% 1399997
#% 1426510
#% 1426513
#% 1482228
#% 1492390
#% 1560413
#% 1597258
#% 1642126
#% 1770332
#% 1770356
#% 1798387
#% 1846748
#% 1912935
#% 1924355
#! We propose a new exact method for shortest-path distance queries on large-scale networks. Our method precomputes distance labels for vertices by performing a breadth-first search from every vertex. Seemingly too obvious and too inefficient at first glance, the key ingredient introduced here is pruning during breadth-first searches. While we can still answer the correct distance for any pair of vertices from the labels, it surprisingly reduces the search space and sizes of labels. Moreover, we show that we can perform 32 or 64 breadth-first searches simultaneously exploiting bitwise operations. We experimentally demonstrate that the combination of these two techniques is efficient and robust on various kinds of large-scale real-world networks. In particular, our method can handle social networks and web graphs with hundreds of millions of edges, which are two orders of magnitude larger than the limits of previous exact methods, with comparable query time to those of previous methods.

#index 1972788
#* ODYS: an approach to building a massively-parallel search engine using a DB-IR tightly-integrated parallel DBMS for higher-level functionality
#@ Kyu-Young Whang;Tae-Seob Yun;Yeon-Mi Yeo;Il-Yeol Song;Hyuk-Yoon Kwon;In-Joong Kim
#t 2013
#c 5
#% 578337
#% 723279
#% 774207
#% 800613
#% 869534
#% 963669
#% 984942
#% 998845
#% 1002142
#% 1166469
#% 1278123
#% 1328186
#! Recently, parallel search engines have been implemented based on scalable distributed file systems such as Google File System. However, we claim that building a massively-parallel search engine using a parallel DBMS can be an attractive alternative since it supports a higher-level (i.e., SQL-level) interface than that of a distributed file system for easy and less error-prone application development while providing scalability. Regarding higher-level functionality, we can draw a parallel with the traditional O/S file system vs. DBMS. In this paper, we propose a new approach of building a massively-parallel search engine using a DB-IR tightly-integrated parallel DBMS. To estimate the performance, we propose a hybrid (i.e., analytic and experimental) performance model for the parallel search engine. We argue that the model can accurately estimate the performance of a massively-parallel (e.g., 300-node) search engine using the experimental results obtained from a small-scale (e.g., 5-node) one. We show that the estimation error between the model and the actual experiment is less than 2.13% by observing that the bulk of the query processing time is spent at the slave (vs. at the master and network) and by estimating the time spent at the slave based on actual measurement. Using our model, we demonstrate a commercial-level scalability and performance of our architecture. Our proposed system ODYS is capable of handling 1 billion queries per day (81 queries/sec) for 30 billion Web pages by using only 43,472 nodes with an average query response time of 194 ms. By using twice as many (86,944) nodes, ODYS can provide an average query response time of 148 ms. These results show that building a massively-parallel search engine using a parallel DBMS is a viable approach with advantages of supporting the high-level (i.e., DBMS-level), SQL-like programming interface.

#index 1972789
#* Efficient sentiment correlation for large-scale demographics
#@ Mikalai Tsytsarau;Sihem Amer-Yahia;Themis Palpanas
#t 2013
#c 5
#% 810058
#% 823413
#% 824709
#% 915316
#% 993961
#% 1332376
#% 1426516
#% 1482457
#% 1550734
#% 1756063
#% 1913110
#! Analyzing sentiments of demographic groups is becoming important for the Social Web, where millions of users provide opinions on a wide variety of content. While several approaches exist for mining sentiments from product reviews or micro-blogs, little attention has been devoted to aggregating and comparing extracted sentiments for different demographic groups over time, such as 'Students in Italy' or 'Teenagers in Europe'. This problem demands efficient and scalable methods for sentiment aggregation and correlation, which account for the evolution of sentiment values, sentiment bias, and other factors associated with the special characteristics of web data. We propose a scalable approach for sentiment indexing and aggregation that works on multiple time granularities and uses incrementally updateable data structures for online operation. Furthermore, we describe efficient methods for computing meaningful sentiment correlations, which exploit pruning based on demographics and use top-k correlations compression techniques. We present an extensive experimental evaluation with both synthetic and real datasets, demonstrating the effectiveness of our pruning techniques and the efficiency of our solution.

#index 1972790
#* Crowd mining
#@ Yael Amsterdamer;Yael Grossman;Tova Milo;Pierre Senellart
#t 2013
#c 5
#% 152934
#% 248791
#% 280456
#% 481290
#% 481779
#% 614619
#% 735357
#% 768635
#% 799792
#% 835045
#% 956564
#% 1065099
#% 1083692
#% 1526538
#% 1581851
#% 1699611
#% 1746876
#% 1846744
#! Harnessing a crowd of Web users for data collection has recently become a wide-spread phenomenon. A key challenge is that the human knowledge forms an open world and it is thus difficult to know what kind of information we should be looking for. Classic databases have addressed this problem by data mining techniques that identify interesting data patterns. These techniques, however, are not suitable for the crowd. This is mainly due to properties of the human memory, such as the tendency to remember simple trends and summaries rather than exact details. Following these observations, we develop here for the first time the foundations of crowd mining. We first define the formal settings. Based on these, we design a framework of generic components, used for choosing the best questions to ask the crowd and mining significant patterns from the answers. We suggest general implementations for these components, and test the resulting algorithm's performance on benchmarks that we designed for this purpose. Our algorithm consistently outperforms alternative baseline algorithms.

#index 1972791
#* Quality and efficiency for kernel density estimates in large data
#@ Yan Zheng;Jeffrey Jestes;Jeff M. Phillips;Feifei Li
#t 2013
#c 5
#% 1331
#% 91390
#% 91780
#% 210190
#% 221326
#% 227866
#% 248820
#% 259995
#% 280480
#% 281671
#% 299982
#% 300193
#% 333931
#% 345611
#% 393059
#% 464215
#% 465060
#% 479648
#% 593888
#% 724162
#% 743284
#% 863400
#% 866990
#% 874903
#% 903219
#% 954620
#% 1211829
#% 1385997
#% 1426450
#% 1581908
#% 1588455
#% 1614964
#! Kernel density estimates are important for a broad variety of applications. Their construction has been well-studied, but existing techniques are expensive on massive datasets and/or only provide heuristic approximations without theoretical guarantees. We propose randomized and deterministic algorithms with quality guarantees which are orders of magnitude more efficient than previous algorithms. Our algorithms do not require knowledge of the kernel or its bandwidth parameter and are easily parallelizable. We demonstrate how to implement our ideas in a centralized setting and in MapReduce, although our algorithms are applicable to any large-scale data processing framework. Extensive experiments on large real datasets demonstrate the quality, efficiency, and scalability of our techniques.

#index 1972792
#* Reverse engineering complex join queries
#@ Meihui Zhang;Hazem Elmeleegy;Cecilia M. Procopiuc;Divesh Srivastava
#t 2013
#c 5
#% 659990
#% 660011
#% 960259
#% 1207007
#% 1217187
#% 1217198
#% 1424594
#% 1770325
#% 1869829
#! We study the following problem: Given a database D with schema G and an output table Out, compute a join query Q that generates OUT from D. A simpler variant allows Q to return a superset of Out. This problem has numerous applications, both by itself, and as a building block for other problems. Related prior work imposes conditions on the structure of Q which are not always consistent with the application, but simplify computation. We discuss several natural SQL queries that do not satisfy these conditions and cannot be discovered by prior work. In this paper, we propose an efficient algorithm that discovers queries with arbitrary join graphs. A crucial insight is that any graph can be characterized by the combination of a simple structure, called a star, and a series of merge steps over the star. The merge steps define a lattice over graphs derived from the same star. This allows us to explore the set of candidate solutions in a principled way and quickly prune out a large number of infeasible graphs. We also design several optimizations that significantly reduce the running time. Finally, we conduct an extensive experimental study over a benchmark database and show that our approach is scalable and accurately discovers complex join queries.

#index 1972793
#* On optimal worst-case matching
#@ Cheng Long;Raymond Chi-Wing Wong;Philip S. Yu;Minhao Jiang
#t 2013
#c 5
#% 6788
#% 44219
#% 122671
#% 300163
#% 320969
#% 1022250
#% 1063470
#% 1120944
#% 1174127
#% 1265149
#% 1328203
#% 1400781
#% 1432748
#% 1622578
#% 1972793
#! Bichromatic reverse nearest neighbor (BRNN) queries have been studied extensively in the literature of spatial databases. Given a set P of service-providers and a set O of customers, a BRNN query is to find which customers in O are "interested" in a given service-provider in P. Recently, it has been found that this kind of queries lacks the consideration of the capacities of service-providers and the demands of customers. In order to address this issue, some spatial matching problems have been proposed, which, however, cannot be used for some real-life applications like emergency facility allocation where the maximum matching cost (or distance) should be minimized. In this paper, we propose a new problem called Spatial Matching for Minimizing Maximum matching distance (SPM-MM). Then, we design two algorithms for SPM-MM, Threshold-Adapt and Swap-Chain. Threshold-Adapt is simple and easy to understand but not scalable to large datasets due to its relatively high time/space complexity. Swap-Chain, which follows a fundamentally different idea from Threshold-Adapt, runs faster than Threshold-Adapt by orders of magnitude and uses significantly less memory. We conducted extensive empirical studies which verified the efficiency and scalability of Swap-Chain.

#index 1972794
#* BitWeaving: fast scans for main memory data processing
#@ Yinan Li;Jignesh M. Patel
#t 2013
#c 5
#% 227861
#% 321443
#% 333930
#% 397361
#% 864446
#% 875026
#% 1127400
#% 1206624
#% 1217168
#% 1328141
#% 1523855
#% 1523974
#% 1628175
#% 1846777
#% 1972794
#! This paper focuses on running scans in a main memory data processing system at "bare metal" speed. Essentially, this means that the system must aim to process data at or near the speed of the processor (the fastest component in most system configurations). Scans are common in main memory data processing environments, and with the state-of-the-art techniques it still takes many cycles per input tuple to apply simple predicates on a single column of a table. In this paper, we propose a technique called BitWeaving that exploits the parallelism available at the bit level in modern processors. BitWeaving operates on multiple bits of data in a single cycle, processing bits from different columns in each cycle. Thus, bits from a batch of tuples are processed in each cycle, allowing BitWeaving to drop the cycles per column to below one in some case. BitWeaving comes in two flavors: BitWeaving/V which looks like a columnar organization but at the bit level, and BitWeaving/H which packs bits horizontally. In this paper we also develop the arithmetic framework that is needed to evaluate predicates using these BitWeaving organizations. Our experimental results show that both these methods produce significant performance benefits over the existing state-of-the-art methods, and in some cases produce over an order of magnitude in performance improvement.

#index 1972795
#* Efficiently computing k-edge connected components via graph decomposition
#@ Lijun Chang;Jeffrey Xu Yu;Lu Qin;Xuemin Lin;Chengfei Liu;Weifa Liang
#t 2013
#c 5
#% 108785
#% 237380
#% 577356
#% 823357
#% 956459
#% 1068802
#% 1083508
#% 1168382
#% 1426539
#% 1523970
#% 1584827
#% 1594586
#% 1798416
#% 1846696
#% 1878116
#! Efficiently computing k-edge connected components in a large graph, G = (V, E), where V is the vertex set and E is the edge set, is a long standing research problem. It is not only fundamental in graph analysis but also crucial in graph search optimization algorithms. Consider existing techniques for computing k-edge connected components are quite time consuming and are unlikely to be scalable for large scale graphs, in this paper we firstly propose a novel graph decomposition paradigm to iteratively decompose a graph G for computing its k-edge connected components such that the number of drilling-down iterations h is bounded by the "depth" of the k-edge connected components nested together to form G, where h usually is a small integer in practice. Secondly, we devise a novel, efficient threshold-based graph decomposition algorithm, with time complexity O(l × |E|), to decompose a graph G at each iteration, where l usually is a small integer with l « |V|. As a result, our algorithm for computing k-edge connected components significantly improves the time complexity of an existing state-of-the-art technique from O(|V|2|E| + |V|3 log |V|) to O(h × l × |E|). Finally, we conduct extensive performance studies on large real and synthetic graphs. The performance studies demonstrate that our techniques significantly outperform the state-of-the-art solution by several orders of magnitude.

#index 1972796
#* Efficient top-k algorithms for approximate substring matching
#@ Younghoon Kim;Kyuseok Shim
#t 2013
#c 5
#% 299984
#% 333679
#% 480654
#% 616528
#% 765262
#% 824684
#% 869500
#% 956458
#% 991786
#% 1016219
#% 1022218
#% 1022227
#% 1180235
#% 1181283
#% 1198208
#% 1206665
#% 1206985
#% 1215244
#% 1217200
#% 1217204
#% 1418196
#% 1426578
#% 1581890
#! There is a wide range of applications that require to query a large database of texts to search for similar strings or substrings. Traditional approximate substring matching requests a user to specify a similarity threshold. Without top-k approximate substring matching, users have to try repeatedly different maximum distance threshold values when the proper threshold is unknown in advance. In our paper, we first propose the efficient algorithms for finding the top-k approximate substring matches with a given query string in a set of data strings. To reduce the number of expensive distance computations, the proposed algorithms utilize our novel filtering techniques which take advantages of q-grams and inverted q-gram indexes available. We conduct extensive experiments with real-life data sets. Our experimental results confirm the effectiveness and scalability of our proposed algorithms.

#index 1972797
#* Controlled lock violation
#@ Goetz Graefe;Mark Lillibridge;Harumi Kuno;Joseph Tucek;Alistair Veitch
#t 2013
#c 5
#% 5333
#% 14204
#% 286994
#% 289399
#% 427195
#% 464699
#% 479602
#% 990408
#% 1077184
#% 1181215
#% 1328149
#% 1426413
#% 1523856
#% 1668635
#% 1770319
#% 1871522
#! In databases with a large buffer pool, a transaction may run in less time than it takes to log the transaction's commit record on stable storage. Such cases motivate a technique called early lock release: immediately after appending its commit record to the log buffer in memory, a transaction may release its locks. Thus, it cuts overall lock duration to a fraction and reduces lock contention accordingly. Early lock release also has its problems. The initial mention of early lock release was incomplete, the first detailed description and implementation was incorrect with respect to read-only transactions, and the most recent design initially had errors and still does not cover unusual lock modes such as "increment" locks. Thus, we set out to achieve the same goals as early lock release but with a different, simpler, and more robust approach. The resulting technique, controlled lock violation, requires no new theory, applies to any lock mode, promises less implementation effort and slightly less run-time effort, and also optimizes distributed transactions, e.g., in systems that rely on multiple replicas for high availability and high reliability. In essence, controlled lock violation retains locks until the transaction is durable but permits other transactions to violate its locks while flushing its commit log record to stable storage.

#index 1972798
#* X-FTL: transactional FTL for SQLite databases
#@ Woon-Hak Kang;Sang-Won Lee;Bongki Moon;Gi-Hwan Oh;Changwoo Min
#t 2013
#c 5
#% 131555
#% 152905
#% 287725
#% 464055
#% 960130
#% 979345
#% 985754
#% 1468431
#% 1601165
#% 1765829
#% 1765837
#% 1905691
#! In the era of smartphones and mobile computing, many popular applications such as Facebook, twitter, Gmail, and even Angry birds game manage their data using SQLite. This is mainly due to the development productivity and solid transactional support. For transactional atomicity, however, SQLite relies on less sophisticated but costlier page-oriented journaling mechanisms. Hence, this is often cited as the main cause of tardy responses in mobile applications. Flash memory does not allow data to be updated in place, and the copy-on-write strategy is adopted by most flash storage devices. In this paper, we propose X-FTL, a transactional flash translation layer(FTL) for SQLite databases. By offloading the burden of guaranteeing the transactional atomicity from a host system to flash storage and by taking advantage of the copy-on-write strategy used in modern FTLs, X-FTL drastically improves the transactional throughput almost for free without resorting to costly journaling schemes. We have implemented X-FTL on an SSD development board called OpenSSD, and modified SQLite and ext4 file system minimally to make them compatible with the extended abstractions provided by X-FTL. We demonstrate the effectiveness of X-FTL using real and synthetic SQLite workloads for smartphone applications, TPC-C benchmark for OLTP databases, and FIO benchmark for file systems.

#index 1972799
#* NADEEF: a commodity data cleaning system
#@ Michele Dallachiesa;Amr Ebaid;Ahmed Eldawy;Ahmed Elmagarmid;Ihab F. Ilyas;Mourad Ouzzani;Nan Tang
#t 2013
#c 5
#% 420072
#% 480496
#% 480499
#% 749847
#% 752741
#% 810019
#% 833132
#% 903332
#% 913783
#% 1022228
#% 1054480
#% 1063725
#% 1180000
#% 1201863
#% 1206764
#% 1209770
#% 1309312
#% 1328143
#% 1426508
#% 1550749
#% 1581885
#% 1675299
#% 1693866
#% 1763276
#% 1890006
#! Despite the increasing importance of data quality and the rich theoretical and practical contributions in all aspects of data cleaning, there is no single end-to-end off-the-shelf solution to (semi-)automate the detection and the repairing of violations w.r.t. a set of heterogeneous and ad-hoc quality constraints. In short, there is no commodity platform similar to general purpose DBMSs that can be easily customized and deployed to solve application-specific data quality problems. In this paper, we present NADEEF, an extensible, generalized and easy-to-deploy data cleaning platform. NADEEF distinguishes between a programming interface and a core to achieve generality and extensibility. The programming interface allows the users to specify multiple types of data quality rules, which uniformly define what is wrong with the data and (possibly) how to repair it through writing code that implements predefined classes. We show that the programming interface can be used to express many types of data quality rules beyond the well known CFDs (FDs), MDs and ETL rules. Treating user implemented interfaces as black-boxes, the core provides algorithms to detect errors and to clean data. The core is designed in a way to allow cleaning algorithms to cope with multiple rules holistically, i.e. detecting and repairing data errors without differentiating between various types of rules. We showcase two implementations for core repairing algorithms. These two implementations demonstrate the extensibility of our core, which can also be replaced by other user-provided algorithms. Using real-life data, we experimentally verify the generality, extensibility, and effectiveness of our system.

#index 1972800
#* An efficient query indexing mechanism for filtering geo-textual data
#@ Lisi Chen;Gao Cong;Xin Cao
#t 2013
#c 5
#% 124009
#% 158911
#% 300179
#% 333938
#% 481607
#% 632050
#% 765453
#% 810061
#% 864410
#% 1015320
#% 1046418
#% 1063471
#% 1206801
#% 1206997
#% 1328137
#% 1581877
#% 1594674
#% 1618262
#% 1633088
#% 1641963
#% 1720754
#% 1846721
#% 1943522
#! Massive amount of data that are geo-tagged and associated with text information are being generated at an unprecedented scale. Users may want to be notified of interesting geo-textual objects during a period of time. For example, a user may want to be informed when tweets containing term "garage sale" are posted within 5 km of the user's home in the next 72 hours. In this paper, for the first time we study the problem of matching a stream of incoming Boolean Range Continuous queries over a stream of incoming geo-textual objects in real time. We develop a new system for addressing the problem. In particular, we propose a hybrid index, called IQ-tree, and novel cost models for managing a stream of incoming Boolean Range Continuous queries. We also propose algorithms for matching the queries with incoming geo-textual objects based on the index. Results of empirical studies with implementations of the proposed techniques demonstrate that the paper's proposals offer scalability and are capable of excellent performance.

#index 1972801
#* DeltaNI: an efficient labeling scheme for versioned hierarchical data
#@ Jan Finis;Robert Brunel;Alfons Kemper;Thomas Neumann;Franz Färber;Norman May
#t 2013
#c 5
#% 58371
#% 102809
#% 397358
#% 397375
#% 428150
#% 449870
#% 480475
#% 480489
#% 480659
#% 565265
#% 571296
#% 742561
#% 753529
#% 765488
#% 800523
#% 860355
#% 1015298
#% 1016147
#% 1072638
#% 1127611
#% 1217201
#% 1327570
#% 1422744
#% 1504056
#% 1594617
#% 1666131
#% 1667313
#% 1683914
#! Main-memory database systems are emerging as the new backbone of business applications. Besides flat relational data representations also hierarchical ones are essential for these modern applications; therefore we devise a new indexing and versioning approach for hierarchies that is deeply integrated into the relational kernel. We propose the DeltaNI index as a versioned pendant of the nested intervals (NI) labeling scheme. The index is space- and time-efficient and yields a gapless, fixed-size integer NI labeling for each version while also supporting branching histories. In contrast to a naive NI labeling, it facilitates even complex updates of the tree structure. As many query processing techniques that work on top of the NI labeling have already been proposed, our index can be used as a building block for processing various kinds of queries. We evaluate the performance of the index on large inputs consisting of millions of nodes and thousands of versions. Thereby we show that DeltaNI scales well and can deliver satisfying performance for large business scenarios.

#index 1972802
#* PrivGene: differentially private model fitting using genetic algorithms
#@ Jun Zhang;Xiaokui Xiao;Yin Yang;Zhenjie Zhang;Marianne Winslett
#t 2013
#c 5
#% 197394
#% 366182
#% 369236
#% 465859
#% 466020
#% 1029084
#% 1372692
#% 1426454
#% 1451189
#% 1523886
#% 1581862
#% 1581864
#% 1584903
#% 1606359
#% 1670071
#% 1740518
#% 1770124
#% 1770348
#% 1813854
#% 1846816
#% 1846817
#% 1880451
#% 1880452
#% 1880453
#! epsilon-differential privacy is rapidly emerging as the state-of-the-art scheme for protecting individuals' privacy in published analysis results over sensitive data. The main idea is to perform random perturbations on the analysis results, such that any individual's presence in the data has negligible impact on the randomized results. This paper focuses on analysis tasks that involve model fitting, i.e., finding the parameters of a statistical model that best fit the dataset. For such tasks, the quality of the differentially private results depends upon both the effectiveness of the model fitting algorithm, and the amount of perturbations required to satisfy the privacy guarantees. Most previous studies start from a state-of-the-art, non-private model fitting algorithm, and develop a differentially private version. Unfortunately, many model fitting algorithms require intensive perturbations to satisfy -differential privacy, leading to poor overall result quality. Motivated by this, we propose PrivGene, a general-purpose differentially private model fitting solution based on genetic algorithms (GA). PrivGene needs significantly less perturbations than previous methods, and it achieves higher overall result quality, even for model fitting tasks where GA is not the first choice without privacy considerations. Further, PrivGene performs the random perturbations using a novel technique called the enhanced exponential mechanism, which improves over the exponential mechanism by exploiting the special properties of model fitting tasks. As case studies, we apply PrivGene to three common analysis tasks involving model fitting: logistic regression, SVM classification, and k-means clustering. Extensive experiments using real data confirm the high result quality of PrivGene, and its superiority over existing methods.

#index 1972803
#* DBMS metrology: measuring query time
#@ Sabah Currim;Richard T. Snodgrass;Young-Kyoon Suh;Rui Zhang;Matthew Wong Johnson;Cheng Yi
#t 2013
#c 5
#% 119485
#% 480077
#% 480803
#% 745518
#% 819203
#% 1206984
#% 1426489
#% 1668633
#% 1846730
#% 1846789
#! It is surprisingly hard to obtain accurate and precise measurements of the time spent executing a query. We review relevant process and overall measures obtainable from the Linux kernel and introduce a structural causal model relating these measures. A thorough correlational analysis provides strong support for this model. Using this model, we developed a timing protocol, which (1) performs sanity checks to ensure validity of the data, (2) drops some query executions via clearly motivated predicates, (3) drops some entire queries at a cardinality, again via clearly motivated predicates, (4) for those that remain, for each computes a single measured time by a carefully justified formula over the underlying measures of the remaining query executions, and (5) performs post-analysis sanity checks. The resulting query time measurement procedure, termed the Tucson Protocol, applies to proprietary and open-source DBMSes.

#index 1972804
#* Indexing methods for moving object databases: games and other applications
#@ Hanan Samet;Jagan Sankaranarayanan;Michael Auerbach
#t 2013
#c 5
#% 23266
#% 42091
#% 58369
#% 78354
#% 172379
#% 236617
#% 319508
#% 342595
#% 427199
#% 462041
#% 462781
#% 465015
#% 479473
#% 480093
#% 481920
#% 510366
#% 572293
#% 605182
#% 632072
#% 632105
#% 654479
#% 765419
#% 818938
#% 1206754
#% 1217236
#% 1230827
#! Moving object databases arise in numerous applications such as traffic monitoring, crowd tracking, and games. They all require keeping track of objects that move and thus the database of objects must be constantly updated. The cover fieldtree (more commonly known as the loose quadtree and the loose octree, depending on the dimension of the underlying space) is designed to overcome the drawback of spatial data structures that associate objects with their minimum enclosing quadtree (octree) cells which is that the size of these cells depends more on the position of the objects and less on their size. In fact, the size of these cells may be as large as the entire space from which the objects are drawn. The loose quadtree (octree) overcomes this drawback by expanding the size of the space that is spanned by each quadtree (octree) cell c of width w by a cell expansion factor p (p0) so that the expanded cell is of width (1+p)*w and an object is associated with its minimum enclosing expanded quadtree (octree) cell. It is shown that for an object o with minimum bounding hypercube box b of radius r (i.e., half the length of a side of the hypercube), the maximum possible width w of the minimum enclosing expanded quadtree cell c is just a function of r and p, and is independent of the position of o. Normalizing w via division by 2r enables calculating the range of possible expanded quadtree cell sizes as a function of p. For p = 0.5 the range consists of just two values and usually just one value for p = 1. This makes updating very simple and fast as for p = 0.5, there are at most two possible new cells associated with the moved object and thus the update can be done in O(1) time. Experiments with random data showed that the update time to support motion in such an environment is minimized when p is infinitesimally less than 1, with as much as a one order of magnitude increase in the number of updates that can be handled vis-a-vis the p=0 case in a given unit of time. Similar results for updates were obtained for an N-body simulation where improved query performance and scalability were also observed. Finally, in order amplify the paper, a video tiled "Crates and Barrels" was produced which is an N-body simulation of 14,000 objects. The video is available from the following URL: http://www.youtube.com/watch?v=Sokq3FRGc0s. An applet to illustrate the behavior of the loose quadtree was developed and is available from http://donar.umiacs.umd. edu/quadtree/rectangles/loosequad.html.

#index 1972805
#* Generalized scale independence through incremental precomputation
#@ Michael Armbrust;Eric Liang;Tim Kraska;Armando Fox;Michael J. Franklin;David A. Patterson
#t 2013
#c 5
#% 13016
#% 18614
#% 153722
#% 210210
#% 227944
#% 264263
#% 273917
#% 300141
#% 333962
#% 480141
#% 480158
#% 480623
#% 788999
#% 805864
#% 998845
#% 1015304
#% 1217160
#% 1426451
#% 1557845
#% 1606436
#% 1654050
#% 1869832
#! Developers of rapidly growing applications must be able to anticipate potential scalability problems before they cause performance issues in production environments. A new type of data independence, called scale independence, seeks to address this challenge by guaranteeing a bounded amount of work is required to execute all queries in an application, independent of the size of the underlying data. While optimization strategies have been developed to provide these guarantees for the class of queries that are scale-independent when executed using simple indexes, there are important queries for which such techniques are insufficient. Executing these more complex queries scale-independently requires precomputation using incrementally-maintained materialized views. However, since this precomputation effectively shifts some of the query processing burden from execution time to insertion time, a scale-independent system must be careful to ensure that storage and maintenance costs do not threaten scalability. In this paper, we describe a scale-independent view selection and maintenance system, which uses novel static analysis techniques that ensure that created views do not themselves become scaling bottlenecks. Finally, we present an empirical analysis that includes all the queries from the TPC-W benchmark and validates our implementation's ability to maintain nearly constant high-quantile query and update latency even as an application scales to hundreds of machines.

#index 1972806
#* Automatic synthesis of out-of-core algorithms
#@ Yannis Klonatos;Andres Nötzli;Andrej Spielmann;Christoph Koch;Victor Kuncak
#t 2013
#c 5
#% 55316
#% 350064
#% 393844
#% 428324
#% 461158
#% 464540
#% 566566
#% 874997
#% 1024171
#% 1092380
#% 1108020
#% 1127552
#% 1247817
#% 1269068
#% 1344663
#% 1426530
#% 1538272
#% 1556573
#% 1668735
#% 1706736
#! We present a system for the automatic synthesis of efficient algorithms specialized for a particular memory hierarchy and a set of storage devices. The developer provides two independent inputs: 1) an algorithm that ignores memory hierarchy and external storage aspects; and 2) a description of the target memory hierarchy, including its topology and parameters. Our system is able to automatically synthesize memory-hierarchy and storage-device-aware algorithms out of those specifications, for tasks such as joins and sorting. The framework is extensible and allows developers to quickly synthesize custom out-of-core algorithms as new storage technologies become available.

#index 1972807
#* Toward practical query pricing with QueryMarket
#@ Paraschos Koutris;Prasang Upadhyaya;Magdalena Balazinska;Bill Howe;Dan Suciu
#t 2013
#c 5
#% 571217
#% 1206951
#% 1661430
#% 1730735
#% 1770132
#% 1895089
#! We develop a new pricing system, QueryMarket, for flexible query pricing in a data market based on an earlier theoretical framework (Koutris et al., PODS 2012). To build such a system, we show how to use an Integer Linear Programming formulation of the pricing problem for a large class of queries, even when pricing is computationally hard. Further, we leverage query history to avoid double charging when queries purchased over time have overlapping information, or when the database is updated. We then present a technique that fairly shares revenue when multiple sellers are involved. Finally, we implement our approach in a prototype and evaluate its performance on several query workloads.

#index 1972808
#* Data stream warehousing
#@ Lukasz Golab;Theodore Johnson
#t 2013
#c 5
#% 227944
#% 480816
#% 1015283
#% 1033350
#% 1068971
#% 1206909
#% 1217211
#% 1217212
#% 1217239
#% 1426597
#% 1467748
#% 1468411
#% 1523814
#% 1581918
#% 1581938
#% 1591789
#% 1594567
#% 1615861
#% 1654872
#% 1755333
#% 1770406
#% 1791599
#% 1846782
#% 1846826
#% 1855859
#% 1869832
#% 1895062

#index 1972809
#* Machine learning for big data
#@ Tyson Condie;Paul Mineiro;Neoklis Polyzotis;Markus Weimer
#t 2013
#c 5
#% 264164
#% 891060
#% 1023420
#% 1426513
#% 1468231
#% 1769265
#% 1783374
#! Statistical Machine Learning has undergone a phase transition from a pure academic endeavor to being one of the main drivers of modern commerce and science. Even more so, recent results such as those on tera-scale learning [1] and on very large neural networks [2] suggest that scale is an important ingredient in quality modeling. This tutorial introduces current applications, techniques and systems with the aim of cross-fertilizing research between the database and machine learning communities. The tutorial covers current large scale applications of Machine Learning, their computational model and the workflow behind building those. Based on this foundation, we present the current state-of-the-art in systems support in the bulk of the tutorial. We also identify critical gaps in the state-of-the-art. This leads to the closing of the seminar, where we introduce two sets of open research questions: Better systems support for the already established use cases of Machine Learning and support for recent advances in Machine Learning research.

#index 1972810
#* Rethinking eventual consistency
#@ Philip A. Bernstein;Sudipto Das
#t 2013
#c 5
#% 3083
#% 58378
#% 69312
#% 124019
#% 240016
#% 287303
#% 307360
#% 320187
#% 397295
#% 416023
#% 476793
#% 603821
#% 803470
#% 993199
#% 998845
#% 1014037
#% 1127560
#% 1523795
#% 1625057
#% 1625058
#% 1769270
#% 1804748
#% 1825993
#% 1911327
#% 1987867
#! There has been a resurgence of work on replicated, distributed database systems to meet the demands of intermittently-connected clients and of disaster-tolerant databases that span data centers. Many systems weaken the criteria for replica-consistency or isolation, and in some cases add new mechanisms, to improve partition-tolerance, availability, and performance. We present a framework for comparing these criteria and mechanisms, to help architects navigate through this complex design space.

#index 1972811
#* FriendRouter: real-time path finder in social networks
#@ Wladston Viana;Mirella M. Moro
#t 2013
#c 5
#% 241
#! Online social networks have become a platform for running and optimizing classical algorithms. Here, we introduce a tool for finding paths between social network users in real-time, a task that classical solutions are not tailored for.

#index 1972812
#* Adaptive log compression for massive log data
#@ Robert Christensen;Feifei Li
#t 2013
#c 5
#% 960250
#! We present a novel adaptive log compression scheme. Results show 30% improvement on compression ratios over existing approaches.

#index 1972813
#* BUZZARD: a NUMA-aware in-memory indexing system
#@ Lukas M. Maas;Thomas Kissinger;Dirk Habich;Wolfgang Lehner
#t 2013
#c 5
#! With the availability of large main memory capacities, in-memory index structures have become an important component of modern data management platforms. Current research even suggests index-based query processing as an alternative or supplement for traditional tuple-at-a-time processing models. However, while simple sequential scan operations can fully exploit the high bandwidth provided by main memory, indexes are mainly latency bound and spend most of their time waiting for memory accesses. Considering current hardware trends, the problem of high memory latency is further exacerbated as modern shared-memory multiprocessors with non-uniform memory access (NUMA) become increasingly common. On those NUMA platforms, the execution time of index operations is dominated by memory access latency that increases dramatically when accessing memory on remote sockets. Therefore, good index performance can only be achieved through careful optimization of the index structure to the given topology. BUZZARD is a NUMA-aware in-memory indexing system. Using adaptive data partitioning techniques, BUZZARD distributes a prefix-tree-based index across the NUMA system and hands off incoming requests to worker threads located on each partition's respective NUMA node. This approach reduces the number of remote memory accesses to a minimum and improves cache utilization. In addition, all indexes inside BUZZARD are only accessed by their respective owner, eliminating the need for synchronization primitives like compare-and-swap.

#index 1972814
#* Resa: realtime elastic streaming analytics in the cloud
#@ Tian Tan;Richard T.B. Ma;Marianne Winslett;Yin Yang;Yong Yu;Zhenjie Zhang
#t 2013
#c 5
#% 726621
#% 975023
#! We propose Resa, a novel framework for robust, elastic and realtime stream processing in the cloud. In addition to traditional functionalities of streaming and cloud systems, Resa provides (i) a novel mechanism that handles dynamic additions and removals nodes in an operator, and (ii) a node re-assignment scheme that minimizes output latency using a queuing model. We have implemented Resa on top of Twitter Storm. Experiments using real data demonstrate the effectiveness and efficiency of Resa.

#index 1972815
#* Mobile interaction and query optimizationin a protein-ligand data analysis system
#@ Marvin Lapeine;Katherine G. Herbert;Emily Hill;Nina M. Goodey
#t 2013
#c 5
#! With current trends in integrating phylogenetic analysis into pharma-research, computing systems that integrate the two areas can help the drug discovery field. DrugTree is a tool that overlays ligand data on a protein-motivated phylogenetic tree. While initial tests of DrugTree are successful, it has been noticed that there are a number of lags concerning querying the tree. Due to the interleaving nature of the data, query optimization can become problematic since the data is being obtained from multiple sources, integrated and then presented to the user with the phylogenetic imposed upon the phylogenetic analysis layer. This poster presents our initial methodologies for addressing the query optimization issues. Our approach applies standards as well as uses novel mechanisms to help improve performance time.

#index 1972816
#* We are drowning in a sea of least publishable units (LPUs)
#@ David J. DeWitt;Ihab F. Ilyas;Jeffrey Naughton;Michael Stonebraker
#t 2013
#c 5
#! Our field is drowning in a sea of conference submissions. We assert that the sheer number of papers has begun to seriously hurt the quality of the work that the field is doing and that the field is going to implode unless we take action to remedy the situation. In order to improve the quality of the papers being published we must reduce the number being submitted. This will require a change in the culture of our field where "more" is being equated to "better" by both hiring and promotion committees. In this panel we will explore some ideas for correcting the situation.

#index 1974615
#* A trichotomy for regular simple path queries on graphs
#@ Guillaume Bagan;Angela Bonifati;Benoit Groz
#t 2013
#c 5
#% 32904
#% 53756
#% 91428
#% 101944
#% 164160
#% 179784
#% 189741
#% 197751
#% 292677
#% 298179
#% 308881
#% 336026
#% 408396
#% 415004
#% 481434
#% 600179
#% 632039
#% 731485
#% 857282
#% 1058237
#% 1206916
#% 1266687
#% 1347829
#% 1426512
#% 1552657
#% 1581833
#% 1594585
#% 1746861
#% 1770126
#% 1808505
#% 1818411
#% 1882083
#! Regular path queries (RPQs) select vertices connected by some path in a graph. The edge labels of such a path have to form a word that matches a given regular expression. We investigate the evaluation of RPQs with an additional constraint that prevents multiple traversals of the same vertices. Those regular simple path queries (RSPQs) quickly become intractable, even for basic languages such as (aa)* or a*ba*. In this paper, we establish a comprehensive classification of regular languages with respect to the complexity of the corresponding regular simple path query problem. More precisely, we identify for which languages RSPQs can be evaluated in polynomial time, and show that evaluation is NP-complete for languages outside this fragment. We thus fully characterize the frontier between tractability and intractability for RSPQs, and we refine our results to show the following trichotomy: evaluation of RSPQs is either AC0 , NL-complete or NP-complete in data complexity, depending on the language L. The fragment identified also admits a simple characterization in terms of regular expressions. Finally, we also discuss the complexity of deciding whether a language L belongs to the fragment above. We consider several alternative representations of L: DFAs, NFAs or regular expressions, and prove that this problem is NL-complete for the first representation and PSPACE-complete for the other two. As a conclusion we extend our results from edge-labeled graphs to vertex-labeled graphs.

#index 1974616
#* Foundations of data-aware process analysis: a database theory perspective
#@ Diego Calvanese;Giuseppe De Giacomo;Marco Montali
#t 2013
#c 5
#% 11804
#% 11815
#% 23901
#% 27043
#% 36179
#% 47975
#% 64284
#% 64439
#% 65912
#% 66122
#% 100613
#% 116045
#% 137886
#% 146202
#% 168262
#% 188333
#% 189636
#% 194990
#% 196419
#% 198469
#% 213957
#% 213971
#% 248013
#% 248029
#% 248039
#% 268796
#% 273709
#% 277326
#% 277344
#% 287631
#% 291885
#% 299971
#% 319244
#% 321054
#% 342119
#% 384978
#% 415960
#% 415961
#% 415971
#% 415991
#% 416011
#% 416028
#% 416050
#% 443257
#% 464708
#% 464858
#% 467630
#% 479745
#% 481772
#% 502749
#% 535067
#% 543344
#% 564260
#% 570649
#% 576091
#% 630964
#% 690480
#% 770373
#% 801671
#% 801675
#% 807689
#% 809239
#% 810053
#% 824147
#% 824702
#% 874885
#% 875055
#% 888015
#% 893117
#% 900782
#% 942360
#% 1060757
#% 1062932
#% 1063731
#% 1065709
#% 1099727
#% 1100590
#% 1103257
#% 1132251
#% 1137809
#% 1153042
#% 1165081
#% 1179996
#% 1180017
#% 1270568
#% 1326580
#% 1385073
#% 1394312
#% 1395966
#% 1415590
#% 1472960
#% 1490271
#% 1538776
#% 1538777
#% 1566271
#% 1581817
#% 1590450
#% 1622305
#% 1622330
#% 1700118
#% 1738489
#% 1746933
#% 1747245
#% 1871520
#% 1880440
#% 1888845
#% 1932113
#% 1959483
#% 1972715
#! In this work we survey the research on foundations of data-aware (business) processes that has been carried out in the database theory community. We show that this community has indeed developed over the years a multi-faceted culture of merging data and processes. We argue that it is this community that should lay the foundations to solve, at least from the point of view of formal analysis, the dichotomy between data and processes still persisting in business process management.

#index 1974617
#* Secure database-as-a-service with Cipherbase
#@ Arvind Arasu;Spyros Blanas;Ken Eguro;Manas Joglekar;Raghav Kaushik;Donald Kossmann;Ravi Ramamurthy;Prasang Upadhyaya;Ramarathnam Venkatesan
#t 2013
#c 5
#% 1195803
#% 1309521
#% 1581863
#% 1625037
#! Data confidentiality is one of the main concerns for users of public cloud services. The key problem is protecting sensitive data from being accessed by cloud administrators who have root privileges and can remotely inspect the memory and disk contents of the cloud servers. While encryption is the basic mechanism that can leveraged to provide data confidentiality, providing an efficient database-as-a-service that can run on encrypted data raises several interesting challenges. In this demonstration we outline the functionality of Cipherbase --- a full fledged SQL database system that supports the full generality of a database system while providing high data confidentiality. Cipherbase has a novel architecture that tightly integrates custom-designed trusted hardware for performing operations on encrypted data securely such that an administrator cannot get access to any plaintext corresponding to sensitive data.

#index 1974618
#* Fine-grained disclosure control for app ecosystems
#@ Gabriel M. Bender;Lucja Kot;Johannes Gehrke;Christoph Koch
#t 2013
#c 5
#% 198465
#% 333964
#% 384978
#% 572307
#% 599549
#% 765447
#% 826032
#% 874973
#% 942359
#% 1128847
#% 1206973
#% 1426418
#% 1527000
#% 1591691
#% 1670071
#% 1770132
#% 1850591
#% 1868688
#! The modern computing landscape contains an increasing number of app ecosystems, where users store personal data on platforms such as Facebook or smartphones. APIs enable third-party applications (apps) to utilize that data. A key concern associated with app ecosystems is the confidentiality of user data. In this paper, we develop a new model of disclosure in app ecosystems. In contrast with previous solutions, our model is data-derived and semantically meaningful. Information disclosure is modeled in terms of a set of distinguished security views. Each query is labeled with the precise set of security views that is needed to answer it, and these labels drive policy decisions. We explain how our disclosure model can be used in practice and provide algorithms for labeling conjunctive queries for the case of single-atom security views. We show that our approach is useful by demonstrating the scalability of our algorithms and by applying it to the real-world disclosure control system used by Facebook.

#index 1974619
#* Trinity: a distributed graph engine on a memory cloud
#@ Bin Shao;Haixun Wang;Yatao Li
#t 2013
#c 5
#% 328257
#% 360802
#% 769155
#% 989488
#% 998842
#% 1091073
#% 1206699
#% 1278373
#% 1318636
#% 1350336
#% 1374374
#% 1380974
#% 1426513
#% 1475077
#% 1475163
#% 1529319
#% 1625033
#% 1770359
#% 1848107
#% 1911310
#% 1911311
#% 1992369
#! Computations performed by graph algorithms are data driven, and require a high degree of random data access. Despite the great progresses made in disk technology, it still cannot provide the level of efficient random access required by graph computation. On the other hand, memory-based approaches usually do not scale due to the capacity limit of single machines. In this paper, we introduce Trinity, a general purpose graph engine over a distributed memory cloud. Through optimized memory management and network communication, Trinity supports fast graph exploration as well as efficient parallel computing. In particular, Trinity leverages graph access patterns in both online and offline computation to optimize memory and communication for best performance. These enable Trinity to support efficient online query processing and offline analytics on large graphs with just a few commodity machines. Furthermore, Trinity provides a high level specification language called TSL for users to declare data schema and communication protocols, which brings great ease-of-use for general purpose graph management and computing. Our experiments show Trinity's performance in both low latency graph queries as well as high throughput graph analytics on web-scale, billion-node graphs.

#index 1974620
#* Performance and resource modeling in highly-concurrent OLTP workloads
#@ Barzan Mozafari;Carlo Curino;Alekh Jindal;Samuel Madden
#t 2013
#c 5
#% 69477
#% 137753
#% 210179
#% 415972
#% 463753
#% 571083
#% 722887
#% 1044489
#% 1119400
#% 1206984
#% 1349794
#% 1523975
#% 1549874
#% 1581457
#% 1581872
#% 1581874
#% 1688297
#! Database administrators of Online Transaction Processing (OLTP) systems constantly face difficult questions. For example, "What is the maximum throughput I can sustain with my current hardware?", "How much disk I/O will my system perform if the requests per second double?", or "What will happen if the ratio of transactions in my system changes?". Resource prediction and performance analysis are both vital and difficult in this setting. Here the challenge is due to high degrees of concurrency, competition for resources, and complex interactions between transactions, all of which non-linearly impact performance. Although difficult, such analysis is a key component in enabling database administrators to understand which queries are eating up the resources, and how their system would scale under load. In this paper, we introduce our framework, called DBSeer, that addresses this problem by employing statistical models that provide resource and performance analysis and prediction for highly concurrent OLTP workloads. Our models are built on a small amount of training data from standard log information collected during normal system operation. These models are capable of accurately measuring several performance metrics, including resource consumption on a per-transaction-type basis, resource bottlenecks, and throughput at different load levels. We have validated these models on MySQL/Linux with numerous experiments on standard benchmarks (TPC-C) and real workloads (Wikipedia), observing high accuracy (within a few percent error) when predicting all of the above metrics.

#index 1974621
#* Workload management for big data analytics
#@ Ashraf Aboulnaga;Shivnath Babu
#t 2013
#c 5
#% 442968
#% 481459
#% 1022202
#% 1088627
#% 1181224
#% 1206984
#% 1207105
#% 1247816
#% 1278391
#% 1328079
#% 1386049
#% 1426544
#% 1453005
#% 1468423
#% 1532788
#% 1549874
#% 1567923
#% 1567925
#% 1581459
#% 1581874
#% 1668642
#% 1688297
#% 1726440
#% 1783374
#% 1846730
#% 1846788
#% 1880469
#% 1891266
#% 1895061

#index 1974622
#* Data management perspectives on business process management: tutorial overview
#@ Richard Hull;Jianwen Su;Roman Vaculin
#t 2013
#c 5
#% 261267
#% 287631
#% 615784
#% 770373
#% 807689
#% 910587
#% 1017219
#% 1062932
#% 1180017
#% 1181213
#% 1255308
#% 1267647
#% 1335434
#% 1538776
#% 1546732
#% 1576946
#% 1580599
#% 1581323
#% 1622323
#% 1636393
#% 1650826
#% 1650838
#% 1726214
#% 1887599
#% 1942493
#% 1943542
#% 1954659
#% 1972715
#! Traditional approaches to Business Process Management (BPM) focus primarily on the process aspects, and treat the persistent data accessed and manipulated by the business processes as second class citizens. A recent approach to BPM, based on "business artifacts", is centered on a modeling framework that places data and process on an equal footing. The approach has been shown useful in various application domains, and one variant of business artifacts forms the basis of the emerging OMG Case Management Model and Notation (CMMN) standard. Research results have been developed around conceptual models, enterprise interoperation, business intelligence, and verification. This data-centric approach has the potential to provide the basis for a new generation of BPM technology in support of diverse application, and fueled by the insights into abstraction and data management that have been the hallmark of database research since the 70's.

#index 1986881
#* The continuous distributed monitoring model
#@ Graham Cormode
#t 2013
#c 5
#% 238182
#% 336610
#% 397443
#% 453512
#% 654443
#% 654488
#% 810009
#% 821933
#% 824652
#% 864444
#% 874994
#% 874995
#% 960368
#% 981652
#% 985896
#% 1016155
#% 1022282
#% 1039655
#% 1039695
#% 1063739
#% 1141469
#% 1217129
#% 1217131
#% 1232249
#% 1584980
#% 1651455
#% 1723621
#% 1770144
#% 1770341
#% 1770497
#% 1812433
#! In the model of continuous distributed monitoring, a number of observers each see a stream of observations. Their goal is to work together to compute a function of the union of their observations. This can be as simple as counting the total number of observations, or more complex non-linear functions such as tracking the entropy of the induced distribution. Assuming that it is too costly to simply centralize all the observations, it becomes quite challenging to design solutions which provide a good approximation to the current answer, while bounding the communication cost of the observers, and their other resources such as their space usage. This survey introduces this model, and describe a selection results in this setting, from the simple counting problem to a variety of other functions that have been studied.

#index 1986882
#* Marketplaces for data: an initial survey
#@ Fabian Schomm;Florian Stahl;Gottfried Vossen
#t 2013
#c 5
#% 636361
#% 1063570
#% 1127964
#% 1286345
#% 1743948
#% 1869633
#! Data is becoming more and more of a commodity, so that it is not surprising that data has reached the status of tradable goods. An increasing number of data providers is recognizing this and is consequently setting up platforms for selling, buying, or trading data. We identify several categories and dimensions of data marketplaces and data vendors and provide a snapshot of the situation as of Summer 2012.

#index 1986883
#* Hank Korth speaks out on two-career issues, why not to write a book in the beginning of your career, and more
#@ Marianne Winslett;Vanessa Braganholo
#t 2013
#c 5

#index 1986884
#* Data-based research at IIT Bombay
#@ Soumen Chakrabarti;Ganesh Ramakrishnan;Krithi Ramamritham;Sunita Sarawagi;S. Sudarshan
#t 2013
#c 5
#% 399551
#% 451536
#% 660011
#% 805844
#% 824693
#% 881457
#% 956526
#% 981608
#% 983805
#% 993975
#% 1108083
#% 1127439
#% 1130928
#% 1181251
#% 1206890
#% 1214667
#% 1225192
#% 1227607
#% 1328133
#% 1415874
#% 1523913
#% 1536526
#% 1551236
#% 1560156
#% 1588228
#% 1594637
#% 1594644
#% 1594658
#% 1607851
#% 1692087
#% 1746810
#% 1770330
#% 1869827
#% 1913581
#% 1946438
#% 1978713

#index 1986885
#* Intel "big data" science and technology center vision and execution plan
#@ Michael Stonebraker;Sam Madden;Pradeep Dubey
#t 2013
#c 5
#% 479602
#% 479973
#% 535933
#% 1062637
#% 1063543
#% 1217161
#% 1581867
#% 1615854
#% 1770319
#% 1846832
#% 1895072
#! Intel has moved to a collaboration model with universities consisting of "Science and Technology Centers" (ISTCs). These are located at a "hub" university with participation from other universities, contain embedded Intel personnel, and are focused on some research theme. Intel held a national competition for a 5th Science and Technology center in 2012 and selected a proposal from M.I.T. with a theme of "Big Data". This paper presents the big data vision of this technology center and the execution plan for the first few years.

#index 1986886
#* Report on the first international workshop on energy data management (EnDM 2012)
#@ Torben Bach Pedersen;Wolfgang Lehner;Gregor Hackenbroich
#t 2013
#c 5
#% 1855830

#index 1986887
#* Report on the fourth international workshop on cloud data management (CloudDB 2012)
#@ Xiaofeng Meng;Fusheng Wang;Adam Silberstein
#t 2013
#c 5
#% 1472967
#% 1482492
#% 1642333
#% 1918331
#% 1919746
#% 1920044

#index 1988482
#* Proceedings of the 2013 Sigmod/PODS Ph.D. symposium on PhD symposium
#@ Lei Chen;Xin Luna Dong
#t 2013
#c 5
#! It is our great pleasure to welcome you to the 2013 Sigmod/PODS Ph.D. Symposium, which is co-located with SIGMOD/PODS 2013 and takes place on Sunday June 23rd, 2013 in New York City, USA. This is a forum where PhD students have an opportunity of presenting their research ideas, receiving feedback from and interacting with senior members of the community. The focus of the Symposium is on mentorship and providing constructive feedback to the students. Many members of the Program Committee attend the Symposium, providing ample opportunity for discussions and feedback. Each student whose submission is accepted for presentation is paired with a member of the PC for mentorship and one-on-one discussions. This year we have two tracks for submission. The General Track is for students who are in early stages of their studies; the submissions specify the problem they plan to focus on, the thesis that they plan to investigate, and the outline of the methodology that they plan to follow. The Towards Graduation Track is for students who expect to graduate by the end of 2013; the submissions specify the contributions they have made and what remains to be accomplished. We received 26 papers in response to the call for papers: 14 for the General Track and 12 for the Towards Graduation Track. Each of these was reviewed by three PC members with a focus on selecting submissions that were ready and could benefit from the feedbacks. In the end, we selected 12 submissions, 6 for each track, for discussions at the Symposium. The proceedings include short descriptions of these PhD theses.

#index 1988483
#* Information diffusion in online social networks
#@ Adrien Guille
#t 2013
#c 5
#% 1176853
#% 1214671
#% 1425621
#% 1432574
#% 1451242
#% 1475157
#% 1535333
#% 1688545
#% 1747165
#% 1948125
#% 1971507
#! Online social networks play a major role in the spread of information at very large scale and it becomes essential to provide means to analyze this phenomenon. Analyzing information diffusion proves to be a challenging task since the raw data produced by users of these networks are a flood of ideas, recommendations, opinions, etc. The aim of this PhD work is to help in the understanding of this phenomenon. So far, our contributions are the following: (i) a survey of developments in the field; (ii) T-BaSIC, a graph-based model for information diffusion prediction; (iii) SONDY, an open source platform that helps understanding social network users' interests and activity by providing emerging topics and events detection as well as network analysis functionalities.

#index 1988484
#* Learning queries for relational, semi-structured, and graph databases
#@ Radu Ciucanu
#t 2013
#c 5
#% 697
#% 23905
#% 378409
#% 411759
#% 489044
#% 826032
#% 911086
#% 961692
#% 994015
#% 1015271
#% 1039061
#% 1039062
#% 1206612
#% 1209667
#% 1217187
#% 1223424
#% 1232194
#% 1310057
#% 1424594
#% 1426468
#% 1499980
#% 1504036
#% 1538780
#% 1538787
#% 1549883
#% 1573139
#% 1628171
#% 1642117
#% 1721253
#% 1728758
#% 1818416
#% 1818419
#% 1959485
#% 1959489
#% 1962392
#! Web applications store their data within various database models, such as relational, semi-structured, and graph data models to name a few. We study learning algorithms for queries for the above mentioned models. As a further goal, we aim to apply the results to learning cross-model database mappings, which can also be seen as queries across different schemas.

#index 1988485
#* Designing a database system for modern processing architectures
#@ Max Heimel
#t 2013
#c 5
#% 32889
#% 273906
#% 411554
#% 480803
#% 481930
#% 850738
#% 864426
#% 961238
#% 1023420
#% 1052066
#% 1063508
#% 1089604
#% 1241839
#% 1268643
#% 1270566
#% 1328185
#% 1426486
#% 1444832
#% 1500048
#% 1541400
#% 1694375
#% 1933410
#! The hardware landscape is getting increasingly diverse. A modern desktop computer can contain multiple different processing architectures like multi-core CPUs or GPUs. This diversity is expected to grow significantly in the next ten years, with micro-architectures themselves diverging towards highly parallel and heterogeneous designs. We believe that preparing database systems to exploit this diverse landscape of processing architectures will be one of the major challenges for the coming decade in database research. In this paper, we present our thoughts and results on modifying the components of a database system to efficiently use modern processing architectures. In particular, we discuss our work on offloading parts of the Query Optimizer to highly parallel processors such as graphics cards, and present our work on designing a hardware-oblivious Execution Engine that can run unchanged on a multitude of different processing architectures.

#index 1988486
#* DeepSea: self-adaptive data partitioning and replication in scalable distributed data systems
#@ Jiang Du
#t 2013
#c 5
#% 36117
#% 333965
#% 465167
#% 571169
#% 572311
#% 723279
#% 765176
#% 893175
#% 963669
#% 983467
#% 1217159
#% 1328080
#% 1328186
#% 1474889
#% 1523839
#% 1523841
#% 1526978
#% 1730737
#% 1869832
#% 1971524
#% 1972762
#! While originally proposed to provide fault-tolerance and scalability for data analysis queries on unstructured data over massive clusters, MapReduce systems today are being used for analysis of rich combinations of unstructured, semi-structured and structured data. To achieve performance on these new workloads, MapReduce systems (and the distributed file systems on which they are built) can no longer rely on static data placement strategies. In this thesis, we propose new physical data independence and adaptive data tuning solutions that can greatly improve the performance of analysis queries in systems where workloads are not static and where workloads may include complex queries with overlapping or related computations (subqueries). While profiting from the work on physical data independence in relational systems, we propose novel strategies that recognize the central role of data partitioning (and co-partitioning) in shared-nothing distributed file systems.

#index 1988487
#* Exploiting in-network processing for big data management
#@ Lukas Rupprecht
#t 2013
#c 5
#% 654497
#% 893176
#% 1054227
#% 1083754
#% 1084463
#% 1127596
#% 1246354
#% 1246355
#% 1246357
#% 1328095
#% 1350336
#% 1400975
#% 1426589
#% 1449176
#% 1479676
#% 1523824
#% 1621132
#% 1625033
#% 1667313
#% 1770324
#% 1783362
#% 1783375
#% 1874957
#! Data processing systems face the task of efficiently storing and processing data at petabyte scale, with the amount set to increase in the future. To meet such a requirement, highly scalable, shared-nothing systems, e.g. Google's BigTable [6] or Facebook's Cassandra [14], are built to partition data and process it in parallel on distributed nodes in a cluster. This allows the handling of data at scale but introduces new challenges due to the distribution of data. Running queries involves a high network overhead because data has to be exchanged between cluster nodes and hence, the network becomes a critical part of the system. To avoid the network bottleneck, it is essential for distributed data processing systems (DDPS) to be aware of the network rather than treating it as a black box. We propose in-network processing as a way of achieving network-awareness to decrease bandwidth usage by custom routing, redundancy elimination, and on-path data reduction. Thereby, we can increase the query throughput of a DDPS. The challenges of an in-network processing system range from design issues, such as performance and transparency, to the integration with query optimisation and deployment in data centres. We formulate these challenges as possible research directions and provide a prototype implementation. Our preliminary results suggest that we can significantly improve query throughput in a DDPS by performing partial data reduction within the network.

#index 1988488
#* Turning scientists into data explorers
#@ Yağız Kargın
#t 2013
#c 5
#% 248863
#% 479593
#% 654470
#% 765129
#% 845351
#% 1375919
#% 1615857
#% 1641841
#% 1652704
#% 1770339
#% 1882103
#! Nowadays scientists receive increasingly large volumes of data daily. These volumes and accompanying metadata that describes them are collected in scientific file repositories. Today's scientists need a data management tool that makes these file repositories accessible and performs a number of exploration steps near-instantly. Current database technology, however, has a long data-to-insight time, and does not provide enough interactivity to shorten the exploration time. We envision that exploiting metadata helps solving these problems. To this end, we propose a novel query execution paradigm, in which we decompose the query execution into two stages. During the first stage, we process only metadata, whereas the rest of the data is processed during the second stage. So that, we can exploit metadata to boost interactivity and to ingest only required data per query transparently. Preliminary experiments show that up-front ingestion time is reduced by orders of magnitude, while query performance remains similar. Motivated by these results, we identify the challenges on the way from the new paradigm to efficient interactive data exploration.

#index 1988489
#* RDF-4G: algorithmic building blocks for large-scale graph analytics
#@ Stephan Seufert
#t 2013
#c 5
#% 58365
#% 1355056
#% 1366460
#% 1482228
#% 1535447
#% 1878275
#! We present RDF-4G, the first three miles towards a large-scale graph-analytics engine built on top of the state-of-the-art RDF engine, RDF-3X. The algorithmic building blocks that make up this work help answering fundamental questions about relationships between entities in a graph-structured world. More precisely, our system provides insights into what we define as the trilogy of relationship analyis: Is there a relationship between entities? Who participates in the connection? How can the relationship be characterized? While the first two questions correspond to the algorithmic primitives of graph processing, reachability and shortest path queries, for answering the third question we propose a novel graph-theoretic concept, relatedness cores. The technical contributions we make in this work are efficient index structures for reachability and shortest path query processing together with a new notion of and algorithms for relationship characterization. The latter can be efficiently computed based on the techniques we have developed in our work on graph indexing. All our methods are integrated into the RDF-3X engine, the state-of-the-art system for querying RDF-structured data. Future work includes the exposure of our algorithmic building blocks to the user, via extensions to the de-facto standard query language for graph-structured data, SPARQL.

#index 1988490
#* Discovering and disambiguating named entities in text
#@ Johannes Hoffart
#t 2013
#c 5
#% 249321
#% 939376
#% 1214667
#% 1451234
#% 1536527
#% 1586117
#% 1592066
#% 1711796
#% 1746862
#% 1913604
#% 1918389
#% 1925702
#% 1987071
#! Disambiguating named entities in natural language texts maps ambiguous names to canonical entities registered in a knowledge base such as DBpedia, Freebase, or YAGO. Knowing the specific entity is an important asset for several other tasks, e.g. entity-based information retrieval or higher-level information extraction. Our approach to named entity disambiguation makes use of several ingredients: the prior probability of an entity being mentioned, the similarity between the context of the mention in the text and an entity, as well as the coherence among the entities. Extending this method, we present a novel and highly efficient measure to compute the semantic coherence between entities. This measure is especially powerful for long-tail entities or such entities that are not yet present in the knowledge base. Reliably identifying names in the input text that are not part of the knowledge base is the current focus of our work.

#index 1988491
#* The tantalizing new prospect of index-based diversified retrieval
#@ George Tsatsanifos
#t 2013
#c 5
#% 262112
#% 1026428
#% 1166473
#% 1181244
#! In this paper, we propose efficient algorithms for result diversification over indexed multidimensional data. We develop algorithms under the prism of a centralized approach, as in a database. Specifically, we rely on widely used multidimensional indexes, like the R-tree. In principle, our schemes adopt a maximal marginal relevance (MMR) ranking strategy and leverage interchange and greedy diversification techniques. Hitherto, mostly combinatorial aspects of this problem have been considered which require scanning the entire data, and therefore, existing solutions are costly.

#index 1988492
#* Exploratory mining of collaborative social content
#@ Mahashweta Das
#t 2013
#c 5
#% 249321
#% 319705
#% 408396
#% 420053
#% 768305
#% 769892
#% 907489
#% 1075132
#% 1181244
#% 1246154
#% 1429407
#% 1429408
#% 1429409
#% 1429421
#% 1451236
#% 1605973
#% 1880470
#% 1895095
#% 1919715
#! The widespread use and growing popularity of online collaborative content sites (e.g., Yelp, Amazon, IMDB) has created rich resources for users to consult in order to make purchasing decisions on various items such as restaurants, e-commerce products, movies, etc. It has also created new opportunities for producers of such items to improve business by designing better products, composing succinct advertisement snippets and building smart personalized recommendation systems. This motivates us to develop a framework for exploratory mining of user feedback on items in collaborative content sites. Typically, the amount of user feedback associated with item(s) can easily reach hundreds or thousands of ratings, tags or reviews, resulting in an overwhelming amount of information, which users may find difficult to cope with. For example, popular restaurants listed in the review site Yelp routinely receive several thousand ratings and reviews. Moreover, most online activities involve interactions between multiple items and different users, and interpreting such complex user-item interactions becomes intractable too. My PhD research concerns developing novel data mining and exploration algorithms, that account for the above-mentioned challenges, for performing aggregate analytics over available user feedback. Our analysis goal is focused towards helping (a) content consumers make more informed judgment (e.g., if a user will enjoy eating at a particular restaurant), as well as (b) content producers conduct better business (e.g., a re-designed menu to attract more people of a certain demographic group to a restaurant). My dissertation identifies a family of mining tasks, and proposes a suite of algorithms - exact, approximation with theoretical properties, and efficient heuristics - for solving the problems. We conduct a comprehensive set of experiments on the proposed techniques over both synthetic and real data crawled from the web to validate the effectiveness of our framework.

#index 1988493
#* Effective hashing for large-scale multimedia search
#@ Jingkuan Song
#t 2013
#c 5
#% 227937
#% 342828
#% 479462
#% 479649
#% 479973
#% 762054
#% 810069
#% 814646
#% 898309
#% 1022281
#% 1040539
#% 1215859
#% 1426417
#% 1450831
#% 1495433
#% 1581931
#% 1594627
#% 1649056
#% 1661257
#% 1750268
#% 1770364
#% 1867997
#% 1868030
#% 1878997
#% 1884017
#% 1972748
#! With the rapid development of the Internet and multimedia technologies over the last decade, a huge amount of data has become available, from text corpus, to collections of online images and videos. Cheap storage cost and modern database technologies have made it possible to accumulate large-scale datasets. However, the ever-growing sizes of the datasets make it harder to search useful information from such data. A fundamental computational primitive for dealing with massive multimedia datasets is the similarity search problem. Multimedia similarity search aims to preprocess a database so that given a query object, one can quickly find its similar objects in the database. Searching similar objects from a large dataset in high-dimensional spaces is at the heart of many multimedia applications, such as near-duplicate retrieval, multimedia tagging, recommendation, and so on. Driven by its significance, lots of efforts have been made on this topic. The goal of my research is to design efficient hashing methods for large-scale multimedia search. In this paper, we first present the general framework for multimedia similarity search and discuss the latest improvements and progresses in the field. Then we describe the contributions we have made to effectively and efficiently search similar multimedia objects from large-scale databases. Finally, we discuss the future work and draw a conclusion.

#index 1988494
#* Efficient and scalable monitoring and summarization of large probabilistic data
#@ Mingwang Tang
#t 2013
#c 5
#% 201921
#% 248822
#% 347226
#% 479648
#% 654487
#% 810098
#% 866990
#% 893189
#% 977008
#% 991156
#% 1016178
#% 1016201
#% 1016202
#% 1022259
#% 1030848
#% 1033340
#% 1039695
#% 1063568
#% 1206765
#% 1206892
#% 1229789
#% 1292584
#% 1328153
#% 1426559
#% 1581908
#% 1581976
#% 1615075
#% 1654044
#% 1846706
#! In numerous real applications, uncertainty is inherently introduced when massive data are generated. Modern database management systems aim to incorporate and handle data with uncertainties as a first-class citizen, where uncertain data are represented as probabilistic relations. In my thesis, my work has focused on monitoring and summarization of large probabilistic data. Specifically, we extended the distributed threshold monitoring problem to distributed probabilistic data. Instead, we actually need to monitor the aggregated value (e.g. sum) of distributed probabilistic data against both the score threshold and the probability threshold, which make the techniques designed for deterministic data are not directly applicable. Our algorithms have significantly reduced both the communication and computation costs as shown by an extensive experimental evaluation on large real datasets. On the other hand, building histograms to summarize the distribution of certain feature in a large data set is a fundamental problem in data management. Recent work have extended this studies to probabilistic data, but their methods suffer from the limited scalability. We present novel methods to build scalable histograms over large probabilistic data using distributed and parallel algorithms. Extensive experiments on large real data sets have demonstrated the superb scalability and efficiency achieved by our implementations in MapReduce, when compared to the existing, state-of-the-art centralized methods.

#index 1988495
#* SIGMOD 2013 new researcher symposium
#@ Anish Das Sarma;Xin Luna Dong
#t 2013
#c 5

#index 1989142
#* First International Workshop on Graph Data Management Experiences and Systems
#@ Peter Boncz;Thomas Neumann
#t 2013
#c 5

#index 1989374
#* Proceedings of the ACM SIGMOD Workshop on Databases and Social Networks
#@ Kristen LeFevre;Ashwin Machanavajjhala;Adam Silberstein
#t 2013
#c 5
#! The ACM SIGMOD Workshop on Databases and Social Networks (DBSocial) aims at disseminating results founded on database research and practice that advance the state-of-theart in the observation, management, and analysis of inherently networked data that results primarily from social phenomena. In particular, DBSocial is intended to foster a discussion about the role that the database community should play in the area of social network research. As such, DBSocial welcomes papers whose approaches are fundamentally centered on theoretical foundations and best practices in databases and very closely related areas such as data mining and information retrieval. In its third edition, DBSocial features two invited keynote talks and nine peer-reviewed papers (selected out of nineteen submissions). The keynote talks cover challenges and advances in building the infrastructure necessary for storing, accessing, and analyzing applications that leverage large graphs in the industry. Philip Bohannon will speak about research challenges pertaining to curating, querying and searching over entity graphs at Facebook. Vahab Mirrokni from Google will outline recent advances in graph clustering over parallel infrastructures like MapReduce.

#index 1989375
#* Event identification for local areas using social media streaming data
#@ Andreas Weiler;Marc H. Scholl;Franz Wanner;Christian Rohrdantz
#t 2013
#c 5
#% 94368
#% 279755
#% 731721
#% 1016144
#% 1040837
#% 1202160
#% 1292518
#% 1314738
#% 1355297
#% 1399992
#% 1400018
#% 1573368
#% 1592152
#% 1662943
#% 1796717
#% 1798404
#% 1872363
#% 1879089
#! Unprecedented success and active usage of social media services result in massive amounts of user-generated data. An increasing interest in the contained information from social media data leads to more and more sophisticated analysis and visualization applications. Because of the fast pace and distribution of news in social media data it is an appropriate source to identify events in the data and directly display their occurrence to analysts or other users. This paper presents a method for event identification in local areas using the Twitter data stream. We implement and use a combined log-likelihood ratio approach for the geographic and time dimension of real-life Twitter data in predefined areas of the world to detect events occurring in the message contents. We present a case study with two interesting scenarios to show the usefulness of our approach.

#index 1989376
#* Interesting event detection through hall of fame rankings
#@ Foteini Alvanaki;Evica Ilieva;Sebastian Michel;Aleksandar Stupar
#t 2013
#c 5
#% 279755
#% 411762
#% 452821
#% 785436
#% 813989
#% 816186
#% 1019124
#% 1117006
#% 1206698
#% 1366500
#! Everything is relative. Cars are compared by gas per mile, websites by page rank, students based on GPA, scientists by number of publications, and celebrities by beauty or wealth. In this paper, we study the characteristics of such entity rankings based on a set of rankings obtained from a popular Web portal. The obtained insights are integrated in our approach, coined Pantheon. Pantheon maintains sets of top-k rankings and reports identified changes in a way that appeals to users, using a novel combination of different characteristics like competitiveness, information entropy, and scale of change. Entity rankings are assembled by combining entity type attributes with data-driven categorical constraints and sorting criteria on numeric attributes. We report on the results of an experimental evaluation using real-world data obtained from a basketball statistics website.

#index 1989377
#* Scalable, continuous tracking of tag co-occurrences between short sets using (almost) disjoint tag partitions
#@ Foteini Alvanaki;Sebastian Michel
#t 2013
#c 5
#% 2833
#% 724866
#% 726628
#% 1219786
#% 1798404
#% 1869834
#! In this work we consider the continuous computation of set correlations over a stream of set-valued attributes, such as Tweets and their hashtags, social annotations of blog posts obtained through RSS, or updates to set-valued attributes of databases. In order to compute tag correlations in a distributed fashion, all necessary information has to be present at the computing node(s). Our approach makes use of a partitioning scheme based on set covers for efficient and replication-lean information flow. We report on the results of a preliminary performance evaluation using Tweets obtained through Twitter's streaming API.

#index 1989378
#* curso: protect yourself from curse of attribute inference: a social network privacy-analyzer
#@ Eunsu Ryu;Yao Rong;Jie Li;Ashwin Machanavajjhala
#t 2013
#c 5
#% 722904
#% 840722
#% 956511
#% 983903
#% 1190108
#% 1190207
#% 1260273
#% 1400119
#% 1449326
#% 1678449
#! While social networking platforms allow users to control how their private information is shared, recent research has shown that a user's sensitive attribute can be inferred based on friendship links and group memberships, even when the attribute value is not shared with anyone else. Thus, existing access control mechanisms are unable to protect against such privacy breaches. Our research goal is to develop tools that help a user Alice be aware of privacy breaches via attribute inference. In this paper, we specifically focus on two problems: (a) whether Alice's sensitive attribute can be inferred based on public information in Alice's neighborhood, and (b) whether making Alice's sensitive attribute public leads to the disclosure of sensitive information of another user Bob in Alice's neighborhood. We propose three algorithms to detect the aforementioned privacy breaches. We limit our scope to the one-hop neighbors of Alice -- information that is visible to an app that can be executed on behalf of Alice. Our results indicate that analyzing local networks is sufficient to extract a significant amount of information about most users.

#index 1989379
#* STK-anonymity: k-anonymity of social networks containing both structural and textual information
#@ Yifan Hao;Huiping Cao;Kabi Bhattarai;Satyajayant Misra
#t 2013
#c 5
#% 800515
#% 801690
#% 1063476
#% 1127360
#% 1206763
#% 1259854
#% 1328173
#% 1328188
#% 1372691
#% 1426540
#% 1523977
#% 1524388
#% 1581951
#% 1587704
#% 1618521
#% 1798384
#% 1882108
#! We study the problem of anonymizing social networks to prevent individual identifications which use both structural (node degrees) and textual (edge labels) information in social networks. We introduce the concept of Structural and Textual (ST)-equivalence of individuals at two levels (strict and loose), and formally define the problem as Structure and Text aware K-anonymity of social networks (STK-Anonymity). In an STK-anonymized network, each individual is ST-equivalent to at least K-1 other nodes. The major challenge in achieving STK-Anonymity comes from the correlation of edge labels, which causes the propagation of edge anonymization. To address the challenge, we present a two-phase approach. In particular, a set-enumeration tree based approach and three pruning strategies are introduced in the second phase to avoid the propagation problem during anonymization. Experimental results on both real and synthetic datasets are presented to show the effectiveness and efficiency of our approaches.

#index 1989380
#* How people describe themselves on Twitter
#@ Konstantinos Semertzidis;Evaggelia Pitoura;Panayiotis Tsaparas
#t 2013
#c 5
#% 955712
#% 1181273
#% 1192930
#% 1399992
#% 1426611
#% 1693876
#% 1711595
#% 1930671
#% 1984704
#! Twitter, being both a micro-blogging service and a social network, has become one of the primary means of communicating and disseminating information online. As such, significant amount of research has been devoted to analyzing the Twitter graph, the tweets, and the behavior of its users. In this work, we undertake a study of the user profile bios on Twitter. The goal of our study is two-fold: first, to understand what Twitter users choose to expose about themselves in their profile bio, and second, to investigate if it is possible to exploit the information in the user bio for tasks such as predicting connections between Twitter users.

#index 1989381
#* Cache augmented database management systems
#@ Shahram Ghandeharizadeh;Jason Yap
#t 2013
#c 5
#% 287230
#% 340175
#% 340176
#% 442700
#% 654504
#% 745536
#% 1015314
#% 1063543
#% 1127394
#% 1526992
#% 1551289
#% 1573340
#% 1747340
#% 1770414
#% 1842360
#% 1985115
#! Cache Augmented Database Management Systems, CADBMSs, enhance the velocity of simple operations that read and write a small amount of data from big data. They are most suitable for those applications with workloads that exhibit a high read to write ratio, e.g., interactive social networking actions. This study surveys state of the art with CADBMSs and presents physical data independence as the next step in their evolution. We detail the requirements of this evolution, technological trends and software practices, and our research efforts in this area.

#index 1989382
#* Implementing link-prediction for social networks in a database system
#@ Sara Cohen;Netanel Cohen-Tzemach
#t 2013
#c 5
#% 730089
#% 1206916
#% 1586957
#% 1642027
#! Storing and querying large social networks is a challenging problem, due both to the scale of the data, and to intricate querying requirements. One common type of query over a social network is link prediction, which is used to suggest new friends for existing nodes in the network. There is no gold standard metric for predicting new links. However, past work has been effective at identifying a number of metrics that work well for this problem. These metrics vastly differ one from another in their computational complexity, e.g., they may consider a small neighborhood of a node for which new links should be predicted, or they may perform random walks over the entire social network graph. This paper considers the problem of implementing metrics for link prediction in a social network over different types of database systems. We consider the use of a relational database, a key-value store and a graph database. We show the type of database system affects the ease in which link prediction may be performed. Our results are empirically validated by extensive experimentation over real social networks of varying sizes.

#index 1989383
#* The predictive value of young and old links in a social network
#@ Hung-Hsuan Chen;David J. Miller;C. Lee Giles
#t 2013
#c 5
#% 209021
#% 577273
#% 729923
#% 823342
#% 955712
#% 989613
#% 1581923
#% 1796601
#% 1806122
#% 1842362
#% 1872418
#% 1872508
#! Recent studies show that vertex similarity measures are good at predicting link formation over the near term, but are less effective in predicting over the long term. This indicates that, generally, as links age, their degree of influence diminishes. However, few papers have systematically studied this phenomenon. In this paper, we apply a supervised learning approach to study age as a factor for link formation. Experiments on several real-world datasets show that younger links are more informative than older ones in predicting the formation of new links. Since older links become less useful, it might be appropriate to remove them when studying network evolution. Several previously observed network properties and network evolution phenomena, such as "the number of edges grows super-linearly in the number of nodes" and "the diameter is decreasing as the network grows", may need to be reconsidered under a dynamic network model where old, inactive links are removed.

#index 1989384
#* Proceedings of the Fifth Workshop on Semantic Web Information Management
#@ Roberto De Virgilio;Fausto Giunchiglia;Letizia Tanca
#t 2013
#c 5
#! The ceaseless expansion of the World Wide Web is making more and more complex for humans to efficiently find the needed information. The underlying idea of having a description of the data on the Web, organized in such a way as to be used by machines for automation, integration and reuse across various applications, has been exploited in several research fields. As in the previous editions, the International Workshop on "Semantic Web Information Management" (SWIM) aims at reviewing the most recent data-centered solutions for the Semantic Web. In particular, its ambition is to present and analyze the techniques for semantic information management, by taking advantage of the synergisms between the logical basis of the semantic web and the logical foundations of conceptual modeling. Indeed, the leitmotif of these researches is the proposal of models and methods conceived to represent and manage the so-called "semantic data", that is, data appropriately structured to be easily machine-processable on the Web, according to semantic models (e.g. RDF, RDF(S), OWL). The long-standing experience of the information modeling community can provide a priceless contribution to the substantial problems arising in semantic data management. The research issues can be summarized by the following problems: (1) How can we store efficiently and effectively large amounts of semantic data? (2) How can we query semantic data and reason on them in a feasible way? (3) How can we exploit such semantic data in real world scenarios? This workshop covers the emerging area of Semantic Web gathering researchers to debate, propose, and elaborate the foundations for a data-modeling approach to these problems, by presenting running research and projects on these topics.

#index 1989869
#* Proceedings of the Ninth International Workshop on Data Management on New Hardware
#@ Ryan Johnson;Alfons Kemper
#t 2013
#c 5
#! The aim of this one-day workshop is to bring together researchers who are interested in optimizing database performance on modern computing infrastructure by designing new data management techniques and tools.

#index 1990568
#* Big data in capital markets
#@ Alex Nazaruk;Michael Rauchman
#t 2013
#c 5
#! Over the past decade global securities markets have dramatically changed. Evolution of market structure in combination with advances in computer technologies led to emergence of electronic securities trading. Securities transactions that used to be conducted in person and over the phone are now predominantly executed by automated trading systems. This resulted in significant fragmentation of the markets, vast increase in the exchange volumes and even greater increase in the number of orders. In this talk we present and analyze forces behind the wide proliferation of electronic securities trading in US stocks and options markets. We also make a high-level introduction into electronic securities market structure. We discuss trading objectives of different classes of market participants and analyze how their activity affects data volumes. We also present typical securities trading firm data flow and analyze various types of data it uses in its trading operations. We close with the implications this "sea change" has on DBMS requirements in capital markets.

#index 1990569
#* Managing database technology at enterprise scale
#@ Paul Yaron
#t 2013
#c 5
#! Paul Yaron is responsible for Non-Mainframe, Relational Database Architecture, Engineering and Strategy for JPMC globally. JP Morgan is a leading financial services firm with assets over $2 trillion, operates 40 major datacenters around the globe, servicing over 60 countries with over 250,000 employees. It partners with 170 regulators and manages 230 Petabytes of data, JPMC depends on over 23,000 database instances to service multiple business units. With a deployment of such scope, JPMC leverages solutions from most major database, security and operating system vendors. This talk will discuss the challenges and strategies of managing the evolving ecosystem of "all data", from information security, to internal virtualization strategies. Engineering reliable globally scalable and compliant data management solutions demands a model for proactively measuring the risk complexity of an ecosystem for expert focus and potential proactive remediation. The research for quantitative measurement of database (or other) ecosystem entropy appears sparse. JPMC is looking to share its ideas in this space with the academic community as the need for such quantitative measures are increasingly important as ecosystems move from islands of single tenant risk into multi-tenant risk clusters.

#index 1990570
#* Proceedings of the 12th International ACM Workshop on Data Engineering for Wireless and Mobile Acess
#@ Mario Nascimento;Mohamed Sharaf;Feifei Li;Kyriakos Mouratidis
#t 2013
#c 5
#! It is our great pleasure to welcome you all to the Twelfth ACM International Workshop on Data Engineering for Wireless and Mobile Access (MobiDE 2013), held in conjunction with SIGMOD/PODS 2013. MobiDE continues its tradition of bringing together researchers and practitioners in databases, mobile computing, and networking, and providing a day of exciting presentations and discussions. As in previous years, the workshop is a forum to present latest research and engineering results, and set directions in wireless and mobile data management.

#index 1991172
#* Proceedings of the Second Workshop on Data Analytics in the Cloud
#@ Kostas Tzoumas;Shivnath Babu
#t 2013
#c 5
#! Data analytics has the potential to be a transformer of scientific research, and data-driven business decisions. By effectively analyzing huge volumes of data, scientific research can be transformed from hypothesis-driven to data-driven, where forming scientific hypotheses will be aided by discovering patterns in vast quantities of data. For most technology companies that operate on a Web scale, analyzing customer data can provide insights on customer behavior, and lead to answers for critical business decisions. Cloud computing has emerged as a cost-effective and elastic computing paradigm. Cloud infrastructures scale to massive numbers of commodity computing nodes and provide adaptive provisioning without prohibitive initial investments. Data analytics has the potential to be a significant cloud application, and to constitute a large fraction of the workload of modern data centers. Designing the infrastructures and systems for data management in the new computing environments remains an open challenge.

#index 1993056
#* Proceedings of the Workshop on Dynamic Networks Management and Mining
#@ 
#t 2013
#c 5

#index 1996615
#* The ACM PODS Alberto O. Mendelzon test-of-time award 2013
#@ Michael Benedikt;Tova Milo;Dirk Van Gucht
#t 2013
#c 5

#index 2001489
#* Proceedings of the 2nd ACM SIGMOD Workshop on Scalable Workflow Execution Engines and Technologies
#@ Jan Hidders;Paolo Missier;Jacek Sroka
#t 2013
#c 5
#! The ACM SIGMOD Workshop on Scalable Workflow Execution Engines and Technologies (SWEET) aims to bring together researchers and practitioners to explore the state of the art in workflow-based programming for data-intensive applications and the potential of cloud-based computing in this area.

#index 2001490
#* DynamicCloudSim: simulating heterogeneity in computational clouds
#@ Marc Bux;Ulf Leser
#t 2013
#c 5
#% 348459
#% 398097
#% 832825
#% 869394
#% 879809
#% 897426
#% 954295
#% 983705
#% 1017294
#% 1034470
#% 1069198
#% 1080468
#% 1092796
#% 1106885
#% 1157491
#% 1301033
#% 1315246
#% 1464926
#% 1468423
#% 1523836
#% 1528466
#% 1532896
#% 1549900
#% 1596182
#% 1596190
#% 1615508
#% 1691601
#% 1827709
#% 1880211
#% 1976714
#! Simulation has become a commonly employed first step in evaluating novel approaches towards resource allocation and task scheduling on distributed architectures. However, existing simulators fall short in their modeling of the instability common to shared computational infrastructure, such as public clouds. In this work, we present DynamicCloudSim which extends the popular simulation toolkit CloudSim with several factors of instability, including inhomogeneity and dynamic changes of performance at runtime as well as failures during task execution. As a use case and validation of the introduced functionality, we simulate the impact of instability on scientific workflow scheduling by assessing and comparing the performance of four schedulers in the course of several experiments. Results indicate that our model seems to adequately capture the most important aspects of cloud performance instability, though a validation on real hardware is still pending. The source code of DynamicCloudSim and the examined schedulers is available at https://code.google.com/p/dynamiccloudsim/.

#index 2001491
#* A continuous workflow scheduling framework
#@ Panayiotis Neophytou;Panos K. Chrysanthis;Alexandros Labrinidis
#t 2013
#c 5
#% 383590
#% 428155
#% 578391
#% 654510
#% 664071
#% 771257
#% 779298
#% 832825
#% 845650
#% 879809
#% 954295
#% 993949
#% 1016169
#% 1026964
#% 1035310
#% 1291844
#% 1581979
#% 1587719
#% 1601206
#% 1770401
#! Traditional workflow management or enactment systems (WfMS) and workflow design processes view the workflow as a one-time interaction with the various data sources, i.e., when a workflow is invoked, its steps are executed once and in-order. The fundamental underlying assumption has been that data sources are passive and all interactions are structured along the request/reply (query) model. Hence, traditional WfMS cannot effectively support business or scientific monitoring applications that require the processing of data streams such as those generated nowadays by sensing devices as well as mobile and web applications. Our hypothesis is that WfMS, both in the scientific and business domains, can be extended to support data stream semantics to enable monitoring applications. This includes the ability to apply flexible bounds on unbounded data streams and the ability to facilitate on-the-fly processing of bounded bundles of data (window semantics). In our previous work we have developed and implemented a Continuous Workflow Model that supports our hypothesis. This implementation of a CONtinuous workFLow ExeCution Engine (CONFLuEnCE) led to the realization that different applications have different performance requirements and hence an integrated workflow scheduling framework is essential. Such a framework is the main contribution of this paper. In particular, we designed and implemented STAFiLOS, a STreAm FLOw Scheduling for Continuous Workflows framework within CONFLuEnCE and evaluated STAFiLOS based on the Linear Road Benchmark.

#index 2001492
#* OSIRIS-SR: a scalable yet reliable distributed workflow execution engine
#@ Nenad Stojnić;Heiko Schuldt
#t 2013
#c 5
#% 340175
#% 821921
#% 866984
#% 900816
#% 953831
#% 998845
#% 1127560
#% 1177805
#% 1315138
#% 1400975
#% 1408701
#% 1459206
#% 1538766
#% 1563830
#% 1567929
#% 1625032
#% 1911326
#% 2009591
#! Workflows provide an easy to use programming model for the construction of complex services that are (recursively) composed of simpler services. When it comes to high performance workflow execution, the distribution (outscaling) of the constituent services of the workflow across an environment of computational nodes is a key concept and also a very straightforward advantage of the workflow paradigm. However, scalable workflow execution cannot only be provided by the distribution of services but also necessitates novel architectures for the workflow engine in charge of service orchestration. Even though workflow orchestration is commonly provided by centralized solutions, these architectures imply performance bottlenecks and single points of failure. Hence, the workflow engine has to be distributed as well, by efficiently replicating workflow metadata across several nodes in a network. A particular challenge stems from the requirement of providing scalable workflow execution that is at the same time also reliable. In this paper, we present OSIRIS-SR, a decentralized middleware for the distributed execution of workflows. It has particularly been designed to jointly provide a high degree of scalability and reliability. OSIRIS-SR locally leverages the concurrent and redundant Actor model for workflow processing, whereas globally OSIRIS-SR runs a number of scalable system services for the management of workflow metadata, with the Safety Ring being the most prominent one. The Safety Ring service features a self-healing node overlay for the purpose of active workflow instance supervision that serves at the same time as a scalable and reliable metadata storage. We discuss in detail the Safety Ring architecture and the mechanics behind the scalable and reliable workflow management in OSIRIS-SR. The evaluation results of OSIRIS-SR show that support for reliable workflow execution does not significantly impact the system's scalability characteristics.

#index 2001493
#* User-steering of HPC workflows: state-of-the-art and future directions
#@ Marta Mattoso;Kary Ocaña;Felipe Horta;Jonas Dias;Eduardo Ogasawara;Vitor Silva;Daniel de Oliveira;Flavio Costa;Igor Araújo
#t 2013
#c 5
#% 261139
#% 864639
#% 875046
#% 914570
#% 983709
#% 1034470
#% 1060033
#% 1069195
#% 1081399
#% 1164233
#% 1174009
#% 1292897
#% 1375919
#% 1423927
#% 1459379
#% 1486258
#% 1581933
#% 1616352
#% 1681405
#% 1681406
#% 1681413
#% 1686975
#% 1828537
#% 1909611
#% 1910047
#% 1926186
#% 1967009
#% 1976715
#% 1982779
#! In 2006 a group of leading researchers was gathered to discuss several challenges to scientific workflow supporting technologies and many of which still remain open challenges, such as the steering of workflows by users. Due to big data and long lasting workflows, many users demand steering features such as real-time monitoring, analysis and specially execution interference. The workflow execution should respond dynamically to such interference in the execution, to support the experimentation process in high performance computing. This paper revisits the issues in the user steering and dynamic workflows, presenting the state-of-the-art in it, and the open challenges. Our goal is to discuss research issues related to scientists' steering and present some ideas on how these demands may be supported in current scientific workflow technologies.

#index 2004556
#* Towards efficient indexing of arbitrary similarity: vision paper
#@ Tomáš Bartoš;Tomáš Skopal;Juraj Moško
#t 2013
#c 5
#% 124074
#% 342827
#% 465702
#% 818263
#% 818938
#% 857113
#% 963669
#% 997496
#% 1154026
#% 1434103
#% 1573313
#% 1586177
#% 1622362
#% 1641914
#% 1896050
#! The popularity of similarity search expanded with the increased interest in multimedia databases, bioinformatics, or social networks, and with the growing number of users trying to find information in huge collections of unstructured data. During the exploration, the users handle database objects in different ways based on the utilized similarity models, ranging from simple to complex models. Efficient indexing techniques for similarity search are required especially for growing databases. In this paper, we study implementation possibilities of the recently announced theoretical framework SIMDEX, the task of which is to algorithmically explore a given similarity space and find possibilities for efficient indexing. Instead of a fixed set of indexing properties, such as metric space axioms, SIMDEX aims to seek for alternative properties that are valid in a particular similarity model (database) and, at the same time, provide efficient indexing. In particular, we propose to implement the fundamental parts of SIMDEX by means of the genetic programming (GP) which we expect will provide highquality resulting set of expressions (axioms) useful for indexing.

#index 2004557
#* Research endogamy as an indicator of conference quality
#@ Sergio Lopez Montolio;David Dominguez-Sal;Josep Lluis Larriba-Pey
#t 2013
#c 5
#% 810821
#% 845353
#% 967276
#% 967277
#% 1040933
#% 1069293
#% 1213431
#% 1694253
#! Endogamy in scientific publications is a measure of the degree of collaboration between researchers. In this paper, we analyze the endogamy of a large set of computer science conferences and journals. We observe a strong correlation between the quality of those conferences and the endogamy of their authors: conferences where researchers collaborate with new peers have significantly more quality than conferences where researchers work in groups that are stable along time.

#index 2004558
#* Information diffusion in online social networks: a survey
#@ Adrien Guille;Hakim Hacid;Cecile Favre;Djamel A. Zighed
#t 2013
#c 5
#% 46803
#% 324817
#% 406493
#% 416810
#% 577220
#% 722904
#% 729923
#% 735078
#% 1083624
#% 1176853
#% 1214671
#% 1259854
#% 1355042
#% 1432574
#% 1451242
#% 1475157
#% 1535333
#% 1536507
#% 1536508
#% 1556479
#% 1560424
#% 1613641
#% 1617342
#% 1688545
#% 1746850
#% 1747165
#% 1872232
#% 1886582
#% 1948125
#% 1971507
#% 1978756
#! Online social networks play a major role in the spread of information at very large scale. A lot of effort have been made in order to understand this phenomenon, ranging from popular topic detection to information diffusion modeling, including influential spreaders identification. In this article, we present a survey of representative methods dealing with these issues and propose a taxonomy that summarizes the state-of-the-art. The objective is to provide a comprehensive analysis and guide of existing efforts around information diffusion in social networks. This survey is intended to help researchers in quickly understanding existing works and possible improvements to bring.

#index 2004559
#* Discovering semantic relations from the web and organizing them with PATTY
#@ Ndapandula Nakashole;Gerhard Weikum;Fabian Suchanek
#t 2013
#c 5
#% 956564
#% 1063570
#% 1355026
#% 1409954
#% 1523913
#% 1592311
#% 1654055
#% 1711857
#% 1711858
#% 1711865
#% 1770359
#% 1895094
#% 1913604
#% 1913673
#! PATTY is a system for automatically distilling relational patterns from the Web, for example, the pattern "X covered Y" between a singer and someone else's song. We have extracted a large collection of such patterns and organized them in a taxonomic manner, similar in style to the WordNet thesaurus but capturing relations (binary predicates) instead of concepts and classes (unary predicates). The patterns are organized by semantic types and synonyms, and they form a hierarchy based on subsumptions. For example, "X covered Y" is subsumed by "X sang Y", which in turn is subsumed by "X performed Y" (where X can be any musician, not just a singer). In this paper we give an overview of the PATTY system and the resulting collections of relational patterns. We discuss the four main components of PATTY's architecture and a variety of use cases, including the paraphrasing of relations, and semantic search over subjectpredicate- object triples. This kind of search can handle entities, relations, semantic types, noun phrases, and relational phrases.

#index 2004560
#* Jeff Vitter speaks out on being a southerner, duties of a dean, and more
#@ Marianne Winslett;Vanessa Braganholo
#t 2013
#c 5

#index 2004561
#* Database research at the National University of Singapore
#@ Stephane Bressan;Chee Yong Chan;Wynne Hsu;Mong-Li Lee;Tok-Wang Ling;Beng Chin Ooi;Kian-Lee Tan;Anthony K.H. Tung
#t 2013
#c 5
#% 824706
#% 1063499
#% 1063503
#% 1217187
#% 1422710
#% 1426503
#% 1426504
#% 1426551
#% 1484485
#% 1494944
#% 1523837
#% 1523840
#% 1523902
#% 1581926
#% 1594632
#% 1602033
#% 1621136
#% 1697230
#% 1846707
#% 1846761
#% 1846769
#% 1846824
#% 1869835
#% 1869836
#% 1869838
#% 1918373
#% 1918428
#% 1959788
#% 1962344
#% 1971503
#% 1972779
#% 1989511
#% 1999316

#index 2004562
#* What does an associate editor actually do?
#@ Graham Cormode
#t 2013
#c 5
#% 164792
#% 825674
#% 1046316
#% 1183381
#! What does a Associate Editor (AE) of a journal actually do? The answer may be far from obvious. This article describes the steps that one AE follows in handling a submission. The aim is to shed light on the process, for the benefit of authors, reviewers, and other AEs.

#index 2004563
#* Report on the first workshop on innovative querying of streams
#@ Michael Benedikt;Dan Olteanu
#t 2013
#c 5
#% 1044505
#% 1063523
#% 1291122
#% 1426444
#% 1523815
#% 1581969
#% 1668637
#% 1710797
#% 1770144
#% 1770341
#% 1798404
#% 1846782
#% 1869834
#% 1880478
#% 1905346
#% 1972756

#index 2004564
#* The relational model is dead, SQL is dead, and I don't feel so good myself
#@ Paolo Atzeni;Christian S. Jensen;Giorgio Orsi;Sudha Ram;Letizia Tanca;Riccardo Torlone
#t 2013
#c 5
#% 1022298
#% 1551289
#% 1573168
#% 1573340
#% 1891766
#% 1962313
#! We report the opinions expressed by well-known database researchers on the future of the relational model and SQL during a panel at the International Workshop on Non-Conventional Data Access (NoCoDa 2012), held in Florence, Italy in October 2012 in conjunction with the 31st International Conference on Conceptual Modeling. The panelists include: Paolo Atzeni (Università Roma Tre, Italy), Umeshwar Dayal (HP Labs, USA), Christian S. Jensen (Aarhus University, Denmark), and Sudha Ram (University of Arizona, USA). Quotations from movies are used as a playful though effective way to convey the dramatic changes that database technology and research are currently undergoing.

#index 2030740
#* Skyline queries, front and back
#@ Jan Chomicki;Paolo Ciaccia;Niccolo' Meneghetti
#t 2013
#c 5
#% 62323
#% 278287
#% 278831
#% 288976
#% 289148
#% 465167
#% 643566
#% 654480
#% 731407
#% 806212
#% 823654
#% 824670
#% 849816
#% 857276
#% 893150
#% 903013
#% 912241
#% 943612
#% 953599
#% 993957
#% 1022203
#% 1022224
#% 1022225
#% 1022226
#% 1083667
#% 1092017
#% 1181301
#% 1206819
#% 1229788
#% 1259554
#% 1272396
#% 1372732
#% 1537152
#% 1565406
#% 1581826
#% 1581852
#% 1594608
#% 1594651
#% 1602704
#% 1680123
#% 1747249
#% 1818426
#% 1853527
#% 1919884
#% 1984699
#% 2000877
#! Skyline queries are a popular way to obtain preferred answers from the database by providing only the orderings of attribute values. The result of a skyline query consists of those input tuples for which there is no input tuple having better or equal values in all the attributes and a better value in at least one attribute. In this article, we summarize the basic notions and properties of skyline queries, and discuss their extensions and generalizations. In particular, we consider skyline algorithms and skyline cardinality issues.

#index 2030741
#* Towards mega-modeling: a walk through data analysis experiences
#@ Stefano Ceri;Themis Palpanas;Emanuele Della Valle;Dino Pedreschi;Johann-Christoph Freytag;Roberto Trasarti
#t 2013
#c 5
#% 123997
#% 216508
#% 893104
#% 989604
#% 1400111
#% 1606061
#% 1621337
#% 1920529
#% 1932719
#% 1943521

#index 2030742
#* Web table taxonomy and formalization
#@ Larissa R. Lautert;Marcelo M. Scheidt;Carina F. Dorneles
#t 2013
#c 5
#% 322880
#% 348147
#% 384978
#% 544484
#% 819715
#% 956500
#% 1127393
#% 1165336
#% 1183375
#% 1372718
#% 1409523
#% 1523913
#% 1536559
#% 1592311
#% 1645361
#! The Web is the largest repository of data available, with over 150 million high-quality tables. Several works have combined efforts to allow queries on these tables, but there are still challenges, like the various different types of structures found on the Web. In this paper, we propose a taxonomy for the tabular structures and formalize the ones used with relational data and show, through an experimental evaluation, that WTClassifier, our supervised framework, classifies Web tables with high accuracy. Additionally, we use WTClassifier to categorize more than 300 thousandWeb tables into our taxonomy and found that 82.25% are not formatted similarly to relational structure.

#index 2030743
#* XQuery 3.0 is nearing completion
#@ Andrew Eisenberg
#t 2013
#c 5
#% 845361

#index 2030744
#* PAIRSE: a privacy-preserving service-oriented data integration system
#@ Djamal Benslimane;Mahmoud Barhamgi;Frederic Cuppens;Franck Morvan;Bruno Defude;Ebrahim Nageba
#t 2013
#c 5
#% 572311
#% 765432
#% 765448
#% 1016138
#% 1486152
#% 1512788
#% 1622184
#% 1640737
#% 1743948
#% 1895108
#% 1962387
#% 2021050
#! Privacy is among the key challenges to data integration in many sectors, including healthcare, e-government, etc. The PAIRSE project aims at providing a flexible, looselycoupled and privacy-preserving data integration system in P2P environments. The project exploits recent Web standards and technologies such asWeb services and ontologies to export data from autonomous data providers as reusable services, and proposes the use of service composition as a viable solution to answer data integration needs on the fly. The project proposed new composition algorithms and service/composition execution models that preserve privacy of data manipulated by services and compositions. The proposed integration system was demonstrated at EDBT 2013 and VLDB 2011.

#index 2030745
#* Medical data management in the SYSEO project
#@ Yahia Chabane;Laurent d'Orazio;Le Gruenwald;Baraa Mohamad;Christophe Rey
#t 2013
#c 5
#% 116043
#% 723279
#% 728007
#% 737279
#% 801412
#% 935898
#% 1054227
#% 1063553
#% 1328095
#% 1369577
#% 1549515
#% 1865160
#% 1895056
#! The SYSEO project aims at producing a software solution suitable for endoscopic imaging in order to enable physicians to manage, manipulate and share medical images. This paper presents our two main components for data management in this system: (1) a novel hybrid rowcolumn database for medical data storage within the cloud and (2) a system for semantic image annotation and retrieval relying on an ontology for polyps.

#index 2030746
#* Andreas Reuter speaks out on transactions, reinventing things, creating a university, and more
#@ Marianne Winslett;Vanessa Braganholo
#t 2013
#c 5

#index 2030747
#* Data centric research at the University of Queensland
#@ Xiaofang Zhou;Shazia Sadiq
#t 2013
#c 5
#% 67245
#% 308139
#% 308144
#% 320330
#% 320504
#% 421050
#% 527170
#% 745459
#% 810069
#% 819790
#% 864466
#% 882477
#% 946436
#% 957151
#% 1127436
#% 1174740
#% 1194683
#% 1206625
#% 1426507
#% 1426523
#% 1426560
#% 1491653
#% 1581931
#% 1603413
#% 1686945
#% 1697257
#% 1750621
#% 1846708
#% 1852789
#% 1867487
#% 1912324
#% 1912840
#% 1915650
#% 1942887
#% 1969420
#% 1972748
#% 1972775
#% 1989503
#% 2002315
#% 2002861
#% 2003211
#% 2010390

#index 2030748
#* Report on the 6th international workshop on business intelligence for the real time enterprise
#@ Malu Castellanos;Umeshwar Dayal;Elke Rundensteiner
#t 2013
#c 5

#index 2030749
#* Databases, information retrieval and knowledge management: exploring paths and crossing bridges
#@ Mouna Kacimi;Fabian M. Suchanek;Aparna Varde
#t 2013
#c 5
#% 1077041
#% 1301009
#% 1542529
#! The International Conference on Information Retrieval and Knowledge Management (CIKM) brings together three avenues of data-oriented research, namely, Database Management, Information Retrieval and Knowledge Management. The confluence of these avenues becomes evident also in the PhD theses of doctoral students: Stream processing makes use of knowledge representation techniques, linked data is emerging as a research topic that bridges information retrieval and knowledge representation, and new forms of querying draw on techniques from both information retrieval and databases. In this paper, we survey new PhD theses at the meeting point of the three research avenues. Our survey is based on the 5th PhD workshop at the ACM CIKM conference. The topics include themes as diverse as link prediction, source code querying, and video stream processing.

#index 2073863
#* Skew strikes back: new developments in the theory of join algorithms
#@ Hung Q Ngo;Christopher Ré;Atri Rudra
#t 2014
#c 5
#% 583
#% 136740
#% 159244
#% 237180
#% 248014
#% 287316
#% 287339
#% 289425
#% 303886
#% 321058
#% 393844
#% 411554
#% 480761
#% 480966
#% 598376
#% 599549
#% 643572
#% 825672
#% 847068
#% 993437
#% 1063548
#% 1176970
#% 1217119
#% 1224352
#% 1328057
#% 1357840
#% 1560415
#% 1581849
#% 1770120
#% 1776167

#index 2073864
#* Analyzing analytics
#@ Rajesh Bordawekar;Bob Blainey;Chidanand Apte
#t 2014
#c 5
#% 754440
#% 770163
#% 818916
#% 907530
#% 1023380
#% 1136641
#% 1189780
#% 1214729
#% 1278124
#% 1631439
#% 1693954
#% 1693957
#! Many organizations today are faced with the challenge of processing and distilling information from huge and growing collections of data. Such organizations are increasingly deploying sophisticated mathematical algorithms to model the behavior of their business processes to discover correlations in the data, to predict trends and ultimately drive decisions to optimize their operations. These techniques, are known collectively as analytics, and draw upon multiple disciplines, including statistics, quantitative analysis, data mining, and machine learning. In this survey paper, we identify some of the key techniques employed in analytics both to serve as an introduction for the non-specialist and to explore the opportunity for greater optimizations for parallelization and acceleration using commodity and specialized multi-core processors. We are interested in isolating and documenting repeated patterns in analytical algorithms, data structures and data types, and in understanding howthese could be most effectively mapped onto parallel infrastructure. To this end, we focus on analytical models that can be executed using different algorithms. For most major model types, we study implementations of key algorithms to determine common computational and runtime patterns. We then use this information to characterize and recommend suitable parallelization strategies for these algorithms, specifically when used in data management workloads.

#index 2073865
#* A survey on tree edit distance lower bound estimation techniques for similarity join on XML data
#@ Fei Li;Hongzhi Wang;Jianzhong Li;Hong Gao
#t 2014
#c 5
#% 66654
#% 289193
#% 349489
#% 375017
#% 397373
#% 547947
#% 768815
#% 806218
#% 810071
#% 824676
#% 826007
#% 944026
#% 1206601
#% 1312537
#% 1495308
#% 1734146
#% 1916534

#index 2073866
#* Data profiling revisited
#@ Felix Naumann
#t 2014
#c 5
#% 54047
#% 89751
#% 210190
#% 264263
#% 269634
#% 427873
#% 458869
#% 480499
#% 488766
#% 765455
#% 893145
#% 924747
#% 1022222
#% 1035692
#% 1054480
#% 1063546
#% 1127443
#% 1165134
#% 1166724
#% 1209082
#% 1467748
#% 1503758
#% 1567974
#% 1573139
#% 1614580
#% 1642098
#% 1800591
#% 1876163
#% 1919879
#% 1920014
#! Data profiling comprises a broad range of methods to efficiently analyze a given data set. In a typical scenario, which mirrors the capabilities of commercial data profiling tools, tables of a relational database are scanned to derive metadata, such as data types and value patterns, completeness and uniqueness of columns, keys and foreign keys, and occasionally functional dependencies and association rules. Individual research projects have proposed several additional profiling tasks, such as the discovery of inclusion dependencies or conditional functional dependencies. Data profiling deserves a fresh look for two reasons: First, the area itself is neither established nor defined in any principled way, despite significant research activity on individual parts in the past. Second, more and more data beyond the traditional relational databases are being created and beg to be profiled. The article proposes new research directions and challenges, including interactive and incremental profiling and profiling heterogeneous and non-relational data.

#index 2073867
#* Anand Rajaraman speaks out on startups and social data
#@ Marianne Winslett;Vanessa Braganhol
#t 2014
#c 5

#index 2073868
#* Data management research at the technical university of crete
#@ Stavros Christodoulakis;Minos Garofalakis;Euripides G.M. Petrakis;Antonios Deligiannakis;Vasilis Samoladas;Ekaterini Ioannou;Odysseas Papapetrou;Stelios Sotiriadis
#t 2014
#c 5
#% 997490
#% 1523889
#% 1581889
#% 1770341
#% 1798440
#% 1846705
#% 1848066
#% 1869834
#% 1876163
#% 1972755
#% 1999227
#% 2030474
#% 2030890

#index 2073869
#* Report on the first international workshop on cloud intelligence (Cloud-I 2012)
#@ Jérôme Darmont;Torben Bach Pedersen
#t 2014
#c 5

#index 2073870
#* Report on the second international workshop on energy data management (EnDM 2013)
#@ Torben Bach Pedersen;Wolfgang Lehner
#t 2014
#c 5

#index 2073871
#* Report from the second workshop on scalable workflow enactment engines and technology (SWEET'13)
#@ Jacek Sroka;Jan Hidders;Paolo Missier
#t 2014
#c 5
#% 869394
#% 879809
#% 1016169
#% 1034470
#% 1459379
#% 1468423
#% 1523836
#% 1528466
#% 1532896
#% 1967009
#! This report summarizes the Second International Workshop on ScalableWorkflow Enactment Engines and Technologies (SWEET'13). This workshop was held in conjunction with the 2013 SIGMOD conference in New York, NY, USA on June 23th, 2013. The goal of the workshop was to bring together researchers and practitioners to explore the state of the art in workflow-based programming for data-intensive applications, and the potential of cloud-based computing in this area. The program featured 4 paper presentations and two very well attended invited talks by Prof. Paul Watson, Newcastle University, UK and Dr Jelena Pjesivac-Grbovic from Google, Inc.

#index 2073986
#* Proceedings of the 17th International Conference on Management of Data
#@ Vaishali Sadaphal;Jayant R Haritsa;Umeshwar Dayal;Prasad M Deshpande
#t 2011
#c 5
#! For close to two decades, the International Conference on Management of Data (COMAD), modeled along the lines of ACM SIGMOD, has been the premier international database conference hosted in India. The first COMAD was held in Hyderabad in 1989, and the most recent version was hosted in Nagpur during December 2010. The 17th edition in the COMAD series will be held at the International Institute of Information Technology, Bangalore (IIITB), during December 19-21, 2011. Bangalore, also called Bengaluru, is nicknamed the Garden City, and was once called a pensioner's paradise. Today, it is better known as the primary hub for India's Information Technology sector. COMAD seeks to provide the community of researchers, practitioners, developers and users of data management technologies, a forum to present and discuss problems, solutions, innovations, experiences and emerging trends. During the past few years, the scope of COMAD 2011 has expanded to include, in addition to traditional database areas, topics in Web, Information Retrieval and Data Mining.

#index 2073987
#* Records retention: addressing insider threats to data integrity
#@ Marianne Winslett
#t 2011
#c 5
#! Inaccurate financial statements from major companies, dead people who still vote in elections, world-class gymnasts with uncertain birth dates: insiders often have the power and ability to make inappropriate changes to the content of electronic records. As electronic records replace paper records, it becomes easy to make such alterations without leaving behind evidence that can be used to detect the changes and determine who made them. The US Sarbanes-Oxley Act is perhaps the most (in)famous law that addresses these problems, but it is just one of many regulations that require long-term high-integrity retention of electronic records, all with the goal of ensuring that societal trust in business and government at reasonable cost. In this talk, we will discuss some of the technical challenges posed by the need for "tamper-proof" retention of records. We will describe how industry has responded to these challenges, the security weaknesses in current product offerings, and the role that researchers and government can play in addressing these weaknesses. We will give an overview of research progress to date and describe the major open research problems in this area.

#index 2073988
#* Information management in the cloud: parallel dataflow programming beyond map/reduce
#@ Volker Markl
#t 2011
#c 5
#! We have been researching a massively parallel data processor in the Stratosphere Research Unit, a DFG funded project among TU Berlin, FU Berlin, and HPI Potsdam. The research of the data management group at TU Berlin in this area focuses on a new flavor of data processors that goes beyond the popular map/reduce paradigm. We propose a programming model based on second order functions that describe what we call parallelization contracts (PACTs). PACTs are a generalization of the map/reduce programming model, extending it with additional higher order functions and output contracts that give guarantees about the behavior of a function. A PACT program is transformed into a data flow for a massively parallel execution engine, which executes its sequential building blocks in parallel and provides communication, synchronization and fault tolerance. The concept of PACTs allows the system to abstract parallelization from the specification of the data flow and thus enables several types of optimizations on the data flow. The system as a whole is as generic as map/reduce systems, but can provide higher performance through optimization and adaptation of the system to changes in the execution environment. Moreover, it enables the execution of tasks that traditional map/reduce systems cannot execute without mixing data flow program specification and parallelization, like joins, time-series analysis or data mining operations. We will present our research vision and preliminary research results that we have achieved during the last year. We will also highlight our research agenda for the upcoming year.

#index 2073989
#* Challenges in high dimensional data visualization
#@ Kamal Karlapalem
#t 2011
#c 5
#! High dimensional real data sets need to be mined for applications such as, social networks, bio-informatics, and for many business critical applications. A major challenge for mining such data is lack of tools to comprehend both the data and patterns mined from the data. Traditionally, data visualization helps in comprehending the data and validating the mined patterns especially for two and three dimensional data. The problem is to come up with tools and solutions for visualizing very high dimensional real data and the mined patterns. In this talk, I shall (i) present current approaches to address the problem, (ii) introduce three tools we have built - Heidi, Beads and CROVHD for visualizing high dimensional data, and (iii) list a set of open problems to be addressed.

#index 2073990
#* International Conference on Management of Data: COMAD 2011
#@ Srikanta Bedathur
#t 2011
#c 5
#! In the last decade or so, the scope of database research has witnessed an explosive expansion. When one looks at the research publications in top DB conferences, it is not surprising to see papers on topics ranging from machine learning to distributed systems, multi-modal datasets to petabytes of scientific data, solutions customized for modern hardware to visualization-driven analytics, and so on. In fact, these papers dominate the proceedings compared to papers on "traditional" DB topics.

#index 2073991
#* Tutorial on probabilistic topic models
#@ Indrajit Bhattacharya
#t 2011
#c 5
#! Over the last decade, probabilistic topic models have emerged as an extremely powerful and popular tool for analyzing large collections of unstructured data. While originally proposed for textual data, topic models have since been applied for various other types of data, such as images, videos, music, social networks and biological data. In this tutorial, I will discuss both the modeling and algorithmic aspects of topic models. I will review the fundamentals of probabilistic generative models, and explain how they can be applied for textual data, starting from simple unigram models to the Latent Dirichlet Allocation model. Then I will look at the problem of learning and inference using topic models, explain why exact inference is intractable for them, review the principle of inference using sampling, and discuss Gibbs Sampling strategies for inference in topic models. As applications of topic models, we will look at semantic search and sentiment analysis. Finally, I will discuss some short-comings of LDA, and briefly touch upon more advanced topic models, such as syntactic, correlated, dynamic and supervised topic models.

#index 2073992
#* Tutorial on the state of data privacy
#@ Srivatsan Laxman
#t 2011
#c 5
#! As learning and data mining algorithms mature, we find ourselves increasingly surrounded and reliant on many applications, like search, social-networking or business intelligence. The data in such settings often contain sensitive information of individuals, corporations or governments, and this leads us to the important issue of data privacy. There is a growing concern that algorithms used for analyzing data may also (inadvertently) compromise privacy by revealing specific information about the parties involved. Early work in data privacy established that mere removal or encryption of Personally Identifiable Information in user records is insufficient to guarantee privacy. This led to a sequence of works that tried to formalize a definition for privacy, starting with k-anonymity, and followed by notions like l-diversity, t-closeness and m-invariance. However, all these definitions were broken by a sequence of simple attacks, based either on responses to multiple queries or on suitable auxiliary information available to an adversary. In 2006, Dwork, et al., proposed the idea of Differential Privacy (DP) where, by adding a calibrated amount of noise, it is possible to guarantee that an adversary will learn essentially the same thing about a user, whether or not the user's record was included in the data. The main benefits of DP are that the guarantees are agnostic to auxiliary information and it is possible to precisely quantify the deterioration in DP guarantee under multiple queries (or composition). DP has quickly gained popularity (especially among the theory community) as an important formal notion of privacy with significant potential. Despite its growing success, there are several drawbacks of DP that have prevented its adoption in practice. Foremost among them is that DP adds very high levels of noise to the output, oftentimes leading to unusable query responses. This is because the DP framework assumes the adversary knows almost all the entries of the data base and disregards any possible probabilistic data generation model for the data. This is contrary to what we see in the real world, where data often has strong statistical characterizations and the knowledge of the adversary about specific data entries is often limited. The statistics community has also explored techniques for disclosure control as a privacy-preservation mechanism, but so far, a broad consensus has evaded the privacy community regarding suitability of statistical assumptions under which disclosure control guarantees may be provided. In this tutorial, I will introduce the area of data privacy and highlight the main challenges in this field of research. A wide range of privacy definitions will be covered including k-anonymity (and its variants), Differential Privacy and statistical disclosure control. One of the goals of the tutorial is to bring out the fundamental difficulties in developing formal notions of privacy and the inherent contradictions that exist between privacy and data analysis. A second goal is to analyze the merits and demerits of various privacy definitions that exist today, hopefully throwing light on what needs to be done in-future to achieve formal, yet practical frameworks for privacy preservation.

#index 2073993
#* Ranking mechanisms for interaction networks
#@ Sameep Mehta;Ramasuri Narayanam;Vinayaka Pandit
#t 2011
#c 5
#% 1272211
#% 1355042
#% 1425621
#% 1482201
#% 1581906
#% 1605972
#% 1617332
#! Interaction networks are prevalent in real world applications and they manifest in several forms such as online social networks, collaboration networks, technological networks, and biological networks. In the analysis of interaction networks, an important aspect is to determine a set of key nodes either with respect to positional power in the network or with respect to behavioral influence. This calls for designing ranking mechanisms to rank nodes/edges in the networks and there exists several well known ranking mechanisms in the literature such as Google page rank and centrality measures in social sciences. We note that these traditional ranking mechanisms are based on the structure of the underlying network. More recently, we witness applications wherein the ranking mechanisms should take into account not only the structure of the network but also other important aspects of the networks such as the value created by the nodes in the network and the marginal contribution of the nodes in the network. Motivated by this observation, the goal of this tutorial is to provide conceptual understanding of recent advances in designing efficient and scalable ranking mechanisms for large interaction networks along with applications to social network analysis.

#index 2073994
#* Tutorial on text mining of biomedical literature repositories
#@ Ashish Tendulkar
#t 2011
#c 5
#! There is an increasing interest in the development of biomedical text mining applications not only to enable improved literature search, but also to automatically detect pointers between biologically relevant entities described in articles and their corresponding records in existing annotation databases. The rapid growth of natural language data in biomedical sciences (including scientific articles, patents, patient records, database textual descriptions) together with the practical relevance of these resources for the design, interpretation and evaluation of bioinformatics and experimental research resulted in the implementation of a considerable number of new applications. For the development and maintenance of manually annotated database, text mining assisted literature duration has been especially promising, as well as for the construction of gold standard datasets and gene lists in the context of Systems Biology and gene set enrichment. Attempts have been made also to integrate text mining with other bioinformatics data such as sequence, structural and gene expression information. We plan to focus primarily on applications of text mining and issues in building text mining systems. We will begin with gentle introduction to text mining and its application in various Biology and Bioinformatics related domains. Existing resources for building text mining applications will be presented in terms of (1) useful data collections, (2) lexical resources, (3) features of natural language data that can be exploited by text mining systems and (4) data mining and natural language processing systems. Also the main types of currently available text mining applications will be discussed, including the retrieval and classification of articles, the identification of mentions of biological entities such as genes, proteins and cell types and the extraction of functional descriptions or protein interaction. The use of literature for knowledge discovery and hypothesis generation will be described. A crucial aspect of literature mining systems is evaluation and usability; these two aspects will be covered trough recent community evaluation efforts such as the BioCreative challenge and the BioCreative metaserver initiative. In order to show what kind of queries and results are currently supported by text mining and information extraction systems, practical example cases will be illustrated in detail, complementing the previously introduced basic descriptions of the underlying methodology. Finally a practical case study will show the step by step implementation of a text mining system illustrating how it is possible to construct such a system for a particular information need. After the tutorial, the participants should be aware of the importance of the biomedical literature as a central data and information source for biology and bioinformatics. They should be able to understand how existing text mining systems work and on what features they rely. Participants would have an overview of currently available tools and how to construct such an application in practice.

#index 2073995
#* Clustering of data streams: a second look
#@ Vasudha Bhatnagar;Sharanjit Kaur
#t 2011
#c 5
#! Revolution in digitized technologies has made it possible to acquire data on-line in the form of data streams, which are continuous and infinite in nature. Multiple applications varying from critical scientific applications to business and financial applications generate transient data. Since streaming data is ordered sequence of continuously growing unlabeled data instances, it is not feasible to apply traditional data mining techniques to reveal hidden, useful and novel patterns.

#index 2073996
#* Automated content labeling using context in email
#@ Aravindan Raghuveer
#t 2011
#c 5
#% 464434
#% 531952
#% 751818
#% 812486
#% 903606
#% 987205
#% 997189
#% 1040539
#% 1065169
#% 1065186
#% 1071116
#% 1131878
#% 1214718
#% 1232040
#% 1279778
#% 1279790
#% 1502479
#% 1727359
#! Through a recent survey, we observe that a significant percentage of people still share photos through email. When an user composes an email with an attachment, he/she most likely talks about the attachment in the body of the email. In this paper, we develop a supervised machine learning framework to extract keywords relevant to the attachments from the body of the email. The extracted keywords can then be stored as an extended attribute to the file on the local filesystem. Both desktop indexing software and web-based image search portals can leverage the context-enriched keywords to enable a richer search experience. Our results on the public Enron email dataset shows that the proposed extraction framework provides both high precision and recall. As a proof of concept, we have also implemented a version of the proposed algorithms as a Mozilla Thunderbird Add-on.

#index 2073997
#* Subquery plan reuse based query optimization
#@ Meduri Venkata Vamsikrishna;Kian-Lee Tan
#t 2011
#c 5
#% 210166
#% 288990
#% 315024
#% 480430
#% 481104
#% 632031
#% 810072
#% 826916
#% 1063510
#% 1707612
#! In this paper, we revisit the problem of query optimization in relational DBMS. We propose a scheme to reduce the search space of Dynamic Programming based on reuse of query plans among similar subqueries. The method generates the cover set of similar subgraphs present in the query graph and allows their corresponding subqueries to share query plans among themselves in the search space. Numerous variants to this scheme have been developed for enhanced memory efficiency. Our implementation and experimental study in PostgreSQL show that one of the schemes is better suited to improve the performance of (Iterative) Dynamic Programming.

#index 2073998
#* SPRING: ranking the results of SPARQL queries on linked data
#@ Kunal Mulay;P. Sreenivasa Kumar
#t 2011
#c 5
#% 290830
#% 783560
#% 805850
#% 805896
#% 1333442
#% 1540311
#% 1655394
#% 1719969
#! Linked Open Data (LOD) is a huge effort in the direction of making the Web of Data a reality. The LOD cloud consists of about 200 datasets contributed by several independent data providers from various domains such as Publications, Geography, Government, Media, Biology and Drugs etc. The data is represented in RDF and SPARQL is the query language. Due to the open nature of the data and its heterogeneous origin, it is often the case that a real-world entity appears in different datasets and with different names. When such entities are to be reported in the query results, a mechanism of rank ordering them becomes essential. In this paper, we propose a new framework for calculating the importance scores of datasets and also resources inside the datasets. As consensus is a key element that adds value to the data in the Web of Data, we base our ranking framework on constructs that are used to express sameness or equivalence among the entities in RDF data. We also make use of the underlying graph structure of LOD in the framework. The framework is experimentally verified on the Billion Triple Challenge (BTC-2010) dataset. The results indicate that the framework is successful in giving entities from the most relevant dataset a higher score compared to other datasets.

#index 2073999
#* Minimally infrequent itemset mining using pattern-growth paradigm and residual trees
#@ Ashish Gupta;Akshay Mittal;Arnab Bhattacharya
#t 2011
#c 5
#% 273916
#% 300120
#% 481290
#% 630984
#% 767654
#% 824931
#% 844318
#% 1031364
#% 1035693
#% 1099669
#% 1105801
#! Itemset mining has been an active area of research due to its successful application in various data mining scenarios including finding association rules. Though most of the past work has been on finding frequent itemsets, infrequent itemset mining has demonstrated its utility in web mining, bioinformatics and other fields. In this paper, we propose a new algorithm based on the pattern-growth paradigm to find minimally infrequent itemsets. A minimally infrequent itemset has no subset which is also infrequent. We also introduce the novel concept of residual trees. We further utilize the residual trees to mine multiple level minimum support itemsets where different thresholds are used for finding frequent itemsets for different lengths of the itemset. Finally, we analyze the behavior of our algorithm with respect to different parameters and show through experiments that it outperforms the competing ones.

#index 2074000
#* Discovering diverse-frequent patterns in transactional databases
#@ Somya Srivastava;R. Uday Kiran;P. Krishna Reddy
#t 2011
#c 5
#% 152934
#% 232136
#% 300120
#% 392618
#% 443092
#% 443313
#% 452846
#% 717219
#% 796210
#% 800181
#% 842021
#% 856785
#% 867057
#% 984426
#% 1000871
#% 1035593
#% 1195968
#% 1384882
#% 1737157
#! In the area of data mining, the process of frequent pattern extraction finds interesting information about the association among the items in a transactional database. The notion of support is employed to extract the frequent patterns. Normally, a frequent pattern may contain items which belong to different categories of a particular domain. The existing approaches do not consider the notion of diversity while extracting the frequent patterns. For certain types of applications, it may be useful to distinguish between the frequent patterns with items belonging to different categories and the frequent patterns with items belonging to the same category. In this paper we propose a new interestingness measure, called DiverseRank, to rank the frequent patterns based on the items' categories. Given a set of frequent patterns, we propose an efficient algorithm to extract the diverse-frequent patterns. Experiments on the real-world data set show that the diverse-frequent patterns extracted with the proposed DiverseRank measure are different from the frequent patterns extracted with the support measure.

#index 2074001
#* Agreement based source selection for the multi-topic deep web integration
#@ Manishkumar Jha;Raju Balakrishnan;Subbarao Kambhampati
#t 2011
#c 5
#% 194246
#% 232703
#% 268079
#% 290830
#% 340146
#% 641979
#% 643012
#% 654469
#% 660011
#% 745490
#% 765465
#% 818210
#% 987254
#% 1016177
#% 1081580
#% 1082158
#% 1190105
#% 1328155
#% 1399971
#% 1400041
#% 1523915
#% 1560235
#% 1560377
#% 1614889
#! One immediate challenge in searching the deep web databases is source selection---i.e. selecting the most relevant web databases for answering a given query. For open collections like the deep web, the source selection must be sensitive to trustworthiness and importance of sources. Recent advances solve these problems for a single topic deep web search adapting an agreement based approach (c.f. SourceRank [10]). In this paper we introduce a source selection method sensitive to trust and importance for multi topic deep web search. We compute multiple quality scores of a source tailored to different topics, based on the topic specific crawl data. At the query time, we classify the query to determine its probability of membership in different topics. These fractional memberships are used as the weights to the topic specific quality scores of sources to select sources for the query. Extensive experiments on more than a thousand sources in multiple topics show 18-85% improvements in result quality over Google Product Search and other existing methods.

#index 2074002
#* JovianDATA: a multidimensional database for the cloud
#@ Sandeep Akinapelli;Satya Ramachandran;Bharat Rane;Ravi Shetye;Vipul Agrawal;Anupam Singh;Shrividya Upadhya
#t 2011
#c 5
#% 223781
#% 316978
#% 462204
#% 893173
#% 941033
#% 1127559
#% 1248268
#% 1630216
#! The JovianDATA MDX engine is a data processing engine, designed specifically for managing multidimensional datasets spanning several terabytes. Implementing a terascale, native multidimensional database engine has required us to invent new ways of loading the data, partitioning the data in multi-dimensional space and an MDX (MultiDimensional eXpressions) query compiler capable of transforming MDX queries onto this native, multi-dimensional data model. The ever growing demand for analytics on huge amount of data needs to embrace distributed technologies such as cloud computing to efficiently fulfill the requirements. This paper provides an overview of the architecture of massively parallel, shared nothing implementation of a multi-dimensional database on the cloud environment. We highlight our innovations in 3 specific areas - dynamic cloud provisioning to build data cube over a massive dataset, techniques such as replication to help improve the overall performance and key isolation on dynamically provisioned nodes to improve performance. The query engine using these innovations exploits the ability of the cloud computing to provide on demand computing resources.

#index 2074003
#* WOOster: a map-reduce based platform for graph mining
#@ Aravindan Raghuveer
#t 2011
#c 5
#% 309749
#% 960304
#% 1023420
#% 1206703
#% 1217114
#% 1237170
#% 1318636
#! Large scale graphs containing O(billion) of vertices are becoming increasingly common in various applications. With graphs of such proportion, efficient querying infrastructure becomes crucial. In this paper, we propose WOOster a hosted querying infrastructure designed specifically for the large graphs. We make two key contributions: a) Design of the WOOster framework. b)Scalable map-reduce algorithms for two popular graph queries: subgraph match and reachability. Our experiments show that the proposed map-reduce algorithms scale well with large synthetic datasets.

#index 2074004
#* An application of sensor and streaming analytics to oil production
#@ Krishnamurthy Viswanathan;Chetan Gupta;Choudur Lakshminarayan;Ming Hao;Umeshwar Dayal;Ravigopal Vennelakanti;Paul Helm;Sumitha Rangaiah;Harikrishnam-Raju Sagiraju;Sunil Doddmani
#t 2011
#c 5
#% 260645
#% 297171
#% 726621
#% 1070828
#% 1255307
#% 1426599
#% 1592892
#% 1869435
#% 1908484
#! At HP Labs, we are building "Live Operational Intelligence (Live OI) System" -- a system that ingests streams of operational data generated by multiple sources such as sensors and operational logs, and provides the operational staff real time insights in terms of suggested actions, event correlations, predictions, root cause analysis and visualization. In a Live OI framework some models are learnt offline and then deployed online, and some models are learnt online. Live OI system also supports querying of historical data to find past occurrences of patterns and suggested actions, and a dashboard for humans to monitor and interact with the operational system. This paper describes the highlights of the Live OI system as applied to monitoring oil production operation, through the discussion of use cases.

