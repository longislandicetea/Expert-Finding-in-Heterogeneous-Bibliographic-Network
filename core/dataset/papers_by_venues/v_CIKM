#index 214592
#* Educating the next generation of information and knowledge experts, in collaboration with industry (abstract)
#@ Michael Mulder
#t 1996
#c 1

#index 214594
#* OLAP and statistical databases: similarities and differences (abstract)
#@ Arie Shoshani
#t 1996
#c 1

#index 214595
#* Efficient retrieval for browsing large image databases
#@ Daniel Wu;Ambuj Singh;Divyakant Agrawal;Amr El Abbadi;Terence R. Smith
#t 1996
#c 1
#% 55490
#% 86950
#% 132779
#% 169940
#% 172949
#% 201876
#% 212690
#% 437407
#% 460862

#index 214597
#* Adapting a spatial access structure for document representations in vector space
#@ Andreas Henrich
#t 1996
#c 1
#% 32898
#% 46803
#% 57624
#% 67565
#% 77928
#% 83319
#% 85447
#% 86950
#% 184486
#% 201876
#% 228097
#% 285932
#% 321455
#% 321635
#% 411694
#% 427199
#% 435141
#% 462479
#% 464195
#% 480093
#% 527026
#% 527027

#index 214599
#* A terrain database representation based on an extended vector product format (EVPF)
#@ Mahdi Abdelguerfi;Edgar Cooper;Kevin Shaw;Chris Wynne;Vincent Miller;Robert Broome;Barbara Ray
#t 1996
#c 1
#% 435767
#% 437375
#% 688101

#index 214601
#* Recursive query processing using graph traversal techniques
#@ Estrella Pulido
#t 1996
#c 1
#% 53395
#% 66097
#% 134568
#% 137876
#% 172951
#% 181049
#% 220395
#% 480952
#% 748315

#index 214602
#* Processing queries for first-few answers
#@ Roberto J. Bayardo, Jr.;Daniel P. Miranker
#t 1996
#c 1
#% 2078
#% 77654
#% 86949
#% 102784
#% 175394
#% 289424
#% 411554
#% 462649
#% 462789
#% 463730
#% 479938
#% 480627
#% 481429

#index 214604
#* Spatial query processing using object decomposition method
#@ Yong-Ju Lee;Ho-Hyun Park;Nam-Hee Hong;Chin-Wan Chung
#t 1996
#c 1
#% 13041
#% 58369
#% 59539
#% 68091
#% 77928
#% 90724
#% 108509
#% 114577
#% 172908
#% 285932
#% 318051
#% 435137
#% 445701
#% 462781
#% 463425
#% 526849

#index 214661
#* An object-oriented approach to multi-level association rule mining
#@ Scott Fortin;Ling Liu
#t 1996
#c 1
#% 82276
#% 111351
#% 152934
#% 172386
#% 199515
#% 199556
#% 201894
#% 232146
#% 412588
#% 442808
#% 481290
#% 481588
#% 481754
#% 535829
#% 591538
#% 651342

#index 214664
#* Background for association rules and cost estimate of selected mining algorithms
#@ Jia Liang Han;Ashley W. Plank
#t 1996
#c 1
#% 152934
#% 172386
#% 201894
#% 322885
#% 463883
#% 479939
#% 481290

#index 214667
#* A data model for supporting on-line analytical processing
#@ Chang Li;X. Sean Wang
#t 1996
#c 1
#% 113841
#% 201950
#% 210182
#% 286236
#% 442695
#% 452799
#% 464215
#% 503540
#% 503544
#% 673645

#index 214670
#* Fast retrieval of cursive handwriting
#@ Ibrahim Kamel
#t 1996
#c 1
#% 86950
#% 114667
#% 130435
#% 153260
#% 172949
#% 288885
#% 427199
#% 481455

#index 214675
#* Notes Explorer: entity-based retrieval in shared, semi-structured information spaces
#@ Scott Huffman;Catherine Baudin
#t 1996
#c 1
#% 111922
#% 172378
#% 481280
#% 748613

#index 214679
#* INFOMOD: a knowledge-based moderator for electronic mail help lists
#@ Robert J. Hall
#t 1996
#c 1
#% 6311
#% 325050
#% 441148
#% 840583

#index 214683
#* Dynamic query optimization on a distributed object management platform
#@ Fatma Ozcan;Sena Nural;Pinar Koksal;Cem Evrendilek;Asuman Dogac
#t 1996
#c 1
#% 40633
#% 51237
#% 177829
#% 191170
#% 201926
#% 208383
#% 252369
#% 394646
#% 464692
#% 480788
#% 482067
#% 563732
#% 614600

#index 214688
#* A model of object database applications and its use in cost estimation
#@ Neil Ching;Eric Hughes;Marianne Winslett
#t 1996
#c 1
#% 16280
#% 36119
#% 118698
#% 125620
#% 152904
#% 240155
#% 320113
#% 445769
#% 460666

#index 214693
#* Processing OODB queries by O-Algebra
#@ Jie Lin;Z. Meral Ozsoyoglu
#t 1996
#c 1
#% 27056
#% 67459
#% 116090
#% 125620
#% 152942
#% 287005
#% 462775
#% 481273

#index 214698
#* Incorporating latent semantic indexing into a neural network model for information retrieval
#@ Inien Syu;S. D. Lang;Narsingh Deo
#t 1996
#c 1
#% 3682
#% 27049
#% 53921
#% 55490
#% 80399
#% 111303
#% 118749
#% 118762
#% 132779
#% 169805
#% 172346
#% 174664
#% 224113
#% 306495
#% 678757

#index 214700
#* Learning to extract information from text based on user-provided examples
#@ Scott B. Huffman
#t 1996
#c 1
#% 20845
#% 179800
#% 442981
#% 449564
#% 449587
#% 814960
#% 814962
#% 814976
#% 1290067

#index 214702
#* Object-oriented and database concepts for the design of networked information retrieval systems
#@ Norbert Fuhr
#t 1996
#c 1
#% 83336
#% 118759
#% 120106
#% 186336
#% 194141
#% 194249
#% 194290
#% 215225
#% 360914
#% 437405
#% 443052

#index 214706
#* Modeling a vocabulary in an object-oriented database
#@ Li-min Liu;Michael Halper;Huanying Gu;James Geller;Yehoshua Perl
#t 1996
#c 1
#% 38696
#% 55234
#% 57954
#% 82275
#% 116091
#% 116598
#% 142396
#% 198058
#% 286856
#% 405391
#% 463584
#% 479913
#% 1275324
#% 1275326

#index 214710
#* Integrating constraints in complex objects
#@ C. Oussalah;V. Puig
#t 1996
#c 1
#% 43653
#% 104754
#% 126819
#% 196457
#% 480621
#% 487576
#% 487734

#index 214712
#* Constructing information systems based on schema reuse
#@ Wen-Syan Li;Richard D. Holowczak
#t 1996
#c 1
#% 91103
#% 116575
#% 146205
#% 157753
#% 198058
#% 201977
#% 230903
#% 231859
#% 463581
#% 481280
#% 586763

#index 214717
#* S-signature: a new scheme for efficient query processing of complex objects in OODB
#@ Hakgene Shin;Kangseok Kim;Jaewoo Chang
#t 1996
#c 1
#% 57955
#% 83148
#% 86518
#% 115466
#% 154984
#% 172333
#% 442665
#% 480274

#index 214719
#* Effective graph clustering for path queries in digital map databases
#@ Yun-Wu Huang;Ning Jing;Elke A. Rundensteiner
#t 1996
#c 1
#% 13014
#% 47621
#% 58364
#% 77979
#% 139176
#% 172380
#% 214769
#% 463421
#% 565463
#% 605157

#index 214722
#* Indexing values of time sequences
#@ Ling Lin;Tore Risch;Martin Sköld;Dushan Badal
#t 1996
#c 1
#% 102759
#% 135384
#% 201923
#% 317933
#% 427199
#% 452782
#% 460862
#% 461885
#% 463749
#% 464196
#% 481609
#% 481611
#% 503708
#% 565462

#index 214724
#* Performance evaluation of G-tree and its application in fuzzy databases
#@ Chengwen Liu;Aris Ouksel;Prasad Sistla;Jing Wu;Clement Yu;Naphtali Rishe
#t 1996
#c 1
#% 2692
#% 13041
#% 32898
#% 32913
#% 49336
#% 54711
#% 78359
#% 83105
#% 112495
#% 285932
#% 291854
#% 415957
#% 415983
#% 415984
#% 442868
#% 461852
#% 462479
#% 463888

#index 214726
#* The personal electronic program guide—towards the pre-selection of individual TV programs
#@ Michael Ehrmantraut;Theo Härder;Hartmut Wittig;Ralf Steinmetz
#t 1996
#c 1
#% 46809
#% 57990
#% 114519
#% 173637
#% 173638
#% 173639
#% 461859
#% 661694

#index 214766
#* CROSS-DB: a feature-extended multidimensional data model for statistical and scientific databases
#@ Wolfgang Lehner;Thomas Ruf;Michael Teschke
#t 1996
#c 1
#% 11906
#% 54713
#% 77312
#% 287730
#% 411703
#% 411704
#% 464215
#% 482049
#% 503367
#% 503373
#% 852959

#index 214769
#* Hierarchical optimization of optimal path finding for transportation applications
#@ Ning Jing;Yun-Wu Huang;Elke A. Rundensteiner
#t 1996
#c 1
#% 70370
#% 77979
#% 83158
#% 83159
#% 139176
#% 152958
#% 214719
#% 340642
#% 463583
#% 464223
#% 565445

#index 214771
#* Handling uncertainties in workflow applications
#@ Jian Tang;San-Yih Hwang
#t 1996
#c 1
#% 83248
#% 96218
#% 122904
#% 146202
#% 159236
#% 168959
#% 185413
#% 480771
#% 481261
#% 566116

#index 214776
#* Distributed processing of time-constrained queries in CASE-DB
#@ SungKil Lee;Gültekin Özsoyoğlu
#t 1996
#c 1
#% 58348
#% 102316
#% 114988
#% 115006
#% 135874
#% 277347
#% 442995
#% 462797
#% 463108
#% 463260
#% 695828

#index 214779
#* A new conflict relation for concurrency control and recovery in object-based databases
#@ SangKeun Lee;SoonYoung Jung;Chong-Sun Hwang
#t 1996
#c 1
#% 459
#% 1382
#% 9241
#% 39634
#% 46607
#% 57956
#% 68143
#% 114583
#% 117082
#% 139179
#% 277340
#% 287220
#% 459245
#% 669952
#% 676445

#index 214782
#* Information agents for automated browsing
#@ Chanda Dharap;Martin Freeman
#t 1996
#c 1
#% 107693
#% 118771
#% 144708
#% 159110
#% 172394
#% 176503
#% 188101
#% 478927
#% 609889

#index 214783
#* On implementing SchemaLog—a database programming language
#@ Alanoly J. Andrews;Nematollaah Shiri;Laks V. S. Lakshmanan;Iyer N. Subramanian
#t 1996
#c 1
#% 36683
#% 83606
#% 89649
#% 140407
#% 189739
#% 442702
#% 464215
#% 480952
#% 486431

#index 214787
#* A case study of Venus and a declarative basis for rule modules
#@ Lane B. Warshaw;Daniel P. Miranker
#t 1996
#c 1
#% 1797
#% 44698
#% 46609
#% 51207
#% 54049
#% 199801
#% 205241
#% 370260
#% 442816
#% 442824
#% 514694
#% 695634

#index 214793
#* Optimal unification of bound simple set-terms
#@ Sergio Greco
#t 1996
#c 1
#% 816
#% 18016
#% 18017
#% 36683
#% 39854
#% 53398
#% 54020
#% 55408
#% 58003
#% 58354
#% 58356
#% 69086
#% 100591
#% 101621
#% 124755
#% 124783
#% 145721
#% 289304
#% 458584
#% 464533
#% 559927

#index 214796
#* Smart mediators and intelligent agents (panel)
#@ V. S. Subrahmanian;Su-Shing Chen;James Hendler;Richard Hull;Val-Breazu Tannen
#t 1996
#c 1

#index 216977
#* Web, distributed object management and component software
#@ Bruce Cottman;Annrai O'Toole;Mark Ryland;Richard Soley
#t 1996
#c 1

#index 240019
#* Proceedings of the sixth international conference on Information and knowledge management
#@ Forouzan Golshani;Kia Makki;Charles Nicholas;Niki Pissinou
#t 1997
#c 1

#index 240024
#* Block addressing indices for approximate text retrieval
#@ Ricardo Baeza-Yates;Gonzalo Navarro
#t 1997
#c 1
#% 69506
#% 375076
#% 546270
#% 547609

#index 240026
#* Applications of approximate word matching in information retrieval
#@ James C. French;Allison L. Powell;Eric Schulman
#t 1997
#c 1
#% 131061
#% 219033
#% 288885
#% 288948
#% 317975
#% 322316
#% 324015
#% 504892
#% 647588

#index 240146
#* Improving relevance feedback in the vector space model
#@ Carol Lundquist;David A. Grossman;Ophir Frieder
#t 1997
#c 1
#% 67565
#% 118726
#% 218982
#% 224702

#index 240149
#* Modeling temporal primitives: back to basics
#@ Iqbal A. Goralwalla;Yuri Leontiev;M. Tamer Özsu;Duane Szafron
#t 1997
#c 1
#% 43028
#% 99442
#% 105467
#% 152936
#% 201923
#% 225003
#% 268788
#% 348919
#% 361445
#% 463571
#% 463572
#% 467630
#% 480934
#% 546596

#index 240153
#* A framework for the management of past experiences with time-extended situations
#@ Michel Jaczynski
#t 1997
#c 1
#% 399
#% 116185
#% 163434
#% 172949
#% 176887
#% 490132
#% 490262
#% 490273
#% 490429
#% 490442
#% 494272
#% 494412
#% 533357
#% 559202

#index 240156
#* I-regular expression: regular expression with continuous interval constraints
#@ Ken Nakayama;Kazunori Yamaguchi;Satoru Kawai
#t 1997
#c 1
#% 101940
#% 197908
#% 197911
#% 319244
#% 356652
#% 404772
#% 434690
#% 452796

#index 240157
#* A uniform approach to global concurrency control and recovery in multidatabase environment
#@ SangKeun Lee;Chong-Sun Hwang;WonGye Lee
#t 1997
#c 1
#% 9241
#% 46607
#% 65722
#% 68143
#% 112319
#% 114583
#% 123077
#% 166215
#% 214779
#% 268752
#% 286836
#% 317988
#% 435100
#% 435104
#% 435118
#% 442853
#% 459245
#% 463101
#% 463124
#% 464688

#index 240160
#* Commit-reordering validation scheme for transaction scheduling in client-server based teleputing systems: COREV
#@ Youngkon Lee;Songchun Moon
#t 1997
#c 1
#% 9241
#% 137753
#% 172874
#% 286836
#% 609905
#% 660942

#index 240163
#* Global nested transaction management for ODMG-compliant multi-database systems
#@ Thomas Tesch;Jürgen Wäsch
#t 1997
#c 1
#% 43175
#% 83248
#% 85086
#% 91076
#% 122904
#% 122909
#% 194963
#% 194984
#% 380441
#% 395735
#% 435104
#% 435118
#% 442853
#% 463101
#% 464723
#% 511738

#index 240165
#* Efficient global probabilistic deduction from taxonomic and probabilistic knowledge-bases over conjunctive events
#@ Thomas Lukasiewicz
#t 1997
#c 1
#% 3034
#% 13742
#% 36683
#% 42485
#% 44876
#% 73571
#% 89958
#% 90371
#% 100324
#% 101210
#% 130150
#% 167626
#% 170207
#% 199790
#% 287295
#% 289305
#% 503681
#% 1290137

#index 240167
#* Knowledge mining from textual sources
#@ Udo Hahn;Klemens Schnattinger
#t 1997
#c 1
#% 41351
#% 58549
#% 72266
#% 92533
#% 177237
#% 198058
#% 459153
#% 459694
#% 496867
#% 496875
#% 539445
#% 757350
#% 1290067

#index 240169
#* Using consistency-driven pairwise comparisons in knowledge-based systems
#@ Waldemar W. Koczkodaj;Michael W. Herman;Marian Orłowski
#t 1997
#c 1
#% 59299
#% 175666
#% 197371
#% 219506

#index 240171
#* The AudioWeb
#@ Daniel Barbará
#t 1997
#c 1
#% 149269
#% 157706
#% 190656
#% 194188

#index 240174
#* Retrieving video data via motion tracks of content symbols
#@ Tim T. Y. Wai;Arbee L. P. Chen
#t 1997
#c 1
#% 173638
#% 190650
#% 194231
#% 452790
#% 461885
#% 481609
#% 641571
#% 1180066
#% 1180122

#index 240178
#* Video keyframe extraction and filtering: a keyframe is not a keyframe to everyone
#@ Nevenka Dimitrova;Thomas McGee;Herman Elenbaas
#t 1997
#c 1
#% 156430
#% 190650
#% 194185
#% 194209
#% 206633
#% 219840
#% 228351
#% 403487
#% 527880
#% 622056

#index 240181
#* Distributed knowledge revision/integration
#@ Aldo Franco Dragoni;Paolo Giorgini;Paolo Puliti
#t 1997
#c 1
#% 3460
#% 21137
#% 36784
#% 74217
#% 74848
#% 136805
#% 412522
#% 558407
#% 565192
#% 569060
#% 743870
#% 1290097

#index 240182
#* Matching and indexing sequences of different lengths
#@ Tolga Bozkaya;Nasser Yazdani;Meral Özsoyoğlu
#t 1997
#c 1
#% 70370
#% 80995
#% 86950
#% 172949
#% 179696
#% 460862
#% 481460
#% 481609
#% 503559
#% 503713

#index 240185
#* The update of index structures in object-oriented DBMS
#@ Andreas Henrich
#t 1997
#c 1
#% 46803
#% 152928
#% 161102
#% 168778
#% 184486
#% 201929
#% 214597
#% 219012
#% 317933
#% 321635
#% 395735
#% 501937
#% 616343

#index 240187
#* Evaluating triggers using decision trees
#@ Lance Obermeyer;Daniel P. Miranker
#t 1997
#c 1
#% 58361
#% 58362
#% 59350
#% 69399
#% 121397
#% 206915
#% 287324
#% 449588
#% 464043
#% 480763
#% 681393

#index 240188
#* Mining association rules with adjustable accuracy
#@ Jong Soo Park;Philip S. Yu;Ming-Syan Chen
#t 1997
#c 1
#% 152934
#% 201894
#% 227917
#% 449588
#% 463257
#% 480940
#% 480964
#% 481290
#% 481588
#% 481754
#% 481758
#% 481779

#index 240190
#* A practical approach to static analysis and execution of rules in active databases
#@ Seung-Kyum Kim;Sharma Chakravarthy
#t 1997
#c 1
#% 77680
#% 86946
#% 152921
#% 182421
#% 480620
#% 480621
#% 480765
#% 481448
#% 501935

#index 240192
#* A spatial match representation scheme for indexing and querying in iconic image databases
#@ Jae-Woo Chang;Yeon-Jung Kim;Ki-Jin Chang
#t 1997
#c 1
#% 23998
#% 55490
#% 102358
#% 169940
#% 176344
#% 318437

#index 240193
#* Using a sequential index in terrain-aided navigation
#@ Ling Lin;Tore Risch
#t 1997
#c 1
#% 102759
#% 214722
#% 427199
#% 460862
#% 461885
#% 464196
#% 481609
#% 481611

#index 240195
#* Digital information retrieval
#@ Chabane Djeraba;marinette Bouet
#t 1997
#c 1
#% 219847
#% 437404
#% 437408
#% 437409
#% 443889

#index 240197
#* Generating association rules from semi-structured documents using an extended concept hierarchy
#@ Lisa Singh;Peter Scheuermann;Bin Chen
#t 1997
#c 1
#% 152934
#% 172381
#% 172386
#% 191154
#% 201894
#% 214661
#% 232102
#% 481290
#% 481588
#% 481758
#% 993508

#index 240198
#* Intensional query processing using data mining approaches
#@ S. C. Yoon;I. Y. Song;E. K. Park
#t 1997
#c 1
#% 29252
#% 45280
#% 77960
#% 86936
#% 152934
#% 172361
#% 199580
#% 201894
#% 395669
#% 412588
#% 442756
#% 442877
#% 452747
#% 452822
#% 462336

#index 240200
#* Mining fuzzy association rules
#@ Keith C. C. Chan;Wai-Ho Au
#t 1997
#c 1
#% 84511
#% 152934
#% 210160
#% 232102
#% 232106
#% 296926
#% 412588
#% 443880
#% 481290
#% 481588
#% 481758
#% 589686

#index 240201
#* Evaluating document retrieval in patent database: a preliminary report
#@ Mark Osborn;Tomek Strzalkowski;Mihnea Marinescu
#t 1997
#c 1
#% 184494
#% 742368

#index 240203
#* View materialization techniques for complex hierarchical objects
#@ Matthew C. Jones;Elke A. Rundensteiner
#t 1997
#c 1
#% 47621
#% 79131
#% 144998
#% 151432
#% 172325
#% 172927
#% 189868
#% 221756
#% 233835
#% 273834
#% 415958
#% 452816
#% 464234
#% 480958

#index 240205
#* Assistant for an information database
#@ Michael Früchtl;Jürgen Kreuziger;Michael Beigl
#t 1997
#c 1
#% 159108
#% 1275346

#index 240206
#* On disk caching of Web objects in proxy servers
#@ Charu G. Aggarwal;Philip S. Yu
#t 1997
#c 1
#% 43171
#% 149242
#% 152943
#% 176498
#% 176500
#% 194197
#% 197517
#% 209651
#% 209698
#% 209891
#% 223400
#% 255027
#% 255035
#% 255038
#% 481450
#% 642534

#index 240207
#* Experimental evaluation of PFS continuous media file system
#@ Wonjun Lee;Difu Su;Duminda Wijesekera;Jaideep Srivastava;Deepak Kenchammana-Hosekote;Mark Foresti
#t 1997
#c 1
#% 29572
#% 114572
#% 124017
#% 159084
#% 164679
#% 173593
#% 173687
#% 187204
#% 240207
#% 452791
#% 520284
#% 591408

#index 240208
#* Analysis and design of server informative WWW-sites
#@ Amir M. Zarkesh;Jafar Adibi;Cyrus Shahabi;Reza Sadri;Vishal Shah
#t 1997
#c 1
#% 115608
#% 143187
#% 209662
#% 232102
#% 233805
#% 375388
#% 404362
#% 614610

#index 240210
#* A framework for global optimization of aggregate queries
#@ Chengwen Liu;Andrei Ursu
#t 1997
#c 1
#% 36117
#% 411554
#% 411750
#% 463735
#% 463747
#% 464056
#% 464078
#% 464215
#% 480268
#% 481288
#% 481608
#% 564426

#index 240211
#* An analysis of cardinality constraints in redundant relationships
#@ James Dullea;Il-Yeol Song
#t 1997
#c 1
#% 4797
#% 72552
#% 108252
#% 140389
#% 207730
#% 287631
#% 368804
#% 369136

#index 240212
#* A corpus analysis approach for automatic query expansion
#@ Susan Gauch;Jianying Wang
#t 1997
#c 1
#% 78171
#% 111308
#% 118726
#% 118738
#% 118739
#% 144029
#% 169729
#% 218978

#index 240214
#* A distributed, graphical, topic-oriented document search system
#@ John Light
#t 1997
#c 1
#% 1921
#% 142621
#% 290703
#% 641057
#% 641060

#index 240215
#* Towards maintaining consistency of spatial databases
#@ Alia I. Abdelmoty;Chris B. Jones
#t 1997
#c 1
#% 81968
#% 199810
#% 362928
#% 464860
#% 527008
#% 637564
#! This paper focuses on the consistency issues related to integrating multiple sets of spatial data in spatial information systems such as Geographic Information Systems (GISs). Data sets to be integrated are assumed to hold information about the same geographic features which can be drawn from different sources at different times, which may vary in reliability and accuracy, and which may vary in the scale of presentation resulting in possible multiple spatial representations for these features. A systematic approach is proposed which relies first on breaking down the consistency issue by identifying a range of consistency classes which can be checked in isolation. These classes are a representative set of properties and relationships which can completely identify the geographic objects in the data sets. Different levels of consistency are then proposed, namely, total, partial and conditional, which can be checked for every consistency class. This provides the flexibility for two data sets to be integrated without necessarily being totally consistent in every aspect. The second step of the proposed approach is to explicitly represent the different classes and levels of consistency in the system. As an example, a simple structure which stores adjacency relationships is given which can be used for the explicit representation of topological consistency. The paper also proposes that the set of consistent knowledge in the data sets (which is mostly qualitative) be explicitly represented in the database and that uncertainty or ambiguity inherent in the knowledge be represented as well.

#index 240217
#* Real-time transactions with execution histories: priority assignment and load control
#@ Erdoğan Doğdu;Gültekin Özsoyoğlu
#t 1997
#c 1
#% 152933
#% 205094
#% 314484
#% 385668

#index 240218
#* Reducing match time variance in production systems with HAL
#@ Pou-yung Lee;Albert Mo Kim Cheng
#t 1997
#c 1
#% 91220
#% 163721
#% 377175
#% 394745
#% 405349
#% 442929

#index 240220
#* Discovering similar resources by content part-linking
#@ Brad Perry;Wesley W. Chu
#t 1997
#c 1
#% 67565
#% 115446
#% 119916
#% 176534
#% 209662
#% 211512
#% 211513
#% 211526
#% 217248
#% 258945

#index 240222
#* Learning belief networks from data: an information theory based approach
#@ Jie Cheng;David A. Bell;Weiru Liu
#t 1997
#c 1
#% 44876
#% 67866
#% 129497
#% 129987
#% 443025
#% 527675

#index 240223
#* Incorporating association pattern and operation specification in ODMG's OQL
#@ Vanja Josifovski;Stanley Y. W. Su
#t 1997
#c 1
#% 43210
#% 77999
#% 102800
#% 116203
#% 123589
#% 125614
#% 154342
#% 168695
#% 168696
#% 168721
#% 189321
#% 213223
#% 435125
#% 452815
#% 458613
#% 480104

#index 240225
#* Exploiting enterprise models for the automatic distribution of corporate information
#@ Michael Wolverton
#t 1997
#c 1
#% 2298
#% 124004
#% 159108
#% 219048
#% 406493
#% 445079
#% 978507

#index 240226
#* Pharos: a scalable distributed architecture for locating heterogeneous information sources
#@ R. Dolin;D. Agrawal;A. El Abbadi;L. Dillon
#t 1997
#c 1
#% 55490
#% 161754
#% 194244
#% 194245
#% 194246
#% 200694
#% 406493
#% 511331
#% 682332

#index 240227
#* Xmas: an extensible main-memory storage system
#@ Sang Kyun Cha;Jang Ho Park;Byoung Dae Park
#t 1997
#c 1
#% 12638
#% 32885
#% 47623
#% 63503
#% 114582
#% 187411
#% 442706
#% 442832
#% 442835
#% 463884
#% 479769
#% 480266
#% 481454
#% 481590
#% 615164
#% 615245
#% 680476

#index 240229
#* A contention based dynamic consistency maintenance scheme for client cache
#@ IlYoung Chung;JongMin Lee;Chong-Sun Hwang
#t 1997
#c 1
#% 9241
#% 27057
#% 83127
#% 100060
#% 102802
#% 102803
#% 117082
#% 166234
#% 172347
#% 175205
#% 340609
#% 464066
#% 480927
#% 481108

#index 240231
#* Dealing with partial failures in multiple processor primary-backup systems
#@ Sharad Mehrotra;Kexiang Hu;Simon Kaplan
#t 1997
#c 1
#% 9241
#% 83135
#% 91620
#% 114582
#% 167264
#% 403195
#% 463442

#index 241113
#* Proceedings of the 5th ACM international workshop on Advances in geographic information systems
#@ Niki Pissinou;Kia Makki;Patrick Bergougnoux;Robert Laurini
#t 1997
#c 1

#index 248041
#* Proceedings of the 1997 workshop on New paradigms in information visualization and manipulation
#@ David S. Ebert;Charles K. Nicholas
#t 1997
#c 1

#index 259977
#* Proceedings of the seventh international conference on Information and knowledge management
#@ Niki Pissinou;Charles Nicholas;James French;George Gardarin;K. Makki;L. Bouganim
#t 1998
#c 1

#index 259984
#* Dynamic skew handling in parallel mining of association rules
#@ Lilian Harada;Naoki Akaboshi;Kazutaka Ogihara;Riichiro Take
#t 1998
#c 1
#% 3107
#% 152924
#% 152934
#% 201894
#% 227917
#% 227922
#% 252608
#% 340290
#% 340291
#% 458611
#% 459006
#% 463095
#% 463903
#% 480608
#% 480761
#% 480966
#% 481290
#% 481588
#% 481754
#% 481758
#% 511189

#index 259985
#* Focusing search in hierarchical structures with directory sets
#@ Guy Jacobson;Balachander Krishnamurthy;Divesh Srivastava;Dan Suciu
#t 1998
#c 1
#% 172924
#% 191574
#% 198335
#% 406493

#index 259986
#* Flexible list management in a directory
#@ H. V. Jagadish;Mark A. Jones;Divesh Srivastava;Dimitra Vista
#t 1998
#c 1
#% 36683
#% 123587
#% 154334
#% 210214
#% 238087
#% 255197
#% 268797
#% 374001
#% 411554

#index 259987
#* Incremental maintenance for dynamic database-derived HTML pages in digital libraries
#@ Ken C. K. Lee;Hong V. Leong;Antonio Si
#t 1998
#c 1
#% 13015
#% 13016
#% 152928
#% 169844
#% 201928
#% 210211
#% 224763
#% 227945
#% 227947
#% 289282
#% 296374
#% 340300
#% 442663
#% 442767
#% 442781
#% 452530
#% 979356

#index 259988
#* Just-in-time databases and the World-Wide Web
#@ Ellen Spertus;Lynn Andrea Stein
#t 1998
#c 1
#% 202011
#% 210214
#% 220706
#% 236416
#% 249995
#% 255165
#% 255197
#% 340295
#% 1499471

#index 259989
#* Locating passages using a case-base of excerpts
#@ Jody J. Daniels;Edwina L. Rissland
#t 1998
#c 1
#% 144012
#% 154075
#% 169809
#% 194282
#% 258442
#% 367665
#% 676714

#index 259990
#* Accurate user directed summarization from existing tools
#@ Mark Sanderson
#t 1998
#c 1
#% 144013
#% 169809
#% 194251
#% 218978
#% 262036

#index 259991
#* Ontology-based extraction and structuring of information from data-rich unstructured documents
#@ David W. Embley;Douglas M. Campbell;Randy D. Smith;Stephen W. Liddle
#t 1998
#c 1
#% 57961
#% 107082
#% 115462
#% 210214
#% 210985
#% 216231
#% 237194
#% 240955
#% 244103
#% 244118
#% 248808
#% 357984
#% 462062
#% 479465
#% 479471
#% 481602
#% 535353
#% 562324
#% 614598

#index 259992
#* Information extraction from case law and retrieval of prior cases by partial parsing and query generation
#@ Peter Jackson;Khalid Al-Kofahi;Chris Kreilick;Brian Grom
#t 1998
#c 1
#% 199335
#% 814934
#% 814963
#% 814964
#% 835096

#index 259993
#* Efficient enumeration of frequent sequences
#@ Mohammed J. Zaki
#t 1998
#c 1
#% 232136
#% 420063
#% 459006
#% 461903
#% 463903
#% 481754

#index 259994
#* Online algorithms for finding profile association rules
#@ Charų C. Aggarwal;Zheng Sun;Philip S. Yu
#t 1998
#c 1
#% 152934
#% 172386
#% 201894
#% 210160
#% 427199
#% 461909
#% 462238
#% 463883
#% 463903
#% 481281
#% 481290
#% 481588
#% 481754
#% 481758
#% 481779
#% 527028

#index 259995
#* Data cube approximation and histograms via wavelets
#@ Jeffrey Scott Vitter;Min Wang;Bala Iyer
#t 1998
#c 1
#% 41684
#% 43163
#% 82346
#% 116084
#% 168862
#% 190330
#% 210182
#% 227866
#% 227880
#% 227883
#% 248023
#% 248812
#% 248822
#% 257637
#% 463760
#% 464062
#% 464215
#% 481951
#% 482092
#% 617012

#index 259996
#* Memory-adaptive scheduling for large query execution
#@ Luc Bouganim;Olga Kapitskaia;Patrick Valduriez
#t 1998
#c 1
#% 4683
#% 58352
#% 58375
#% 58377
#% 102784
#% 136740
#% 152943
#% 157130
#% 172900
#% 172910
#% 201925
#% 340305
#% 427195
#% 435107
#% 442706
#% 461895
#% 479461
#% 481110
#% 481275
#% 481289
#% 481784
#% 565457
#% 571094
#% 571294

#index 259997
#* Memory allocation strategies for complex decision support queries
#@ Biswadeep Nag;David J. DeWitt
#t 1998
#c 1
#% 83132
#% 83232
#% 172910
#% 211569
#% 223781
#% 227934
#% 339717
#% 411554
#% 435107
#% 479461
#% 480943
#% 481131
#% 481275
#% 481617
#% 481753

#index 259998
#* Document classification using multiword features
#@ Ron Papka;James Allan
#t 1998
#c 1
#% 81669
#% 120110
#% 127850
#% 144011
#% 165115
#% 169729
#% 194301
#% 219052
#% 232670
#% 237788
#% 375017
#% 854961
#% 854984

#index 259999
#* A hierarchical approach to the automatic categorization of medical documents
#@ Luciano R. S. de Lima;Alberto H. F. Laender;Berthier A. Ribeiro-Neto
#t 1998
#c 1
#% 44876
#% 46803
#% 115462
#% 120649
#% 219051

#index 260000
#* A comparison of regression, neural net, and pattern recognition approaches to IR
#@ Aitao Chen
#t 1998
#c 1
#% 65946
#% 80995
#% 91872
#% 92148
#% 109206
#% 118756
#% 136733
#% 157135
#% 182423
#% 183255
#% 194283
#% 211794
#% 211820
#% 232653
#% 840583
#% 855048

#index 260001
#* Inductive learning algorithms and representations for text categorization
#@ Susan Dumais;John Platt;David Heckerman;Mehran Sahami
#t 1998
#c 1
#% 116149
#% 118731
#% 165110
#% 165111
#% 169718
#% 190581
#% 194283
#% 197387
#% 197394
#% 210986
#% 219052
#% 219053
#% 269218
#% 406493
#% 458379
#% 461692
#% 465746
#% 465754
#% 592108
#% 1650705

#index 260002
#* A new on-line learning algorithm for adaptive text filtering
#@ Kwok Leung Yu;Wai Lam
#t 1998
#c 1
#% 67056
#% 219053
#% 451055
#% 682442

#index 260003
#* Learning fuzzy knowledge from training examples
#@ Tzung-Pei Hong;Chai-Ying Lee
#t 1998
#c 1
#% 17631
#% 25443
#% 26348
#% 38530
#% 106651
#% 160873
#% 170741
#% 191031
#% 212710
#% 221731
#% 282854
#% 835738
#% 1788920

#index 260004
#* A database disk buffer management algorithm based on prefetching
#@ H. Seok Jeon;Sam H. Noh
#t 1998
#c 1
#% 1822
#% 4683
#% 77005
#% 83337
#% 86748
#% 152939
#% 152943
#% 159338
#% 201696
#% 202140
#% 212063
#% 214946
#% 232791
#% 442388
#% 462806
#% 480607
#% 480780
#% 480967
#% 481450
#% 979694

#index 260005
#* Continual computation policies for utility-directed prefetching
#@ Eric Horvitz
#t 1998
#c 1
#% 42214
#% 44876
#% 169803
#% 233808
#% 1275346
#% 1478774
#% 1650590
#% 1650593

#index 260006
#* BROOM: buffer replacement using online optimization by mining
#@ Anthony K. H. Tung;Y. C. Tay;Hongjun Lu
#t 1998
#c 1
#% 735
#% 32910
#% 65981
#% 86748
#% 152943
#% 244119
#% 459029
#% 481290
#% 481450

#index 260007
#* Associated biological information retrieval from distributed databases
#@ Mousheng Xu;Susan Gauch
#t 1998
#c 1
#% 114999
#% 115462
#% 194246
#% 213437
#% 237495

#index 260008
#* Clustering and singular value decomposition for approximate indexing in high dimensional spaces
#@ Alexander Thomasian;Vittorio Castelli;Chung-Sheng Li
#t 1998
#c 1
#% 8161
#% 201876
#% 227856
#% 227924
#% 237187
#% 252304
#% 359751
#% 381086
#% 481956

#index 260009
#* Static and dynamic information organization with star clusters
#@ Javed Aslam;Katya Pelekhov;Daniela Rus
#t 1998
#c 1
#% 3426
#% 11646
#% 36672
#% 46809
#% 54221
#% 67565
#% 109223
#% 111456
#% 119916
#% 144023
#% 157886
#% 197847
#% 217248
#% 218992
#% 228105
#% 232768
#% 275929
#% 375017
#% 649735
#% 649740

#index 260010
#* An efficient hierarchical scheme for locating highly mobile users
#@ Evaggelia Pitoura;Ioannis Fudos
#t 1998
#c 1
#% 102951
#% 179135
#% 194478
#% 248888
#% 384050
#% 403713
#% 675312
#% 1797801
#% 1830212
#% 1852662

#index 260011
#* Querying future telecommunication networks
#@ Béatrice Finance;Thierry Delot;Abdelghani Ridaoui
#t 1998
#c 1
#% 116185
#% 194984
#% 236416
#% 463919
#% 511738

#index 260012
#* Informia: a mediator for integrated access to heterogeneous information sources
#@ Maria Luisa Barja;Tore Bratvold;Jussi Myllymaki;Gabriele Sonnenberger
#t 1998
#c 1
#% 116303
#% 154361
#% 188853
#% 191446
#% 227992
#% 227993
#% 235914
#% 237303
#% 262134
#% 504755
#% 614579

#index 260013
#* Integrating information from multiple independently developed data sources
#@ I-Min A. Chen;Doron Rotem
#t 1998
#c 1
#% 52377
#% 75604
#% 116304
#% 169052
#% 462352
#% 462483
#% 462619
#% 464013
#% 464050
#% 464222
#% 481923
#% 481935

#index 260014
#* Supporting fast search in time series for movement patterns in multiple scales
#@ Yunyao Qu;Changzhou Wang;X. Sean Wang
#t 1998
#c 1
#% 30438
#% 172949
#% 227857
#% 460862
#% 461885
#% 481609
#% 481611
#% 534183

#index 260015
#* Attribute weighting: a method of applying domain knowledge in the decision tree process
#@ Caroline St. Clair;Chengwen Liu;Niki Pissinou
#t 1998
#c 1

#index 260016
#* MALM: a framework for mining sequence database at multiple abstraction levels
#@ Chung-Sheng Li;Philip S. Yu;Vittorio Castelli
#t 1998
#c 1
#% 140617
#% 172949
#% 201893
#% 227857
#% 236692
#% 452821
#% 460862
#% 461885
#% 464196
#% 477479
#% 481609
#% 481611
#% 661026
#% 663582

#index 260017
#* Using incremental pruning to increase the efficiency of dynamic itemset counting for mining association rules
#@ Jian Tang
#t 1998
#c 1
#% 201894
#% 227917
#% 227922
#% 240188
#% 443085
#% 443091
#% 481290
#% 481754
#% 481758
#% 481779

#index 260018
#* Efficient repeating pattern finding in music databases
#@ Jia-Lien Hsu;Arbee L. P. Chen;C.-C. Liu
#t 1998
#c 1
#% 166097
#% 194192
#% 219841
#% 245521
#% 434753
#% 435932
#% 452796
#% 463902
#% 481272
#% 614622
#% 641558
#% 641568

#index 260019
#* Image similarity retrieval by spatial constraints
#@ Dimitris Papadias;Nikos Mamoulis;Dimitris Meretakis
#t 1998
#c 1
#% 1145
#% 68183
#% 86950
#% 124680
#% 181409
#% 362928
#% 427199
#% 443054
#% 463595
#% 479620
#% 534159
#% 534160
#% 1275343

#index 260020
#* Multi-resolution indexing for shape images
#@ Tzi-cker Chiueh;Allen Ballman;Kevin Kreeger
#t 1998
#c 1
#% 93221
#% 102772
#% 151343
#% 173193
#% 196977
#% 201893
#% 257637
#% 359751
#% 437405
#% 437409
#% 463414
#% 481279

#index 260021
#* An automatic technique for detecting type conflicts in database schemes
#@ Luigi Palopoli;Domenico Saccá;Domenico Ursino
#t 1998
#c 1
#% 3938
#% 22948
#% 55294
#% 106916
#% 126330
#% 158907
#% 442861
#% 462048
#% 463544
#% 464717
#% 481923
#% 562650
#% 641044

#index 260022
#* SERF: schema evolution through an extensible, re-usable and flexible framework
#@ Kajal T. Claypool;Jing Jin;Elke A. Rundensteiner
#t 1998
#c 1
#% 23960
#% 32903
#% 57952
#% 235914
#% 264996
#% 442704
#% 443145
#% 458608
#% 463907
#% 481445
#% 481768
#% 487956
#% 511350
#% 555016
#% 647199
#% 677292

#index 260023
#* An incremental approach to schema integration by refining extensional relationships
#@ Ingo Schmitt;Can Türker
#t 1998
#c 1
#% 13048
#% 22948
#% 49594
#% 55294
#% 85086
#% 194968
#% 435102
#% 442917
#% 511913
#% 535518
#% 535820

#index 260024
#* An analysis of the structural validity of ternary relationships in entity relationship modeling
#@ James Dullea;Ii-Yeol Song
#t 1998
#c 1
#% 72552
#% 140389
#% 154969
#% 207730
#% 240211
#% 260024
#% 287631
#% 369136
#% 374518
#% 535021

#index 260025
#* Selectivity estimation of Window queries for line segment datasets
#@ Guido Proietti;Christos Faloutsos
#t 1998
#c 1
#% 13041
#% 86950
#% 137887
#% 153260
#% 164360
#% 172949
#% 252304
#% 285924
#% 435137
#% 481620
#% 481941

#index 260026
#* Iterated DFT based techniques for join size estimation
#@ Kamil Saraç;Ömer Eğecioǧlu;Amr El Abbadi
#t 1998
#c 1
#% 43163
#% 54047
#% 67552
#% 82346
#% 102784
#% 201921
#% 227857
#% 227924
#% 242366
#% 285924
#% 411554
#% 460862
#% 464062
#% 481266

#index 260027
#* An iterative approach for rules and data allocation in distributed deductive database systems
#@ Ladjel Bellatreche;Kamalakar Karlapalem;Qing Li
#t 1998
#c 1
#% 1451
#% 38688
#% 63691
#% 71544
#% 73005
#% 83933
#% 183944
#% 222444
#% 232098
#% 481441
#% 562340

#index 260028
#* FRAMBOISE—an approach to framework-based active database management system construction
#@ Hans Fritschi;Stella Gatziu;Klaus R. Dittrich
#t 1998
#c 1
#% 116575
#% 157281
#% 163437
#% 170602
#% 172964
#% 201973
#% 210178
#% 236446
#% 246000
#% 394417
#% 437791
#% 437792
#% 463739
#% 501937
#% 501939
#% 501956
#% 501959
#% 681316
#% 681393

#index 260029
#* A toggle transaction management technique for mobile multidatabases
#@ Ravi A. Dirckze;Le Gruenwald
#t 1998
#c 1
#% 77982
#% 191168
#% 245017
#% 403195
#% 463101
#% 660942
#% 679047

#index 260033
#* Dynamic restructuring of transactional workflow activities: a practical implementation method
#@ Tong Zhou;Calton Pu;Ling Liu
#t 1998
#c 1
#% 86939
#% 114706
#% 116077
#% 122904
#% 122911
#% 138580
#% 185412
#% 202156
#% 209568
#% 214021
#% 221390
#% 340317
#% 403195
#% 461900
#% 462220
#% 464211
#% 480259
#% 481596
#% 481752
#% 535525
#% 596250
#% 614585
#% 660970
#% 704797

#index 260035
#* Triple-node hierarchies for object-oriented database indexing
#@ Frank Hing-Wah Luk;Ada Waichee Fu
#t 1998
#c 1
#% 18614
#% 57955
#% 83148
#% 86954
#% 109178
#% 116056
#% 116091
#% 199545
#% 201891
#% 260035
#% 286189
#% 442665
#% 442941
#% 462798
#% 462936
#% 463603
#% 463720
#% 481276
#% 481417
#% 481435
#% 481449

#index 260036
#* Sibling clustering of tree-based spatial indexes for efficient spatial query processing
#@ Kihong Kim;Sang K. Cha
#t 1998
#c 1
#% 47621
#% 68091
#% 86950
#% 102746
#% 116057
#% 152937
#% 153260
#% 172948
#% 201876
#% 201880
#% 210174
#% 214719
#% 244119
#% 252608
#% 317933
#% 403195
#% 411694
#% 427199
#% 435141
#% 442669
#% 443105
#% 462239
#% 463264
#% 479453
#% 479472
#% 480093
#% 481295
#% 481455
#% 481956
#% 482090

#index 260038
#* Layered index structures in document database systems
#@ Yangjun Chen;Karl Aberer
#t 1998
#c 1
#% 1921
#% 54451
#% 71755
#% 115465
#% 115466
#% 115467
#% 118744
#% 118760
#% 125595
#% 172329
#% 252608
#% 395073
#% 406493
#% 463740
#% 464230
#% 481125
#% 481418
#% 571098

#index 260039
#* Temporal granularity for unanchored temporal data
#@ Iqbal A. Goralwalla;Yuri Leontiev;M. Tamer Özsu;Duane Szafron;Carlo Combi
#t 1998
#c 1
#% 105467
#% 131548
#% 225003
#% 240149
#% 361445
#% 435153
#% 442919
#% 463572
#% 707025
#% 1778424

#index 260041
#* Handling temporal grouping and pattern-matching queries in a temporal object model
#@ Marlon Dumas;Marie-Christine Fauvet;Pierre-Claude Scholl
#t 1998
#c 1
#% 5171
#% 101955
#% 168773
#% 172369
#% 172949
#% 186970
#% 240156
#% 361445
#% 482093
#% 494020
#% 535378
#% 562167
#% 562359
#% 587413

#index 260042
#* Proceedings of the 6th ACM international symposium on Advances in geographic information systems
#@ Kia Makki;Niki Pissinou;Robert Laurini
#t 1998
#c 1

#index 260043
#* Modeling time from a conceptual perspective
#@ Stefano Spaccapietra;Christine Parent;Esteban Zimanyi
#t 1998
#c 1
#% 135384
#% 163438
#% 168773
#% 234903
#% 235914
#% 262249
#% 319244
#% 361445
#% 421031
#% 463090
#% 463265
#% 467630
#% 487970

#index 260044
#* Providing semantics for indefinite deductive databases
#@ Sei Chun;Jonghoon Chun
#t 1998
#c 1
#% 33376
#% 53385
#% 53388
#% 103705
#% 169288
#% 277342
#% 295977
#% 556918
#% 837647

#index 264951
#* Proceedings of the 1st ACM international workshop on Data warehousing and OLAP
#@ Il-Yeol Song;Toby J. Teorey
#t 1998
#c 1

#index 287191
#* Searching the web (keynote address): can you find what you want?
#@ C. Lee Giles
#t 1999
#c 1
#! The World Wide Web has revolutionized communication and information distribution, storage, and access. Its impact has been felt everywhere - e.g. science and technology, commerce and business, education, government, religion, law, entertainment, health care. Even so, there are many ways the web can be improved. We discuss what the web consists of and how it has changed, what is the size of the web, and what is covered. Results for the publicly indexable web show that the web though Terabytes in size and growing is still less than large commercial databases and the Library of Congress. Though the web started out as an academic-government endeavor, it is now primarily commerce. Furthermore, the major web search engines cover only a fraction of the publicly indexable web and appear to base their indexing strategy on the popularity of information. Since current search on the web is primarily done with the search engines, what would be the economic, political and scientific implications of these results?

#index 287192
#* A practitioner's view of techniques used in data warehousing for sifting through data to provide information (keynote address)
#@ Jim Scoggins
#t 1999
#c 1
#! Over the past 10 years data warehousing evolved from providing 'nice to know' data to 'need to know' information. The decision support systems providing summarized reports to executives have advanced to integrated information factories providing vital information to the desktops of knowledge workers. Data warehousing has benefited through the use advanced techniques of sifting through data to produce information. This discussion will cover data mining techniques used in specific business cases as well as attempt to describe problems that still exist (and could be researched) in the business intelligence arena.

#index 287193
#* Simple QSF-trees: an efficient and scalable spatial access method
#@ Byunggu Yu;Ratko Orlandic;Martha Evens
#t 1999
#c 1
#% 2692
#% 32898
#% 32913
#% 77928
#% 83319
#% 86950
#% 201878
#% 201880
#% 252304
#% 278609
#% 285932
#% 411694
#% 415957
#% 427199
#% 442768
#% 462503
#% 464195
#% 480093
#% 481455
#% 481956
#% 526864
#% 566113
#! The development of high-performance spatial access methods that can support complex operations of large spatial databases continues to attract considerable attention. This paper introduces QSF-trees, an efficient and scalable structure for indexing spatial objects, which has some important advantages over R*-trees. QSF-trees eliminate overlapping of index regions without forcing object clipping or sacrificing the selectivity of spatial operations. The method exploits the semantics of topological relations between spatial objects to further reduce the number of index nodes visited during the search. A series of experiments involving randomly-generated spatial objects was conducted to compare the structure with two variations of R*-trees. The experiments show QSF-trees to be more efficient and more scalable to the increase in the data-set size, the size of spatial objects, and the number of dimensions of the spatial universe.

#index 287194
#* Transformation-based spatial join
#@ Ju-Won Song;Kyu-Young Whang;Young-Koo Lee;Min-Jae Lee;Sang-Wook Kim
#t 1999
#c 1
#% 13041
#% 68091
#% 77928
#% 83319
#% 86950
#% 152937
#% 172908
#% 172909
#% 210186
#% 210187
#% 285932
#% 435124
#% 435137
#% 443303
#% 462041
#% 463595
#% 479453
#% 481286
#% 510675
#% 526864
#% 566113
#! Spatial join finds pairs of spatial objects having a specific spatial relationship in spatial database systems. A number of spatial join algorithms have recently been proposed in the literature. Most of them, however, perform the join in the original space. Joining in the original space has a drawback of dealing with sizes of objects and thus has difficulty in developing a formal algorithm that does not rely on heuristics. In this paper, we propose a spatial join algorithm based on the transformation technique. An object having a size in the two-dimensional original space is transformed into a point in the four-dimensional transform space, and the join is performed on these point objects. This can be easily extended to n-dimensional cases. We show the excellence of the proposed approach through analysis and extensive experiments. The results show that the proposed algorithm has a performance generally better than that of the R*-based algorithm proposed by Brinkhoff et al. This is a strong indicating that corner transformation preserves clustering among objects and that spatial operations can be performed better in the transform space than in the original space. This reverses the common belief that transformation will adversely affect clustering. We believe that our result will provide a new insight towards transformation-based spatial query processing.

#index 287195
#* Binary string relations: a foundation for spatiotemporal knowledge representation
#@ Delis Vasilis;Hadzilacos Thanasis
#t 1999
#c 1
#% 108
#% 399
#% 23998
#% 31919
#% 82720
#% 84513
#% 116335
#% 319244
#% 320265
#% 407995
#% 435140
#% 443054
#% 479620
#% 481283
#% 546592
#! The paper is concerned with the qualitative representation of spatiotemporal relations. We initially propose a multiresolution framework for the representation of relations among 1D intervals, based on a binary string encoding. We subsequently extend this framework to multiple dimensions, thus allowing the description of spatiotemporal relations at various contexts. The feasible relations at a particular resolution level are inherently permeated by a poset structure, called conceptual neighbourhood, upon which we propose efficient relation inferencing mechanisms. Finally, we discuss the application of our model to spatiotemporal reasoning, which refers to the classic problems of satisfiability and deductive closure of a set of spatiotemporal assertions.

#index 287196
#* Extracting significant time varying features from text
#@ Russell Swan;James Allan
#t 1999
#c 1
#% 142400
#% 214715
#% 280849
#% 677173
#% 815335
#! We propose a simple statistical model for the frequency of occurrence of features in a stream of text. Adoption of this model allows us to use classical significance tests to filter the stream for interesting events. We tested the model by building a system and running it on a news corpus. By a subjective evaluation, the system worked remarkably well: almost all of the groups of identified tokens corresponded to news stories and were appropriately placed in time. A preliminary objective evaluation was also used to measure the quality of the system and it showed some of the weaknesses and the power of our approach.

#index 287197
#* Training a selection function for extraction
#@ Chin-Yew Lin
#t 1999
#c 1
#% 55490
#% 71752
#% 109220
#% 136350
#% 146789
#% 169770
#% 194251
#% 194252
#% 198294
#% 230530
#% 266370
#% 288614
#% 382141
#% 705920
#% 708199
#% 708427
#% 742437
#% 747887
#% 747994
#! In this paper we compare performance of several heuristics in generating informative generic/query-oriented extracts for newspaper articles in order to learn how topic prominence affects the performance of each heuristic. We study how different query types can affect the performance of each heuristic and discuss the possibility of using machine learning algorithms to automatically learn good combination functions to combine several heuristics. We also briefly describe the design, implementation, and performance of a multilingual text summarization system SUMMARIST.

#index 287198
#* Ready for prime time: pre-generation of web pages in TIScover
#@ Birgit Pröll;Heinrich Starck;Werner Retschitzegger;Harald Sighart
#t 1999
#c 1
#% 227995
#% 261737
#% 261741
#% 278608
#% 406493
#% 458746
#% 504565
#% 565262
#! In large data- and access-intensive web sites, efficient and reliable access is hard to achieve. This situation gets even worse for web sites providing precise structured query facilities and requiring topicality of the presented information even in face of a highly dynamic content. The achievement of these partly conflicting goals is strongly influenced by the approach chosen for page generation, ranging from composing a web page upon a user's request to its generation in advance. The official Austrian web-based tourism information and booking system TIScover tries to reconcile these goals by employing a hybrid approach of page generation. In TIScover, web pages are not only generated on request in order to support precise structured queries on the content managed by a database system. Rather, the whole web site is also pre-generated out of the extremely dynamic content and synchronized with the database on the basis of metadata. Thus, topicality of information is guaranteed, while ensuring efficient and reliable access. This paper discusses the hybrid approach as realized in TIScover, focussing in particular on the concepts used for pre-generation.1

#index 287199
#* Local replication for proxy web caches with hash routing
#@ Kun-Lung Wu;Philip S. Yu
#t 1999
#c 1
#% 36103
#% 65721
#% 176499
#% 232779
#% 256883
#% 268093
#% 437442
#% 444336
#% 679349
#% 979357
#% 1862985
#! This paper studies controlled local replication for hash routing, such as CARP, among a collection of loosely-coupled proxy web cache servers. Hash routing partitions the entire URL space among the shared web caches, creating a single logical cache. Each partition is assigned to a cache server. Duplication of cache contents is eliminated and total incoming traffic to the shared web caches is minimized. Client requests for non-assigned-partition objects are forwarded to sibling caches. However, request forwarding increases not only inter-cache traffic but also cpu utilization, thus slows the client response time. We propose a controlled local replication of non-assigned-partition objects in each cache server to effectively reduce the inter-cache traffic. We use a multiple-exit LRU to implement controlled local replication. Trace-driven simulations are conducted to study the performance impact of local replication. The results show that (1) regardless of cache sizes, with a controlled local replication, the average response time, inter-cache traffic and CPU overhead can be effectively reduced without noticeable increases in incoming traffic; (2) for very large cache sizes, a larger amount of local replication can be allowed to reduce inter-cache traffic without increasing incoming traffic; and (3) local replication is effective even if clients are dynamically assigned to different cache servers.

#index 287200
#* Semantic caching via query matching for web sources
#@ Dongwon Lee;Wesley W. Chu
#t 1999
#c 1
#% 36683
#% 44638
#% 77005
#% 166821
#% 169844
#% 198465
#% 209634
#% 210176
#% 261741
#% 273707
#% 443052
#% 464203
#% 481108
#% 481916
#% 482116
#% 497938
#% 571216
#% 599549
#! A semantic caching scheme suitable for wrappers wrapping web sources is presented. Since the web sources have typically weaker querying capabilities than conventional databases, existing semantic caching schemes cannot be applied directly. A seamlessly integrated query translation and capability mapping between the wrappers and web sources in semantic caching is described. In addition, an analysis on the match types between the user's input query and cached queries is presented. Semantic knowledge acquired from the data can be used to avoid unnecessary access to the web sources by transforming the cache miss to the cache hit. A polynomial time algorithm based on the proposed query matching technique is presented to find the best matched query in the cache. Experimental results reveal the effectiveness of the proposed semantic caching scheme.

#index 287201
#* Automatically extracting structure and data from business reports
#@ Stephen W. Liddle;Douglas M. Campbell;Chad Crawford
#t 1999
#c 1
#% 3888
#% 24522
#% 90659
#% 240955
#% 244103
#% 248808
#% 259991
#% 288885
#% 300288
#% 535703
#! A considerable amount of clean semistructured data is internally available to companies in the form of business reports. However, business reports are untapped for data mining, data warehousing, and querying because they are not in relational form. Business reports have a regular structure that can be reconstructed. We present algorithms that automatically infer the regular structure underlying business reports and automatically generate wrappers to extract relational data.

#index 287202
#* Extracting semi-structured data through examples
#@ Berthier Ribeiro-Neto;Alberto H. F. Laender;Altigran S. da Silva
#t 1999
#c 1
#% 169809
#% 210985
#% 227987
#% 232677
#% 237191
#% 237194
#% 237328
#% 244103
#% 248808
#% 259991
#% 387427
#% 511897
#% 535703
#% 617190
#! In this paper, we describe an innovative approach to extracting semi-structured data from Web sources. The idea is to collect a couple of example objects from the user and to use this information to extract new objects from new pages or texts. To perform the extraction of new objects, we introduce a bottom-up extration strategy and, through experimentation, demonstrate that it works quite effectively with distinct Web sources, even if only a few examples are provided by the user.

#index 287203
#* Discovering quasi-equivalence relationships from database systems
#@ Mei-Ling Shyu;Shu-Ching Chen;R. L. Kashyap
#t 1999
#c 1
#% 55294
#% 111059
#% 152934
#% 232106
#% 443082
#% 443206
#% 443432
#! Association rule mining has recently attracted strong attention and proven to be a highly successful technique for extracting useful information from very large databases. In this paper, we explore a generalized affinity-based association mining which discovers quasi-equivalent media objects in a distributed information-providing environment consisting of a network of heterogeneous databases which could be relational databases, hierarchical databases, object-oriented databases, multimedia databases, etc. Online databases, consisting of millions of media objects, have been used in business management, government administration, scientific and engineering data management, and many other applications owing to the recent advances in high-speed communication networks and large-capacity storage devices. Because of the navigational characteristic, queries in such an information-providing environment tend to traverse equivalent media objects residing in different databases for the related data records. As the number of databases increases, query processing efficiency depends heavily on the capability to discover the equivalence relationships of the media objects from the network of databases. Theoretical terms along with an empirical study of real databases are presented.

#index 287204
#* Task-oriented world wide web retrieval by document type classification
#@ Katsushi Matsuda;Toshikazu Fukushima
#t 1999
#c 1
#% 165110
#% 194283
#% 219053
#% 253188
#% 255161
#% 266215
#% 676170
#! This paper proposes a novel approach to accurately searching Web pages for relevant information in problem solving by specifying a Web document category instead of the user's task. Accessing information from World Wide Web pages as an approach to problem solving has become commonplace. However, such a search is difficult with current search services, since these services only provide keyword-based search methods that are equivalent to narrowing down the target references according to domains. However, problem solving usually involves both a domain and a task. Accordingly, our approach is based on problem solving tasks. To specify a user's problem solving task, we introduce the concept of document types that directly relate to the problem solving tasks; with this approach, users can easily designate problem solving tasks. We implemented PageTypeSearch system based on our approach. Classifier of PageTypeSearch classifies Web pages into the document types by comparing their pages with typical structural characteristics of the types. We compare PageTypeSearch using the document typeindices with a conventional keyword-based search system in experiments. The average precision of the document type-based search is 88.9%, while the average precision of the keyword-based search is 31.2%. Moreover, the number of irrelevant references gathered by our system is about one-thirteenth that of traditional keyword-based search systems. Our approach has practical advantages for problem solving by introducing the viewpoint of tasks to achieve higher performance.

#index 287205
#* Classification algorithms for NETNEWS articles
#@ Wen-Lin Hsu;Sheau-Dong Lang
#t 1999
#c 1
#% 46803
#% 46809
#% 115462
#% 169718
#% 172922
#% 194285
#% 203770
#% 219052
#% 260001
#% 262050
#% 443093
#% 444874
#% 458379
#% 479621
#% 481439
#% 840583
#! We propose several algorithms using the vector space model to classify the news articles posted on the NETNEWS according to the newsgroup categories. The baseline method combines the terms of all the articles of each newsgroup in the training set to represent the newsgroups as single vectors. After training, the incoming news articles are classified based on their similarity to the existing newsgroup categories. We propose to use the following techniques to improve the classification performance of the baseline method: (1) use routing (classification) accuracy and the similarity values to refine the training set; (2) update the underlying term structures periodically during testing; and (3) apply k-means clustering to partition the newsgroup articles and represent each newsgroup by k vectors. Our test collection consists of the real news articles and the 519 subnewsgroups under the REC newsgroup of NETNEWS in a period of 3 months. Our experimental results demonstrate that the technique of refining the training set reduces from one-third to two-thirds of the storage. The technique of periodical updates improves the routing accuracy ranging from 20% to 100% but incurs runtime overhead. Finally, representing each newsgroup by k vectors (with k = 2 or 3) using clustering yields the most significant improvement in routing accuracy, ranging from 60% to 100%, while causing only slightly higher storage requirements.

#index 287206
#* Text classification using ESC-based stochastic decision lists
#@ Hang Li;Kenji Yamanishi
#t 1999
#c 1
#% 131686
#% 165110
#% 190581
#% 194283
#% 219052
#% 260001
#% 262085
#% 458379
#% 465747
#% 465754
#% 746868
#% 1808676
#% 1809407
#! We propose a new method of text classification using stochastic decision lists. A stochastic decision list is an ordered sequence of IF-THEN rules, and our method can be viewed as a rule-based method for text classification having advantages of readability and refinability of acquired knowledge. Our method is unique in that decision lists are automatically constructed on the basis of the principle of minimizing Extended Stochastic Complexity (ESC), and with it we are able to construct decision lists that have fewer errors in classification. The accuracy of classification achieved with our method appears better than or comparable to those of existing rule-based methods.

#index 287207
#* Database model for web-based cooperative applications
#@ Waldemar Wieczerzycki
#t 1999
#c 1
#% 1486
#% 32897
#% 86478
#% 86938
#% 111351
#% 122904
#% 257348
#% 479776
#% 562665
#! In this paper we propose a model of a database that could become a kernel of cooperative database applications. First, we propose a new data model CDM (Collaborative Data Model) that is oriented for the specificity of multiuser environments, in particular: cooperation scenarios, cooperation techniques and cooperation management. Second, we propose to apply to databases supporting collaboration so called multiuser transactions. Multiuser transactions are flat transactions in which, in comparison to classical ACID transactions, the isolation property is relaxed.

#index 287209
#* Indexing and retrieval of scientific literature
#@ Steve Lawrence;Kurt Bollacker;C. Lee Giles
#t 1999
#c 1
#% 86532
#% 168512
#% 172922
#% 201935
#% 220708
#% 237316
#% 249143
#% 252750
#% 255137
#% 262036
#% 262061
#% 268079
#% 268087
#% 281366
#% 281446
#% 282905
#% 290703
#% 433674
#% 466250
#% 481439
#! The web has greatly improved access to scientific literature. However, scientific articles on the web are largely disorganized, with research articles being spread across archive sites, institution sites, journal sites, and researcher homepages. No index covers all of the available literature, and the major web search engines typically do not index the content of Postscript/PDF documents at all. This paper discusses the creation of digital libraries of scientific literature on the web, including the efficient location of articles, full-text indexing of the articles, autonomous citation indexing, information extraction, display of query-sensitive summaries and citation context, hubs and authorities computation, similar document detection, user profiling, distributed error correction, graph analysis, and detection of overlapping documents. The software for the system is available at no cost for non-commercial use.

#index 287210
#* Metadata and data structures for the historical newspaper digital library
#@ Robert B. Allen;John Schalow
#t 1999
#c 1
#% 238194
#! We examine metadata and data-structure issues for the Historical Newspaper Digital Library. This project proposes to digitize and then do OCR and linguisting processing on several years worth of historical newspapers. Newspapers are very complex information objects so developing a rich description of their content is challenging. In addition to frameworks for the logical structure and physical layout, we propose metadata relevant to the image processing and to the historians who will use this collection. Finally, we consider how the metadata infrastructure might be managed as it evolves with improved text processing capabilities and how an infrastructure might be developed to support a community of users.

#index 287211
#* A horizontal fragmentation algorithm for the fact relation in a distributed data warehouse
#@ Amin Y. Noaman;Ken Barker
#t 1999
#c 1
#% 121
#% 83933
#% 184208
#% 189984
#% 223781
#% 237198
#% 282431
#! Data warehousing is one of the major research topics of appliedside database investigators. Most of the work to date has focused on building large centralized systems that are integrated repositories founded on pre-existing systems upon which all corporate-wide data are based. Unfortunately, this approach is very expensive and tends to ignore the advantages realized during the past decade in the area of distribution and support for data localization in a geographically dispersed corporate structure. This research investigates building distributed data warehouses with particular emphasis placed on distribution design for the data warehouse environment. The article provides an architectural model for a distributed data warehouse, the formal definition of the relational data model for data warehouse and a methodology for distributed data warehouse design along with a “horizontal” fragmentation algorithm for the fact relation.

#index 287212
#* Requirement-based data cube schema design
#@ David W. Cheung;Bo Zhou;Ben Kao;Hongjun Lu;Tak Wah Lam;Hing Fung Ting
#t 1999
#c 1
#% 191154
#% 210182
#% 223781
#% 227861
#% 227880
#% 464215
#% 479646
#% 481604
#% 481951
#! On-line analytical processing (OLAP) requires efficient processing of complex decision support queries over very large databases. It is well accepted that pre-computed data cubes can help reduce the response time of such queries dramatically. A very important design issue of an efficient OLAP system is therefore the choice of the right data cubes to materialize. We call this problem the data cube schema design problem. In this paper we show that the problem of finding an optimal data cube schema for an OLAP system with limited memory is NP-hard. As a more computationally efficient alternative, we propose a greedy approximation algorithm cMP and its variants. Algorithm cMP consists of two phases. In the first phase, an initial schema consisting of all the cubes required to efficiently answer the user queries is formed. In the second phase, cubes in the initial schema are selectively merged to satisfy the memory constraint. We show that cMP is very effective in pruning the search space for an optimal schema. This leads to a highly efficient algorithm. We report the efficiency and the effectiveness of cMP via an empirical study using the TPC-D benchmark. Our results show that the data cube schemas generated by cMP enable very efficient OLAP query processing.

#index 287213
#* Extending complex ad-hoc OLAP
#@ Theodore Johnson;Damianos Chatziantoniou
#t 1999
#c 1
#% 102748
#% 111912
#% 136740
#% 140389
#% 191175
#% 201883
#% 214667
#% 227934
#% 248807
#% 248813
#% 248816
#% 287664
#% 380546
#% 461897
#% 461921
#% 464215
#% 479460
#% 479618
#% 481288
#% 481608
#% 481951
#% 482082
#% 482093
#% 504020
#% 632007
#! Large scale data analysis and mining activities require sophisticated information extraction queries. Many queries require complex aggregation, and many of these aggregates are non-distributive. Conventional solutions to this problem involve defining User Defined Aggregate Functions (UDAFs). However, the use of UDAFs entails several problems. Defining a new UDAF can be a significant burden for the user, and optimizing queries involving UDAFs is difficult because of the “black box” nature of the UDAF.In this paper, we present a method for expressing nested aggregates in a declarative way. A nested aggregate, which is a rollup of another aggregated value, expresses a wide range of useful non-distributive aggregation. For example, most frequent type aggregation can be naturally expressed using nested aggregation, e.g. “For each product, report its total sales during the month with the largest total sales of the product”. By expressing compex aggregates declaratively, we relieve the user of the burden of defining UDAFs, and allow the evalution of the complex aggregates to be optimized.We use the Extended Multi-Feature (EMF) syntax as the basis for expressing nested aggregation. An advantage of this approach is that EMF SQL can already express a wide range of complex aggregation in a succinct way, and EMF SQL is easily optimized into efficient query plans. We show that nested aggregation queries can be evaluated efficiently by using a small extension to the EMF SQL query evaluation algorithm. A side effect of this extension is to extend EMF SQL to permit complex aggregation of data from multiple sources.

#index 287214
#* Yahoo! as an ontology: using Yahoo! categories to describe documents
#@ Yannis Labrou;Tim Finin
#t 1999
#c 1
#% 212361
#% 458389
#% 465747
#% 560829
#! We suggest that one (or a collection) of names of Yahoo! (or any other WWW indexer's) categories can be used to describe the content of a document. Such categories offer a standardized and universal way for referring to or describing the nature of real world objects, activities, documents and so on, and may be used (we suggest) to semantically characterize the content of documents. WWW indices, like Yahoo! provide a huge hierarchy of categories (topics) that touch every aspect of human endeavors. Such topics can be used as descriptors, similarly to the way librarians use for example, the Library of Congress cataloging system to annotate and categorize books.In the course of investigating this idea, we address the problem of automatic categorization of webpages in the Yahoo! directory. We use Telltale as our classifier; Telltale uses n-grams to compute the similarity between documents. We experiment with various types of descriptions for the Yahoo! categories and the webpages to be categorized. Our findings suggest that the best results occur when using the very brief descriptions of the Yahoo! categorized entries; these brief descriptions are provided either by the entries' submitters or by the Yahoo! human indexers and accompany most Yahoo!-indexed entries.

#index 287215
#* Browsing large digital library collections using classification hierarchies
#@ S. Geffner;D. Agrawal;A. El Abbadi;T. Smith
#t 1999
#c 1
#% 118773
#% 127860
#% 185268
#% 223782
#% 227866
#% 227927
#% 234233
#% 441058
#% 461921
#% 464215
#% 479659
#% 584899
#% 631947
#% 670380
#! Summarization of intermediary query result sets plays an important role when users browse through digital library collections. Summarization enables users to quickly digest the results of their queries, and provides users with important information they can use to narrow their search interactively. Techniques from the field of data analysis may be applied to the problem of generating summaries of query results efficiently. Such techniques should permit the incorporation of classification hierarchies in order to provide powerful browsing environments for digital library users.

#index 287216
#* ZBroker: a query routing broker for Z39.50 databases
#@ Yong Lin;Jian Xu;Ee-Peng Lim;Wee-Keong Ng
#t 1999
#c 1
#% 67565
#% 194246
#% 227891
#% 237303
#% 262065
#% 287463
#% 567255
#! A query routing broker is a software agent that determines from a large set of accessing information sources the ones most relevant to a user's information need. As the number of information sources on the Internet increases dramatically, future users will have to rely on query routing brokers to decide a small number of information sources to query without incurring too much query processing overheads. In this paper, we describe a query routing broker known as ZBroker developed for bibliographic database servers that support the Z39.50 protocol. ZBroker samples the content of each bibliographic database by using training queries and their results, and summarizes the bibliographic database content into a knowledge base. We present the design and implementation of ZBroker and describe its Web-based user interface.

#index 287217
#* Architecture of a metasearch engine that supports user information needs
#@ Eric J. Glover;Steve Lawrence;William P. Birmingham;C. Lee Giles
#t 1999
#c 1
#% 83855
#% 249190
#% 268078
#% 433674
#% 433677
#! When a query is submitted to a metasearch engine, decisions are made with respect to the underlying search engines to be used, what modifications will be made to the query, and how to score the results. These decisions are typically made by considering only the user's keyword query, neglecting the larger information need. Users with specific needs, such as “research papers” or “homepages,” are not able to express these needs in a way that affects the decisions made by the metasearch engine. In this paper, we describe a metasearch engine architecture that considers the user's information need for each decision. Users with different needs, but the same keyword query, may search different sub-search engines, have different modifications made to their query, and have results ordered differently. Our architecture combines several powerful approaches together in a single general purpose metasearch engine.

#index 287218
#* Efficient mining of association rules in text databases
#@ John D. Holt;Soon M. Chung
#t 1999
#c 1
#% 67565
#% 227917
#% 251514
#% 443082
#% 443164
#% 481290
#% 481754
#% 481779
#% 678196
#! In this paper, we propose two new algorithms for mining association rules between words in text databases. The characteristics of text databases are quite different from those of retail transaction databases, and existing mining algorithms cannot handle text databases efficiently because of the large number of itemsets (i.e., words) that need to be counted. Two well-known mining algorithms, Apriori algorithm and Direct Hashing and Pruning (DHP) algorithm, are evaluated in the context of mining text databases, and are compared with the new proposed algorithms named Multipass-Apriori (M-Apriori) and Multipass-DHP (M-DHP). It has been shown that the proposed algorithms have better performance for large text databases.

#index 287237
#* Efficient and effective metasearch for a large number of text databases
#@ Clement Yu;Weiyi Meng;King-Lup Liu;Wensheng Wu;Naphtali Rishe
#t 1999
#c 1
#% 176501
#% 194246
#% 194275
#% 232701
#% 245788
#% 262063
#% 406493
#% 479642
#% 481748
#% 567255
#% 584914
#% 631997
#% 978381
#! Metasearch engines can be used to facilitate ordinary users for retrieving information from multiple local sources (text databases). In a metasearch engine, the contents of each local database is represented by a representative. Each user query is evaluated against the set of representatives of all databases in order to determine the appropriate databases to search. When the number of databases is very large, say in the order of tens of thousands or more, then a traditional metasearch engine may become inefficient as each query needs to be evaluated against too many database representatives. Furthermore, the storage requirement on the site containing the metasearch engine can be very large. In this paper, we propose to use a hierarchy of database representatives to improve the efficiency. We provide an algorithm to search the hierarchy. We show that the retrieval effectiveness of our algorithm is the same as that of evaluating the user query against all database representatives. We also show that our algorithm is efficient. In addition, we propose an alternative way of allocating representatives to sites so that the storage burden on the site containing the metasearch engine is much reduced.

#index 287239
#* Mining inter-transaction associations with templates
#@ Ling Feng;Hongjun Lu;Jeffrey Xu Yu;Jiawei Han
#t 1999
#c 1
#% 152934
#% 213963
#% 236696
#% 248784
#% 248785
#% 481290
#% 481954
#! Multi-dimensional, inter-transaction association rules extend the traditional association rules to describe more general associations among items with multiple properties cross transactions. “After McDonald and Burger King open branches, KFC will open a branch two months later and one mile away” is an example of such rules. Since the number of potential inter-transaction association rules tends to be extremely large, mining inter-transaction associations poses more challenges on efficient processing than mining intra-transaction associations. In order to make such association mining truly practical and computationally tractable, in this study, we present a template model to help users declare the interesting inter-transaction associations to be mined. With the guidance of templates, several optimization techniques are devised to speed up the discovery of inter-transaction association rules. We show, through a series of experiments, that these optimization techniques can yield significant performance benefits.

#index 287240
#* Using domain knowledge in knowledge discovery
#@ Suk-Chung Yoon;Lawrence J. Henschen;E. K. Park;Sam Makki
#t 1999
#c 1
#% 877
#% 16798
#% 36683
#% 69272
#% 82258
#% 111378
#% 116203
#% 125595
#% 152934
#% 172327
#% 172361
#% 172386
#% 199580
#% 201894
#% 232146
#% 318050
#% 395669
#% 412588
#% 452747
#% 452822
#% 463903
#! With the explosive growth of the size of databases, many knowledge discovery applications deal with large quantities of data. There is an urgent need to develop methodologies which will allow the applications to focus search to a potentially interesting and relevant portion of the data, which can reduce the computational complexity of the knowledge discovery process and improve the interestingness of discovered knowledge. Previous work on semantic query optimization, which is an approach to take advantage of domain knowledge for query optimization, has demonstrated that significant cost reduction can be achieved by reformulating a query into a less expensive yet equivalent query which produces the same answer as the original one. In this paper, we introduce a method to utilize three types of domain knowledge in reducing the cost of finding a potentially interesting and relevant portion of the data while improving the quality of discovered knowledge. In addition, we propose a method to select relevant domain knowledge without an exhaustive search of all domain knowledge. The contribution of this paper is that we lay out a general framework for using domain knowledge in the knowledge discovery process effectively by providing guidelines.

#index 287242
#* Incremental and interactive sequence mining
#@ S. Parthasarathy;M. J. Zaki;M. Ogihara;S. Dwarkadas
#t 1999
#c 1
#% 172386
#% 236697
#% 248785
#% 259993
#% 420063
#% 459006
#% 462238
#% 464204
#! The discovery of frequent sequences in temporal databases is an important data mining problem. Most current work assumes that the database is static, and a database update requires rediscovering all the patterns by scanning the entire old and new database. In this paper, we propose novel techniques for maintaining sequences in the presence of a) database updates, and b) user interaction (e.g. modifying mining parameters). This is a very challenging task, since such updates can invalidate existing sequences or introduce new ones. In both the above scenarios, we avoid re-executing the algorithm on the entire dataset, thereby reducing execution time. Experimental results confirm that our approach results in execution time improvements of up to several orders of magnitude in practice.

#index 287243
#* SemQL: a semantic query language for multidatabase systems
#@ Jeong-Oog Lee;Doo-Kwon Baik
#t 1999
#c 1
#% 111912
#% 194968
#% 207803
#% 227886
#% 232707
#% 535820
#! An essential prerequisite to achieving interoperability in multidatabase systems is to be able to identify semantically equivalent or related data items in component databases. Another problem in multidatabase systems is allowing users to handle information from different databases that refer to the same realworld entity. In this paper, we provide semantic networks so that multidatabase systems can detect and resolve semantic heterogeneities among component databases. And we provide a semantic query language, SemQL, to capture the concepts about what users want. It enables users to issue queries to a large number of autonomous databases without prior knowledge of their schemas.

#index 287244
#* Rule-based query optimization, revisited
#@ Lane B. Warshaw;Daniel P. Miranker
#t 1999
#c 1
#% 241
#% 23011
#% 32889
#% 51207
#% 116043
#% 168251
#% 199776
#% 210203
#% 214602
#% 214787
#% 248789
#% 248793
#% 463117
#% 463886
#% 479467
#% 480759
#% 564428
#% 565457
#% 690973
#! We present the architecture and a performance assessment of an extensible query optimizer written in Venus. Venus is a general-purpose active-database rule language embedded in C++. Following the developments in extensible database query optimizers, first in rule-based form, followed by optimizers written as object-oriented programs, the Venus-based optimizer avails to the advantages of both. Venus' modular structure allows us to go a step further and provide extensibility in search by defining parameterized search components in a declarative form that has the additional effect of integrating heuristic and cost-based optimization. We compare optimizers developed with Volcano, OPT++ and Venus. Venus' optimizing compiler yields code whose performance is comparable with Volcano and OPT++ on smaller queries. The ability to introduce additional pruning heuristics yields better scalability on larger queries. Evaluation of the system using quantitative software metrics supports a claim that the Venus-based optimizer is more easily maintained and extended than are its predecessors.

#index 287245
#* Page access scheduling in join processing
#@ Andrew Lim;Jennifer Lai-Pheng Kwan;Wee-Chong Oon
#t 1999
#c 1
#% 1388
#% 51375
#% 114577
#% 136740
#% 237380
#% 411554
#% 442675
#% 443180
#! The join relational operation is one of the most expensive among database operations. In this study, we consider the problem of scheduling page accesses in join processing. This raises two interesting problems: 1) determining a page access sequence that uses the minimum number of buffer pages without any page reaccesses, and 2) determining a page access sequence that minimizes the number of page reaccesses for a given buffer size. We use a graph model to represent the pages from the relations that contain tuples to be joined, and present new heuristics for the two problems based on the sort-merge join and the simple TID algorithm. Our experimental results show that the new heuristics perform well.

#index 287247
#* Queryable acyclic production systems
#@ David Tanzer;Dennis Shasha
#t 1999
#c 1
#% 16185
#% 36683
#% 145194
#% 190332
#% 247425
#% 252363
#% 707531
#! We pose a query problem about the behavior of a consultation system S: given a constraint formula q and a potential conclusion c for S, determine if there is a user input binding that satisfies q and causes S to conclude c. Existing rule-based expert systems, both forward and backward chaining[3], implement a consultation mechanism S, but are not designed for these queries about S. For general production systems, the queries are undecidable. Here we solve the problem for useful sublanguages of acyclic production systems.We implement a query tool in a Datalog + constraints framework, and optimize for “embedded decision trees” in the rule system. Our data complexity is &THgr;(n·ƒ(n)) in the size of the embedded trees, versus &THgr;(n·ƒ(n) + n2) for existing datalog evaluation algorithms, where ƒ(n) is the cost of destructively conjoining a constraint of unit size into a conjunction of n constraints.

#index 287249
#* Self maintenance of multiple views in data warehousing
#@ S. Samtani;V. Kumar;M. Mohania
#t 1999
#c 1
#% 135476
#% 152928
#% 199537
#% 201928
#% 340301
#% 481933
#% 482098
#! Materialized views (MV) at the data warehouse (DW) can be kept up to date in response to changes in data sources without accessing data sources for additional information. This process is usually refered to as “self maintenance of views”. A number of algorithms have been proposed for self maintenance of views, which use auxiliary views (AV) to keep some additional information in DW. In this paper we propose an algorithm for self maintainability of multiple MVs using the above approach. Our algorithm generates a simple maintenance query to incrementally maintain an MV along with its AV at DW. The algorithm maintains these views by minimizing the number and the size of the AVs. Our approach provides better insight into view maintenance issues by exploiting the dependencies and constraints that might exist in the data sources and multiple MVs at DW.

#index 287251
#* Updates and view maintenance in soft real-time database systems
#@ Ben Kao;K. Y. Lam;Brad Adelberg;Reynold Cheng;Tony Lee
#t 1999
#c 1
#% 158051
#% 187412
#% 187413
#% 201922
#% 442967
#% 442985
#% 458544
#% 480266
#% 481422
#% 615538
#! A database system contains base data items which record and model a physical, real world environment. For better decision support, base data items are summarized and correlated to derive views. These base data and views are accessed by application transactions to generate the ultimate actions taken by the system. As the environment changes, updates are applied to the base data, which subsequently trigger view recomputations. There are thus three types of activities: base data update, view recomputation, and transaction execution. In a real-time system, two timing constrains need to be enforced. We require transactions meet their deadlines (transaction timeliness) and read fresh data (data timeliness). In this paper we define the concept of absolute and relative temporal consistency from the perspective of transactions. We address the important issue of transaction scheduling among the three types of activities such that the two timing requirements can be met. We also discuss how a real-time database system should be designed to enforce different levels of temporal consistency.

#index 287252
#* An adaptive view element framework for multi-dimensional data management
#@ John R. Smith;Chung-Sheng Li
#t 1999
#c 1
#% 181378
#% 201893
#% 219847
#% 227866
#% 227924
#% 248040
#% 259995
#% 260008
#% 437405
#% 461921
#% 464215
#% 470220
#% 509928
#% 626847
#% 682349
#% 1775062
#! We present an adaptive wavelet view element framework for managing different types of multi-dimensional data in storage and retrieval applications. We consider the problems of multi-dimensional data compression, multi-resolution subregion access, selective materialization, progressive retrieval and similarity searching. The framework uses wavelets to partition the multi-dimensional data into view elements that form the building blocks for synthesizing views of the data. The view elements are organized and managed using different view element graphs. The graphs are used to guide cost-based view element selection algorithms for optimizing compression, access, retrieval and search performance.We present the adaptive wavelet view element framework and describe its application in managing multi-dimensional data such as 1-D time series data, 2-D images, video sequences, and multi-dimensional data cubes. We present experimental results that demonstrate that the adaptive wavelet view element framework improves performance of compressing, accessing, and retrieving multi-dimensional data compared to non-adaptive methods.

#index 287253
#* A general language model for information retrieval
#@ Fei Song;W. Bruce Croft
#t 1999
#c 1
#% 118030
#% 184489
#% 262096
#% 279755
#% 280850
#% 300542
#% 836019
#! Statistical language modeling has been successfully used for speech recognition, part-of-speech tagging, and syntactic parsing. Recently, it has also been applied to information retrieval. According to this new paradigm, each document is viewed as a language sample, and a query as a generation process. The retrieved documents are ranked based on the probabilities of producing a query from the corresponding language models of these documents. In this paper, we will present a new language model for information retrieval, which is based on a range of data smoothing techniques, including the Good-Turning estimate, curve-fitting functions, and model combinations. Our model is conceptually simple and intuitive, and can be easily extended to incorporate probabilities of phrases such as word pairs and word triples. The experiments with the Wall Street Journal and TREC4 data sets showed that the performance of our model is comparable to that of INQUERY and better than that of another language model for information retrieval. In particular, word pairs are shown to be useful in improving the retrieval performance.

#index 287254
#* Practical evaluation of IR within automated classification systems
#@ R. Dolin;J. Pierre;M. Butler;R. Avedon
#t 1999
#c 1
#% 49501
#% 206512
#% 648114
#% 707580
#! This paper describes some of the work we have done to evaluate and compare the use of three IR systems (Verity, LSI, and SMART) as black boxes within an automated classification environment. We use automated classification to make a quantitative comparison of the effectiveness of the systems within this context. In so doing, we also develop criteria for the construction of a useful training set. These results lead to metrics useful in the integration of IR systems into larger applications. We conclude with an initial API for an IR component within an automated classification architecture.

#index 287255
#* A unified environment for fusion of information retrieval approaches
#@ M. Catherine McCabe;Abdur Chowdhury;David A. Grossman;Ophir Frieder
#t 1999
#c 1
#% 71772
#% 169774
#% 218982
#% 224702
#% 232703
#% 262080
#% 269899
#! Prior work has shown that combining results of various retrieval approaches and query representations can improve search effectiveness. Today, many meta-search engines exist which combine the results of various search engines in the hopes of improving overall effectiveness. However, the combination of results from different search engines masks variations in parsers, and other indexing techniques (stemming, stop words, etc.) This makes it difficult to assess the utility of the fusion technique. We have implemented the two most prevalent retrieval strategies: probabilistic and vector space using the same parser and the same relational retrieval engine. First, we identified a model that enables the fusion of an arbitrary number of sources. Next, we tested various linear combinations of these two methods as well as various thresholds for identifying retrieved documents. Our results show some improvement of effectiveness, but they also provide us for a baseline from which we can continue with other retrieval strategies and test the effect of fusing these strategies.

#index 287256
#* Indexing field values in field oriented systems: interval Quadtree
#@ Myoung-Ah Kang;Sylvie Servigne;Ki-Joune Joune Li;Robert Laurini
#t 1999
#c 1
#% 68089
#% 68091
#% 86950
#% 182700
#% 260057
#% 296090
#% 359751
#% 427199
#% 480093
#% 549073
#! With the extension of spatial database applications, field oriented systems emerge as an important research issue in order to deal with continuous natural phenomena during the last years. It however has a large volume of data and efficient indexing methods for field data are necessary to overcome the performance obstacle. In special, we introduce indexing methods for field value queries (i.e. searching some regions where the temperature is more 20 degrees). We introduce the concept of subfield and show how we make use of this concept to index field values in field oriented systems. We present two implementation methods based on Quadtree space subdivision. We modify traditional linear quadtree implementation method for field value query processing using subfields. We analyze the performance of our methods. Experimentation with real terrain data shows that proposed indexing methods improve the query processing time of field value queries in comparison with the case of no indexing method.

#index 287257
#* Clustering declustered data for efficient retrieval
#@ Hakan Ferhatosmanoglu;Divyakant Agrawal;Amr El Abbadi
#t 1999
#c 1
#% 68091
#% 88056
#% 135559
#% 227856
#% 248796
#% 250026
#% 252304
#% 262113
#% 275367
#% 286962
#% 339622
#% 427199
#% 435141
#% 462233
#% 464195
#% 481599
#% 481956
#% 562329
#% 631955
#% 682228
#! Modern databases increasingly integrate new kinds of information, such as multimedia information in the form of image, video, and audio data. Both the dimensionality and the amount of data that need to be processed is increasing rapidly, increasing the demand for the efficient retrieval of large amounts of multi-dimensional data. Declustering techniques for multi-disk architectures have been effectively used for storage. In this paper, we first establish that besides exploiting the parallelism, a careful organization of each disk must be considered for fast searching. We introduce the notion of page allocation and data space mapping which can be used to organize and retrieve multidimensional data. We develop these notions based on three different partitioning strategies: regular grid partitioning, concentric hypercubes and hyperpyramids. We develop techniques that satisfy efficient retrieval by optimizing the number of buckets retrieved by the query, disk arm movement and I/O parallelism. We prove that concentric hypercube-based mapping satisfies the optimal clustering and optimal parallelism. We develop a technique based on hyperpyramid partitioning that reduces the number of buckets retrieved by the query and has efficient inter- and intra-disk organizations. We evaluate the performance of proposed techniques by comparing them with the current approaches. The new techniques lead to very significant improvement over the existing techniques, and result in fast retrieval of multi-dimensional data.

#index 287258
#* Indexing techniques for wireless data broadcast under data clustering and scheduling
#@ Quinglong Hu;Wang-Chien Lee;Dik Lun Lee
#t 1999
#c 1
#% 201897
#% 247246
#% 259634
#% 340718
#% 430426
#% 443127
#% 482107
#% 628099
#% 635819
#! This paper investigates power conserving indexing techniques for data disseminated on a broadcast channel. A hybrid indexing method combining strengths of the signature and the index tree techniques is presented. Different from previous studies, our research takes into consideration two important data organization factors, namely, clustering and scheduling. Cost models for index, signature and hybrid methods are derived by taking into account various data organizations accommodating these two factors. Based on our analytical comparisons, the signature and the hybrid indexing techniques are the best choices for power conserving indexing of various data organizations on wireless broadcast channels.

#index 287259
#* Towards data warehouse design
#@ Franck Ravat;Olivier Teste;Giles Zurfluh
#t 1999
#c 1
#% 37972
#% 83206
#% 199537
#% 201928
#% 207552
#% 223781
#% 227997
#% 240149
#% 260041
#% 273918
#% 319244
#% 442967
#% 459026
#% 481931
#% 482093
#% 482098
#% 482111
#% 546596
#% 562524
#% 568186
#! This paper focuses on data warehouse modelling. The conceptual model we defined, is based on object concepts extended with specific concepts like generic classes, temporal classes and archive classes. The temporal classes are used to store the detailed evolutions and the archive classes store the summarised data evolutions. We also provide a flexible concept allowing the administrator to define historised parts and non-historised parts into the warehouse schema. Moreover, we introduce constraints which configure the data warehouse behaviour and these various parts. To validate our propositions, we describe a prototype dedicated to the data warehouse design.

#index 287260
#* Obsolescent materialized views in query processing of enterprise information systems
#@ Avigdor Gal
#t 1999
#c 1
#% 248038
#% 278605
#% 340301
#% 340305
#% 459026
#% 462168
#% 462176
#% 464056
#! In recent years, query processing has become more complex as data sources are frequently replicated and data are periodically processed and embedded within several data sources simultaneously. These trends have necessitated the optimization of techniques for query processing in order to exploit these new alternatives. Accordingly, this paper introduces an improved query optimization technique, which is capable of assessing query plans that use both current and obsolescent data. In particular, we provide a cost model by which the trade-offs of using obsolescent materialized views can be evaluated and we also discuss the method's applicability to contemporary query optimization techniques.

#index 287261
#* Efficient refreshment of materialized views with multiple sources
#@ Hui Wang;Maria Orlowska;Weifa Liang
#t 1999
#c 1
#% 13016
#% 59350
#% 152928
#% 201928
#% 201929
#% 210210
#% 210211
#% 217179
#% 227944
#% 227947
#% 340300
#% 340301
#% 442663
#% 458556
#% 462074
#% 480623
#% 482098
#! A data warehouse collects and maintains a large amount of data from multiple distributed and autonomous data sources. Often the data in it is stored in the form of materialized views in order to provide fast access to the integrated data. However, maintaining a certain level consistency of warehouse data with the source data is challenging in a distributed multiple source environment. Transactions containing multiple updates at one or more sources further complicate the consistency issue.Following the four level consistency definition of view in a warehouse, we first present a complete consistency algorithm for maintaining SPJ-type materialized views incrementally. Our algorithm speed-ups the view refreshment time, provided that some extra moderate space in the warehouse is available. We then give a variant of the proposed algorithm by taking the update frequencies of sources into account. We finally discuss the relationship between a view's certain level consistency and its refresh time. It is difficult to propose an incremental maintenance algorithm such that the view is always kept at a certain level consistency with the source data and the view's refresh time is as fast as possible. We trade-off these two factors by giving an algorithm with faster view refresh time, while the view maintained by the algorithm is strong consistency rather than complete consistency with the source data.

#index 287262
#* An effective mechanism for index update in structured documents
#@ Hyunchi Jang;Youngil Kim;Dongwook Shin
#t 1999
#c 1
#% 71755
#% 90844
#% 184486
#% 194253
#% 204662
#% 249160
#! Indexing and retrieval of structured documents have been drawing attention increasingly since they enable to retrieve and access a certain part of a document easily. So far, several methods have been proposed in the setting that documents are rarely changed. These can be applied for the books or journals possessed in libraries, but hardly work for the documents that are subject to change frequently in the business domain. This paper aims at enabling incremental update of indices whenever parts of documents are changed. For this, it employs the index-organized table that has been developed for the full-text retrieval in Oracle. It creates several index-organized tables that are essential in implementing the Bottom Up Scheme strategy, which has been developed for manipulating structured documents efficiently.Along with an experiment, the technique presented here does not add much index overhead to the original one taken to the index organized table. In addition, the updates of indices are performed quickly as soon as parts of documents are changed.

#index 287263
#* Performance and implications of semantic indexing in a distributed environment
#@ Conrad T. K. Chang;Bruce R. Schatz
#t 1999
#c 1
#% 79403
#% 187192
#% 212683
#% 234793
#% 438052
#! A research prototype is presented for semantic indexing and retrieval in Information Retrieval. The prototype is motivated by a desire to provide a more efficient and effective information retrieval system compared to the current state of the art. An overview of the Interspace architecture layers is discussed. An object model supporting semantic operations is developed. The model contains a rich set of classes and relationships of the data for the semantic indexing module. The basis of our semantic indexing is done by the creation of concept space. A concept space is an index of a collection that uses document statistics to capture the relationships between concepts. It is useful for boosting text search, by term suggestion of alternative terms semantically related to query terms. Over the years, we have developed generic technology for concept spaces computation on large collections across many subjects. Recent computations on discipline-scale collections have been made on high-end supercomputers. This paper describes our implementation and implications of the computation in a distributed computing environment. Experimental results using different collection sizes and number of processes are presented to show the feasibility of this approach. We also show that laboratory and community collections are already easily computable using a group of PCs in a lab via a message-passing model. We conclude that PC clusters will shortly be able to compute semantic indexes for any real collections.

#index 287265
#* Quality of service transferred to information retrieval: the adaptive information retrieval system
#@ Claudia Rolker;Ralf Kramer
#t 1999
#c 1
#% 111456
#% 118726
#% 223772
#% 250246
#% 262084
#% 277025
#! Users often quit an information retrieval system (IR system) very frustrated, because they cannot find the information matching their information needs. We have identified the following two main reasons: too high expectations and wrong use of the system. Our approach which addresses both issues is based on the transfer of the concept of Quality of Service to IR systems: The user first negotiates the retrieval success rates with the IR system, so he knows what to expect from the system in advance. Second, by dynamic adaptation to the retrieval context, the IR system tries to improve the user's queries and thereby tries to exploit the underlying information source as best as possible.

#index 287279
#* Architecture of a networked image search and retrieval system
#@ R. Weber;J. Bollinger;T. Gross;H.-J. Schek
#t 1999
#c 1
#% 102772
#% 171433
#% 194234
#% 212690
#% 213489
#% 217205
#% 236743
#% 240013
#% 251111
#% 259653
#% 437405
#% 464211
#% 479462
#% 479477
#% 479649
#% 481956
#% 610665
#! Large scale networked image retrieval systems face a number of problems that are not fully satisfied by current systems. On one hand, integrated solutions that store all image data centrally are often limited in terms of scalability and autonomy of data providers. On the other hand, WWW-based search engines proved to be fairly scalable, and data providers retain their autonomy. However, such engines often confront users with links to servers that are not available or to images that no longer exist, i.e., they are unable to keep their meta-database consistent with the repositories' contents. Furthermore, existing solutions often neglect the cost of image delivery. The considerable variations in the effective bandwidth in today's Internet lead to highly unpredictable response times, which are often intolerable from the user's point of view.This paper presents the architecture of Chariot, a networked image search and retrieval system that tackles these concerns. With respect to scalability and autonomy, Chariot follows the approach of WWW-based search engines by maintaining only the meta-data in a central database. Various specialized components (feature extraction, indexes, images servers) are coordinated by a middleware component that employs transactional process management to enforce consistency between the meta-data and all components. Moreover, Chariot incorporates mechanisms to provide more predictable response times for the image delivery over the Internet by employing network-aware image servers. These servers trade off the quality of the images to be delivered with the bandwidth required to transmit the images.

#index 287280
#* A comparison of alternative continuous display techniques with heterogeneous multi-zone disks
#@ Shahram Ghandeharizadeh;Seon Ho Kim
#t 1999
#c 1
#% 151340
#% 172881
#% 201931
#% 201932
#% 201933
#% 204397
#% 236085
#% 237195
#% 239689
#% 254151
#% 422967
#% 458738
#% 460869
#% 461913
#% 614631
#% 632238
#% 661691
#% 661705
#! A number of recent technological trends have made data intensive applications such as continuous media (audio and video) servers a reality. These servers are expected to play an important role in applications such as video-on-demand, digital library, news-on-demand, distance learning, etc. Continuous media applications are data intensive and might require storage subsystems that consist of hundreds of (multi-zone) disk drives. With the current technological trends, a homogeneous disk subsystem might evolve to consist of a heterogeneous collection of disk drives. Given such a storage subsystem, the system must continue to support a hiccup-free display of audio and video clips. This study describes extensions of four continuous display techniques for multi-zone disk drives to a heterogeneous platform. These techniques include IBM's Logical Track [21], HP's Track Pairing [4], and USC's FIXB [9] and deadline driven techniques [10]. We quantify the performance tradeoff associated with these techniques using analytical models and simulation studies. The obtained results demonstrate tradeoffs between the cost per simultaneous stream supported by a technique, the wasted disk space, and the incurred startup latency.

#index 287281
#* Spatial match representation scheme supporting ranking in iconic images databases
#@ Yeon-Jung Kim;Choon-Bo Sim;Jae-Woo Chang
#t 1999
#c 1
#% 23998
#% 102358
#% 181409
#% 201880
#% 234793
#% 240192
#% 318437
#% 406493
#% 443054
#% 488109
#! Because content-based image retrieval is essential to retrieve relevant multimedia documents, we represent images as a set of recognizable symbols, i.e., icon objects, and do indexing by regarding the icon object as a representative of a given document. When users request content-based image retrieval, we convert a query image into icon objects and retrieve relevant images in the database. In this paper, we propose a new spatial-match representation scheme, called SRR(Spatial-match Representation supporting Ranking) scheme, which combine directional operators with positional operators. Therefore, our SRR scheme can represent spatial relationships between icon objects precisely and can provide ranking for the retrieved images. In addition, we compare our scheme with the conventional 9DLT and SMR schemes in terms of retrieval effectiveness. Finally, we show from our experiment that our SRR scheme holds about 25% higher recall and about 10% higher precision, compared with the 9DLT and the SMR.

#index 287282
#* Word segmentation and recognition for web document framework
#@ Chi-Hung Chi;Chen Ding;Andrew Lim
#t 1999
#c 1
#% 162463
#% 756901
#! It is observed that a better approach to Web information understanding is to base on its document framework, which is mainly consisted of (i) the title and the URL name of the page, (ii) the titles and the URL names of the Web pages that it points to, (iii) the alternative information source for the embedded Web objects, and (iv) its linkage to other Web pages of the same document. Investigation reveals that a high percentage of words inside the document framework are “compound words” which cannot be understood by ordinary dictionaries. They might be abbreviations or acronyms, or concatenations of several (partial) words. To recover the content hierarchy of Web documents, we propose a new word segmentation and recognition mechanism to understand the information derived from the Web document framework. A maximal bi-directional matching algorithm with heuristic rules is used to resolve ambiguous segmentation and meaning in compound words. An adaptive training process is further employed to build a dictionary of recognisable abbreviations and acronyms. Empirical results show that over 75% of the compound words found in the Web document framework can be understood by our mechanism. With the training process, the success rate of recognising compound words can be increased to about 90%.

#index 287283
#* An automated approach for retrieving hierarchical data from HTML tables
#@ Seung-Jin Lim;Yiu-Kai Ng
#t 1999
#c 1
#% 237193
#% 464825
#% 479471
#% 481602
#% 511496
#! Among the HTML elements, HTML tables [RHJ98] encapsulate hierarchically structured data (hierarchical data in short) in a tabular structure. HTML tables do not come with a rigid schema and almost any forms of two-dimensional tables are acceptable according to the HTML grammar. This relaxation complicates the process of retrieving hierarchical data from HTML tables. In this paper, we propose an automated approach for retrieving hierarchical data from HTML tables. The proposed approach constructs the content tree of an HTML table, which captures the intended hierarchy of the data content of the table, without requiring the internal structure of the table to be known beforehand. Also, the user of the content tree does not deal with HTML tags while retrieving the desired data from the content tree. Our approach can be employed by (i) a query language written for retrieving hierarchically structured data, extracted from either the contents of HTML tables or other sources, (ii) a processor for converting HTML tables to XML documents, and (iii) a data warehousing repository for collecting hierarchical data from HTML tables and storing materialized views of the tables. The time complexity of the proposed retrieval approach is proportional to the number of HTML elements in an HTML table.

#index 287284
#* A probabilistic description-oriented approach for categorizing web documents
#@ Norbert Gövert;Mounia Lalmas;Norbert Fuhr
#t 1999
#c 1
#% 46803
#% 65953
#% 73046
#% 111304
#% 169718
#% 176530
#% 248810
#% 318412
#! The automatic categorisation of web documents is becoming crucial for organising the huge amount of information available in the Internet. We are facing a new challenge due to the fact that web documents have a rich structure and are highly heterogeneous. Two ways to respond to this challenge are (1) using a representation of the content of web documents that captures these two characteristics and (2) using more effective classifiers.Our categorisation approach is based on a probabilistic description-oriented representation of web documents, and a probabilistic interpretation of the k-nearest neighbour classifier. With the former, we provide an enhanced document representation that incorporates the structural and heterogeneous nature of web documents. With the latter, we provide a theoretical sound justification for the various parameters of the k-nearest neighbour classifier.Experimental results show that (1) using an enhanced representation of web documents is crucial for an effective categorisation of web documents, and (2) a theoretical interpretation of the k-nearest neighbour classifier gives us improvement over the standard k-nearest neighbour classifier.

#index 287285
#* Clustering transactions using large items
#@ Ke Wang;Chu Xu;Bing Liu
#t 1999
#c 1
#% 36672
#% 46809
#% 118771
#% 152934
#% 210173
#% 232117
#% 238413
#% 255137
#% 375017
#% 462243
#% 479658
#! In traditional data clustering, similarity of a cluster of objects is measured by pairwise similarity of objects in that cluster. We argue that such measures are not appropriate for transactions that are sets of items. We propose the notion of large items, i.e., items contained in some minimum fraction of transactions in a cluster, to measure the similarity of a cluster of transactions. The intuition of our clustering criterion is that there should be many large items within a cluster and little overlapping of such items across clusters. We discuss the rationale behind our approach and its implication on providing a better solution to the clustering problem. We present a clustering algorithm based on the new clustering criterion and evaluate its effectiveness.

#index 287286
#* A multiple-resolution method for edge-centric data clustering
#@ Scott Epter;Mukkai Krishnamoorthy
#t 1999
#c 1
#% 2115
#% 36672
#% 210173
#% 241124
#% 248790
#% 248792
#% 260009
#% 260036
#% 479659
#% 479799
#% 481281
#% 481741
#! Recent works in spatial data clustering view the input data set in terms of inter-point edge lengths rather than the points themselves. Cluster detection in such a system is a matter of finding connected paths of edges whose weight is no greater than some user input threshold or cutoff value. The SMTIN algorithm[9] is one such system that uses Delaunay triangulation to compute the set of nearest neighbor edges quickly and efficiently. Experiments demonstrate a substantial performance and accuracy improvement using SMTIN in comparison to other clustering systems.The resolution of the clusters discovered in the SMTIN system is directly related to the choice of a cutoff threshold, which makes SMTIN perform poorly for input sets with clusters at multiple resolutions. In this work we introduce an edge-centric clustering method that detects clusters at multiple resolutions. Our algorithm detects differences in density among groups of points and uses multiple cutoff points in order to account for clusters at different resolutions. One of the main benefits of the multi-resolution approach of our system is the ability to accurately cluster points that other systems would consider to be noise. Experiments indicate a substantial improvement in the clustering quality of our system in comparison to SMTIN as well as the removal of the requirement of an input distance-threshold, achieved with comparable theoretical as well as actual runtime performance. We present promising directions for this new algorithm.

#index 287287
#* A self-organized file cabinet
#@ Dawn Lawrie;Daniela Rus
#t 1999
#c 1
#% 46809
#% 109223
#% 111456
#% 131580
#% 144023
#% 151709
#% 198113
#% 201568
#% 260009
#% 275929
#% 282435
#% 318453
#% 443698
#% 457641
#% 622080
#! The self-organizing file cabinet is an information retrieval system associated with a user's physical file cabinet. It enhances a physical file cabinet with electronic information about the papers in it. It can remember, organize, update, and help the user find documents contained in the physical file cabinet. The system consists of a module for extracting electronic information about the papers stored in the file cabinet, a module for representing and storing this information in multiple views, and a module that allows a user to interact with this information. The focus of this paper is on the design and evaluation of the self-organized file cabinet.

#index 287288
#* Incremental encoding of multiple inheritance hierarchies
#@ M. F. van Bommel;T. J. Beck
#t 1999
#c 1
#% 51391
#% 58365
#% 148890
#% 442907
#% 564268
#% 704698
#! Incremental updates to multiple inheritance hierarchies are becoming more prevalent with the increasing number of persistent applications supporting complex objects, making the efficient computation of lattice operations such as greatest lower bound (GLB), least upper bound (LUB), and subsumption more and more important. General techniques for the compact encoding of a hierarchy are presented. One such method is to plunge the given ordering into a boolean lattice of binary words, leading to an almost constant-time complexity of the lattice operations. The method is based on an inverted version of the encoding of Ait-Kaci et al. to allow incremental update. Simple grouping is used to reduce the code space while keeping the lattice operations efficient. Comparisons are made to an incremental version of the range compression scheme of Agrawal et al., where each class is assigned an interval, and relationships are based on containment in the interval. The result is two encoding methods which have their relative merits. The former being better for smaller, more structured hierarchies, and the latter for larger, less organized hierarchies.

#index 287289
#* From object evolution to object emergence
#@ Dalila Tamzalit;Chabane Oussalah
#t 1999
#c 1
#% 30567
#% 32903
#% 32970
#% 81925
#% 86321
#% 89644
#% 114994
#% 165948
#% 370029
#% 480251
#! Database applications which model aspects of the real world should be able to express as accurately as possible the different nuances of reality; that includes the need to evolve internally in response to signals of updates coming from the environment. These updates are not always supplied in an ideal and complete manner and are not always predefined or precisely defined. In practice, requirements for evolution generally occur during the manipulation of objects while running the database. It is frequently necessary to change individual objects, less frequently the database schema. Database systems need to have mechanisms capable, whenever and as well as possible, of assimilating this new information correctly and diagnosing and implementing the changes necessary.This paper concerns the evolution of objects inside databases. Our two main objectives are:to allow objects to evolve their structures dynamically during database maintenance and use, with all necessary impacts on the database schema;to allow, similarly, the creation and display of different plans for evolving the design, like ways of schema evolution, giving in this way a simulation tool for database design and maintenance.So, we propose a Genetic Evolution Object Model developed to have inherent capabilities for auto-adaptation between classes and instances.

#index 287290
#* Graph-based object-oriented approach for structural and behavioral representation of multimedia data
#@ Ivan Radev;Niki Pissinou;Kia Makki;E. K. Park
#t 1999
#c 1
#% 18606
#% 193199
#% 237191
#% 442988
#% 459277
#% 586836
#! The management of multimedia information poses special requirements for multimedia information systems. Both representation and retrieval of the complex and multifaceted multimedia data are not easily handled with the flat relational model and require new data models. In the last several years, object-oriented and graph-based data models are actively pursued approaches for handling the multimedia information. In this paper the characteristics of the novel graph-based object-oriented data model are presented. This model represents the structural and behavioral aspects of data that form multimedia information systems. It also provides for handling the continuously changing user requirements and the complexity of the schema and data representation in multimedia information systems using the schema versioning approach and perspective version abstraction.

#index 287291
#* A learning approach to processor allocation in parallel systems
#@ Amy W. Apon;Thomas D. Wagner;Lawrence W. Dowdy
#t 1999
#c 1
#% 2257
#% 29037
#% 65998
#% 99040
#% 161797
#% 253578
#% 442509
#! Given a typical parallel system and a collection of applications that are to execute on the system, a common problem is determining an effective allocation of processors among the applications. In this paper a learning approach is applied to processor allocation. The approach is to use a stochastic learning automaton (SLA) as a decision tool. An SLA uses values of the current state description, makes an allocation decision, evaluates its decision at some later time, modifies its decision making process, and tries to find the best allocation strategy by learning from its previous mistakes. The method is applied to the problem of allocating processors to parallel applications in a distributed system such as a cluster of workstations, and is validated through simulation. The result of this study show that a learning approach that utilizes a stochastic learning automaton is effective at making processor allocation decisions in a parallel system.

#index 287292
#* Adaptive information filtering: detecting changes in text streams
#@ Carsten Lanquillon;Ingrid Renz
#t 1999
#c 1
#% 169717
#% 194284
#% 219049
#% 260001
#% 449529
#! The task of information filtering is to classify documents from a stream as either relevant or non-relevant according to a particular user interest with the objective to reduce information load. When using an information filter in an environment that is changing with time, methods for adapting the filter should be considered in order to retain classification accuracy. We favor a methodology that attempts to detect changes and adapts the information filter only if inevitable in order to minimize the amount of user feedback for providing new training data. Yet, detecting changes may require costly user feedback as well. This paper describes two methods for detecting changes without user feedback. The first method is based on evaluating an expected error rate, while the second one observes the fraction of classification decisions made with a confidence below a given threshold. Further, a heuristics for automatically determining this threshold is suggested and the performance of this approach is experimentally explored as a function of the threshold parameter. Some empirical results show that both methods work well in a simulated change scenario with real world data.

#index 287293
#* Archiving telemeetings
#@ Constantin Arapis
#t 1999
#c 1
#% 194235
#% 266380
#% 267568
#% 296058
#% 626874
#! This paper presents a prototype system for modeling and managing the complete life-time of telemeetings/teleconferences. The system provides services for modeling telemeetings, storing telemeetings in a telemeeting database, annotating telemeetings and querying the telemeeting database.

#index 290148
#* A method of geographical name extraction from Japanese text for thematic geographical search
#@ Yasusi Kanada
#t 1999
#c 1
#% 249145
#! A text retrieval method called the thematic geographical search method has been developed and applied to a Japanese encyclopedia called the World Encyclopædia. In this method, the user specifies a search theme using free words, then obtains a sorted list of excerpts and hyperlinks to encyclopedia sentences that contain geographical names. Using this list, the user can also open maps that indicate the locations of the names. To generate an index of names for this searching, a method of extracting geographical names has been developed. In this method, geographical names are extracted, matched to names in a geographical name database, and identified. Geographical names, however, often have several types of ambiguities. Ambiguities are resolved by using non-local context analysis, which uses a stack and several other techniques. As a result, the precision of extracted names is more than 96% on average. This method depends on features of the Japanese language, but the strategy and most of the techniques can be applied to texts in English or other languages.

#index 290149
#* An adaptive algorithm for learning changes in user interests
#@ Dwi H. Widyantoro;Thomas R. Ioerger;John Yen
#t 1999
#c 1
#% 204531
#% 219049
#% 219054
#% 234990
#% 234992
#% 241033
#% 241036
#% 252753
#% 271082
#% 707278
#! In this paper, we describe a new scheme to learn dynamic user's interests in an automated information filtering and gathering system running on the Internet. Our scheme is aimed to handle multiple domains of long-term and short-term user's interests simultaneously, which is learned through positive and negative user's relevance feedback. We developed a 3-descriptor approach to represent the user's interest categories. Using a learning algorithm derived for this representation, our scheme adapts quickly to significant changes in user interest, and is also able to learn exceptions to interest categories.

#index 290150
#* Haystack: per-user information environments
#@ Eytan Adar;David Karger;Lynn Andrea Stein
#t 1999
#c 1
#% 23668
#% 115462
#% 118771
#% 157710
#% 236416
#% 672134
#% 1275346
#! Traditional Information Retrieval (IR) systems are designed to provide uniform access to centralized corpora by large numbers of people. The Haystack project emphasizes the relationship between a particular individual and his corpus. An individual's own haystack priviliges information with which that user interacts, gathers data about those interactions, and uses this metadata to further personalize the retrieval process. This paper describes the prototype Haystack system.

#index 290151
#* Information integration with attribution support for corporate profiles
#@ Thomas Lee;Melanie Chams;Robert Nado;Michael Siegel;Stuart Madnick
#t 1999
#c 1
#% 172378
#% 227981
#% 236409
#% 236416
#% 244108
#% 282425
#% 631868
#! The proliferation of electronically available data within large organizations as well as publicly available data (e.g. over the World Wide Web) poses challenges for users who wish to efficiently interact with and integrate multiple heterogeneous sources. This paper presents CI3, a corporate information integrator, which applies XML as a tool to facilitate data mediation and integration amongst heterogeneous sources in the context of financial analysts creating corporate profiles. Sources include Lotus Notes, relational databases, and the World Wide Web. CI3 applies a unified XML data model to automate integration. By preserving metadata about the source of each datum in the integrated result set, CI3 supports source attribution. Users may trace the attribution metadata from the result back to the underlying sources and leverage their expertise in interpreting the data and, if necessary, use their judgment in assessing the authenticity and veracity of results. We present a functional overview of CI3, its system architecture including the XML data model, and the integration procedures. We conclude by reflecting on lessons learned.

#index 290977
#* Proceedings of the 1998 workshop on New paradigms in information visualization and manipulation
#@ David S. Ebert;Charles K. Nicholas;Chris D. Shaw
#t 1998
#c 1

#index 294995
#* Ontology-based web site mapping for information exploration
#@ Xiaolan Zhu;Susan Gauch;Lutz Gerhard;Nicholas Kral;Alexander Pretschner
#t 1999
#c 1
#! Centralized search process requires that the whole collection reside at a single site. This imposes a burden on both the system storage of the site and the network traffic near the site. It thus comes to require the search process to be distributed. Recently, more and more Web sites provide the ability to search their local collection of Web pages. Query brokering systems are used to direct queries to the promising sites and merge the results from these sites. Creation of meta-information of the sites plays an important role in such systems. In this article, we introduce an ontology-based web site mapping method used to produce conceptual meta-information, the Vector Space approach, and present a serial of experiments comparing it with Naïve-Bayes approach. We found that the Vector Space approach produces better accuracy in ontology-based web site mapping.

#index 314485
#* Adaptive real-time transactions and risk-based load control
#@ Erdoğan Doğdu;Gültekin Özsoyoğlu
#t 1996
#c 1
#% 122904
#% 268747
#% 385668
#% 463260
#% 562157
#% 641557

#index 314486
#* Incorporation of multimedia capabilities in distributed real-time applications
#@ Oscar González;Subhabrata Sen;Krithi Ramamritham;John A. Stankovic
#t 1996
#c 1
#% 609901
#% 615000
#% 615532
#% 1852658

#index 314487
#* RODAIN: a real-time object-oriented database system for telecommunications
#@ Juha Taina;Kimmo Raatikainen
#t 1996
#c 1
#% 395735

#index 314488
#* The monitoring of complex active rules with vector representation
#@ Dongwook Kim;Myoung Ho Kim;Yoon Joon Lee
#t 1996
#c 1
#% 78005
#% 116044
#% 464208
#% 480942

#index 314489
#* Research issues in real-time DBMS in the context of electronic commerce
#@ Prabhudev Konana;Alok Gupta;Andrew B. Whinston
#t 1996
#c 1
#% 1336
#% 57529
#% 77704
#% 158051
#% 201922
#% 206024
#% 208275
#% 339349
#% 340597
#% 571217

#index 314490
#* Escalations in workflow management systems
#@ E. Panagos;M. Rabinovich
#t 1996
#c 1
#% 185412
#% 185413
#% 202703
#% 288821
#% 480266
#% 535525
#% 571215
#% 672556

#index 314491
#* Using Petri nets for rule termination analysis
#@ Detlef Zimmer;Axel Meckenstock;Rainer Unland
#t 1996
#c 1
#% 153004
#% 374605
#% 481456
#% 501948

#index 314492
#* Sensor data management in manufacturing systems
#@ Hiren Parikh;Kang Shin;Nandit Soparkar
#t 1996
#c 1
#% 32915
#% 405152
#% 405319
#% 464058
#% 482088
#% 503562

#index 314493
#* Porting an expert database application to an active database: an experience report
#@ Lance Obermeyer;Lane Warshaw;Daniel P. Miranker
#t 1996
#c 1
#% 152910
#% 199801
#% 214787
#% 618480

#index 314494
#* A model based reasoning approach to network monitoring
#@ Anoop Singhal;Gary Weiss;Johannes P. Ros
#t 1996
#c 1
#% 116185
#% 1290131
#% 1499538

#index 314495
#* Timely and fault-tolerant data access from broadcast disks: a pinwheel-based approach
#@ Sanjoy Baruah;Azer Bestavros
#t 1996
#c 1
#% 54037
#% 66172
#% 101543
#% 108126
#% 121587
#% 126411
#% 158051
#% 175253
#% 339362
#% 564239
#% 614999
#% 615536
#% 674235

#index 314496
#* Why commercial database systems are not real-time systems
#@ Anant Jhingran
#t 1996
#c 1
#% 442967

#index 314497
#* Impact of timing constraints on real-time database recovery
#@ Jing Huang;Le Gruenwald
#t 1996
#c 1
#% 3652
#% 12638
#% 32910
#% 102806
#% 114582
#% 124813
#% 158051
#% 166509
#% 216042
#% 403195
#% 442012
#% 442832
#% 442834
#% 481268
#% 677178

#index 314498
#* Behavioral situations and active database systems
#@ Agnès Front;Claudia Roncancio;Jean-Pierre Giraudin
#t 1996
#c 1
#% 116049
#% 116708
#% 425200
#% 481285
#% 481457
#% 481773

#index 314499
#* Process systems and data bases
#@ Alfs T. Berztiss
#t 1996
#c 1
#% 22947
#% 65836
#% 91005
#% 122904
#% 125792
#% 154352
#% 204808
#% 238926
#% 277385
#% 286831
#% 314490
#% 368701
#% 394417
#% 408591
#% 452817
#% 657990

#index 314500
#* An architecture and two new research problems in ARCS databases
#@ Anindya Datta;Sharma Chakravarthy;Shiby Thomas;Igor R. Viguier
#t 1996
#c 1
#% 37972
#% 158051
#% 168750
#% 172877
#% 214836
#% 286256
#% 458544

#index 314501
#* Active real-time database management for command & control applications
#@ Bhavani Thuraisingham;Eric Hughes;Peter Krupp;Gary Gengo;Alice Schafer;Mike Squadrito
#t 1996
#c 1

#index 314502
#* Production information management for batch manufacturing plants based on ECA mechanism and view generation
#@ Hideyuki Takada;Hiromitsu Shimakawa;Yoshitomo Asano;Morikazu Takegaki
#t 1996
#c 1
#% 135384
#% 288821
#% 501937

#index 314503
#* Project synopsis: evaluating STRIP
#@ Brad Adelberg;Hector Garcia-Molina
#t 1996
#c 1
#% 227879
#% 339366
#% 458544

#index 316462
#* Proceedings of the ninth international conference on Information and knowledge management
#@ Arvin Agah;Jamie Callan;Elke Rundensteiner;Susan Gauch
#t 2000
#c 1

#index 316471
#* A relevant research agenda for the decision support industry
#@ Erik Thomsen
#t 2000
#c 1

#index 316473
#* Digital libraries: extending and applying library and information science and technology
#@ Edward A. Fox
#t 2000
#c 1

#index 316474
#* Scalable association-based text classification
#@ Dimitris Meretakis;Dimitris Fragoudis;Hongjun Lu;Spiros Likothanassis
#t 2000
#c 1
#% 118736
#% 165110
#% 165111
#% 232653
#% 246832
#% 260001
#% 269217
#% 275837
#% 280439
#% 280817
#% 300120
#% 458369
#% 458379
#% 481290
#% 1499571

#index 316478
#* Fast supervised dimensionality reduction algorithm with applications to document categorization & retrieval
#@ George Karypis;Eui-Hong (Sam) Han
#t 2000
#c 1
#% 36672
#% 60576
#% 67565
#% 118771
#% 136350
#% 169777
#% 190429
#% 200694
#% 229972
#% 252836
#% 262059
#% 280404
#% 280492
#% 280817
#% 304423
#% 318412
#% 385564
#% 458379
#% 465754
#% 710542
#% 1650665

#index 316481
#* Clustering through decision tree construction
#@ Bing Liu;Yiyuan Xia;Philip S. Yu
#t 2000
#c 1
#% 36672
#% 80995
#% 136350
#% 210173
#% 248790
#% 248792
#% 273891
#% 273900
#% 280417
#% 280419
#% 287285
#% 300131
#% 424759
#% 451052
#% 479787
#% 479799
#% 479962
#% 481281
#% 481945
#% 520745
#% 566128
#% 631985

#index 316483
#* A semi-supervised document clustering technique for information organization
#@ Han-Joon Kim;Sang-Goo Lee
#t 2000
#c 1
#% 35942
#% 36438
#% 93901
#% 115469
#% 118726
#% 144023
#% 194289
#% 197847
#% 227796
#% 232655
#% 240146
#% 249155
#% 280492
#% 375017
#% 466395
#% 1378221

#index 316486
#* Dynamic generation of data broadcasting programs for a broadcast disk array in a mobile computing environment
#@ Wen-Chih Peng;Ming-Syan Chen
#t 2000
#c 1
#% 172913
#% 201897
#% 247246
#% 259632
#% 259634
#% 265208
#% 281536
#% 287068
#% 287258
#% 443127
#% 443263
#% 479961
#% 482107
#% 632067
#% 635819

#index 316491
#* SAIU: an efficient cache replacement policy for wireless on-demand broadcasts
#@ Jianliang Xu;Qinglong Hu;Dik Lun Lee;Wang-Chien Lee
#t 2000
#c 1
#% 172874
#% 175253
#% 201897
#% 245014
#% 259634
#% 269631
#% 274200
#% 279165
#% 281536
#% 290747
#% 419606
#% 443262
#% 443294
#% 566126
#% 1848595

#index 316493
#* A framework for designing update objects to improve server scalability in intermittently synchronized databases
#@ Wai Gen Yee;Michael J. Donahoo;Shamkant B. Navathe
#t 2000
#c 1
#% 98469
#% 172875
#% 172956
#% 175253
#% 201897
#% 210210
#% 281469
#% 287082
#% 462227
#% 707804

#index 316498
#* A framework for modeling buffer replacement strategies
#@ Stephane Bressan;Chong Leng Goh;Beng Chin Ooi;Kian-Lee Tan
#t 2000
#c 1
#% 2196
#% 77990
#% 83060
#% 83337
#% 152943
#% 248794
#% 462218
#% 479647
#% 479964
#% 480967
#% 482046

#index 316500
#* Boosting for document routing
#@ Raj D. Iyer;David D. Lewis;Robert E. Schapire;Yoram Singer;Amit Singhal
#t 2000
#c 1
#% 46803
#% 86371
#% 169774
#% 184488
#% 194301
#% 218982
#% 232646
#% 235377
#% 262037
#% 262085
#% 302391
#% 375017
#% 564279
#% 1272396

#index 316508
#* An improved boosting algorithm and its application to text categorization
#@ Fabrizio Sebastiani;Alessandro Sperduti;Nicola Valdambrini
#t 2000
#c 1
#% 194284
#% 219050
#% 219051
#% 235377
#% 262085
#% 275837
#% 302391
#% 311034
#% 318412
#% 445319
#% 458379
#% 465754
#% 465895
#% 466101
#% 500919
#% 564279
#% 650844

#index 316509
#* Analyzing the effectiveness and applicability of co-training
#@ Kamal Nigam;Rayid Ghani
#t 2000
#c 1
#% 246831
#% 252011
#% 283180
#% 304917
#% 311027
#% 333797
#% 458369
#% 465895
#% 466263
#% 565531
#% 748550
#% 1272282

#index 316510
#* Estimating nested selectivity in object-oriented databases
#@ Wan-Sup Cho;Wook-Shin Han;Ki-Hyung Hong;Kyu-Young Whang
#t 2000
#c 1
#% 54047
#% 69273
#% 86948
#% 116091
#% 116203
#% 140389
#% 172931
#% 206913
#% 285924
#% 320113
#% 411554
#% 435124
#% 435135
#% 443137
#% 534716

#index 316511
#* Maintaining views in object-relational databases
#@ Jixue Liu;Millist Vincent;Mukesh Mohania
#t 2000
#c 1
#% 152928
#% 385828
#% 479629
#% 562196
#% 637834

#index 316512
#* Indexing inheritance and aggregation
#@ Karen C. Davis;Unmi Tina Kang;Shobha Ravishankar
#t 2000
#c 1

#index 316513
#* Relevance and reinforcement in interactive browsing
#@ Anton Leuski
#t 2000
#c 1
#% 55490
#% 65946
#% 109206
#% 118728
#% 169783
#% 183255
#% 194283
#% 194301
#% 218978
#% 218992
#% 219049
#% 219050
#% 219052
#% 232653
#% 241033
#% 252758
#% 262085
#% 262087
#% 262173
#% 262193
#% 283716
#% 384911
#% 641139

#index 316514
#* Models for reader interaction systems
#@ Daniel Berleant
#t 2000
#c 1
#% 41665
#% 56449
#% 60635
#% 144033
#% 152258
#% 187992
#% 194251
#% 237079
#% 238195
#% 245840
#% 252750
#% 266219
#% 268109
#% 270942
#% 287209
#% 290800
#% 387791
#% 424263
#% 856129

#index 316515
#* Elicitation queries to the excite Web search engine
#@ 
#t 2000
#c 1

#index 316516
#* Object and query transformation: supporting multi-dimensional queries through code reuse
#@ Byunggu Yu;Ratko Orlandic
#t 2000
#c 1
#% 77928
#% 83319
#% 86950
#% 86951
#% 114582
#% 116087
#% 169822
#% 169940
#% 201878
#% 201880
#% 227864
#% 248796
#% 252304
#% 273888
#% 286929
#% 287193
#% 317933
#% 317950
#% 382327
#% 411694
#% 415957
#% 427199
#% 462956
#% 480278
#% 481599
#% 481956
#% 495247
#% 526845
#% 526864
#% 566113
#% 711733

#index 316517
#* Complex object retrieval via structural join index hierarchy mechanisms: evaluation and selection approaches
#@ Chi-wai Fung;Kamalakar Karlapalem;Qing Li
#t 2000
#c 1
#% 18614
#% 58373
#% 86954
#% 102761
#% 286189
#% 311884
#% 315765
#% 320113
#% 442665
#% 442888
#% 443137
#% 463603
#% 481276
#% 481417
#% 481623
#% 482090
#% 511480
#% 614566
#% 708430

#index 316518
#* Sampling from databases using B+-trees
#@ Dimuthu Makawita;Kian-Lee Tan;Huan Liu
#t 2000
#c 1
#% 1389
#% 3276
#% 77967
#% 210190
#% 238413
#% 248821
#% 273908
#% 273909
#% 274152
#% 280406
#% 427219
#% 479931

#index 316519
#* Creating and evaluating multi-document sentence extract summaries
#@ Jade Goldstein;Vibhu Mittal;Jaime Carbonell;Jamie Callan
#t 2000
#c 1
#% 67565
#% 71752
#% 198296
#% 262042
#% 262112
#% 280835
#% 283171
#% 381263
#% 648114
#% 741106
#% 748577
#% 853647
#% 1478826

#index 316520
#* Automatically summarising Web sites: is there a way around it?
#@ Einat Amitay;Cécile Paris
#t 2000
#c 1
#% 64904
#% 136350
#% 184763
#% 211514
#% 247317
#% 249143
#% 268079
#% 282905
#% 742437
#% 742440
#% 746875
#% 757855

#index 316521
#* Retrieving descriptive phrases from large amounts of free text
#@ Hideo Joho;Mark Sanderson
#t 2000
#c 1
#% 144033
#% 198058
#% 288334
#% 742428
#% 756964

#index 316522
#* Learning a monolingual language model from a multilingual text database
#@ Rayid Ghani;Rosie Jones
#t 2000
#c 1
#% 252472
#% 273926
#% 278102
#% 466250
#% 740915
#% 748738

#index 316523
#* Space efficient bitmap indexing
#@ Nick Koudas
#t 2000
#c 1
#% 115608
#% 118767
#% 191154
#% 201921
#% 210190
#% 217812
#% 227861
#% 248814
#% 248815
#% 273904
#% 273905
#% 408396
#% 462217
#% 466953
#% 479648
#% 481266
#% 482100

#index 316524
#* Vector approximation based indexing for non-uniform high dimensional data sets
#@ Hakan Ferhatosmanoglu;Ertem Tuncel;Divyakant Agrawal;Amr El Abbadi
#t 2000
#c 1
#% 68091
#% 88056
#% 114667
#% 169940
#% 172949
#% 212690
#% 227999
#% 232484
#% 237187
#% 252304
#% 262113
#% 275367
#% 464195
#% 479649
#% 479973
#% 481947
#% 481956
#% 631963

#index 316525
#* The subspace coding method: a new indexing scheme for high-dimensional data
#@ Yasushi Sakurai;Masatoshi Yoshikawa;Shunsuke Uemura;Haruhiko Kojima
#t 2000
#c 1
#% 41923
#% 86950
#% 201876
#% 227939
#% 237187
#% 464195
#% 479649
#% 481455
#% 481956
#% 527026

#index 316526
#* Dimensionality reduction and similarity computation by inner product approximations
#@ Ömer Eğecioğlu;Hakan Ferhatosmanoğlu
#t 2000
#c 1
#% 86950
#% 169805
#% 169940
#% 172949
#% 212690
#% 232484
#% 237187
#% 245787
#% 248796
#% 248797
#% 248798
#% 275367
#% 427199
#% 460862
#% 479973
#% 481947
#% 482109
#% 682214
#% 682235

#index 316527
#* Personal ontologies for web navigation
#@ Jason Chaffee;Susan Gauch
#t 2000
#c 1
#% 55490
#% 115462
#% 179876
#% 218982
#% 232708
#% 240957
#% 252753
#% 262054
#% 280817
#% 287204
#% 287205
#% 287214
#% 287284
#% 290149
#% 294995
#% 445309
#% 560829
#% 637576
#% 1275346
#% 1499473

#index 316528
#* Persistence of information on the web: analyzing citations contained in research articles
#@ Steve Lawrence;Frans Coetzee;Eric Glover;Gary Flake;David Pennock;Bob Krovetz;Finn Nielsen;Andries Kruger;Lee Giles
#t 2000
#c 1
#% 209663
#% 209685
#% 255684
#% 268079
#% 268109
#% 281346
#% 287209
#% 433674
#% 438103
#% 584893
#% 674852

#index 316529
#* Semantic search on Internet tabular information extraction for answering queries
#@ H. L. Wang;S. H. Wu;I. C. Wang;C. L. Sung;W. L. Hsu;W. K. Shih
#t 2000
#c 1
#% 443386
#% 493170
#% 703070

#index 316530
#* Learning to extract hierarchical information from semi-structured documents
#@ Wai-Yip Lin;Wai Lam
#t 2000
#c 1
#% 244103
#% 248809
#% 266216
#% 273925
#% 275915
#% 278109
#% 281218
#% 283050
#% 283136
#% 287202
#% 287283
#% 312860
#% 331772
#% 431536
#% 511496
#% 535703

#index 316531
#* A visual tool for structuring and modeling organizational memories
#@ Tang-Ho Lê;Luc Lamontagne;Tho-Hau Nguyen
#t 2000
#c 1
#% 258186
#% 290655
#% 363546
#% 494448

#index 316532
#* On equivalence of queries in uncertain databases
#@ Michael F. Bianco;Fereidoon Sadri
#t 2000
#c 1

#index 316533
#* DEADLINER: building a new niche search engine
#@ A. Kruger;C. L. Giles;F. M. Coetzee;E. Glover;G. W. Flake;S. Lawrence;C. Omlin
#t 2000
#c 1
#% 190581
#% 248808
#% 269218
#% 287217
#% 304935
#% 341964
#% 365897
#% 438103
#% 458379
#% 480309
#% 527984
#% 632051

#index 316534
#* Collection selection and results merging with topically organized U.S. patents and TREC data
#@ Leah S. Larkey;Margaret E. Connell;Jamie Callan
#t 2000
#c 1
#% 55490
#% 184489
#% 194244
#% 194245
#% 194246
#% 194275
#% 237312
#% 262063
#% 267454
#% 273926
#% 280853
#% 280856
#% 281396
#% 309133
#% 375017
#% 481748
#% 567255

#index 316535
#* Discovery of similarity computations of search engines
#@ King-Kup Liu;Weiyi Meng;Clement Yu
#t 2000
#c 1
#% 46803
#% 115462
#% 176530
#% 183255
#% 194246
#% 194275
#% 218982
#% 232701
#% 273926
#% 385946
#% 406493
#% 443561
#% 479451
#% 479642
#% 481748
#% 591588

#index 316536
#* High performance clustering based on the similarity join
#@ Christian Böhm;Bernhard Braunmüller;Markus Breunig;Hans-Peter Kriegel
#t 2000
#c 1
#% 36672
#% 86950
#% 102772
#% 152934
#% 152937
#% 172909
#% 201893
#% 210186
#% 210187
#% 227932
#% 248790
#% 248792
#% 252304
#% 273890
#% 300136
#% 363781
#% 376266
#% 411694
#% 412588
#% 421052
#% 427199
#% 435141
#% 443083
#% 458741
#% 462070
#% 462236
#% 464205
#% 479453
#% 479473
#% 479658
#% 479791
#% 479799
#% 480093
#% 481281
#% 481947
#% 481956
#% 527021

#index 316537
#* Using star clusters for filtering
#@ Javed Aslam;Katya Pelekhov;Daniela Rus
#t 2000
#c 1
#% 46809
#% 109223
#% 119916
#% 144023
#% 194283
#% 197847
#% 201073
#% 219048
#% 219050
#% 226099
#% 228105
#% 232768
#% 237052
#% 260009
#% 262043
#% 262087
#% 275929
#% 282435
#% 375017
#% 677440
#% 978507

#index 316538
#* Supporting subseries nearest neighbor search via approximation
#@ Changzhou Wang;X. Sean Wang
#t 2000
#c 1
#% 172949
#% 201876
#% 237187
#% 248796
#% 248797
#% 248831
#% 252304
#% 271202
#% 273713
#% 460862
#% 462239
#% 481947
#% 617886
#% 631923

#index 316539
#* n23tool: a tool for exploring large relational datasets through 3D dynamic projections
#@ Li Yang
#t 2000
#c 1
#% 1211
#% 17144
#% 76703
#% 86286
#% 434477
#% 619521
#% 641108

#index 316540
#* Visual query and analysis tool of the object-relational GIS framework
#@ Zoran Stojanovic;Slobodanka Djordjevic-Kajan;Dragan Stojanovic
#t 2000
#c 1
#% 26168
#% 435140
#% 442847
#% 526995

#index 316541
#* An access control model for video database systems
#@ Elisa Bertino;Moustafa A. Hammad;Walid G. Aref;Ahmed K. Elmagarmid
#t 2000
#c 1
#% 164560
#% 202026
#% 204453
#% 245787
#% 250250
#% 263730
#% 263982
#% 319244
#% 381998
#% 382491
#% 435932
#% 442862
#% 443103
#% 443485
#% 571076

#index 316542
#* A meta model and an infrastructure for the non-transparent replication of object databases
#@ Werner Dreyer;Klaus R. Dittrich
#t 2000
#c 1
#% 9241
#% 29590
#% 70068
#% 116185
#% 124019
#% 150431
#% 163434
#% 173870
#% 201946
#% 202146
#% 320187
#% 380821
#% 402902
#% 443232
#% 503562
#% 503711
#% 567850

#index 316543
#* Polar: an architecture for a parallel ODMG compliant object database
#@ Jim Smith;Paul Watson;Sandra de F. Mendes Sampaio;Norman Paton
#t 2000
#c 1
#% 86929
#% 99074
#% 115661
#% 136740
#% 152904
#% 152942
#% 165947
#% 172939
#% 201873
#% 213223
#% 217052
#% 235914
#% 339790
#% 380564
#% 385828
#% 442698
#% 464816
#% 481597
#% 489207
#% 489368
#% 509439
#% 523417
#% 588602

#index 316544
#* Data replication for external searching in static tree structures
#@ Susanne E. Hambrusch;Chuan-Ming Liu
#t 2000
#c 1
#% 68089
#% 68115
#% 91077
#% 164362
#% 225006
#% 237197
#% 237204
#% 248023
#% 281731
#% 296253
#% 463743
#% 481278
#% 482090

#index 316545
#* Query optimisation using an improved genetic algorithm
#@ L. Tamine;M. Boughanem
#t 2000
#c 1

#index 316546
#* First story detection in TDT is hard
#@ James Allan;Victor Lavrenko;Hubert Jin
#t 2000
#c 1
#% 262042
#% 262043
#% 375017
#% 406493
#% 408396
#% 710374

#index 316547
#* A distributed multi-agent system for collaborative information management and sharing
#@ James R. Chen;Shawn R. Wolfe;Stephen D. Wragg
#t 2000
#c 1
#% 55490
#% 73033
#% 115476
#% 220708
#% 241021
#% 241033
#% 249155
#% 252834
#% 255125
#% 268226
#% 281485
#% 283055
#% 465747
#% 742724

#index 316548
#* Language models for financial news recommendation
#@ Victor Lavrenko;Matt Schmill;Dawn Lawrie;Paul Ogilvie;David Jensen;James Allan
#t 2000
#c 1
#% 14749
#% 194301
#% 219048
#% 246831
#% 280413
#% 300542
#% 646510

#index 316549
#* On efficient storage space distribution among materialized views and indices in data warehousing environments
#@ Ladjel Bellatreche;Kamalakar Karlapalem;Michel Schneider
#t 2000
#c 1
#% 36117
#% 36119
#% 210208
#% 227861
#% 248815
#% 273697
#% 273917
#% 289282
#% 305947
#% 442999
#% 462079
#% 464706
#% 479809
#% 482110
#% 482111
#% 631946
#% 631950

#index 316550
#* Extending OLAP querying to external object databases
#@ Torben Bach Pedersen;Arie Shoshani;Junmin Gu;Christian S. Jensen
#t 2000
#c 1
#% 77312
#% 85086
#% 235914
#% 239243
#% 308509
#% 420053
#% 459010
#% 479618
#% 480123
#% 503731
#% 562299
#% 564420
#% 565454
#% 631925

#index 316551
#* Using wavelet decomposition to support progressive and approximate range-sum queries over data cubes
#@ Yi-Leh Wu;Divyakant Agrawal;Amr El Abbadi
#t 2000
#c 1
#% 210190
#% 227866
#% 227880
#% 248040
#% 248822
#% 257637
#% 259995
#% 273902
#% 420053
#% 463760
#% 464062
#% 464215
#% 481951
#% 631947

#index 316552
#* Sequence mining in categorical domains: incorporating constraints
#@ Mohammed J. Zaki
#t 2000
#c 1
#% 172386
#% 248785
#% 259993
#% 280488
#% 287242
#% 420063
#% 459006
#% 461903
#% 463903
#% 479971

#index 316553
#* Retrieval from captioned image databases using natural language processing
#@ David Elworthy
#t 2000
#c 1
#% 129662
#% 219036
#% 478263
#% 742272
#% 747944
#% 1275285

#index 316554
#* The webspace method: on the integration of database technology with multimedia retrieval
#@ Roelof van Zwol;Peter M. G. Apers
#t 2000
#c 1
#% 109218
#% 111368
#% 227995
#% 273681
#% 281149
#% 281150
#% 309726
#% 309729
#% 464816
#% 464825
#% 504574
#% 665559

#index 316555
#* Extensible perfect hashing
#@ Takao Miura;Wataru Matsumoto;Isamu Shioya;Yukio Wada
#t 2000
#c 1
#% 662
#% 36360
#% 40634
#% 287317
#% 319568
#% 320478

#index 316556
#* A query based approach for integrating heterogeneous data sources
#@ Ruxandra Domenig;Klaus R. Dittrich
#t 2000
#c 1
#% 22948
#% 116303
#% 227886
#% 227995
#% 229827
#% 291299
#% 294600
#% 298602
#% 435102
#% 443235
#% 464720
#% 479951
#% 481923
#% 591538

#index 316557
#* Theoretical foundations of schema restructuring in heterogeneous multidatabase systems
#@ Joseph Albert
#t 2000
#c 1
#% 5374
#% 11284
#% 18234
#% 24408
#% 54584
#% 102748
#% 158906
#% 166203
#% 169061
#% 213983
#% 286831
#% 292680
#% 322880
#% 415979
#% 462001
#% 462344
#% 462619
#% 463849
#% 464007
#% 464035
#% 480969
#% 481911
#% 481944
#% 482067
#% 534707
#% 535820
#% 591533
#% 614600
#% 704598

#index 316558
#* A market-based resource management and QoS support framework for distributed multimedia systems
#@ Wonjun Lee;Jaideep Srivastava
#t 2000
#c 1
#% 42408
#% 57529
#% 203554
#% 223278
#% 239984
#% 340597
#% 343943
#% 423001
#% 520437
#% 565360
#% 615026
#% 615570
#% 636338
#% 1180231

#index 316559
#* Index interpolation: an approach to subsequence matching supporting normalization transform in time-series databases
#@ Woong-Kee Loh;Sang-Wook Kim;Kyu-Young Whang
#t 2000
#c 1
#% 86950
#% 132779
#% 172949
#% 227857
#% 273704
#% 358276
#% 403487
#% 427199
#% 460862
#% 462231
#% 479649
#% 480093
#% 481609
#% 481611
#% 481956
#% 534183
#% 631923

#index 316560
#* A comparison of DFT and DWT based similarity search in time-series databases
#@ Yi-Leh Wu;Divyakant Agrawal;Amr El Abbadi
#t 2000
#c 1
#% 172949
#% 227857
#% 257637
#% 460862
#% 462231
#% 481609
#% 631923
#% 632088
#% 632089

#index 316561
#* A comparative study of log-only and in-place update based temporal object database systems
#@ Kjetil Nørvåg
#t 2000
#c 1
#% 107692
#% 116057
#% 317988
#% 463430
#% 479483
#% 480096
#% 566134
#% 617874

#index 316562
#* Rule-assisted prefetching in Web-server caching
#@ Bin Lan;Stephane Bressan;Beng Chin Ooi;Kian-Lee Tan
#t 2000
#c 1
#% 152934
#% 152939
#% 199863
#% 209891
#% 211739
#% 271420
#% 443262
#% 480780
#% 635321

#index 316563
#* WebCQ-detecting and delivering information changes on the web
#@ Ling Liu;Calton Pu;Wei Tang
#t 2000
#c 1
#% 227859
#% 227885
#% 424263
#% 443298
#% 978507

#index 316564
#* Collaborative proxy system for distributed Web content transcoding
#@ Valeria Cardellini;Philip S. Yu;Yun-Wu Huang
#t 2000
#c 1
#% 239969
#% 271423
#% 286466
#% 298015
#% 424280
#% 443262
#% 963893
#% 979357
#% 1775058

#index 316754
#* Proceedings of the 3rd ACM international workshop on Data warehousing and OLAP
#@ Rokia Missaoui;Il-Yeol Song
#t 2000
#c 1
#% 29439
#% 73570
#% 128036
#% 131859
#% 157340
#% 188338
#% 211885
#% 234819
#% 262249
#% 385704
#% 452272
#% 460079
#% 479384
#% 484888
#% 502089
#% 511522
#% 511713
#% 512035
#% 514790
#% 516936
#% 529264
#% 543670
#% 559002
#% 566511
#% 1395388
#% 1782996

#index 316760
#* Proceedings of the 3rd ACM international workshop on Data warehousing and OLAP
#@ Rokia Missaoui;Il-Yeol Song
#t 2000
#c 1

#index 316934
#* Proceedings of the 8th ACM international symposium on Advances in geographic information systems
#@ Ki-Joune Li;Kia Makki;Niki Pissinou;Siva Ravada
#t 2000
#c 1

#index 342435
#* Proceedings of the 2001 ACM Symposium on Document engineering
#@ Ethan V. Munson
#t 2001
#c 1
#! Document engineering is an emerging discipline within computer science that investigates sys-tems for documents in any form and in all media. Document engineering is concerned with prin-ciples, tools and processes that improve our ability to create, manage and maintain documents, just as software engineering examines the same issues for software.The importance of documents and document technology is difficult to over-emphasize. Docu-ment software was one of the key reasons for the widespread adoption of personal computers. Document technology is one of the key underpinnings of the World Wide Web, in the form of representation languages, concepts of structure and hypermedia, and analysis and retrieval tech-niques. Document technology is so useful that it is now being extended to support applications such as interprocess communication that have previously used special-purpose encodings and techniques.This proceedings is the research record of the inaugural meeting of the ACM Symposium on Document Engineering (DocEng '01), held November 2001 in Atlanta. DocEng '01 is the de-scendant of several different conferences: the long series of biennial Electronic Publishing (EP) conferences and Principles of Document Processing workshops, the 1988 ACM Conference on Document Processing Systems, and the 1981 ACM Symposium on Text Manipulation.

#index 342656
#* Efficient processing of conical queries
#@ Hakan Ferhatosmanoglu;Divyakant Agrawal;Amr El Abbadi
#t 2001
#c 1
#% 68091
#% 86950
#% 183360
#% 232484
#% 245787
#% 248796
#% 252304
#% 275367
#% 285932
#% 287257
#% 290703
#% 316524
#% 427199
#% 464195
#% 479649
#% 631955
#% 631963
#! Conical queries are a novel type of query with an increasing number of applications. Traditional index structures and retrieval mechanisms, in general, have been optimized for rectangular and circular queries, rather than conical queries. In this paper, we focus on conical queries which can be defined as a multi-dimensional cone in a multi-dimensional data space. We develop a model for expressing such queries and suggest efficient techniques for evaluating them. In particular, we explore the retrieval problem in the context of conical query processing and develop multi-disk allocation methods specifically for processing conical queries.

#index 342657
#* Effective nearest neighbor indexing with the euclidean metric
#@ Sang-Wook Kim;Charu C. Aggarwal;Philip S. Yu
#t 2001
#c 1
#% 43265
#% 86950
#% 102772
#% 201876
#% 217292
#% 227856
#% 227939
#% 237187
#% 435141
#% 462239
#% 463597
#% 464195
#% 479462
#% 479649
#% 480132
#% 481947
#% 481956
#% 632035
#! The nearest neighbor search is an important operation widely-used in multimedia databases. In higher dimensions, most of previous methods for nearest neighbor search become inefficient and require to compute nearest neighbor distances to a large fraction of points in the space. In this paper, we present a new approach for processing nearest neighbor search with the Euclidean metric, which searches over only a small subset of the original space. This approach effectively approximates clusters by encapsulating them into geometrically regular shapes and also computes better upper and lower bounds of the distances from the query point to the clusters. For showing the effectiveness of the proposed approach, we perform extensive experiments. The results reveal that the proposed approach significantly outperforms the X-tree as well as the sequential scan.

#index 342658
#* Query-sensitive similarity measures for the calculation of interdocument relationships
#@ Anastasios Tombros;C. J. van Rijsbergen
#t 2001
#c 1
#% 3585
#% 5182
#% 11646
#% 29587
#% 32813
#% 46809
#% 83855
#% 167557
#% 169729
#% 187772
#% 218992
#% 227796
#% 228106
#% 306468
#% 375017
#% 427921
#% 840583
#! The application of document clustering to information retrieval has been motivated by the potential effectiveness gains postulated by the Cluster Hypothesis. The hypothesis states that relevant documents tend to be highly similar to each other, and therefore tend to appear in the same clusters. In this paper we propose that, for any given query, pairs of relevant documents will exhibit an inherent similarity which is dictated by the query itself. Our research describes an attempt to devise means by which this similarity can be detected. We propose the use of query-sensitive similarity measures that bias interdocument relationships towards pairs of documents that jointly possess attributes that are expressed in a query. We experimentally tested query-sensitive measures against conventional ones that do not take the context of the query into account. We calculated interdocument relationships for varying numbers of top-ranked documents for five document collections. Our results show a consistent and significant increase in the number of relevant documents that become nearest neighbours of any given relevant document when query-sensitive measures are used. These results suggest that the effectiveness of a cluster-based IR system has the potential to increase through the use of query-sensitive similarity measures.

#index 342659
#* Bipartite graph partitioning and data clustering
#@ Hongyuan Zha;Xiaofeng He;Chris Ding;Horst Simon;Ming Gu
#t 2001
#c 1
#% 194388
#% 224113
#% 280819
#% 309128
#% 319865
#% 327474
#% 420083
#% 592143
#! Many data types arising from data mining applications can be modeled as bipartite graphs, examples include terms and documents in a text corpus, customers and purchasing items in market basket analysis and reviewers and movies in a movie recommender system. In this paper, we propose a new data clustering method based on partitioning the underlying bipartite graph. The partition is constructed by minimizing a normalized sum of edge weights between unmatched pairs of vertices of the bipartite graph. We show that an approximate solution to the minimization problem can be obtained by computing a partial singular value decomposition (SVD) of the associated edge weight matrix of the bipartite graph. We point out the connection of our clustering algorithm to correspondence analysis used in multivariate analysis. We also briefly discuss the issue of assigning data objects to multiple clusters. In the experimental results, we apply our clustering algorithm to the problem of document clustering to illustrate its effectiveness and efficiency.

#index 342660
#* Evaluating document clustering for interactive information retrieval
#@ Anton Leuski
#t 2001
#c 1
#% 29585
#% 46809
#% 55490
#% 111303
#% 118731
#% 144023
#% 194278
#% 214709
#% 218978
#% 218992
#% 219049
#% 228105
#% 230521
#% 262075
#% 316513
#% 375017
#% 714020
#! We consider the problem of organizing and browsing the top ranked portion of the documents returned by an information retrieval system. We study the effectiveness of a document organization in helping a user to locate the relevant material among the retrieved documents as quickly as possible. In this context we examine a set of clustering algorithms and experimentally show that a clustering of the retrieved documents can be significantly more effective than traditional ranked list approach. We also show that the clustering approach can be as effective as the interactive relevance feedback based on query expansion while retaining an important advantage -- it provides the user with a valuable sense of control over the feedback process.

#index 342661
#* Extracting meaningful labels for WEBSOM text archives
#@ Arnulfo P. Azcarraga;Teddy N. Yap, Jr.
#t 2001
#c 1
#% 67565
#% 109213
#% 232653
#% 304421
#% 318412
#% 477812
#% 511811
#! Self-Organizing Maps, being used mainly with data that are not pre-labeled, need automatic procedures for extracting keywords as labels for each of the map units. The WEBSOM methodology for building very large text archives has a very slow method for extracting such unit labels. It computes the relative frequencies of all the words of all the documents associated to each unit and then compares these to the relative frequencies of all the words of all the other units of the map. Since maps may have more than 100,000 units and the archive may contain up to 7 million documents, the existing WEBSOM method is not practical. This paper describes how the meaningful labels per map unit can be deduced by analyzing the relative weight distribution of the SOM weight vectors and by taking advantage of some characteristics of the random projection method used in dimensionality reduction. The effectiveness of this technique is demonstrated on archives of the well studied Reuters and CNN collections. Comparisons with the WEBSOM method are provided.

#index 342662
#* Exposing the vagueness of query results on partly inaccessible databases
#@ Oliver Haase;Andreas Henrich
#t 2001
#c 1
#% 32879
#% 64412
#% 84656
#% 199860
#% 287007
#% 443354
#% 461214
#! Query processing on partly inaccessible databases generally does not yield exact, but vague result sets. A good notion of vague sets fulfills two aims: It keeps the degree of vagueness of the query result as small as possible, and it clarifies the degree of and the reasons for the vagueness to the end user. The first goal requires a good internal representation, while the second goal requires a good external representation of a vague set. In this paper, we present a novel calculus for expressive vague sets that meets both requirements. This is the first approach that is well suited for both internal and external representation of vagueness induced by partial inaccessibility. It consists of a data representation that is capable of holding all the necessary information. Complementary, we have accordingly adapted the usual query language operations. These adaptations are independent of a concrete query language, to make them applicable to most existing query languages. The adapted operations minimize the vagueness of the result, propagate the reasons of uncertainty of the individual vague candidates, and compute an expressive description of the missing elements.

#index 342663
#* Towards a visual query interface for phylogenetic databases
#@ Hasan M. Jamil;Giovanni A. Modica;Maria A. Teran
#t 2001
#c 1
#% 306043
#% 589391
#! Querying and visualization of phylogenetic databases remain a great challenge due to their complex tree type semi structured nature. Naturally, successful phylogenetic databases such as the Tree of Life database at the University of Arizona are implemented as Web documents in HTML. While Web implementation of such databases facilitate the representation, and in part, visualization of their contents, querying remains an issue. The interoperability of Web-based phylogenetic databases with other similar databases such as TreeBase and RDB which are implemented using traditional database management systems, has not been possible due to the impedance mismatch between the underlying query and data representation framework. In this paper, we present a novel approach to phylogentic database management using existing database technologies without compromising potential opportunities for visualization and interoperability. We present a Web-based tool for the creation, querying and visualization of phylogenetic databases. We demonstrate the functional capabilities and strengths of our system by recreating the Tree of Life database in our system and performing queries that are not possible in the original Tree of Life database.

#index 342664
#* A relational algebra for data/metadata integration in a federated database system
#@ Catharine Wyss;Dirk Van Gucht
#t 2001
#c 1
#% 102748
#% 123121
#% 199537
#% 213969
#% 214783
#% 227989
#% 287333
#% 435111
#% 479618
#% 479968
#% 481944
#% 535513
#% 535820
#% 571060
#% 679109
#! The need for interoperability among databases has increased dramatically with the proliferation of readily available DBMS and application software. Even within a single organization, data from disparate relational databases must be integrated. A framework for interoperability in a federated system of relational databases should be inherently relational, so that it can use existing techniques for query evaluation and optimization where possible and retain the key features of SQL, such as a modest complexity and ease of query formulation. Our contribution is a logspace relational algebra, the Meta-Algebra (MA), for data/metadata integration among relational databases containing semantically similar information in schematically disparate formats. The MA is a simple yet powerful extension of the classical relational algebra (RA). The MA has a natural declarative counterpart, the Meta-Query Language (MQL), which we briefly describe. We state a result showing MQL and the MA are computationally equivalent, which enables us to algebratize MQL queries in fundamentally the same way as ordinary SQL queries. This algebratization in turn enables us to use MA equivalences to facilitate the application of known query optimization techniques to MQL query evaluation.

#index 342665
#* Approximately common patterns in shared-forests
#@ M. Vilares;F. J. Ribadas;J. Graña
#t 2001
#c 1
#% 162626
#% 256623
#% 286669
#% 748507
#! We present a proposal intended to demonstrate the applicability of tabulation techniques for detecting approximately common patterns when dealing with structures sharing some common parts. This sharing saves on the space needed to represent the structures and also on their later processing, by factorizing the filtering of substructure matching. As a consequence, preliminary experimental tests indicate a reduction of the running time.

#index 342666
#* Multi-dimensional sequential pattern mining
#@ Helen Pinto;Jiawei Han;Jian Pei;Ke Wang;Qiming Chen;Umeshwar Dayal
#t 2001
#c 1
#% 172892
#% 210160
#% 259993
#% 273916
#% 333925
#% 420063
#% 459006
#% 463903
#% 464839
#% 464996
#% 465022
#% 477791
#% 479627
#% 479971
#% 631926
#! Sequential pattern mining, which finds the set of frequent subsequences in sequence databases, is an important data-mining task and has broad applications. Usually, sequence patterns are associated with different circumstances, and such circumstances form a multiple dimensional space. For example, customer purchase sequences are associated with region, time, customer group, and others. It is interesting and useful to mine sequential patterns associated with multi-dimensional information.In this paper, we propose the theme of multi-dimensional sequential pattern mining, which integrates the multidimensional analysis and sequential data mining. We also thoroughly explore efficient methods for multi-dimensional sequential pattern mining. We examine feasible combinations of efficient sequential pattern mining and multi-dimensional analysis methods, as well as develop uniform methods for high-performance mining. Extensive experiments show the advantages as well as limitations of these methods. Some recommendations on selecting proper method with respect to data set properties are drawn.

#index 342667
#* Mining confident rules without support requirement
#@ Ke Wang;Yu He;David W. Cheung
#t 2001
#c 1
#% 152934
#% 239588
#% 310539
#% 479817
#% 480940
#% 481290
#% 631970
#% 632029
#! An open problem is to find all rules that satisfy a minimum confidence but not necessarily a minimum support. Without the support requirement, the classic support-based pruning strategy is inapplicable. The problem demands a confidence-based pruning strategy. In particular, the following monotonicity of confidence, called the universal-existential upward closure, holds: if a rule of size k is confident (for the given minimum confidence), for every other attribute not in the rule, some specialization of size k+1 using the attribute must be confident. Like the support-based pruning, the bottleneck is at the memory that often is too small to store the candidates required for search. We implement this strategy on disk and study its performance.

#index 342668
#* Combining multiple classifiers for text categorization
#@ Khalid Al-Kofahi;Alex Tyrrell;Arun Vachher;Tim Travers;Peter Jackson
#t 2001
#c 1
#% 46803
#% 55490
#% 169718
#% 169729
#% 169774
#% 219051
#% 219053
#% 259998
#% 260001
#% 262096
#% 280817
#% 298735
#% 316500
#% 375017
#% 461692
#! A major problem facing online information services is how to index and supplement large document collections with respect to a rich set of categories. We focus upon the routing of case law summaries to various secondary law volumes in which they should be cited. Given the large number ( 13,000) of closely related categories, this is a challenging task that is unlikely to succumb to a single algorithmic solution. Our fully implemented and recently deployed system shows that a superior classification engine for this task can be constructed from a combination of classifiers. The multi-classifier approach helps us leverage all the relevant textual features and meta data, and appears to generalize to related classification tasks.

#index 342669
#* Text classification in a hierarchical mixture model for small training sets
#@ Kristina Toutanova;Francine Chen;Kris Popat;Thomas Hofmann
#t 2001
#c 1
#% 260001
#% 266292
#% 269217
#% 279755
#% 280817
#% 280819
#% 309141
#% 376266
#% 420466
#% 458369
#% 465747
#% 466078
#% 495795
#% 668807
#! Documents are commonly categorized into hierarchies of topics, such as the ones maintained by Yahoo! and the Open Directory project, in order to facilitate browsing and other interactive forms of information retrieval. In addition, topic hierarchies can be utilized to overcome the sparseness problem in text categorization with a large number of categories, which is the main focus of this paper. This paper presents a hierarchical mixture model which extends the standard naive Bayes classifier and previous hierarchical approaches. Improved estimates of the term distributions are made by differentiation of words in the hierarchy according to their level of generality/specificity. Experiments on the Newsgroups and the Reuters-21578 dataset indicate improved performance of the proposed classifier in comparison to other state-of-the-art methods on datasets with a small number of positive examples.

#index 342670
#* Using LSI for text classification in the presence of background text
#@ Sarah Zelikovitz;Haym Hirsh
#t 2001
#c 1
#% 124009
#% 165111
#% 200694
#% 252011
#% 266215
#% 304876
#% 311027
#% 316509
#% 458379
#% 466263
#% 466580
#% 650844
#! This paper presents work that uses Latent Semantic Indexing (LSI) for text classification. However, in addition to relying on labeled training data, we improve classification accuracy by also using unlabeled data and other forms of available "background" text in the classification process. Rather than performing LSI's singular value decomposition (SVD) process solely on the training data, we instead use an expanded term-by-document matrix that includes both the labeled data as well as any available and relevant background text. We report the performance of this approach on data sets both with and without the inclusion of the background text, and compare our work to other efforts that can incorporate unlabeled data and other background text in the classification process.

#index 342671
#* Keeping found things found on the web
#@ William Jones;Harry Bruce;Susan Dumais
#t 2001
#c 1
#% 2995
#% 199528
#% 206035
#% 214751
#% 232922
#% 233808
#% 247268
#% 249092
#% 259946
#% 265154
#% 272821
#% 272913
#% 272917
#% 318453
#! This paper describes the results of an observational study into the methods people use to manage web information for re-use. People observed in our study used a diversity of methods and associated tools. For example, several participants emailed web addresses (URLs) along with comments to themselves and to others. Other methods observed included printing out web pages, saving web pages to the hard drive, pasting the address for a web page into a document and pasting the address into a personal web site. Ironically, two web browser tools that have been explicitly developed to help users track web information - the bookmarking tool and the history list - were not widely used by participants in this study. A functional analysis helps to explain the observed diversity of methods. Methods vary widely in the functions they provide. For example, a web address pasted into a self-addressed email can provide an important reminding function together with a context of relevance: The email arrives in an inbox which is checked at regular intervals and the email can include a few lines of text that explain the URL's relevance and the actions to be taken. On the other hand, for most users in the study, the bookmarking tool ("Favorites" or "Bookmarks" depending on the browser) provided neither a reminding function nor a context of relevance. The functional analysis can help to assess the likely success of various tools, current and proposed.

#index 342672
#* Merging techniques for performing data fusion on the web
#@ Theodora Tsikrika;Mounia Lalmas
#t 2001
#c 1
#% 111303
#% 115462
#% 184496
#% 194246
#% 227891
#% 230432
#% 248218
#% 263704
#% 268078
#% 268079
#% 281174
#% 387427
#% 479451
#! Data fusion on the Web refers to the merging, into a unified single list, of the ranked document lists, which are retrieved in response to a user query by more than one Web search engine. It is performed by metasearch engines and their merging algorithms utilise the information present in the ranked lists of retrieved documents provided to them by the underlying search engines, such as the rank positions of the retrieved documents and their retrieval scores. In this paper, merging techniques are introduced that take into account not only the rank positions, but also the title and the summary accompanying the retrieved documents. Furthermore, the data fusion process is viewed as being similar to the combination of belief in uncertain reasoning and is modelled using Dempster-Shafer's theory of evidence. Our evaluation experiments indicate that the above merging techniques yield improvements in the effectiveness and that their effectiveness is comparable to that of the approach that merges the ranked lists by downloading and analysing the Web documents.

#index 342673
#* Using navigation data to improve IR functions in the context of web search
#@ Mark H. Hansen;Elizabeth Shriver
#t 2001
#c 1
#% 194275
#% 210173
#% 220709
#% 237312
#% 268079
#% 280817
#% 281209
#% 300971
#% 308763
#% 309779
#% 310567
#% 319666
#% 857390
#! As part of the process of delivering content, devices like proxies and gateways log valuable information about the activities and navigation patterns of users on the Web. In this study, we consider how this navigation data can be used to improve Web search. A query posted to a search engine together with the set of pages accessed during a search task is known as a search session. We develop a mixture model for the observed set of search sessions, and propose variants of the classical EM algorithm for training. The model itself yields a type of navigation-based query clustering. By implicitly borrowing strength between related queries, the mixture formulation allows us to identify the "highly relevant" URLs for each query cluster. Next, we explore methods for incorporating existing labeled data (the Yahoo! directory, for example) to speed convergence and help resolve low-traffic clusters. Finally, the mixture formulation also provides for a simple, hierarchical display of search results based on the query clusters. The effectiveness of our approach is evaluated using proxy access logs for the outgoing Lucent proxy.

#index 342674
#* Mining the web for answers to natural language questions
#@ Dragomir R. Radev;Hong Qi;Zhiping Zheng;Sasha Blair-Goldensohn;Zhu Zhang;Weiguo Fan;John Prager
#t 2001
#c 1
#% 81669
#% 252472
#% 262084
#% 262096
#% 279755
#% 280851
#% 309124
#% 341964
#% 529158
#% 740915
#% 741114
#% 742082
#% 742162
#% 748465
#% 817581
#% 818045
#! The web is now becoming one of the largest information and knowledge repositories. Many large scale search engines (Google, Fast, Northern Light, etc.) have emerged to help users find information. In this paper, we study how we can effectively use these existing search engines to mine the Web and discover the "correct" answers to factual natural language questions.We propose a probabilistic algorithm called QASM (Question Answering using Statistical Models) that learns the best query paraphrase of a natural language question. We validate our approach for both local and web search engines using questions from the TREC evaluation. We also show how this algorithm can be combined with another algorithm (AnSel) to produce precise answers to natural language questions.

#index 342675
#* Induction of integrated view for XML data with heterogeneous DTDs
#@ Euna Jeong;Chun-Nan Hsu
#t 2001
#c 1
#% 115478
#% 159113
#% 210214
#% 248809
#% 296931
#% 300157
#! This paper proposes a novel approach to integrating heterogeneous XML DTDs. With this approach, an information agent can be easily extended to integrate heterogeneous XML-based contents and perform federated search. Based on a tree grammar inference technique, this approach derives an integrated view of XML DTDs in an information integration framework. The derivation takes advantages of naming and structural similarities among DTDs in similar domains. The complete approach consists of three main steps. (1) DTD clustering clusters DTDs in similar domains into classes. (2) Schema learning applies a tree grammar inference technique to generate a set of tree grammar rules from the DTDs in a class from the previous step. (3) Minimization optimizes the rules generated in the previous step and transforms them into an integrated view. We have implemented the proposed approach into a system called DEEP and tested the system on artificial and real domains. The experimental results reveal that this system can effectively and efficiently integrate radically different DTDs.

#index 342676
#* Structural inference for semistructured data
#@ Jason Sankey;Raymond K. Wong
#t 2001
#c 1
#% 236416
#% 238555
#% 244109
#% 300157
#% 454221
#% 466716
#% 479465
#% 542161
#! Semistructured data presents many challenges, mainly due to its lack of a strict schema. These challenges are further magnified when large amounts of data are gathered from heterogeneous sources. We address this by investigation and development of methods to automatically infer structural information from example data. Using XML as a reference format, we approach the schema generation problem by application of inductive inference theory. In doing so, we review and extend results relating to the search spaces of grammatical inferences. We then adapt a method for evaluating the result of an inference process from computational linguistics. Further, we combine several inference algorithms, including both new techniques introduced by us and those from previous work. Comprehensive experimentation reveals our new hybrid method, based upon recently developed optimisation techniques, to be the most effective.

#index 342677
#* XOO7: applying OO7 benchmark to XML query processing tool
#@ Ying Guang Li;Stéphane Bressan;Gillian Dobbie;Zoé Lacroix;Mong Li Lee;Ullas Nambiar;Bimlesh Wadhwa
#t 2001
#c 1
#% 57555
#% 152904
#% 237191
#% 291299
#% 308463
#% 365700
#% 458584
#% 464720
#% 504578
#% 650962
#! If XML is to play the critical role of the lingua franca for Internet data interchange that many predict, it is necessary to start designing and adopting benchmarks allowing the comparative performance analysis of the tools being developed and proposed. The effectiveness of existing XML query languages has been studied by many, with a focus on the comparison of linguistic features, implicitly reflecting the fact that most XML tools exist only on paper. In this paper, with a focus on efficiency and concreteness, we propose a pragmatic first step toward the systematic benchmarking of XML query processing platforms with an initial focus on the data (versus document) point of view. We propose XOO7, an XML version of the OO7 benchmark. We discuss the applicability of XOO7, its strengths, limitations and the extensions we are considering. We illustrate its use by presenting and discussing the performance comparison against XOO7 of three different query processing platforms for XML.

#index 342678
#* Structural proximity searching for large collections of semi-structured data
#@ Michael Barg;Raymond K. Wong
#t 2001
#c 1
#% 236416
#% 237053
#% 309726
#% 464720
#% 479803
#% 480822
#! The richness of the XML data format allows data to be structured in a way which precisely captures the semantics required by the author. It is the structure of the data, however, which forms the basis of all XML query languages. Without at least some notion of the structure, a user cannot meaningfully query the data. This problem is compounded when one considers that heterogeneous data adhering to different schema are likely to exist in the database(s) being queried. This paper proposes a solution based on an efficient proximity index. In particular, we describe a family of encoding and compression schemes which enable us to build an index to efficiently implement the proximity search. Our index is extremely small, and can reflect updates in the underlying database in modest time. Experiments show that our algorithm and implementation are fast and scale well.

#index 342679
#* The effectiveness of query expansion for distributed information retrieval
#@ Paul Ogilvie;Jamie Callan
#t 2001
#c 1
#% 169806
#% 218978
#% 262063
#% 267454
#% 280853
#% 287463
#% 301225
#% 309133
#% 316534
#% 340146
#% 504755
#% 567255
#! Query expansion has been shown effective for both single database retrieval and for distributed information retrieval where complete collection information is available. One might expect that query expansion would then work for distributed information retrieval when complete collection information is not available. However, this does not appear to be the case. When using local context analysis for query expansion in distributed retrieval with partial information, the most significant reason query expansion does not work is that merging scores of documents retrieved by expanded queries is very difficult. However, we have found that using sampled information for query expansion can give boosts in a single database environment, and that when more information is available, query expansion can work in distributed environments. We also show that most of the benefit of query expansion in distributed retrieval comes from finding good documents, and not from selecting good databases.

#index 342680
#* Approaches to collection selection and results merging for distributed information retrieval
#@ Yves Rasolofo;Faïza Abbaci;Jacques Savoy
#t 2001
#c 1
#% 194244
#% 194246
#% 194275
#% 262063
#% 267454
#% 268078
#% 280853
#% 280856
#% 287463
#% 301225
#% 306504
#% 309133
#% 309253
#% 316534
#% 648114
#% 708847
#! We have investigated two major issues in Distributed Information Retrieval (DIR), namely: collection selection and search results merging. While most published works on these two issues are based on pre-stored metadata, the approaches described in this paper involve extracting the required information at the time the query is processed. In order to predict the relevance of collections to a given query, we analyse a limited number of full documents (e.g., the top five documents) retrieved from each collection and then consider term proximity within them. On the other hand, our merging technique is rather simple since input only requires document scores and lengths of results lists. Our experiments evaluate the retrieval effectiveness of these approaches and compare them with centralised indexing and various other DIR techniques (e.g., CORI). We conducted our experiments using two testbeds: one containing news articles extracted from four different sources (2 GB) and another containing 10 GB of Web pages. Our evaluations demonstrate that the retrieval effectiveness of our simple approaches is worth considering.

#index 342681
#* Exploiting a controlled vocabulary to improve collection selection and retrieval effectiveness
#@ James C. French;Allison L. Powell;Fredric Gey;Natalia Perelman
#t 2001
#c 1
#% 54435
#% 169777
#% 194246
#% 204668
#% 232719
#% 262063
#% 262065
#% 267454
#% 280853
#% 282422
#% 282424
#% 287237
#% 287463
#% 301225
#% 309133
#% 424308
#% 479642
#% 481748
#% 561160
#% 567255
#% 740900
#% 815097
#! Vocabulary incompatibilities arise when the terms used to index a document collection are largely unknown, or at least not well-known to the users who eventually search the collection. No matter how comprehensive or well-structured the indexing vocabulary, it is of little use if it is not used effectively in query formulation. This paper demonstrates that techniques for mapping user queries into the controlled indexing vocabulary have the potential to radically improve document retrieval performance. We also show how the use of controlled indexing vocabulary can be employed to achieve performance gains for collection selection. Finally, we demonstrate the potential benefit of combining these two techniques in an interactive retrieval environment. Given a user query, our evaluation approach simulates the human user's choice of terms for query augmentation given a list of controlled vocabulary terms suggested by a system. This strategy lets us evaluate interactive strategies without the need for human subjects.

#index 342682
#* Predicting the cost-quality trade-off for information retrieval queries: facilitating database design and query optimization
#@ Henk Ernst Blok;Djoerd Hiemstra;Sunil Choenni;Franciska de Jong;Henk M. Blanken;Peter M.G. Apers
#t 2001
#c 1
#% 36683
#% 46803
#% 214198
#% 228097
#% 248794
#% 262096
#% 385946
#% 479623
#% 479795
#% 479816
#% 479967
#% 489542
#% 1783137
#! Efficient, flexible, and scalable integration of full text information retrieval (IR) in a DBMS is not a trivial case. This holds in particular for query optimization in such a context. To facilitate the bulk-oriented behavior of database query processing, a priori knowledge of how to limit the data efficiently prior to query evaluation is very valuable at optimization time. The usually imprecise nature of IR querying provides an extra opportunity to limit the data by a trade-off with the quality of the answer. In this paper we present a mathematically derived model to predict the quality implications of neglecting information before query execution. In particular we investigate the possibility to predict the retrieval quality for a document collection for which no training information is available, which is usually the case in practice. Instead, we construct a model that can be trained on other document collections for which the necessary quality information is available, or can be obtained quite easily. We validate our model for several document collections and present the experimental results. These results show that our model performs quite well, even for the case were we did not train it on the test collection itself.

#index 342683
#* How foreign function integration conquers heterogeneous query processing
#@ Klaudia Hergula;Theo Härder
#t 2001
#c 1
#% 85086
#% 169052
#% 273912
#% 284600
#% 411554
#% 462001
#% 463919
#% 479449
#% 481101
#% 481923
#% 571060
#% 632002
#% 665529
#! With the emergence of application systems which encapsulate databases and related application components, pure data integration using, for example, a federated database system is not possible anymore. Instead, access via predefined functions is the only way to get data from an application system. As a result, retrieval of such heterogeneous and encapsulated data sources needs the combination of generic query as well as predefined function access. In this paper, we present a middleware approach supporting such a novel and extended kind of integration. Starting with the overall architecture, we explain the functionality and cooperation of its core components: a federated database system and a workflow management system connected via a wrapper. Afterwards, we concentrate on essential aspects of query processing across these heterogeneous components focusing on the impact of the functions included. We discuss the operations the wrapper should provide in order to extend the workflow system's native functionality. In addition to selection and projection, these operations could include aggregation and the support of subqueries. Moreover, we point out modifications to the traditional cost model needed to consider the cost estimates for the function calls as well.

#index 342684
#* Joint optimization of cost and coverage of query plans in data integration
#@ Zaiqing Nie;Subbarao Kambhampati
#t 2001
#c 1
#% 264263
#% 273912
#% 342869
#% 411554
#% 479813
#% 480149
#% 481923
#% 482108
#% 496091
#% 571037
#% 1499470
#! Existing approaches for optimizing queries in data integration use decoupled strategies--attempting to optimize coverage and cost in two separate phases. Since sources tend to have a variety of access limitations, such phased optimization of cost and coverage can unfortunately lead to expensive planning as well as highly inefficient plans. In this paper we present techniques for joint optimization of cost and coverage of the query plans. Our algorithms search in the space of parallel query plans that support multiple sources for each subgoal conjunct. The refinement of the partial plans takes into account the potential parallelism between source calls, and the binding compatibilities between the sources included in the plan. We start by introducing and motivating our query plan representation. We then briefly review how to compute the cost and coverage of a parallel plan. Next, we provide both a System-R style query optimization algorithm as well as a greedy local search algorithm for searching in the space of such query plans. Finally we present a simulation study that demonstrates that the plans generated by our approach will be significantly better, both in terms of planning cost, and in terms of plan execution cost, compared to the existing approaches.

#index 342685
#* A music recommendation system based on music data grouping and user interests
#@ Hung-Chen Chen;Arbee L. P. Chen
#t 2001
#c 1
#% 36672
#% 124010
#% 202011
#% 220709
#% 220710
#% 220711
#% 261882
#% 266281
#% 284794
#% 301043
#% 308762
#% 664044
#! With the growth of the World Wide Web, a large amount of music data is available on the Internet. In addition to searching expected music objects for users, it becomes necessary to develop a recommendation service. In this paper, we design the Music Recommendation System (MRS) to provide a personalized service of music recommendation. The music objects of MIDI format are first analyzed. For each polyphonic music object, the representative track is first determined, and then six features are extracted from this track. According to the features, the music objects are properly grouped. For users, the access histories are analyzed to derive user interests. The content-based, collaborative and statistics-based recommendation methods are proposed, which are based on the favorite degrees of the users to the music groups. A series of experiments are carried out to show that our approach is feasible.

#index 342686
#* Selecting relevant instances for efficient and accurate collaborative filtering
#@ Kai Yu;Xiaowei Xu;Martin Ester;Hans-Peter Kriegel
#t 2001
#c 1
#% 92533
#% 126949
#% 173879
#% 202011
#% 229972
#% 243727
#% 280852
#% 307100
#% 314933
#% 381704
#% 428413
#% 465928
#% 655897
#% 1650569
#! Collaborative filtering uses a database about consumers' preferences to make personal product recommendations and is achieving widespread success in both E-Commerce and Information Filtering Applications nowadays. However, the traditional collaborative filtering algorithms do not scale well to the ever-growing number of consumers. The quality of the recommendation also needs to be improved in order to gain more trust from the consumers. In this paper, we present a novel method to improve the scalability and the accuracy of the collaborative filtering algorithm. We introduce an information theoretic approach to measure the relevance of a consumer (instance) for predicting the preference for the given product (target concept). The proposed method reduces the training data set by selecting only highly relevant instances. Our experimental evaluation on the well-known EachMovie data set shows that our method doesn't only significantly speed up the prediction, but also results in a better accuracy.

#index 342687
#* Evaluation of Item-Based Top-N Recommendation Algorithms
#@ George Karypis
#t 2001
#c 1
#% 67565
#% 173879
#% 202009
#% 202011
#% 220706
#% 220707
#% 220709
#% 220711
#% 266281
#% 280447
#% 283169
#% 301590
#% 310567
#% 310572
#% 314933
#% 465928
#% 1650569
#! The explosive growth of the world-wide-web and the emergence of e-commerce has led to the development of recommender systems---a personalized information filtering technology used to identify a set of N items that will be of interest to a certain user. User-based Collaborative filtering is the most successful technology for building recommender systems to date, and is extensively used in many commercial recommender systems. Unfortunately, the computational complexity of these methods grows linearly with the number of customers that in typical commercial applications can grow to be several millions. To address these scalability concerns item-based recommendation techniques have been developed that analyze the user-item matrix to identify relations between the different items, and use these relations to compute the list of recommendations.In this paper we present one such class of item-based recommendation algorithms that first determine the similarities between the various items and then used them to identify the set of items to be recommended. The key steps in this class of algorithms are (i) the method used to compute the similarity between the items, and (ii) the method used to combine these similarities in order to compute the similarity between a basket of items and a candidate recommender item. Our experimental evaluation on five different datasets show that the proposed item-based algorithms are up to 28 times faster than the traditional user-neighborhood based recommender systems and provide recommendations whose quality is up to 27% better.

#index 342688
#* Prefix-querying: an approach for effective subsequence matching under time warping in sequence databases
#@ Sanghyun Park;Sang-Wook Kim;June-Suh Cho;Sriram Padmanabhan
#t 2001
#c 1
#% 2115
#% 86950
#% 137711
#% 153260
#% 172949
#% 174226
#% 227857
#% 232122
#% 273704
#% 427199
#% 443082
#% 460862
#% 462070
#% 462231
#% 477479
#% 480093
#% 481609
#% 481956
#% 534183
#% 564263
#% 632088
#! This paper discusses an index-based subsequence matching that supports time warping in large sequence databases. Time warping enables finding sequences with similar patterns even when they are of different lengths. In our earlier work, we suggested an efficient method for whole matching under time warping. This method constructs a multi-dimensional index on a set of feature vectors, which are invariant to time warping, from data sequences. For filtering at feature space, it also applies a lower-bound function, which consistently underestimates the time warping distance as well as satisfies the triangular inequality.In this paper, we incorporate the prefix-querying approach based on sliding windows into the earlier approach. For indexing, we extract a feature vector from every subsequence inside a sliding window and construct a multi-dimensional index using a feature vector as indexing attributes. For query processing, we perform a series of index searches using the feature vectors of qualifying query prefixes. Our approach provides effective and scalable subsequence matching even with a large volume of a database. We also prove that our approach does not incur false dismissal. To verify the superiority of our method, we perform extensive experiments. The results reveal that our method achieves significant speedup with real-world S&P 500 stock data and with very large synthetic data.

#index 342689
#* Sliding-window filtering: an efficient algorithm for incremental mining
#@ Chang-Hung Lee;Cheng-Ru Lin;Ming-Syan Chen
#t 2001
#c 1
#% 152934
#% 227917
#% 273899
#% 280467
#% 310558
#% 310559
#% 320944
#% 329598
#% 338580
#% 438134
#% 443082
#% 443164
#% 443194
#% 462219
#% 464204
#% 464989
#% 480154
#% 481290
#% 481754
#% 481758
#% 481779
#% 511333
#! We explore in this paper an effective sliding-window filtering (abbreviatedly as SWF) algorithm for incremental mining of association rules. In essence, by partitioning a transaction database into several partitions, algorithm SWF employs a filtering threshold in each partition to deal with the candidate itemset generation. Under SWF, the cumulative information of mining previous partitions is selectively carried over toward the generation of candidate itemsets for the subsequent partitions. Algorithm SWF not only significantly reduces I/O and CPU cost by the concepts of cumulative filtering and scan reduction techniques but also effectively controls memory utilization by the technique of sliding-window partition. Algorithm SWF is particularly powerful for efficient incremental mining for an ongoing time-variant transaction database. By utilizing proper scan reduction techniques, only one scan of the incremented dataset is needed by algorithm SWF. The I/O cost of SWF is, in orders of magnitude, smaller than those required by prior methods, thus resolving the performance bottleneck. Experimental studies are performed to evaluate performance of algorithm SWF. It is noted that the improvement achieved by algorithm SWF is even more prominent as the incremented portion of the dataset increases and also as the size of the database increases.

#index 342690
#* Efficient and robust feature extraction and pattern matching of time series by a lattice structure
#@ Polly Wan Po Man;Man Hon Wong
#t 2001
#c 1
#% 172949
#% 227924
#% 260014
#% 264633
#% 273704
#% 280846
#% 460862
#% 462231
#% 479649
#% 617843
#% 631923
#! The efficiency of searching scaling-invariant and shifting-invariant shapes in a set of massive time series data can be improved if searching is performed on an approximated sequence which involves less data but contains all the significant features. However, commonly used smoothing techniques, such as moving averages and best-fitting polylines, usually miss important peaks and troughs and deform the time series. In addition, these techniques are not robust, as they often requires users to supply a set of smoothing parameters which has direct effect on the resultant approximation pattern. To address these problems, an algorithm to construct a lattice structure as an underlying framework for pattern matching is proposed in this paper. As inputs, the algorithm takes a time series and users' requirements of level of detail. The algorithm then identifies all the important peaks and troughs (known as controlm points) in the time series and classifies the points into appropriate layers of the lattice structure. The control points in each layer of the structure form an approximation pattern an yet preserve the overall shape of the original series with approximation error lies within certain bound. The lower the layer, the more precise the approximation pattern is. Putting in another way, the algorithm takes different levels of data smoothing into account. Also, the lattice structure can be indexed to further improve the performance of pattern matching.

#index 342691
#* Mining the web to create minority language corpora
#@ Rayid Ghani;Rosie Jones;Dunja Mladenić
#t 2001
#c 1
#% 144007
#% 252472
#% 273926
#% 278102
#% 304423
#% 316522
#% 465754
#% 466250
#% 466266
#% 480309
#% 615723
#% 665552
#% 740915
#% 786575
#! The Web is a valuable source of language specific resources but the process of collecting, organizing and utilizing these resources is difficult. We describe CorpusBuilder, an approach for automatically generating Web-search queries for collecting documents in a minority language. It differs from pseudo-relevance feedback in that retrieved documents are labeled by an automatic language classifier as relevant or irrelevant, and this feedback is used to generate new queries. We experiment with various query-generation methods and query-lengths to find inclusion/exclusion terms that are helpful for retrieving documents in the target language and find that using odds-ratio scores calculated over the documents acquired so far was one of the most consistently accurate query-generation methods. We also describe experiments using a handful of words elicited from a user instead of initial documents and show that the methods perform similarly. Experiments applying the same approach to multiple languages are also presented showing that our approach generalizes to a variety of languages.

#index 342692
#* Automatic recognition of distinguishing negative indirect history language in judicial opinions
#@ Jack G. Conrad;Daniel P. Dabney
#t 2001
#c 1
#% 99895
#% 124010
#% 180137
#% 200397
#% 235446
#% 235463
#% 259992
#% 290607
#% 340544
#% 742410
#% 746867
#% 1478820
#% 1478939
#! We describe a model-based filtering application that generates candidate case-to-case distinguishing citations. We developed the system to aid editors in identifying indirect relationships among judicial opinions in a database of over 5 million documents. Using a training collection of approximately 30,000 previously edited cases, the filter application provides ranked sets of textual evidence for current case law documents in the editorial process. These sets contain judicial language with a strong probability of containing distinguishing relationships. Integrating this application into the editorial review environment has greatly improved the coverage and efficiency of the work flow to identify and generate new distinguishing relationship entries.

#index 342693
#* Effective arabic-english cross-language information retrieval via machine-readable dictionaries and machine translation
#@ Mohammed Aljlayl;Ophir Frieder
#t 2001
#c 1
#% 68997
#% 177564
#% 218978
#% 218988
#% 218989
#% 232656
#% 262046
#% 262047
#% 270945
#% 280858
#% 562054
#! In Cross-Language Information Retrieval (CLIR), queries in one language retrieve relevant documents in other languages Machine-Readable Dictionary (MRD) and Machine Translation (MT) are important resources for query translation in CLIR. We investigate MT and MRD to Arabic-English CLIR. The translation ambiguity associated with these resources is the key problem. We present three methods of query translation using a bilingual dictionary for Arabic-English CLIR. First, we present the Every-Match (EM) method. This method yields ambiguous translations since many extraneous terms are added to the original query. To disambiguate the query translation, we present the First-Match (FM) method that considers the first match in the dictionary as the candidate term. Finally, we present the Two-Phase (TP) method. We show that good retrieval effectiveness can be achieved without complex resources using the Two-Phase method for Arabic-English CLIR. We also empirically evaluate the effectiveness of the MT-based method using short, medium, and long queries from TREC. The effects of the query length on the quality of the MT-based CLIR are investigated.

#index 342694
#* A near optimal algorithm for generating broadcast programs on multiple channels
#@ Chih-Hao Hsu;Guanling Lee;Arbee L. P. Chen
#t 2001
#c 1
#% 32884
#% 140613
#% 152954
#% 172913
#% 201897
#% 268151
#% 274199
#% 274200
#% 316486
#% 384050
#% 464065
#% 481777
#% 632025
#% 632067
#! In a wireless environment, the bandwidth of the channels and the energy of the portable devices are limited. Data broadcast has become an excellent method for efficient data dissemination. In this paper, the problem for generating a broadcast program of a set of data items with the associated access frequencies on multiple channels is explored. In our approach, an expected average access time of the broadcast data items is first derived. The broadcast program is then generated, which minimizes the expected average access time. Simulation is performed to compare the performance of our approach with two existing approaches. The result of the experiments shows that our approach outperforms others and is in fact close to the optimal.

#index 342695
#* Managing trust in a peer-2-peer information system
#@ Karl Aberer;Zoran Despotovic
#t 2001
#c 1
#% 340175
#% 340176
#% 438351
#% 543383
#% 607998
#! Managing trust is a problem of particular importance in peer-to-peer environments where one frequently encounters unknown agents. Existing methods for trust management, that are based on reputation, focus on the semantic properties of the trust model. They do not scale as they either rely on a central database or require to maintain global knowledge at each agent to provide data on earlier interactions. In this paper we present an approach that addresses the problem of reputation-based trust management at both the data management and the semantic level. We employ at both levels scalable data structures and algorithms that require no central control and allow to assess trust by computing an agents reputation from its former interactions with other agents. Thus the meethod can be implemented in a peer-to-peer environment and scales well for very large numbers of participants. We expect that scalable methods for trust management are an important factor, if fully decentralized peer-to-peer systems should become the platform for more serious applications than simple file exchange.

#index 342696
#* Alternative representations and abstractions for moving sensors databases
#@ J. Eisenstein;S. Ghandeharizadeh;C. Shahabi;G. Shanbhag;R. Zimmermann
#t 2001
#c 1
#% 211820
#% 241027
#% 274143
#% 300173
#% 315005
#% 336746
#% 421073
#% 461923
#% 527792
#% 589555
#% 641036
#! Moving sensors refers to an emerging class of data intensive applications that inpacts disciplines such as communication, health-care, scientific applications, etc. These applications consist of a fixed number of sensors that move and produce streams of data as a function of time. They may require the system to match these streams against stored streams to retrieve relevant data (patterns). With communication, for example, a speaking impaired individual might utilize a haptic glove that translates hand signs into written (spoken) words. The glove consists of sensors for different finger joints. These sensors report their location and values as a function of time, producing streams of data. These streams are matched against a repository of spatio-temporal streams to retrieve the corresponding English character or word.The contributions of this study are two fold. First, it introduces a framework to store and retrieve "moving sensors" data. The framework advocates physical data independence and software-reuse. Second, we investigate alternative representations for storage and retrieve of data in support of query processing. We quantify the tradeoff associated with these alternatives using empirical data RoboCup soccer matches.

#index 342697
#* Termination analysis of active rules modular sets
#@ Alain Couchot
#t 2001
#c 1
#% 52373
#% 83315
#% 116045
#% 205241
#% 248039
#% 459018
#% 461891
#% 479811
#% 481456
#% 501948
#% 501949
#% 562328
#! This paper presents an algorithm for static termination analysis of active rules in a context of modular design. Several recent works have suggested proving termination by using the concept of triggering graph. We propose here an original approach, based on these works, and that allows to guarantee the termination of a set of rules, conceived by several designers, even when none of the designers knows the set of the active rules. We introduce the notions of private event and of public event, and we refine the notion of triggering graph (by enclosing also events in graphs). We replace then the notion of cycle (which is no more relevant in a context of modular design) by the notion of maximal private path preceding a rule. By means of these tools, we show that it is possible to prove termination of active rules modular sets.

#index 342698
#* Index filtering and view materialization in ROLAP environment
#@ Shi Guang Qiu;Tok Wang Ling
#t 2001
#c 1
#% 210182
#% 227861
#% 227868
#% 248805
#% 462204
#% 479630
#! Using materialized view to accelerate OLAP queries is one of the most common methods used in ROLAP systems. However, high storage and computation cost make this method very difficult to be implemented in the actual environment. Among various issues associated with this, index selection and view materialization are two of the top challenges. In this paper, we propose to build indexes on subsets of the primary keys rather than the full sets if the index selectivity for these smaller indexes can be maintained above the required level. Based on that we propose an index filtering rule, Dominant Prime (DPrime) Index Set Filter, to filter out candidate indexes that have insufficient index selectivity or have cheaper alternatives. In the second part, we propose a view materialization method, Nested Relation Approach, to group tuples with the same value for index attributes into one super tuple using a nested relation and implement this method using Oracle VARRAY. In performance tests, our method outperforms others significantly.

#index 342699
#* Dynamic and hierarchical spatial access method using integer searching
#@ Kyoosang Cho;Yijie Han;Yugyung Lee;E. K. Park
#t 2001
#c 1
#% 2692
#% 86950
#% 201880
#% 252304
#% 278609
#% 285932
#% 287193
#% 300096
#% 321455
#% 415957
#% 427199
#% 442768
#% 462041
#% 462617
#% 462781
#% 464195
#% 480093
#% 481455
#% 526849
#% 593693
#% 656723
#! Dynamic and complex computation in the area of Geographic Information System (GIS) or Mobile Computing System involves huge amount of spatial objects such as points, boxes, polygons, etc and requires a scalable data structure and an efficient management tool for this information. In this paper, for a dynamic management of spatial objects, we construct a hierarchical dynamic data structure, called an IST/OPG hierarchy, which may overcome some limitations of existing Spatial Access Methods (SAMs). The hierarchy is constructed by combining three primary components: (1) Minimum Boundary Rectangle (MBR), which is the most widely used method among SAMs; (2) the population-based domain slicing, which is modified from the Grid File [14]; (3) extended optimal Integer Searching algorithm [4]. For dynamic management of spatial objects in the IST/OPG hierarchy, a number of primary and supplementary operations are introduced. This paper includes a comparative analysis of our approach with previous SAMs, such as R-Tree, R+-Tree and R*-Tree and QSF-Tree. The results of analysis show that our approach is better than other SAMs in construction and query time and space requirements. Specifically, for a given search domain with n objects, our query operations yield $O($ \scriptsize $\sqrt {\frac {\log n} {\log\log n}}$\normalsize $)$ compared to $O(\log n)$ of the fast SAM and an IST/OPG hierarchy containing $n$ objects can be constructed in $O(n$ \scriptsize $\sqrt {\frac {\log n}{\log\log n}}$\normalsize $)$ time and O(n) space.

#index 342700
#* Efficient incremental view maintenance in data warehouses
#@ Ki Yong Lee;Jin Hyun Son;Myoung Ho Kim
#t 2001
#c 1
#% 13016
#% 152928
#% 199537
#% 201928
#% 201929
#% 207552
#% 210210
#% 227869
#% 227947
#% 273918
#% 300141
#% 333962
#% 340300
#% 411562
#% 442767
#% 480623
#% 669819
#! In the data warehouse environment, the concept of a materialized view is nowadays common and important in an objective of efficiently supporting OLAP query processing. Materialized views are generally derived from select-project-join of several base relations. These materialized views need to be updated when the base relations change. Since the propagation of updates to the views may impose a significant overhead, it is very important to update the warehouse views efficiently. Though various view maintenance strategies have been discussed so far, they typically require too much access to base relations, resulting in the performance degradation.In this paper we propose an efficient incremental view maintenance strategy called delta propagation that can minimize the total size of base relations accessed by analyzing the properties of base relations. We first define the delta expression and a delta propagation tree which are core concepts of the strategy. Then, a dynamic programming algorithm that can find the optimal delta expression are proposed. We also present various experimental results that show the usefulness and efficiency of the strategy.

#index 342701
#* Improved string matching under noisy channel conditions
#@ Kevyn Collins-Thompson;Charles Schweizer;Susan Dumais
#t 2001
#c 1
#% 120649
#% 251405
#% 375017
#% 420481
#% 546439
#% 817577
#! Many document-based applications, including popular Web browsers, email viewers, and word processors, have a 'Find on this Page' feature that allows a user to find every occurrence of a given string in the document. If the document text being searched is derived from a noisy process such as optical character recognition (OCR), the effectiveness of typical string matching can be greatly reduced. This paper describes an enhanced string-matching algorithm for degraded text that improves recall, while keeping precision at acceptable levels. The algorithm is more general than most approximate matching algorithms and allows string-to-string edits with arbitrary costs. We develop a method for evaluating our technique and use it to examine the relative effectiveness of each sub-component of the algorithm. Of the components we varied, we find that using confidence information from the recognition process lead to the largest improvements in matching accuracy.

#index 342702
#* Summarization as feature selection for text categorization
#@ Aleksander Kolcz;Vidya Prabakarmurthi;Jugal Kalita
#t 2001
#c 1
#% 99690
#% 165110
#% 190581
#% 198294
#% 260001
#% 280419
#% 280817
#% 280836
#% 327474
#% 420525
#% 445372
#% 458369
#% 458379
#% 465754
#% 853817
#! We address the problem of evaluating the effectiveness of summarization techniques for the task of document categorization. It is argued that for a large class of automatic categorization algorithms, extraction-based document categorization can be viewed as a particular form of feature selection performed on the full text of the document and, in this context, its impact can be compared with state-of-the-art feature selection techniques especially devised to provide good categorization performance. Such a framework provides for a better assessment of the expected performance of a categorizer if the compression rate of the summarizer is known.

#index 342703
#* Bootstrapping for example-based data extraction
#@ Paulo B. Golgher;Altigran S. da Silva;Alberto H. F. Laender;Berthier Ribeiro-Neto
#t 2001
#c 1
#% 275915
#% 283050
#% 283180
#% 287202
#% 300288
#% 312860
#% 660272
#! The effortless generation of wrappers for Web data sources is a crucial task if proper access to the huge amount of semi-structured data on the Web is to be granted. In particular, the development of strategies for wrapper generation based on user-given examples is currently one of the most promising research directions in Web data extraction. In this paper we show how to use a pre-existing data repository to automatically generate examples and allow full automated example-based data extraction. To demonstrate the feasibility of our approach we provide a number of results obtained from experiments we carried out and discuss how our ideas can be used to improve extraction rates and for providing resilience and adaptiveness for example-based generated wrappers.

#index 342704
#* SQL database primitives for decision tree classifiers
#@ Kai-Uwe Sattler;Oliver Dunemann
#t 2001
#c 1
#% 136350
#% 248813
#% 249985
#% 252304
#% 273900
#% 300213
#% 310555
#% 316709
#% 449588
#% 459008
#% 479787
#% 479972
#% 480144
#% 480291
#% 481101
#% 481945
#% 481954
#% 495275
#% 631966
#! Scalable data mining in large databases is one of today's challenges to database technologies. Thus, substantial effort is dedicated to a tight coupling of database and data mining systems leading to database primitives supporting data mining tasks. In order to support a wide range of tasks and to be of general usage these primitives should be rather building blocks than implementations of specific algorithms. In this paper, we describe primitives for building and applying decision tree classifiers. Based on the analysis of available algorithms and previous work in this area we have identified operations which are useful for a number of classification algorithms. We discuss the implementation of these primitives on top of a commercial DBMS and present experimental results demonstrating the performance benefit.

#index 342705
#* Learning probabilistic datalog rules for information classification and transformation
#@ Henrik Nottelmann;Norbert Fuhr
#t 2001
#c 1
#% 697
#% 36683
#% 46803
#% 162223
#% 176471
#% 280038
#% 292510
#% 318412
#% 396021
#! Probabilistic Datalog is a combination of classical Datalog (i.e., function-free Horn clause predicate logic) with probability theory. Therefore, probabilistic weights may be attached to both facts and rules. But it is often impossible to assign exact rule weights or even to construct the rules themselves. Instead of specifying them manually, learning algorithms can be used to learn both rules and weights. In practice, these algorithms are very slow because they need a large example set and have to test a high number of rules. We apply a number of extensions to these algorithms in order to improve efficiency. Several applications demonstrate the power of learning probabilistic Datalog rules, showing that learning rules is suitable for low dimensional problems (e.g., schema mapping) but inappropriate for higher dimensions like e.g. in text classification.

#index 342706
#* SVM binary classifier ensembles for image classification
#@ King-Shy Goh;Edward Chang;Kwang-Ting Cheng
#t 2001
#c 1
#% 190581
#% 209021
#% 266255
#% 341269
#% 420077
#% 458352
#% 465746
#% 466263
#% 466652
#% 589970
#% 722756
#! We study how the SVM-based binary classifiers can be effectively combined to tackle the multi-class image classification problem. We study several ensemble schemes, including OPC (one per class), PWC (pairwise coupling), and ECOC (error-correction output coding), that aim to achieve good error correction capability through redundancy. To enhance these ensemble schemes' accuracy, we propose methods that on the one hand boost the margins (i.e., confidence) of the SVM-based binary classifiers, and, on the other hand, remove the noise of irrelevant classifiers from class prediction. From empirical study we show that our margin boosting and noise reduction methods lead to higher classification accuracy than ensemble schemes that are solely designed for maximum error correction capability.

#index 342707
#* Model-based feedback in the language modeling approach to information retrieval
#@ Chengxiang Zhai;John Lafferty
#t 2001
#c 1
#% 73045
#% 115608
#% 262096
#% 280850
#% 280851
#% 280856
#% 280864
#% 300542
#% 340899
#% 340901
#% 340948
#! The language modeling approach to retrieval has been shown to perform well empirically. One advantage of this new approach is its statistical foundations. However, feedback, as one important component in a retrieval system, has only been dealt with heuristically in this new retrieval approach: the original query is usually literally expanded by adding additional terms to it. Such expansion-based feedback creates an inconsistent interpretation of the original and the expanded query. In this paper, we present a more principled approach to feedback in the language modeling approach. Specifically, we treat feedback as updating the query language model based on the extra evidence carried by the feedback documents. Such a model-based feedback strategy easily fits into an extension of the language modeling approach. We propose and evaluate two different approaches to updating a query language model based on feedback documents, one based on a generative probabilistic model of feedback documents and one based on minimization of the KL-divergence over feedback documents. Experiment results show that both approaches are effective and outperform the Rocchio feedback approach.

#index 342708
#* PowerDB-IR: information retrieval on top of a database cluster
#@ Torsten Grabs;Klemens Böhm;Hans-Jörg Schek
#t 2001
#c 1
#% 43171
#% 172922
#% 224702
#% 236110
#% 239969
#% 274483
#% 464071
#% 464855
#% 479204
#% 481439
#% 482052
#% 571098
#! Our current concern is a scalable infrastructure for information retrieval (IR) with up-to-date retrieval results in the presence of frequent, continuous updates. Timely processing of updates is important with novel application domains, e.g., e-commerce. We want to use off-the-self hardware and software as much as possible. These issues are challenging, given the additional requirement that the resulting system must scale well. We have built PowerDB-IR, a system that has the characteristics sought. This paper describes its design, implementation, and evaluation. PowerDB-IR is a coordination layer for a database cluster. The rationale behind a database cluster is to 'scale-out', i.e., to add further cluster nodes, whenever necessary for better performance. We build on IR-to-database mappings and service decomposition to support high-level parallelism. We follow a three-tier architecture with the database cluster as the bottom layer for storage management. The middle tier provides IR-specific processing and update services. PowerDB-IR has the following features: It allows to insert and retrieve documents concurrently, and it ensures freshness with almost no overhead. Alternative physical data organization schemes provide adequate performance for different workloads. Query processing techniques for the different data organizations efficiently integrate the ranked retrieval results from the cluster nodes. We have run extensive experiments with our prototype using commercial database systems and middleware software products. The main result is that PowerDB-IR shows surprisingly ideal scalability and low response times.

#index 342709
#* Automatic query expansion based on divergence
#@ D. Cai;C. J. van Rijsbergen;J. M. Jose
#t 2001
#c 1
#% 92696
#% 194301
#! In this paper we are mainly concerned with discussion of a formal model, based on the basic concept of divergence from information theory, for automatic query expansion. The basic principles and ideas on which our study is based are described. A theoretical framework is established, which allows the comparison and evaluation of different term scoring functions for identifying good terms for query expansion. The approaches proposed in this paper have been implemented and evaluated on collections from TREC. Preliminary results show that our approaches are viable and worthy of continued investigation.

#index 342710
#* Relevance score normalization for metasearch
#@ Mark Montague;Javed A. Aslam
#t 2001
#c 1
#% 71772
#% 174664
#% 219050
#% 232703
#% 273033
#% 340934
#% 340936
#% 340959
#% 420464
#% 708847
#% 709230
#! Given the ranked lists of documents returned by multiple search engines in response to a given query, the problem of metasearch is to combine these lists in a way which optimizes the performance of the combination. This problem can be naturally decomposed into three subproblems: (1) normalizing the relevance scores given by the input systems, (2) estimating relevance scores for unretrieved documents, and (3) combining the newly-acquired scores for each document into one, improved score.Research on the problem of metasearch has historically concentrated on algorithms for combining (normalized) scores. In this paper, we show that the techniques used for normalizing relevance scores and estimating the relevance scores of unretrieved documents can have a significant effect on the overall performance of metasearch. We propose two new normalization/estimation techniques and demonstrate empirically that the performance of well known metasearch algorithms can be significantly improved through their use.

#index 342711
#* Binary interpolation search for solution mapping on broadcast and on-demand channels in a mobile computing environment
#@ Jiun-Long Huang;Wen-Chih Peng;Ming-Syan Chen
#t 2001
#c 1
#% 4185
#% 172913
#% 201897
#% 247246
#% 259632
#% 259634
#% 274209
#% 287068
#% 287258
#% 305097
#% 316486
#% 316491
#% 341704
#% 443127
#% 443263
#% 465622
#% 482107
#% 536178
#% 554885
#% 610617
#% 632025
#% 632067
#% 635906
#! We explore in this paper the problem of dynamic data and channel allocations with the number of communication channels and the number of data items given. It is noted that the combined use of broadcast and on-demand channels can utilize the bandwidth effectively for data dissemination in a mobile computing environment. We first derive the an-alytical models of the expected delays when the data are requested through the broadcast and on-demand channels. Then, we transform this problem into to a guided search problem. In light of the theoretical properties derived, we devise an algorithm based on binary interpolation search, referred to as algorithm BIS, to obtain solutions of high quality efficiently. In essence, algorithm BIS is guided to explore the solution space with higher likelihood to be the optimal first, thereby leading to an efficient and effective search. It is shown by our simulation results that the solution obtained by algorithm BIS is of very high quality and is in fact very close to the optimal one. Sensitivity analysis on several parameters, including the number of data items and the number of communication channels, is conducted.

#index 342712
#* Caching constrained mobile data
#@ Subhasish Mazumdar;Mateusz Pietrzyk;Panos Chrysanthis
#t 2001
#c 1
#% 877
#% 43173
#% 77005
#% 210179
#% 235017
#% 268793
#% 381812
#% 384050
#% 458535
#% 480935
#% 511000
#% 588651
#! As mobile devices get ubiquitous and grow in computational power, their management of interdependent data also becomes increasingly important. The mobile environment exhibits all the characteristics of a distributed database plus the feature of whimsical connectivity. Consequently, transactions respecting data consistency can suffer unbounded and unpredictable delays at both mobile and stationary nodes. The currently popular multi-tier model, in which mobile devices are in one end and always-connected stationary servers in the other, has certain practical advantages. However, it assumes that all integrity constraints are evaluated at the servers and hence relies on the semantics of operations for any autonomy enhancement of the mobile devices. In this paper, we examine the idea of constraint localization in cases where two mobile nodes each own data that share a constraint. It relies on reformulation of a constraint into more flexible local constraints that give more autonomy to the mobile nodes. The scheme also involves dynamic changes of these local constraints through negotiation, which we call re-localization. To overcome the problem of simultaneous requests for such re-localization, we give algorithms along with experimental results indicating their effectiveness.

#index 342713
#* Scaling replica maintenance in intermittently synchronized mobile databases
#@ Wai Gen Yee;Michael J. Donahoo;Edward Omiecinski;Shamkant B. Navathe
#t 2001
#c 1
#% 872
#% 210179
#% 210182
#% 237197
#% 264263
#% 281469
#% 284657
#% 316493
#% 462227
#! To avoid the high cost of continuous connectivity, a class of mobile applications employs replicas of shared data that are periodically updated. Updates to these replicas are typically performed on a client-by-client basis--that is, the server individually computes and transmits updates to each client--limiting scalability. By basing updates on replica groups (instead of clients), however, update generation complexity is no longer bound by client population size. Clients then download updates of pertinent groups. Proper group design reduces redundancies in server processing, disk usage and bandwidth usage, and dimininishes the tie between the complexity of updating replicas and the size of the client population. In this paper, we expand on previous work done on group design, include a detailed I/O cost model for update generation, and propose a heuristic-based greedy algorithm for group computation. Experimental results with an adapted commercial replication system demonstrate a significant increase in overall scalability over the client-centric approach.

#index 342714
#* An optimal construction of invalidation reports for mobile databases
#@ Wen-Chi Hou;Meng Su;Hongyan Zhang;Hong Wang
#t 2001
#c 1
#% 65498
#% 83127
#% 102802
#% 102803
#% 172874
#% 172876
#% 186522
#% 198045
#% 245014
#% 341169
#% 443263
#% 461919
#% 464214
#% 480927
#! Mobile computing is characterized by frequent disconnection, limited communication capability, narrow bandwidth, etc. Caching can play a vital role in mobile computing by reducing the amount of data transferred. In order to reuse caches after short disconnections, invalidation reports are broadcasted to clients to help update/invalidate their caches. Detailed reports may not be desirable because they can be very long and consume large bandwidth. On the other hand, false invalidations may set in if detailed timing information of updates is not provided in the report. In this research, we aim to reduce the false invalidation rates of the reports. It is found that false invalidation rates are closely related to clients' reconnection patterns (i.e., the distribution of the time spans between disconnections and reconnections). By using Newton's method, we show how a report with a minimal false invalidation rate can be constructed for any given disconnection pattern.

#index 342715
#* Efficient runtime generation of association rules
#@ Richard Relue;Xindong Wu;Hao Huang
#t 2001
#c 1
#% 172386
#% 201894
#% 227919
#% 248785
#% 248791
#% 248813
#% 280409
#% 300120
#% 329598
#% 420063
#% 443082
#% 443427
#% 461909
#% 463903
#% 479484
#% 481290
#% 481754
#% 631926
#! Mining frequent patterns in transaction databases has been a popular subject in data mining research. Common activities include finding patterns in database transactions, times-series, and exceptions. The Apriori algorithm is a widely accepted method of generating frequent patterns. The algorithm can require many scans of the database and can seriously tax resources. New methods of finding association rules, such as the Frequent Pattern Tree (FP-Tree) have improved performance, but still have problems when new data becomes available and require two scans of the database.This paper proposes a new method, which requires only one scan of the database and supports update of patterns when new data becomes available. We design a new structure called Pattern Repository (PR), which stores all of the relevant information in a highly compact form and allows direct derivation of the FP-Tree and association rules quickly with a minimum of resources. In addition, it supports run-time generation of association rules by considering only those patterns that meet on-line data requirements.

#index 342716
#* Rapid association rule mining
#@ Amitabha Das;Wee-Keong Ng;Yew-Kwong Woon
#t 2001
#c 1
#% 273898
#% 300120
#% 388922
#% 393792
#% 443164
#% 462238
#% 464204
#% 481290
#% 511333
#% 589303
#! Association rule mining is a well-researched area where many algorithms have been proposed to improve the speed of mining. In this paper, we propose an innovative algorithm called Rapid Association Rule Mining (RARM) to once again break this speed barrier. It uses a versatile tree structure known as the Support-Ordered Trie Itemset (SOTrieIT) structure to hold pre-processed transactional data. This allows RARM to generate large 1-itemsets and 2-itemsets quickly without scanning the database and without candidate 2-itemset generation. It achieves significant speed-ups because the main bottleneck in association rule mining using the Apriori property is the generation of candidate 2-itemsets. RARM has been compared with the classical mining algorithm Apriori and it is found that it outperforms Apriori by up to two orders of magnitude (100 times), much more than what recent mining algorithms are able to achieve.

#index 342717
#* Mining generalised disjunctive association rules
#@ Amit A. Nanavati;Krishna P. Chitrapura;Sachindra Joshi;Raghu Krishnapuram
#t 2001
#c 1
#% 152934
#% 199538
#% 210160
#% 227919
#% 247854
#% 259994
#% 464714
#% 481758
#% 708398
#! This paper introduces generalised disjunctive association rules such as "People who buy bread also buy butter jam", and "People who buy either raincoats or umbrellas also buy flashlights". A generalised disjunctive association rule allows the disjunction of conjuncts, "People who buy jackets also buy bow ties or neckties and tiepins". Such rules capture contextual inter-relationships among items.Given a context (antecedent), there may be a large number of generalised disjunctive association rules that satisfy the minsupp and minconf constraints. It is computationally expensive to find all such rules. We present algorithm thrifty traverse which borrows concepts such as subsumption from propositional logic to mine a subset of such rules in a computationally feasible way. We experimented with our algorithm on US census data as well as transaction data from a grocery superstore to demonstrate its computational feasibility, utility and scalability.

#index 342718
#* Automatic discovery of salient segments in imperfect speech transcripts
#@ Dulce Ponceleon;Savitha Srinivasan
#t 2001
#c 1
#% 249191
#% 262061
#% 309102
#% 618418
#% 748583
#! This paper addresses the problem of automatic detection of salient video segments for real-world applications such as corporate training based on associated speech transcriptions. We present a novel segmentation algorithm based on automatic speech recognition (ASR) applied to the audio track of the video. Our feature set consists of word n-grams extracted from the imperfect speech transcriptions. We use a two-pass algorithm that combines a boundary-based method with a content-based method. In the first pass, we analyze the temporal distribution and the rate of arrival of features to compute an initial segmentation. In the second pass, we detect changes in content-bearing words by using the content-bearing features as queries in an information retrieval system. The content-based second pass validates the initial segments and merges them as needed. Variations in the structure of the audio/video content, and the accuracy of ASR have an impact on the feasibility of the segmentation task. For realistic data we observe that we can identify content-rich segments of the audio. In the best scenario a high-level table-of-contents is generated and in the worse scenario a single salient segment is identified. We illustrate the algorithm in detail with some examples and validate the data with manual segmentation boundaries.

#index 342719
#* Finding similar images quicky using object shapes
#@ Heng Tao Shen
#t 2001
#c 1
#% 23998
#% 86950
#% 93221
#% 181409
#% 210173
#% 248790
#% 273919
#% 300132
#% 311805
#% 316196
#% 316198
#% 316199
#% 427199
#% 431486
#% 435141
#% 462059
#% 480093
#! Retrieving images from a large image collection has been an active area of research. Most of the existing works have focused on content representation. In this paper, we address the issue of identifying relevant images quickly. This is important in order to meet the users' performance requirements. We propose a framework for fast image retrieval based on object shapes extracted from objects within images. The framework builds a hierarchy of approximations on object shapes such that shape representation at a higher level is a coarser representation of a shape at the lower level. In other words, multiple shapes at a lower level can be mapped into a single shape at a higher level. In this way, the hierarchy serves to partition the database at various granularities. Given a query shape, by searching only the relevant paths in the hierarchy, a large portion of the database can thus be pruned away. We propose the angle mapping (AM) method to transform a shape from one level to another (higher) level. AM essentially replaces some edges of a shape by a smaller number of edges based on the angles between the edges, thus reducing the complexity of the original shape. Based on the framework, we also propose two hierarchical structures to facilitate speedy retrieval. The first, called Hierarchical Partitioning on Shape Representation (HPSR), uses the shape representation as the indexing key. The second, called Hierarchical Partitioning on Angle Vector (HPAV), captures the angle information from the shape representation. We conducted an extensive study on both methods to see their quality and efficiency. Our experiments on sets of images, each of which has objects around from 1 to 30, showed that the framework can provide speedy image retrieval without sacrificing on the quality. Both proposed schemes can improve the efficiency by as much as hundreds of times to sequential scanning. The improvement grows as image database size, objects per image or object dimension increase.

#index 342720
#* Content-based retrieval of MP3 music objects
#@ Chih-Chin Liu;Po-Jun Tsai
#t 2001
#c 1
#% 194192
#% 219841
#% 245521
#% 260018
#% 267568
#% 434710
#% 434753
#% 614622
#% 631989
#% 636391
#% 641568
#! In recent years, the searching and indexing techniques for multimedia data are getting more attention in the area of multimedia databases. As many research works were done on the content-based retrieval of image and video data, less attention was received to the content-based retrieval of audio data. In this paper, we propose an approach to retrieve MP3 music objects based on their content. In our approach, the coefficients extracting from the output of the polyphase filters are used to compute the MP3 features for indexing the MP3 objects. We also propose an MP3 similarity measuring function to provide users the ability to approximately retrieve the desired MP3 objects. Experiments are performed and analyzed to show the efficiency and the effectiveness of the proposed method.

#index 342721
#* Irregularity in multi-dimensional space-filling curves with applications in multimedia databases
#@ Mohamed F. Mokbel;Walid G. Aref
#t 2001
#c 1
#% 13041
#% 45766
#% 64431
#% 86951
#% 109053
#% 118213
#% 149140
#% 152937
#% 153260
#% 196975
#% 202617
#% 232806
#% 235402
#% 261401
#% 274592
#% 317933
#% 332133
#% 346704
#% 415957
#% 427199
#% 443397
#% 443457
#% 462956
#% 465015
#% 481455
#% 481920
#% 527181
#% 548766
#! A space-filling curve is a way of mapping the multi-dimensional space into the one-dimensional space. It acts like a thread that passes through every cell element (or pixel) in the N-dimensional space so that every cell is visited at least once. Thus, a space-filling curve imposes a linear order of the cells in the N-dimensional space. There are numerous kinds of space-filling curves. The difference between such curves is in their way of mapping to the one-dimensional space. Selecting the appropriate curve for any application requires a brief knowledge of the mapping scheme provided by each space-filling curve. Irregularity is proposed as a quantitative measure of the quality of the mapping of the space-filling curve. Closed formulas are developed to compute the irregularity for any general dimension D with N points in each dimension for different space-filling curves.A comparative study of different space-filling curves with respect to irregularity is conducted and results are presented and discussed. The applicability of this research is the area of multimedia databases is illustrated with a discussion of the problems that arise.

#index 342722
#* Tempus Fugit: a system for making semantic connections
#@ Daniel A. Ford;Joann Ruvolo;Stefan Edlund;Jussi Myllymaki;James Kaufman;Jared Jackson;Martin Gerlach
#t 2001
#c 1
#% 334304
#! Tempus Fugit ("Time Flies") is the first of a new generation of Personal Information Management (PIM) systems. A PIM system incorporates an electronic calendar, "to-do" list and address book. The premise behind Tempus Fugit is that information stored in electronic calendars, to-do lists and address books can be given richer semantic interpretation and automatically processed to make its users more effective. Tempus Fugit also tracks the physical and virtual locations of users and uses this information to predict meeting attendance and help them as they travel during their day.

#index 342723
#* FOCI: flexible organizer for competitive intelligence
#@ Hwee-Leng Ong;Ah-Hwee Tan;Jamie Ng;Hong Pan;Qiu-Xiang Li
#t 2001
#c 1
#% 23408
#% 111415
#% 185921
#! This paper describes how an integrated web-based application, code-named FOCI (Flexible Organizer for Competitive Intelligence), can help the knowledge worker in the gathering, organizing, tracking, and dissemination of competitive intelligence or knowledge bases on the web. It shows how text mining techniques including a novel user-configurable clustering, trend analysis and visualization techniques can be used synergistically to address the problem of managing information gathered from the web. FOCI allows a user to define and personalize the organization of the information clusters according to their needs and preferences into portfolios. Predefined sections for organizing information in specific domains is also supported. The personalized portfolios created can be saved and subsequently tracked and shared with other users. In addition, FOCI is designed to handle multilingual documents.

#index 342724
#* Towards speech as a knowledge resource
#@ Eric Brown;Savitha Srinivasan;Anni Coden;Dulce Ponceleon;James Cooper;Arnon Amir;Jan Pieper
#t 2001
#c 1
#% 218978
#% 237340
#% 309124
#% 438470
#% 607977
#% 608367
#% 608370
#% 771356
#! Speech is a tantalizing mode of human communication. On the one hand, humans understand speech with ease and use speech to express complex ideas, information, and knowledge. On the other hand, automatic speech recognition with computers is still very hard, and extracting knowledge from speech is even harder. In this paper we motivate the study of speech as a knowledge resource and briefly survey a family of related applications and systems being developed at IBM Research aimed towards the goal of exploiting speech as a knowledge resource.

#index 342725
#* Recent developments in text summarization
#@ Inderjeet Mani
#t 2001
#c 1
#% 198058
#% 330780
#% 387791
#% 438324
#% 815850
#% 853649
#! With the explosion in the quantity of on-line text and multimedia information in recent years, demand for text summarization technology is growing. Increased pressure for technology advances is coming from users of the web, on-line information sources, and new mobile devices, as well as from the need for corporate knowledge management. Commercial companies are increasingly starting to offer text summarization capabilities, often bundled with information retrieval tools. In this paper, I will discuss the significance of some recent developments in summarization technology.

#index 342726
#* Summarization of discussion groups
#@ Robert Farrell;Peter G. Fairweather;Kathleen Snyder
#t 2001
#c 1
#% 169770
#% 288614
#% 607945
#! In this paper, we describe an algorithm to generate textual summaries of discussion groups. Our system combines sentences extracted from individual postings into variable-length summaries by utilizing the hierarchical discourse context provided by discussion threads. We have incorporated this algorithm into a Web-based application called IDS (Interactive Discussion Summarizer).

#index 342727
#* Question answering in TREC
#@ Ellen M. Voorhees
#t 2001
#c 1
#% 309124
#% 309127
#% 312689
#% 741892
#% 815828
#% 816061
#! Traditional text retrieval systems return a ranked list of documents in response to a user's request. While a ranked list of documents can be an appropriate response for the user, frequently it is not. Usually it would be better for the system to provide the answer itself instead of requiring the user to search for the answer in a set of documents. The Text REtrieval Conference (TREC) is sponsoring a question answering "track" to foster research on the problem of retrieving answers rather than document lists.TREC is a workshop series sponsored by the National Institute of Standards and Technology and the U.S. Department of Defense [7]. The purpose of the conference series is to encourage research on text retrieval for realistic applications by providing large test collections, uniform scoring procedures, and a forum for organizations interested in comparing results. The conference has focused primarily on the traditional IR problem of retrieving a ranked list of documents in response to a statement of information need, but has also included other tasks, called tracks, that focus on new areas or particularly difficult aspects of information retrieval. A question answering track was introduced in TREC-8 1999. The track has generated wide-spread interest in the QA problem [2, 3, 4], and has documented significant improvements in question answering system effectiveness in its two-year history.This paper provides a brief summary of the findings of the TREC question answering track to date and discusses the future directions of the track. The paper is extracted from a fuller description of the track given in "The TREC Question Answering Track" [8]. Complete details about the TREC question answering track can be found in the TREC proceedings.

#index 342728
#* The Enosys Markets data integration platform: lessons from the trenches
#@ Yannis Papakonstantinou;Vasilis Vassalos
#t 2001
#c 1
#! Enosys Markets offers a state-of-the-art data integration software platform to support the development of the next generation of eBusiness applications that deliver value by providing new levels of function for customer relationship management, e-commerce, supply chain management, and decision support. These applications require that data be integrated from information sources that exist both within and across organizational boundaries. The Enosys Markets data integration architecture and product family provides a complete end-to-end XML-based solution for integrating and querying distributed information sources. It incorporates advanced research into XML and database technology. We present the product architecture and components, discuss the key technical challenges, and outline the technical concepts and innovations employed in the Enosys platform.

#index 342729
#* Self-managing technology in IBM DB2 universal database
#@ Daniel Zilio;Sam Lightstone;Kelly Lyons;Guy Lohman
#t 2001
#c 1
#% 36119
#% 170893
#% 218149
#% 248815
#% 248855
#% 275367
#% 287258
#% 458523
#% 462204
#% 480158
#% 481459
#% 632100
#! As the cost of both hardware and software falls due to technological advancements and economies of scale, the cost of ownership for database applications is increasingly dominated by the cost of people to manage them. Databases are growing rapidly in scale and complexity, while skilled database administrators (DBAs) are becoming rarer and more expensive. The scope of responsibility of DBAs is indeed daunting. This paper describes the self-managing technology in IBM DB2 Universal Database to illustrate how self-managing technology can enhance the usability of enterprise middleware and reduce the total cost of ownership (TCO).

#index 342730
#* Document release versus data access controls: two sides of the same coin?
#@ Arnon Rosenthal;Gio Wiederhold
#t 2001
#c 1
#% 164560
#% 338856
#% 488636
#! The database and document worlds have traditionally had different approaches to security. Databases provide access controls on structured data, while document security interrogates the outgoing information, based on document markings and actual contents. For the emerging world in which many documents are generated from structured data (and vice versa), the separation can cause failure, implementation-dependence, inconsistency, and wasted effort. After comparing approaches and mechanisms in the two areas, we identify issues in security administration and implementation in military and medical applications. We then present elements of a unifying model.

#index 342731
#* Advanced grouping and aggregation for data integration
#@ Eike Schallehn;Kai-Uwe Sattler;Gunter Saake
#t 2001
#c 1
#% 201889
#% 443055
#% 495275
#! New applications from the areas of analytical data processing and data integration require powerful features to condense and reconcile available data. As outlined in [1], the general concept of grouping and aggregation appears to be a fitting paradigm for a number of these issues, but in its common form of equality based groups or with current extensions like simple user-defined functions to derive group-by values on a per tuple basis and restricted aggregate functions a number of problems remain unsolved. We describe two extensions to the grouping mechanism, a generic one to support holistic user-defined grouping functions and higher level construct that provides similarity based grouping suitable in a number of applications like duplicate detection and elimination.

#index 342732
#* Dynamic versioning concurrency control for index-based data access in main memory database systems
#@ Ying Xia;Sung-Hee Kim;Sook-Kyoung Cho;Kee-Wook Rim;Hae-Young Bae
#t 2001
#c 1
#% 3951
#% 252458
#% 273940
#% 479769
#! We present a concurrency control scheme using dynamic versioning for index-based data access in main memory database systems. This scheme enables read-only transactions read correct version without holding any locks or latches, while update transactions only obtain a few locks or latches without deadlocks. Efficient version management is designed to support high concurrency level and low space overhead. The interaction between dynamic versioning and indexing is considered so that all available versions can be accessed through indexing. Experiment results show that dynamic versioning can improve the performance in concurrent environment significantly.

#index 342733
#* O-PreH: optimistic transaction processing algorithm based on pre-reordering in hybrid broadcast environments
#@ SungSuk Kim;SangKeun Lee;SoonYoung Jung;Chong-Sun Hwang
#t 2001
#c 1
#% 9241
#% 201897
#% 661499
#! In recent years, there has been a lot of research effort in the periodic push model where the server repetitively disseminates information without explicit request. We call the broadcast model supporting backchannel as hybrid broadcast. In this paper, we devise a new transaction processing algorithm called O-PreH, which is based on the notion of pre-reordering. If one or more conflicts for mobile transactions are found from server's periodic invalidation report, conflict orders are determined not to violate the consistency( pre-reordering) and then the remaining operations have to be executed pessimistically.

#index 342734
#* Algorithm for discovering multivalued dependencies
#@ Men Hin Yan;Ada Wai-chee Fu
#t 2001
#c 1
#% 125557
#% 129570
#% 169370
#% 346917
#% 464837

#index 342735
#* A performance comparison of bitmap indexes
#@ Kesheng Wu;Ekow J. Otoo;Arie Shoshani
#t 2001
#c 1
#% 479808
#% 571294
#% 617842
#! We present a comparison of two new word-aligned schemes with some schemes for compressing bitmap indexes, including the well-known byte-aligned bitmap code (BBC). On both synthetic data and real application data, the new word-aligned schemes use only 50% more space, but perform logical operations on compressed data 12 times faster than BBC. The new schemes achieve this performance advantage by guaranteeing that during logical operations every machine instruction performs useful work on words rather than on bytes or bits as in BBC.

#index 342736
#* Facilitating knowledge flow through the enterprise
#@ Jeanette Bruno
#t 2001
#c 1
#% 290707
#! This paper is concerned with the issues we encountered when attempting to achieve enterprise level knowledge reuse. We present 3 pilot studies where new visualization techniques were used to allow manufacturing and service operations take advantage of engineering knowledge embodied in 3D models. Though all these studies showed dramatic productivity increases, only one business unit from the studies is currently working to achieve the reuse. There are a number of reasons why this is so, but the key underlying theme is a lack of enterprise level commitment to knowledge sharing and a lack of an adequate knowledge architecture for sharing knowledge across organizational boundaries. We conclude with an approach for facilitating knowledge flow across functional units.

#index 342737
#* Information access in implicit culture framework
#@ Enrico Blanzieri;Paolo Giorgini;Sabrina Recla;Paolo Massa
#t 2001
#c 1
#% 280852
#% 1650569
#! The goal of a System for Implicit Culture Support (SICS) is to establish an implicit culture phenomenon, namely when the elements of a set behave according to the culture of a generally different group of agents. Earlier work claimed that Implicit Culture support can be seen as a generalization of Collaborative Filtering. In this paper, we recall the concept of Implicit Culture, show how it is useful for automatically exploit tacit knowledge and we present an implementation of a System for Implicit Culture Support.

#index 342738
#* Real time user context modeling for information retrieval agents
#@ Travis Bauer;David B. Leake
#t 2001
#c 1
#% 67565
#% 262179
#% 292210
#% 297550
#% 1650593
#! The success of personal information agents depends on their ability to provide task-relevant information. This paper presents WordSieve, a new algorithm that generates context descriptions to guide document indexing and retrieval. WordSieve exploits information about the sequence of accessed documents to identify words which indicate a shift in context. We have tested WordSieve in a personal information agent, Calvin, which monitors a user's document access, generates a representation of the user's task context, indexes the resources consulted, and presents recommendations for other resources that were consulted in similar prior contexts. In initial experiments, WordSieve outperforms term frequency/inverse document frequency at matching documents to hand-coded vector representations of the task contexts in which they were originally consulted, where the task context representations are term vectors representing a specific search task given to the user.

#index 342739
#* A clustering algorithm for asymmetrically related data with applications to text mining
#@ K. Krishna;Raghu Krishnapuram
#t 2001
#c 1
#% 115462
#% 280849
#% 375388
#% 406493
#! Clustering techniques find a collection of subsets of a data set such that the collection satisfies a criterion that is dependent on a relation defined on the data set. The underlying relation is traditionally assumed to be symmetric. However, there exist many practical scenarios where the underlying relation is asymmetric. One example of an asymmetric relation in text analysis is the inclusion relation, i.e., the inclusion of the meaning of a block of text in the meaning of another block. In this paper, we consider the general problem of clustering of asymmetrically related data and propose an algorithm to cluster such data. To demonstrate its usefulness, we consider two applications in text mining: (1) summarization of short documents, and (2) generation of a concept hierarchy from a set of documents. Our experiments show that the performance of the proposed algorithm is superior to that of more traditional algorithms.

#index 342740
#* A statistical model for scientific readability
#@ Luo Si;Jamie Callan
#t 2001
#c 1
#! In this paper, we present a new method of using statistical models to estimate readability [1]. Language Model is used to capture the content information. It is combined with linguistic feature model by a linear form. Experiments show that this new method has a better performance than the widely used Flesch-Kincaid readability formula.

#index 342741
#* Discovering the representative of a search engine
#@ King-Lup Liu;Adrain Santoso;Clement Yu;Weiyi Meng
#t 2001
#c 1
#% 273926
#% 287237
#% 333945
#% 443561
#% 479642
#% 584914
#! Given a large number of search engines on the Internet, it is difficult for a person to determine which search engines could serve his/her information needs. A common solution is to construct a metasearch engine on top of the search engines. Upon receiving a user query, the metasearch engine sends it to those underlying search engines which are likely to return the desired documents for the query. The selection algorithm used by a metasearch engine to determine whether a search engine should be sent the query typically makes the decision based on the search-engine representative, which contains characteristic information about the database of a search engine. However, an underlying search engine may not be willing to provide the needed information to the metasearch engine. This paper shows that the needed information can be estimated from an uncooperative search engine with good accuracy. Two pieces of information which permit accurate search engine selection are the number of documents indexed by the search engine and the maximum weight of each term. In this paper, we present techniques for the estimation of these two pieces of information.

#index 342742
#* Advances in phonetic word spotting
#@ Arnon Amir;Alon Efrat;Savitha Srinivasan
#t 2001
#c 1
#% 218984
#% 219033
#% 235941
#% 237291
#% 262039
#% 309102
#% 968469
#! Phonetic speech retrieval is used to augment word based retrieval in spoken document retrieval systems, for in and out of vocabulary words. In this paper, we present a new indexing and ranking scheme using metaphones and a Bayesian phonetic edit distance. We conduct an extensive set of experiments using a hundred hours of HUB4 data with ground truth transcript and twenty-four thousands query words. We show improvement of up to 15% in precision compare to results obtained speech recognition alone, at a processing time of 0.5 Sec per query.

#index 342743
#* Reorganizing web sites based on user access patterns
#@ Yongjian Fu;Mario Creado;Chunhua Ju
#t 2001
#c 1
#% 614610
#% 1273676
#! In this paper, an approach for reorganizing Web sites based on user access patterns is proposed. The approach consists of three steps: preprocessing, page classification, and site reorganization. In preprocessing, pages on a Web site are processed to create an internal representation of the site, and page access information of its users is extracted from its server log. In page classification, the Web pages on the site are classified into two categories, index pages and content pages, based on the page access information. After the pages are classified, in site reorganization, the Web site is examined to find better ways to organize and arrange the pages on the site. Our experiments on a large real data set show that the approach is efficient and practical for adaptive Web sites.

#index 342744
#* A domain independent environment for creating information extraction modules
#@ Ronen Feldman;Yonatan Aumann;Yair Liberzon;Kfir Ankori;Jonathan Schler;Benjamin Rosenfeld
#t 2001
#c 1
#% 236701
#% 815335
#! Text-Mining is a growing area of interest within the field of Data Mining and Knowledge Discovery. Given a collection of text documents, most approaches to Text Mining perform knowledge-discovery operations either on external tags associated with each document, or on the set of all words within each document. Both approaches suffer from limitations. This paper focuses on an intermediate approach, one that we call text mining via information extraction, in which knowledge discovery takes place on focused, relevant terms, phrases and facts, as extracted from the documents.

#index 342745
#* Ordinal association rules for error identification in data sets
#@ Andrian Marcus;Jonathan I. Maletic;King-Ip Lin
#t 2001
#c 1
#% 152934
#% 210160
#% 227953
#% 248785
#% 320944
#% 420072
#% 479482
#% 480499
#! A new extension of the Boolean association rules, ordinal association rules, that incorporates ordinal relationships among data items, is introduced. One use for ordinal rules is to identify possible errors in data. A method that finds these rules and identifies potential errors in data is proposed.

#index 342746
#* XML, the web and database functionality?
#@ Erich J. Neuhold
#t 2001
#c 1
#! With the advance of XML, DTD's, XML-Schema, X-Path, X-Link, XML-Query, XML-Protocol, RDF, Semantic WEB and other information description tools and mechanisms the WEB develops more and more into a huge structured information and data network that becomes computer processable via "semi"-automatic "intelligent" means.However, most of these multimedia information resources are kept in hierarchical file-type systems or relatively unstructured relational systems with none or very little of what is commonly known as "database functionality" for the various information types represented. Do we still need transaction properties, interoperability, security, reliability, scalability and others?If yes, then why are these concepts not really taken into the WEB-world?Are the database people too far away to be able to adopt their existing solutions?Are the information system people too process-oriented to care about data?Are the knowledge handlers too occupied with semantic representations and ontology discussions?Does the document community even see the problem?And do the WEB people care whether human resources are wasted through attempts to achieve database functionality through careful "behaviour"?The panel will offer opinions about these aspects but will also want to raise these issues with the audience.

#index 342747
#* What can researchers do to improve security of data and documents? (panel description)
#@ Arnon Rosenthal
#t 2001
#c 1
#! Data security (protection of information rather than systems) goes far beyond the traditional questions of RDBMS grant/revole, or security markings on documents. We will discuss what the new research agenda should be to impact the masses of systems.

#index 342868
#* Proceedings of the 3rd international workshop on Web information and data management
#@ Ee-Peng Lim
#t 2001
#c 1
#! The 2001 International Workshop on Web Information and Data Management (WIDM?01) is the third in a series of workshops on Web Information and Data Management held in conjunction with the International Conference on Information and Knowledge Management (CIKM). The objective of the workshop is to bring together researchers, industrial practitioners, and developers to study how the web information can be extracted, stored, analysed and processed to provide useful knowledge to the end users for various advanced database applications. Apart from the sponsorship from both ACM SIGIR and ACM SIGMIS, WIDM?01 has received generous sponsorship from the E-Book Systems Pte Ltd.The Call for Papers attracted 34 submission of research papers, out of which 11 were accepted. All papers were thoroughly reviewed by the program committee and external reviewers. These papers have divided into 4 sessions: ?Web Data Mining,? ?Web Information Management,? ?Web Performance Optimization,? and ?Web Information Integration.? The workshop would not be a success without the help of a few important people. Lay-Choo Tan and Chee-Keong Lai, the technical officers from the Centre for Advanced Information Systems (CAIS) did a wonderful job in setting the website for the electronic submission of abstracts and papers. We are also indebted to the CIKM?s main committee for their support: Il-Yeol Song for coordinating WIDM?01 with the other workshops; Eun-Kyo Park for managing the financial and registration matters; and Alvin Lim for local arrangement. Finally we would like to thank the authors and contributors for their effort in supporting the workshop.

#index 348587
#* Proceedings of the 9th ACM international symposium on Advances in geographic information systems
#@ Kia Makki;Niki Pissinou;Walid G. Aref
#t 2001
#c 1
#! Over the past decade, the ACM Symposium on Advances in Geographic Information Systems - ACM GIS - has been conceived as a forum for dissemination of original research and experience in the field of geographic data studies. This symposium traditionally attracts submissions from all continents. The broad spectrum of subjects and applications covered serve as an indicator of the increasing number of problems that require handling of geographic data.The program committee of ACM GIS 2001 has the pleasure of announcing an outstanding program for this year. Out of 93 submissions, only 29 are accepted. Each submission was reviewed by three members of the program committee. The selection was based on the quality and originality of the contribution. The excellent collection of technical papers as well as the exciting invited talks address a wide variety of timely issues on GIS. We hope you will find the technical program stimulating, the discussions fruitful, and your stay in Atlanta relaxing.

#index 348631
#* Proceedings of the 4th ACM international workshop on Data warehousing and OLAP
#@ Joachim Hammer
#t 2001
#c 1
#! Data Warehousing and Online Analytical Processing have emerged as key technologies for enterprises wishing to streamline the management of their operational and business data and to improve decision support activities. In addition, data warehousing continues to take on broader roles, for example, in the context of integrating heterogeneous data sources, knowledge sharing in extended enterprises, and internet- based e-commerce.The Fourth ACM International Workshop on Data Warehousing and Online Analytical Processing (DOLAP 2001), which is held in Atlanta, GA, USA, provides an international forum for researchers and practitioners to share their findings in theoretical foundations, methodologies, practical experiences, and new research directions in the areas of data warehousing and online analytical processing.These proceedings contain the technical papers selected for presentation at the workshop. We received 31 submissions from 16 different countries. The submitted research papers covered the state- of- the- art in data warehousing and related fields including data warehouse architecture, design and evolution, multidimensional modeling, query optimization, indexing, view materialization and maintenance, data warehouse quality, XML- and object- based warehouses, and data warehousing and the Web. In addition, we received a number of industrial submissions describing ongoing data warehousing projects and novel applications for warehouses. After careful review by the program committee, 12 research and industrial papers were selected for the workshop.

#index 413116
#* Proceedings of the 5th ACM international workshop on Data Warehousing and OLAP
#@ Il-Yeol Song;Dimitri Theodoratos
#t 2002
#c 1
#! Data Warehousing and On-Line Analytical Processing (OLAP) are technologies for providing executives and managers with timely, integrated access to critical information from multiple, distributed, heterogeneous databases and other information sources for analysis and decision making activities.The ACM International Workshop on Data Warehousing and Online Analytical Processing (DOLAP) is an annual event that provides an international forum where both researchers and practitioners can share their findings in theoretical foundations, current methodologies, practical experiences, and new research directions in the areas of data warehousing and online analytical processing. The fifth DOLAP workshop was held this year in McLean, VA, USA.These proceedings contain the papers selected for presentation at the workshop. We received 22 submissions form 16 different countries. The submitted papers covered various aspects of Data Warehousing and OLAP including multidimensional modeling, materialized view selection, extract-transform-merge processes, view maintenance, temporal and object-oriented aspects of data warehouses, query languages for OLAP, query processing and optimization, indexing, and XML data warehouses. After careful review the program committee selected 11 papers for presentation at the workshop. The accepted papers were presented in three sessions: data warehouse conceptual modeling, data warehouse logical design, and data warehouse operation and query optimization.

#index 413544
#* Proceedings of the eleventh international conference on Information and knowledge management
#@ Charles Nicholas;David Grossman;Konstantinos Kalpakis;Sajda Qureshi;Han van Dissel;Len Seligman
#t 2002
#c 1
#! Welcome to the Eleventh ACM International Conference on Information and Knowledge Management! The organizers and sponsors of CIKM'02 join me in the hope that this conference proves to be an opportunity for the sharing of knowledge, and the creation and renewal of friendships.The first CIKM conference took place about ten years ago. In cybertime that is truly ancient history, and you may agree that the world has changed in many ways since then. One thing that has not changed is the need for scientists to meet each other, to exchange ideas, and to welcome new members of the scientific community. To the extend that CIKM does this, it serves its purpose.

#index 413545
#* On scalable information retrieval systems
#@ Ophir Frieder
#t 2002
#c 1
#! Implementing scalable information retrieval systems requires the design and development of efficient methods to ingest data from multiple sources, search and retrieve results from both English and foreign language document collections and from collections comprising of multiple data types, harness high performance computer technology, and accurately answer user questions. Some recent efforts related to the development of scalable information retrieval systems are described. Particular emphasis is placed on those efforts that were adopted into commercial use.

#index 413546
#* Future directions in data mining: streams, networks, self-similarity and power laws
#@ Christos Faloutsos
#t 2002
#c 1
#! How to spot abnormalities in a stream of temperature data from a sensor? Or from a network of sensors? How does the Internet look like? Are there 'abnormal' sub-graphs in a given social network, possibly indicating, e.g., money-laundering rings?We present some recent work and list many remaining challenges for these two fascinating issues in data mining, namely, streams and networks. Streams appear in numerous settings, in the form of, e.g., temperature readings, road traffic data, series of video frames for surveillance, patient physiological data. In all these settings, we want to equip the sensors with nimble, but powerful enough algorithms to look for patterns and abnormalities,(a) on a semi-infinite stream,(b) using finite memory, and (c) without human intervention.

#index 413547
#* Knowledge and information management: is it possible to do interesting and important research, get funded, be useful and appreciated?
#@ Maria Zemankova
#t 2002
#c 1
#! The survey of the CIKM Call for Papers for the period 1998 - 2002 demonstrates that the CIKM organizers very accurately "identify challenging problems facing the development of future knowledge and information systems [in] applied and theoretical research" [1998] and also play an important role fostering "bridging traditionally separated areas such as databases and information retrieval, or those that apply techniques from one area to another" [2001, 2002]. The presented CIKM papers also indicate that researchers work on interesting problems. This talk will discuss some additional research topics for future consideration by the CIKM community.In most cases, to achieve important results, research needs to be well supported. If you are not getting the funding you need, this talk may provide some pointers where you can look for finding. If you are well supported, I will try to convince you that you can be instrumental in improving the funding scenario for everybody, by mentoring the junior members of the CIKM community, by forming collaborative (international, interdisciplinary) teams and by letting the funders know what you find conducive to your research and what you consider a hindrance. Regardless if you are well funded or not, it is most helpful if you are active in identifying new research directions and also assist in evaluating the priorities.The most frequent problem for inadequate funding is lack of funds. However, researchers can help! This can be achieved in many different ways: thinking about long-term applications of fundamental research to societal needs; working with communities that can directly benefit from research; sharing the research results not only with the research colleagues, but also wider constituencies - at their appropriate levels; and informing your funders about your spectacular achievements, i.e., providing good reasons for increasing the research funding.CIKM strives to bring together research communities that traditionally do not work together. Providing a forum for interdisciplinary research is laudable and very important, as very often interdisciplinary or international research is not "appreciated". This talk will discuss how we could gradually change the discipline- and country-based "appraisal" cultures. We will also attempt to answer the question: "What is the ultimate appreciation?" (Nobel Prize? ...successful .com?... ???).

#index 413548
#* F4: large-scale automated forecasting using fractals
#@ Deepayan Chakrabarti;Christos Faloutsos
#t 2002
#c 1
#% 172949
#% 227924
#% 287070
#% 310502
#% 333926
#% 333941
#% 334059
#% 427199
#% 428155
#% 480146
#% 480307
#% 481620
#! Forecasting has attracted a lot of research interest, with very successful methods for periodic time series. Here, we propose a fast, automated method to do non-linear forecasting, for both periodic as well as chaotic time series. We use the technique of delay coordinate embedding, which needs several parameters; our contribution is the automated way of setting these parameters, using the concept of `intrinsic dimensionality'. Our operational system has fast and scalable algorithms for preprocessing and, using R-trees, also has fast methods for forecasting. The result of this work is a black-box which, given a time series as input, finds the best parameter settings, and generates a prediction system. Tests on real and synthetic data show that our system achieves low error, while it can handle arbitrarily large datasets.

#index 413549
#* An iterative strategy for pattern discovery in high-dimensional data sets
#@ Chun Tang;Aidong Zhang
#t 2002
#c 1
#% 60576
#% 248792
#% 273891
#% 290758
#% 316709
#% 328317
#% 397641
#% 480307
#% 659967
#% 720338
#! High-dimensional data representation in which each data item (termed target object) is described by many features, is a necessary component of many applications. For example, in DNA microarrays, each sample (target object) is represented by thousands of genes as features. Pattern discovery of target objects presents interesting but also very challenging problems. The data sets are typically not task-specific, many features are irrelevant or redundant and should be pruned out or filtered for the purpose of classifying target objects to find empirical pattern. Uncertainty about which features are relevant makes it difficult to construct an informative feature space. This paper proposes an iterative strategy for pattern discovery in high-dimensional data sets. In this approach, the iterative process consists of two interactive components: discovering patterns within target objects and pruning irrelevant features. The performance of the proposed method with various real data sets is also illustrated.

#index 413550
#* Mining sequential patterns with constraints in large databases
#@ Jian Pei;Jiawei Han;Wei Wang
#t 2002
#c 1
#% 210160
#% 248785
#% 300120
#% 310558
#% 420063
#% 425006
#% 463903
#% 464989
#% 464996
#% 479971
#% 481290
#! Constraints are essential for many sequential pattern mining applications. However, there is no systematic study on constraint-based sequential pattern mining. In this paper, we investigate this issue and point out that the framework developed for constrained frequent-pattern mining does not fit our missions well. An extended framework is developed based on a sequential pattern growth methodology. Our study shows that constraints can be effectively and efficiently pushed deep into sequential pattern mining under this new framework. Moreover, this framework can be extended to constraint-based structured pattern mining as well.

#index 413551
#* Searching web databases by structuring keyword-based queries
#@ Pável Calado;Altigran S. da Silva;Rodrigo C. Vieira;Alberto H. F. Laender;Berthier A. Ribeiro-Neto
#t 2002
#c 1
#% 44876
#% 111303
#% 219047
#% 309104
#% 309726
#% 387427
#% 406493
#% 431501
#% 479782
#% 479803
#% 479816
#% 659990
#% 659993
#! On-line information services have become widespread in the Web nowadays. However, Web users are non-specialized and have a great variety of interests. Thus, interfaces for Web databases must be simple and uniform. In this paper we present an approach, based on Bayesian networks, for querying Web databases using keywords only. According to this approach, the user inputs a query through a simple search-box interface. From the input query, one or more plausible structured queries are derived and submitted to Web databases. The results are then retrieved and presented to the user as ranked answers. Our approach reduces the complexity of existing on-line interfaces and offers a solution to the problem of querying several distinct Web databases with a single interface. The applicability of the proposed approach was demonstrated by experimental results with 3 databases, obtained with a prototype search system that implements it. We have found that from 77% to 95% of the time, one of the top three resulting structured queries is the proper one. Further, when the user selects one of these three top queries for processing, the ranked answers present average precision figures from 60% to about 100%.

#index 413552
#* Topic-oriented collaborative crawling
#@ Chiasen Chung;Charles L. A. Clarke
#t 2002
#c 1
#% 118731
#% 248810
#% 255137
#% 262085
#% 267454
#% 268079
#% 268087
#% 273796
#% 280817
#% 281251
#% 290830
#% 300176
#% 309141
#% 309145
#% 309151
#% 309787
#% 311027
#% 312861
#% 330599
#% 330604
#% 330616
#% 340903
#% 340924
#% 340953
#% 376266
#% 424292
#% 465895
#% 480309
#% 715251
#! A major concern in the implementation of a distributed Web crawler is the choice of a strategy for partitioning the Web among the nodes in the system. Our goal in selecting this strategy is to minimize the overlap between the activities of individual nodes. We propose a topic-oriented approach, in which the Web is partitioned into general subject areas with a crawler assigned to each. We examine design alternatives for a topic-oriented distributed crawler, including the creation of a Web page classifier for use in this context. The approach is compared experimentally with a hash-based partitioning, in which crawler assignments are determined by hash functions computed over URLs and page contents. The experimental evaluation demonstrates the feasibility of the approach, addressing issues of communication overhead, duplicate content detection, and page quality assessment.

#index 413553
#* Meta-recommendation systems: user-controlled integration of diverse recommendations
#@ J. Ben Schafer;Joseph A. Konstan;John Riedl
#t 2002
#c 1
#% 124004
#% 124010
#% 159108
#% 173879
#% 202009
#% 202011
#% 220707
#% 220709
#% 241036
#% 252755
#% 262178
#% 283169
#% 316169
#% 319273
#% 420117
#% 420121
#! In a world where the number of choices can be overwhelming, recommender systems help users find and evaluate items of interest. They do so by connecting users with information regarding the content of recommended items or the opinions of other individuals. Such systems have become powerful tools in domains such as electronic commerce, digital libraries, and knowledge management. In this paper, we address such systems and introduce a new class of recommender system called meta-recommenders. Meta-recommenders provide users with personalized control over the generation of a single recommendation list formed from a combination of rich data using multiple information sources and recommendation techniques. We discuss experiments conducted to aid in the design of interfaces for a meta-recommender in the domain of movies. We demonstrate that meta-recommendations fill a gap in the current design of recommender systems. Finally, we consider the challenges of building real-world, usable meta-recommenders across a variety of domains.

#index 413554
#* Removing redundancy and inconsistency in memory-based collaborative filtering
#@ Kai Yu;Xiaowei Xu;Anton Schwaighofer;Volker Tresp;Hans-Peter Kriegel
#t 2002
#c 1
#% 92533
#% 126949
#% 173879
#% 202011
#% 307100
#% 391410
#% 465928
#% 528156
#% 722799
#% 1650569
#! The application range of memory-based collaborative filtering (CF) is limited due to CF's high memory consumption and long runtime. The approach presented in this paper removes redundant and inconsistent instances (users) from the data. This paper aims to distinguish informative instances (users) from large raw user preference database and thus alleviate the memory and runtime cost of the widely used memory-based collaborative filtering (CF) algorithm. Our work shows that a satisfactory accuracy can be achieved by using only a small portion of the original data set, thereby alleviating the storage and runtime cost of the CF algorithm. In our approach, we consider instance selection as the problem of selecting informative data that increase the We begin by discussing the instance selection problem in a general sense that is to increase a posteriori probability of the optimal model by selecting informative data. We evaluate the empirical performance of our approach PF on two real-world data sets and attain very promisingpositive experimental results. The dData size and the prediction time are significantly reduced, while the prediction accuracy is on a par with almost the same as the results achieved by using the complete database.

#index 413555
#* Analysis of pre-computed partition top method for range top-k queries in OLAP data cubes
#@ Zheng Xuan Loh;Tok Wang Ling;Chuan Heng Ang;Sin Yeung Lee
#t 2002
#c 1
#% 227866
#% 275072
#% 312399
#% 461921
#% 464215
#% 479816
#% 479822
#% 479967
#% 480323
#% 563966
#% 591258
#% 631947
#! In decision support systems, having knowledge on the top k values is more informative and crucial than the maximum value. Unfortunately, the naive method involves high computational cost and the existing methods for range-max query are inefficient if applied directly. In this paper, we propose a Pre-computed Partition Top method (PPT) to partition the data cube and pre-store a number of top values for improving query performance. The main focus of this study is to find the optimum values for two parameters, i.e., the partition factor (b) and the number of pre-stored values (r), through analytical approach. A cost function based on Poisson distribution is used for the analysis. The analytical results obtained are verified against simulation results. It is shown that the PPT method outperforms other alternative methods significantly when proper b and r are used.

#index 413556
#* Batch data warehouse maintenance in dynamic environments
#@ Bin Liu;Songting Chen;Elke A. Rundensteiner
#t 2002
#c 1
#% 13016
#% 201898
#% 201928
#% 210210
#% 227869
#% 227947
#% 300141
#% 333848
#% 443527
#% 487993
#% 533937
#% 564109
#% 637833
#! Data warehouse view maintenance is an important issue due to the growing use of warehouse technology for information integration and data analysis. Given the dynamic nature of modern distributed environments, both data updates and schema changes are likely to occur in different data sources. In applications that the real-time refreshment of data warehouse extent under source changes is not critical, the source updates are usually maintained in a batch fashion to reduce the maintenance overhead. However, most prior work can only deal with batch source data updates. In this paper, we provide a solution strategy that is capable of batching both source data updates and schema changes. We propose techniques to first preprocess the initial source updates to summarize delta changes for each source. We then design a view adaptation algorithm to adapt the warehouse view under these delta changes. We have implemented our solutions and incorporated into an existing data warehouse prototype system. The experimental studies demonstrate excellent performance achievable by our batch techniques.

#index 413557
#* A fast filtering scheme for large database cleansing
#@ Sam Y. Sung;Zhao Li;Peng Sun
#t 2002
#c 1
#% 54571
#% 201889
#% 211348
#% 221899
#% 223781
#% 248801
#% 287222
#% 288885
#% 310546
#% 339369
#% 388009
#% 443703
#% 480496
#% 480499
#% 481944
#! Existing data cleansing methods are costly and will take very long time to cleanse large databases. Since large databases are common nowadays, it is necessary to reduce the cleansing time. Data cleansing consists of two main components, detection method and comparison method. In this paper, we first propose a simple and fast comparison method, TI-Similarity, which reduces the time for each comparison. Based on TI-Similarity, we propose a new detection method, RAR, to further reduce the number of comparisons. With RAR and TI-Similarity, our new approach for cleansing large databases is composed of two processes: Filtering process and Pruning process. In filtering process, a fast scan on the database is carried out with RAR and TI-Similarity. This process guarantees the detection of potential duplicate records but may introduce false positives. In pruning process, the duplicate result from the filtering process is pruned to eliminate the false positives using more trustworthy comparison methods. The performance study shows that our approach is efficient and scalable for cleansing large databases, and is about an order of magnitude faster than existing cleansing methods.

#index 413558
#* Semantic-based delivery of OLAP summary tables in wireless environments
#@ Mohamed A. Sharaf;Panos K. Chrysanthis
#t 2002
#c 1
#% 172876
#% 201897
#% 210182
#% 259634
#% 274199
#% 279165
#% 290747
#% 424925
#% 464215
#% 464706
#% 481948
#% 482107
#% 632027
#% 632046
#% 635901
#% 665436
#! With the rapid growth in mobile and wireless technologies and the availability, pervasiveness and cost effectiveness of wireless networks, mobile computers are quickly becoming the normal front-end devices for accessing enterprise data. In this paper, we are addressing the issue of efficient delivery of business decision support data in the form of summary tables to mobile clients equipped with OLAP front-end tools. Towards this, we propose a new on-demand scheduling algorithm, called SBS, that exploits both the derivation semantics among OLAP summary tables and the mobile clients' capabilities of executing simple SQL queries. It maximizes the aggregated data sharing between clients and reduces the broadcast length compared to the already existing techniques. The degree of aggregation can be tuned to control the tradeoff between access time and energy consumption. Further, the proposed scheme adapts well to different request rates, access patterns and data distributions. The algorithm effectiveness with respect to access time and power consumption is evaluated using simulation.

#index 413559
#* Symbolic photograph content-based retrieval
#@ Philippe Mulhem;Joo Hwee Lim
#t 2002
#c 1
#% 2298
#% 144076
#% 169940
#% 187773
#% 214709
#% 219055
#% 219847
#% 232703
#% 247889
#% 260241
#% 262095
#% 285540
#% 286582
#% 316199
#% 318785
#% 334502
#% 406493
#% 437405
#% 480625
#% 563749
#% 626558
#% 1854912
#! Photograph retrieval systems face the difficulty to deal with the different ways to apprehend the content of images. We consider and demonstrate here the use of multiple index representations of photographs to achieve effective retrieval. The use of multiple indexes allows integration of the complementary strengths of different indexing and retrieval models. The proposed representation supports multiple labels for regions and attributes, and handles inferences and relationships. We define links between indexing levels and the related query modes. The experiment conducted on 2400 home photographs shows the behavior of the multiple indexing levels during retrieval.

#index 413560
#* A compact and efficient image retrieval approach based on border/interior pixel classification
#@ Renato O. Stehling;Mario A. Nascimento;Alexandre X. Falcão
#t 2002
#c 1
#% 122671
#% 219845
#% 286582
#% 287144
#% 297155
#% 309093
#% 316196
#% 318785
#% 334771
#% 346560
#% 387427
#% 429884
#% 495395
#% 522575
#% 522727
#% 731529
#! This paper presents \bic (Border/Interior pixel Classification), a compact and efficient CBIR approach suitable for broad image domains. It has three main components: (1) a simple and powerful image analysis algorithm that classifies image pixels as either border or interior, (2) a new logarithmic distance (dLog) for comparing histograms, and (3) a compact representation for the visual features extracted from images. Experimental results show that the BIC approach is consistently more compact, more efficient and more effective than state-of-the-art CBIR approaches based on sophisticated image analysis algorithms and complex distance functions. It was also observed that the dLog distance function has two main advantages over vectorial distances (e.g., L1): (1) it is able to increase substantially the effectiveness of (several) histogram-based CBIR approaches and, at the same time, (2) it reduces by 50% the space requirement to represent a histogram.

#index 413561
#* Vulnerabilities in similarity search based systems
#@ Ali Saman Tosun;Hakan Ferhatosmanoglu
#t 2002
#c 1
#% 169940
#% 172949
#% 201876
#% 227999
#% 232484
#% 237187
#% 245787
#% 249321
#% 282552
#% 316524
#% 443242
#% 443248
#% 465014
#% 469413
#% 480482
#% 481947
#% 527328
#% 632011
#! Similarity based queries are common in several modern database applications, such as multimedia, scientific, and biomedical databases. In most of these systems, database responds with the tuple with the closest match according to some metric. In this paper we investigate some important security issues related to similarity search in databases. We investigate the vulnerability of such systems against users who try to copy the database by sending automated queries. We analyze two models for similarity search, namely reply model and score model. Reply model responds with the tuple with best match and score model responds with only the score of similarity search. For these models we analyze possible ways of attacks and strategies that can be used to detect attacks. Our analysis shows that in score model it is much easier to plug the vulnerabilities than in reply model. Sophisticated attacks can easily be used in reply model and the database is limited in capability to prevent such attacks.

#index 413562
#* Efficient evaluation of multiple queries on streaming XML data
#@ Mong Li Lee;Boon Chin Chua;Wynne Hsu;Kian-Lee Tan
#t 2002
#c 1
#% 281149
#% 300179
#% 428155
#% 480296
#% 480489
#% 504578
#! Traditionally, XML documents are processed at where they are stored. This allows the query processor to exploit pre-computed data structures (e.g., index) to retrieve the desired data efficiently. However, this mode of processing is not suitable for many applications where the documents are frequently updated. In such situations, efficient evaluation of multiple queries over streaming XML documents becomes important. This paper introduces a new operator, mqX-scan, which efficiently evaluates multiple queries with a single pass on streaming XML data. To facilitate matching, mqX-scan utilizes templates containing paths that have been traversed to match regular path expression patterns in a pool of queries. Results of the experiments demonstrate the efficiency and scalability of the mqX-scan operator.

#index 413563
#* Query processing of streamed XML data
#@ Leonidas Fegaras;David Levine;Sujoe Bose;Vamsi Chaluvadi
#t 2002
#c 1
#% 163444
#% 201897
#% 210214
#% 227883
#% 248799
#% 261741
#% 300143
#% 335725
#% 340635
#% 428155
#% 480663
#% 562135
#! We are addressing the efficient processing of continuous XML streams, in which the server broadcasts XML data to multiple clients concurrently through a multicast data stream, while each client is fully responsible for processing the stream. In our framework, a server may disseminate XML fragments from multiple documents in the same stream, can repeat or replace fragments, and can introduce new fragments or delete invalid ones. A client uses a light-weight database based on our proposed XML algebra to cache stream data and to evaluate XML queries against these data. The synchronization between clients and servers is achieved through annotations and punctuations transmitted along with the data streams. We are presenting a framework for processing XML queries in XQuery form over continuous XML streams. Our framework is based on a novel XML algebra and a new algebraic optimization framework based on query decorrelation, which is essential for non-blocking stream processing.

#index 413564
#* Multi-level operator combination in XML query processing
#@ Shurug Al-Khalifa;H. V. Jagadish
#t 2002
#c 1
#% 58366
#% 300143
#% 333981
#% 397375
#% 458836
#% 464041
#% 562456
#! A core set of efficient access methods is central to the development of any database system. In the context of an XML database, there has been considerable effort devoted to defining a good set of primitive operators and inventing efficient access methods for each individual operator. These primitive operators have been defined either at the macro-level (using a "pattern tree" to specify a selection, for example) or at the micro-level (using multiple explicit containment joins to instantiate a single XPath expression).In this paper we argue that it is valuable to consider operations at each level. We do this through a study of operator merging: the development of a new access method to implement a combination of two or more primitive operators. It is frequently the case that access methods for merged operators are superior to a pipelined execution of separate access methods for each operator. We show operator merging to be valuable at both the micro-level and the macro-level. Furthermore, we show that the corresponding merged operators are hard to reason with at the other level.Specifically, we consider the influence of projections and set operations on pattern-based selections and containment joins. We show, through both analysis and extensive experimentation, the benefits of considering these operations all together. Even though our experimental verification is only with a native XML database, we have reason to believe that our results apply equally to RDBMS-based XML query engines.

#index 413565
#* XMLTM: efficient transaction management for XML documents
#@ Torsten Grabs;Klemens Böhm;Hans-Jörg Schek
#t 2002
#c 1
#% 9241
#% 46607
#% 91076
#% 122917
#% 247424
#% 273922
#% 333979
#% 403195
#% 464071
#% 464705
#% 479204
#% 479465
#% 480152
#% 481955
#% 495278
#% 632058
#% 637818
#% 650962
#! A common approach to storage and retrieval of XML documents is to store them in a database, together with materialized views on their content. The advantage over "native" XML storage managers seems to be that transactions and concurrency are for free, next to other benefits. But a closer look and preliminary experiments reveal that this results in poor performance of concurrent queries and updates. The reason is that database lock contention hinders parallelism unnecessarily. We therefore investigate concurrency control at the semantic, i.e., XML level and describe a respective transaction manager XMLTM. It features a new locking protocol DGLOCK. It generalizes the protocol for locking on directed acyclic graphs by adding simple predicate locking on the content of elements, e.g., on their text. Instead of using the original XML documents, we propose to take advantage of an abstraction of the XML document collection known as DataGuides. XMLTM allows to run XML processing at the underlying database at low ANSI isolation degrees and to release database locks early without sacrificing correctness in this setting. We have built a complete prototype system that is implemented on top of the XML Extender for IBM DB2. Our evaluation shows that our approach consistently yields performance improvements by an order of magnitude. We stress that our approach can also be implemented within a native XML storage manager, and we expect even better performance.

#index 413566
#* Efficient synchronization for mobile XML data
#@ Franky Lam;Nicole Lam;Raymond Wong
#t 2002
#c 1
#% 158911
#% 201897
#% 261741
#% 271199
#% 320088
#% 333979
#% 342446
#% 342713
#% 378393
#% 462227
#% 480296
#% 659995
#! Many handheld applications receive data from a primary database server and operate in an intermittently connected environment these days. They maintain data consistency with data sources through sychronization. In certain applications such as sales force automation, it is highly desirable if updates on the data source can be reflected at the handheld applications immediately. This paper proposes an efficient method to synchronize XML data on multiple mobile devices. Each device retrieves and caches a local copy of data from the database source based on a regular path expression. These local copies may be overlapping or disjoint with each other. An efficient mechanism is proposed to find all the disjoint copies to avoid unnecessary synchronizations. Each update to the data source will then be checked to identify all handheld applications which are affected by the update. Communication costs can be further reduced by eliminating the forwarding of unnecessary operations to groups of mobile clients.

#index 413567
#* An object-oriented extension of XML for autonomous web applications
#@ Hasan M. Jamil;Giovanni A. Modica
#t 2002
#c 1
#% 384872
#% 392332
#% 509715
#! While the idea of extending XML to include object-oriented features has been gaining popularity in general, the potential of inheritance in document design has not been well recognized in contemporary research. In this paper we demonstrate that XML with dynamic inheritance aids better document designs and decreased management overheads and support increased autonomy. As an extended application, we point out that dynamic inheritance also helps effective automated web portal and ontology designs.We present an object-oriented extension to the language of XML to include dynamic inheritance and describe a middle layer that implements our system. We explain our system with several practical examples.

#index 413568
#* Efficient prediction of web accesses on a proxy server
#@ Wenwu Lou;Hongjun Lu
#t 2002
#c 1
#% 83126
#% 211739
#% 271420
#% 281209
#% 314933
#% 342655
#% 390132
#% 443194
#% 458852
#% 963898
#! Web access prediction is an active research topic with many applications. Various approaches have been proposed for Web access prediction in the domain of individual Web servers but they have to be tailored to the domain of proxy servers to satisfy its special requirements in prediction efficiency and scalability. In this paper, the design and implementation of proxy-based prediction service (PPS) is presented. For prediction efficiency, PPS applies a new prediction scheme which employs a two-layer navigation model to capture both inter-site and intra-site access patterns, incorporated with a bottom-up prediction mechanism that exploits reference locality in proxy logs. For system scalability, PPS manages the navigation model in disk database and adopts a predictive cache replacement strategy for data shipping between the model database and cache. We show the superiority of our prediction scheme over existing approaches and validate our model management and caching strategies, with a detailed performance study using real-world data.

#index 413569
#* A self-managing data cache for edge-of-network web applications
#@ Khalil Amiri;Sanghyun Park;Renu Tewari
#t 2002
#c 1
#% 174226
#% 198465
#% 223400
#% 256883
#% 300138
#% 302760
#% 333964
#% 333965
#% 397400
#% 397402
#% 464056
#% 479792
#% 480149
#% 480495
#% 480816
#% 481916
#% 571216
#% 661477
#% 1848711
#! Database caching at proxy servers enables dynamic content to be generated at the edge of the network, thereby improving the scalability and response time of web applications. The scale of deployment of edge servers coupled with the rising costs of their administration demand that such caching middleware be adaptive and self-managing. To achieve this, a cache must be dynamically populated and pruned based on the application query stream and access pattern. In this paper, we describe such a cache which maintains a large number of materialized views of previous query results. Cached "views" share physical storage to avoid redundancy, and are usually added and evicted dynamically to adapt to the current workload and to available resources. These two properties of large scale (large number of cached views) and overlapping storage introduce several challenges to query matching and storage management which are not addressed by traditional approaches. In this paper, we describe an edge data cache architecture with a flexible query matching algorithm and a novel storage management policy which work well in such an environment. We perform an evaluation of a prototype of such an architecture using the TPC-W benchmark and find that it reduces query response times by up to 75%, while reducing network and server load.

#index 413570
#* Cooperative caching by mobile clients in push-based information systems
#@ Takahiro Hara
#t 2002
#c 1
#% 32884
#% 66172
#% 124011
#% 171449
#% 172874
#% 172875
#% 201897
#% 235857
#% 245014
#% 248894
#% 259632
#% 259642
#% 269631
#% 274200
#% 299930
#% 309465
#% 310282
#% 316486
#% 345722
#% 408638
#% 461919
#% 464214
#% 554751
#% 622728
#% 628027
#% 632025
#% 660942
#% 720827
#% 1848956
#! Recent advances in computer and wireless communication technologies have increased interest in push-based information systems in which a server repeatedly broadcasts data to clients through a broadband channel. In this paper, assuming an environment where clients in push-based information systems construct ad hoc networks, we propose three caching strategies in which clients cooperatively cache broadcast data items. These strategies shorten the average response time for data access by replacing cached items based on their access frequencies, the network topology, and the time remaining until each item is broadcast next. We also show the results of simulation experiments conducted to evaluate the performance of our proposed strategies.

#index 413571
#* AuGEAS: authoritativeness grading, estimation, and sorting
#@ Ayman Farahat;Geoff Nunberg;Francine Chen
#t 2002
#c 1
#% 136350
#% 262061
#% 268079
#% 282905
#% 341964
#% 387427
#% 420508
#% 740900
#% 1272369
#! When searching for content in in a large heterogeneous document collections like the World Wide Web it is not easy to know which documents provide reliable authoritative information about a subject. The problem is particularly pointed as it concerns content search for "high-value" informational needs such as retrieving medical information, where the cost of error may be high. In this paper, a method is described for estimating the authoritativeness of a document based on textual, non-topical cues. This method is complementary to estimates of authoritativeness based on link structure, such as the PageRank and HITS algorithms. This method is particularly suited to "high-value" content search where the user is interested in searching for information about a specific topic. A method for combining textual estimates of authoritativeness with link analysis is also presented. The types of textual cues to authoritativeness that are easily computed and utilized by our method are described, as well as the method used to select a subset of cues to increase the computation speed. Methods for applying authoritativeness estimates to re-ranking documents returned from search engines, combining textual authoritativeness with social authority, and use in query expansion are also presented. By combining textual authority with link analysis, a more complete and robust estimate can be made of a document's authoritativeness.

#index 413572
#* Structural extraction from visual layout of documents
#@ Binyamin Rosenfeld;Ronen Feldman;Yonatan Aumann
#t 2002
#c 1
#% 197240
#% 310518
#% 410276
#! Most information extraction systems focus on the textual content of the documents. They treat documents as sequences or of words, disregarding the physical and typographical layout of the information.. While this strategy helps in focusing the extraction process on the key semantic content of the document, much valuable information can also be derived form the document physical appearance. Often, fonts, physical positioning and other graphical characteristics are used to provide additional context to the information. This information is lost with pure-text analysis. In this paper we describe a general procedure for structural extraction, which allows for automatic extraction of entities from the document based on their visual characteristics and relative position in the document layout. Our structural extraction procedure is a learning algorithm, which knows how to automatically generalizes from examples. The procedure is a general one, applicable to any document format with visual and typographical information. We also then describe a specific implementation of the procedure to PDF documents, called PES (PDF Extraction System). PES works with PDF documents and is able to extract such fields such as Author(s), Title, Date, etc. with very high accuracy.

#index 413573
#* Topic-based document segmentation with probabilistic latent semantic analysis
#@ Thorsten Brants;Francine Chen;Ioannis Tsochantaridis
#t 2002
#c 1
#% 144012
#% 278106
#% 280819
#% 425011
#% 448786
#% 742204
#% 748482
#% 786511
#% 853847
#! This paper presents a new method for topic-based document segmentation, i.e., the identification of boundaries between parts of a document that bear on different topics. The method combines the use of the Probabilistic Latent Semantic Analysis (PLSA) model with the method of selecting segmentation points based on the similarity values between pairs of adjacent blocks. The use of PLSA allows for a better representation of sparse information in a text block, such as a sentence or a sequence of sentences. Furthermore, segmentation performance is improved by combining different instantiations of the same model, either using different random initializations or different numbers of latent classes. Results on commonly available data sets are significantly better than those of other state-of-the-art systems.

#index 413574
#* How to improve the pruning ability of dynamic metric access methods
#@ Caetano Traina, Jr.;Agma Traina;Roberto Santos Filho;Christos Faloutsos
#t 2002
#c 1
#% 227937
#% 227939
#% 252304
#% 281750
#% 300160
#% 322309
#% 435141
#% 465160
#% 479462
#% 481460
#% 481956
#% 631963
#! Complex data retrieval is accelerated using index structures, which organize the data in order to prune comparisons between data during queries. In metric spaces, comparison operations can be specially expensive, so the pruning ability of indexing methods turns out to be specially meaningful. This paper shows how to measure the pruning power of metric access methods, and defines a new measurement, called "prunability," which indicates how well a pruning technique carries out the task of cutting down distance calculations at each tree level. It also presents a new dynamic access method, aiming to minimize the number of distance calculations required to answer similarity queries. We show that this novel structure is up to 3 times faster and requires less than 25% distance calculations to answer similarity queries, as compared to existing methods. This gain in performance is achieved by taking advantage of a set of global representatives. Although our technique uses multiple representatives, the index structure still remains dynamic and balanced.

#index 413575
#* On the efficient evaluation of relaxed queries in biological databases
#@ Yangjun Chen;Duren Che;Karl Aberer
#t 2002
#c 1
#% 112698
#% 115467
#% 143306
#% 179696
#% 219024
#% 223368
#% 252608
#% 262067
#% 262084
#% 288578
#% 289010
#% 313719
#% 408638
#% 459007
#% 468476
#% 559183
#% 584883
#% 835096
#! In this paper, a new technique is developed to support the query relaxation in biological databases. Query relaxation is required due to the fact that queries tend not to be expressed exactly by the users, especially in scientific databases such as biological databases, in which complex domain knowledge is heavily involved. To treat this problem, we propose the concept of the so-called fuzzy equivalence classes to capture important kinds of domain knowledge that is used to relax queries. This concept is further integrated with the canonical techniques for pattern searching such as the position tree and automaton theory. As a result, fuzzy queries produced through relaxation can be efficiently evaluated. This method has been successfully utilized in a practical biological database - the GPCRDB.

#index 413576
#* Similarity based retrieval from sequence databases using automata as queries
#@ A. Prasad Sistla;Tao Hu;Vikas Chowdhry
#t 2002
#c 1
#% 120649
#% 172949
#% 201924
#% 227856
#% 227857
#% 245788
#% 273919
#% 334038
#% 443195
#% 460862
#% 461912
#% 463903
#% 479971
#% 480938
#% 481609
#% 481611
#! Similarity based retrieval from sequence databases is of importance in many applications such as time-series, video and textual databases. In this paper, automata based formalisms are introduced for specifying queries over such databases. Various measures defining the distance of a database sequence from an automaton are defined. Efficient methods for similarity based retrieval are presented for each of the distance measures. These methods answer nearest neighbor queries (i.e. retrieval of k closest subsequences), or range queries (i.e., retrieval of all sequences with in a given distance).

#index 413577
#* Detecting similar documents using salient terms
#@ James W. Cooper;Anni R. Coden;Eric W. Brown
#t 2002
#c 1
#% 237340
#% 255137
#% 595987
#% 607942
#% 768305
#! We describe a system for rapidly determining document similarity among a set of documents obtained from an information retrieval (IR) system. We obtain a ranked list of the most important terms in each document using a rapid phrase recognizer system. We store these in a database and compute document similarity using a simple database query. If the number of terms found to not be contained in both documents is less than some predetermined threshold compared to the total number of terms in the document, these documents are determined to be very similar. We compare this to the shingles approach.

#index 413578
#* The role of variance in term weighting for probabilistic information retrieval
#@ Warren R. Greiff;William T. Morgan;Jay M. Ponte
#t 2002
#c 1
#% 118756
#% 169780
#% 169781
#% 184490
#% 262096
#% 280850
#% 280851
#% 287253
#% 288166
#! In probabilistic approaches to information retrieval, the occurrence of a query term in a document contributes to the probability that the document will be judged relevant. It is typically assumed that the weight assigned to a query term should be based on the expected value of that contribution. In this paper we show that the degree to which observable document features such as term frequencies are expected to vary is also important. By means of stochastic simulation, we show that increased variance results in degraded retrieval performance. We further show that by decreasing term weights in the presence of variance, this degradation can be reduced. Hence, probabilistic models of information retrieval must take into account not only the expected value of a query term's contribution but also the variance of document features.

#index 413579
#* Inferring query models by computing information flow
#@ P. D. Bruza;D. Song
#t 2002
#c 1
#% 144029
#% 229348
#% 240212
#% 280819
#% 298183
#% 340899
#% 340901
#% 340946
#% 349280
#% 353872
#! The language modelling approach to information retrieval can also be used to compute query models. A query model can be envisaged as an expansion of an initial query. The more prominent query models in the literature have a probabilistic basis. This paper introduces an alternative, non-probabilistic approach to query modelling whereby the strength of information flow is computed between a query Q and a term w. Information flow is a reflection of how strongly w is informationally contained within the query Q. The information flow model is based on Hyperspace Analogue to Language (HAL) vector representations, which reflects the lexical co-occurrence information of terms. Research from cognitive science has demonstrated the cognitive compatibility of HAL representations with human processing. Query models computed from TREC queries by HAL-based information flow are compared experimentally with two probabilistic query language models. Experimental results are provided showing the HAL-based information flow model be superior to query models computed via Markov chains, and seems to be as effective as a probabilistically motivated relevance model.

#index 413580
#* Logical and physical support for heterogeneous data
#@ Sihem Amer-Yahia;Mary Fernández;Rick Greer;Divesh Srivastava
#t 2002
#c 1
#% 69094
#% 236416
#% 273943
#% 309851
#% 368248
#% 391981
#% 464720
#% 464988
#% 465006
#% 479956
#% 480821
#% 480826
#% 504573
#! Heterogeneity arises naturally in virtually all real-world data. This paper presents evolutionary extensions to a relational database system for supporting three classes of data heterogeneity: variational, structural and annotational heterogeneities. We define these classes and show the impact of these new features on data storage, data-access mechanisms, and the data-description language. Since XML is an important source of heterogeneity, we describe how the system automatically utilizes these new features when storing XML documents.

#index 413581
#* NeT & CoT: translating relational schemas to XML schemas using semantic constraints
#@ Dongwon Lee;Murali Mani;Frank Chiu;Wesley W. Chu
#t 2002
#c 1
#% 3808
#% 10245
#% 169810
#% 299943
#% 300157
#% 309851
#% 325754
#% 395135
#% 397344
#% 416030
#% 479956
#% 533902
#% 536375
#% 665626
#! Two algorithms, called NeT and CoT, to translate relational schemas to XML schemas using various semantic constraints are presented. The XML schema representation we use is a language-independent formalism named XSchema, that is both precise and concise. A given XSchema can be mapped to a schema in any of the existing XML schema language proposals. Our proposed algorithms have the following characteristics: (1) NeT derives a nested structure from a flat relational model by repeatedly applying the nest operator on each table so that the resulting XML schema becomes hierarchical, and (2) CoT considers not only the structure of relational schemas, but also semantic constraints such as inclusion dependencies during the translation. It takes as input a relational schema where multiple tables are interconnected through inclusion dependencies and converts it into a good XSchema. To validate our proposals, we present experimental results using both real schemas from the UCI repository and synthetic schemas from TPC-H.

#index 413582
#* XClust: clustering XML schemas for effective integration
#@ Mong Li Lee;Liang Huai Yang;Wynne Hsu;Xia Yang
#t 2002
#c 1
#% 55294
#% 227992
#% 229827
#% 248809
#% 273911
#% 333990
#% 334025
#% 443235
#% 443408
#% 463918
#% 464720
#% 479465
#% 479783
#% 480645
#% 481923
#% 504573
#% 504578
#% 571297
#! It is increasingly important to develop scalable integration techniques for the growing number of XML data sources. A practical starting point for the integration of large numbers of Document Type Definitions (DTDs) of XML sources would be to first find clusters of DTDs that are similar in structure and semantics. Reconciling similar DTDs within such a cluster will be an easier task than reconciling DTDs that are different in structure and semantics as the latter would involve more restructuring. We introduce XClust, a novel integration strategy that involves the clustering of DTDs. A matching algorithm based on the semantics, immediate descendents and leaf-context similarity of DTD elements is developed. Our experiments to integrate real world DTDs demonstrate the effectiveness of the XClust approach.

#index 413583
#* A local search mechanism for peer-to-peer networks
#@ Vana Kalogeraki;Dimitrios Gunopulos;D. Zeinalipour-Yazti
#t 2002
#c 1
#% 262063
#% 280853
#% 309133
#% 309841
#% 330704
#% 330706
#% 340175
#% 347646
#% 480647
#% 481748
#% 636008
#% 636009
#% 729437
#% 1650569
#! One important problem in peer-to-peer (P2P) networks is searching and retrieving the correct information. However, existing searching mechanisms in pure peer-to-peer networks are inefficient due to the decentralized nature of such networks. We propose two mechanisms for information retrieval in pure peer-to-peer networks. The first, the modified Breadth-First Search (BFS) mechanism, is an extension of the current Gnuttela protocol, allows searching with keywords, and is designed to minimize the number of messages that are needed to search the network. The second, the Intelligent Search mechanism, uses the past behavior of the P2P network to further improve the scalability of the search procedure. In this algorithm, each peer autonomously decides which of its peers are most likely to answer a given query. The algorithm is entirely distributed, and therefore scales well with the size of the network. We implemented our mechanisms as middleware platforms. To show the advantages of our mechanisms we present experimental results using the middleware implementation.

#index 413584
#* Intelligent knowledge discovery in peer-to-peer file sharing
#@ Yugyung Lee;Changgyu Oh;Eun Kyo Park
#t 2002
#c 1
#% 232640
#% 287215
#% 287237
#% 303701
#% 340176
#% 433981
#% 433982
#% 449396
#% 496291
#% 516342
#% 662110
#! Emerging peer-to-peer computing provides new possibilities but also challenges for distributed applications. Despite their significant potential, current peer-to-peer networks lack efficient knowledge discovery and management. This paper addresses this deficiency and proposes the Intelligent File Sharing framework, which provides an effective and flexible query for P2P file sharing. The IFS is based on powerful schema and flexible inference, as well as efficiently integrated and extensible retrieval algorithms. Experimental results have provided evidence of the high performance and scalability of the Intelligent File Sharing (IFS) system in peer-to-peer environments.

#index 413585
#* Partial rollback in object-oriented/object-relational database management systems
#@ Won-Young Kim;Kyu-Young Whang;Byung Suk Lee;Young-Koo Lee;Ji-Woong Chang
#t 2002
#c 1
#% 9241
#% 114582
#% 116203
#% 171457
#% 235017
#% 317988
#% 403195
#% 411700
#% 511489
#% 555031
#! In a database management system (DBMS), partial rollback is an important mechanism for canceling only part of the operations executed in a transaction back to a savepoint. Partial rollback complicates buffer management because it should restore the state of the buffers as well as that of the database. Several relational DBMSs (RDBMSs) currently provide this mechanism using page buffers. However, object-oriented or object-relational DBMSs (OO/ORDBMSs) cannot utilize the partial rollback scheme of RDBMSs as is because, unlike RDBMSs, many of them use a dual buffer consisting of an object buffer and a page buffer. In this paper, we propose a thorough study of partial rollback schemes of OO/ORDBMSs with a dual buffer. First, we classify the partial rollback schemes of OO/ORDBMSs into a single buffer-based scheme and a dual buffer-based scheme by the number of buffers used to process rollback. Next, we propose four alternative partial rollback schemes: a page buffer-based scheme, an object buffer-based scheme, a dual buffer-based scheme using a soft log, and a dual buffer-based scheme using shadows. We then evaluate their performance through simulations. The results show that the dual buffer-based partial rollback scheme using shadows provides the best performance. Partial rollback in OO/ORDBMS has not been addressed in the literature; yet, it is a useful mechanism that must be implemented. The proposed schemes are practical ones that can be implemented in such DBMSs.

#index 413586
#* Query association for effective retrieval
#@ Falk Scholer;Hugh E. Williams
#t 2002
#c 1
#% 71752
#% 120099
#% 169806
#% 169809
#% 184486
#% 194252
#% 194299
#% 201992
#% 232713
#% 262036
#% 262102
#% 284931
#% 290703
#% 309116
#% 323131
#% 328532
#% 342961
#% 344926
#% 344930
#% 397138
#! We introduce a novel technique for document summarisation which we call query association. Query association is based on the notion that a query that is highly similar to a document is a good descriptor of that document. For example, the user query "richmond football club" is likely to be a good summary of the content of a document that is ranked highly in response to the query. We describe this process of defining, maintaining, and presenting the relationship between a user query and the documents that are retrieved in response to that query. We show that associated queries are an excellent technique for describing a document: for relevance judgement, associated queries are as effective as a simple online query-biased summarisation technique. As future work, we suggest additional uses for query association including relevance feedback and query expansion.

#index 413587
#* Pruning long documents for distributed information retrieval
#@ Jie Lu;Jamie Callan
#t 2002
#c 1
#% 194246
#% 340146
#% 340882
#% 340887
#% 340916
#% 342679
#% 397125
#% 567255
#! Query-based sampling is a method of discovering the contents of a text database by submitting queries to a search engine and observing the documents returned. In prior research sampled documents were used to build resource descriptions for automatic database selection, and to build a centralized sample database for query expansion and result merging. An unstated assumption was that the associated storage costs were acceptable.When sampled documents are long, storage costs can be large. This paper investigates methods of pruning long documents to reduce storage costs. The experimental results demonstrate that building resource descriptions and centralized sample databases from the pruned contents of sampled documents can reduce storage costs by 54-93% while causing only minor losses in the accuracy of distributed information retrieval.

#index 413588
#* On arabic search: improving the retrieval effectiveness via a light stemming approach
#@ Mohammed Aljlayl;Ophir Frieder
#t 2002
#c 1
#% 68997
#% 144034
#% 144074
#% 208934
#% 218985
#% 241238
#% 270945
#% 1271427
#! The inflectional structure of a word impacts the retrieval accuracy of information retrieval systems of Latin-based languages. We present two stemming algorithms for Arabic information retrieval systems. We empirically investigate the effectiveness of surface-based retrieval. This approach degrades retrieval precision since Arabic is a highly inflected language. Accordingly, we propose root-based retrieval. We notice a statistically significant improvement over the surface-based approach. Many variant word senses are based on an identical root; thus, the root-based algorithm creates invalid conflation classes that result in an ambiguous query which degrades the performance by adding extraneous terms. To resolve ambiguity, we propose a novel light-stemming algorithm for Arabic texts. This automatic rule-based stemming algorithm is not as aggressive as the root extraction algorithm. We show that the light stemming algorithm significantly outperforms the root-based algorithm. We also show that a significant improvement in retrieval precision can be achieved with light inflectional analysis of Arabic words.

#index 413589
#* Boosting to correct inductive bias in text classification
#@ Yan Liu;Yiming Yang;Jaime Carbonell
#t 2002
#c 1
#% 136350
#% 169719
#% 194284
#% 209021
#% 262085
#% 269217
#% 280817
#% 311034
#% 318412
#% 375017
#% 413637
#% 424997
#% 445319
#% 458379
#% 465754
#% 466263
#% 520224
#% 604688
#% 840583
#% 1499573
#! This paper studies the effects of boosting in the context of different classification methods for text categorization, including Decision Trees, Naive Bayes, Support Vector Machines (SVMs) and a Rocchio-style classifier. We identify the inductive biases of each classifier and explore how boosting, as an error-driven resampling mechanism, reacts to those biases. Our experiments on the Reuters-21578 benchmark show that boosting is not effective in improving the performance of the base classifiers on common categories. However, the effect of boosting for rare categories varies across classifiers: for SVMs and Decision Trees, we achieved a 13-17% performance improvement in macro-averaged F1 measure, but did not obtain substantial improvement for the other two classifiers. This interesting finding of boosting on rare categories has not been reported before.

#index 413590
#* Using conjunction of attribute values for classification
#@ Mukund Deshpande;George Karypis
#t 2002
#c 1
#% 73374
#% 136350
#% 152934
#% 269217
#% 280488
#% 300120
#% 310494
#% 310507
#% 332074
#% 376266
#% 442814
#% 443350
#% 466483
#% 466653
#% 677030
#! Advances in the efficient discovery of frequent itemsets have led to the development of a number of schemes that use frequent itemsets to aid developing accurate and efficient classifiers. These approaches use the frequent itemsets to generate a set of composite features that expand the dimensionality of the underlying dataset. In this paper, we build upon this work and (i) present a variety of schemes for composite feature selection that achieve a substantial reduction in the number of features without adversely affecting the accuracy gains, and (ii) show (both analytically and experimentally) that the composite features can lead to improved classification models even in the context of support vector machines, in which the dimensionality can automatically be expanded by the use of appropriate kernel functions.

#index 413591
#* Categorizing information objects from user access patterns
#@ Mao Chen;Andrea LaPaugh;Jaswinder Pal Singh
#t 2002
#c 1
#% 209662
#% 220709
#% 232912
#% 234992
#% 260779
#% 262061
#% 268087
#% 281155
#% 281209
#% 290830
#% 309101
#% 309104
#% 309112
#% 309141
#% 309142
#% 309783
#% 330700
#% 330708
#% 340894
#% 340903
#% 397131
#% 433773
#% 609438
#! Many web sites have dynamic information objects whose topics change over time. Classifying these objects automatically and promptly is a challenging and important problem for site masters. Traditional content-based and link structure based classification techniques have intrinsic limitations for this task. This paper proposes a framework to classify an object into an existing category structure by analyzing the users' traversals in the category structure. The key idea is to infer an object's topic from the predicted preferences of users when they access the object. We compare two approaches using this idea. One analyzes collective user behavior and the other each user's accesses. We present experimental results on actual data that demonstrate a much higher prediction accuracy and applicability with the latter approach. We also analyze the correlation between classification quality and various factors such as the number of users accessing the object. To our knowledge, this work is the first effort in combining object classification with user access prediction.

#index 413592
#* Passage retrieval based on language models
#@ Xiaoyong Liu;W. Bruce Croft
#t 2002
#c 1
#% 111456
#% 144011
#% 144012
#% 169809
#% 184491
#% 194298
#% 197837
#% 232677
#% 262096
#% 280850
#% 280851
#% 280864
#% 328532
#% 340899
#% 340901
#% 340948
#% 397145
#% 504890
#% 674957
#! Previous research has shown that passage-level evidence can bring added benefits to document retrieval when documents are long or span different subject areas. Recent developments in language modeling approach to IR provided a new effective alternative to traditional retrieval models. These two streams of research motivate us to examine the use of passages in a language model framework. This paper reports on experiments using passages in a simple language model and a relevance model, and compares the results with document-based retrieval. Results from the INQUERY search engine, which is not based on a language modeling approach, are also given for comparison. Test data include two heterogeneous and one homogeneous document collections. Our experiments show that passage retrieval is feasible in the language modeling context, and more importantly, it can provide more reliable performance than retrieval based on full documents.

#index 413593
#* Capturing term dependencies using a language model based on sentence trees
#@ Ramesh Nallapati;James Allan
#t 2002
#c 1
#% 70370
#% 86371
#% 86535
#% 109190
#% 169784
#% 228088
#% 262096
#% 279755
#% 287253
#% 375017
#% 575574
#% 575579
#% 995518
#! We describe a new probabilistic Sentence Tree Language Modeling approach that captures term dependency patterns in Topic Detection and Tracking's (TDT) Story Link Detection task. New features of the approach include modeling the syntactic structure of sentences in documents by a sentence-bin approach and a computationally efficient algorithm for capturing the most significant sentence-level term dependencies using a Maximum Spanning Tree approach, similar to Van Rijsbergen's modeling of document-level term dependencies.The new model is a good discriminator of on-topic and off-topic story pairs providing evidence that sentence-level term dependencies contain significant information about relevance. Although runs on a subset of the TDT2 corpus show that the model is outperformed by the unigram language model, a mixture of the unigram and the Sentence Tree models is shown to improve on the best performance especially in the regions of low false alarms.

#index 413594
#* A language modeling framework for resource selection and results merging
#@ Luo Si;Rong Jin;Jamie Callan;Paul Ogilvie
#t 2002
#c 1
#% 172898
#% 184489
#% 194246
#% 227891
#% 262096
#% 267454
#% 280850
#% 280853
#% 280856
#% 287253
#% 316534
#% 340146
#% 340948
#% 397125
#% 481748
#! Statistical language models have been proposed recently for several information retrieval tasks, including the resource selection task in distributed information retrieval. This paper extends the language modeling approach to integrate resource selection, ad-hoc searching, and merging of results from different text databases into a single probabilistic retrieval model. This new approach is designed primarily for Intranet environments, where it is reasonable to assume that resource providers are relatively homogeneous and can adopt the same kind of search engine. Experiments demonstrate that this new, integrated approach is at least as effective as the prior state-of-the-art in distributed IR.

#index 413595
#* An efficient and effective algorithm for density biased sampling
#@ Alexandros Nanopoulos;Yannis Manolopoulos;Yannis Theodoridis
#t 2002
#c 1
#% 1331
#% 77967
#% 86950
#% 137887
#% 164368
#% 213975
#% 235939
#% 248790
#% 300132
#% 333933
#% 333975
#% 420079
#% 427199
#% 463577
#% 477657
#% 479962
#% 480812
#% 480953
#% 481779
#% 527022
#% 614619
#% 721137
#! In this paper we describe a new density-biased sampling algorithm. It exploits spatial indexes and the local density information they preserve, to provide improved quality of sampling result and fast access to elements of the dataset. It attains improved sampling quality, with respect to factors like skew, noise or dimensionality. Moreover, it has the advantage of efficiently handling dynamic updates, and it requires low execution times. The performance of the proposed method is examined experimentally. The comparative results illustrate its superiority over existing methods.

#index 413596
#* "GeoPlot": spatial data mining on video libraries
#@ Jia-Yu Pan;Christos Faloutsos
#t 2002
#c 1
#% 152934
#% 218982
#% 283833
#% 342609
#% 376266
#% 438054
#% 481620
#! Are "tornado" touchdowns related to "earthquakes"? How about to "floods", or to "hurricanes"? In Informedia [14], using a gazetteer on news video clips, we map news onto points on the globe and find correlations between sets of points. In this paper we show how to find answers to such questions, and how to look for patterns on the geo-spatial relationships of news events. The proposed tool is "GeoPlot", which is fast to compute and gives a lot of useful information which traditional text retrieval can not find.We describe our experiments on 2-year worth of video data (~ 20 Gbytes). There we found that GeoPlot can find unexpected correlations that text retrieval would never find, such as those between "earthquake" and "volcano", and "tourism" and "wine".In addition, GeoPlot provides a good visualization of a data set's characteristics. Characteristics at all scales are shown in one plot and a wealth of information is given, for example, geo-spatial clusters, characteristic scales, and intrinsic (fractal) dimensions of the events' locations.

#index 413597
#* Trajectory queries and octagons in moving object databases
#@ Hongjun Zhu;Jianwen Su;Oscar H. Ibarra
#t 2002
#c 1
#% 2115
#% 12189
#% 316965
#% 333940
#% 443181
#% 458857
#% 462617
#% 463425
#% 464195
#% 479645
#% 480473
#% 480817
#% 502775
#% 527166
#% 527198
#% 565447
#% 617878
#% 617892
#! An important class of queries in moving object databases involves trajectories. We propose to divide trajectory predicates into topological and non-topological parts; extend the 9 intersection model of Egenhofer-Franzosa to a 3-step evaluation strategy for trajectory queries: a filter step, a refinement step, and a tracing step.The filter and refinement steps are similar to region searches. As in spatial databases, approximations of trajectories are typically used in evaluating trajectory queries. In earlier studies, minimum bounding boxes (mbrs) are used to approximate trajectory segments which allow index structures to be built, e.g., TB-trees and R*-trees. The use of mbrs hinders the efficiency since mbrs are very coarse approximations especially for trajectory segments. To overcome this problem, we propose a new type of approximations, "minimum bounding octagon prism" mbop. We extend R*-tree to a new index structure "Octagon-Prism tree" (OP-tree) for mbops of trajectory segments. We conducted experiments to evaluate efficiency of OP-trees in performing region searches and trajectory queries. The results show that OP-trees improve region searches significantly over synthetic trajectory data sets to TB-trees and R*-trees and can significantly reduce the evaluation cost of trajectory queries compared to TB-trees.

#index 413598
#* The effectiveness study of various music information retrieval approaches
#@ Jia-Lien Hsu;Arbee L. P. Chen;Hung-Chen Chen;Ning-Han Liu
#t 2002
#c 1
#% 115462
#% 133892
#% 194192
#% 235941
#% 261882
#% 261908
#% 280845
#% 286744
#% 289010
#% 309101
#% 394709
#% 406493
#% 422958
#% 582014
#% 614622
#% 641568
#% 1180235
#! In this paper, we describe the Ultima project which aims to construct a platform for evaluating various approaches of music information retrieval. Two kinds of approaches are adopted in this project. These approaches differ in various aspects, such as representations of music objects, index structures, and approximate query processing strategies. For a fair comparison, we propose a measurement of the retrieval effectiveness by recall-precision curves with a scaling factor adjustment. Finally, the performance study of the retrieval effectiveness based on various factors of these approaches is presented.

#index 413599
#* Harmonic models for polyphonic music retrieval
#@ Jeremy Pickens;Tim Crawford
#t 2002
#c 1
#% 245521
#% 279755
#% 300542
#% 340997
#% 641568
#! Most work in the ad hoc music retrieval field has focused on the retrieval of monophonic documents using monophonic queries. Polyphony adds considerably more complexity. We present a method by which polyphonic music documents may be retrieved by polyphonic music queries. A new harmonic description technique is given, wherein the information from all chords, rather than the most significant chord, is used. This description is then combined in a new and unique way with Markov statistical methods to create models of both documents and queries. Document models are compared to query models and then ranked by score. Though test collections for music are currently scarce, we give the first known recall-precision graphs for polyphonic music retrieval, and results are favorable.

#index 413600
#* A singer identification technique for content-based classification of MP3 music objects
#@ Chih-Chin Liu;Chuan-Sung Huang
#t 2002
#c 1
#% 80995
#% 194192
#% 286747
#% 316259
#% 342720
#% 434710
#% 434753
#% 586835
#% 614622
#% 624464
#% 631989
#% 636391
#% 641568
#% 968560
#% 970034
#% 970586
#% 971098
#% 1180235
#% 1775124
#% 1775138
#! As there is a growing amount of MP3 music data available on the Internet today, the problems related to music classification and content-based music retrieval are getting more attention recently. In this paper, we propose an approach to automatically classify MP3 music objects according to their singers. First, the coefficients extracted from the output of the polyphase filters are used to compute the MP3 features for segmentation. Based on these features, an MP3 music object can be decomposed into a sequence of notes (or phonemes). Then for each MP3 phoneme in the training set, its MP3 feature is extracted and used to train an MP3 classifier which can identify the singer of an unknown MP3 music object. Experiments are performed and analyzed to show the effectiveness of the proposed method.

#index 413601
#* XKvalidator: a constraint validator for XML
#@ Yi Chen;Susan B. Davidson;Yifeng Zheng
#t 2002
#c 1
#% 330627
#% 404772
#% 479956
#% 480489
#! The role of XML in data exchange is evolving from one of merely conveying the structure of data to one that also conveys its semantics. In particular, several proposals for key and foreign key constraints have recently appeared, and aspects of these proposals have been adopted within XMLSchema.In this paper, we examine the problem of checking keys and foreign keys in XML documents using a validator based on SAX. The algorithm relies on an indexing technique based on the paths found in key definitions, and can be used for checking the correctness of an entire document (bulk checking) as well as for checking updates as they are made to the document (incremental checking). The asymptotic performance of the algorithm is linear in the size of the document or update. Furthermore, experimental results demonstrate reasonable performance.

#index 413602
#* Discovering approximate keys in XML data
#@ Gösta Grahne;Jianfei Zhu
#t 2002
#c 1
#% 129570
#% 189872
#% 262071
#% 289335
#% 299943
#% 332151
#% 333855
#% 378395
#% 384978
#% 397349
#% 464837
#% 481290
#% 562455
#! Keys are very important in many aspects of data management, such as guiding query formulation, query optimization, indexing, etc. We consider the situation where an XML document does not come with key definitions, and we are interested in using data mining techniques to obtain a representation of the keys holding in a document. In order to have a compact representation of the set of keys holding in a document, we define a partial order on the set of all key expressions. This order is based on an analysis of the properties of absolute and relative keys for XML. Given the existence of the partial order, only a reduced set of key expressions need to be discovered.Due to the semistructured nature of XML documents, it turns out to be useful to consider keys that hold in "almost" the whole document, that is, they are violated only in a small part of the document. To this end, the support and confidence of a key expression are also defined, and the concept of approximate key expression is introduced. We give an efficient algorithm to mine a reduced set of approximate keys from an XML document.

#index 413603
#* Information retrieval on the semantic web
#@ Urvi Shah;Tim Finin;Anupam Joshi;R. Scott Cost;James Matfield
#t 2002
#c 1
#% 281149
#% 281189
#% 330616
#% 388253
#% 464825
#% 566448
#! We describe an approach to retrieval of documents that contain of both free text and semantically enriched markup. In particular, we present the design and implementation prototype of a framework in which both documents and queries can be marked up with statements in the DAML+OIL semantic web language. These statements provide both structured and semi-structured information about the documents and their content. We claim that indexing text and semantic markup together will significantly improve retrieval performance. Our approach allows inferencing to be done over this information at several points: when a document is indexed, when a query is processed and when query results are evaluated.

#index 413604
#* RHist: adaptive summarization over continuous data streams
#@ Lin Qiao;Divyakant Agrawal;Amr El Abbadi
#t 2002
#c 1
#% 172902
#% 210190
#% 242366
#% 248822
#% 273901
#% 273907
#% 299982
#% 333926
#% 333931
#% 333947
#% 333983
#% 379445
#% 397354
#% 427219
#% 479648
#% 480471
#% 480628
#% 482123
#% 659922
#% 660003
#% 927337
#% 993969
#! Maintaining approximate aggregates and summaries over data streams is crucial to handle the OLAP query workload that arises in applications, such as network monitoring and telecommunications. Furthermore, since the entire data is not available at all times the maintenance task must be done incrementally. We show that R(elaxed)Hist(ogram) is an appropriate summarization under data stream scenario. In order to reduce query estimation errors, we propose adaptive approaches which not only capture the data distribution, but also integrate independent query patterns. We introduce a workload decay model to efficiently capture global workload information and ensure that the query patterns from the recent past are weighted more than queries that are further in the past. We verify experimentally that our approach successfully adapts to continuously changing workload as well as data streams.

#index 413605
#* Efficient query monitoring using adaptive multiple key hashing
#@ Kun-Lung Wu;Philip S. Yu
#t 2002
#c 1
#% 68091
#% 86945
#% 206915
#% 252608
#% 271199
#% 297191
#% 300179
#% 307470
#% 333938
#% 443298
#% 480296
#% 511917
#% 631962
#! Monitoring continual queries or subscriptions is to determine the subset of all queries or subscriptions whose predicates match a given event. Predicates contain not only equality but also non-equality clauses. Event matching is usually accomplished by first identifying a "small" candidate set of subscriptions for an event and then determining the matched subscriptions from the candidate set. Prior work has focused on using equality clauses to identify the candidate set. However, we found that completely ignoring non-equality clauses can result in a much larger candidate set. In this paper, we present and evaluate an adaptive multiple key hashing (AMKH) method to judiciously include an effective subset of non-equality clauses in candidate set identification. Each subscription is mapped to a data point in a multidimensional space based on its predicate clauses. AMKH is then used to maintain subscriptions and perform event matching. AMKH further provides a controlling mechanism to limit the hash range of a non-equality clause, hence reducing the size of the candidate set. Simulations are conducted to study the performance of AMKH. The results show that (1) a small number of non-equality clauses can be effectively included by AMKH and (2) the attributes whose overall non-equality predicate clauses are most selective should be chosen for inclusion by AMKH.

#index 413606
#* Evaluating continuous nearest neighbor queries for streaming time series via pre-fetching
#@ Like Gao;Zhengrong Yao;X. Sean Wang
#t 2002
#c 1
#% 116082
#% 172949
#% 201876
#% 211438
#% 227857
#% 248797
#% 248831
#% 260004
#% 300179
#% 333926
#% 333941
#% 397380
#% 427199
#% 428155
#% 443298
#% 460862
#% 462500
#% 481947
#% 481956
#% 631923
#% 662180
#! For many applications, it is important to quickly locate the nearest neighbor of a given time series. When the given time series is a streaming one, nearest neighbors may need to be found continuously at all time positions. Such a standing request is called a continuous nearest neighbor query. This paper seeks fast evaluation of continuous queries on large databases. The initial strategy is to use the result of one evaluation to restrict the search space for the next. A more fundamental idea is to extend the existing indexing methods, used in many traditional nearest neighbor algorithms, with pre-fetching. Specifically, pre-fetching is to predict the next value of the stream before it arrives, and to process the query as if the predicted value were the real one in order to load the needed index pages and time series into the allocated cache memory. Furthermore, if the pre-fetched candidates cannot fit into the cache memory, they are stored in a sequential file to facilitate fast access to them. Experiments show that pre-fetching improves the response time greatly over the direct use of traditional algorithms, even if the caching provided by the operating system is taken into consideration.

#index 413607
#* Mining temporal classes from time series data
#@ Masahiro Motoyoshi;Takao Miura;Kohei Watanabe
#t 2002
#c 1
#% 18615
#% 43028
#% 64150
#% 140389
#% 153286
#% 232102
#% 319244
#% 481290
#% 618546
#% 618590
#! In this investigation, we discuss how to mine Temporal Class Schemes to model a collection of time series data. From the viewpoint of temporal data mining, this problem can be seen as discretizing time series data or aggregating them. Also this can be considered as screening (or noise filtering). From the viewpoint of temporal databases, the issue is how we represent the data and how we can obtain intensional aspects as temporal schemes. In other words, we discuss scheme discovery for temporal data. Given a collection of temporal objects along with time axis (called log), we examine the data and we introduce a notion of temporal frequent classes to describe them. As the main results of this investigation, we can show that there exists one and only one interval decomposition and the temporal classes related to them. Also we give experimental results that prove the feasibility to time series data.

#index 413608
#* Evaluating contents-link coupled web page clustering for web search results
#@ Yitong Wang;Masaru Kitsuregawa
#t 2002
#c 1
#% 36672
#% 118771
#% 211526
#% 232912
#% 249110
#% 255137
#% 262045
#% 268079
#% 281186
#% 281209
#% 281214
#% 282905
#% 304321
#% 481290
#% 665639
#! Clustering is currently one of the most crucial techniques for dealing (e.g. resources locating, information interpreting) with massive amount of heterogeneous information on the web. Unlike clustering in other fields, web page clustering separates unrelated pages and clusters related pages (to a specific topic) into semantically meaningful groups, which is useful for discrimination, summarization, organization and navigation of unstructured web pages. We have proposed a contents-link coupled clustering algorithm that clusters web pages by combining contents and link analysis. In this paper, we particularly study the effects of out-links (from the web pages), in-links (to the web page) and terms on the final clustering results as well as how to effectively combine these three parts to improve the quality of clustering results. We apply it to cluster web search results. Preliminary experiments and evaluations are conducted on various topics. As the experimental results show, the proposed clustering algorithm is effective and promising.

#index 413609
#* Inferring hierarchical descriptions
#@ Eric Glover;David M. Pennock;Steve Lawrence;Robert Krovetz
#t 2002
#c 1
#% 280849
#% 281214
#% 348178
#% 668807
#% 756964
#% 786523
#% 853819
#! We create a statistical model for inferring hierarchical term relationships about a topic, given only a small set of example web pages on the topic, without prior knowledge of any hierarchical information. The model can utilize either the full text of the pages in the cluster or the context of links to the pages. To support the model, we use "ground truth" data taken from the category labels in the Open Directory. We show that the model accurately separates terms in the following classes: self terms describing the cluster, parent terms describing more general concepts, and child terms describing specializations of the cluster. For example, for a set of biology pages, sample parent, self, and child terms are science, biology, and genetics respectively. We create an algorithm to predict parent, self, and child terms using the new model, and compare the predictions to the ground truth data. The algorithm accurately ranks a majority of the ground truth terms highly, and identifies additional complementary terms missing in the Open Directory.

#index 413610
#* Evaluation of hierarchical clustering algorithms for document datasets
#@ Ying Zhao;George Karypis
#t 2002
#c 1
#% 36672
#% 67565
#% 118771
#% 232117
#% 248790
#% 252836
#% 280404
#% 280492
#% 304321
#% 304423
#% 420083
#% 438137
#% 481281
#% 631985
#% 729437
#! Fast and high-quality document clustering algorithms play an important role in providing intuitive navigation and browsing mechanisms by organizing large amounts of information into a small number of meaningful clusters. In particular, hierarchical clustering solutions provide a view of the data at different levels of granularity, making them ideal for people to visualize and interactively explore large document collections.In this paper we evaluate different partitional and agglomerative approaches for hierarchical clustering. Our experimental evaluation showed that partitional algorithms always lead to better clustering solutions than agglomerative algorithms, which suggests that partitional clustering algorithms are well-suited for clustering large document datasets due to not only their relatively low computational requirements, but also comparable or even better clustering performance. We present a new class of clustering algorithms called constrained agglomerative algorithms that combine the features of both partitional and agglomerative algorithms. Our experimental results showed that they consistently lead to better hierarchical solutions than agglomerative or partitional algorithms alone.

#index 413611
#* Strategies for minimising errors in hierarchical web categorisation
#@ Wahyu Wibowo;Hugh E. Williams
#t 2002
#c 1
#% 55490
#% 116149
#% 165110
#% 219051
#% 219052
#% 280866
#% 309141
#% 318412
#% 461692
#% 465747
#% 465895
#! On the Web, browsing and searching categories is a popular method of finding documents. Two well-known category-based search systems are the Yahoo!~and DMOZ hierarchies, which are maintained by experts who assign documents to categories. However, manual categorisation by experts is costly, subjective, and not scalable with the increasing volumes of data that must be processed. Several methods have been investigated for effective automatic text categorisation. These include selection of categorisation methods, selection of pre-categorised training samples, use of hierachies, and selection of document fragments or features. In this paper, we further investigate categorisation into Web hierarchies and the role of hierarchical information in improving categorisation effectiveness. We introduce new strategies to reduce errors in hierarchical categorisation. In particular, we propose novel techniques that shift the assignment into higher level categories when lower level assignment is uncertain. Our results show that absolute error rates can be reduced by over 2%.

#index 413612
#* Knowledge-based extraction of named entities
#@ Jamie Callan;Teruko Mitamura
#t 2002
#c 1
#% 278107
#% 309845
#% 695970
#% 742424
#% 814902
#! The usual approach to named-entity detection is to learn extraction rules that rely on linguistic, syntactic, or document format patterns that are consistent across a set of documents. However, when there is no consistency among documents, it may be more effective to learn document-specific extraction rules.This paper presents a knowledge-based approach to learning rules for named-entity extraction. Document-specific extraction rules are created using a generate-and-test paradigm and a database of known named-entities. Experimental results show that this approach is effective on Web documents that are difficult for the usual methods.

#index 413613
#* Condorcet fusion for improved retrieval
#@ Mark Montague;Javed A. Aslam
#t 2002
#c 1
#% 71772
#% 71774
#% 169774
#% 174664
#% 194141
#% 194275
#% 194276
#% 219050
#% 232703
#% 273033
#% 303351
#% 340936
#% 340959
#% 389801
#% 420464
#% 708847
#% 709230
#! We present a new algorithm for improving retrieval results by combining document ranking functions: Condorcet-fuse. Beginning with one of the two major classes of voting procedures from Social Choice Theory, the Condorcet procedure, we apply a graph-theoretic analysis that yields a sorting-based algorithm that is elegant, efficient, and effective. The algorithm performs very well on TREC data, often outperforming existing metasearch algorithms whether or not relevance scores and training data is available. Condorcet-fuse significantly outperforms Borda-fuse, the analogous representative from the other major class of voting algorithms.

#index 413614
#* I/O-efficient techniques for computing pagerank
#@ Yen-Yu Chen;Qingqing Gan;Torsten Suel
#t 2002
#c 1
#% 186522
#% 214673
#% 248810
#% 255170
#% 262061
#% 268073
#% 268079
#% 268087
#% 268186
#% 281214
#% 281251
#% 281655
#% 281763
#% 282905
#% 290703
#% 309749
#% 309779
#% 309786
#% 330609
#% 340141
#% 348173
#% 387427
#% 438136
#% 438553
#% 479969
#% 596259
#% 656242
#% 656281
#% 656282
#% 659994
#! Over the last few years, most major search engines have integrated link-based ranking techniques in order to provide more accurate search results. One widely known approach is the Pagerank technique, which forms the basis of the Google ranking scheme, and which assigns a global importance measure to each page based on the importance of other pages pointing to it. The main advantage of the Pagerank measure is that it is independent of the query posed by a user; this means that it can be precomputed and then used to optimize the layout of the inverted index structure accordingly. However, computing the Pagerank measure requires implementing an iterative process on a massive graph corresponding to billions of web pages and hyperlinks.In this paper, we study I/O-efficient techniques to perform this iterative computation. We derive two algorithms for Pagerank based on techniques proposed for out-of-core graph algorithms, and compare them to two existing algorithms proposed by Haveliwala. We also consider the implementation of a recently proposed topic-sensitive version of Pagerank. Our experimental results show that for very large data sets, significant improvements over previous results can be achieved on machines with moderate amounts of memory. On the other hand, at most minor improvements are possible on data sets that are only moderately larger than memory, which is the case in many practical scenarios.

#index 413615
#* Personalized web search by mapping user queries to categories
#@ Fang Liu;Clement Yu;Weiyi Meng
#t 2002
#c 1
#% 115462
#% 124009
#% 165111
#% 194285
#% 219049
#% 224113
#% 234992
#% 252753
#% 280817
#% 281366
#% 287214
#% 290149
#% 309133
#% 333932
#% 333945
#% 376266
#% 385946
#% 406493
#% 465747
#% 481748
#% 615723
#% 637576
#% 647622
#% 978507
#! Current web search engines are built to serve all users, independent of the needs of any individual user. Personalization of web search is to carry out retrieval for each user incorporating his/her interests. We propose a novel technique to map a user query to a set of categories, which represent the user's search intention. This set of categories can serve as a context to disambiguate the words in the user's query. A user profile and a general profile are learned from the user's search history and a category hierarchy respectively. These two profiles are combined to map a user query into a set of categories. Several learning and combining algorithms are evaluated and found to be effective. Among the algorithms to learn a user profile, we choose the Rocchio-based method for its simplicity, efficiency and its ability to be adaptive. Experimental results indicate that our technique to personalize web search is both effective and efficient.

#index 413616
#* Using micro information units for internet search
#@ Xiaoli Li;Tong-Heng Phang;Minqing Hu;Bing Liu
#t 2002
#c 1
#% 144011
#% 144012
#% 169813
#% 219052
#% 232677
#% 262084
#% 268079
#% 278106
#% 281209
#% 330676
#% 433672
#% 433674
#% 742204
#% 746910
#% 786504
#% 786553
#! Internet search is one of the most important applications of the Web. A search engine takes the user's keywords to retrieve and to rank those pages that contain the keywords. One shortcoming of existing search techniques is that they do not give due consideration to the micro-structures of a Web page. A Web page is often populated with a number of small information units, which we call micro information units (MIU). Each unit focuses on a specific topic and occupies a specific area of the page. During the search, if all the keywords in the user query occur in a single MIU of a page, the top ranking results returned by a search engine are generally relevant and useful. However, if the query words scatter at different MIUs in a page, the pages returned can be quite irrelevant (which causes low precision). The reason for this is that although a page has information on individual MIUs, it may not have information on their intersections. In this paper, we propose a technique to solve this problem. At the off-line pre-processing stage, we segment each page to identify the MIUs in the page, and index the keywords of the page according to the MIUs in which they occur. In searching, our retrieval and ranking algorithm utilizes this additional information to return those most relevant pages. Experimental results show that this method is able to significantly improve the search precision.

#index 413617
#* Entropy-based link analysis for mining web informative structures
#@ Hung-Yu Kao;Ming-Syan Chen;Shian-Hua Lin;Jan-Ming Ho
#t 2002
#c 1
#% 67565
#% 214673
#% 255137
#% 262061
#% 268073
#% 268079
#% 271060
#% 282905
#% 309151
#% 309749
#% 309779
#% 330676
#% 330700
#% 330784
#% 340919
#% 387427
#% 438136
#% 443194
#% 480824
#% 705442
#! In this paper, we study the problem of mining the informative structure of a news Web site which consists of thousands of hyperlinked documents. We define the informative structure of a news Web site as a set of index pages (or referred to as TOC, i.e., table of contents, pages) and a set of article pages linked by TOC pages through informative links. It is noted that the Hyperlink Induced Topics Search (HITS) algorithm has been employed to provide a solution to analyzing authorities and hubs of pages. However, most of the content sites tend to contain some extra hyperlinks, such as navigation panels, advertisements and banners, so as to increase the add-on values of their Web pages. Therefore, due to the structure induced by these extra hyperlinks, HITS is found to be insufficient to provide a good precision in solving the problem. To remedy this, we develop an algorithm to utilize entropy-based Link Analysis on Mining Web Informative Structures. This algorithm is referred to as LAMIS. The key idea of LAMIS is to utilize information entropy for representing the knowledge that corresponds to the amount of information in a link or a page in the link analysis. Experiments on several real news Web sites show that the precision and the recall of LAMIS are much superior to those obtained by heuristic methods and conventional ink analysis methods.

#index 413618
#* COOLCAT: an entropy-based algorithm for categorical clustering
#@ Daniel Barbará;Yi Li;Julia Couto
#t 2002
#c 1
#% 36672
#% 210173
#% 232117
#% 248790
#% 280417
#% 280419
#% 310537
#% 344588
#% 345859
#% 369349
#% 376266
#% 408396
#% 479659
#% 631985
#% 729437
#! In this paper we explore the connection between clustering categorical data and entropy: clusters of similar poi lower entropy than those of dissimilar ones. We use this connection to design an incremental heuristic algorithm, COOLCAT, which is capable of efficiently clustering large data sets of records with categorical attributes, and data streams. In contrast with other categorical clustering algorithms published in the past, COOLCAT's clustering results are very stable for different sample sizes and parameter settings. Also, the criteria for clustering is a very intuitive one, since it is deeply rooted on the well-known notion of entropy. Most importantly, COOLCAT is well equipped to deal with clustering of data streams(continuously arriving streams of data point) since it is an incremental algorithm capable of clustering new points without having to look at every point that has been clustered so far. We demonstrate the efficiency and scalability of COOLCAT by a series of experiments on real and synthetic data sets.

#index 413619
#* FREM: fast and robust EM clustering for large data sets
#@ Carlos Ordonez;Edward Omiecinski
#t 2002
#c 1
#% 169358
#% 169378
#% 210173
#% 216500
#% 248790
#% 248792
#% 278011
#% 280419
#% 280463
#% 300131
#% 300213
#% 304945
#% 320942
#% 333929
#% 333933
#% 420081
#% 466513
#% 466660
#% 479962
#% 479986
#% 480812
#% 481281
#% 631985
#% 668901
#% 712129
#% 729437
#% 857390
#! Clustering is a fundamental Data Mining technique. This article presents an improved EM algorithm to cluster large data sets having high dimensionality, noise and zero variance problems. The algorithm incorporates improvements to increase the quality of solutions and speed. In general the algorithm can find a good clustering solution in 3 scans over the data set. Alternatively, it can be run until it converges. The algorithm has a few parameters that are easy to set and have defaults for most cases. The proposed algorithm is compared against the standard EM algorithm and the On-Line EM algorithm.

#index 413620
#* Alternatives to the k-means algorithm that find better clusterings
#@ Greg Hamerly;Charles Elkan
#t 2002
#c 1
#% 280463
#% 282481
#% 299535
#% 313959
#% 342626
#% 349208
#% 361100
#% 374537
#% 420057
#% 425007
#% 464292
#% 466083
#% 466425
#% 527853
#% 1650729
#! We investigate here the behavior of the standard k-means clustering algorithm and several alternatives to it: the k-harmonic means algorithm due to Zhang and colleagues, fuzzy k-means, Gaussian expectation-maximization, and two new variants of k-harmonic means. Our aim is to find which aspects of these algorithms contribute to finding good clusterings, as opposed to converging to a low-quality local optimum. We describe each algorithm in a unified framework that introduces separate cluster membership and data weight functions. We then show that the algorithms do behave very differently from each other on simple low-dimensional synthetic datasets and image segmentation tasks, and that the k-harmonic means method is superior. Having a soft membership function is essential for finding high-quality clusterings, but having a non-constant data weight function is useful also.

#index 413621
#* Thematic mapping - from unstructured documents to taxonomies
#@ Christina Yip Chung;Raymond Lieu;Jinhui Liu;Alpha Luk;Jianchang Mao;Prabhakar Raghavan
#t 2002
#c 1
#% 198058
#% 280840
#% 280849
#% 320930
#! Verity Inc. has developed a comprehensive suite of tools for accurately and efficiently organizing enterprise content which involves four basic steps: (i) creating taxonomies, (ii) building classification models, (iii) populating taxonomies with documents, and (iv) deploying populated taxonomies in enterprise portals. A taxonomy is a hierarchical representation of categories. A taxonomy provides a navigation structure for exploring and understanding the underlying corpus without sifting through a huge volume of documents. Thematic Mapping automatically discovers a concept tree from a corpus of unstructured documents and assigns meaningful labels to concepts based on a semantic network. Integrating with Verity Intelligent Classifier's user-friendly GUI, a user can drill down a concept tree for navigation, perform a conceptual search to retrieve documents pertaining to a concept, build a taxonomy from the concept tree, as well as edit a taxonomy to tailor it into various views (customized taxonomies) of the same corpus. Classification rules can be generated automatically from concepts. These classification rules can be used for populating documents into the taxonomy.

#index 413622
#* Semantic technology applications for homeland security
#@ D. Avant;M. Baum;C. Bertram;M. Fisher;A. Sheth;Y. Warke
#t 2002
#c 1
#% 434033

#index 413623
#* Rule-based data quality
#@ David Loshin
#t 2002
#c 1
#% 322679
#! In the business intelligence/data warehouse user community, there is a growing confusion as to the difference between data cleansing and data quality. While many data cleansing products can help in applying data edits to name and address data, or help in transforming data during an ETL process, there is usually no persistence in this cleansing. This paper describes how we have implemented a business rules approach to build a data validation engine, called GuardianIQ, that transforms declarative data quality rules into code that objectively measures and reports levels of data quality based on user expectations.

#index 413624
#* Comparison of interestingness functions for learning web usage patterns
#@ experimentation Huang;Nick Cercone;Aijun An
#t 2002
#c 1
#% 227917
#% 342643
#% 443092
#% 463903
#% 481290
#! Livelink is a collaborative intranet, extranet and e-business application that enables employees and business partners of an organization to capture, share and reuse business information and knowledge. The usage of the Livelink software has been recorded by the Livelink Web server in its log files. We present an application of data mining techniques to the Livelink Web usage data. In particular, we focus on how to find interesting association rules and sequential patterns from the Livelink log files. A number of interestingness measures are used in our application to identify interesting rules and patterns. We present a comparison of these measures based on the feedback from domain experts. Some of the interestingness measures are found to be better than others.

#index 413625
#* The verity federated infrastructure
#@ Kiam Choo;Rajat Mukherjee;Rami Smair;Wei Zhang
#t 2002
#c 1
#! In the course of researching a subject, it is often necessary to submit the same search request to multiple heterogeneous information sources in order to (a) aggregate as much information as possible, and (b) integrate different aspects of the subject into a coherent report. While it is clear that there is value in providing a federated search solution to make dealing with multiple sources less time-consuming, not all organizations aggregate from the same sources, and once the information has been retrieved, not all organizations want them to be integrated in the same way.The Verity Federated Infrastructure addresses this problem by providing a flexible framework for adding new sources and customizing the way in which results are integrated, post-processed and presented. A new source is made available by writing a Java module called a worker that abides by the search interface of the source. Sources can range from simple information feeds to more complex applications, e.g., CRM systems, relational databases, etc. Workers also perform post-processing on the results returned by other workers, e.g., to provide uniform scores for results from different sources, filtering, etc. This post-processing enables different results to be integrated into a coherent report. Post-processing is triggered by events that propagate between workers and is done asynchronously in the background while results are being viewed. This ability to do background post-processing allows execution of time-consuming operations that provide substantial value without adversely affecting user experience. Finally, search results are returned and viewed incrementally, which enables searching of peer-to-peer networks via peer workers that we have developed.

#index 413626
#* Automatically classifying database workloads
#@ Said Elnaffar;Pat Martin;Randy Horman
#t 2002
#c 1
#% 481945
#% 581671
#! The type of the workload on a database management system (DBMS) is a key consideration in tuning the system. Allocations for resources such as main memory can be very different depending on whether the workload type is Online Transaction Processing (OLTP) or Decision Support System (DSS). In this paper, we present an approach to automatically identifying a DBMS workload as either OLTP or DSS. We build a classification model based on the most significant workload characteristics that differentiate OLTP from DSS, and then use the model to identify any change in the workload type. We construct a workload classifier from the Browsing and Ordering profiles of the TPC-W benchmark. Experiments with an industry-supplied workload show that our classifier accurately identifies the mix of OLTP and DSS work within an application workload.

#index 413627
#* A mapping mechanism to support bitmap index and other auxiliary structures on tables stored as primary B+-trees
#@ Eugene Inseok Chong;Jagannathan Srinivasan;Souripriya Das;Chuck Freiwald;Aravind Yalamanchi;Mahesh Jagannath;Anh-Tuan Tran;Ramkumar Krishnan;Richard Jiang
#t 2002
#c 1
#% 143796
#% 223781
#% 227861
#% 248814
#% 317933
#% 391588
#% 427218
#% 465011
#% 466944
#% 480458
#% 632099
#! Any auxiliary structure, such as a bitmap or a B+-tree index, that refers to rows of a table stored as a primary B+-tree (e.g., tables with clustered index in Microsoft SQL Server, or index-organized tables in Oracle) by their physical addresses would require updates due to inherent volatility of those addresses. To address this problem, we propose a mapping mechanism that 1) introduces a single mapping table, with each row holding one key value from the primary B+-tree, as an intermediate structure between the primary B+-tree and the associated auxiliary structures, and 2) augments the primary B+-tree structure to include in each row the physical address of the corresponding mapping table row. The mapping table row addresses can then be used in the auxiliary structures to indirectly refer to the primary B+-tree rows. The two key benefits are: 1) the mapping table shields the auxiliary structures from the volatility of the primary B+-tree row addresses, and 2) the method allows reuse of existing conventional table mechanisms for supporting auxiliary structures on primary B+-trees. The mapping mechanism is used for supporting bitmap indexes on index-organized tables in Oracle9i. The analytical and experimental studies show that the method is storage efficient, and (despite the mapping table overhead) provides performance benefits that are similar to those provided by bitmap indexes implemented on conventional tables.

#index 413628
#* Using specification-driven concepts for distributed data management and dissemination
#@ M. Brian Blake
#t 2002
#c 1
#% 417989
#% 438368
#% 484396
#% 590687
#% 664060
#! At the MITRE Corporation-Center for Advanced Aviation System Development (CAASD), software engineers work closely with both analyst and domain experts to develop software simulations in the air traffic management domain. In this environment, software simulations are applications that take large amounts of real-world operational information, and through calculations, derivations, and display extends the original information to produce some new insight into the domain. This new insight or knowledge typically comes in the form of a pertinent set of data. Based on this set of information other research groups can further extend this knowledge. The challenge in this environment is a distributed data management system that will allow a distributed set of researchers to share their extended knowledge. This paper presents the motivation and design of such an architecture to support this collaborative knowledge/data sharing environment. This run-time configurable architecture is implemented using web-based technologies such as the Extensible Markup Language (XML), Java Servlets, Extensible Stylesheets (XSL), and a relational database management system (RDBMS).

#index 413629
#* A new cache replacement algorithm for the integration of web caching and prefectching
#@ Cheng-Yue Chang;Ming-Syan Chen
#t 2002
#c 1
#% 223400
#% 309777
#% 316562
#% 443262
#% 443294
#% 1830688
#! Web caching and Web prefetching are two important techniques to reduce the noticeable response time perceived by users. Note that by integrating Web caching and Web prefetching, these two techniques can complement each other since Web caching technique exploits the temporal locality whereas Web prefetching technique utilizes the spatial locality of Web objects. However, without circumspect design, the integration of these two techniques might cause significant performance degradation to each other. In view of this, we propose in this paper an innovative cache replacement algorithm, which not only considers the caching effect in the Web environment but also evaluates the prefetching rules provided by various prefetching schemes. Specifically, we formulate a normalized profit function to evaluate the profit from caching an object (i.e., either a non-implied object or an implied object according to some prefetching rule). Based on the normalized profit function devised, we devise an innovative Web cache replacement algorithm, referred to as algorithm IWCP (standing for the Integration of Web Caching and Prefetching). Using an event-driven simulation, we evaluate the performance of algorithm IWCP under several circumstances. The experimental results show that algorithm IWCP consistently outperforms the companion schemes in various performance metrics.

#index 413630
#* A syntactic approach for searching similarities within sentences
#@ Federica Mandreoli;Riccardo Martoglia;Paolo Tiberio
#t 2002
#c 1
#% 172949
#% 333679
#% 432569
#% 464851
#% 480654
#% 617188
#% 675379
#! Textual data is the main electronic form of knowledge representation. Sentences, meant as logic units of meaningful word sequences, can be considered its backbone. In this paper, we propose a solution based on a purely syntactic approach for searching similarities within sentences, named approximate sub2sequence matching. This process being very time consuming, efficiency in retrieving the most similar parts available in large repositories of textual data is ensured by making use of new filtering techniques. As far as the design of the system is concerned, we chose a solution that allows us to deploy approximate sub2 sequence matching without changing the underlying database.

#index 413631
#* A system for knowledge management in bioinformatics
#@ Sudeshna Adak;Vishal S. Batra;Deo N. Bhardwaj;P. V. Kamesam;Pankaj Kankar;Manish P. Kurhekar;Biplav Srivastava
#t 2002
#c 1
#% 481614
#% 772131
#! The emerging biochip technology has made it possible to simultaneously study expression (activity level) of thousands of genes or proteins in a single experiment in the laboratory. However, in order to extract relevant biological knowledge from the biochip experimental data, it is critical not only to analyze the experimental data, but also to cross-reference and correlate these large volumes of data with information available in external biological databases accessible online. We address this problem in a comprehensive system for knowledge management in bioinformatics called e2e. To the biologist or biological applications, e2e exposes a common semantic view of inter-relationship among biological concepts in the form of an XML representation called eXpressML, while internally, it can use any data integration solution to retrieve data and return results corresponding to the semantic view. We have implemented an e2e prototype that enables a biologist to analyze her gene expression data in GEML or from a public site like Stanford, and discover knowledge through operations like querying on relevant annotated data represented in eXpressML using pathways data from KEGG, publication data from Medline and protein data from SWISS-PROT.

#index 413632
#* An agent-based approach to knowledge management
#@ Bin Yu;Munindar P. Singh
#t 2002
#c 1
#% 3772
#% 326790
#% 445434
#! Traditional approaches to knowledge management are essentially limited to document management. However, much knowledge in organizations or communities resides in an informal social network and may be accessed only by asking the right people. This paper describes MARS, a multiagent referral system for knowledge management. MARS assigns a software agent to each user. The agents facilitate their users' interactions and help manage their personal social networks. Moreover, the agents cooperate with one another by giving and taking referrals to help their users find the right parties to contact for a specific knowledge need.

#index 413633
#* Features of documents relevant to task- and fact- oriented questions
#@ Diane Kelly;Xiao-jun Yuan;Nicholas J. Belkin;Vanessa Murdock;W. Bruce Croft
#t 2002
#c 1
#% 309151
#% 397170
#% 742102
#! We describe results from an ongoing project that considers question types and document features and their relationship to retrieval techniques. We examine eight document features from the top 25 documents retrieved from 74 questions and find that lists and FAQs occur in more documents judged relevant to task-oriented questions than those judged relevant to fact-oriented questions.

#index 413634
#* Data fusion with estimated weights
#@ Shengli Wu;Fabio Crestani
#t 2002
#c 1
#% 169774
#% 232703
#% 340890
#% 340936
#% 342710
#! This paper proposes an adptive approach for data fusion of information retrieval systems, which exploits estimated performances of all component input systems without relevance judgement or training. The estimation is conducted prior to the fusion but uses the same data as fusion applies. The experiment shows that our algorithms are competitive with, and often outperform CombMNZ, one of the most effective algorithms in use.

#index 413635
#* Discovering the representative of a search engine
#@ King-Lup Liu;Clement Yu;Weiyi Meng
#t 2002
#c 1
#% 46803
#% 273926
#% 287237
#% 333945
#% 344448
#% 406493
#% 443561
#% 479642
#% 584914
#! Given a large number of search engines on the Internet, it is difficult for a person to determine which search engines could serve his/her information needs. A common solution is to construct a metasearch engine on top of the search engines. Upon receiving a user query, the metasearch engine sends it to those underlying search engines which are likely to return the desired documents for the query. The selection algorithm used by a metasearch engine to determine whether a search engine should be sent the query typically makes the decision based on the search-engine representative, which contains characteristic information about the database of a search engine. However, an underlying search engine may not be willing to provide the needed information to the metasearch engine. This paper shows that the needed information can be estimated from an uncooperative search engine with good accuracy. Two pieces of information which permit accurate search engine selection are the number of documents indexed by the search engine and the maximum weight of each term. In this paper, we present techniques for the estimation of these two pieces of information.

#index 413636
#* Ginga: a self-adaptive query processing system
#@ Henrique Paques;Ling Liu;Calton Pu
#t 2002
#c 1
#% 1978
#% 3771
#% 83232
#% 248793
#% 340305
#% 632087

#index 413637
#* High-performing feature selection for text classification
#@ Monica Rogati;Yiming Yang
#t 2002
#c 1
#% 262059
#% 464288
#% 464444
#% 465754
#% 466912
#% 1289271
#! This paper reports a controlled study on a large number of filter feature selection methods for text classification. Over 100 variants of five major feature selection criteria were examined using four well-known classification algorithms: a Naive Bayesian (NB) approach, a Rocchio-style classifier, a k-nearest neighbor (kNN) method and a Support Vector Machine (SVM) system. Two benchmark collections were chosen as the testbeds: Reuters-21578 and small portion of Reuters Corpus Version 1 (RCV1), making the new results comparable to published results. We found that feature selection methods based on chi2 statistics consistently outperformed those based on other criteria (including information gain) for all four classifiers and both data collections, and that a further increase in performance was obtained by combining uncorrelated and high-performing feature selection methods.The results we obtained using only 3% of the available features are among the best reported, including results obtained with the full feature set.

#index 413638
#* Index compression vs. retrieval time of inverted files for XML documents
#@ Norbert Fuhr;Norbert Gövert
#t 2002
#c 1
#% 68073
#% 213786
#% 290703
#% 340914
#! Query languages for retrieval of XML documents allow for conditions referring both to the content and the structure of documents. In this paper, we investigate two different approaches for reducing index space of inverted files for XML documents. First, we consider methods for compressing index entries. Second, we develop the new XS tree data structure which contains the structural description of a document in a rather compact form, such that these descriptions can be kept in main memory. Experimental results on two large XML document collections show that very high compression rates for indexes can be achieved, but any compression increases retrieval time. On the other hand, highly compressed indexes may be feasible for applications where storage is limited, such as in PDAs or E-book devices.

#index 413639
#* Interactive methods for taxonomy editing and validation
#@ Scott Spangler;Jeffrey Kreulen
#t 2002
#c 1
#% 46803
#% 115478
#% 232108
#% 290482
#% 393792
#% 406493
#% 430746
#% 449588
#% 528008
#% 729437
#! Taxonomies are meaningful hierarchical categorizations of documents into topics reflecting the natural relationships between the documents and their business objectives. Improving the quality of these taxonomies and reducing the overall cost required to create them is an important area of research. Supervised and unsupervised text clustering are important technologies that comprise only a part of a complete solution. However, there exists a great need for the ability for a human to efficiently interact with a taxonomy during the editing and validation phase. We have developed a comprehensive approach to solving this problem, and implemented this approach in a software tool called eClassifier. eClassifier provides features to help the taxonomy editor understand and evaluate each category of a taxonomy and visualize the relationships between the categories. Multiple techniques allow the user to make changes at both the category and document level. Metrics then establish how well the resultant taxonomy can be modeled for future document classification. In this paper, we present a comprehensive set of viewing, editing and validation techniques we have implemented in the Lotus Discovery Server resulting in a significant reduction in the time required to create a quality taxonomy.

#index 413640
#* Knowledge discovery from texts: a concept frame graph approach
#@ Kanagasabai Rajaraman;Ah-Hwee Tan
#t 2002
#c 1
#% 111415
#% 815922
#! We address the text content mining problem through a concept based framework by constructing a conceptual knowledge base and discovering knowledge therefrom. Defining a novel representation called the Concept Frame Graph (CFG), we propose a learning algorithm for constructing a CFG knowledge base from text documents. An interactive concept map visualization technique is presented for user-guided knowledge discovery from the knowledge base. Through experimental studies on real life documents, we observe that the proposed approach is promising for mining deeper knowledge.

#index 413641
#* Knowledge discovery in patent databases
#@ Konstantinos Markellos;Katerina Perdikuri;Penelope Markellou;Spiros Sirmakessis;George Mayritsakis;Athanasios Tsakalidis
#t 2002
#c 1
#% 105536
#% 216500
#! In our days the business, scientific and personal databases are growing in an exponential rate. However, what is truly valuable is the knowledge that can be extracted from the stored data. Knowledge Discovery in patent databases was traditionally based on manual analysis carried out from statistical experts. Nowadays the increasing interest of many actors have led to the development of new tools for discovering and exploiting information related to technological activities and innovation, "hidden" in patent databases. In this paper we present a system that combines efficient and innovative methodologies and tools for the analysis of patent data stored in international databases and the production of scientific and technological indicators.

#index 413642
#* Web-DL: an experience in building digital libraries from the web
#@ Pável P. Calado;Altigran S. da Silva;Berthier Ribeiro-Neto;Alberto H. F. Laender;Juliano P. Lage;Davi C. Reis;Pablo A. Roberto;Monique V. Vieira;Marcos A. Gonçalves;Edward A. Fox
#t 2002
#c 1
#% 197531
#% 378521
#% 424931
#! The Web contains a huge volume of information, almost all unstructured and, therefore, difficult to manage. In Digital Libraries, however, information is explicitly organized, described, and managed. In this paper, we propose an architecture that allows the construction of digital libraries from the Web, using standard protocols and archival technologies, and incorporating powerful digital library and data extraction tools, thus benefiting from the breadth of the Web contents, but supporting services and organization available in digital libraries. The proposed architecture was applied to the Networked Digital Library of Theses and Dissertations, providing an important first step toward rapid construction of large DLs from the Web, as well as a large-scale solution for interoperability between independent digital libraries.

#index 413643
#* Mining coverage statistics for websource selection in a mediator
#@ Zaiqing Nie;Ullas Nambiar;Sreelakshmi Vaddi;Subbarao Kambhampati
#t 2002
#c 1
#% 316709
#% 333932
#% 342684
#% 342869
#% 481290
#% 482108
#! Recent work in data integration has shown the importance of statistical information about the coverage and overlap of sources for efficient query processing. Despite this recognition there are no effective approaches for learning the needed statistics. The key challenge in learning such statistics is keeping the number of needed statistics low enough to have the storage and learning costs manageable. Naive approaches can become infeasible very quickly. In this paper we present a set of connected techniques that estimate the coverage and overlap statistics while keeping the needed statistics tightly under control. Our approach uses a hierarchical classification of the queries, and threshold based variants of familiar data mining techniques to dynamically decide the level of resolution at which to learn the statistics. We describe the details of our method, and present experimental results demonstrating the efficiency of the learning algorithms and the effectiveness of the learned statistics.

#index 413644
#* Mining soft-matching association rules
#@ Un Yong Nahm;Raymond J. Mooney
#t 2002
#c 1
#% 67565
#% 481290
#! Variation and noise in database entries can prevent data mining algorithms, such as association rule mining, from discovering important regularities. In particular, textual fields can exhibit variation due to typographical errors, mispellings, abbreviations, etc.. By allowing partial or "soft matching" of items based on a similarity metric such as edit-distance or cosine similarity, additional important patterns can be detected. This paper introduces an algorithm, SoftApriori that discovers soft-matching association rules given a user-supplied similarity metric for each field. Experimental results on several "noisy" datasets extracted from text demonstrate that SoftApriori discovers additional relationships that more accurately reflect regularities in the data.

#index 413645
#* Parallelizing the buckshot algorithm for efficient document clustering
#@ Eric C. Jensen;Steven M. Beitzel;Angelo J. Pilotto;Nazli Goharian;Ophir Frieder
#t 2002
#c 1
#% 118771
#% 220037
#% 237949
#% 471064
#! We present a parallel implementation of the Buckshot document clustering algorithm. We demonstrate that this parallel approach is highly efficient both in terms of load balancing and minimization of communication. In a series of experiments using the 2GB of SGML data from TReC disks 4 and 5, our parallel approach was shown to be scalable in terms of processors efficiently used and the number of clusters created.

#index 413646
#* Proceedings of the 4th international workshop on Web information and data management
#@ Roger Chiang;Ee-Peng Lim
#t 2002
#c 1
#! The 2002 International Workshop on Web Information and Data Management (WIDM'02) is the fourth in a series of workshops on Web Information and Data Management held in conjunction with the International Conference on Information and Knowledge Management (CIKM). The objective of the workshop is to bring together researchers, industrial practitioners, and developers to study how the web information can be extracted, stored, analyzed and processed to provide useful knowledge to the end users for various advanced database applications. WIDM'02 has received sponsorship from both ACM SIGIR and ACM SIGMIS.The call for papers resulted in the submission of 53 papers from 21 countries around the world. Compared to WIDM'01, the number of paper received had increased by 50%. All papers were thoroughly reviewed by the program committee and external reviewers. The program committee accepted 12 full papers and 6 short papers for the one-day's program. The authors of these papers are from 9 countries. The twelve full papers were divided into 3 sessions: "Advanced XML Technologies and Applications", "Web Mining", and "Web Services and Performance Evaluation." The workshop would not be a success without the support from the Department of Systems Engineering and Engineering Management (SEEM) of the Chinese University of Hong Kong. The department provided both the manpower and computing resources to host the workshop web site. Song Gao, a graduate student at the department, did a wonderful job running the ConfMan paper submission and review system. We would like to express our appreciation of his assistance.

#index 413752
#* Proceedings of the 2002 ACM symposium on Document engineering
#@ Ethan Munson;Richard Furuta;Jonathan I. Maletic
#t 2002
#c 1
#! Welcome to the ACM Symposium on Document Engineering (DocEng) 2002, the second in a series of meetings on the emerging area of document engineer-ing, which investigates, from the perspective of computer science, systems for creating, managing, and maintaining documents in any form and in all media.DocEng'02 is the second in what we hope will be a long series of meet-ings. The first of the series, DocEng'01, was held in Atlanta, GA during November of 2001. The symposium continues the tradition of several other con-ferences, including the long series of biennial Electronic Publishing (EP) confer-ences and Principles of Document Processing workshops, the 2000 Digital Documents and Electronic Publishing conference, the 1988 ACM Conference on Document Processing Systems, and the 1981 ACM Symposium on Text Manipulation.DocEng'02 once again joins with the ACM Conference on Information and Knowledge Management (CIKM) to present this symposium. We wish to thank the CIKM organizing committee for their assistance.The technical program results from the efforts of the Program Committee and additional reviewers. Each paper received at least three reviews. We thank the PC and reviewers for their careful reviews and prompt efforts, especially as our review schedule coincided with many committee members' vacation schedules. Forty-six papers were submitted and the committee selected twenty-one papers for presentation at DocEng'02.We hope you will agree with us that the Symposium's papers reflect a broad and dynamic area of research, and share with us our excitement over the directions and activities presented here.

#index 413781
#* Proceedings of the 10th ACM international symposium on Advances in geographic information systems
#@ Kia Makki;Niki Pissinou;Agnòs Voisard;Shu-Ching Chen
#t 2002
#c 1
#! These proceedings contain the papers selected for presentation at the 10th ACM International Symposium on Geographic Information Systems (McLean, VA, USA, November 8-9, 2002). It is the 10th of a series of symposia/workshops that started in 1993 with the aim of bringing together researchers, developers, users, and practitioners carrying out research and development in novel systems based on geo-spatial data and knowledge, and fostering interdisciplinary discussions and research in all aspects of geographic information systems.These papers reflect current trends in the GIS research area. Topics such as spatio-temporal data handling, user interfaces and visualization, data clustering, and raster data handling, still are focuses of interest in the research community. More recent research topics, such as web-based GIS and location-based services (more precisely, novel algorithms and data models), receive a growing interest from both the geoscience and the computer science communities, as illustrated by the number of papers submitted and accepted in this forum. Many of these concepts are illustrated in applications described at the end of these proceedings.We received eighty-one submissions. Each of them was reviewed by at least three referees. The program committee, which was constituted of researchers and practitioners from many different fields within the geospatial arena, selected twenty-eight papers to be presented at the conference.

#index 727489
#* Proceedings of the 1st ACM international workshop on Multimedia databases
#@ Shu-Ching Chen;Mei-Ling Shyu
#t 2003
#c 1
#! These proceedings contain the papers selected for presentation at The First ACM International Workshop on Multimedia Databases (New Orleans, Louisiana, USA, November 7, 2003). Its aim is to bring together university researchers, scientists, industry professionals, software engineers and graduate students who need to become acquainted with new theories and technologies in multimedia databases, and to all those who wish to gain a detailed technical understanding of what multimedia databases involve.We received thirty-three submissions. Each of them was reviewed by at least three referees. The program committee, which was constituted of researchers and practitioners from the multimedia database research areas, selected twelve papers to be presented at the workshop. The authors were asked to address each and every comment made by the referees to improve the quality of the papers. These papers reflect current trends in the multimedia database research areas with topics such as Video Analysis, Video Retrieval, Video Summarizing, Multimedia Communications, Relevance Feedback, Multimedia Indexing, and Content-Based Image Retrieval for Multimedia Databases.

#index 729573
#* Proceedings of the 6th ACM international workshop on Data warehousing and OLAP
#@ Stefano Rizzi;Il-Yeol Song
#t 2003
#c 1
#! Data Warehouses are databases specialized for business intelligence applications, and have been rapidly spreading within the industrial world over the last decade, due to their undeniable contribution to increasing the effectiveness and efficiency of the decision process within business and scientific domains. This wide diffusion was supported by remarkable research results on the one hand, by the quick advancement of commercial tools on the other.The ACM International Workshop on Data Warehousing and Online Analytical Processing (DOLAP) is an annual event that provides an international forum where both researchers and practitioners can share their findings in theoretical foundations, current methodologies, practical experiences, and new research directions in the areas of data warehousing and online analytical processing. The sixth DOLAP workshop was held this year in New Orleans, LA, USA.This year, we received 28 submissions form 16 different countries. The submitted papers covered various aspects of Data Warehousing and OLAP including ETL processes, multidimensional modeling, query languages, optimization and tuning, physical organization of data warehouses, data quality, evolution, XML data warehouses. After careful review, 12 papers were selected by the program committee to be presented at the workshop and included in the proceedings. The accepted papers were presented in four sessions: OLAP, XML and architecture, query processing, maintenance and workload.

#index 729849
#* Proceedings of the 11th ACM international symposium on Advances in geographic information systems
#@ Erik Hoel;Philippe Rigaux
#t 2003
#c 1
#! On behalf of the Program Committee, it is our pleasure to welcome the participants to the 11th International Symposium of ACM GIS, held in New Orleans, Louisiana on November 7th and 8th, 2003. This year's ACM-GIS is the eleventh of a series of symposia/workshops that began in 1993 with the aim of bringing together researchers, developers, users, and practitioners carrying out research and development in novel systems based on geo-spatial data and knowledge, and fostering interdisciplinary discussions and research in all aspects of geographic information systems.Following an international call for papers, 82 papers were submitted and reviewed by an international panel of experts in the field of GIS. Of the 82 submitted papers, 22 were accepted (yielding an acceptance rate of 27%). The size of the submission indicates the continued health of the field and interest in it. All final papers, modified according to comments from the reviewers, appear in this volume.The invited keynote talk was from Dr. David Maguire, who presented a lecture entitled "Beyond the GIS Desktop".We would like to thank the Program Committee for giving so freely of their time and insightful opinions. Their reviews were based upon paper originality, significance, relevance, presentation and technical quality. Many of the submitted papers required further review and discussion. We appreciate the efforts of the reviewers in arriving at consensus on the papers involved. This process requires much time and effort.

#index 729872
#* Proceedings of the 5th ACM international workshop on Web information and data management
#@ Roger Chiang;Alberto H. F. Laender;Ee-Peng Lim
#t 2003
#c 1
#! The 2003 International Workshop on Web Information and Data Management (WIDM 2003) is the fifth in a series of workshops on Web Information and Data Management held in conjunction with the International Conference on Information and Knowledge Management (CIKM). The objective of the workshop is to bring together researchers, industrial practitioners, and developers to study how web information can be extracted, stored, analyzed, and processed to provide useful knowledge to the end users for various advanced database applications. WIDM 2003 has received sponsorship from both ACM SIGIR and ACM SIGMIS.The call for papers resulted in the submission of 54 papers from 18 countries around the world. All papers were thoroughly reviewed by the program committee and external reviewers. The program committee accepted 15 full papers and 10 short papers for the two-days' program. The authors of these papers are from 12 countries. The 25 accepted papers were divided into 7 sessions: "Web Data Extraction and Structure Mining", "XML Data Modeling and Storage", "Tools for Integrating and Querying Web Information", "Web Clustering and Usage Mining", "XML and Information Integration", "Intelligent Web Information Access", and "Query and View Processing." In addition, the WIDM 2003 program also includes an invited talk on "Business-Aware Management: the Next Frontier for Web Services" by Dr Fabio Casati from Palo Alto HP Labs.The workshop would not be a success without the support from the Centre for Advanced Information Systems of the Nanyang Technological University, Singapore. The center provided both the manpower and computing resources to host the workshop web site. Zehua Liu, a graduate student at the Nanyang Technological University, did a wonderful job running the ConfMan paper submission and review system. We would like express our appreciation to his assistance.We are also indebted to the CIKM 2003's organizing committee for its support. We are particularly thankful to Eun-Kyo Park for managing the proceedings coordination as well as financial and registration matters, and to Il-Yeol Song for coordinating WIDM 2003 with the other workshops.

#index 730005
#* Proceedings of the twelfth international conference on Information and knowledge management
#@ Donald Kraft;Ophir Frieder;Joachim Hammer;Sajda Qureshi;Len Seligman
#t 2003
#c 1
#! Welcome to the Twelfth ACM International Conference on Information and Knowledge Management (CIKM 2003)! The Organizing Committee, as well as the sponsors of CIKM 2003, join me in the desire that this conference will be the opportunity for you to learn, to grow, to share knowledge and skills about information and knowledge management.In twelve years, the world of information and knowledge management has changed greatly. However, the need for information scientists and technologists and practitioners to meet face-to-face and exchange ideas, as well as to welcome new members into our scientific community, is still here. To the extent that we are successful in these goals, CIKM 2003 will have served its purpose.

#index 730006
#* Grand challenges for information management
#@ Jamie Carbonell
#t 2003
#c 1

#index 730007
#* Query expansion using associated queries
#@ Bodo Billerbeck;Falk Scholer;Hugh E. Williams;Justin Zobel
#t 2003
#c 1
#% 184486
#% 194299
#% 232713
#% 262102
#% 280847
#% 284931
#% 290703
#% 316513
#% 323131
#% 324129
#% 326522
#% 340141
#% 340882
#% 340928
#% 340964
#% 375017
#% 387427
#% 397163
#% 406493
#% 413586
#% 438557
#% 643033
#% 729027
#! Hundreds of millions of users each day use web search engines to meet their information needs. Advances in web search effectiveness are therefore perhaps the most significant public outcomes of IR research. Query expansion is one such method for improving the effectiveness of ranked retrieval by adding additional terms to a query. In previous approaches to query expansion, the additional terms are selected from highly ranked documents returned from an initial retrieval run. We propose a new method of obtaining expansion terms, based on selecting terms from past user queries that are associated with documents in the collection. Our scheme is effective for query expansion for web retrieval: our results show relative improvements over unexpanded full text retrieval of 26%--29%, and 18%--20% over an optimised, conventional expansion approach.

#index 730008
#* A study of parameter tuning for term frequency normalization
#@ Ben HE;Iadh Ounis
#t 2003
#c 1
#% 169781
#% 218982
#% 321635
#% 324129
#% 340948
#% 397183
#% 411760
#% 458402
#! Most current term frequency normalization approaches for information retrieval involve the use of parameters. The tuning of these parameters has an important impact on the overall performance of the information retrieval system. Indeed, a small variation in the involved parameter(s) could lead to an important variation in the precision/recall values. Most current tuning approaches are dependent on the document collections. As a consequence, the effective parameter value cannot be obtained for a given new collection without extensive training data. In this paper, we propose a novel and robust method for the tuning of term frequency normalization parameter(s), by measuring the normalization effect on the within document frequency of the query terms. As an illustration, we apply our method on Amati \& Van Rijsbergen's so-called normalization 2. The experiments for the ad-hoc TREC-6,7,8 tasks and TREC-8,9,10 Web tracks show that the new method is independent of the collections and able to provide reliable and good performance.

#index 730009
#* Using titles and category names from editor-driven taxonomies for automatic evaluation
#@ Steven M. Beitzel;Eric C. Jensen;Abdur Chowdhury;David Grossman
#t 2003
#c 1
#% 262105
#% 280041
#% 281174
#% 286304
#% 296646
#% 306468
#% 309093
#% 309146
#% 330787
#% 340892
#% 340921
#% 348165
#% 397203
#% 420508
#% 424260
#% 424333
#% 438557
#% 590523
#% 643026
#! Evaluation of IR systems has always been difficult because of the need for manually assessed relevance judgments. The advent of large editor-driven taxonomies on the web opens the door to a new evaluation approach. We use the ODP (Open Directory Project) taxonomy to find sets of pseudo-relevant documents via one of two assumptions: 1) taxonomy entries are relevant to a given query if their editor-entered titles exactly match the query, or 2) all entries in a leaf-level taxonomy category are relevant to a given query if the category title exactly matches the query. We compare and contrast these two methodologies by evaluating six web search engines on a sample from an America Online log of ten million web queries, using MRR measures for the first method and precision-based measures for the second. We show that this technique is stable with respect to the query set selected and correlated with a reasonably large manual evaluation.

#index 730010
#* Approximate searches: k-neighbors + precision
#@ Sid-Ahmed Berrani;Laurent Amsaleg;Patrick Gros
#t 2003
#c 1
#% 210173
#% 227526
#% 243299
#% 280452
#% 342828
#% 359751
#% 443517
#% 464195
#% 465014
#% 479649
#% 479973
#% 480304
#% 632011
#% 632043
#% 763595
#! It is known that all multi-dimensional index structures fail to accelerate content-based similarity searches when the feature vectors describing images are high-dimensional. It is possible to circumvent this problem by relying on approximate search-schemes trading-off result quality for reduced query execution time. Most approximate schemes, however, provide none or only complex control on the precision of the searches, especially when retrieving the k nearest neighbors (NNs) of query points.In contrast, this paper describes an approximate search scheme for high-dimensional databases where the precision of the search can be probabilistically controlled when retrieving the k NNs of query points. It allows a fine and intuitive control over this precision by setting at run time the maximum probability for a vector that would be in the exact answer set to be missed in the approximate set of answers eventually returned. This paper also presents a performance study of the implementation using real datasets showing its reliability and efficiency. It shows, for example, that our method is 6.72 times faster than the sequential scan when it handles more than 5 106 24-dimensional vectors, even when the probability of missing one of the true nearest neighbors is below 0.01.

#index 730011
#* Replication and retrieval strategies of multidimensional data on parallel disks
#@ Chung-Min Chen;Christine T. Cheng
#t 2003
#c 1
#% 164363
#% 300159
#% 303086
#% 378390
#% 387088
#% 461922
#% 564098
#% 578398
#% 582135
#% 609704
#% 664833
#! Aside from enhancing data availability during disk failures, replication of data is also used to speed up I/O performance of read-intensive applications. There are two issues that need to be addressed: (a) data placement (Which disks should store the copies of each data block?) and (b) scheduling (Given a query Q, and a placement scheme P of the data, from which disk should each block in Q be retrieved so that retrieval time is minimized?) In this paper, we consider range queries and assume that the dataset is a multidimensional grid and r copies of each unit block of the grid must be stored among M disks. To accurately measure performance of a scheduling algorithm, we consider a metric that takes into account the scheduling overhead as well as the time it takes to retrieve the data blocks from the disks. We describe several combinations of data placement schemes and scheduling algorithms and analyze their performance for range queries with respect to the above metric. We then present simulation results for the most interesting case r=2, showing that the strategies do perform better than the previously known method, especially for large queries.

#index 730012
#* Multi-resolution modeling of large scale scientific simulation data
#@ Chuck Baldwin;Ghaleb Abdulla;Terence Critchlow
#t 2003
#c 1
#% 116390
#% 168862
#% 177401
#% 233632
#% 260657
#% 333941
#% 337432
#% 392992
#% 479799
#% 572308
#! To provide scientists and engineers with the ability to explore and analyze tera-scale size data-sets we are using a twofold approach. First, we model the data with the objective of creating a compressed yet manageable representation. Second, with that compressed representation, we provide the ability to query the resulting approximation in order to obtain approximate yet sufficient answers; a process called ad-hoc querying. This paper is concerned with a wavelet modeling technique that seeks to capture the important physical characteristics of the target scientific data. Our approach is driven by the compression, which is necessary for viable throughput, along with the end user requirements from the discovery process. Our work contrasts existing research which applies wavelets to range querying, change detection, and clustering problems by working directly with the wavelet decomposition of the data. The difference in this procedure is due primarily to the nature of the data and the requirements of the scientists and engineers. Our approach directly uses the wavelet coefficients of the data to compress as well as query. We describe how the wavelet decomposition is used to facilitate data compression and how queries are posed on the resulting compressed model. Results of this process will be shown for several problems of interest.

#index 730013
#* Visual structures for image browsing
#@ Ricardo S. Torres;Celmar G. Silva;Claudia B. Medeiros;Heloisa V. Rocha
#t 2003
#c 1
#% 172811
#% 173463
#% 259754
#% 270633
#% 281382
#% 318785
#% 324983
#% 342528
#% 434613
#% 437405
#% 437407
#% 443413
#% 449013
#% 450805
#% 479462
#% 581911
#% 619859
#% 641148
#% 641161
#% 641175
#! Content-Based Image Retrieval (CBIR) presents several challenges and has been subject to extensive research from many domains, such as image processing or database systems. Database researchers are concerned with indexing and querying, whereas image processing experts worry about extracting appropriate image descriptors. Comparatively little work has been done on designing user interfaces for CBIR systems. This, in turn, has a profound effect on these systems since the concept of image similarity is strongly influenced by user perception. This paper describes an initial effort to fill this gap, combining recent research in CBIR and Information Visualization, studied from a Human-Computer Interface perspective. It presents two visualization techniques based on Spiral and Concentric Rings implemented in a CBIR system to explore query results. The approach is centered on keeping user focus on both the query image, and the most similar retrieved images. Experiments conducted so far suggest that the proposed visualization strategies improves system usability.

#index 730014
#* Visualization of Communication Patterns in Collaborative Innovation Networks - Analysis of Some W3C Working Groups
#@ Peter A. Gloor;Rob Laubacher;Scott B. C. Dynes;Yan Zhao
#t 2003
#c 1
#% 122797
#! Collaborative Innovation Networks (COINs) are groups of self-motivated individuals from various parts of an organization or from multiple organizations, empowered by the Internet, who work together on a new idea, driven by a common vision. In this paper we report first results of a project that examines innovation networks by analyzing the e-mail archives of some W3C (WWW consortium) working groups. These groups exhibit ideal characteristics for our purpose, as they form truly global networks working together over the Internet to develop next generation technologies. We first describe the software tools we developed to visualize the temporal communication flow, which represent communication patterns as directed acyclic graphs, We then show initial results, which revealed significant variations between the communication patterns and network structures of the different groups., We were also able to identify distinctive communication patterns among group leaders, both those who were officially appointed and other who were assuming unofficial coordinating roles.

#index 730015
#* Similarity among melodies for music information retrieval
#@ Takao MIURA;Isamu SHIOYA
#t 2003
#c 1
#% 261882
#% 286744
#% 316555
#% 333679
#% 346559
#% 385946
#% 413599
#! Here we discuss how to look for similar melody in music databases by giving monophonic melody in sheet. In this work, we utilize text expression (or sheet music) to describe music and introduce pitch spectrum of melodies. By this feature, we concisely distinguish music from tempo, transposition or other arbitrary expressions. We show the usefulness by experimental results.

#index 730016
#* Efficient region-based image retrieval
#@ Roger Weber;Michael Mlivoncic
#t 2003
#c 1
#% 1921
#% 172949
#% 248797
#% 273919
#% 341442
#% 345848
#% 457965
#% 479462
#% 479649
#% 479655
#% 481947
#% 481956
#% 588726
#% 589983
#! Region-based image retrieval(RBIR) was recently proposed as an extension of content-based image retrieval(CBIR). An RBIR system automatically segments images into a variable number of regions, and extracts for each region a set of features. Then, a dissimilarity function determines the distance between a database image and a set of reference regions. Unfortunately, the large evaluation costs of the dissimilarity function are restricting RBIR to relatively small databases. In this paper, we apply a multi-step approach to enable region-based techniques for large image collections. We provide cheap lower and upper bounding distance functions for a recently proposed dissimilarity measure. As our experiments show, these bounding functions are so tight, that we have to evaluate the expensive distance function for less than 0.5\%of the images. For a typical image database with more than 370,000images, our multi-step approach improved retrieval performance by a factor of more than5 compared to the currently fastest methods.

#index 730017
#* Speech user interfaces for information retrieval
#@ Juan E. Gilbert;Yapin Zhong
#t 2003
#c 1
#% 169777
#% 253188
#% 319273
#% 406493
#% 414601
#% 501797
#! The research proposed here concentrates on the problem of designing and developing a spoken query retrieval (SQR) system to access large document databases via voice. The main challenge is to identify and address issues related to designing an effective and efficient speech user interface (SUI), especially if the aim is to facilitate spoken queries of large document databases. Furthermore, the task of presenting large query result sets aurally should be performed such that the user's short term memory is not overloaded. In this paper, a framework allowing information retrieval to large document databases via voice is presented and findings from a research study using the framework will be discussed as well.

#index 730018
#* The power-method: a comprehensive estimation technique for multi-dimensional queries
#@ Yufei Tao;Christos Faloutsos;Dimitris Papadias
#t 2003
#c 1
#% 86950
#% 137887
#% 164360
#% 237187
#% 248822
#% 273887
#% 273903
#% 273906
#% 300132
#% 300160
#% 300193
#% 318703
#% 333946
#% 333947
#% 333955
#% 333983
#% 397385
#% 443327
#% 464850
#% 465162
#% 480465
#% 481620
#% 482092
#% 503535
#% 527194
#% 654486
#% 1015322
#! Existing estimation approaches for multi-dimensional databases often rely on the assumption that data distribution in a small region is uniform, which seldom holds in practice. Moreover, their applicability is limited to specific estimation tasks under certain distance metric. This paper develops the Power-method, a comprehensive technique applicable to a wide range of query optimization problems under various metrics. The Power-method eliminates the local uniformity assumption and is accurate even in scenarios where existing approaches completely fail. Furthermore, it performs estimation by evaluating only one simple formula with minimal computational overhead. Extensive experiments confirm that the Power-method outperforms previous techniques in terms of accuracy and applicability to various optimization scenarios.

#index 730019
#* High dimensional reverse nearest neighbor queries
#@ Amit Singh;Hakan Ferhatosmanoglu;Ali Şaman Tosun
#t 2003
#c 1
#% 33209
#% 86950
#% 88056
#% 201876
#% 248797
#% 248798
#% 252304
#% 285932
#% 300163
#% 411694
#% 427199
#% 465009
#% 479462
#% 479649
#% 480661
#% 504162
#% 527026
#% 631963
#% 632009
#% 993999
#! Reverse Nearest Neighbor (RNN) queries are of particular interest in a wide range of applications such as decision support systems, profile based marketing, data streaming, document databases, and bioinformatics. The earlier approaches to solve this problem mostly deal with two dimensional data. However most of the above applications inherently involve high dimensions and high dimensional RNN problem is still unexplored. In this paper, we propose an approximate solution to answer RNN queries in high dimensions. Our approach is based on the strong correlation in practice between k-NN and RNN. It works in two phases. In the first phase the k-NN of a query point is found and in the next phase they are further analyzed using a novel type of query Boolean Range Query (BRQ). Experimental results show that BRQ is much more efficient than both NN and range queries, and can be effectively used to answer RNN queries. Performance is further improved by running multiple BRQ simultaneously. The proposed approach can also be used to answer other variants of RNN queries such as RNN of order k, bichromatic RNN, and Matching Query which has many applications of its own. Our technique can efficiently answer NN, RNN, and its variants with approximately same number of I/O as running a NN query.

#index 730020
#* Dimensionality reduction using magnitude and shape approximations
#@ Ümit Y. Ogras;Hakan Ferhatosmanoglu
#t 2003
#c 1
#% 67552
#% 68091
#% 169805
#% 169940
#% 172949
#% 203439
#% 214595
#% 227939
#% 227999
#% 237187
#% 245787
#% 248797
#% 248798
#% 252304
#% 262113
#% 275367
#% 316526
#% 333941
#% 342828
#% 397396
#% 404757
#% 435141
#% 460862
#% 465014
#% 479649
#% 479973
#% 481947
#% 481956
#% 482109
#% 577290
#% 631963
#% 682214
#! High dimensional data sets are encountered in many modern database applications. The usual approach is to construct a summary of the data set through a lossy compression technique, and use this lower dimensional synopsis to provide fast, approximate answers to the queries. In this paper, we develop a novel dimensionality reduction technique based on partitioning the high dimensional vector space into orthogonal subspaces. First, we find a relation between the Euclidian distance of two n-dimensional vectors and the Euclidian distances of their projections on the orthogonal subspaces. Then, based on this relation we develop a method to approximate the Euclidian distance using novel inner product approximation. This process allows us to incorporate the shape information of the vectors to this approximation. While the inner product approximation is symmetric, i.e., captures only the magnitude information of the data, the proposed method takes both the magnitude and shape information of the original vectors into account through partitioning. In the experiments, we demonstrate the effectiveness of our technique by comparing it with commonly used methods.

#index 730021
#* Web unit mining: finding and classifying subgraphs of web pages
#@ Aixin Sun;Ee-Peng Lim
#t 2003
#c 1
#% 248810
#% 269212
#% 269217
#% 303049
#% 309141
#% 309151
#% 340904
#% 340928
#% 344447
#% 397126
#% 413663
#% 425016
#% 430761
#% 464267
#% 577236
#! In web classification, most researchers assume that the objects to classify are individual web pages from one or more web sites. In practice, the assumption is too restrictive since a web page itself may not always correspond to a concept instance of some semantic concept (or category) given to the classification task. In this paper, we want to relax this assumption and allow a concept instance to be represented by a subgraph of web pages or a set of web pages. We identify several new issues to be addressed when the assumption is removed, and formulate the web unit mining problem. We also propose an iterative web unit mining (iWUM) method that first finds subgraphs of web pages using some knowledge about web site structure. From these web subgraphs, web units are constructed and classified into semantic concepts (or categories) in an iterative manner. Our experiments using the WebKB dataset showed that iWUM improves the overall classification performance and works very well on the more structured parts of a web site.

#index 730022
#* Question answering from the web using knowledge annotation and knowledge mining techniques
#@ Jimmy Lin;Boris Katz
#t 2003
#c 1
#% 261741
#% 266215
#% 271065
#% 330616
#% 340953
#% 474645
#% 571493
#% 705442
#% 815195
#% 815799
#% 853905
#% 854668
#% 854932
#! We present a strategy for answering fact-based natural language questions that is guided by a characterization of real-world user queries. Our approach, implemented in a system called Aranea, extracts answers from the Web using two different techniques: knowledge annotation and knowledge mining. Knowledge annotation is an approach to answering large classes of frequently occurring questions by utilizing semi\-structured and structured Web sources. Knowledge mining is a statistical approach that leverages massive amounts of Web data to overcome many natural language processing challenges. We have integrated these two different paradigms into a question answering system capable of providing users with concise answers that directly address their information needs.

#index 730023
#* Learning cross-document structural relationships using boosting
#@ Zhu Zhang;Jahna Otterbacher;Dragomir Radev
#t 2003
#c 1
#% 230530
#% 288541
#% 311034
#% 465914
#% 578722
#% 646547
#% 708199
#% 708427
#% 708948
#% 748561
#% 815902
#% 815909
#% 853799
#% 1275285
#! Multi-document discoure analysis has emerged with the potential of improving various information retrieval applications. Based on the newly proposed Cross-document Structure Theory (CST), this paper describes an empirical study that uses boosting to classify CST relationships between sentence pairs extracted from topically related documents. We show that the binary classifier for determining existence of structural relationships significantly outperforms the baseline. We also achieve promising results on the multi-class case in which the full taxonomy of relationships are considered.

#index 730024
#* A novel method for stemmer generation based on hidden markov models
#@ Massimo Melucci;Nicola Orio
#t 2003
#c 1
#% 55490
#% 71752
#% 115470
#% 137711
#% 144034
#% 208934
#% 218985
#% 218989
#% 241238
#% 286307
#% 406493
#% 489889
#% 741043
#% 1387545
#! In this paper, we present a method based on Hidden Markov Models (HMMs) to generate statistical stemmers. Using a list of words as training set, the method estimates the HMM parameters which are used to calculate the most probable stem for an arbitrary word. Stemming is performed by computing the most probable path, through the HMM states, corresponding to the input word. Linguistic knowledge or a training set of manually stemmed words are not required. We describe the method and the results of the experiments carried out using standard test collections for five different languages.

#index 730025
#* Statistical transliteration for english-arabic cross language information retrieval
#@ Nasreen AbdulJaleel;Leah S. Larkey
#t 2003
#c 1
#% 159177
#% 262046
#% 262047
#% 746879
#% 817596
#% 854584
#% 1271423
#! Out of vocabulary (OOV) words are problematic for cross language information retrieval. One way to deal with OOV words when the two languages have different alphabets, is to transliterate the unknown words, that is, to render them in the orthography of the second language. In the present study, we present a simple statistical technique to train an English to Arabic transliteration model from pairs of names. We call this a selected n-gram model because a two-stage training procedure first learns which n-gram segments should be added to the unigram inventory for the source language, and then a second stage learns the translation model over this inventory. This technique requires no heuristics or linguistic knowledge of either language. We evaluate the statistically-trained model and a simpler hand-crafted model on a test set of named entities from the Arabic AFP corpus and demonstrate that they perform better than two online translation sources. We also explore the effectiveness of these systems on the TREC 2002 cross language IR task. We find that transliteration either of OOV named entities or of all OOV words is an effective approach for cross language IR.

#index 730026
#* Addressing the lack of direct translation resources for cross-language retrieval
#@ Lisa Ballesteros;Mark Sanderson
#t 2003
#c 1
#% 232656
#% 262046
#% 262047
#% 309112
#% 340894
#% 420520
#% 456926
#% 643017
#! Most cross language information retrieval research concentrates on language pairs for which direct, rich, and often multiple translation resources already exist. However, for most language pairs, translation via an intermediate language is necessary. Two distinct methods for dealing with the additional ambiguity introduced by the extra translation step have been proposed and individually, shown to improve retrieval effectiveness. Two previous works indicated that in combination, the methods were ineffective. This paper provides strong empirical evidence that the methods can be combined to produce consistent and often significant improvements in retrieval effectiveness. The improvement is shown across a number of different intermediate languages and test collections.

#index 730027
#* Efficient data access to multi-channel broadcast programs
#@ Wai Gen Yee;Shamkant B. Navathe
#t 2003
#c 1
#% 662
#% 32884
#% 172876
#% 201897
#% 227885
#% 247246
#% 249304
#% 274199
#% 274209
#% 290747
#% 316486
#% 342694
#% 378388
#% 442624
#% 632025
#% 632067
#% 1793846
#! This paper studies fast access to data that are broadcast on multiple channels. Broadcast is a useful data dissemination technique because of its scalability, but is lacking when it comes to response time. Increasing the number of available broadcast channels is a logical way of increasing throughput. Little work, however, has considered the access structures necessary for making effective use of the additional channels. We propose various indexing schemes for a multi-channel broadcast program. We demonstrate the effectiveness of our techniques in decreasing response time and tuning time via extensive experiments over a wide range of parameters.

#index 730028
#* Exploring group mobility for replica data allocation in a mobile environment
#@ Jiun-Long Huang;Ming-Syan Chen;Wen-Chih Peng
#t 2003
#c 1
#% 248894
#% 281345
#% 349932
#% 398476
#% 413570
#% 432281
#% 452847
#% 554751
#! The growth in wireless communication technologies attracts a considerable amount of attention in mobile ad-hoc networks. Since mobile hosts in an ad-hoc network usually move freely, the topology of the network changes dynamically and disconnection occurs frequently. These characteristics make a mobile ad-hoc network be likely to be separated into several disconnected partitions, and the data accessibility is hence reduced. Several schemes are proposed to alleviate the reduction of data accessibility by replicating data items. However, little research effort was elaborated upon exploiting the group mobility where the group mobility refers to the phenomenon that several mobile nodes tend to move together. In this paper, we address the problem of replica allocation in a mobile ad-hoc network by exploring group mobility. We first analyze the group mobility model and derive several theoretical results. In light of these results, we propose a replica allocation scheme to improve the data accessibility. Several experiments are conducted to evaluate the performance of the proposed scheme. The experimental results show that the proposed scheme is able to not only obtain higher data accessibility but also produce lower network traffic than prior schemes.

#index 730029
#* Lessons from the implementation of an adaptive parts acquisition ePortal
#@ Rafael Alonso;Jeffrey A. Bloom;Hua Li
#t 2003
#c 1
#% 729993
#! In recent work we have developed a novel approach to the design and implementation of an online portal (ePortal) to help application engineers find replacements for electronic parts that have become obsolete (and hence will no longer be produced). Our approach makes use of machine learning techniques to improve the performance of a database search function. However, the purpose of this note is not to describe in detail the application nor our technical solution - that has been done elsewhere (see [1,2]). Rather, it is our intention to present some of the lessons learned from our project. Below, we provide a brief introduction to the technical approach, concentrate on several of the most salient lessons, and conclude with a description of the current state of the project.

#index 730030
#* Event analyzer: a tool for sequential data processing
#@ Lilian Harada;Yuuji Hotta;Naoki Akaboshi;Kazumi Kubota;Tadashi Ohmori;Riichiro Take
#t 2003
#c 1
#% 333850
#% 464058
#% 485015
#% 503878
#% 587746
#! In this paper we present a tool called Event Analyzer that processes events that compose a sequence. We present the data model in which Event Analyzer is based, as well as its query language that allows the expression of complex patterns to be searched over the sequence of events. The Event Analyzer has been developed and it now integrates the Fujitsu Symfoware e-Business Intelligence Suite Premium.

#index 730031
#* XML parsing: a threat to database performance
#@ Matthias Nicola;Jasmi John
#t 2003
#c 1
#% 397366
#% 482636
#! XML parsing is generally known to have poor performance characteristics relative to transactional database processing. Yet, its potentially fatal impact on overall database performance is being underestimated. We report real-word database applications where XML parsing performance is a key obstacle to a successful XML deployment. There is a considerable share of XML database applications which are prone to fail at an early and simple road block: XML parsing. We analyze XML parsing performance and quantify the extra overhead of DTD and schema validation. Comparison with relational database performance shows that the desired response times and transaction rates over XML data can not be achieved without major improvements in XML parsing technology. Thus, we identify research topics which are most promising for XML parser performance in database systems.

#index 730032
#* Managing IFC for civil engineering projects
#@ Renaud Vanlande;Christophe Cruz;Christophe Nicolle
#t 2003
#c 1
#% 264494
#% 545295
#% 569805
#% 640355
#% 959788
#! The "Industrial Foundation Classes" (IFC) are an ISO norm to define all components of a building in a civil engineering project. IFC files are textual files whose size can reach 100 megabytes. Several IFC files can coexist on the same civil engineering project. Due to their size, their handling and sharing is a complex task. In this paper, we present an approach to automatically identify business objects in the IFC files and simplify their visualization and manipulation on the Internet. We construct an IFC Viewer which transforms the IFC file into a XML IFC tree manipulated through the 3D visualization of the building. The IFC Viewer composed a web-based platform called ACTIVe3D BUILD SERVER. This platform lets geographically dispersed project participants-from architects to electricians-directly use and exchange project documents in a centralized virtual environment during the life cycle of a civil engineering project.

#index 730033
#* Misuse detection for information retrieval systems
#@ Rebecca Cathey;Ling Ma;Nazli Goharian;David Grossman
#t 2003
#c 1
#% 55490
#% 296738
#% 306097
#% 346479
#% 385946
#% 413610
#% 413645
#% 713956
#! We present a novel approach to detect misuse within an information retrieval system by gathering and maintaining knowledge of the behavior of the user rather than anticipating attacks by unknown assailants. Our approach is based on building and maintaining a profile of the behavior of the system user through tracking, or monitoring of user activity within the information retrieval system. Any new activity of the user is compared to the user profile to detect a potential misuse for the authorized user. We propose four different methods to detect misuse in information retrieval systems. Our experimental results on $2$ GB collection favorably demonstrate the validity of our approach.

#index 730034
#* Margin-based local regression for adaptive filtering
#@ Yiming Yang;Bryan Kisiel
#t 2003
#c 1
#% 262085
#% 262087
#% 340941
#% 375017
#% 577297
#% 840583
#! Adaptive information filtering is an open challenge in information retrieval. One of the tough issues is the optimization of decision thresholds over time, based on partial relevance feedback on the system-retrieved documents in chronological order. We developed a new approach, namely margin-based local regression, that automatically adjusts the thresholds based on a sliding window over the truly positive examples for which the system predicted "yes" with respect to a particular class, and a second sliding window over the other documents being processed by the system. Using the means of the scores of the documents in the two windows, we monitor the temporal drifting of the margin that is a function of both the current classification model and the threshold calibration strategy, and that suggests the bounds for the optimal threshold at a given time. Examining this approach together with a Rocchio-style classifier on the TREC 2001 and TREC 2002 benchmark data sets in adaptive filtering, we obtained significant improvements in performance (measured using Fβ=0.5) over the baseline system that did not adapt the threshold over time, and the best result ever reported on the TREC 2002 benchmark corpus for adaptive filtering evaluations. These empirical results suggest that it is important to use both system-accepted and system-rejected documents to optimize thresholds instead of just using system-accepted documents alone, as well as to make the thresholding function temporally sensitive to the shifting centroids of on-topic and off-topic documents.

#index 730035
#* Content-based retrieval in hybrid peer-to-peer networks
#@ Jie Lu;Jamie Callan
#t 2003
#c 1
#% 172898
#% 280856
#% 306468
#% 340176
#% 340887
#% 342375
#% 413583
#% 413587
#% 481748
#% 511663
#% 523066
#! Hybrid peer-to-peer architectures use special nodes to provide directory services for regions of the network ("regional directory services"). Hybrid peer-to-peer architectures are a potentially powerful model for developing large-scale networks of complex digital libraries, but peer-to-peer networks have so far tended to use very simple methods of resource selection and document retrieval. In this paper, we study the application of content-based resource selection and document retrieval to hybrid peer-to-peer networks. The directory nodes that provide regional directory services construct and use the content models of neighboring nodes to determine how to route query messages through the network. The leaf nodes that provide information use content-based retrieval to decide which documents to retrieve for queries. The experimental results demonstrate that using content-based retrieval in hybrid peer-to-peer networks is both more accurate and more efficient for some digital library environments than more common alternatives such as Gnutella 0.6.

#index 730036
#* A reliable storage management layer for distributed information retrieval systems
#@ Charles L. A. Clarke;Philip L. Tilker;Allen Quoc-Luan Tran;Kevin Harris;Antonio S. Cheng
#t 2003
#c 1
#% 9241
#% 86532
#% 159275
#% 172922
#% 219028
#% 221973
#% 239969
#% 249141
#% 249153
#% 280833
#% 306510
#% 309133
#% 360802
#% 380821
#% 382477
#% 481439
#% 578872
#% 770355
#! We present a storage management layer that facilitates the implementation of parallel information retrieval systems, and related applications, on networks of workstations. The storage management layer automates the process of adding and removing nodes, and implements a dispersed mirroring strategy to improve reliability. When nodes are added and removed, the document collection managed by the system is redistributed for load balancing purposes. The use of dispersed mirroring minimizes the impact of node failures and system modifications on query performance.

#index 730037
#* Towards integrative enterprise knowledge portals
#@ Torsten Priebe;Günther Pernul
#t 2003
#c 1
#% 223781
#% 387427
#% 413603
#% 434015
#% 519415
#% 721290
#% 721377
#! Knowledge portals make an important contribution to enabling enterprise knowledge management by providing users with a consolidated, personalized user interface that allows efficient access to various types of (structured and unstructured) information. Today's portal systems allow combining access modules to different information sources side by side on a single portal webpage. However, there is no interaction between those so called portlets. When a user navigates within one portlet, the others remain unchanged, which means that each source has to be searched individually for relevant information.This paper discusses integration aspects within enterprise knowledge portals and presents an approach for communicating the user context (revealing the user's information need) among portlets, utilizing Semantic Web technologies. For example, the query context of an OLAP portlet, which provides access to structured data stored in a data warehouse, can be used by an information retrieval portlet in order to automatically provide the user with related documents found in the organization's document management system. The paper shortly presents a prototype that we are building to evaluate our approach, demonstrating such an OLAP and information retrieval integration.

#index 730038
#* On the complexity of schema inference from web pages in the presence of nullable data attributes
#@ Guizhen Yang;I. V. Ramakrishnan;Michael Kifer
#t 2003
#c 1
#% 31215
#% 145591
#% 188429
#% 229828
#% 244103
#% 271065
#% 275915
#% 289130
#% 299975
#% 300157
#% 348146
#% 376266
#% 479471
#% 480824
#% 654469
#% 705442
#% 727916
#! An increasingly large number of Web pages are machine-generated by filling in templates with data stored in backend databases. These templates can be viewed as the implicit schemas of those Web pages. The ability to infer the implicit schema from a collection of Web pages is important for scalable data extraction, since the inferred schema can be used to automatically identify schema attributes that are "encoded" in Web pages.However, the task of inferring a "good" schema is complicated due to the existence of nullable (missing) data attributes. Usually if an attribute contains a null value, then it will be omitted in the generated Web page, giving rise to different variations and permutations of layout structures in Web pages that are generated from the same template.In this paper we investigate the complexity of schema inference from Web pages in the presence of nullable data attributes. We introduce the notion of unambiguity as a quality measure for inferred schemas and prove that the problem of inferring "good" (unambiguous) schemas is NP-complete. Our complexity results imply that ambiguity resolution is one of the root causes of the computational difficulty underlying schema inference from Web pages.

#index 730039
#* Text classification from positive and unlabeled documents
#@ Hwanjo Yu;ChengXiang Zhai;Jiawei Han
#t 2003
#c 1
#% 127850
#% 280817
#% 311027
#% 340903
#% 340904
#% 344447
#% 397135
#% 458379
#% 464641
#% 466263
#% 495944
#% 577235
#% 722811
#% 722812
#% 855583
#% 872759
#% 1279295
#! Most existing studies of text classification assume that the training data are completely labeled. In reality, however, many information retrieval problems can be more accurately described as learning a binary classifier from a set of incompletely labeled examples, where we typically have a small number of labeled positive examples and a very large number of unlabeled examples. In this paper, we study such a problem of performing Text Classification WithOut labeled Negative data TC-WON). In this paper, we explore an efficient extension of the standard Support Vector Machine (SVM) approach, called SVMC (Support Vector Mapping Convergence) [17]for the TC-WON tasks. Our analyses show that when the positive training data is not too under-sampled, SVMC significantly outperforms other methods because SVMC basically exploits the natural "gap" between positive and negative documents in the feature space, which eventually corresponds to improving the generalization performance. In the text domain there are likely to exist many gaps in the feature space because a document is usually mapped to a sparse and high dimensional feature space. However, as the number of positive training data decreases, the boundary of SVMC starts overfitting at some point and end up generating very poor results.This is because when the positive training data is too few, the boundary over-iterates and trespasses the natural gaps between positive and negative class in the feature space and thus ends up fitting tightly around the few positive training data.

#index 730040
#* Adding numbers to text classification
#@ Sofus A. Macskassy;Haym Hirsh
#t 2003
#c 1
#% 92483
#% 99397
#% 190581
#% 262085
#% 269217
#% 283138
#% 344447
#% 376266
#% 377895
#% 384911
#% 450454
#% 465895
#% 466234
#% 1289269
#% 1290045
#% 1387555
#% 1499571
#! Many real-world problems involve a combination of both text- and numerical-valued features. For example, in email classification, it is possible to use instance representations that consider not only the text of each message, but also numerical-valued features such as the length of the message or the time of day at which it was sent. Text-classification methods have thus far not easily incorporated numerical features. In earlier work we described an approach for converting numerical features into bags of tokens so that text classification methods can be applied to numerical classification problems, and showed that the resulting learning methods are competitive with traditional numerical classification methods. In this paper we use this as a way to learn on problems that involve a combination of text and numbers. We show that the results outperform competing methods. Further, we show that selecting a best classification method using text-only features and then adding numerical features to the problem (as might happen if numerical features are only later added to a pre existing text-classification problem) gives performance that rivals a more time-consuming approach of evaluating all classification methods using the full set of both text and numerical features.

#index 730041
#* Boosting support vector machines for text classification through parameter-free threshold relaxation
#@ James G. Shanahan;Norbert Roma
#t 2003
#c 1
#% 190581
#% 197394
#% 269218
#% 340904
#% 406493
#% 458379
#% 464612
#% 466229
#% 592108
#! Support vector machine (SVM) learning algorithms focus on finding the hyperplane that maximizes the margin (the distance from the separating hyperplane to the nearest examples) since this criterion provides a good upper bound of the generalization error. When applied to text classification, these learning algorithms lead to SVMs with excellent precision but poor recall. Various relaxation approaches have been proposed to counter this problem including: asymmetric SVM learning algorithms (soft SVMs with asymmetric misclassification costs); uneven margin based learning; and thresholding. A review of these approaches is presented here. In addition, in this paper, we describe a new threshold relaxation algorithm. This approach builds on previous thresholding work based upon the beta-gamma algorithm. The proposed thresholding strategy is parameter free, relying on a process of retrofitting and cross validation to set algorithm parameters empirically, whereas our previous approach required the specification of two parameters (beta and gamma). The proposed approach is more efficient, does not require the specification of any parameters, and similarly to the parameter-based approach, boosts the performance of baseline SVMs by at least 20% for standard information retrieval measures.

#index 730042
#* Multi-resolution disambiguation of term occurrences
#@ Einat Amitay;Rani Nelken;Wayne Niblack;Ron Sivan;Aya Soffer
#t 2003
#c 1
#% 65964
#% 262084
#% 266293
#% 281251
#% 330604
#% 375017
#% 376266
#% 543398
#% 741080
#% 742425
#% 748550
#! We describe a system for extracting mentions of terms such as company and product names, in a large and noisy corpus of documents, such as the World Wide Web. Since natural language terms are highly ambiguous, a significant challenge in this task is disambiguating which occurrences of each term are truly related to the right meaning, and which are not. We describe our approach for disambiguation, and show that it achieves very high accuracy with only limited training. This serves as a necessary first step for applications that strive to do analytics on term mentions.

#index 730043
#* Flexible intrinsic evaluation of hierarchical clustering for TDT
#@ James Allan;Ao Feng;Alvaro Bolivar
#t 2003
#c 1
#% 11646
#% 46809
#% 342660
#% 350859
#% 375017
#% 406493
#% 575571
#% 575572
#% 575579
#! The Topic Detection and Tracking (TDT) evaluation program has included a "cluster detection" task since its inception in 1996. Systems were required to process a stream of broadcast news stories and partition them into non-overlapping clusters. A system's effectiveness was measured by comparing the generated clusters to "truth" clusters created by human annotators. Starting in 2003, TDT is moving to a more realistic model that permits overlapping clusters (stories may be on more than one topic) and encourages the creation of a hierarchy to structure the relationships between clusters (topics). We explore a range of possible evaluation models for this modified TDT clustering task to understand the best approach for mapping between the human-generated "truth" clusters and a much richer hierarchical structure. We demonstrate that some obvious evaluation techniques fail for degenerate cases. For a few others we attempt to develop an intuitive sense of what the evaluation numbers mean. We settle on some approaches that incorporate a strong balance between cluster errors (misses and false alarms) and the distance it takes to travel between stories within the hierarchy.

#index 730044
#* Queueing analysis of relational operators for continuous data streams
#@ Qingchun Jiang;Sharma Chakravarthy
#t 2003
#c 1
#% 100346
#% 286787
#% 300179
#% 378408
#% 428155
#% 835744
#% 993948
#% 993949
#! Currently, stream data processing is an active area of research, which includes everything from algorithms and architectures for stream processing to modelling, and analysis of various components of a stream processing system. In this paper, we present an analysis of relational operators used for stream processing using queueing theory and study behaviors of streaming data in a query processing system. Our approach enables us to compute the fundamental performance metrics of relational operators ---select, project, and join over data streams. Furthermore, this approach establishes a way to find the probability distribution functions of both the number of tuples and the waiting time of tuples in the system. Finally, we designed and implemented a number of experiments to validate the accuracy and effectiveness of our analysis.

#index 730045
#* Raindrop: a uniform and layered algebraic framework for XQueries on XML streams
#@ Hong Su;Jinhui Jian;Elke A. Rundensteiner
#t 2003
#c 1
#% 201873
#% 378388
#% 413650
#% 562456
#% 570880
#% 654477
#% 659998
#% 660004
#% 993949
#% 1015276
#! XML stream applications bring the challenge of efficientlyprocessing queries on sequentially accessible token-based data.While the automata model is naturally suited for pattern matchingon tokenized XML streams, the algebraic model in contrast is awell-established technique for set-oriented processing ofself-contained tuples. However, neither automata nor algebraicmodels are well-equipped to handle both computation paradigms.The goal of the Raindrop project is to accommodate thesetwo paradigms within one algebraic framework to take advantage ofboth. In our query model, both tokenized data and self-containedtuples are supported in a uniform manner. Query plans can beflexibly rewritten using equivalence rules to change whatcomputation is done using tokenized data versus tuples. This paperhighlights the four abstraction levels in Raindrop, namely,semantics-focused plan, stream logical plan, stream physicalplan and execution plan. Various optimization techniquesare provided at each level. The necessity of such a uniform andlayered plan is shown by experimental study

#index 730046
#* Dynamically maintaining frequent items over a data stream
#@ Cheqing Jin;Weining Qian;Chaofeng Sha;Jeffrey X. Yu;Aoying Zhou
#t 2003
#c 1
#% 69791
#% 199575
#% 214073
#% 248812
#% 307424
#% 322884
#% 378388
#% 446438
#% 479795
#% 481290
#% 492912
#% 548479
#% 569754
#% 576119
#% 654461
#% 993960
#! It is challenge to maintain frequent items over a data stream, with a small bounded memory, in a dynamic environment where both insertion/deletion of items are allowed. In this paper, we propose a new novel algorithm, called hCount, which can handle both insertion and deletion of items with a much less memory space than the best reported algorithm. Our algorithm is also superior in terms of precision, recall and processing time. In addition, our approach does not request the preknowledge on the size of range for a data stream, and can handle range extension dynamically. Given a little modification, algorithm hCount can be improved to hCount*, which even owns significantly better performance than before.

#index 730047
#* Bootstrapping for hierarchical document classification
#@ Giordano Adami;Paolo Avesani;Diego Sona
#t 2003
#c 1
#% 169358
#% 169718
#% 234978
#% 266292
#% 280492
#% 309141
#% 387427
#% 420495
#% 420528
#% 431103
#% 464268
#% 464641
#% 465747
#% 466501
#% 466888
#% 479817
#% 482113
#! Managing the hierarchical organization of data is starting to play a key role in the knowledge management community due to the great amount of human resources needed to create and maintain these organized repositories of information. Machine learning community has in part addressed this problem by developing hierarchical supervised classifiers that help maintainers to categorize new resources within given hierarchies. Although such learning models succeed in exploiting relational knowledge, they are highly demanding in terms of labeled examples, because the number of categories is related to the dimension of the corresponding hierarchy. Hence, the creation of new directories or the modification of existing ones require strong investments.This paper proposes a semi-automatic process (interleaved with human suggestions) whose aim is to minimize (simplify) the work required to the administrators when creating, modifying, and maintaining directories. Within this process, bootstrapping a taxonomy with examples represents a critical factor for the effective exploitation of any supervised learning model. For this reason we propose a method for the bootstrapping process that makes a first hypothesis of categorization for a set of unlabeled documents, with respect to a given empty hierarchy of concepts. Based on a revision of Self-Organizing Maps, namely TaxSOM, the proposed model performs an unsupervised classification, exploiting the a-priori knowledge encoded in a taxonomy structure both at the terminological and topological level. The ultimate goal of TaxSOM is to create the premise for successfully training a supervised classifier.

#index 730048
#* Lattice-based tagging using support vector machines
#@ James Mayfield;Paul McNamee;Christine Piatko;Claudia Pearce
#t 2003
#c 1
#% 190581
#% 740916
#% 742092
#% 742424
#% 815178
#% 854819
#% 855108
#! Tagging algorithms have become increasingly important for identifying lexical and semantic features of unstructured text. We describe an approach to lattice-based tagging that estimates joint transition and emission probabilities using support vector machines. The technique offers several advantages over alternative methods, including the ability to accommodate non-local features, support for hundreds of thousands of features, and language-neutrality. We demonstrate the technique on two tagging applications: named entity recognition and part-of-speech tagging.

#index 730049
#* Collaborative filtering with decoupled models for preferences and ratings
#@ Rong Jin;Luo Si;ChengXiang Zhai;Jamie Callan
#t 2003
#c 1
#% 173879
#% 280852
#% 309192
#% 309204
#% 495929
#% 528156
#% 578684
#% 1650569
#! In this paper, we describe a new model for collaborative filtering. The motivation of this work comes from the fact that two users with very similar preferences on items may have very different rating schemes. For example, one user may tend to assign a higher rating to all items than another user. Unlike previous models of collaborative filtering, which determine the similarity between two users only based on their rating performance, our model treats the user's preferences on items separately from the user's rating scheme. More specifically, for each user, we build two separate models: a preference model capturing which items are favored by the user and a rating model capturing how the user would rate an item given the preference information. The similarity of two users is computed based on the underlying preference model, instead of the surface ratings. We compare the new model with several representative previous approaches on two data sets. Experiment results show that the new model outperforms all the previous approaches that are tested consistently on both data sets.

#index 730050
#* Efficient multi-way text categorization via generalized discriminant analysis
#@ Tao Li;Shenghuo Zhu;Mitsunori Ogihara
#t 2003
#c 1
#% 80995
#% 118736
#% 129316
#% 144009
#% 219053
#% 232653
#% 248027
#% 252836
#% 260001
#% 262050
#% 269217
#% 272518
#% 280817
#% 311034
#% 340903
#% 458369
#% 465754
#% 466737
#% 466762
#% 577267
#% 577269
#% 722757
#% 993986
#% 1272365
#! Text categorization is an important research area and has been receiving much attention due to the growth of the on-line information and of Internet. Automated text categorization is generally cast as a multi-class classification problem. Much of previous work focused on binary document classification problems. Support vector machines (SVMs) excel in binary classification, but the elegant theory behind large-margin hyperplane cannot be easily extended to multi-class text classification. In addition, the training time and scaling are also important concerns. On the other hand, other techniques naturally extensible to handle multi-class classification are generally not as accurate as SVM. This paper presents a simple and efficient solution to multi-class text categorization. Classification problems are first formulated as optimization via discriminant analysis. Text categorization is then cast as the problem of finding coordinate transformations that reflects the inherent similarity from the data. While most of the previous approaches decompose a multi-class classification problem into multiple independent binary classification tasks, the proposed approach enables direct multi-class classification. By using Generalized Singular Value Decomposition (GSVD), a coordinate transformation that reflects the inherent class structure indicated by the generalized singular values is identified. Extensive experiments demonstrate the efficiency and effectiveness of the proposed approach.

#index 730051
#* Categorizing web queries according to geographical locality
#@ Luis Gravano;Vasileios Hatzivassiloglou;Richard Lichtenstein
#t 2003
#c 1
#% 67565
#% 116149
#% 136350
#% 268073
#% 268079
#% 282905
#% 330677
#% 348154
#% 375017
#% 397186
#% 445243
#% 466759
#% 480467
#% 1378224
#% 1499571
#! Web pages (and resources, in general) can be characterized according to their geographical locality. For example, a web page with general information about wildflowers could be considered a global page, likely to be of interest to a geographically broad audience. In contrast, a web page with listings on houses for sale in a specific city could be regarded as a local page, likely to be of interest only to an audience in a relatively narrow region. Similarly, some search engine queries (implicitly) target global pages, while other queries are after local pages. For example, the best results for query [wildflowers] are probably global pages about wildflowers such as the one discussed above. However, local pages that are relevant to, say, San Francisco are likely to be good matches for a query [houses for sale] that was issued by a San Francisco resident or by somebody moving to that city. Unfortunately, search engines do not analyze the geographical locality of queries and users, and hence often produce sub-optimal results. Thus query [wildflowers] might return pages that discuss wildflowers in specific U.S. states (and not general information about wildflowers), while query [houses for sale] might return pages with real estate listings for locations other than that of interest to the person who issued the query. Deciding whether an unseen query should produce mostly local or global pages---without placing this burden on the search engine users---is an important and challenging problem, because queries are often ambiguous or underspecify the information they are after. In this paper, we address this problem by first defining how to categorize queries according to their (often implicit) geographical locality. We then introduce several alternatives for automatically and efficiently categorizing queries in our scheme, using a variety of state-of-the-art machine learning tools. We report a thorough evaluation of our classifiers using a large sample of queries from a real web search engine, and conclude by discussing how our query categorization approach can help improve query result quality.

#index 730052
#* Index construction for linear categorisation
#@ Vaughan R. Shanks;Hugh E. Williams
#t 2003
#c 1
#% 55490
#% 217268
#% 219052
#% 253191
#% 260001
#% 279755
#% 280817
#% 290703
#% 340904
#% 340905
#% 340928
#% 344447
#% 375017
#% 397151
#% 406493
#% 413611
#% 420077
#% 458379
#% 465747
#% 465754
#% 465895
#% 582447
#% 655485
#% 993986
#! Categorisation is a useful method for organising documents into subcollections that can be browsed or searched to more accurately and quickly meet information needs. On the Web, category-based portals such as Yahoo! and DMOZ are extremely popular: DMOZ is maintained by over 56,000 volunteers, is used as the basis of the popular Google directory, and is perhaps used by millions of users each day. Support Vector Machines (SVM) is a machine-learning algorithm which has been shown to be highly effective for automatic text categorisation. However, a problem with iterative training techniques such as SVM is that during their learning or training phase, they require the entire training collection to be held in main-memory; this is infeasible for large training collections such as DMOZ or large news wire feeds. In this paper, we show how inverted indexes can be used for scalable training in categorisation, and propose novel heuristics for a fast, accurate, and memory efficient approach. Our results show that an index can be constructed on a desktop workstation with little effect on categorisation accu-racy compared to a memory-based approach. We conclude that our techniques permit automatic categorisation using very large train-ing collections, vocabularies, and numbers of categories.

#index 730053
#* Light-weight xPath processing of XML stream with deterministic automata
#@ Makoto Onizuka
#t 2003
#c 1
#% 3888
#% 70370
#% 300153
#% 300179
#% 342372
#% 390964
#% 465061
#% 479465
#% 480296
#% 480649
#% 487257
#% 570879
#% 654476
#% 659987
#% 740916
#! Several applications based on XML stream processing have recently emerged, such as those for air traffic control and the selective dissemination of information (SDI). Their common need is to process a large number of XPath expressions in continuous XML streams at high throughput.This paper proposes four techniques for XPath expression processing based on Deterministic Finite Automata (DFA) for two purposes: to improve the memory usage efficiency of the automata and to support the processing of branching XPath expressions. The first technique, called n-DFA, clusters the given XPath expressions into n clusters to reduce the number of DFA states. The second, called shared NFA state table, lets the Non-Deterministic Finite Automata (NFA) state set be shared among the DFA states. Our experiments show that memory usage in an 8-DFA can, with the shared NFA state table, be reduced to 1/40th that of the original 1-DFA. The optimized NFA conversion and general XPath expression processing algorithm techniques contribute to the processing of branching XPath expressions efficiently; overall performance is better than is possible with earlier approaches.

#index 730054
#* Efficient ordering for XML data
#@ Damien K. Fisher;Franky Lam;William M. Shui;Raymond K. Wong
#t 2003
#c 1
#% 23651
#% 204662
#% 236416
#% 281149
#% 333981
#% 378412
#% 397366
#% 411759
#% 464720
#% 479465
#% 479803
#% 479806
#% 480489
#% 548643
#% 570875
#% 598374
#% 1015283
#! With the increasing popularity of XML, there arises the need for managing and querying information in this form. Several query languages, such as XQuery, have been proposed which return their results in document order. However, most recent efforts focused on query optimization have disregarded order. This paper presents a simple yet elegant method to maintain document ordering for XML data. Analysis of our method shows that it is indeed efficient and scalable, even for changing data.

#index 730055
#* Building XML statistics for the hidden web
#@ Ashraf Aboulnaga;Jeffrey F. Naughton
#t 2003
#c 1
#% 210176
#% 273911
#% 397364
#% 397379
#% 465018
#% 479951
#% 480479
#% 480488
#% 565470
#% 993968
#% 993970
#! There have been several techniques proposed for building statistics for static XML data. However, very little work has been done in the area of building XML statistics for data sources that export XML views of data that is stored in relational or other databases. For such data sources, we need statistics that are built in an on-line manner, by observing the XML queries to the data sources and their results. In this paper, we present a technique for building on-line XML statistics by observing the XPath queries issued to a data source and their result sizes. These XPath queries select parts of the virtual XML document representing the XML view of the data at the data source. We convert these XPath queries to a more abstract and generalized form that we call annotated path expressions. We present a technique for storing these annotated path expressions and information about their selectivity for use in estimating the selectivity of future XPath queries. We also present an experimental evaluation of our proposed approach.

#index 730056
#* Ontologies for semantically interoperable systems
#@ Leo Obrst
#t 2003
#c 1
#% 278426
#% 344371
#% 344391
#% 410955
#% 523704
#% 644109
#% 731211
#! In this paper, we discuss the use of ontologies for semantic interoperability and integration. We argue that information technology has evolved into a world of largely loosely coupled systems and as such, needs increasingly more explicit, machine-interpretable semantics. Ontologies in the form of logical domain theories and their knowledge bases offer the richest representations of machine-interpretable semantics for systems and databases in the loosely coupled world, thus ensuring greater semantic interoperability and integration. Finally, we discuss how ontologies support semantic interoperability in the real, commercial and governmental world.

#index 730057
#* The virGIS WFS-based spatial mediation system
#@ Omar Boucelma;Jean-Yves Garinet;Zoé Lacroix
#t 2003
#c 1
#% 116303
#% 229827
#% 238757
#% 248799
#% 333245
#% 443235
#% 479449
#! The proliferation of spatial data on the Internet is beginning to allow a much wider access to data currently available in various Geographic Information Systems (GIS). In order to provide effective spatial database integration, we need to provide flexible and powerful GIS data integration solutions. Indeed, GIS are highly heterogeneous: not only they differ by their data representations, but they also offer radically different query languages. A GIS mediation approach should provide an integrated view of the data supplied by all sources, and a geographical query language to access and manipulate integrated data.

#index 730058
#* Securely sharing neuroimagery
#@ Kenneth Smith;Vipin Swarup;Sushil Jajodia;Donald Faatz;Todd Cornett;Jeff Hoyt
#t 2003
#c 1
#% 13400
#% 411251

#index 730059
#* Representing interests as a hyperlinked document collection
#@ Michelle Fisher;Richard Everson
#t 2003
#c 1
#% 218978
#% 268079
#% 280819
#% 290830
#% 419139
#% 466574
#% 1387536
#% 1499473
#! We describe a latent variable model for representing a user's interests as a hyperlinked document collection. By collecting hyper-text documents that a user views, creates or updates whilst at their computer, we are able to use not only the content of these documents but also the inter-connectivity of the collection to model the user's interests. The model uses Probabilistic Latent Semantic Analysis and Probabilistic Hypertext Induced Topic Selection and decomposes the user's document collection into a set of factors each of which represents a user's interest. This model can be used to personalise information access tasks such as a personalised search engine or a personalised news service. Our latent variable model's performance is compared with that of a more conventional vector space clustering algorithm.

#index 730060
#* Automated index management for distributed web search
#@ Rinat Khoussainov;Nicholas Kushmerick
#t 2003
#c 1
#% 223835
#% 287463
#% 301565
#% 341699
#% 375017
#% 527987
#% 715676
#% 1289288
#% 1499474
#% 1837204
#! Distributed heterogeneous search systems are an emerging phenomenon in Web search, in which independent topic-specific search engines provide search services, and metasearchers distribute user's queries to only the most suitable search engines. Previous research has investigated methods for engine selection and merging of search results (i.e. performance improvements from the user's perspective). We focus instead on performance from the service provider's point of view (e.g, income from queries processed vs. resources used to answer them). We consider a scenario in which individual search engines compete for user queries by choosing which documents (topics) to index. The difficulty here stems from the fact that the utilities of local engine actions should depend on the uncertain actions of competitors. Thus, naive strategies (e.g, blindly indexing lots of popular documents) are ineffective. We model the competition between search engines as a stochastic game, and propose a reinforcement learning approach to managing search index contents. We evaluate our approach using a large log of user queries to 47 real search engines.

#index 730061
#* Combining link-based and content-based methods for web document classification
#@ Pável Calado;Marco Cristo;Edleno Moura;Nivio Ziviani;Berthier Ribeiro-Neto;Marcos André Gonçalves
#t 2003
#c 1
#% 44876
#% 111303
#% 169718
#% 219047
#% 248810
#% 268079
#% 279109
#% 281209
#% 287284
#% 290830
#% 299941
#% 309142
#% 348178
#% 376266
#% 413663
#% 430761
#% 447947
#% 458379
#% 464267
#% 466896
#% 617186
#% 764561
#% 1387536
#% 1835183

#index 730062
#* HyperThesis: the gRNA spell on the curse of bioinformatics applications integration
#@ Sourav S. Bhowmick;Vivek Vedagiri;Amey Laud
#t 2003
#c 1
#% 431019
#% 589446
#% 772135
#% 994011
#! In this paper, we describe a graphical workflow management system called HyperThesis to address the challenges of integrating bioinformatics applications. HyperThesis is an integral component of the Genomics Research Network Architecture (gRNA). The gRNA was designed and developed to address the challenges of developing new bioinformatics applications. Specifically, HyperThesis makes constructing workflows (pipelines of execution of applications) in the gRNA fast and intuitive for biologists and bio-programmers alike. It provides a large repository of interconnectable, parameterized workflow components for processing and relating diverse biological data and software programs. It also enables us to add new workflow components as new algorithms develop in ones area of interest. HyperThesis has been fully implemented using Java.

#index 730063
#* Information extraction from biomedical literature: methodology, evaluation and an application
#@ L. Venkata Subramaniam;Sougata Mukherjea;Pankaj Kankar;Biplav Srivastava;Vishal S. Batra;Pasumarti V. Kamesam;Ravi Kothari
#t 2003
#c 1
#% 755821
#% 854570
#! Journals and conference proceedings represent the dominant mechanisms of reporting new biomedical results. The unstructured nature of such publications makes it difficult to utilize data mining or automated knowledge discovery techniques. Annotation (or markup) of these unstructured documents represents the first step in making these documents machine analyzable. In this paper we first present a system called BioAnnotator for identifying and annotating biological terms in documents. BioAnnotator uses domain based dictionary look-up for recognizing known terms and a rule engine for discovering new terms. The combination and dictionary look-up and rules result in good performance (87% precision and 94% recall on the GENIA 1.1 corpus for extracting general biological terms based on an approximate matching criterion). To demonstrate the subsequent mining and knowledge discovery activities that are made feasible by BioAnnotator, we also present a system called MedSummarizer that uses the extracted terms to identify the common concepts in a given group of genes.

#index 730064
#* Mining multiple phenotype structures underlying gene expression profiles
#@ Chun Tang;Aidong Zhang
#t 2003
#c 1
#% 248792
#% 273890
#% 328301
#% 397641
#% 413549
#% 443531
#% 469422
#% 469425
#% 659967
#! DNA microarray technology is now widely used in basic biomedical research for mRNA expression profiling and are increasingly being used to explore patterns of gene expression in clinical research. Automatically detecting phenotype structures from gene expression profiles can provide deep insight into the nature of many diseases as well as lead in the development of new drugs. While most of the previous studies focus on only mining empirical phenotype structure which the experiment controls, it is also interesting to detect possible hidden phenotype structures underlying gene expression profiles.Since the number of samples is usually limited, such data sets are very sparse in high-dimensional gene space. Furthermore, most of the genes of interest are buried in large amount of noise. Unsupervised phenotype structure discovery of such sparse high-dimensional data sets present interesting but challenging problems. In this paper, we propose the model of simultaneously mining both empirical and hidden phenotype structures from gene expression data. We demonstrate the effectiveness and efficiency of the proposed method on various real-world data sets.

#index 730065
#* Efficient query evaluation using a two-level retrieval process
#@ Andrei Z. Broder;David Carmel;Michael Herscovici;Aya Soffer;Jason Zien
#t 2003
#c 1
#% 65964
#% 69791
#% 169817
#% 198335
#% 228097
#% 262099
#% 262118
#% 268079
#% 319273
#% 322884
#% 463737
#! We present an efficient query evaluation method based on a two level approach: at the first level, our method iterates in parallel over query term postings and identifies candidate documents using an approximate evaluation taking into account only partial information on term occurrences and no query independent factors; at the second level, promising candidates are fully evaluated and their exact scores are computed. The efficiency of the evaluation process can be improved significantly using dynamic pruning techniques with very little cost in effectiveness. The amount of pruning can be controlled by the user as a function of time allocated for query evaluation. Experimentally, using the TREC Web Track data, we have determined that our algorithm significantly reduces the total number of full evaluations by more than 90%, almost without any loss in precision or recall. At the heart of our approach there is an efficient implementation of a new Boolean construct called WAND or Weak AND that might be of independent interest.

#index 730066
#* Operational requirements for scalable search systems
#@ Abdur Chowdhury;Greg Pass
#t 2003
#c 1
#% 1270
#% 1821
#% 35207
#% 65954
#% 104439
#% 109209
#% 109210
#% 118037
#% 167607
#% 172898
#% 194245
#% 194246
#% 204008
#% 219013
#% 249153
#% 268079
#% 280832
#% 287463
#% 290703
#% 309139
#% 339621
#% 397150
#% 397151
#% 481748
#% 508059
#% 616995
#% 617172
#% 635851
#% 677664
#% 703681
#! Prior research into search system scalability has primarily addressed query processing efficiency [1, 2, 3] or indexing efficiency [3], or has presented some arbitrary system architecture [4]. Little work has introduced any formal theoretical framework for evaluating architectures with regard to specific operational requirements, or for comparing architectures beyond simple timings [5] or basic simulations [6, 7]. In this paper, we present a framework based upon queuing network theory for analyzing search systems in terms of operational requirements. We use response time, throughput, and utilization as the key operational characteristics for evaluating performance. Within this framework, we present a scalability strategy that combines index partitioning and index replication to satisfy a given set of requirement.

#index 730067
#* Online duplicate document detection: signature reliability in a dynamic retrieval environment
#@ Jack G. Conrad;Xi S. Guo;Cindy P. Schriber
#t 2003
#c 1
#% 73489
#% 111456
#% 132779
#% 169779
#% 170465
#% 201935
#% 255137
#% 268079
#% 306497
#% 340146
#% 345087
#% 397124
#% 413577
#% 504572
#! As online document collections continue to expand, both on the Web and in proprietary environments, the need for duplicate detection becomes more critical. Few users wish to retrieve search results consisting of sets of duplicate documents, whether identical duplicates or close matches. Our goal in this work is to investigate the phenomenon and determine one or more approaches that minimize its impact on search results. Recent work has focused on using some form of signature to characterize a document in order to reduce the complexity of document comparisons. A representative technique constructs a 'fingerprint' of the rarest or richest features in a document using collection statistics as criteria for feature selection. One of the challenges of this approach, however, arises from the fact that in production environments, collections of documents are always changing, with new documents, or new versions of documents, arriving frequently, and other documents periodically removed. When an enterprise proceeds to freeze a training collection in order to stabilize the underlying repository of such features and its associated collection statistics, issues of coverage and completeness arise. We show that even with very large training collections possessing extremely high feature correlations before and after updates, underlying fingerprints remain sensitive to subtle changes. We explore alternative solutions that benefit from the development of massive meta-collections made up of sizable components from multiple domains. This technique appears to offer a practical foundation for fingerprint stability. We also consider mechanisms for updating training collections while mitigating signature instability. Our research is divided into three parts. We begin with a study of the distribution of duplicate types in two broad-ranging news collections consisting of approximately 50 million documents. We then examine the utility of document signatures in addressing identical or nearly identical duplicate documents and their sensitivity to collection updates. Finally, we investigate a flexible method of characterizing and comparing documents in order to permit the identification of non-identical duplicates. This method has produced promising results following an extensive evaluation using a production-based test collection created by domain experts.

#index 730068
#* Hierarchical graph indexing
#@ James Abello;Yannis Kotidis
#t 2003
#c 1
#% 44638
#% 86950
#% 153260
#% 208047
#% 248805
#% 283833
#% 286237
#% 309749
#% 321455
#% 342955
#% 411694
#% 427199
#% 434615
#% 461925
#% 479473
#% 480651
#% 641162
#% 1214054
#! Traffic analysis, in the context of Telecommunications or Internet and Web data, is crucial for large network operations. Data in such networks is often provided as large graphs with hundreds of millions of vertices and edges. We propose efficient techniques for managing such graphs at the storage level in order to facilitate its processing at the interface level(visualization). The methods are based on a hierarchical decomposition of the graph edge set that is inherited from a hierarchical decomposition of the vertex set. Real time navigation is provided by an efficient two level indexing schema called the gkd*-tree. The first level is a variation of a kd-tree index that partitions the edge set in a way that conforms to the hierarchical decomposition and the data distribution (the gkd-tree). The second level is a redundant R-tree that indexes the leaf pages of the gkd-tree. We provide computational results that illustrate the superiority of the gkd-tree against conventional indexes like the kd-tree and the R*-tree both in creation as well as query response times.

#index 730069
#* iTopN: incremental extraction of the N most visible objects
#@ Linas Bukauskas;Leo Mark;Edward Omiecinski;Michael H. Böhlen
#t 2003
#c 1
#% 86950
#% 300174
#% 397377
#% 411694
#% 427084
#% 427199
#% 464195
#% 464841
#% 480830
#% 481599
#% 481956
#% 527187
#% 720581
#! The visual exploration of large databases calls for a tight coupling of database and visualization systems. Current visualization systems typically fetch all the data and organize it in a scene tree, which is then used to render the visible data. For immersive data explorations, where an observer navigates in a potentially huge data space and explores selected data regions this approach is inadequate. A scalable approach is to make the database system observer-aware and exchange the data that is visible and most relevant to the observer.In this paper we present iTopN an incremental algorithm for extracting the most visible objects relative to the current position of the observer. We implement iTopN and compare it to an improved version of the R-tree that extends LRU with the caching of the top levels of the R-tree (LW-LRU). Our experiments show that iTopN is orders of magnitude faster than LW-LRU given the same amount of memory. Our experiments also show that for LW-LRU to perform as fast as iTopN it needs three times as much memory.

#index 730070
#* Time-based language models
#@ Xiaoyan Li;W. Bruce Croft
#t 2003
#c 1
#% 262096
#% 280850
#% 280864
#% 309096
#% 340883
#% 340899
#% 340901
#% 397126
#! We explore the relationship between time and relevance using TREC ad-hoc queries. A type of query is identified that favors very recent documents. We propose a time-based language model approach to retrieval for these queries. We show how time can be incorporated into both query-likelihood models and relevance models. These models were used for experiments comparing time-based language models to heuristic techniques for incorporating document recency in the ranking. Our results show that time-based models perform as well as or better than the best of the heuristic techniques.

#index 730071
#* Exploiting syntactic structure of queries in a language modeling approach to IR
#@ Munirathnam Srikanth;Rohini Srihari
#t 2003
#c 1
#% 54405
#% 81669
#% 252472
#% 262096
#% 280850
#% 280851
#% 280864
#% 309168
#% 340899
#% 340901
#% 340948
#% 375017
#% 387427
#% 397205
#% 413593
#% 642974
#% 643042
#% 742084
#% 742095
#% 747913
#! Natural Language Processing (NLP) techniques have been explored to enhance the performance of Information Retrieval (IR) methods with varied results. Most efforts in using NLP techniques have been to identify better index terms for representing documents. This use in the indexing phase of IR has implicit effect on retrieval performance. However, the explicit use of NLP techniques during the retrieval or information seeking phase has been restricted to interactive or dialogue systems. Recent advances in IR are based on using Statistical Language Models (SLM) to represent documents and ranking them based on their model generating a given user query. This paper presents a novel method for using NLP techniques on user queries, specifically, a syntactic parse of a query, in the statistical language modeling approach to IR. In the proposed method, named Concept Language Models, a query is viewed as a sequence of concepts and a concept as a sequence terms. The paper presents different approximations to estimate the concept and term probabilities and compute the query likelihood estimate for documents. Some empirical results on TREC test collections comparing Concept Language Models with smoothed N-gram language models are presented.

#index 730072
#* A unified model for metasearch, pooling, and system evaluation
#@ Javed A. Aslam;Virgiliu Pavlu;Robert Savell
#t 2003
#c 1
#% 165663
#% 169774
#% 194276
#% 232703
#% 235377
#% 262034
#% 262097
#% 262102
#% 340934
#% 340936
#% 413613
#! We present a unified model which, given the ranked lists of documents returned by multiple retrieval systems in response to a given query, simultaneously solves the problems of (1) fusing the ranked lists of documents in order to obtain a high-quality combined list (metasearch); (2) generating document collections likely to contain large fractions of relevant documents (pooling); and (3) accurately evaluating the underlying retrieval systems with small numbers of relevance judgments (efficient system assessment). Our approach is based on the Hedge algorithm for on-line learning. In effect, our proposed system "learns" which documents are likely to be relevant from a sequence of on-line relevance judgments. In experiments using TREC data, our methodology is shown to outperform standard methods for metasearch, pooling, and system evaluation, often remarkably so.

#index 730073
#* Intelligent metasearch engine for knowledge management
#@ Eui-Hong Han;George Karypis;Doug Mewhort;Keith Hatchard
#t 2003
#c 1
#% 118771
#% 136350
#% 230432
#% 252753
#% 268078
#% 280817
#% 304423
#% 348156
#% 420464
#% 458379
#% 1499473
#! The explosive growth of available information sources and the resulting information overload pose several problems for users in many business organizations and educational institutions. First, searching through several information sources, one at a time, is a source of enormous frustration for users. Second, top-ranked documents in search results are frequently irrelevant to what users are interested in. To address these problems, we have developed ixmeta™, a powerful metasearch engine that gathers, evaluates, ranks, and reports the most relevant results from multiple information sources, including library catalogs, proprietary databases, intranets, and Web search engines. In addition to basic metasearch capabilities, ixmetafind uses personalization and clustering techniques to find the most relevant results for users. In this paper, we briefly describe technologies used in ixmetafind and present pinpoint™ from Sagebrush Corporation, the smart research tool™ in the kindergarten through twelfth grade (K-12) school environment. Pinpoint showcases ixmetafind in the knowledge management domain of the K-12 school environment.

#index 730074
#* SQL text parsing for information retrieval
#@ David Holmes
#t 2003
#c 1
#% 224702
#% 269899
#% 324015
#% 336784
#% 660339
#! The concept of using a relational database to perform information retrieval (IR) search functions is well established. Prior work demonstrates the capability to perform common functions and advanced ranking algorithms using standard, unchanged SQL. The previous work does not address the preprocessing of unstructured text within the relational model. In fact, the parsing of the unstructured data into a structured data set was done outside of the database, usually using sequential programming languages such as C. This work proves that IR preprocessing does not require proprietary application code to build the framework necessary for searching document databases. Furthermore, the resulting environment is relational and integrates with other data sources within an organization.

#index 730075
#* Industrial evaluation of a highly-accurate academic IR system
#@ Tahia Infantes-Morris;Philip J. Bernhard;Kevin L. Fox;Gary J. Faulkner;Kristina Stripling
#t 2003
#c 1
#% 359132
#% 385946
#% 387427
#! In this paper we report the results of an independent experimental evaluation of an information retrieval (IR) system developed at the Illinois Institute of Technology (IIT). The system, which is called the Advanced Information Retrieval Engine (AIRE), consists of a set of tools and utilities providing indexing, extraction, searching and visualization. We evaluated AIRE on three data sets from the Text REtrieval Conference (TREC) - TREC 8, 9 and 10. Overall, our results indicate that AIRE is a highly accurate IR system. Compared with results published by IIT, in our experiments AIRE consistently scored higher in recall. AIRE also scored higher in precision, but only for automatic tasks. In manual tasks, AIRE scored lower in precision in our experiments, but we attributed that to factors external to AIRE. Our final conclusion is that AIRE is a highly accurate IR system.

#index 730076
#* An approach for implicitly detecting information needs
#@ Ryen W. White;Joemon M. Jose;Ian Ruthven
#t 2003
#c 1
#% 292179
#% 340974
#% 420474
#% 717120
#! Searchers can have problems devising queries that accurately express their, often dynamic, information needs. In this paper we describe an adaptive approach that uses unobtrusive monitoring of interaction to help alleviate such problems and support searchers in their seeking. The approach we propose implicitly selects terms to better represent information needs, gathers evidence on potential changes in these needs, and uses this evidence to tailor the result presentation accordingly. A user evaluation of an interface implementing our approach, presented in [7], shows it can select terms that approximate current information needs and provide evidence to track changes in these needs.

#index 730077
#* Summarization evaluation using relative utility
#@ Dragomir R. Radev;Daniel Tam
#t 2003
#c 1
#% 280835
#% 853647
#% 853652
#! We present a series of experiments to demonstrate the validity of Relative Utility (RU) as a measure for evaluating extractive summarizers. RU is applicable in both single-document and multi-document summarization, is extendable to arbitrary compression rates with no extra annotation effort, and takes into account both random system performance and interjudge agreement. Our results using the JHU summary corpus indicate that RU is a reasonable and often superior alternative to several common evaluation metrics.

#index 730078
#* Extracting unstructured data from template generated web documents
#@ Ling Ma;Nazli Goharian;Abdur Chowdhury;Misun Chung
#t 2003
#c 1
#% 80995
#% 167553
#% 345087
#% 348180
#% 385946
#% 397415
#% 413616
#% 546695
#% 577257
#% 654469
#! We propose a novel approach that identifies web page templates and extracts the unstructured data. Extracting only the body of the page and eliminating the template increases the retrieval precision for the queries that generate irrelevant results. We believe that by reducing the number of irrelevant results; the users are encouraged to go back to a given site to search. Our experimental results on several different web sites and on the whole cnnfn collection demonstrate the feasibility of our approach.

#index 730079
#* Techniques for efficient fragment detection in web pages
#@ Lakshmish Ramaswamy;Arun Iyengar;Ling Liu;Fred Douglis
#t 2003
#c 1
#% 348180
#% 397357
#% 616528
#% 660272
#! The existing approaches to fragment-based publishing, delivery and caching of web pages assume that the web pages are manually fragmented at their respective web sites. However manual fragmentation of web pages is expensive, error prone, and not scalable. This paper proposes a novel scheme to automatically detect and flag possible fragments in a web site. Our approach is based on an analysis of the web pages dynamically generated at given web sites with respect to their information sharing behavior, personalization characteristics and change patterns.

#index 730080
#* MASS: a multi-axis storage structure for large XML documents
#@ Kurt Deschler;Elke Rundensteiner
#t 2003
#c 1
#% 252608
#% 300153
#% 378412
#% 397358
#% 397359
#% 397366
#% 479465
#% 479806
#% 480489
#% 480656
#! Effective indexing for XML must consider both the query requirements of the XPath language and the dynamic nature of XML. We introduce MASS, a Multiple Axis Storage Structure, to provide scalable indexing for XPath expressions with guaranteed update performance. We describe the building blocks of MASS and provide results that demonstrate MASS's scalability. We show that MASS can outperform other state-of-the-art XML indexing solutions, even with constrained system resources.

#index 730081
#* User assisted text classification and knowledge management
#@ Anne Kao;Lesley Quach;Steve Poteet;Steve Woods
#t 2003
#c 1
#% 332533
#% 344447
#% 386027
#% 529524
#% 771353
#! While there are many aspects to managing corporate knowledge, one key issue is how to organize corporate documents into categories of interest. In this paper, we focus on using user assisted text classification in conjunction with a web portal, multiple document management systems and an ontology, to provide a powerful solution for organizing information about a company's technology. We propose a system that interacts with an author using an automatic text classifier to suggest controlled keywords to be used as metadata. The proposed approach does not require professional librarians or that the end users have extensive training. The use of a controlled vocabulary allows for a more consistent description of corporate documents, and promotes easier access by people across the company. It is easier to find similar documents which use different nomenclature. Finally, the interactive nature of the system results in a more correct and precise description of each document than a fully automatic system would.

#index 730082
#* Expertise identification using email communications
#@ Christopher S. Campbell;Paul P. Maglio;Alex Cozzi;Byron Dom
#t 2003
#c 1
#% 146494
#% 260775
#% 290830
#% 549563
#% 662755
#% 1499467
#! A common method for finding information in an organization is to use social networks---ask people, following referrals until someone with the right information is found. Another way is to automatically mine documents to determine who knows what. Email documents seem particularly well suited to this task of "expertise location", as people routinely communicate what they know. Moreover, because people explicitly direct email to one another, social networks are likely to be contained in the patterns of communication. Can these patterns be used to discover experts on particular topics? Is this approach better than mining message content alone? To find answers to these questions, two algorithms for determining expertise from email were compared: a content-based approach that takes account only of email text, and a graph-based ranking algorithm (HITS) that takes account both of text and communication patterns. An evaluation was done using email and explicit expertise ratings from two different organizations. The rankings given by each algorithm were compared to the explicit rankings with the precision and recall measures commonly used in information retrieval, as well as the d' measure commonly used in signal-detection theory. Results show that the graph-based algorithm performs better than the content-based algorithm at identifying experts in both cases, demonstrating that the graph-based algorithm effectively extracts more information than is found in content alone.

#index 730083
#* A visual interface technique for exploring OLAP data with coordinated dimension hierarchies
#@ Mark Sifer
#t 2003
#c 1
#% 102726
#% 268066
#% 282427
#% 308862
#% 438504
#% 577222
#% 717323
#! Multi-dimensional data occurs in many domains while a wide variety of text based and visual interfaces for querying such data exists. But many of these interfaces are not applicable to OLAP, as they do not support use of dimension hierarchies for selection and aggregation. We introduce an interface technique which supports visual querying of OLAP data, that has been implemented in the SGViewer tool. It is based on a data graph rather than a data cube representation of the data. Our interface presents each dimension hierarchy in a zoomable panel which supports selection and aggregation at multiple levels. Users explore data and query it by making selections in several dimension views. Three view coordinations are identified; progressive, global and result only. Our main contribution, the progressive view coordination provides better support for query refinement than existing interfaces, by helping users decide the next query step with intermediate result overviews, and by helping users change a previous selection decision with retained selection context views. Our interface technique is demonstrated with a web log dataset of visits organised into time, download, visitor and referrer address dimensions.

#index 730084
#* estWin: adaptively monitoring the recent change of frequent itemsets over online data streams
#@ Joong Hyuk Chang;Won Suk Lee
#t 2003
#c 1
#% 273898
#% 342689
#% 379445
#% 481290
#% 993960
#! Knowledge embedded in a data stream is likely to be changed as time goes by. Consequently, identifying the recent change of the knowledge quickly can provide valuable information for the analysis of the data stream. However, most of mining algorithms or frequency approximation algorithms for a data stream do not able to extract the recent change of information in a data stream adaptively. This paper proposes a sliding window-based method that finds recently frequent itemsets over an online data stream adaptively. The size of a window defines a desired life-time of the information in a newly generated transaction. Consequently, only recently generated transactions in the range of the window are considered to find the frequent itemsets of a data stream.

#index 730085
#* Automatic identification of best entry points for focused structured document retrieval
#@ Mounia Lalmas;Jane Reid
#t 2003
#c 1
#% 340914
#% 458404
#% 503227
#% 1387539
#! Focussed structured document retrieval employs the concept of best entry points (BEPs), which are intended to provide optimal starting-points from which users can browse to relevant document components. This paper describes two small-scale studies, using experimental data from the Shakespeare user study, which developed and evaluated different approaches to the problem of automatic identification of BEPs.

#index 730086
#* Auto-generation of topic hierarchies for web images from users' perspectives
#@ Pu-Jen Cheng;Lee-Feng Chien
#t 2003
#c 1
#% 46803
#% 280404
#% 310567
#% 330617
#% 589914
#% 629672
#! In this paper, we propose an approach to automatically generating a Yahoo!-like topic hierarchy for organizing Web images from users' perspectives. Relatively little effort has been devoted towards providing such a taxonomy simultaneously considering users' image requests for semantic and visual information. Based on the characteristic that a Web-image query may be refined by various attributes, the proposed approach hierarchically groups similar queries from search engine logs into topic classes at different semantic levels. The generated topic hierarchy has the advantages of organizing image data from users' perspectives for browsing, searching, annotation and users' needs analysis.A series of experiments have been conducted on real-world image search engine logs. Experimental results show that the proposed approach is feasible to generate topic hierarchies for Web images. Moreover, the generated hierarchy has been successfully applied to analysis of users' search interests, which have more focuses on some specific domains when compared with document requests.

#index 730087
#* Tracking changes in user interests with a few relevance judgments
#@ Dwi H. Widyantoro;Thomas R. Ioerger;John Yen
#t 2003
#c 1
#% 36672
#% 194985
#% 214321
#% 262084
#% 306468
#% 323130
#% 629674
#! Keeping track of changes in user interests from a document stream with a few relevance judgments is not an easy task. To tackle this problem, we propose a novel method that integrates (1) pseudo-relevance feedback mechanism, (2) assumption about the persistence of user interests and (3) incremental method for data clustering. This approach has been empirically evaluated using Reuters-21578 corpus in a setting for information filtering. The experiment results reveal that it significantly improves the performances of existing user-interest-tracking systems without requiring additional, actual relevance judgments.

#index 730088
#* Digital annotation of printed documents
#@ Corsin Decurtins;Moira C. Norrie;Beat Signer
#t 2003
#c 1
#% 232895
#% 237318
#% 249090
#% 330770
#% 489178
#% 504168
#% 535026
#% 579439
#% 836020
#! We present a general model and information server for the digital annotation of printed documents. The resulting annotation framework supports both informal and structured annotations as well as context-dependent services. A demonstrator application for mammography that features both enhanced writing and reading activities is described.

#index 730089
#* The link prediction problem for social networks
#@ David Liben-Nowell;Jon Kleinberg
#t 2003
#c 1
#% 220708
#% 268079
#% 406493
#% 433988
#% 577219
#% 577273
#% 786511
#! Given a snapshot of a social network, can we infer which new interactions among its members are likely to occur in the near future? We formalize this question as the link prediction problem, and develop approaches to link prediction based on measures the "proximity" of nodes in a network. Experiments on large co-authorship networks suggest that information about future interactions can be extracted from network topology alone, and that fairly subtle measures for detecting node proximity can outperform more direct measures.

#index 730090
#* Relevant query feedback in statistical language modeling
#@ Ramesh Nallapati;Bruce Croft;James Allan
#t 2003
#c 1
#% 183255
#% 262096
#% 280851
#% 290830
#% 321876
#% 387427
#% 406493
#! In traditional relevance feedback, researchers have explored relevant document feedback, wherein, the query representation is updated based on a set of relevant documents returned by the user. In this work, we investigate relevant query feedback, in which we update a document's representation based on a set of relevant queries. We propose four statistical models to incorporate relevant query feedback.To validate our models, we considered anchor text of incoming links to a given document as feedback queries and performed experiments on the home-page retrieval task of TREC 2001. Our results show that three of our four models outperform the query-likelihood baseline by at least 35% in MRR score on a test set.

#index 730091
#* Ontology-driven peer profiling in peer-to-peer enabled semantic web
#@ Olena Parkhomenko;Yugyung Lee;E. K. Park
#t 2003
#c 1
#% 309789
#% 449212
#% 449214
#% 449215
#% 449217
#% 644923
#! Peer-to-peer (P2P) systems and Semantic Web are two novel technologies that face a lot of shortcomings if considered as isolated paradigms. We present an approach that utilizes ontologies to set up a peer profile containing all the data, necessary for peer-to-peer interoperability. Using this profile can help eliminate some major issues persistent in current P2P networks, such as security, resource aggregation, group management. We also consider applications of peer profiling for Semantic Web built on P2P networks, such as an improved semantic search for resources, not explicitly published on the Web, but available in a P2P system. We develop the ontology-based peer profile in RDF format and demonstrate its manifold benefits for peer communication and knowledge discovery in both P2P networks and Semantic Web.

#index 730092
#* Finding more useful information faster from web search results
#@ Yi-fang Brook Wu;Latha Shankar;Xin Chen
#t 2003
#c 1
#% 234793
#% 262504
#% 280849
#% 287215
#% 340951
#% 438557
#! In this paper, we propose a prototype system for automatic generation of concept hierarchies to be used as an overview of search results. The system sends a user's query to five search engines and receives a returned list of relevant web pages. The system then extracts query-oriented concept terms from snippets that come with the returned hits. Concept terms are organized into a concept hierarchy using a co-occurrence-based classification technique. Finally, concepts in returned documents are dynamically highlighted according to terms in the selected concept branch that lead to the chosen document. The user study shows that concept hierarchies do provide easy navigation and browsing of web returned documents. The results also show that users can find a document of interest no matter how low it is ranked in the retrieved list.

#index 730093
#* An evaluation of the incorporation of a semantic network into a multidimensional retrieval engine
#@ Jinho Lee;David Grossman;Ratko Orlandic
#t 2003
#c 1
#% 77928
#% 144031
#% 214597
#% 227939
#% 252304
#% 280849
#% 340951
#% 411694
#% 464195
#% 480587
#% 481956
#% 660292
#% 742724
#! This paper describes a new method for incorporating a hierarchical category dimension into an Information Retrieval framework. The approach is to use the synonym sets and the hyponym ("is-a") relations defined within Wordnet in order to derive a conceptual hierarchical category dimension. The hierarchical nature of a category dimension not only provides an overview of a set of documents but also facilitates the effectiveness and the efficiency of searching documents. An evaluation is performed on two different types of models and the multidimensional approach shows a significant reduction in the number of page accesses over a large document collection.

#index 783467
#* The EDAM project: mining mass spectra and more
#@ Raghu Ramakrishnan
#t 2004
#c 1
#! The EDAM project is a collaborative effort between computer scientists and environmental chemists at Carleton College and UW-Madison. The goal is to develop data mining techniques for advancing the state of the art in analyzing atmospheric aerosol datasets. The traditional approach for particle measurement, which is the collection of bulk samples of particulates on filters, is not adequate for studying particle dynamics and real-time correlations. This has led to the development of a new generation of real-time instruments that provide continuous or semi-continuous streams of data about certain aerosol properties. However, these instruments have added a significant level of complexity to atmospheric aerosol data, and dramatically increased the amounts of data to be collected, managed, and analyzed. We are investigating techniques for automatically labeling mass spectra from different kinds of aerosol mass spectrometers, and then analyzing and exploring the rich spatiotemporal information collected from multiple geographically distributed instruments. In this talk, I will present an overview of some novel data mining problems, describe some of the techniques we are developing to address them, and discuss the broader applicability of these techniques to problems from other domains.

#index 783468
#* Of parts and relationships: an unending quest
#@ Isidore Rigoutsos
#t 2004
#c 1
#! Systems biology is a field which focuses on the interpretation of large, diverse sets of biological measurements in order to elucidate the complex mechanisms that underly important and (seemingly simple) macroscopic phenotypes. The problem at hand is hierarchical in nature, with the hierarchy spanning many levels. Each of these levels can be thought of as comprising multiple active agents that are diverse in their nature (e.g. genes, proteins, pathways, organelles, etc) and also in their behavior. It is within this setting that one seeks to build an integrated view of the system under study, as soon as the relevant units and the complex inter- and intra-level relationships in which these units participate have been characterized. Implicit in the above outline are the assumptions that a) a complete and presumably correct list of parts exists for the system that is being studied; and, b) most, if not all, of the relevant relationships involving these parts are available. Through the research work of my group and of others, there is increasing evidence that the situation is likely to be more complicated than initially estimated, and that one should be watchful when it comes to making or relying on the above two assumptions. In fact, more surprising and currently undiscovered things may be lurking in the ?genomics? box: support for this possibility will be provided through the brief summaries of recent advances that we have made in diverse areas such as association discovery, gene discovery, horizontal gene transfer and RNA interference. Throughout this quest, repositories of biological information will continue to remain our guiding light, whereas time-honored computational methods will continue to be the mainstay of our arsenal.

#index 783469
#* The semantic web: managing knowledge for planet earth
#@ Patrick Hayes
#t 2004
#c 1
#! The Semantic Web (SWeb) is a vision - actually, a number of visions - which is beginning to show some signs of potential reality. The core of the vision is a synergy between machine-accessible knowledge and the global reach of the Web. The deployment of knowledge technologies on a planet-wide open network brings new challenges and opportunities, many of them not yet fully realized. This talk briefly surveys the current state of play in SWeb standards and technology, including some of the rather heated controversies, and tries to give a high-level view of some of these challenges and opportunities. Some traditional hard problems in KM may largely evaporate; but new ones will take their place.

#index 783470
#* Composable XML integration grammars
#@ Wenfei Fan;Minos Garofalakis;Ming Xiong;Xibei Jia
#t 2004
#c 1
#% 128013
#% 215389
#% 229827
#% 248799
#% 271173
#% 273911
#% 274160
#% 333857
#% 333935
#% 476951
#% 479461
#% 479783
#% 479956
#% 481597
#% 481784
#% 632087
#% 654464
#% 654465
#% 654485
#% 993941
#% 993981
#% 994001
#! The proliferation of XML as a standard for data representation and exchange in diverse, next-generation Web applications has created an emphatic need for effective XML data-integration tools. For several real-life scenarios, such XML data integration needs to be DTD-directed -- in other words, the target, integrated XML database must conform to a prespecified, user- or application-defined DTD. In this paper, we propose a novel formalism, XML Integration Grammars (XIGs), for specifying DTD-directed integration of XML data. Abstractly, an XIG maps data from multiple XML sources to a target XML document that conforms to a predefined DTD. An XIG extracts source XML data via queries expressed in a fragment of XQuery, and controls target document generation with tree-valued attributes and the target DTD. The novelty of XIGs consists in not only their automatic support for DTD-conformance but also in their: an XIG may embed local and remote XIGs in its definition, and invoke these XIGs during its evaluation. This yields an important modularity property for our XIGs that allows one to divide a complex integration task into manageable sub-tasks and conquer each of them separately. To efficiently evaluate XIGs we provide algorithms for merging XML queries in an XIG and for scheduling queries and embedded XIGs. These lead to an effective framework, as well as a design tool for XQuery, for effectively specifying and computing complex, DTD-directed XML integration.

#index 783471
#* Extending and inferring functional dependencies in schema transformation
#@ Qi He;Tok Wang Ling
#t 2004
#c 1
#% 22948
#% 23878
#% 85086
#% 102748
#% 197383
#% 213969
#% 248800
#% 316557
#% 342957
#% 384978
#% 386623
#% 428985
#% 443386
#% 479968
#% 480629
#% 482089
#% 491358
#% 535691
#% 764475
#! We study the representation, derivation and utilization of a special kind of constraints in multidatabase systems. A major challenge is when component database schemas are schematic discrepant from each other, i.e., data values of one database correspond to schema labels of another. We propose "qualified functional dependencies" (or qualified FDs), an extension to conventional FDs to formalize integrity constraints in multidatabase systems. We first give inference rules to derive qualified FDs in fixed schemas, then study the derivation of qualified FDs during the transformations between schematic discrepant schemas. Propagation rules are given to derive qualified FDs of transformed schemas from qualified FDs of original schemas. Our work can be used in different stages of building and accessing a multidatabase system, e.g., to detect and resolve value inconsistency in schema integration, to verify lossless schema transformations, to normalize integrated schemas, to verify the integrity of data, and to optimize queries at an integration level. In particular, as an application of our theory, we will use FDs to check the validity of SchemaSQL views (SchemaSQL is a powerful multidatabase language).

#index 783472
#* Organizing structured web sources by query schemas: a clustering approach
#@ Bin He;Tao Tao;Kevin Chen-Chuan Chang
#t 2004
#c 1
#% 36672
#% 261741
#% 262045
#% 262096
#% 273926
#% 274604
#% 280419
#% 296738
#% 314054
#% 333932
#% 376266
#% 413608
#% 413618
#% 464222
#% 464717
#% 479642
#% 481923
#% 570885
#% 654459
#% 765410
#% 769890
#% 783791
#% 993964
#% 1650609
#! In the recent years, the Web has been rapidly "deepened" with the prevalence of databases online. On this deep Web, many sources are structured by providing structured query interfaces and results. Organizing such structured sources into a domain hierarchy is one of the critical steps toward the integration of heterogeneous Web sources. We observe that, for structured Web sources, query schemas ie, attributes in query interfaces) are discriminative representatives of the sources and thus can be exploited for source characterization. In particular, by viewing query schemas as a type of categorical data, we abstract the problem of source organization into the clustering of categorical data. Our approach hypothesizes that "homogeneous sources" are characterized by the same hidden generative models for their schemas. To find clusters governed by such statistical distributions, we propose a new objective function, model-differentiation, which employs principled hypothesis testing to maximize statistical heterogeneity among clusters. Our evaluation over hundreds of real sources indicates that (1) the schema-based clustering accurately organizes sources by object domains eg, Books, Movies), and (2) on clustering Web query schemas, the model-differentiation function outperforms existing ones, such as likelihood, entropy, and context linkages, with the hierarchical agglomerative clustering algorithm.

#index 783473
#* Unified utility maximization framework for resource selection
#@ Luo Si;Jamie Callan
#t 2004
#c 1
#% 184489
#% 227891
#% 282422
#% 309133
#% 344448
#% 397125
#% 447946
#% 584845
#% 643011
#% 643012
#% 722311
#% 722312
#% 730035
#% 993937
#% 993964
#! This paper presents a unified utility framework for resource selection of distributed text information retrieval. This new framework shows an efficient and effective way to infer the probabilities of relevance of all the documents across the text databases. With the estimated relevance information, resource selection can be made by explicitly optimizing the goals of different applications. Specifically, when used for database recommendation, the selection is optimized for the goal of high-recall (include as many relevant documents as possible in the selected databases); when used for distributed document retrieval, the selection targets the high-precision goal (high precision in the final merged list of documents). This new model provides a more solid framework for distributed information retrieval. Empirical studies show that it is at least as effective as other state-of-the-art algorithms.

#index 783474
#* Simple BM25 extension to multiple weighted fields
#@ Stephen Robertson;Hugo Zaragoza;Michael Taylor
#t 2004
#c 1
#% 169781
#% 169811
#% 262069
#% 346556
#% 642992
#% 642993
#% 1389025
#! This paper describes a simple way of adapting the BM25 ranking formula to deal with structured documents. In the past it has been common to compute scores for the individual fields (e.g. title and body) independently and then combine these scores (typically linearly) to arrive at a final score for the document. We highlight how this approach can lead to poor performance by breaking the carefully constructed non-linear saturation of term frequency in the BM25 function. We propose a much more intuitive alternative which weights term frequencies before the non-linear term frequency saturation function is applied. In this scheme, a structured document with a title weight of two is mapped to an unstructured document with the title content repeated twice. This more verbose unstructured document is then ranked in the usual way. We demonstrate the advantages of this method with experiments on Reuters Vol1 and the TREC dotGov collection.

#index 783475
#* Scoring missing terms in information retrieval tasks
#@ Egidio Terra;Charles L.A. Clarke
#t 2004
#c 1
#% 241238
#% 262084
#% 262096
#% 280851
#% 298183
#% 321635
#% 324192
#% 340901
#% 340953
#% 342707
#% 397146
#% 397159
#% 397177
#% 398019
#% 642979
#% 643017
#% 766520
#% 816185
#% 939795
#! An usual approach to address mismatching vocabulary problem is to augment the original query using dictionaries and other lexical resources and/or by looking at pseudo-relevant documents. Either way, terms are added to form a new query that will be used to score all documents in a subsequent retrieval pass, and as consequence the original query's focus may drift because of the newly added terms. We propose a new method to address the mismatching vocabulary problem, expanding original query terms only when necessary and complementing the user query for missing terms while scoring documents. It allows related semantic aspects to be included in a conservative and selective way, thus reducing the possibility of query drift. Our results using replacements for the missing query terms in modified document and passages retrieval methods show significant improvement over the original ones.

#index 783476
#* Goal-oriented methods and meta methods for document classification and their parameter tuning
#@ Stefan Siersdorfer;Sergej Sizov;Gerhard Weikum
#t 2004
#c 1
#% 132938
#% 209021
#% 252011
#% 276506
#% 279755
#% 309141
#% 342682
#% 397155
#% 397161
#% 420077
#% 458379
#% 465895
#% 466737
#% 466759
#% 629610
#% 703747
#% 729437
#% 729932
#! Automatic text classification methods come with various calibration parameters such as thresholds for probabilities in Bayesian classifiers or for hyperplane distances in SVM classifiers. In a given application context these parameters should be set so as to meet the relative importance of various result quality metrics such as precision versus recall. In this paper we consider classifiers that can accept a document for a topic, reject it, or abstain. We aim to meet the application's goals in terms of accuracy (i.e., avoid false acceptances or rejections) and loss (i.e., limit the fraction of documents for which no decision is made). To this end we investigate restrictive forms of Support Vector Machine classifiers and we develop meta methods that split the training data into subsets for independently trained classifiers and then combine the results of these classifiers. These techniques tend to improve accuracy at the expense of document loss. We develop estimators that help to predict the accuracy and loss for a given setting of the methods' tuning parameters, and a methodology for efficiently deriving a setting that meets the application's goals. Our experiments confirm the practical viability of the approach.

#index 783477
#* Using bi-modal alignment and clustering techniques for documents and speech thematic segmentations
#@ Dalila Mekhaldi;Denis Lalanne;Rolf Ingold
#t 2004
#c 1
#% 36672
#% 104472
#% 211514
#% 278106
#% 448786
#% 730988
#% 738493
#% 747943
#% 748583
#% 750574
#% 780849
#% 811356
#! In this paper, we describe a new method for a simultaneous thematic segmentation of the meeting dialogs and the documents discussed or visible throughout the meeting. This bi-modal method is suitable for multimodal applications that are centered on documents, such as meetings and lectures, where documents can be aligned with meeting dialogs. Bringing into play this alignment, our bi-modal segmentation method first transforms its results into a set of nodes in a 2D graph space, where the two axes represent respectively the document units and the meeting dialogs units. Secondly, via a clustering method, the most connected regions in the constituted bi-graph are detected. Finally, the denser clusters are projected on the two axes. The two sequences of segments, obtained on both axes, represent the thematic structure of the document and of the meeting dialogs respectively. We present in this article this bi-modal segmentation technique and its performance compared with two mono-modal segmentation methods.

#index 783478
#* Hierarchical document categorization with support vector machines
#@ Lijuan Cai;Thomas Hofmann
#t 2004
#c 1
#% 169718
#% 190581
#% 262085
#% 280817
#% 309141
#% 311034
#% 342669
#% 420466
#% 420528
#% 458369
#% 458379
#% 465747
#% 466078
#% 466501
#% 479817
#% 482113
#% 722816
#% 770763
#! Automatically categorizing documents into pre-defined topic hierarchies or taxonomies is a crucial step in knowledge and content management. Standard machine learning techniques like Support Vector Machines and related large margin methods have been successfully applied for this task, albeit the fact that they ignore the inter-class relationships. In this paper, we propose a novel hierarchical classification method that generalizes Support Vector Machine learning and that is based on discriminant functions that are structured in a way that mirrors the class hierarchy. Our method can work with arbitrary, not necessarily singly connected taxonomies and can deal with task-specific loss functions. All parameters are learned jointly by optimizing a common objective function corresponding to a regularized upper bound on the empirical loss. We present experimental results on the WIPO-alpha patent collection to show the competitiveness of our approach.

#index 783479
#* Interval query indexing for efficient stream processing
#@ Kun-Lung Wu;Shyh-Kwei Chen;Philip S. Yu
#t 2004
#c 1
#% 68091
#% 86945
#% 102759
#% 206915
#% 252304
#% 333938
#% 397353
#% 413605
#% 427199
#% 773538
#% 993949
#! A large number of continual range queries can be issued against a data stream. Usually, a main memory-based query index with a small storage cost and a fast search time is needed, especially if the stream is rapid. In this paper, we present a CEI-based query index that meets both criteria for efficient processing of continual interval queries in a streaming environment. This new query index is centered around a set of predefined virtual containment-encoded intervals, or CEIs. The CEIs are used to first decompose query intervals and then perform efficient search operations. The CEIs are defined and labeled such that containment relationships among them are encoded in their IDs. The containment encoding makes decomposition and search operations efficient because integer additions and logical shifts can be used to carry out most of the operations. Simulations are conducted to evaluate the effectiveness of the CEI-based query index and to compare it with alternative approaches. The results show that the CEI-based query index significantly outperforms existing approaches in terms of both storage cost and search time.

#index 783480
#* Evaluating window joins over punctuated streams
#@ Luping Ding;Elke A. Rundensteiner
#t 2004
#c 1
#% 159341
#% 273910
#% 300179
#% 378408
#% 480642
#% 578391
#% 660004
#% 726621
#% 745488
#% 765437
#% 771230
#% 1015279
#% 1015296
#% 1016269
#% 1712538
#! We explore join optimizations in the presence of both time-based constraints (sliding windows) and value-based constraints (punctuations). We present the first join solution named PWJoin that exploits such combined constraints to shrink the runtime join state and to propagate punctuations to benefit downstream operators. We design a state structure for PWJoin that facilitates the exploitation of both constraint types. We also explore optimizations enabled by the interactions between window and punctuation, e.g., early punctuation propagation. The costs of the PWJoin are analyzed using a cost model. We also conduct an experimental study using CAPE continuous query system. The experimental results show that in most cases, by exploiting punctuations, PWJoin outperforms the pure window join with regard to both memory overhead and throughput. Our technique complements the joins in the literature, such as symmetric hash join or window join, to now require less runtime resources without compromising the accuracy of the result.

#index 783481
#* EXPedite: a system for encoded XML processing
#@ Yi Chen;George A. Mihaila;Susan B. Davidson;Sriram Padmanabhan
#t 2004
#c 1
#% 300153
#% 333981
#% 334976
#% 397358
#% 397375
#% 458847
#% 465061
#% 480489
#% 576108
#% 654451
#% 654477
#% 654493
#% 659995
#% 659997
#% 659999
#% 993950
#% 993953
#% 1015276
#! As XML becomes an increasingly popular format for information exchange, the efficient processing of broadcast XML data on a constrained device (for example, a cell phone or a PDA) becomes a critical task. In this paper we present the EXPedite system: a new model of data processing in an information exchange environment, which "migrates" the power of the data-sending server to receivers for efficient processing. It consists of a simple and general encoding scheme for servers, and streaming query processing algorithms on encoded XML stream for data receivers with constrained computing abilities. Experiments show the impressive performance of EXPedite.

#index 783482
#* Optimizing web search using web click-through data
#@ Gui-Rong Xue;Hua-Jun Zeng;Zheng Chen;Yong Yu;Wei-Ying Ma;WenSi Xi;WeiGuo Fan
#t 2004
#c 1
#% 27049
#% 54413
#% 194299
#% 253188
#% 268073
#% 268079
#% 308745
#% 310567
#% 330617
#% 340928
#% 387427
#% 577224
#% 577273
#% 591792
#% 641976
#! The performance of web search engines may often deteriorate due to the diversity and noisy information contained within web pages. User click-through data can be used to introduce more accurate description (metadata) for web pages, and to improve the search performance. However, noise and incompleteness, sparseness, and the volatility of web pages and queries are three major challenges for research work on user click-through log mining. In this paper, we propose a novel iterative reinforced algorithm to utilize the user click-through data to improve search performance. The algorithm fully explores the interrelations between queries and web pages, and effectively finds "virtual queries" for web pages and overcomes the challenges discussed above. Experiment results on a large set of MSN click-through log data show a significant improvement on search performance over the naive query log mining algorithm as well as the baseline search engine.

#index 783483
#* A practical web-based approach to generating topic hierarchy for text segments
#@ Shui-Lung Chuang;Lee-Feng Chien
#t 2004
#c 1
#% 46803
#% 46809
#% 218992
#% 262045
#% 262059
#% 280404
#% 280849
#% 281186
#% 309128
#% 310567
#% 340951
#% 342961
#% 413609
#% 466675
#% 528008
#% 577230
#% 577326
#% 629672
#% 643050
#% 657201
#% 748354
#% 748465
#% 754067
#% 766433
#! It is crucial in many information systems to organize short text segments, such as keywords in documents and queries from users, into a well-formed topic hierarchy. In this paper, we address the problem of generating topic hierarchies for diverse text segments with a general and practical approach that uses the Web as an additional knowledge source. Unlike long documents, short text segments typically do not contain enough information to extract reliable features. This work investigates the possibilities of using highly ranked search-result snippets to enrich the representation of text segments. A hierarchical clustering algorithm is then applied to create the hierarchical topic structure of text segments. Different from traditional clustering algorithms, which tend to produce cluster hierarchies with a very unnatural shape, the approach tries to produce a more natural and comprehensive hierarchy. Extensive experiments were conducted on different domains of text segments. The obtained results have shown the potential of the proposed approach, which is believed able to benefit many information systems.

#index 783484
#* Acquisition of categorized named entities for web search
#@ Marius Pasca
#t 2004
#c 1
#% 283180
#% 301241
#% 312861
#% 504443
#% 576214
#% 740916
#% 742092
#% 742101
#% 744539
#% 747944
#% 754068
#% 756253
#% 756964
#% 786523
#% 815107
#% 815868
#% 854652
#% 1255966
#% 1290034
#! The recognition of names and their associated categories within unstructured text traditionally relies on semantic lexicons and gazetteers. The amount of effort required to assemble large lexicons confines the recognition to either a limited domain (e.g., medical imaging), or a small set of pre-defined, broader categories of interest (e.g., persons, countries, organizations, products). This constitutes a serious limitation in an information seeking context. In this case, the categories of potential interest to users are more diverse (universities, agencies, retailers, celebrities), often refined (e.g., SLR digital cameras, programming languages, multinational oil companies), and usually overlapping (e.g., the same entity may be concurrently a brand name, a technology company, and an industry leader). We present a lightly supervised method for acquiring named entities in arbitrary categories. The method applies lightweight lexico-syntactic extraction patterns to the unstructured text of Web documents. The method is a departure from traditional approaches to named entity recognition in that: 1) it does not require any start-up seed names or training; 2) it does not encode any domain knowledge in its extraction patterns; 3) it is only lightly supervised, and data-driven; 4) it does not impose any a-priori restriction on the categories of extracted names. We illustrate applications of the method in Web search, and describe experiments on 500 million Web documents and news articles.

#index 783485
#* BioDIFF: an effective fast change detection algorithm for genomic and proteomic data
#@ Yang Song;Sourav S. Bhowmick
#t 2004
#c 1
#% 806627

#index 783486
#* Protein structure alignment using geometrical features
#@ S. Alireza Aghili;Divyakant Agrawal;Amr El Abbadi
#t 2004
#c 1
#% 717359
#% 717363
#! A novel approach for similarity search on protein structure databases is proposed which incorporates the three dimensional coordinates of the main atoms of each amino acid and extracts a geometrical signature along with the direction of the given amino acid. As a result, each protein is presented by a series of feature vectors representing local geometry, shape, direction, and secondary structure assignment of its amino acid constituents. Furthermore, a residue-to-residue distance matrix is calculated and is incorporated into a local alignment dynamic programming algorithm to find the similar portions of two given proteins and finally a sequence alignment step is used as the last filtration step. The optimal superimposition of the detected similar regions is used to assess the quality of the results. The proposed algorithm is fast and accurate and hence could be used for the analysis of large protein structure similarity.

#index 783487
#* Mining gene expression datasets using density-based clustering
#@ Seokkyung Chung;Jongeun Jun;Dennis McLeod
#t 2004
#c 1
#! Given the recent advancement of microarray technologies, we present a density-based clustering approach for the purpose of co-expressed gene cluster identification. The underlying hypothesis is that a set of co-expressed gene clusters can be used to reveal a common biological function. By addressing the strengths and limitations of previous density-based clustering approaches, we present a novel clustering algorithm that utilizes a neighborhood defined by k-nearest neighbors. Experimental results indicate that the proposed method identifies biologically meaningful and co-expressed gene clusters.

#index 783488
#* Semi-supervised learning for music artists style identification
#@ Tao Li;Mitsunori Ogihara
#t 2004
#c 1
#% 780760
#% 795274

#index 783489
#* Integrating heterogeneous reatures for efficient content based music retrieval
#@ Jialie Shen;John Shepherd;Anne H.H. Ngu
#t 2004
#c 1
#% 356892
#! In this paper, we present a novel feature extraction method facilitating efficient content-based music retrieval and classification, called InMAF. The goal of our approach is to allow straightforward incorporation of multiple musical features, such as timbral texture, pitch and rhythm structure, into a single low dimensional vector that is effective for retrieval and classification. Unlike earlier approaches that used only acoustic properties as the basis for retrieval, our approach can easily incoporate human music perception to improve accuracy of retrieval and classification process. The superiority of our method is demonstrated by comparing it with state-of-the-art approaches in the areas of music classification (using a variety of machine learning algorithms), query effectiveness and robustness against audio distortion.

#index 783490
#* Unified filtering by combining collaborative filtering and content-based filtering via mixture model and exponential model
#@ Luo Si;Rong Jin
#t 2004
#c 1
#% 578684
#% 1650569
#% 1673052
#! Collaborative filtering and content-based filtering are two types of information filtering techniques. Combining these two techniques can improve the recommendation effectiveness. The main problem with previous research is that the content information and the rating information are not combined in an integrated way. This paper presents a unified probabilistic framework that allows the mutual interaction between these two types of information. Experiments have shown that the new unified filtering algorithm outperforms a pure collaborative filtering approach, a pure content-based filtering approach and another unified filtering algorithm.

#index 783491
#* A framework for refining similarity queries using learning techniques
#@ Yiming Ma;Qi Zhong;Sharad Mehrotra;Dawit Yimam Seid
#t 2004
#c 1
#% 387427
#% 406493
#% 449588
#% 458860
#% 479788
#% 480302
#% 1857498
#! In numerous applications that deal with similarity search, a user may not have an exact idea of his information need and/or may not be able to construct a query that exactly captures his notion of similarity. A promising approach to mitigate this problem is to enable the user to submit a rough approximation of the desired query and use the feedback on the relevance of the retrieved objects to refine the query. In this paper, we explore such a refinement strategy for a general class of SQL similarity queries. Our approach casts the refinement problem as that of learning concepts using examples. This is achieved by viewing the tuples on which a user provides feedback as a labeled training set for a learner. Under this setup, SQL query refinement consists of two learning tasks, namely learning the structure of the SQL query and learning the relative importance of the query components. The paper develops appropriate machine learning approaches suitable for these two learning tasks. The primary contribution of the paper is a general refinement framework that decides when each learner is invoked in order to quickly learn the user query. Experimental analyses over many real life datasets and queries show that our strategy outperforms the existing approaches significantly in terms of retrieval accuracy and query simplicity.

#index 783492
#* A dimensionality reduction technique for efficient similarity analysis of time series databases
#@ Vasileios Megalooikonomou;Guo Li;Qiang Wang
#t 2004
#c 1
#% 114667
#% 480146
#! Efficiently searching for similarities among time series and discovering interesting patterns is an important and non-trivial problem with applications in many domains. The high dimensionality of the data makes the analysis very challenging. To solve this problem, many dimensionality reduction methods have been proposed. PCA (Piecewise Constant Approximation) and its variant have been shown efficient in time series indexing and similarity retrieval. However, in certain applications, too many false alarms introduced by the approximation may reduce the overall performance dramatically. In this paper, we introduce a new piecewise dimensionality reduction technique that is based on Vector Quantization. The new technique, PVQA (Piecewise Vector Quantized Approximation), partitions each sequence into equi-length segments and uses vector quantization to represent each segment by the closest (based on a distance metric) codeword from a codebook of key-sequences. The efficiency of calculations is improved due to the significantly lower dimensionality of the new representation. We demonstrate the utility and efficiency of the proposed technique on real and simulated datasets. By exploiting prior knowledge about the data, the proposed technique generally outperforms PCA and its variants in similarity searches.

#index 783493
#* Combining structural and citation-based evidence for text classification
#@ Baoping Zhang;Marcos André Gonçalves;Weiguo Fan;Yuxin Chen;Edward A. Fox;Pável Calado;Marco Cristo
#t 2004
#c 1
#% 124073
#% 287284
#% 420090
#! This paper discusses how citation-based information and structural content (e.g., title, abstract) can be combined to improve classification of text documents into predefined categories. We evaluate different measures of similarity derived from the citation structure and the structural content of the collection, and determine how they can be fused to improve classification effectiveness. To discover the best fusion framework, we apply Genetic Programming (GP) techniques. Our empirical experiments using documents from the ACM Digital Library and the ACM Computing Classification System show that we can discover similarity functions that work better than using evidence in isolation and whose combined performance through a simple majority voting is comparable to that of Support Vector Machine classifiers.

#index 783494
#* Using relevance feedback to detect misuse for information retrieval systems
#@ Ling Ma;Nazli Goharian
#t 2004
#c 1
#% 642391
#% 730033

#index 783495
#* An extended logic programming based multi-agent system formalization in mobile environments
#@ Jianwen Chen;Yan Zhang
#t 2004
#c 1
#% 304816
#% 385733
#% 392811
#% 434344
#! In this paper, we propose an extended logic programming based formalization for a multi-agent system in mobile environments. Such a system consists of a number of agents connected via wire or wireless communication channels. Our formalization is knowledge oriented and has declarative semantics inherited from extended logic programming. This model can be used to study the details of knowledge transaction in mobile environments.

#index 783496
#* Framework and algorithms for trend analysis in massive temporal data sets
#@ Sreenivas Gollapudi;D. Sivakumar
#t 2004
#c 1
#% 248792
#% 249183
#% 278835
#% 310500
#% 333931
#% 342600
#% 345857
#% 347226
#% 419377
#% 465010
#% 480628
#% 481290
#% 577220
#% 654489
#% 656745
#% 993995
#% 1015293
#! Mining massive temporal data streams for significant trends, emerging buzz, and unusually high or low activity is an important problem with several commercial applications. In this paper, we propose a framework based on relational records and metric spaces to study such problems. Our framework provides the necessary mathematical underpinnings for this genre of problems, and leads to efficient algorithms in the stream/sort model of massive data sets (where the algorithm makes passes over the data, computes a new stream on the fly, and is allowed to sort the intermediate data). Our algorithm makes novel use of metric approximations in the data stream context, and highlights the role of hierarchical organization of large data sets in designing efficient algorithms in the stream/sort model.

#index 783497
#* Scalable sequential pattern mining for biological sequences
#@ Ke Wang;Yabo Xu;Jeffrey Xu Yu
#t 2004
#c 1
#% 172892
#% 248791
#% 310507
#% 333942
#% 397383
#% 420063
#% 425006
#% 459006
#% 463903
#% 464996
#% 465003
#% 469577
#% 481779
#% 546276
#% 577256
#! Biosequences typically have a small alphabet, a long length, and patterns containing gaps (i.e., "don't care") of arbitrary size. Mining frequent patterns in such sequences faces a different type of explosion than in transaction sequences primarily motivated in market-basket analysis. In this paper, we study how this explosion affects the classic sequential pattern mining, and present a scalable two-phase algorithm to deal with this new explosion. The Segment Phase first searches for short patterns containing no gaps, called segments. This phase is efficient. The Pattern Phase searches for long patterns containing multiple segments separated by variable length gaps. This phase is time consuming. The purpose of two phases is to exploit the information obtained from the first phase to speed up the pattern growth and matching and to prune the search space in the second phase. We evaluate this approach on synthetic and real life data sets.

#index 783498
#* Discovering frequently changing structures from historical structural deltas of unordered XML
#@ Qiankun Zhao;Sourav S. Bhowmick;Mukesh Mohania;Yahiko Kambayashi
#t 2004
#c 1
#% 300120
#% 466644
#% 577218
#% 629708
#% 637694
#% 659923
#% 729941
#! Recently, a large amount of work has been done in XML data mining. However, we observed that most of the existing works focus on the snapshot XML data, while XML data is dynamic in real applications. To the best of our knowledge, none of the existing works has addressed the issue of mining the history of changes to XML documents. Such mining results can be useful in many applications such as XML change detection, XML indexing, association rule mining, and classification etc. In this paper, we propose a novel approach to discover the frequently changing structures from the sequence of historical structural deltas of unordered XML. To make the structure discovering process efficient, an expressive and compact data model, Historical-Document Object Model (H-DOM), is proposed. Using this model, two basic algorithms, which can discover all the frequently changing structures with only two scans of the XML sequence, are presented. Experimental results show that our algorithms, together with the optimization techniques, are efficient and scalable.

#index 783499
#* Indexing text data under space constraints
#@ Bijit Hore;Hakan Hacigumus;Bala Iyer;Sharad Mehrotra
#t 2004
#c 1
#% 55490
#% 143306
#% 203281
#% 212287
#% 217815
#% 241156
#% 248814
#% 252608
#% 269546
#% 273888
#% 287715
#% 289010
#% 297675
#% 321327
#% 333679
#% 397390
#% 403195
#% 404772
#% 408396
#% 480654
#% 660009
#! An important class of queries is the LIKE predicate in SQL. In the absence of an index, LIKE queries are subject to performance degradation. The notion of indexing on substrings (or q-grams) has been explored earlier without sufficient consideration of efficiency. q-grams are used to prune away rows that do not qualify for the query. The problem is to identify a finite number of grams subject to storage constraint that gives maximal pruning for a given query workload. Our contributions include: i) a formal problem definition, that produces results within a provable error bound, ii) performance evaluation of the application of the novel method to real data, and iii) parallelization of the algorithm, scaling considerations and a proposal to handle scaling issues.

#index 783500
#* Image similarity search with compact data structures
#@ Qin Lv;Moses Charikar;Kai Li
#t 2004
#c 1
#% 219847
#% 249321
#% 255137
#% 273919
#% 278835
#% 282202
#% 318785
#% 322884
#% 325683
#% 341442
#% 345848
#% 347225
#% 415033
#% 451644
#% 580700
#% 588592
#% 588726
#% 594029
#% 736928
#% 824550
#! The recent theoretical advances on compact data structures (also called "sketches") have raised the question of whether they can effectively be applied to content-based image retrieval systems. The main challenge is to derive an algorithm that achieves high-quality similarity searches while using compact metadata. This paper proposes a new similarity search method consisting of three parts. The first is a new region feature representation with weighted $=1 distance function, and EMD* match, an improved EMD match, to compute image similarity. The second is a thresholding and transformation algorithm to convert feature vectors into very compact data structures. The third is an EMD embedding based filtering method to speed up the query process. We have implemented a prototype system with the proposed method and performed experiments with a 10,000 image database. Our results show that the proposed method can achieve more effective similarity searches than previous approaches with metadata 3 to 72 times more compact than previous systems. The experiments also show that our EMD embedding based filtering technique can speed up the query process by a factor of 5 or more with little loss in query effectiveness.

#index 783501
#* Energy management schemes for memory-resident database systems
#@ Jayaprakash Pisharath;Alok Choudhary;Mahmut Kandemir
#t 2004
#c 1
#% 77978
#% 172876
#% 205047
#% 249985
#% 287461
#% 303994
#% 336866
#% 392275
#% 393844
#% 451766
#% 479821
#% 482086
#% 572304
#% 631540
#% 654482
#% 657610
#! With the tremendous growth of system memories, memory-resident databases are increasingly becoming important in various domains. Newer memories provide a structured way of storing data in multiple chips, with each chip having a bank of memory modules. Current memory-resident databases are yet to take full advantage of the banked storage system, which offers a lot of room for performance and energy optimizations. In this paper, we identify the implications of a banked memory environment in supporting memory-resident databases, and propose hardware (memory-directed) and software (query-directed) schemes to reduce the energy consumption of queries executed on these databases. Our results show that high-level query-directed schemes (hosted in the query optimizer) better utilize the low-power modes in reducing the energy consumption than the respective hardware schemes (hosted in the memory controller), due to their complete knowledge of query access patterns. We extend this further and propose a query restructuring scheme and a multi-query optimization. Queries are restructured and regrouped based on their table access patterns to maximize the likelihood that data accesses are clustered. This helps increase the inter-access idle times of memory modules, which in turn enables a more effective control of their energy behavior. This heuristic is eventually integrated with our hardware optimizations to achieve maximum savings. Our experimental results show that the memory energy reduces by 90% if query restructuring method is applied along with basic energy optimizations over the unoptimized version. The system-wide performance impact of each scheme is also studied simultaneously.

#index 783502
#* Restructuring batch view maintenance efficiently
#@ Bin Liu;Elke A. Rundensteiner;David Finkel
#t 2004
#c 1
#% 13016
#% 201928
#% 227947
#% 273918
#% 300141
#% 413556
#% 791180
#! Materialized views defined over distributed data sources are a well recognized technology for modern applications. State-of the-art incremental view maintenance requires O(n2) or more maintenance queries to remote data sources with n being the number of data sources in the view definition. In this poster, we illustrate basic ideas of novel view maintenance strategies that dramatically reduce the number of maintenance queries. Such reduction brings the tradeoff between the number of maintenance queries and the complexity of each query. These algorithms have been implemented in a working prototype system. Experimental studies illustrate major performance improvement in terms of total processing time compared with existing batch algorithms.

#index 783503
#* On semantic matching of multilingual attributes in relational systems
#@ A. Kumaran;Jayant R. Haritsa
#t 2004
#c 1
#% 77979

#index 783504
#* Compression schemes for differential categorical stream clustering
#@ Weiyun Huang;Edward Omiecinski;Leo Mark
#t 2004
#c 1
#% 248790
#% 413618
#! Stream data analysis differs significantly from traditional data processing. To process the data online the algorithm has to work in one pass, incorporating new data into a model maintained in main memory. Storing a model or synopsis of processed data in the memory, which we call "data compression", is an important technique in both incremental and differential stream mining. This paper proposes several data compression schemes in one-pass categorical data clustering, and demonstrates their performance on synthetic and real data. Our compression schemes can efficiently generate compact representations of original data, so as to enable the algorithm to process streams at high speed and detect the changes in underlying data. The example algorithm based on these compression schemes achieves good accuracy in short execution time.

#index 783505
#* Using a compact tree to index and query XML data
#@ Qinghua Zou;Shaorong Liu;Wesley W. Chu
#t 2004
#c 1
#% 479465
#% 766417
#% 783697
#! Indexing XML is crucial for efficient XML query processing. We propose a compact tree (Ctree) for XML indexing, which provides not only concise path summaries at group level but also detailed child-parent relationships at element level. Based on Ctree, we are able to measure how well XML data is structured. We also propose a three-step query processing method. Its efficiency is achieved by: (1) summarizing large XML data structures into a condensed Ctree; (2) pruning irrelevant groups to significantly reduce the search space; (3) eliminating join operations between the matches for value predicates and those for structure constraints and (4) using Ctree properties such as regular groups to reduce query processing time. Our experiments reveal that Ctree is an effective data structure for managing XML data.

#index 783506
#* A framework for selective query expansion
#@ Steve Cronen-Townsend;Yun Zhou;W. Bruce Croft
#t 2004
#c 1
#% 280864
#% 397161
#% 719598
#! Query expansion is a well-known technique that has been shown to improve average retrieval performance. This technique has not been used in many operational systems because of the fact that it can greatly degrade the performance of some individual queries. We show how comparison between language models of the unexpanded and expanded retrieval results can be used to predict when the expanded retrieval has strayed from the original sense of the query. In these cases, the unexpanded results are used while the expanded results are used in the remaining cases (where such straying is not detected). We evaluate this method on a wide variety of TREC collections.

#index 783507
#* Exploiting hierarchical relationships in conceptual search
#@ Devanand Ravindran;Susan Gauch
#t 2004
#c 1
#! As the number of available Web pages grows, users experience increasing difficulty finding documents relevant to their interests. One of the underlying reasons for this is that most search engines find matches based on keywords, regardless of their meanings. To provide the user with more useful information, we need a system that disambiguates queries by including information about the user's conceptual framework. This is the goal of KeyConcept, a conceptual search engine. During indexing, KeyConcept automatically classifies documents into concepts selected from a reference concept hierarchy. During retrieval, KeyConcept ranks documents based on a combination of keyword and conceptual similarity. This paper describes the system architecture and discusses the results of experiments that evaluate the effect of exploiting the hierarchical relationships between concepts during retrieval. Our results confirm that conceptual match significantly improves the precision of the search results over keyword match alone. In addition, the use of the concept hierarchy to prune irrelevant search results also significantly increases precision.

#index 783508
#* MRSSA: an iterative algorithm for similarity spreading over interrelated objects
#@ Gui-Rong Xue;Hua-Jun Zeng;Zheng Chen;Yong Yu;Wei-Ying Ma;WenSi Xi;Edward Fox
#t 2004
#c 1
#% 281209
#% 643009
#! We introduce the Multiple Relationship Similarity Spreading Algorithm (MRSSA) to enhance IR effectiveness. This method has similarity computed in an iterative "spreading" fashion for multiple object types, combining both inter- and intra-object relationships. We demonstrate the value of this approach in the context of the WWW, where the key objects are web pages and queries, Relationships considered are derived from hyperlinks (in- and out-links) and click-through logs.

#index 783509
#* Web page clustering enhanced by summarization
#@ Xuanhui Wang;Dou Shen;Hua-Jun Zeng;Zheng Chen;Wei-Ying Ma
#t 2004
#c 1
#% 144011
#% 169809
#% 340884
#! Traditional Web page clustering algorithms use the full-text in the documents to generate feature vectors. Such methods often produce unsatisfactory results because there is much noisy information, such as decoration, interaction, and advertisement, in Web pages. The varying-length problem of the Web pages is also a significant negative factor affecting the performance. In this paper, we investigate the use of several summarization techniques to tackle these issues when clustering Web pages. Compared with the full-text representation of the Web pages, our experimental results indicate that our proposed approach effectively solves the problems of noisy information and varying-length, and thus significantly boosts the clustering performance.

#index 783510
#* Grammar-based task analysis of web logs
#@ Savitha Srinivasan;Arnon Amir;Prasad Deshpande;Vladimir Zbarsky
#t 2004
#c 1
#% 3888
#% 209662
#% 443082
#% 481290
#! The daily use of Internet-based services is involved with hundreds of different tasks being performed by multiple users. A single task is typically involved with a sequence of Web URLs invocation. We study the problem of pattern detection in Web logs to identify tasks performed by users, and analyze task trends over time using a grammar-based framework. Our results are demonstrated on a corporate Intranet portal application with 7000 users over a 6 week period and demonstrate compelling business value from this high-level task analysis.

#index 783511
#* Soft clustering criterion functions for partitional document clustering: a summary of results
#@ Ying Zhao;George Karypis
#t 2004
#c 1
#% 374537
#% 413620
#% 755463
#! Recently published studies have shown that partitional clustering algorithms that optimize certain criterion functions, which measure key aspects of inter- and intra-cluster similarity, are very effective in producing hard clustering solutions for document datasets and outperform traditional partitional and agglomerative algorithms. In this paper we study the extent to which these criterion functions can be modified to include soft membership functions and whether or not the resulting soft clustering algorithms can further improve the clustering solutions. Specifically, we focus on four of these hard criterion functions, derive their soft-clustering extensions, and present an experimental evaluation involving twelve different datasets. Our results show that introducing softness into the criterion functions tends to lead to better clustering results for most datasets.

#index 783512
#* Calculating similarity between texts using graph-based text representation model
#@ Junji Tomita;Hidekazu Nakawatase;Megumi Ishii
#t 2004
#c 1
#% 115462
#% 186336
#% 769491
#! Knowledge discovery from a large volumes of texts usually requires many complex analysis steps. The graph-based text representation model has been proposed to simplify the steps. The model represents texts in a formal manner, Subject Graphs, and provides text handling operations whose inputs and outputs are identical in form, i.e. a set of subject graphs, so they can be combined in any order. A subject graph uses node weight to represent the significance of each term, and link weight to represent that of each term-term association. This paper concentrates on the algorithms for making subject graphs and calculating the similarity between them. An evaluation shows that Subject Graphs can calculate the similarity between texts more precisely than term vectors, since they incorporate the significance of association between terms.

#index 783513
#* A design space approach to analysis of information retrieval adaptive filtering systems
#@ Dmitriy Fradkin;Paul Kantor
#t 2004
#c 1
#% 169777
#% 732844
#% 1290263
#! In this paper we suggest a new approach to analysis and design of IR systems. We argue for design space exploration in constructing IR systems and in analyzing the effects of individual modules and parameters. We present results of experiments with parametric interpolation, or "homotopy", between two systems, and show, incidentally, that the best results are not achieved at the endpoints, and may lie outside the bounding hypercube defined by our choice of parameterization. Three distinct classes of interpolation are introduced to deal with the complexities of the specific example.

#index 783514
#* A multi-system analysis of document and term selection for blind feedback
#@ Thomas R. Lynam;Chris Buckley;Charles L. A. Clarke;Gordon V. Cormack
#t 2004
#c 1
#% 92696
#% 262096
#% 340901
#% 340953
#% 342707
#% 397129
#% 411760
#% 413613
#% 648114
#% 766471
#% 766474
#% 766486
#% 766497
#% 766518
#% 766520
#% 766525
#% 924966
#! Experiments were conducted to explore the impact of combining various components of eight leading information retrieval systems. Each system demonstrated improved effectiveness with the use of blind feedback, in which the results of a preliminary retrieval step were used to augment the efficacy of a secondary retrieval step. The hybrid combination of primary and secondary retrieval steps from different systems in a number of cases yielded better effectiveness than either of the constituent systems alone. This positive combining effect was observed when entire documents were passed between the two retrieval steps, but not when only the expansion terms were passed. Several combinations of primary and secondary retrieval steps were fused using the CombMNZ algorithm; all yielded significant effectiveness improvement over the individual systems, with the best yielding a an improvement of 13% (p = 10-6) over the best individual system and an improvement of 4% (p = 10-5) over a simple fusion of the eight systems.

#index 783515
#* Improving document representations using relevance feedback: the RFA algorithm
#@ Razvan Stefan Bot;Yi-fang Brook Wu
#t 2004
#c 1
#% 46803
#% 65946
#% 86373
#% 111304
#% 234793
#% 323131
#% 344447
#% 345763
#% 723328
#% 840583
#! In this paper we present a document representation improvement technique, named the Relevance Feedback Accumulation (RFA) algorithm. Using prior relevance feedback assessments and a data mining measure called "support", the algorithm's learning function gradually improves document representations, over time and across users. Results show that the modified document representations yield lower dimensionality while improving retrieval effectiveness. The algorithm is efficient and scalable, suited for retrieval systems managing large document collections.

#index 783516
#* A vertical distance-based outlier detection method with local pruning
#@ Dongmei Ren;Imad Rahal;William Perrizo;Kirk Scott
#t 2004
#c 1
#% 144708
#% 200033
#% 243752
#% 297618
#% 300183
#% 316709
#% 346513
#% 459025
#% 477821
#% 479791
#% 479986
#% 502134
#% 737342
#% 765496
#% 998623
#! "One person's noise is another person's signal". Outlier detection is used to clean up datasets and also to discover useful anomalies, such as criminal activities in electronic commerce, computer intrusion attacks, terrorist threats, agricultural pest infestations, etc. Thus, outlier detection is critically important in the information-based society. This paper focuses on finding outliers in large datasets using distance-based methods. First, to speedup outlier detections, we revise Knorr and Ng's distance-based outlier definition; second, a vertical data structure, instead of traditional horizontal structures, is adopted to facilitate efficient outlier detection further. We tested our methods against national hockey league dataset and show an order of magnitude of speed improvement compared to the contemporary distance-based outlier detection approaches.

#index 783517
#* ClusterMap: labeling clusters in large datasets via visualization
#@ Keke Chen;Ling Liu
#t 2004
#c 1
#% 36672
#% 210173
#% 248790
#% 273890
#% 296738
#% 310526
#% 316704
#% 317313
#% 339184
#% 342601
#% 397597
#% 436509
#% 438137
#% 443517
#% 462243
#% 479799
#% 720191
#% 727849
#% 728371
#% 853013
#! With the rapid increase of data in many areas, clustering on large datasets has become an important problem in data analysis. Since cluster analysis is a highly iterative process, cluster analysis on large datasets prefers short iteration on a relatively small representative set. Thus, a two-phase framework "sampling/summarization - iterative cluster analysis" is often applied in practice. Since the clustering result only labels the small representative set, there are problems with extending the result to the entire large dataset, which are almost ignored by the traditional clustering research. This extending is often named as labeling process. Labeling irregular shaped clusters, distinguishing outliers and extending cluster boundary are the main problems in this stage. We address these problems and propose a visualization-based approach to dealing with them precisely. This approach partially involves human into the process of defining and refining the structure "ClusterMap". Based on this structure, the ClusterMap algorithm scans the large dataset to adapt the boundary extension and generate the cluster labels for the entire dataset. Experimental result shows that ClusterMap can preserve cluster quality considerably with low computational cost, compared to the distance-comparison-based labeling algorithms.

#index 783518
#* On combining multiple clusterings
#@ Tao Li;Mitsunori Ogihara;Sheng Ma
#t 2004
#c 1
#% 27842
#% 33905
#% 179800
#% 349552
#% 379340
#% 383735
#% 424997
#% 579655
#% 643010
#% 722902
#% 729957
#% 740416
#% 766434
#% 780760
#% 998588
#! Many problems can be reduced to the problem of combining multiple clusterings. In this paper, we first summarize different application scenarios of combining multiple clusterings and provide a new perspective of viewing the problem as a categorical clustering problem. We then show the connections between various consensus and clustering criteria and discuss the complexity results of the problem. Finally we propose a new method to determine the final clustering. Experiments on kinship terms and clustering popular music from heterogeneous feature sets show the effectiveness of combining multiple clusterings.

#index 783519
#* SWAM: a family of access methods for similarity-search in peer-to-peer data networks
#@ Farnoush Banaei-Kashani;Cyrus Shahabi
#t 2004
#c 1
#% 121114
#% 213080
#% 252304
#% 300078
#% 333990
#% 340175
#% 340176
#% 342827
#% 481460
#% 572265
#% 643013
#% 653859
#% 723445
#% 1015281
#! Peer-to-peer Data Networks (PDNs) are large-scale, self-organizing, distributed query processing systems. Familiar examples of PDN are peer-to-peer file-sharing networks, which support exact-match search queries to locate user-requested files. In this paper, we formalize the more general problem of similarity-search in PDNs, and propose a family of distributed access methods, termed Small-World Access Methods (SWAM), for efficient execution of various similarity-search queries, namely exact-match, range, and k-nearest-neighbor queries. Unlike its predecessors, i.e., LH* and DHTs, SWAM does not control the assignment of data objects to PDN nodes; each node autonomously stores its own data. Besides, SWAM supports all similarity-search queries on multiple attributes. SWAM guarantees that the query object will be found (if it exists in the network) in average time logarithmically proportional to the network size. Moreover, once the query object is found, all the similar objects would be in its proximate network neighborhood and hence enabling efficient range and k-nearest-neighbor queries. As a specific instance of SWAM, we propose SWAM-V, a Voronoi-based SWAM that indexes PDNs with multi-attribute data objects. For a PDN with N nodes SWAM-V has query time, communication cost, and computation cost of O(log N) for exact-match queries, and O(log N + sN) and O(log N + k) for range queries (with selectivity s) and kNN queries, respectively. Our experiments show that SWAM-V consistently outperforms a similarity-search enabled version of CAN in query time and communication cost by a factor of 2 to 3.

#index 783520
#* Localized signature table: fast similarity search on transaction data
#@ Qiang Jing;Rui Yang;Panos Kalnis;Anthony K. H. Tung
#t 2004
#c 1
#% 36672
#% 41923
#% 201876
#% 273920
#% 333950
#% 466513
#% 479649
#% 480133
#% 480307
#% 480948
#% 481290
#% 572294
#! Recently, techniques for supporting efficient similarity search over huge transaction datasets have emerged as an important research area. Several indexing schemes have been proposed towards this direction. Typically, these schemes provide a tradeoff between searching efficiency and indexing overhead in terms of space. In this paper, we propose a novel indexing scheme for similarity search on transaction data. Based on well-studied clustering techniques, we develop a construction algorithm for the proposed index and a branch-and-bound searching strategy for answering similarity search. Unlike previous techniques, our indexing scheme exhibits high search efficiency and low space requirements by trading-off the pre-computation time. This behavior is ideal for applications with low update but high read volume e.g., data warehousing, collaborative filtering, etc.). Moreover, our experimental results illustrate that our method is robust to the varying characteristics of the datasets.

#index 783521
#* Distance-function design and fusion for sequence data
#@ Yi Wu;Edward Y. Chang
#t 2004
#c 1
#% 172949
#% 190581
#% 227924
#% 240182
#% 251654
#% 285711
#% 330932
#% 333941
#% 420077
#% 425040
#% 462231
#% 464615
#% 477482
#% 480156
#% 480484
#% 577221
#% 662750
#% 730216
#% 856942
#% 993984
#! Sequence-data mining plays a key role in many scientific studies and real-world applications such as bioinformatics, data stream, and sensor networks, where sequence data are processed and their semantics interpreted. In this paper we address two relevant issues: sequence-data representation, and representation-to-semantics mapping. For representation, since the best one is dependent upon the application being used and even the type of query, we propose representing sequence data in multiple views. For each representation, we propose methods to construct a valid kernel as the distance function to measure similarity between sequences. For mapping, we then find the best combination of the individual distance functions, which measure similarity of different views, to depict the target semantics. We propose a super-kernel function-fusion scheme to achieve the optimal mapping. Through theoretical analysis and empirical studies on UCI and real world datasets, we show our approach of multi-view representation and fusion to be mathematically valid and very effective for practical purposes.

#index 783522
#* Learning similarity measures in non-orthogonal space
#@ Ning Liu;Benyu Zhang;Jun Yan;Qiang Yang;Shuicheng Yan;Zheng Chen;Fengshan Bai;Wei-Ying Ma
#t 2004
#c 1
#% 260001
#% 309129
#% 342670
#% 342707
#% 387427
#% 406493
#% 413561
#% 425047
#% 495413
#! Many machine learning and data mining algorithms crucially rely on the similarity metrics. The Cosine similarity, which calculates the inner product of two normalized feature vectors, is one of the most commonly used similarity measures. However, in many practical tasks such as text categorization and document clustering, the Cosine similarity is calculated under the assumption that the input space is an orthogonal space which usually could not be satisfied due to synonymy and polysemy. Various algorithms such as Latent Semantic Indexing (LSI) were used to solve this problem by projecting the original data into an orthogonal space. However LSI also suffered from the high computational cost and data sparseness. These shortcomings led to increases in computation time and storage requirements for large scale realistic data. In this paper, we propose a novel and effective similarity metric in the non-orthogonal input space. The basic idea of our proposed metric is that the similarity of features should affect the similarity of objects, and vice versa. A novel iterative algorithm for computing non-orthogonal space similarity measures is then proposed. Experimental results on a synthetic data set, a real MSN search click-thru logs, and 20NG dataset show that our algorithm outperforms the traditional Cosine similarity and is superior to LSI.

#index 783523
#* Feature selection with conditional mutual information maximin in text categorization
#@ Gang Wang;Frederick H. Lochovsky
#t 2004
#c 1
#% 115608
#% 190581
#% 194283
#% 200694
#% 246831
#% 266215
#% 332658
#% 458379
#% 458389
#% 465754
#% 650844
#% 740900
#% 756951
#! Feature selection is an important component of text categorization. This technique can both increase a classifier's computation speed, and reduce the overfitting problem. Several feature selection methods, such as information gain and mutual information, have been widely used. Although they greatly improve the classifier's performance, they have a common drawback, which is that they do not consider the mutual relationships among the features. In this situation, where one feature's predictive power is weakened by others, and where the selected features tend to bias towards major categories, such selection methods are not very effective. In this paper, we propose a novel feature selection method for text categorization called conditional mutual information maximin (CMIM). It can select a set of individually discriminating and weakly dependent features. The experimental results show that CMIM can perform much better than traditional feature selection methods.

#index 783524
#* Regularizing translation models for better automatic image annotation
#@ Feng Kang;Rong Jin;Joyce Y. Chai
#t 2004
#c 1
#% 300542
#% 340901
#% 397128
#% 457912
#% 642989
#% 642990
#% 721163
#% 722927
#% 730166
#% 740915
#% 1858012
#! The goal of automatic image annotation is to automatically generate annotations for images to describe their content. In the past, statistical machine translation models have been successfully applied to automatic image annotation task [8]. It views the process of annotating images as a process of translating the content from a 'visual language' to textual words. One problem with the existing translation models is that common words are usually associated with too many different image regions. As a result, uncommon words have little chance to be used for annotating images. Uncommon words are important for automatic image annotation because they are often used in the queries. In this paper, we propose two modified translation models for automatic image annotation, namely the normalized translation model and the regularized translation model, that specifically address the problem of common annotated words. The basic idea is to raise the number of blobs that are associated with uncommon words. The normalized translation model realizes this by scaling translation probabilities of different words with different factors. The same goal is achieved in the regularized translation model through the introduction of a special Dirichlet prior. Empirical study with the Corel dataset has shown that both two modified translation models outperform the original translation model and several existing approaches for automatic image annotation substantially.

#index 783525
#* Key problems in integrating structured and unstructured information
#@ D. Calvin Andrus;David Bernholz;James Scoggins;Erik Thomsen
#t 2004
#c 1
#! This forum will host a discussion among the panelists - and with the audience - on key trends in information technology, as well as on industry and government problems in search of technology solutions. The panel will focus on the core problems inherent in integrating structured and unstructured textual information, moving from application-centric to user-centric software, and merging knowledge management with data management.

#index 783526
#* Providing consistent and exhaustive relevance assessments for XML retrieval evaluation
#@ Benjamin Piwowarski;Mounia Lalmas
#t 2004
#c 1
#% 345708
#% 575729
#! Comparing retrieval approaches requires test collections, which consist of documents, queries and relevance assessments. Obtaining consistent and exhaustive relevance assessments is crucial for the appropriate comparison of retrieval approaches. Whereas the evaluation methodology for flat text retrieval approaches is well established, the evaluation of XML retrieval approaches is a research issue. This is because XML documents are composed of nested components that cannot be considered independent in terms of relevance. This paper describes the methodology adopted in INEX (the INitiative for the Evaluation of XML Retrieval) to ensure consistent and exhaustive relevance assessments.

#index 783527
#* Processing content-oriented XPath queries
#@ Börkur Sigurbjörnsson;Jaap Kamps;Maarten de Rijke
#t 2004
#c 1
#% 169811
#% 194254
#% 218982
#% 340928
#% 340948
#% 345712
#% 387427
#% 397358
#% 642993
#% 766416
#% 766417
#% 772027
#% 773036
#! Document-centric XML collections contain text-rich documents, marked up with XML tags that add lightweight semantics to the text. Querying such collections calls for a hybrid query language: the text-rich nature of the documents suggests a content-oriented (IR) approach, while the mark-up allows users to add structural constraints to their IR queries. Hybrid queries tend to be more expressive, which should lead---in principle---to better retrieval performance. In practice, the processing of these hybrid queries within an IR systems turns out to be far from trivial, because a delicate balance between structural and content information needs to be sought. We propose an approach to processing such hybrid content-and-structure queries that decomposes a query into multiple content-only queries whose results are then combined in ways determined by the structural constraints of the original query. We evaluate our methods using the INEX 2003 test-suite, and show (1) that effective ways of processing of content-oriented XPath queries are non-trivial, (2) that there are differences in the effectiveness for different topics types, but (3) that with appropriate processing methods retrieval effectiveness can improve.

#index 783528
#* Local methods for estimating pagerank values
#@ Yen-Yu Chen;Qingqing Gan;Torsten Suel
#t 2004
#c 1
#% 262061
#% 268073
#% 268079
#% 268087
#% 268186
#% 290830
#% 309748
#% 330609
#% 340141
#% 340932
#% 348173
#% 413614
#% 433672
#% 438136
#% 503228
#% 510723
#% 577328
#% 577329
#% 577330
#% 659994
#! The Google search engine uses a method called PageRank, together with term-based and other ranking techniques, to order search results returned to the user. PageRank uses link analysis to assign a global importance score to each web page. The PageRank scores of all the pages are usually determined off-line in a large-scale computation on the entire hyperlink graph of the web, and several recent studies have focused on improving the efficiency of this computation, which may require multiple hours on a workstation. However, in some scenarios, such as online analysis of link evolution and mining of large web archives such as the Internet Archive, it may be desirable to quickly approximate or update the PageRanks of individual nodes without performing a large-scale computation on the entire graph. We address this problem by studying several methods for efficiently estimating the PageRank score of a particular web page using only a small subgraph of the entire web. In our model, we assume that the graph is accessible remotely via a link database (such as the AltaVista Connectivity Server) or is stored in a relational database that performs lookups on disks to retrieve node and connectivity information. We show that a reasonable estimate of the PageRank value of a node is possible in most cases by retrieving only a moderate number of nodes in the local neighborhood of the node.

#index 783529
#* The liberal media and right-wing conspiracies: using cocitation information to estimate political orientation in web documents
#@ Miles Efron
#t 2004
#c 1
#% 109307
#% 249110
#% 260245
#% 268079
#% 290830
#% 348173
#% 376266
#% 409279
#% 420077
#% 529193
#% 577355
#% 577356
#% 616104
#% 746885
#% 748499
#% 815915
#% 854646
#% 938686
#! This paper introduces a simple method for estimating cultural orientation, the affiliation of online entities in a polarized field of discourse. In particular, cocitation information is used to estimate the political orientation of hypertext documents. A type of cultural orientation, the political orientation of a document is the degree to which it participates in traditionally left- or right-wing beliefs. Estimating documents' political orientation is of interest for personalized information retrieval and recommender systems. In its application to politics, the method uses a simple probabilistic model to estimate the strength of association between a document and left- and right-wing communities. The model estimates the likelihood of cocitation between a document of interest and a small number of documents of known orientation. The model is tested on three sets of data, 695 partisan web documents, 162 political weblogs, and 72 non-partisan documents. Accuracy above 90% is obtained from the cocitation model, outperforming lexically based classifiers at statistically significant levels.

#index 783530
#* Associative document retrieval by query subtopic analysis and its application to invalidity patent search
#@ Toru Takaki;Atsushi Fujii;Tetsuya Ishikawa
#t 2004
#c 1
#% 144012
#% 309127
#% 643006
#% 748583
#% 855011
#% 855223
#! We propose an associative document retrieval method, in which a document is used as a query to search for other similar documents. Because a long document usually includes more than one topic, we first analyze a query document to extract multiple subtopics. For each subtopic element, a sub-query is produced and similar documents are retrieved with a relevance score. The relevance scores are weighted by the importance of each subtopic element and are integrated to determine the final relevant documents. In the calculation of the subtopic importance, the specificity of a query term is evaluated using entropy, which is the deviation degree of the appearances of the term in each subtopic element. We apply this method to an invalidity patent search. By exploiting certain unique features of Japanese patent claims, we use features distinguishing the preamble and the essential portion in a query patent claim. To demonstrate the effectiveness of our method, we experimentally evaluated our associative document retrieval method on five years of patent documents.

#index 783531
#* Taxonomy-driven computation of product recommendations
#@ Cai-Nicolas Ziegler;Georg Lausen;Lars Schmidt-Thieme
#t 2004
#c 1
#% 124010
#% 173879
#% 202011
#% 220706
#% 220709
#% 220712
#% 249135
#% 280852
#% 301590
#% 304425
#% 319705
#% 330687
#% 378485
#% 378577
#% 420515
#% 490939
#% 578684
#% 723398
#% 734590
#% 993934
#% 1275285
#% 1650569
#! Recommender systems have been subject to an enormous rise in popularity and research interest over the last ten years. At the same time, very large taxonomies for product classification are becoming increasingly prominent among e-commerce systems for diverse domains, rendering detailed machine-readable content descriptions feasible. Amazon.com makes use of an entire plethora of hand-crafted taxonomies classifying books, movies, apparel, and various other goods. We exploit such taxonomic background knowledge for the computation of personalized recommendations. Hereby, relationships between super-concepts and sub-concepts constitute an important cornerstone of our novel approach, providing powerful inference opportunities for profile generation based upon the classification of products that customers have chosen. Ample empirical analysis, both offline and online, demonstrates our proposal's superiority over common existing approaches when user information is sparse and implicit ratings prevail.

#index 783532
#* Computing consistent query answers using conflict hypergraphs
#@ Jan Chomicki;Jerzy Marcinkowski;Slawomir Staworko
#t 2004
#c 1
#% 101956
#% 273687
#% 384978
#% 398243
#% 460928
#% 464915
#% 465052
#% 465057
#% 476576
#% 496003
#% 519568
#% 572311
#% 576116
#% 582130
#% 727668
#% 752741
#% 1279213
#% 1347336
#! A consistent query answer in a possibly inconsistent database is an answer which is true in every (minimal) repair of the database. We present here a practical framework for computing consistent query answers for large, possibly inconsistent relational databases. We consider relational algebra queries without projection, and denial constraints. Because our framework handles union queries, we can effectively (and efficiently) extract indefinite disjunctive information from an inconsistent database. We describe a number of novel optimization techniques applicable in this context and summarize experimental results that validate our approach.

#index 783533
#* Motion adaptive indexing for moving continual queries over moving objects
#@ Bugra Gedik;Kun-Lung Wu;Philip Yu;Ling Liu
#t 2004
#c 1
#% 116082
#% 252304
#% 273706
#% 295512
#% 300174
#% 442615
#% 443298
#% 443327
#% 458853
#% 480473
#% 495433
#% 500896
#% 527187
#% 564133
#% 576115
#% 753434
#% 975260
#% 993955
#% 1015320
#! This paper describes a motion adaptive indexing scheme for efficient evaluation of moving continual queries (MCQs) over moving objects. It uses the concept of motion-sensitive bounding boxes (MSBs) to model moving objects and moving queries. These bounding boxes automatically adapt their sizes to the dynamic motion behaviors of individual objects. Instead of indexing frequently changing object positions, we index less frequently changing object and query MSBs, where updates to the bounding boxes are needed only when objects and queries move across the boundaries of their boxes. This helps decrease the number of updates to the indexes. More importantly, we use predictive query results to optimistically precalculate query results, decreasing the number of searches on the indexes. Motion-sensitive bounding boxes are used to incrementally update the predictive query results. Our experiments show that the proposed motion adaptive indexing scheme is efficient for the evaluation of moving continual range queries.

#index 783534
#* On lossy time decompositions of time stamped documents
#@ Parvathi Chundi;Daniel J. Rosenkrantz
#t 2004
#c 1
#% 287196
#% 309096
#% 460862
#% 466506
#% 477479
#% 481611
#% 729931
#! Constructing time decompositions of time stamped documents is an important first step in extracting temporal information from a document set. Efficient algorithms are described for computing optimal lossy decompositions for a given document set, where the loss of information is constrained to be within a specified bound. A novel and efficient algorithm is proposed for computing information loss values required to construct optimal lossy decompositions. Experimental results are reported comparing optimal lossy decompositions and equal length decompositions in terms of a number of parameters such as information loss. In particular, our results show that optimal lossy decompositions outperform equal length decompositions by preserving more of the information content of the underlying document set. The results also demonstrate that permitting even small amounts of variability in the length of the subintervals of a decomposition results in capturing more of the temporal information content of a document set when compared to equal length decompositions. This paper builds upon our earlier work on time decompositions where the problem of computing optimal lossy decomposition of the time period associated with a document set was first formulated.

#index 783535
#* Event threading within news topics
#@ Ramesh Nallapati;Ao Feng;Fuchun Peng;James Allan
#t 2004
#c 1
#% 230532
#% 340883
#% 445316
#% 466501
#% 575568
#% 730043
#% 816132
#! With the overwhelming volume of online news available today, there is an increasing need for automatic techniques to analyze and present news to the user in a meaningful and efficient manner. Previous research focused only on organizing news stories by their topics into a flat hierarchy. We believe viewing a news topic as a flat collection of stories is too restrictive and inefficient for a user to understand the topic quickly. In this work, we attempt to capture the rich structure of events and their dependencies in a news topic through our event models. We call the process of recognizing events and their dependencies event threading. We believe our perspective of modeling the structure of a topic is more effective in capturing its semantics than a flat list of on-topic stories. We formally define the novel problem, suggest evaluation metrics and present a few techniques for solving the problem. Besides the standard word based features, our approaches take into account novel features such as temporal locality of stories for event recognition and time-ordering for capturing dependencies. Our experiments on a manually labeled data sets show that our models effectively identify the events and capture dependencies among them.

#index 783536
#* Approximating the top-m passages in a parallel question answering system
#@ Charles L. A. Clarke;Egidio L. Terra
#t 2004
#c 1
#% 227894
#% 280833
#% 290703
#% 309127
#% 328532
#% 340953
#% 399762
#% 445326
#% 480819
#% 642979
#% 642993
#% 730016
#% 730036
#% 741890
#% 765418
#% 766415
#% 766416
#% 1221076
#! We examine the problem of retrieving the top-m ranked items from a large collection, randomly distributed across an n-node system. In order to retrieve the top m overall, we must retrieve the top m from the subcollection stored on each node and merge the results. However, if we are willing to accept a small probability that one or more of the top-m items may be missed, it is possible to reduce computation time by retrieving only the top k from each node. In this paper, we demonstrate that this simple observation can be exploited in a realistic application to produce a substantial efficiency improvement without compromising the quality of the retrieved results. To support our claim, we present a statistical model that predicts the impact of the optimization. The paper is structured around a specific application~---~passage retrieval for question answering~---~but the primary results are more broadly applicable.

#index 783537
#* Dynamic extraction topic descriptors and discriminators: towards automatic context-based topic search
#@ Ana Maguitman;David Leake;Thomas Reichherzer;Filippo Menczer
#t 2004
#c 1
#% 46803
#% 281251
#% 308745
#% 319666
#% 413586
#% 512440
#% 516174
#% 723396
#% 730007
#% 836006
#% 1275346
#! Effective knowledge management may require going beyond initial knowledge capture, to support decisions about how to extend previously-captured knowledge. Electronic concept maps, interlinked with other concept maps and multimedia resources, can provide rich knowledge models for human knowledge capture and sharing. This paper presents research on methods for supporting experts as they extend these knowledge models, by searching the Web for new context-relevant topics as candidates for inclusion. This topic search problem presents two challenges: First, how to formulate queries to seek topics that reflect the context of the current knowledge model, and, second, how to identify candidate topics with the right balance of novelty and relevance. More generally, this problem raises the broad question of the interaction of topic information from the local analysis space (a collected set of documents) and the global search space (the Web). The paper develops a framework for understanding this interaction, and proposes and evaluates techniques for addressing the query formation and topic identification questions by dynamically extracting topic descriptors and discriminators from a knowledge model, to characterize information needs for retrieval and filtering of relevant material. Using these techniques, we have developed a support tool that starts from a knowledge model under construction and automatically produces a set of suggestions for topics to include, proactively supporting users as they extend knowledge models.

#index 783538
#* Design of a data warehouse system for network/web services
#@ Anoop Singhal
#t 2004
#c 1
#% 199537
#% 207552
#% 223781
#% 250053
#% 269634
#% 316709
#% 333955
#% 413556
#% 632040
#% 632080
#! This paper describes the architecture and design of a data warehouse for AT&T Business Services. The main purpose of our system is to generate reports about the performance and reliability of the network. We describe the architecture of our system and discuss some open research problems in this area.

#index 783539
#* InfoAnalyzer: a computer-aided tool for building enterprise taxonomies
#@ Li Zhang;ShiXia Liu;Yue Pan;LiPing Yang
#t 2004
#c 1
#% 116165
#% 218982
#% 236729
#% 268079
#% 280492
#% 280817
#% 316901
#% 340942
#% 445319
#% 565531
#! In this paper we study the problem of collecting training samples for building enterprise taxonomies. We develop a computer-aided tool named InfoAnalyzer, which can effectively assist the enterprise to prepare large set of samples used for machine learning in text categorization. In our system, the enterprise category tree is initially defined by some keywords, then the Google search engine is used to construct a small set of labeled documents, and topic tracking algorithm based on document length normalization is applied to enlarge the training corpus on the bases of the seed stories. Furthermore, we design a method to check the consistency of the training corpus. Experiments show that the training corpus is good enough for statistical classification methods and meets human's requirements as well.

#index 783540
#* RStar: an RDF storage and query system for enterprise resource management
#@ Li Ma;Zhong Su;Yue Pan;Li Zhang;Tao Liu
#t 2004
#c 1
#% 330710
#% 348181
#% 519567
#% 644923
#% 787552
#! Modern corporations operate in an extremely complex environment and strongly depend on all kinds of information resources across the enterprise. Unfortunately, with the growth of an enterprise, its information resources are not only heterogeneous but also distributed in physically different systems and databases. How to effectively exploit information across the enterprise is becoming a critical but hard problem. In recent years, metadata which is the detailed description of the data is used to efficiently exploit information resources in the web. The World Wide Web Consortium (W3C) recommends the resource description framework (RDF) as a standard for the definition and use of metadata descriptions of resources in the web. In this paper, we present an RDF storage and query system called RStar for enterprise resource management. RStar uses a relational database as the persistent data store and defines RStar Query Language (RSQL) for resource retrieval. Currently, most of existing RDF storage and query systems are evaluated on small data sets and no detailed performance analysis is given for such systems. Therefore, we conduct extensive experiments on a large scale data set to investigate the performance problem in RDF storage. Such analysis will be helpful for designing RDF storage and query systems as well as for understanding not well-solved issues in RDF based enterprise resource management. In addition, experiences and lessons learned in our implementation are presented for further research and development.

#index 783541
#* Processing search queries in a distributed environment
#@ Frederick Knabe;Daniel Tunkelang
#t 2004
#c 1
#% 194245
#% 635851
#! Endeca's approach to processing search queries in a distributed computing environment is predicated on concerns of correctness, scalability, and flexibility in deployment. Using a master-slave architecture, we are able to support classic search as well as more advanced features. We avoid bottlenecking the master with excessive computation or communication by limiting the information from the slaves in the expected case.

#index 783542
#* Intelligent agent for automated manufacturing rule generation
#@ Alan Clark;Dimitar Filev
#t 2004
#c 1
#% 276754
#% 291942
#% 643933

#index 783543
#* Document clustering based on cluster validation
#@ Zheng-Yu Niu;Dong-Hong Ji;Chew-Lim Tan
#t 2004
#c 1
#% 118771
#% 177826
#% 227794
#% 232655
#% 262045
#% 309128
#% 342621
#% 342659
#% 397139
#% 397147
#% 397148
#% 466395
#% 466414
#% 466675
#% 580509
#% 643008
#% 729918
#% 856734
#! This paper presents a cluster validation based document clustering algorithm, which is capable of identifying both important feature words and true model order (cluster number). Important feature subset is selected by optimizing a cluster validity criterion subject to some constraint. For achieving model order identification capability, this feature selection procedure is conducted for each possible value of cluster number. The feature subset and cluster number which maximize the cluster validity criterion are chosen as our answer. We have applied our algorithm to several datasets from 20Newsgroup corpus. Experimental results show that our algorithm can find important feature subset, estimate the model order and yield higher micro-averaged precision than other four document clustering algorithms which require cluster number to be provided.

#index 783544
#* Circumstance-based categorization analysis of knowledge management systems for the japanese market
#@ Makoto Sano;David A. Evans
#t 2004
#c 1
#% 128268
#% 344300
#% 523009
#% 615780
#% 804912
#! We conducted a survey of thirty of the approximately 1,700 customers of Justsystem Corporation's knowledge-management applications. Our goal was to discover the kinds of functions that customers hoped to address in their next-generation use of knowledge management technology and to assess the core processes that we will need to deploy in our products to address their desired solutions. In particular, we sought to analyze our customers' requirements along dimensions that take account of both the context of use of the application and its stage in the cycle of knowledge creation and use. As part of our analysis, we were able to classify all customer cases as focused by one or more of three Goals, supported by one or more of eleven technology Means. To establish appropriate categories of use, we exploited the stages of the SECI Model, several other transactional categories of knowledge use, and whether activities were targeted at internal or external users. Through the analysis, we found the typical technology components (Means) for each stage of knowledge creation and use associated with each set of goals. We consider such analysis essential to the task of designing next-generation knowledge-management applications and critical to overcoming the unfortunate tendency of developers to devise solutions that bear little relation to the true needs of users.

#index 783545
#* Database support for species extraction from the biosystematics literature: a feasibility demonstration
#@ Ralf Duckstein;Klemens Böhm
#t 2004
#c 1
#% 213981
#% 262084
#% 273922
#% 321635
#% 340914
#% 385946
#% 397358
#% 458742
#% 504578
#! A part of the biosystematics literature is currently being digitized and manually marked up with XML. Fast search on such documents shall be feasible. But marking up such documents incurs high costs, and biologists would like to know the value of such an activity in advance. Deploying standard XML database technology in a straightforward way is not feasible, because of two characteristics of biosystematics documents. The first one is that descriptions of taxa are related, i.e., a more specific taxon should inherit from a more general one. The combination of inheritance with information-retrieval mechanisms gives rise to difficulties addressed in this article. The second issue is the frequent occurrence of very specific technical terms in such documents, i.e., geographical information or biological terms. To investigate the characteristics of the search in the presence of such difficulties, we have designed and implemented a respective system, based on relational database technology. We use a collection of XML documents that mimics the characteristics of biosystematics documents, as we will explain. We propose two query-evaluation alternatives and compare them by means of performance experiments. It turns out that our techniques can administer the envisioned corpus of documents efficiently and cope with those problems at the same time.

#index 783546
#* Virtual cursors for XML joins
#@ Beverly Yang;Marcus Fontoura;Eugene Shekita;Sridhar Rajagopalan;Kevin Beyer
#t 2004
#c 1
#% 333981
#% 387508
#% 397360
#% 397366
#% 397375
#% 406493
#% 479465
#% 654442
#% 654450
#% 765406
#% 979743
#% 993951
#% 993953
#% 1015277
#% 1016222
#! Structural joins are a fundamental operation in XML query processing and a large body of work has focused on index-based algorithms for executing them. In this paper, we describe how two well-known index features -- path indices and ancestor information -- can be combined in a novel way to replace one or more of the physical index cursors in a structural join with virtual cursors. The position of a virtual cursor is derived from the path and ancestor information of a physical cursor. Implementation results are provided to show that, by eliminating index I/O, virtual cursors can improve the performance of structural joins by an order of magnitude or more.

#index 783547
#* Efficient processing of XML twig patterns with parent child edges: a look-ahead approach
#@ Jiaheng Lu;Ting Chen;Tok Wang Ling
#t 2004
#c 1
#% 333981
#% 397366
#% 397375
#% 480489
#% 481599
#% 659999
#% 765405
#% 765406
#% 1015277
#! With the growing importance of semi-structure data in information exchange, much research has been done to provide an effective mechanism to match a twig query in an XML database. A number of algorithms have been proposed recently to process a twig query holistically. Those algorithms are quite efficient for quires with only ancestor-descendant edges. But for queries with mixed ancestor-descendant and parent-child edges, the previous approaches still may produce large intermediate results, even when the input and output size are more manageable. To overcome this limitation, in this paper, we propose a novel holistic twig join algorithm, namely TwigStackList. Our main technique is to look-ahead read some elements in input data steams and cache limited number of them to lists in the main memory. The number of elements in any list is bounded by the length of the longest path in the XML document. We show that TwigStackList is I/O optimal for queries with only ancestor-descendant relationships below branching nodes. Further, even when queries contain parent-child relationship below branching nodes, the set of intermediate results in TwigStackList is guaranteed to be a subset of that in previous algorithms. We complement our experimental results on a range of real and synthetic data to show the significant superiority of TwigStackList over previous algorithms for queries with parent-child relationships.

#index 783548
#* QFilter: fine-grained run-time XML access control via NFA-based query rewriting
#@ Bo Luo;Dongwon Lee;Wang-Chien Lee;Peng Liu
#t 2004
#c 1
#% 91075
#% 100862
#% 204453
#% 240055
#% 309716
#% 314755
#% 344639
#% 378393
#% 379248
#% 442862
#% 443057
#% 650962
#% 725290
#% 765447
#% 993971
#% 993972
#% 1016136
#! At present, most of the state-of-the-art solutions for XML access controls are either (1) document-level access control techniques that are too limited to support fine-grained security enforcement; (2) view-based approaches that are often expensive to create and maintain; or (3) impractical proposals that require substantial security-related support from underlying XML databases. In this paper, we take a different approach that assumes no security support from underlying XML databases and examine three alternative fine-grained XML access control solutions, namely primitive, pre-processing and post-processing approaches. In particular, we advocate a pre-processing method called QFilter that uses Non-deterministic Finite Automata (NFA) to rewrite user's query such that any parts violating access control rules are pruned. We show the construction and execution of a QFilter and demonstrate its superiority to other competing methods.

#index 783549
#* CiteSeer-API: towards seamless resource location and interlinking for digital libraries
#@ Yves Petinot;C. Lee Giles;Vivek Bhatnagar;Pradeep B. Teregowda;Hui Han;Isaac Councill
#t 2004
#c 1
#% 249141
#% 249143
#% 281446
#% 287209
#% 614058
#% 643013
#% 727912
#% 760879
#! We introduce CiteSeer-API, a public API to CiteSeer-like services. CiteSeer-API is SOAP/WSDL based and allows for easy programmatical access to all the specific functionalities offered by CiteSeer services, including full text search of documents and citations and citation-based document discovery. In order to enable operability and interlinking with arbitrary software agents and digital library systems, CiteSeer-API uses digital content signatures to create system-independent handles for the Document, Citation and Group resources of CiteSeer servers. We discuss specific functionalities of CiteSeer-API that take advantage of these handlers in order to enable seamless location of CiteSeer resources. Finally we argue that the digital signature scheme used by CiteSeer-API is well suited for the creation of machine-usable semantic descriptions of digital library services which is the key toward seamless discovery and integration of services such as CiteSeer-API. CiteSeer-API is currently showcased on CiteSeer.IST, the CiteSeer server of the School of Information Science and Technology at the Pennsylvania State University.

#index 783550
#* The robustness of content-based search in hierarchical peer to peer networks
#@ M. Elena Renda;Jamie Callan
#t 2004
#c 1
#% 227891
#% 280856
#% 287463
#% 309093
#% 337046
#% 340175
#% 340176
#% 397441
#% 413583
#% 496286
#% 505869
#% 636009
#% 730035
#% 830692
#! Hierarchical peer to peer networks with multiple directory services are an important architecture for large-scale file sharing due to their effectiveness and efficiency. Recent research argues that they are also an effective method of providing large-scale content-based federated search of text-based digital libraries. In both cases the directory services are critical resources that are subject to attack or failure, but the latter architecture may be particularly vulnerable because content is less likely to be replicated throughout the network. This paper studies the robustness, effectiveness and efficiency of content-based federated search in hierarchical peer to peer networks when directory services fail unexpectedly. Several recovery methods are studied using simulations with varying failure rates. Experimental results show that quality of service and efficiency degrade gracefully as the number of directory service failures increases. Furthermore, they show that content-based search mechanisms are more resilient to failures than the match-based search techniques.

#index 783551
#* SERF: integrating human recommendations with search
#@ Seikyung Jung;Kevin Harris;Janet Webster;Jonathan L. Herlocker
#t 2004
#c 1
#% 202009
#% 202011
#% 308763
#% 387427
#% 415107
#% 420515
#% 427915
#% 642981
#% 642985
#% 643000
#% 717120
#% 730051
#% 779952
#% 993934
#! Today's university library has many digitally accessible resources, both indexes to content and considerable original content. Using off-the-shelf search technology provides a single point of access into library resources, but we have found that such full-text indexing technology is not entirely satisfactory for library searching. In response to this, we report initial usage results from a prototype of an entirely new type of search engine - The System for Electronic Recommendation Filtering (SERF) - that we have designed and deployed for the Oregon State University (OSU) Libraries. SERF encourages users to enter longer and more informative queries, and collects ratings from users as to whether search results meet their information need or not. These ratings are used to make recommendations to later users with similar needs. Over time, SERF learns from the users what documents are valuable for what information needs. In this paper, we focus on understanding whether such recommendations can increase other users' search efficiency and effectiveness in library website searching. Based on examination of three months of usage as an alternative search interface available to all users of the Oregon State University Libraries website (http://osulibrary.oregonstate.edu/), we found strong evidence that the recommendations with human evaluation could increase the efficiency as well as effectiveness of the library website search process. Those users who received recommendations needed to examine fewer results, and recommended documents were rated much higher than documents returned by a traditional search engine.

#index 783552
#* Weakly-supervised relation classification for information extraction
#@ Zhu Zhang
#t 2004
#c 1
#% 190581
#% 209021
#% 252011
#% 301241
#% 458379
#% 466888
#% 504443
#% 646547
#% 722926
#% 748550
#% 815796
#% 815908
#% 816081
#% 843654
#% 1677139
#! This paper approaches the relation classification problem in information extraction framework with bootstrapping on top of Support Vector Machines. A new bootstrapping algorithm is proposed and empirically evaluated on the ACE corpus. We show that the supervised SVM classifier using various lexical and syntactic features can achieve promising classification accuracy. More importantly, the proposed BootProject algorithm based on random feature projection can significantly reduce the need for labeled training data with only limited sacrifice of performance.

#index 783553
#* TEG: a hybrid approach to information extraction
#@ Benjamin Rosenfeld;Ronen Feldman;Moshe Fresko;Jonathan Schler;Yonatan Aumann
#t 2004
#c 1
#% 266216
#% 278107
#% 464434
#% 466892
#% 531459
#% 532065
#% 575975
#% 740925
#% 742218
#% 742424
#% 746865
#% 786561
#% 815178
#% 1414746
#! This paper describes a hybrid statistical and knowledge-based information extraction model, able to extract entities and relations at the sentence level. The model attempts to retain and improve the high accuracy levels of knowledge-based systems while drastically reducing the amount of manual labor by relying on statistics drawn from a training corpus. The implementation of the model, called TEG (Trainable Extraction Grammar), can be adapted to any IE domain by writing a suitable set of rules in a SCFG (Stochastic Context Free Grammar) based extraction language, and training them using an annotated corpus. The system does not contain any purely linguistic components, such as PoS tagger or parser. We demonstrate the performance of the system on several named entity extraction and relation extraction tasks. The experiments show that our hybrid approach outperforms both purely statistical and purely knowledge-based systems, while requiring orders of magnitude less manual rule writing and smaller amount of training data. The improvement in accuracy is slight for named entity extraction task and more pronounced for relation extraction.

#index 783554
#* Node ranking in labeled directed graphs
#@ Krishna P. Chitrapura;Srinivas R. Kashyap
#t 2004
#c 1
#% 255170
#% 262061
#% 268073
#% 290830
#% 309749
#% 309868
#% 433902
#% 438136
#% 577337
#% 641979
#! Our work is motivated by the problem of ranking hyper-linked documents for a given query. Given an arbitrary directed graph with edge and node labels, we present a new flow-based model and an efficient method to dynamically rank the nodes of this graph with respect to any of the original labels. Ranking documents for a given query in a hyper-linked document set and ranking of authors/articles for a given topic in a citation database are some typical applications of our method. We outline the structural conditions that the graph must satisfy for our ranking to be different from the traditional PageRank. We have built a system using two indices that is capable of dynamically ranking documents for any given query. We validate our system and method using experiments on a few datasets: a crawl of the IBM Intranet (12 million pages), a crawl of the www (30 million pages) and the DBLP citation dataset. We compare our method to existing schemes for topic-biased ranking that require a classifier and the traditional PageRank. In these experiments, we demonstrate that our method is well suited for fine-grained ranking and that our method performs better than the existing schemes. We also demonstrate that our system can obtain an improved ranking with very little impact on query time.

#index 783555
#* Unsupervised question answering data acquisition from local corpora
#@ Lucian Vlad Lita;Jaime Carbonell
#t 2004
#c 1
#% 340953
#% 397160
#% 748594
#% 815195
#% 815868
#% 816164
#% 816175
#% 816220
#% 817419
#% 817421
#% 817611
#% 854663
#% 938759
#! Data-driven approaches in question answering (QA) are increasingly common. Since availability of training data for such approaches is very limited, we propose an unsupervised algorithm that generates high quality question-answer pairs from local corpora. The algorithm is ontology independent, requiring very small seed data as its starting point. Two alternating views of the data make learning possible: 1) question types are viewed as relations between entities and 2) question types are described by their corresponding question-answer pairs. These two aspects of the data allow us to construct an unsupervised algorithm that acquires high precision question-answer pairs. We show the quality of the acquired data for different question types and perform a task-based evaluation. With each iteration, pairs acquired by the unsupervised algorithm are used as training data to a simple QA system. Performance increases with the number of question-answer pairs acquired confirming the robustness of the unsupervised algorithm. We introduce the notion of semantic drift and show that it is a desirable quality in training data for question answering systems.

#index 783556
#* Distributional term representations: an experimental comparison
#@ Alberto Lavelli;Fabrizio Sebastiani;Roberto Zanoli
#t 2004
#c 1
#% 85447
#% 86531
#% 115608
#% 118738
#% 132648
#% 144029
#% 158687
#% 185288
#% 218989
#% 229348
#% 269217
#% 296738
#% 311034
#% 316508
#% 344447
#% 363038
#% 504888
#% 728352
#% 731938
#% 741083
#% 747738
#% 748465
#% 816146
#% 1387555
#! A number of content management tasks, including term categorization, term clustering, and automated thesaurus generation, view natural language terms (e.g. words, noun phrases) as first-class objects, i.e. as objects endowed with an internal representation which makes them suitable for explicit manipulation by the corresponding algorithms. The information retrieval (IR) literature has traditionally used an extensional (aka distributional) representation for terms according to which a term is represented by the "bag of documents" in which the term occurs. The computational linguistics (CL) literature has independently developed an alternative distributional representation for terms, according to which a term is represented by the "bag of terms" that co-occur with it in some document. This paper aims at discovering which of the two representations is most effective, i.e. brings about higher effectiveness once used in tasks that require terms to be explicitly represented and manipulated. We carry out experiments on (i) a term categorization task, and (ii) a term clustering task; this allows us to compare the two different representations in closely controlled experimental conditions. We report the results of experiments in which we categorize/cluster under 42 different classes the terms extracted from a corpus of more than 65,000 documents. Our results show a substantial difference in effectiveness between the two representation styles; we give both an intuitive explanation and an information-theoretic justification for these different behaviours.

#index 783557
#* Stemming and lemmatization in the clustering of finnish text documents
#@ Tuomo Korenius;Jorma Laurikkala;Kalervo Järvelin;Martti Juhola
#t 2004
#c 1
#% 46809
#% 57990
#% 115478
#% 144034
#% 208934
#% 270945
#% 286307
#% 376266
#% 420519
#% 420540
#% 575729
#% 852321
#% 1200254
#! Stemming and lemmatization were compared in the clustering of Finnish text documents. Since Finnish is a highly inflectional and agglutinative language, we hypothesized that lemmatization, involving splitting of the compound words, would be more appropriate normalization approach than the straightforward stemming. The relevance of the documents were evaluated with a four-point relevance assessment scale, which was collapsed into binary one by considering all the relevant and only the highly relevant documents relevant, respectively. Experiments with four hierarchical clustering methods supported the hypothesis. The stringent relevance scale showed that lemmatization allowed the single and complete linkage methods to recover especially the highly relevant documents better than stemming. In comparison with stemming, lemmatization together with the average linkage and Ward's methods produced higher precision. We conclude that lemmatization is a better word normalization method than stemming, when Finnish text documents are clustered for information retrieval.

#index 783558
#* Towards smarter documents
#@ Vikas Krishna;Prasad M. Deshpande;Savitha Srinivasan
#t 2004
#c 1
#% 80410
#% 202107
#% 234361
#% 248845
#% 248854
#% 287631
#% 311802
#% 481750
#% 625403
#% 625508
#% 731010
#! Document analysis research typically focuses on document image understanding or classic problems in text classification, clustering, summarization and discovery. While that is an important aspect of document management, in practice, documents lifecycles are often determined by the context of the business process that they are relevant to. It therefore becomes necessary for the document analysis techniques to recognize and leverage the contextual information provided by a supporting schema and business process. This paper presents an intelligent document management framework with relevant document analysis, metadata extraction, and business process association algorithms and methodology. The architecture supporting this framework seamlessly integrates a runtime environment with an authoring environment by combining relational data modeling tools with document classification techniques. The runtime environment accepts incoming documents, classifies the document, extracts metadata and executes customized business logic. The authoring environment supports the association of a class of documents with a relational document schema, identification of attribute values that must be extracted automatically, generation of relevant business logic, and deployment of authoring artifacts into the runtime architecture. We demonstrate the use of this framework with representative real-world document transformative applications.

#index 783559
#* On structuring formal, semi-formal and informal data to support traceability in systems engineering environments
#@ Paul Mason;Ken Cosh;Pulyamon Vihakapirom
#t 2004
#c 1
#% 36309
#% 148351
#% 157705
#% 183913
#% 262278
#% 357942
#! The development of large, complex systems poses a number of challenges for systems engineers, not least of which is the ability to ensure user requirements have been satisfied. Effective requirements management - an amalgam of information capture, information storage and management, and information dissemination activities - is crucial in that respect. In this paper we concentrate on one of the core issues of information management in a requirements management context - namely traceability. Traceability is the common term for mechanisms to record and navigate relationships between artifacts produced by development processes. However, realising effective traceability in systems engineering environments is complicated by the fact that engineers use a range of notations to describe complex systems. These range from natural language (informal), to graphical notations such as Statecharts (semi-formal) to languages with a well defined (formal) semantics such as VDM-SL and SPARK Ada. Most have tool support, although a lack of well-defined approaches to integration leads to inconsistencies and limits traceability between their respective data sets (internal models). This paper demonstrates an approach based on meta-modelling that enables traceability links to be established and consistency maintained between tools.

#index 783560
#* Swoogle: a search and metadata engine for the semantic web
#@ Li Ding;Tim Finin;Anupam Joshi;Rong Pan;R. Scott Cost;Yun Peng;Pavan Reddivari;Vishal Doshi;Joel Sachs
#t 2004
#c 1
#% 240957
#% 281189
#% 413603
#% 535998
#% 543700
#% 655350
#% 665856
#! Swoogle is a crawler-based indexing and retrieval system for the Semantic Web. It extracts metadata for each discovered document, and computes relations between documents. Discovered documents are also indexed by an information retrieval system which can use either character N-Gram or URIrefs as keywords to find relevant documents and to compute the similarity among a set of documents. One of the interesting properties we compute is ontology rank, a measure of the importance of a Semantic Web document.

#index 838385
#* Leonardo's laptop: human needs and the new computing technologies
#@ Ben Shneiderman
#t 2005
#c 1
#! The old computing was about what computers could do; the new computing is about what people can do.To accelerate the shift from the old to the new computing designers need to:reduce computer user frustration. Recent studies show 46% of time is lost to crashes, confusing instructions, navigation problems, etc. Public pressure for change could promote design improvements and increase reliability, thereby dramatically enhancing user experiences.promote universal usability. Interfaces must be tailorable to a wide range of hardware, software, and networks, and users. When broad services such as voting, healthcare, and education are envisioned, the challenge to designers is substantial.envision a future in which human needs more directly shape technology evolution. Four circles of human relationships and four human activities map out the human needs for mobility, ubiquity, creativity, and community. The World Wide Med and million-person communities will be accessible through desktop, palmtop and fingertip devices to support e-learning, e-business, e-healthcare, and e-government.Leonardo da Vinci could help as an inspirational muse for the new computing. His example could push designers to improve quality through scientific study and more elegant visual design. Leonardo's example can guide us to the new computing, which emphasizes empowerment, creativity, and collaboration. Information visualization and personal photo interfaces will be shown: PhotoMesa (www.cs.umd.edu/hcil/photomesa) and PhotoFinder (www.cs.umd.edu/hcil/photolib).For more: http://mitpress.mit.edu/leonardoslaptop and http://www.cs.umd.edu/hcil/newcomputing.

#index 838386
#* Emerging data management systems: close-up and personal
#@ Yannis Ioannidis
#t 2005
#c 1
#! Conventional data management occurs primarily in centralized servers or in well-interconnected distributed systems. These are removed from their end users, who interact with the systems mostly through static devices to obtain generic services around main-stream applications: banking, retail, business management, etc. Several recent advances in technologies, however, give rise to a new breed of applications, which change altogether the user experience and sense of data management. Very soon several such systems will be in our pockets, many more in our homes, the kitchen appliances, our clothes, etc. How would these systems operate? Many system and user aspects must be approached in novel ways, while several new issues come up and need to be addressed for the first time. Highlights include personalization, privacy, information trading, annotation, new interaction devices and corresponding interfaces, visualization, etc. In this talk, we take a close look at and give a very personal guided tour to this emerging world of data management, offering some thoughts on how the new technical challenges might be approached.

#index 838387
#* From bits and bytes to information and knowledge
#@ Thomas Hofmann
#t 2005
#c 1
#! Unstructured data is a valuable source of information and implicit knowledge. Yet, the bits and bytes of, e.g., text, image, or click-stream data need to be interpreted in order to transform them into business intelligence and actionable information. Clearly, this process needs to be automated to the largest possible extend in order to be scalable to the typical volumes of data. One way to accomplish this is through the use of machine learning and statistical modelling techniques. This talk will provide an overview of recent progress and new trends in machine learning and discuss their relevance for developing intelligent tools for search, information filtering, categorization, and knowledge extraction.

#index 838388
#* Structured queries in XML retrieval
#@ Jaap Kamps;Maarten Marx;Maarten de Rijke;Börkur Sigurbjörnsson
#t 2005
#c 1
#% 184486
#% 269563
#% 333841
#% 338753
#% 465065
#% 642993
#% 648308
#% 783527
#% 824798
#% 993939
#% 1387547
#! Document-centric XML is a mixture of text and structure. With the increased availability of document-centric XML content comes a need for query facilities in which both structural constraints and constraints on the content of the documents can be expressed. How does the expressiveness of languages for querying XML documents help users to express their information needs? We address this question from both an experimental and a theoretical point of view. Our experimental analysis compares a structure-ignorant with a structure-aware retrieval approach using the test-suite of the 2004 edition of the INEX XML retrieval evaluation initiative. Theoretically, we create mathematical models of users' knowledge of a set of documents and define query languages which exactly fit these models. One of these languages corresponds to an XML version of fielded search, the other to the INEX query language. Our main findings are: First, while structure is used in varying degrees of complexity, over half of the queries can be expressed in a fielded-search like format which does not use the hierarchical structure of the documents. Second, structure is used as a search hint, and not a strict requirement, when judged against the underlying information need. Third, the use of structure in queries functions as a precision enhancing device.

#index 838389
#* Score region algebra: building a transparent XML-R database
#@ Vojkan Mihajlović;Henk Ernst Blok;Djoerd Hiemstra;Peter M. G. Apers
#t 2005
#c 1
#% 118743
#% 169781
#% 191574
#% 237053
#% 309726
#% 333981
#% 340914
#% 385946
#% 406493
#% 479987
#% 654442
#% 750867
#% 754116
#% 766416
#% 1016150
#% 1721855
#% 1721866
#% 1721871
#! A unified database framework that will enable better comprehension of ranked XML retrieval is still a challenge in the XML database field. We propose a logical algebra, named score region algebra, that enables transparent specification of information retrieval (IR) models for XML databases. The transparency is achieved by a possibility to instantiate various retrieval models, using abstract score functions within algebra operators, while logical query plan and operator definitions remain unchanged. Our algebra operators model three important aspects of XML retrieval: element relevance score computation, element score propagation, and element score combination. To illustrate the usefulness of our algebra we instantiate four different, well known IR scoring models, and combine them with different score propagation and combination functions. We implemented the algebra operators in a prototype system on top of a low-level database kernel. The evaluation of the system is performed on a collection of IEEE articles in XML format provided by INEX. We argue that state of the art XML IR models can be transparently implemented using our score region algebra framework on top of any low-level physical database engine or existing RDBMS, allowing a more systematic investigation of retrieval model behavior.

#index 838390
#* Generalized contextualization method for XML information retrieval
#@ Paavo Arvola;Marko Junkkari;Jaana Kekäläinen
#t 2005
#c 1
#% 169809
#% 292684
#% 397366
#% 766417
#% 829332
#% 1721850
#% 1721855
#% 1721860
#% 1721867
#! A general re-weighting method, called contextualization, for more efficient element ranking in XML retrieval is introduced. Re-weighting is based on the idea of using the ancestors of an element as a context: if the element appears in a good context -- good interpreted as probability of relevance -- its weight is increased in relevance scoring; if the element appears in a bad context, its weight is decreased. The formal presentation of contextualization is given in a general XML representation and manipulation frame, which is based on utilization of structural indices. This provides a general approach independent of weighting schemas or query languages.Contextualization is evaluated with the INEX test collection. We tested four runs: no contextualization, parent, root and tower contextualizations. The contextualization runs were significantly better than no contextualization. The root contextualization was the best among the re-weighted runs.

#index 838391
#* Decentralized coordination of transactional processes in peer-to-peer environments
#@ Klaus Haller;Heiko Schuldt;Can Türker
#t 2005
#c 1
#% 9241
#% 66344
#% 102753
#% 210179
#% 247424
#% 264263
#% 286836
#% 286979
#% 336201
#% 345694
#% 435104
#% 443064
#% 571055
#! Business processes executing in peer-to-peer environments usually invoke Web services on different, independent peers. Although peer-to-peer environments inherently lack global control, some business processes nevertheless require global transactional guarantees, i.e., atomicity and isolation applied at the level of processes. This paper introduces a new decentralized serialization graph testing protocol to ensure concurrency control and recovery in peer-to-peer environments. The uniqueness of the proposed protocol is that it ensures global correctness without relying on a global serialization graph. Essentially, each transactional process is equipped with partial knowledge that allows the transactional processes to coordinate. Globally correct execution is achieved by communication among dependent transactional processes and the peers they have accessed. In case of failures, a combination of partial backward and forward recovery is applied. Experimental results exhibit a significant performance gain over traditional distributed locking-based protocols with respect to the execution of transactions encompassing Web service requests.

#index 838392
#* On the complexity of computing peer agreements for consistent query answering in peer-to-peer data integration systems
#@ Gianluigi Greco;Francesco Scarcello
#t 2005
#c 1
#% 273687
#% 384978
#% 460928
#% 464915
#% 576116
#% 577359
#% 765446
#% 801692
#% 810106
#% 880394
#% 1016250
#% 1279213
#% 1279214
#% 1392016
#% 1712581
#% 1712584
#! Peer-to-Peer (P2P) data integration systems have recently attracted significant attention for their ability to manage and share data dispersed over different peer sources. While integrating data for answering user queries, it often happens that inconsistencies arise, because some integrity constraints specified on peers' global schemas may be violated. In these cases, we may give semantics to the inconsistent system by suitably "repairing" the retrieved data, as typically done in the context of traditional data integration systems. However, some specific features of P2P systems, such as peer autonomy and peer preferences (e.g., different source trusting), should be properly addressed to make the whole approach effective. In this paper, we face these issues that were only marginally considered in the literature. We first present a formal framework for reasoning about autonomous peers that exploit individual preference criteria in repairing the data. The idea is that queries should be answered over the best possible database repairs with respect to the preferences of all peers, i.e., the states on which they are able to find an agreement. Then, we investigate the computational complexity of dealing with peer agreements and of answering queries in P2P data integration systems. It turns out that considering peer preferences makes these problems only mildly harder than in traditional data integration systems.

#index 838393
#* Internet scale string attribute publish/subscribe data networks
#@ Ioannis Aekaterinidis;Peter Triantafillou
#t 2005
#c 1
#% 336297
#% 340175
#% 340176
#% 342032
#% 496291
#% 505869
#% 555272
#% 636008
#% 646220
#% 661478
#% 674136
#% 736382
#% 745355
#% 745371
#% 772021
#% 793899
#% 818243
#% 963874
#% 1015281
#% 1722231
#% 1849768
#! With this work we aim to make a three-fold contribution. We first address the issue of supporting efficiently queries over string-attributes involving prefix, suffix, containment, and equality operators in large-scale data networks. Our first design decision is to employ distributed hash tables (DHTs) for the data network's topology, harnessing their desirable properties. Our next design decision is to derive DHT-independent solutions, treating DHT as a black box. Second, we exploit this infrastructure to develop efficient content based publish/subscribe systems. The main contribution here are algorithms for the efficient processing of queries (subscriptions) and events (publications). Specifically, we show that our subscription processing algorithms require O(logN) messages for a N-node network, and our event processing algorithms require O(l x logN) messages (with l being the average string length).Third, we develop algorithms for optimizing the processing of multi-dimensional events, involving several string attributes. Further to our analysis, we provide simulation-based experiments showing promising performance results in terms of number of messages, required bandwidth, load balancing, and response times.

#index 838394
#* Intelligent creation of notification events in information systems: concept, implementation and evaluation
#@ Michael Guppenberger;Burkhard Freitag
#t 2005
#c 1
#% 177755
#% 216284
#% 241207
#% 271199
#% 275367
#% 279905
#% 343818
#% 511917
#% 736387
#% 754075
#! An important feature of information systems is the ability to inform users about changes of the stored information. Therefore, systems have to 'know' what changes a user wants to be informed about. This is well known from the field of publish-/subscribe architectures. In this paper, we propose a solution for information system designers of how to extend their information model in a way that the notification mechanism can consider semantic knowledge when determining which parties to inform. Two different kinds of implementations are introduced and evaluated: one based on aspect oriented programming (AOP), the other one based on traditional database triggers. The evaluation of both approaches leads to a combined approach preserving the advantages of both techniques, using Model Driven Architecture (MDA) to create the triggers from a UML model enhanced with stereotypes.

#index 838395
#* Opportunity map: a visualization framework for fast identification of actionable knowledge
#@ Kaidi Zhao;Bing Liu;Thomas M. Tirpak;Weimin Xiao
#t 2005
#c 1
#% 136350
#% 280436
#% 280487
#% 310525
#% 310531
#% 342631
#% 420101
#% 434613
#% 443092
#% 481290
#% 481954
#% 577216
#% 577252
#% 641130
#% 769893
#% 769926
#! Data mining techniques frequently find a large number of patterns or rules, which make it very difficult for a human analyst to interpret the results and to find the truly interesting and actionable rules. Due to the subjective nature of "interestingness", human involvement in the analysis process is crucial. In this paper, we propose a novel visual data mining framework for the purpose of identifying actionable knowledge quickly and easily from discovered rules and data. This framework is called the Opportunity Map. It is inspired by some interesting ideas from Quality Engineering, in particular Quality Function Deployment (QFD) and the House of Quality. It associates summarized data or discovered rules with the application objective using an interactive matrix, which enables the user to quickly identify where the opportunities are. The proposed system can be used to visually analyze discovered rules, and other statistical properties of the data. The user can also interactively group actionable attributes and values, and see how they affect the targets of interest. Combined with drill-down and comparative analysis, the user can analyze rules and data at different levels of detail. The proposed visualization framework thus represents a systematic and yet flexible method of rule analysis. Applications of the system to large-scale data sets from our industrial partner have yielded promising results.

#index 838396
#* Establishing value mappings using statistical models and user feedback
#@ Jaewoo Kang;Tae Sik Han;Dongwon Lee;Prasenjit Mitra
#t 2005
#c 1
#% 201889
#% 248801
#% 258870
#% 310516
#% 333990
#% 334025
#% 479783
#% 480645
#% 551850
#% 572314
#% 654458
#% 654467
#% 660001
#% 729913
#% 765433
#% 765462
#% 840577
#% 993980
#! In this paper, we present a "value mapping" algorithm that does not rely on syntactic similarity or semantic interpretation of the values. The algorithm first constructs a statistical model (e.g., co-occurrence frequency or entropy vector) that captures the unique characteristics of values and their co-occurrence. It then finds the matching values by computing the distances between the models while refining the models using user feedback through iterations. Our experimental results suggest that our approach successfully establishes value mappings even in the presence of opaque data values and thus can be a useful addition to the existing data integration techniques.

#index 838397
#* Retrieving answers from frequently asked questions pages on the web
#@ Valentin Jijkoun;Maarten de Rijke
#t 2005
#c 1
#% 169811
#% 197538
#% 281251
#% 309126
#% 312860
#% 330616
#% 348163
#% 348956
#% 466892
#% 474645
#% 509533
#% 577319
#% 754068
#% 754069
#% 755394
#% 807358
#% 816228
#% 854791
#% 855235
#% 995494
#% 1348326
#% 1715627
#% 1721849
#! We address the task of answering natural language questions by using the large number of Frequently Asked Questions (FAQ) pages available on the web. The task involves three steps: (1) fetching FAQ pages from the web; (2) automatic extraction of question/answer (Q/A) pairs from the collected pages; and (3) answering users' questions by retrieving appropriate Q/A pairs. We discuss our solutions for each of the three tasks, and give detailed evaluation results on a collected corpus of about 3.6Gb of text data (293K pages, 2.8M Q/A pairs), with real users' questions sampled from a web search engine log. Specifically, we propose simple but effective methods for Q/A extraction and investigate task-specific retrieval models for answering questions. Our best model finds answers for 36% of the test questions in the top 20 results. Our overall conclusion is that FAQ pages on the web provide an excellent resource for addressing real users' information needs in a highly focused manner.

#index 838398
#* Finding similar questions in large question and answer archives
#@ Jiwoon Jeon;W. Bruce Croft;Joon Ho Lee
#t 2005
#c 1
#% 169729
#% 218978
#% 262096
#% 279755
#% 280851
#% 309126
#% 310567
#% 340901
#% 340954
#% 342961
#% 397145
#% 427921
#% 474643
#% 676170
#% 740915
#% 803037
#% 818299
#% 854791
#% 1650298
#! There has recently been a significant increase in the number of community-based question and answer services on the Web where people answer other peoples' questions. These services rapidly build up large archives of questions and answers, and these archives are a valuable linguistic resource. One of the major tasks in a question and answer service is to find questions in the archive that a semantically similar to a user's question. This enables high quality answers from the archive to be retrieved and removes the time lag associated with a community-based system. In this paper, we discuss methods for question retrieval that are based on using the similarity between answers in the archive to estimate probabilities for a translation-based retrieval model. We show that with this model it is possible to find semantically similar questions with relatively little word overlap.

#index 838399
#* Connecting topics in document collections with stepping stones and pathways
#@ Fernando Das-Neves;Edward A. Fox;Xiaoyan Yu
#t 2005
#c 1
#% 1260
#% 118771
#% 235289
#% 249143
#% 251514
#% 262045
#% 273028
#% 286670
#% 323131
#% 345764
#% 769902
#% 952495
#! In this paper, we present Stepping Stones and Pathways (SSP), an alternative model of building and presenting answers for the cases when queries on document collections cannot be answered just by a ranked list. Stepping Stones can handle questions like: "What is the relation of topics X and Y?" SSP addresses when the contents of a small set of related documents is needed as an answer rather than a single document, or when "query splitting" is required to satisfactorily explore a document space. Query results are networks of document groups representing topics, each group relating to and connecting (by documents) to other groups in the network. Thus, a network answers the user's information need. We devise new and more effective representations and techniques to visualize such answers, and to involve users as part of the answer-finding process. In order to verify the validity of our approach, and since the questions we aim to answer involve multiple topics, we performed a study involving a custom built broad collection of operating systems research papers, and evaluated the results with interested computer science students, using multiple measures.

#index 838400
#* Securing XML data in third-party distribution systems
#@ Barbara Carminati;Elena Ferrari;Elisa Bertino
#t 2005
#c 1
#% 342345
#% 379248
#% 397367
#% 513367
#% 659992
#% 772846
#% 1015329
#! Web-based third-party architectures for data publishing are today receiving growing attention, due to their scalability and the ability to efficiently manage large numbers of users and great amounts of data. A third-party architecture relies on a distinction between the Owner and the Publisher of information. The Owner is the producer of information, whereas Publisher provides data management services and query processing functions for (a portion of) the Owner's information. In such architecture, there are important security concerns especially if we do not want to make any assumption on the trustworthy of the Publishers. Although approaches have been proposed [4, 5] providing partial solutions to this problem, no comprehensive framework has been so far developed able to support all the most important security properties in the presence of an untrusted Publisher. In this paper, we develop an XML-based solution to such problem, which makes use of non-conventional digital signature techniques and queries over encrypted data.

#index 838401
#* The case for access control on XML relationships
#@ Béatrice Finance;Saïda Medjdoub;Philippe Pucheral
#t 2005
#c 1
#% 157156
#% 314755
#% 344639
#% 345973
#% 378393
#% 379248
#% 424307
#% 495711
#% 566390
#% 646047
#% 725290
#% 737762
#% 755176
#% 765450
#% 848532
#% 993972
#% 1016136
#! With the emergence of XML as the de facto standard to exchange and disseminate information, the problem of regulating access to XML documents has attracted a considerable attention in recent years. Existing models attach authorizations to nodes of an XML document but disregard relationships between them. However, ancestor and sibling relationships may reveal information as sensitive as the one carried out by the nodes themselves (e.g., classification). This paper advocates the integration of relationships as first class citizen in the access control models for XML and makes the following contributions. First, it characterizes important relationship authorizations and identifies the mechanisms required to translate them accurately in an authorized view of a source document. Second, it introduces a rule-based formulation for expressing these classes of relationship authorizations and defines an associated conflict resolution strategy. Rather than being yet-another XML access control model, the proposed approach allows a seamless integration of relationship authorizations in existing XML access control model.

#index 838402
#* A function-based access control model for XML databases
#@ Naizhen Qi;Michiharu Kudo;Jussi Myllymaki;Hamid Pirahesh
#t 2005
#c 1
#% 204453
#% 287026
#% 309716
#% 314755
#% 333855
#% 344639
#% 379248
#% 397360
#% 424307
#% 462235
#% 465012
#% 480296
#% 480489
#% 564264
#% 602661
#% 646047
#% 659987
#% 659995
#% 725290
#% 993971
#% 993972
#! XML documents are frequently used in applications such as business transactions and medical records involving sensitive information. Typically, parts of documents should be visible to users depending on their roles. For instance, an insurance agent may see the billing information part of a medical document but not the details of the patient's medical history. Access control on the basis of data location or value in an XML document is therefore essential. In practice, the number of access control rules is on the order of millions, which is a product of the number of document types (in 1000's) and the number of user roles (in 100's). Therefore, the solution requires high scalability and performance. Current approaches to access control over XML documents have suffered from scalability problems because they tend to work on individual documents. In this paper, we propose a novel approach to XML access control through rule functions that are managed separately from the documents. A rule function is an executable code fragment that encapsulates the access rules (paths and predicates), and is shared by all documents of the same document type. At runtime, the rule functions corresponding to the access request are executed to determine the accessibility of document fragments. Using synthetic and real data, we show the scalability of the scheme by comparing the accessibility evaluation cost of two rule function models. We show that the rule functions generated on user basis is more efficient for XML databases.

#index 838403
#* Exact match search in sequence data using suffix trees
#@ Mihail Halachev;Nematollaah Shiri;Anand Thamildurai
#t 2005
#c 1
#% 10117
#% 143306
#% 194111
#% 235941
#% 288578
#% 289010
#% 320454
#% 480484
#% 736970
#% 1015330
#% 1016132
#! We study suitable indexing techniques to support efficient exact match search in large biological sequence databases. We propose a suffix tree (ST) representation, called STA-DF, as an alternative to the array representation of ST (STA) proposed in [7] and utilized in [18]. To study the performance of STA and STA-DF, we develop a memory efficient ST-based Exact Match (STEM) search algorithm. We implemented STEM and both representations of ST and conducted extensive experiments. Our results indicate that the STA and STA-DF representations are very similar in construction time, storage utilization, and search time using STEM. In terms of the access patterns by STEM, our results show that compared to STA, the STA-DF representation exhibits better spatial and sequential locality of reference. This suggests that STA-DF would require less number of disk I/Os, and hence is more amenable to efficient and scalable disk-based computation.

#index 838404
#* Rotation invariant indexing of shapes and line drawings
#@ Michail Vlachos;Zografoula Vagena;Philip S. Yu;Vassilis Athitsos
#t 2005
#c 1
#% 437405
#% 443242
#% 443698
#% 443975
#% 444027
#% 571043
#% 572264
#% 592027
#% 765412
#! We present data representations, distance measures and organizational structures for fast and efficient retrieval of similar shapes in image databases. Using the Hough Transform we extract shape signatures that correspond to important features of an image. The new shape descriptor is robust against line discontinuities and takes into consideration not only the shape boundaries, but also the content inside the object perimeter. The object signatures are eventually projected into a space that renders them invariant to translation, scaling and rotation. In order to provide support for real-time query-by-content, we also introduce an index structure that hierarchically organizes compressed versions of the extracted object signatures. In this manner we can achieve a significant performance boost for multimedia retrieval. Our experiments suggest that by exploiting the proposed framework, similarity search in a database of 100,000 images would require under 1 sec, using an off-the-shelf personal computer.

#index 838405
#* DIST: a distributed spatio-temporal index structure for sensor networks
#@ Anand Meka;Ambuj Singh
#t 2005
#c 1
#% 281556
#% 300174
#% 309466
#% 340175
#% 340176
#% 427199
#% 503869
#% 622726
#% 654482
#% 718326
#% 731091
#% 745458
#% 772022
#% 783533
#% 783736
#! We consider the general problem of tracking moving objects in sensor networks. The specific application we consider is that of tracking a chemical plume moving over a large infrastructure network. We present a distributed index structure DIST that stores and updates distributed summaries as the plume moves. We present algorithms for range queries on the history of the plume. DIST localizes information with respect to time and space using a hierarchy that scales with the plume size. The highlight of our work is an analytical model to predict the cost of query algorithms based on the query location, query size, and plume's spatio-temporal distribution. Using this model, our adaptive scheme chooses the optimal scheme. Experimental results show that DIST outperforms alternative techniques in query, update, and storage costs, and scales well with the number of plumes.

#index 838406
#* Focused crawling for both topical relevance and quality of medical information
#@ Thanh Tin Tang;David Hawking;Nick Craswell;Kathy Griffiths
#t 2005
#c 1
#% 54435
#% 92696
#% 136350
#% 268073
#% 268079
#% 268087
#% 268106
#% 281251
#% 290482
#% 309145
#% 342400
#% 480309
#% 862154
#! Subject-specific search facilities on health sites are usually built using manual inclusion and exclusion rules. These can be expensive to maintain and often provide incomplete coverage of Web resources. On the other hand, health information obtained through whole-of-Web search may not be scientifically based and can be potentially harmful.To address problems of cost, coverage and quality, we built a focused crawler for the mental health topic of depression, which was able to selectively fetch higher quality relevant information. We found that the relevance of unfetched pages can be predicted based on link anchor context, but the quality cannot. We therefore estimated quality of the entire linking page, using a learned IR-style query of weighted single words and word pairs, and used this to predict the quality of its links. The overall crawler priority was determined by the product of link relevance and source quality.We evaluated our crawler against baseline crawls using both relevance judgments and objective site quality scores obtained using an evidence-based rating scale. Both a relevance focused crawler and the quality focused crawler retrieved twice as many relevant pages as a breadth-first control. The quality focused crawler was quite effective in reducing the amount of low quality material fetched while crawling more high quality content, relative to the relevance focused crawler.Analysis suggests that quality of content might be improved by post-filtering a very big breadth-first crawl, at the cost of substantially increased network traffic.

#index 838407
#* Hybrid index structures for location-based web search
#@ Yinghua Zhou;Xing Xie;Chuang Wang;Yuchang Gong;Wei-Ying Ma
#t 2005
#c 1
#% 86950
#% 330677
#% 443327
#% 462059
#% 480467
#% 665541
#% 730051
#% 766441
#% 807438
#% 818256
#% 1488810
#% 1715671
#! There is more and more commercial and research interest in location-based web search, i.e. finding web content whose topic is related to a particular place or region. In this type of search, location information should be indexed as well as text information. However, the index of conventional text search engine is set-oriented, while location information is two-dimensional and in Euclidean space. This brings new research problems on how to efficiently represent the location attributes of web pages and how to combine two types of indexes. In this paper, we propose to use a hybrid index structure, which integrates inverted files and R*-trees, to handle both textual and location aware queries. Three different combining schemes are studied: (1) inverted file and R*-tree double index, (2) first inverted file then R*-tree, (3) first R*-tree then inverted file. To validate the performance of proposed index structures, we design and implement a complete location-based web search engine which mainly consists of four parts: (1) an extractor which detects geographical scopes of web pages and represents geographical scopes as multiple MBRs based on geographical coordinates; (2) an indexer which builds hybrid index structures to integrate text and location information; (3) a ranker which ranks results by geographical relevance as well as non-geographical relevance; (4) an interface which is friendly for users to input location-based search queries and to obtain geographical and textual relevant results. Experiments on large real-world web dataset show that both the second and the third structures are superior in query time and the second is slightly better than the third. Additionally, indexes based on R*-trees are proven to be more efficient than indexes based on grid structures.

#index 838408
#* Person resolution in person search results: WebHawk
#@ Xiaojun Wan;Jianfeng Gao;Mu Li;Binggong Ding
#t 2005
#c 1
#% 19917
#% 118771
#% 262045
#% 269217
#% 310516
#% 464612
#% 742425
#% 747890
#% 760866
#% 766433
#% 783704
#% 805885
#% 855094
#% 938728
#% 1260448
#! Finding information about people on the Web using a search engine is difficult because there is a many-to-many mapping between person names and specific persons (i.e. referents). This paper describes a person resolution system, called WebHawk. Given a list of pages obtained by submitting a person query to a search engine, WebHawk facilitates person search in three steps: First of all, a filter removes those pages that contain no information about any person. Secondly, a cluster groups the remaining pages into different clusters, each for one specific person. To make the resulting clusters more meaningful, an extractor is used to induce query-oriented personal information from each page. Finally, a namer generates an informative description for each cluster so that users can find any specific person easily. The architecture of WebHawk is presented, and the four components are discussed in detail, with a separate evaluation of each component presented where appropriate. A user study shows that WebHawk complements most existing search engines and successfully improves users' experience of person search on the Web.

#index 838409
#* Adaptive load shedding for windowed stream joins
#@ Buğgra Gedik;Kun-Lung Wu;Philip S. Yu;Ling Liu
#t 2005
#c 1
#% 378388
#% 479654
#% 577220
#% 578391
#% 654444
#% 654454
#% 783479
#% 788216
#% 801694
#% 805466
#% 853011
#% 993949
#% 1015280
#% 1015296
#% 1016156
#! We present an adaptive load shedding approach for windowed stream joins. In contrast to the conventional approach of dropping tuples from the input streams, we explore the concept of selective processing for load shedding. We allow stream tuples to be stored in the windows and shed excessive CPU load by performing the join operations, not on the entire set of tuples within the windows, but on a dynamically changing subset of tuples that are learned to be highly beneficial. We support such dynamic selective processing through three forms of runtime adaptations: adaptation to input stream rates, adaptation to time correlation between the streams and adaptation to join directions. Indexes are used to further speed up the execution of stream joins. Experiments are conducted to evaluate our adaptive load shedding in terms of output rate. The results show that our selective processing approach to load shedding is very effective and significantly outperforms the approach that drops tuples from the input streams.

#index 838410
#* Integrating DCT and DWT for approximating cube streams
#@ Ming-Jyh Hsieh;Ming-Syan Chen;Philip S. Yu
#t 2005
#c 1
#% 115996
#% 168862
#% 211575
#% 227866
#% 259995
#% 273902
#% 273903
#% 397426
#% 460862
#% 631923
#% 1818266
#! For time-relevant multi-dimensional data sets (MDS), users usually pose a huge amount of data due to the large dimensionality, and approximating query processing has emerged as a viable solution. Specifically, the cube streams handle MDSs in a continuous manner. Traditional cube approximation focuses on generating single snapshots rather than continuous ones. To address this issue, the application of generating snapshots for cube streams, called SCS, is investigated in this paper. Such an application collects data events for cube streams on-line and generates snapshots with limited resources in order to keep the approximated information in synopsis memory for further analysis. As compared to OLAP applications, the SCS ones are subject to much more resource constraints for both processing time and memory and cannot be dealt with by existing methods due to the limited resources. In this paper, the DAWA algorithm, standing for a hybrid algorithm of Dct for Data and discrete WAvelet transform, is proposed to approximate the cube streams. The DAWA algorithm combines the advantage of high compression rate from DWT and that of low memory cost from DCT. Consequently, DAWA costs much smaller working buffer and outperforms both DWT-based and DCT-based methods in execution efficiency. Also, it is shown that DAWA provides answers of good quality for SCS applications with a small working buffer and short execution time. The optimality of algorithm DAWA is theoretically proved and also empirically demonstrated by our experiments.

#index 838411
#* Exploiting redundancy in sensor networks for energy efficient processing of spatiotemporal region queries
#@ Alexandru Coman;Mario A. Nascimento;Jörg Sander
#t 2005
#c 1
#% 309466
#% 360833
#% 449869
#% 654482
#% 654483
#% 712040
#% 745442
#% 751027
#% 765445
#% 799149
#% 800503
#% 800505
#% 1016178
#% 1831234
#! Sensor networks are made of autonomous devices that are able to collect, store, process and share data with other devices. Spatiotemporal region queries can be used for retrieving information of interest from such networks. Such queries require the answers only from the subset of the network nodes that fall into the query region. If the network is redundant in the sense that the measurements of some nodes can be substituted by those of other nodes with a certain degree of confidence, then a much smaller subset of nodes may be sufficient to answer the query at a lower energy cost. We investigate how to take advantage of such data redundancy and propose two techniques to process spatiotemporal region queries under these conditions. Our techniques reduce up to twenty times the energy cost of query processing compared to the typical network flooding, thus prolonging the lifetime of the sensor network.

#index 838412
#* Collective multi-label classification
#@ Nadia Ghamrawi;Andrew McCallum
#t 2005
#c 1
#% 162505
#% 169777
#% 211044
#% 311034
#% 318412
#% 397142
#% 458379
#% 464434
#% 770783
#% 1650403
#% 1673026
#! Common approaches to multi-label classification learn independent classifiers for each category, and employ ranking or thresholding schemes for classification. Because they do not exploit dependencies between labels, such techniques are only well-suited to problems in which categories are independent. However, in many domains labels are highly interdependent. This paper explores multi-label conditional random field (CRF)classification models that directly parameterize label co-occurrences in multi-label classification. Experiments show that the models outperform their single-label counterparts on standard text corpora. Even when multi-labels are sparse, the models improve subset classification error by as much as 40%.

#index 838413
#* Clustering high-dimensional data using an efficient and effective data space reduction
#@ Ratko Orlandic;Ying Lai;Wai Gen Yee
#t 2005
#c 1
#% 201893
#% 210173
#% 248792
#% 397593
#% 413610
#% 479799
#% 479962
#% 481281
#% 566128
#% 612169
#% 636971
#% 664831
#! This paper introduces a new algorithm for clustering data in high-dimensional feature spaces, called GARDENHD. The algorithm is organized around the notion of data space reduction, i.e. the process of detecting dense areas (dense cells) in the space. It performs effective and efficient elimination of empty areas that characterize typical high-dimensional spaces and an efficient adjacency-connected agglomeration of dense cells into larger clusters. It produces a compact representation that can effectively capture the essence of data. GARDENHD is a hybrid of cell-based and density-based clustering. However, unlike typical clustering methods in its class, it applies a recursive partition of sparse regions in the space using a new space-partitioning strategy. The properties of this partitioning strategy greatly facilitate data space reduction. The experiments on synthetic and real data sets reveal that GARDENHD and its data space reduction are effective, efficient, and scalable.

#index 838414
#* Versatile structural disambiguation for semantic-aware applications
#@ Federica Mandreoli;Riccardo Martoglia;Enrico Ronchetti
#t 2005
#c 1
#% 198058
#% 480645
#% 551850
#% 660001
#% 728415
#% 730042
#% 741080
#% 754104
#% 800497
#% 1279327
#! In this paper, we propose a versatile disambiguation approach which can be used to make explicit the meaning of structure based information such as XML schemas, XML document structures, web directories, and ontologies. It can be of support to the semantic-awareness of a wide range of applications, from schema matching and query rewriting to peer data management systems, from XML data clustering to ontology-based automatic annotation of web pages and query expansion. The effectiveness of the achieved results has been experimentally proved and is founded both on a flexible exploitation of the structure context, whose extraction can be tailored on the specific application needs, and of the information provided by commonly available thesauri such as WordNet.

#index 838415
#* D-CAPE: distributed and self-tuned continuous query processing
#@ Timothy M. Sutherland;Bin Liu;Mariana Jbantova;Elke A. Rundensteiner
#t 2005
#c 1
#% 115661
#% 765437
#% 824781
#% 839187
#% 963595
#% 1016269

#index 838416
#* Mining conserved XML query paths for dynamic-conscious caching
#@ Qiankun Zhao;Sourav S. Bhowmick;Le Gruenwald
#t 2005
#c 1
#% 397359
#% 397375
#% 397409
#% 765423
#% 769965
#! Existing XML query pattern-based caching strategies focus on extracting the set of frequently issued query pattern trees based on the number of occurrences of the query pattern trees in the history. Each occurrence of the same query pattern tree is considered equally important for the caching strategy. However, the same query pattern tree may occur at different timepoints in the history of XML queries. This temporal feature can be used to improve the caching strategy. In this paper, we propose a novel type of query pattern called conserved query paths for efficient caching by integrating the support and temporal features together. Conserved query paths are paths in query pattern trees that never change or do not change significantly most of the time (if not always) in terms of their support values during a specific time period. We proposed an algorithm to extract those conserved query paths. By ranking those conserved query paths, a dynamic-conscious caching (DCC) strategy is proposed for efficient XML query processing. Experiments show that the DCC caching strategy outperforms the existing XML query pattern tree-based caching strategies.

#index 838417
#* Optimizing continuous multijoin queries over distributed streams
#@ Yongluan Zhou;Ying Yan;Beng Chin Ooi;Kian-Lee Tan;Aoying Zhou
#t 2005
#c 1

#index 838418
#* Processing XPath queries with XML summaries
#@ Takeharu Eda;Makoto Onizuka;Masashi Yamamuro
#t 2005
#c 1
#% 479465
#% 659999
#% 783787
#! Range labeling and structural joins are well-studied techniques for efficiently processing XPath queries. However, when XPath queries become long, many times of structural joins are required. To solve this problem, we developed a method to reduce the number of joins and nodes read from the disk using strong DataGuides. Our method can process single paths without any joins and twig patterns with joins amongst branching nodes and leaves in queries. Experimental results verified that our approach outperforms the best optimization technique for structural joins by factors of up to several hundreds of times.

#index 838419
#* On reducing redundancy and improving efficiency of XML labeling schemes
#@ Changqing Li;Tok Wang Ling;Jiaheng Lu;Tian Yu
#t 2005
#c 1
#% 333981
#% 397366
#! The basic relationships to be determined in XML query processing are ancestor-descendant (A-D), parent-child (P-C), sibling and ordering relationships. The containment labeling scheme can determine the A-D, P-C and ordering relationships fast, but it is very expensive in determining the sibling relationship. The prefix labeling scheme can determine all the four basic relationships fast if the XML tree is shallow. However, if the XML tree is deep, the prefix scheme is inefficient since the prefix is long. Furthermore, the prefix label is repeated by all the siblings (only the self labels of these siblings are different). Thus in this paper, we propose the P-Containment and P-Prefix schemes which can determine all the four basic relationships faster no matter what the XML structure is; meanwhile P-Prefix can reduce the redundancies in the prefix labeling scheme.

#index 838420
#* Applying cosine series to join size estimation
#@ Cheng Luo;Zhewei Jiang;Wen-Chi Hou
#t 2005
#c 1
#% 273682
#% 458836
#% 650962
#% 654453
#! This paper provides a general overview of two innovative applications of Cosine series in XML joins and data stream joins.

#index 838421
#* Database selection in intranet mediators for natural language queries
#@ Fang Liu;Shuang Liu;Clement Yu;Weiyi Meng;Ophir Frieder;David Grossman
#t 2005
#c 1
#% 428249
#% 433578
#% 481923
#% 659990
#% 993987

#index 838422
#* Structure-based query-specific document summarization
#@ Ramakrishna Varadarajan;Vagelis Hristidis
#t 2005
#c 1
#% 479803
#! Summarization of text documents is increasingly important with the amount of data available on the Internet. The large majority of current approaches view documents as linear sequences of words and create query-independent summaries. However, ignoring the structure of the document degrades the quality of summaries. Furthermore, the popularity of web search engines requires query-specific summaries. We present a method to create query-specific summaries by adding structure to documents by extracting associations between their fragments.

#index 838423
#* Typed functional query languages with equational specifications
#@ Ken Q. Pu;Alberto O. Mendelzon
#t 2005
#c 1
#% 58346
#% 205243
#% 236278
#% 252464
#% 286831
#% 411557
#! We present a framework for functionally modeling query languages and data models. Data and queries are uniformly represented by first-order functions, and query-language constructs by polymorphic higher-order functions. The functions are typed by a database-oriented type system that supports polymorphism and nesting of types, thus one can perform static type-checking and type-inferencing of query-expressions. The query language can be freely extended by introducing new querying constructs as polymorphic higher-order functions.While type information gives the input-output description of the functions, the semantic information is captured by equational specifications. Knowledge about the functions is represented as equalities of functional expressions in the form of equations. By equational axiomatization of the query language, database problems of query equivalence and answering-query with views can be posed as equational word-problems and equational matching.

#index 838424
#* DSAC: integrity for outsourced databases with signature aggregation and chaining
#@ Maithili Narasimha;Gene Tsudik
#t 2005
#c 1
#% 566391
#% 745532
#% 1394513
#! Database outsourcing is an important trend which involves data owners farming out their data management needs to an external service provider. One important requirement is to maintain the integrity and authenticity of outsourced data. Whenever an outsourced database is queried, the corresponding query reply must be demonstrably authentic. Furthermore, a reply must include a proof of completeness to convince the querier that no data matching the query predicate(s) has been omitted. In this paper, we suggest new techniques in support of efficient authenticity and completeness guarantees of such query replies.

#index 838425
#* Answering aggregation queries on hierarchical web sites using adaptive sampling
#@ Foto N. Afrati;Paraskevas V. Lekeas;Chen Li
#t 2005
#c 1
#! We study how to answer aggregation queries over hierarchical Web sites using adaptive sampling.

#index 838426
#* OSQR: overlapping clustering of query results
#@ Bhuvan Bamba;Prasan Roy;Mukesh Mohania
#t 2005
#c 1
#% 296738
#% 765464
#% 838494

#index 838427
#* INFER: a relational query language without the complexity of SQL
#@ Terrence Mason;Ramon Lawrence
#t 2005
#c 1
#% 330774
#% 1016176
#! The INFER query language allows users to express queries without referencing relations or specifying joins. Since the INFER syntax is similar to but less restrictive than SQL, users can easily write highly expressive queries that are automatically completed by INFER's inference engine. INFER's SQL-based syntax is familiar to current database users, and its improved ranking and query explanation system makes it easier to use.

#index 838428
#* Efficient data dissemination using locale covers
#@ Sandeep Gupta;Jinfeng Ni;Chinya V. Ravishankar
#t 2005
#c 1
#% 97037
#% 201897
#% 274199
#% 443127
#% 555048
#! Location-dependent data are central to many emerging applications, ranging from traffic information services to sensor networks. The standard pull- and push-based data dissemination models become unworkable since the data volumes and number of clients are high.We address this problem using locale covers, a subset of the original set of locations of interest, chosen to include at least one location in a suitably defined neighborhood of any client. Since location-dependent values are highly correlated with location, a query can be answered using a location close to the query point.We show that location-dependent queries may be answered satisfactorily using locale covers, with small loss of accuracy. Our approach is independent of locations and speeds of clients, and is applicable to mobile clients.

#index 838429
#* Incremental evaluation of a monotone XPath fragment
#@ Hidetaka Matsumura;Keishi Tajima
#t 2005
#c 1
#% 479806
#% 665410
#% 805907
#! This paper shows a scheme for incremental evaluation of XPath queries. Here, we focus on a monotone fragment of XPath, i.e., when a data is deleted from (or inserted to) the database, only deletion (insertion, resp.) may occur to query answers. For efficiently processing deletions, we store information on partial matchings, i.e., which elements were participating in matchings for which query answers, and also store counters showing how many matchings each query answer had. We use the information on the partial matchings also for skipping a part of computation upon data insertion. We investigate properties of the XPath fragment in order to keep the amount of information we store as small as possible.

#index 838430
#* Discovering strong skyline points in high dimensional spaces
#@ Zhenjie Zhang;Xinyu Guo;Hua Lu;Anthony K. H. Tung;Nan Wang
#t 2005
#c 1
#% 288976
#% 289148
#% 310507
#% 465167
#% 480671
#% 481290
#% 654480
#% 806212
#% 993954
#! Current interests in skyline computation arise due to their relation to preference queries. Since it is guaraneed that a skyline point will not lose out in all dimensions when compared to any other point in the data set, this means that for each skyline point, there exists a set of weight assignments to the dimensions such that the point will become the top user preference.We believe that the usefulness of skyline points is not limited to such application and can be extended to data analysis and knowledge discovery as well. However, since the skyline of high dimensional datasets (which are common in data analysis applications) can contain too many points, various means must be developed to filter off the less interesting skyline points in high dimensions. In this paper, we will propose algorithms to find a set of interesting skyline points called strong skyline points. Extensive experiments show that our proposal is both effective and efficient.

#index 838431
#* Mining undiscovered public knowledge from complementary and non-interactive biomedical literature through semantic pruning
#@ Xiaohua Hu;Illhoi Yoo;Min Song;Yanqing Zhang;Il-Yeol Song
#t 2005
#c 1
#! Two complementary and non-interactive literature sets of articles, when they are considered together, can reveal useful information of scientific interest not apparent in either of the two document sets. Swanson called the existence of such knowledge, undiscovered public knowledge (UDPK). This paper proposes a semantic-based mining model for UDPK. Our method replaces manual ad-hoc pruning with using semantic knowledge from the biomedical ontologies. Using the semantic types and semantic relationships of the biomedical concepts, our prototype system can identify the relevant concepts collected from Medline and generate the novel hypothesis between these concepts. The system successfully replicates Swanson's two famous discoveries: Raynaud disease/fish oils and migraine/magnesium. Compared with previous approaches, our methods generate much fewer but more relevant novel hypotheses, and require much less human intervention in the discovery procedure.

#index 838432
#* Access control for XML: a dynamic query rewriting approach
#@ Sriram Mohan;Arijit Sengupta;Yuqing Wu
#t 2005
#c 1
#% 765450
#! Being able to express and enforce role-based access control on XML data is a critical component of XML data management. However, given the semi-structured nature of XML, this is non-trivial, as access control can be applied on the values of nodes as well as on the structural relationship between nodes. In this context, we adopt and extend a graph editing language for specifying role-based access constraints in the form of security views. A Security Annotated Schema (SAS) is proposed as the internal representation for the security views and can be automatically constructed from the original schema and the security view specification. To enforce the access constraints on user queries, we propose Secure Query Rewrite (SQR) -- a set of rules that can be used to rewrite a user XPath query on the security view into an equivalent XQuery expression against the original data, with the guarantee that the users only see information in the view but not any data that was blocked. Experimental evaluation demonstrates the efficiency and the expressiveness of our approach.

#index 838433
#* Relational computation for mining association rules from XML data
#@ Hong-Cheu Liu;John Zeleznikow
#t 2005
#c 1
#% 737329
#% 807679
#! We develop a fixpoint operator for computing large item sets and demonstrate three query paradigm solutions for association rule mining that use the idea of least fixpoint computation and indicates some optimisation issues. The results of our research provide theoretical foundation for relational computation of association rules and its application on XML mining.

#index 838434
#* Mining all maximal frequent word sequences in a set of sentences
#@ Helena Ahonen-Myka
#t 2005
#c 1
#% 740904
#! We present an efficient algorithm for finding all maximal frequent word sequences in a set of sentences. A word sequence s is considered frequent, if all its words occur in at least σ sentences and the words occur in each of these sentences in the same order as in s, given a frequency threshold σ. Hence, the words of a sequence s do not have to occur consecutively in the sentences.

#index 838435
#* Joint deduplication of multiple record types in relational data
#@ Aron Culotta;Andrew McCallum
#t 2005
#c 1
#% 344568
#% 464434
#! Record deduplication is the task of merging database records that refer to the same underlying entity. In relational data-bases, accurate deduplication for records of one type is often dependent on the decisions made for records of other types. Whereas nearly all previous approaches have merged records of different types independently, this work models these inter-dependencies explicitly to collectively deduplicate records of multiple types. We construct a conditional random field model of deduplication that captures these relational dependencies, and then employ a novel relational partitioning algorithm to jointly deduplicate records. For two citation matching datasets, we show that collectively deduplicating paper and venue records results in up to a 30% error reduction in venue deduplication, and up to a 20% error reduction in paper deduplication.

#index 838436
#* Localized routing trees for query processing in sensor networks
#@ Jie Lian;Lei Chen;Kshirasagar Naik;M. Tamer Özsu;G. Agnew
#t 2005
#c 1
#! In this paper, we propose a novel energy-efficient approach, a localized routing tree (LRT) coupled with a route redirection (RR) strategy, to support various types of queries. LRTs take care of the sensors near the sink and reduce the energy consumption of these sensors, and RR reduces the energy cost of data receptions. Compared to the existing approaches, simulation studies show that LRT together with RR has significant improvement on the query capacity.

#index 838437
#* A latent semantic classification model
#@ Ming-Wen Wang;Jian-Yun Nie;Xue-Qiang Zeng
#t 2005
#c 1
#% 344447
#! Latent Semantic Indexing (LSI) has been successfully applied to information retrieval and text classification. However, when LSI is used in classification, some important features for small classes may be ignored because of their small feature values. To solve this problem, we propose the latent semantic classification (LSC) model which extends the LSI model in the following way: the classification information of the training documents is introduced into the latent semantic structure via a second set of latent variables, so that both indexing and classification information can be taken into account during the classification process. Our experiments on Reuters show that our new model performs better than the existing classification methods such as kNN and SVM.

#index 838438
#* Supporting ranked search in parallel search cluster networks
#@ Fang Xiong;Qiong Luo;Dyce Jing Zhao
#t 2005
#c 1
#% 290703
#% 610851
#% 762651
#! We investigate how to support ranked keyword search in a Parallel Search Cluster Network, which is a newly proposed peer-to-peer network overlay. In particular, we study how to efficiently acquire and distribute the global information required by ranked keyword search by taking advantage of the architectural features of PSCNs.

#index 838439
#* Web opinion poll: extracting people's view by impression mining from the web
#@ Tadahiko Kumamoto;Katsumi Tanaka
#t 2005
#c 1
#% 1713948

#index 838440
#* Statistical relationship determination in automatic thesaurus construction
#@ Libo Chen;Peter Fankhauser;Ulrich Thiel;Thomas Kamps
#t 2005
#c 1
#% 78171
#% 375017
#% 836019
#! Statistical relationship determination among terms is one of the key issues in automatic thesaurus construction. We systematically analyze existing relevant approaches based on their underlying probabilistic assumptions, and propose a combined approach that overcomes their limitations.

#index 838441
#* Model-guided information discovery for intelligence analysis
#@ Rafael Alonso;Hua Li
#t 2005
#c 1
#% 298183
#% 729993
#% 773452
#! Intelligence analysis can be aided and guided by models of the analysts' interests and priorities. This paper describes our approach to analyst modeling as part of the Ant CAFÉ project, in which analyst models are used to guide the searching behavior of a swarm of intelligent agents. Structural elements of our analyst model include concepts and relations, both of which help to capture the analyst's current interest and concerns. In addition, the concepts and relationships have associated scalar parameters to provide a quantitative measure of the user's level of interest. We have developed algorithms for dynamically adapting the weights and evolving the elements of the model itself. To evaluate these algorithms we have built an Analyst Modeling Environment workbench. We have tested our approach on this workbench using traces generated by human analysts, and have demonstrated improvements over current state of the art search engines.

#index 838442
#* Biasing web search results for topic familiarity
#@ Giridhar Kumaran;Rosie Jones;Omid Madani
#t 2005
#c 1
#% 400847
#! Depending on a web searcher's familiarity with a query's target topic, it may be more appropriate to show her introductory or advanced documents. The TREC HARD [1] track defined topic familiarity as meta-data associated with a user's query. We instead define a user-independent and query-independent model of topic-familiarity required to read a document, so it can be matched to a given user in response to a query. An introductory web page is defined as A web page that doesn't presuppose any background knowledge of the topic it is on, and to an extent introduces or defines the key terms in the topic. while an advanced web page is defined as A web page that assumes sufficient background knowledge of the topic it is on, and familiarity with the key technical/ important terms in the topic, and potentially builds on them. We develop a method for biasing the initial mix of documents returned by a search engine to increase the number of documents of desired familiarity level up to position 5, and up to position 10. Our method involves building a supervised text classifier, incorporating features based on reading level, the distribution of stop-words in the text, and non-text features such as average line-length. Using this familiarity classifier, we achieve statistically significant improvements at reranking the result set to show introductory documents higher up the ranked list. Our classifier can be seamlessly integrated into current search engine technology without involving any major modifications to existing architectures.

#index 838443
#* Accurate language model estimation with document expansion
#@ Tao Tao;Xuanhui Wang;Qiaozhu Mei;ChengXiang Zhai
#t 2005
#c 1
#% 340948
#% 719598
#% 766430
#% 766431

#index 838444
#* Mining community structure of named entities from free text
#@ Xin Li;Bing Liu
#t 2005
#c 1
#% 268079
#% 282905
#% 811281
#% 938705
#! Although community discovery has been studied extensively in the Web environment, limited research has been done in the case of free text. Co-occurrence of words and entities in sentences and documents usually implies connections among them. In this paper, we investigate the co-occurrences of named entities in text, and mine communities among these entities. We show that identifying communities from free text can be transformed into a graph clustering problem. A hierarchical clustering algorithm is then proposed. Our experiment shows that the algorithm is effective to discover named entity communities from text documents.

#index 838445
#* A practical system of keyphrase extraction for web pages
#@ Mo Chen;Jian-Tao Sun;Hua-Jun Zeng;Kwok-Yan Lam
#t 2005
#c 1
#% 281480
#% 766412
#% 1279276
#! Keyphrases can be used to facilitate Web users grasping the main topic(s) of a Web page. We present a practical system of automatic keyphrase extraction for Web pages. In this system, a regression model was first trained based on a set of human-labeled documents. Then it was used to extract keyphrases from new pages automatically. This paper makes three contributions. First, the structure information in a Web page was investigated for keyphrase extraction task. Second, the query log data associated with a Web page collected by a search engine server were used to help keyphrase extraction. Third, a method was put forward in this paper in order to evaluate the similarity of phrases.

#index 838446
#* Incremental stock time series data delivery and visualization
#@ Tak-chung Fu;Fu-lai Chung;Pui-ying Tang;Robert Luk;Chak-man Ng
#t 2005
#c 1
#% 861488
#! SB-Tree is a binary tree data structure proposed to represent time series according to the importance of data points. Its use in stock data management is distinguished by preserving the critical data points' attribute values, retrieving time series data according to the importance of data points and facilitating multi-resolution time series retrieval. As new stock data are available continuously, an effective updating mechanism for SB-Tree is needed. In this paper, a study of different updating approaches is reported. Three families of updating methods are proposed. They are periodic rebuild, batch update and point-by-point update. Their efficiency, effectiveness and characteristics are compared and reported.

#index 838447
#* Generating better concept hierarchies using automatic document classification
#@ Razvan Stefan Bot;Yi-fang Brook Wu;Xin Chen;Quanzhi Li
#t 2005
#c 1
#% 196896
#% 280849
#! This paper presents a hybrid concept hierarchy development technique for web returned documents retrieved by a meta-search engine. The aim of the technique is to separate the initial retrieved documents into topical oriented categories, prior to the actual concept hierarchy generation. The topical categories correspond to different semantic aspects of the query. This is done using a 1-of-n automatic document classification, on the initial set of returned documents. Then, an individual topical concept hierarchy is automatically generated inside each of the resulted categories. Both steps are executed on the fly at retrieval time. Due to the efficiency constraints imposed by the web retrieval context, the algorithm only uses document snippets (rather than full web pages) for both document classification and concept hierarchy generation. Experimental results show that the algorithm is able to improve the quality of the concept hierarchy presented to the searcher; at the same time, the efficiency parameters are kept within reasonable intervals.

#index 838448
#* Domain-specific keyphrase extraction
#@ Yi-fang Brook Wu;Quanzhi Li;Razvan Stefan Bot;Xin Chen
#t 2005
#c 1
#% 196896
#% 280841
#% 420487
#% 495937
#! Document keyphrases provide semantic metadata characterizing documents and producing an overview of the content of a document. They can be used in many text-mining and knowledge management related applications. This paper describes a Keyphrase Identification Program (KIP), which extracts document keyphrases by using prior positive samples of human identified domain keyphrases to assign weights to the candidate keyphrases. The logic of our algorithm is: the more keywords a candidate keyphrase contains and the more significant these keywords are, the more likely this candidate phrase is a keyphrase. To obtain prior positive inputs, KIP first populates its glossary database using manually identified keyphrases and keywords. It then checks the composition of all noun phrases of a document, looks up the database and calculates scores for all these noun phrases. The ones having higher scores will be extracted as keyphrases.

#index 838449
#* An RSA-based time-bound hierarchical key assignment scheme for electronic article subscription
#@ Jyh-haw Yeh
#t 2005
#c 1
#% 40355
#% 78832
#% 318404
#% 443477
#% 641957
#% 772848
#! The time-bound hierarchical key assignment problem is to assign time sensitive keys to security classes in a partially ordered hierarchy so that legal data accesses among classes can be enforced. Two time-bound hierarchical key assignment schemes have been proposed in the literature, but both of them were proved insecure against collusive attacks. In this paper, we will propose an RSA-based time-bound hierarchical key assignment scheme and describe its possible application. The security analysis shows that the new scheme is safe against the collusive attacks.

#index 838450
#* Maximal termsets as a query structuring mechanism
#@ Bruno Pôssas;Nivio Ziviani;Berthier Ribeiro-Neto;Wagner Meira, Jr.
#t 2005
#c 1
#% 88950
#% 466664
#% 730071
#% 835230
#! Search engines process queries conjunctively to restrict the size of the answer set. Further, it is not rare to observe a mismatch between the vocabulary used in the text of Web pages and the terms used to compose the Web queries. The combination of these two features might lead to irrelevant query results, particularly in the case of more specific queries composed of three or more terms. To deal with this problem we propose a new technique for automatically structuring Web queries as a set of smaller subqueries. To select representative subqueries we use information on their distributions in the document collection. This can be adequately modeled using the concept of maximal termsets derived from the formalism of association rules theory. Experimentation shows that our technique leads to improved results. For the TREC-8 test collection, for instance, our technique led to gains in average precision of roughly 28% with regard to a BM25 ranking formula.

#index 838451
#* Accurately extracting coherent relevant passages using hidden Markov models
#@ Jing Jiang;ChengXiang Zhai
#t 2005
#c 1
#% 169813
#% 232677
#% 413592
#% 642979
#! In this paper, we present a principled method for accurately extracting coherent relevant passages of variable lengths using HMMs. We show that with appropriate parameter estimation, the HMM method outperforms a number of strong baseline methods on two data sets.

#index 838452
#* Structural features in content oriented XML retrieval
#@ Georgina Ramírez;Thijs Westerveld;Arjen P. de Vries
#t 2005
#c 1
#% 1721871
#! The structural features of XML components are an extra source of information that should be used in a content-oriented retrieval task on this type of documents. In this paper we explore one of the structural features from the INEX collection [1] that could be used in content-oriented search. We analyse the gain this knowledge could add to the performance of an information retrieval system and present a first approach on how this structural information could be extracted from a relevance feedback process to be used as priors in a language modelling framework.

#index 838453
#* Text document clustering based on frequent word sequences
#@ Yanjun Li;Soon M. Chung
#t 2005
#c 1
#% 262045
#! In this paper, we propose a new text clustering algorithm, named Clustering based on Frequent Word Sequences (CFWS). A word sequence is frequent if it occurs in more than certain percentage of the documents in the text database. In the past, the vector space model was commonly used for information retrieval, but it treats documents as bags of words, ignoring the sequential pattern of word occurrences in the documents. However, the meaning of natural languages strongly depends on the word sequences, and the frequent word sequences can provide compact and valuable information about the text database. Bisecting k-means and FIHC algorithms are evaluated on the performance of text clustering, and are compared with the proposed CFWS algorithm. It has been shown that CFWS has much better performance.

#index 838454
#* Information retrieval and machine learning for probabilistic schema matching
#@ Henrik Nottelmann;Umberto Straccia
#t 2005
#c 1
#% 333990
#% 378409
#% 465057
#% 1715598
#! Schema matching is the problem of finding correspondences (mapping rules, e.g. logical formulae) between heterogeneous schemas. This paper presents a probabilistic framework, called sPLMap, for automatically learning schema mapping rules. Similar to LSD, different techniques, mostly from the IR field, are combined.Our approach, however, is also able to give a probabilistic interpretation of the prediction weights of the candidates, and to select the rule set with highest matching probability.

#index 838455
#* Learning to summarise XML documents using content and structure
#@ Massih R. Amini;Anastasios Tombros;Nicolas Usunier;Mounia Lalmas;Patrick Gallinari
#t 2005
#c 1
#% 262036
#% 280838
#% 397136
#% 734915
#! Documents formatted in eXtensible Markup Language (XML) are becoming increasingly available in collections of various document types. In this paper, we present an approach for the summarisation of XML documents. The novelty of this approach lies in that it is based on features not only from the content of documents, but also from their logical structure. We follow a machine learning like, sentence extraction-based summarisation technique. To find which features are more effective for producing summaries this approach views sentence extraction as an ordering task. We evaluated our summarisation model using the INEX dataset. The results demonstrate that the inclusion of features from the logical structure of documents increases the effectiveness of the summariser, and that the learnable system is also effective and well-suited to the task of summarisation in the context of XML documents.

#index 838456
#* Trust-based collaborative filtering
#@ Jianshu Weng;Chunyan Miao;Angela Goh;Dongtao Li
#t 2005
#c 1
#% 173879
#% 783438

#index 838457
#* The earth mover's distance as a semantic measure for document similarity
#@ Xiaojun Wan;Yuxin Peng
#t 2005
#c 1
#% 325683
#% 643064
#! Different words are usually assumed to be semantically independent in most existing similarity measures, which is not often true in practice. The semantic relatedness between words cannot be conveniently employed in the existing measures. We propose a novel similarity measure based on the earth mover's distance (EMD). In the proposed measure, the semantic distances between words are computed based on the electronic lexical database-WordNet and then the EMD is employed to calculate the document similarity with a many-to-many matching between words. Experiments and results demonstrate the effectiveness of the proposed similarity measure.

#index 838458
#* Slicing*-tree based web page transformation for small displays
#@ Xiangye Xiao;Qiong Luo;Dan Hong;Hongbo Fu
#t 2005
#c 1
#% 297600
#% 326757
#! We propose a new Web page transformation method for browsing on mobile devices with small displays. In our approach, an original web page that does not fit into the screen is transformed into a set of pages, each of which fits into the screen. This transformation is done through slicing the original page. The resulting set of transformed pages form a multi-level tree structure, called a slicing*-tree, in which an internal node consists of a thumbnail image with hyperlinks and a leaf node is a block from the original web page. Our slicing*-tree based Web page transformation eases Web browsing on small displays by providing screen-fitting visual context and reducing page scrolling effort.

#index 838459
#* An evaluation of evolved term-weighting schemes in information retrieval
#@ Ronan Cummins;Colm O'Riordan
#t 2005
#c 1
#% 124073
#% 309187
#% 803033
#! This paper presents an evaluation of evolved term-weighting schemes on short, medium and long TREC queries. A previously evolved global (collection-wide) term-weighting scheme is evaluated on unseen TREC data and is shown to increase mean average precision over idf. A local (within-document) evolved term-weighting scheme is presented which is dependent on the best performing global scheme. The full evolved scheme (i.e. the combined local and global scheme) is compared to both the BM25 scheme and the Pivoted Normalisation scheme.Our results show that the local evolved solution does not perform well on some collections due to its document normalisation properties and we conclude that Okapi-tf can be tuned to interact effectively with the evolved global weighting scheme presented and increase mean average precision over the standard BM25 scheme.

#index 838460
#* Web-centric language models
#@ Jaap Kamps
#t 2005
#c 1
#% 397126
#% 642992
#! We investigate language models for informational and navigational web search. Retrieval on the web is a task that differs substantially from ordinary ad hoc retrieval. We perform an analysis of prior probability of relevance for a wide range of non-content features, shedding further light on the importance of non-content features for web retrieval. This directly explains the success or failure of various techniques, e.g., why the link topology is particularly helpful to single out important sites. Language models can naturally incorporate multiple document representations, as well as non-content information. For the former, we employ mixture language models based on document full-text, incoming anchor-text, and document titles. For the latter, we study a range of priors based on document length, URL structure, and link topology. We look at three types of topics--distillation, home page, and named page--as well as for a mixed query set. We find that the mixture models lead to considerable improvement of retrieval effectiveness for all topic types. The web-centric priors generally lead to further improvement of retrieval effectiveness.

#index 838461
#* Using RankBoost to compare retrieval systems
#@ Huyen-Trang Vu;Patrick Gallinari
#t 2005
#c 1
#% 730072
#% 734915
#% 783526
#! This paper presents a new pooling method for constructing the assessment sets used in the evaluation of retrieval systems. Our proposal is based on RankBoost, a machine learning voting algorithm. It leads to smaller pools than classical pooling and thus reduces the manual assessment workload for building test collections. Experimental results obtained on an XML document collection demonstrate the effectiveness of the approach according to different evaluation criteria.

#index 838462
#* Static score bucketing in inverted indexes
#@ Chavdar Botev;Nadav Eiron;Marcus Fontoura;Ning Li;Eugene Shekita
#t 2005
#c 1
#% 268079
#% 728195
#% 747117
#% 1015265
#% 1016222
#! Maintaining strict static score order of inverted lists is a heuristic used by search engines to improve the quality of query results when the entire inverted lists cannot be processed. This heuristic, however, increases the cost of index generation and requires complex index build algorithms. In this paper, we study a new index organization based on static score bucketing. We show that this new technique significantly improves in index build performance while having minimal impact on the quality of search results.

#index 838463
#* Scalable ranking for preference queries
#@ Ying Feng;Divyakant Agrawal;Amr El Abbadi;Ambuj Singh
#t 2005
#c 1
#% 248010
#% 300180
#% 319601
#% 333854
#% 399762
#% 480330
#% 527026
#! Top-k preference queries with multiple attributes are critical for decision-making applications. Previous research has concentrated on improving the computational efficiency mainly by using novel index structures and search strategies. Since current applications need to scale to terabytes of data and thousands of users, performance of such systems is strongly impacted by the amount of available memory. This paper proposes a scalable approach for memory-bounded top-k query processing.

#index 838464
#* Finding experts in community-based question-answering services
#@ Xiaoyong Liu;W. Bruce Croft;Matthew Koll
#t 2005
#c 1
#% 280850
#% 340901
#% 766430

#index 838465
#* Indexing time vs. query time: trade-offs in dynamic information retrieval systems
#@ Stefan Büttcher;Charles L. A. Clarke
#t 2005
#c 1
#% 172922
#% 747117
#! We examine issues in the design of fully dynamic information retrieval systems supporting both document insertions and deletions. The two main components of such a system, index maintenance and query processing, affect each other, as high query performance is usually paid for by additional work during update operations. Two aspects of the system -- incremental updates and garbage collection for delayed document deletions -- are discussed, with a focus on the respective indexing vs. query performance trade-offs. Depending on the relative number of queries and update operations, different strategies lead to optimal overall performance.

#index 838466
#* Poison pills: harmful relevant documents in feedback
#@ Egidio Terra;Robert Warren
#t 2005
#c 1
#% 115473
#% 223810
#% 342707
#% 411762
#% 580059
#% 766497
#% 766518
#% 768903

#index 838467
#* Discretization based learning approach to information retrieval
#@ Dmitri Roussinov;Weiguo Fan;Fernando A. Das Neves
#t 2005
#c 1
#% 340903
#% 838467
#! We have designed a representation scheme, which is based on the discrete representation of a document ranking function, which is capable of reproducing and enhancing the properties of such popular ranking functions as tf.idf, BM25 or those based on language models. Our tests have demonstrated the capability of our approach to achieve the performance of the best known scoring functions solely through training, without using any known heuristic or analytic formulas.

#index 838468
#* Semantic verification for fact seeking engines
#@ Dmitri Roussinov;Weiguo Fan;Fernando A. Das Neves
#t 2005
#c 1
#% 397160
#! We present the architecture of our web question answering (fact seeking) system and introduce a novel algorithm to validate semantic categories of the expected answers. When tested on the questions used by the prior research, our system demonstrated the performance comparable to the current state of the art systems. Our semantic verification algorithm has improved the accuracy of answers of the affected questions by 30%.

#index 838469
#* Fast webpage classification using URL features
#@ Min-Yen Kan;Hoang Oanh Nguyen Thi
#t 2005
#c 1
#% 211044
#% 413663
#% 568786
#% 754077
#% 769395
#! We demonstrate the usefulness of the uniform resource locator (URL) alone in performing web page classification. This approach is faster than typical web page classification, as the pages do not have to be fetched and analyzed. Our approach segments the URL into meaningful chunks and adds component, sequential and orthographic features to model salient patterns. The resulting features are used in supervised maximum entropy modeling. We analyze our approach's effectiveness on two standardized domains. Our results show that in certain scenarios, URL-based methods approach the performance of current state-of-the-art full-text and link-based methods.

#index 838470
#* On the estimation of frequent itemsets for data streams: theory and experiments
#@ Pierre-Alain Laur;Richard Nock;Jean-Emile Symphor;Pascal Poncelet
#t 2005
#c 1
#% 302395
#% 420062
#% 1705297
#! In this paper, we devise a method for the estimation of the true support of itemsets on data streams, with the objective to maximize one chosen criterion among {precision, recall} while ensuring a degradation as reduced as possible for the other criterion. We discuss the strengths, weaknesses and range of applicability of this method that relies on conventional uniform convergence results, yet guarantees statistical optimality from different standpoints.

#index 838471
#* Unapparent information revelation: a concept chain graph approach
#@ Rohini K. Srihari;Sudarshan Lamkhede;Anmol Bhasin
#t 2005
#c 1
#% 723542
#% 769887
#% 855147
#! Information generated by multiple authors working independently at different times when analyzed synergistically reveals more information than apparent. For example, a traditional search for connections between the trucking industry and Iraqi banks may not produce any documents mentioning both. However, a search that follows trails of associations across documents may suggest a connection between an auto parts manufacturer who exports to Iraq, and an Iraqi bank providing loans to buy cars. The work described here extends link analysis based on named entities and labeled relationships to general concepts and unnamed associations. Unapparent Information Revelation involves finding chains connecting concepts across documents: it uses a new representation formalism called Concept Chain Graphs.

#index 838472
#* Document quality models for web ad hoc retrieval
#@ Yun Zhou;W. Bruce Croft
#t 2005
#c 1
#% 280864
#% 309150
#% 397126
#! The quality of document content, which is an issue that is usually ignored for the traditional ad hoc retrieval task, is a critical issue for Web search. Web pages have a huge variation in quality relative to, for example, newswire articles. To address this problem, we propose a document quality language model approach that is incorporated into the basic query likelihood retrieval model in the form of a prior probability. Our results demonstrate that, on average, the new model is significantly better than the baseline (query likelihood model) in terms of precision at the top ranks.

#index 838473
#* Cooperative caching for k-NN search in ad hoc networks
#@ Bo Yang;Ali R. Hurson
#t 2005
#c 1
#% 571043
#% 646221
#! Mobile ad hoc networks have multiple limitations in performing similarity-based nearest neighbor search - dynamic topology, frequent disconnections, limited power, and restricted bandwidth. Cooperative caching is an effective technique to reduce network traffic and increase accessibility. In this paper, we propose to solve the k-nearest-neighbor search problem in ad hoc networks using a semantic-based caching scheme which reflects the content distribution in the network. The proposed scheme describes the semantic similarity among data objects using constraints, and employs cooperative caching to estimate the content distribution in the network. The query resolution based on the cooperative caching scheme is non-flooding and hierarchy-free.

#index 838474
#* A new framework to combine descriptors for content-based image retrieval
#@ Ricardo da S. Torres;Alexandre X. Falcão;Baoping Zhang;Weiguo Fan;Edward A. Fox;Marcos André Gonçalves;Pavel Calado
#t 2005
#c 1
#% 124073
#% 403487
#% 574603
#! In this paper, we propose a novel framework using Genetic Programming to combine image database descriptors for content-based image retrieval (CBIR). Our framework is validated through several experiments involving two image databases and specific domains, where the images are retrieved based on the shape of their objects.

#index 838475
#* A structure-sensitive framework for text categorization
#@ Ganesh Ramakrishnan;Deepa Paranjpe;Byron Dom
#t 2005
#c 1
#% 290482
#! This paper presents a framework called Structure Sensitive CATegorization(SSCAT), that exploits document structure for improved categorization. There are two parts to this framework, viz. (1) Documents often have layout structure, such that logically coherent text is grouped together into fields using some mark-up language. We use a log-linear model, which associates one or more features with each field. Weights associated with the field features are learnt from training data and these weights quantify the per-class importance of the field features in determining the category for the document. (2) We employ a technique that exploits the parse tree of fields that are phrasal constructs, such as title and associates weights with words in these constructs while boosting weights of important words called focus words. These weights are learnt from example instances of phrasal constructs, marked with the corresponding focus words. The learning is accomplished by training a classifier that uses linguistic features obtained from the text's parse structure. The weighted words, in fields with phrasal constructs, are used in obtaining features for the corresponding fields in the overall framework. SSCAT was tested on the supervised categorization task of over one million products from Yahoo!'s on-line shopping data. With an accuracy of over 90%, our classifier outperforms Naive Bayes and Support Vector Machines. This not only shows the effectiveness of SSCAT but also strengthens our belief that linguistic features based on natural language structure can improve tasks such as text categorization.

#index 838476
#* Efficient and effective server-sided distributed clustering
#@ Hans-Peter Kriegel;Martin Pfeifle
#t 2005
#c 1
#% 479462
#% 654487
#% 765438
#% 769946
#% 799757
#% 853058
#% 1707830
#! Clustering has become an increasingly important task in modern application domains where the data are originally located at different sites. In order to create a central clustering, all clients have to transmit their data to a central server. Due to technical limitations and security aspects, at the central site often only vague object descriptions are available. The server then has to carry out the clustering based on vague and uncertain data. In a recent paper, an approach for clustering uncertain data was proposed based on the concept of medoid clusterings. The idea of this approach is to create first several sample clusterings. Then based on suitable distance functions between clusterings the most average clustering, i.e. the medoid clustering, was determined. In this paper, we extend this approach for partitioning clustering algorithms and propose to compute a centroid clustering based on these input sample clusterings. These centroid clusterings are new artificial clusterings which minimize the distance to all the sample clusterings.

#index 838477
#* Evaluation of a MCA-based approach to organize data cubes
#@ Riadh Ben Messaoud;Omar Boussaid;Sabine Loudcher Rabaséda
#t 2005
#c 1
#% 207552
#% 223781
#! In the OLAP context, exploration of huge and sparse data cubes is a tedious task that does not always lead to efficient results. We propose to use a Multiple Correspondence Analysis (MCA) in order to enhance data cube representations and make them more suitable for visualization and thus, easier to analyze. We also provide an original quality criterion to measure the relevance of the obtained data representations. Experimental results we led on real data samples have shown the interest and the efficiency of our approach.

#index 838478
#* Semantic similarity over the gene ontology: family correlation and selecting disjunctive ancestors
#@ Francisco M. Couto;Mário J. Silva;Pedro M. Coutinho
#t 2005
#c 1
#% 465914
#% 1275285
#! Many bioinformatics applications would benefit from comparing proteins based on their biological role rather than their sequence. In most biological databases, proteins are already annotated with ontology terms. Previous studies identified a correlation between the sequence similarity and the semantic similarity of proteins. The semantic similarity of proteins was computed from their annotated GO terms. However, proteins sharing a biological role do not necessarily have a similar sequence.This paper introduces our study of the correlation between GO and family similarity. Family similarity overcomes some of the limitations of sequence similarity, thus we obtained a strong correlation between GO and family similarity. Additionally, this paper introduces GraSM, a novel method that uses all the information in the graph structure of the GO, instead of considering it as a hierarchical tree. When calculating the semantic similarity of two concepts, GraSM selects the disjunctive common ancestors rather than only using the most informative common ancestor. GraSM produced a higher family similarity correlation than the original semantic similarity measures.

#index 838479
#* Frequent pattern discovery with memory constraint
#@ Kun-Ta Chuang;Ming-Syan Chen
#t 2005
#c 1
#% 443194
#% 481290
#% 814645
#! We explore in this paper a practicably interesting mining task to retrieve frequent itemsets with memory constraint. As opposed to most previous works that concentrate on improving the mining efficiency or on reducing the memory size by best effort, we first attempt to constrain the upper memory size that can be utilized by mining frequent itemsets in this paper.

#index 838480
#* Extracting a website's content structure from its link structure
#@ Nan Liu;Christopher C. Yang
#t 2005
#c 1
#% 300966
#% 642980
#% 807359
#! Hierarchical models are commonly used to organize a Website's content. A Website's content structure can be represented by a topic hierarchy, a directed tree rooted at a Website's homepage in which the vertices and edges correspond to Web pages and hyperlinks. In this work, we propose an algorithm for extracting a Website's topic hierarchy from its link structure. The proposed algorithm consists of a construction stage and a refining stage, in which we analyze the semantic relationships between web pages based on link structure, web page content and directory structure. We've done extensive experiments using different Websites and obtained very promising results.

#index 838481
#* Improving intranet search-engines using context information from databases
#@ Christoph Mangold;Holger Schwarz;Bernhard Mitschang
#t 2005
#c 1
#% 479803
#% 577373
#% 584906
#% 660011
#% 754095
#% 1015325
#! Information in enterprises comes in documents and data bases. From a semantic viewpoint, both kinds of information are usually tightly connected. In this paper, we propose to enhance common search-engines with contextual information retrieved from databases. We establish system requirements and anecdotally demonstrate how documents and database information can be represented as the nodes of a graph. Then, we give an example how we exploit this graph information for document retrieval.

#index 838482
#* A new permutation approach for distributed association rule mining
#@ Yiqun Huang;Zhengding Lu;Heping Hu
#t 2005
#c 1
#% 577289
#% 629085
#% 729930
#% 763583
#! Privacy preserving distributed data mining has become a promising research area. This paper addresses the problem of association rule mining where the global database is vertically partitioned. When transactions are distributed in different sites, scalar product is a feasible tool to discover frequent itemsets. We present a new protocol to compute scalar product between two parties with a permutation approach. We analyze the protocol in detail and demonstrate its effectiveness and high privacy properties, and compare it to other published protocols.

#index 838483
#* On off-topic access detection in information systems
#@ Nazli Goharian;Ling Ma
#t 2005
#c 1
#% 730033
#% 810636
#% 1719441
#% 1719456
#% 1719478
#! We focus on detecting insider access violations to off-topic documents. Previously, we utilized information retrieval techniques, e.g., clustering and relevance feedback, to warn of potential misuse. For the relevance feedback approach, we minimize the indicative features needed for detection using data mining techniques. We show that the derived reduced feature subset achieves equivalent performance to that of the previously derived full set of features.

#index 838484
#* Privacy leakage in multi-relational databases via pattern based semi-supervised learning
#@ Hui Xiong;Michael Steinbach;Vipin Kumar
#t 2005
#c 1
#% 93228
#% 329858
#% 727897
#% 731610
#! In multi-relational databases, a view, which is a context- and content-dependent subset of one or more tables (or other views), is often used to preserve privacy by hiding sensitive information. However, recent developments in data mining present a new challenge for database security even when traditional database security techniques, such as database access control, are employed. This paper presents a data mining framework using semi-supervised learning that demonstrates the potential for privacy leakage in multi-relational databases. Many different types of semi-supervised learning techniques, such as the K-nearest neighbor (KNN) method, can be used to demonstrate privacy leakage. However, we also introduce a new approach to semi-supervised learning, hyperclique pattern based semi-supervised learning (HPSL), which differs from traditional semi-supervised learning approaches in that it considers the similarity among groups of objects instead of only pairs of objects. Our experimental results show that both the KNN and HPSL methods have the ability to compromise database security, although HPSL is better at this privacy violation than the KNN method.

#index 838485
#* Document clustering using character N-grams: a comparative evaluation with term-based and word-based clustering
#@ Yingbo Miao;Vlado Kešelj;Evangelos Milios
#t 2005
#c 1
#% 723828
#! We propose a novel method for document clustering using character N-grams. In the traditional vector-space model, the documents are represented as vectors, in which each dimension corresponds to a word. We propose a document representation based on the most frequent character N-grams, with window size of up to 10 characters. We derive a new distance measure, which produces uniformly better results when compared to the word-based and term-based methods. The result becomes more significant in the light of the robustness of the N-gram method with no language-dependent preprocessing. Experiments on the performance of a clustering algorithm on a variety of test document corpora demonstrate that the N-gram representation with n=3 outperforms both word and term representations. The comparison between word and term representations depends on the data set and the selected dimensionality.

#index 838486
#* Inferring document similarity from hyperlinks
#@ David Grangier;Samy Bengio
#t 2005
#c 1
#% 309145
#% 361100
#! Assessing semantic similarity between text documents is a crucial aspect in Information Retrieval systems. In this work, we propose to use hyperlink information to derive a similarity measure that can then be applied to compare any text documents, with or without hyperlinks. As linked documents are generally semantically closer than unlinked documents, we use a training corpus with hyperlinks to infer a function a,b → sim(a,b) that assigns a higher value to linked documents than to unlinked ones. Two sets of experiments on different corpora show that this function compares favorably with OKAPI matching on document retrieval tasks.

#index 838487
#* A hybrid approach to NER by MEMM and manual rules
#@ Moshe Fresko;Binyamin Rosenfeld;Ronen Feldman
#t 2005
#c 1
#% 211044
#% 466892
#% 742424
#% 783553
#! This paper describes a framework for defining domain specific Feature Functions in a user friendly form to be used in a Maximum Entropy Markov Model (MEMM) for the Named Entity Recognition (NER) task. Our system called MERGE allows defining general Feature Function Templates, as well as Linguistic Rules incorporated into the classifier. The simple way of translating these rules into specific feature functions are shown. We show that MERGE can perform better from both purely machine learning based systems and purely-knowledge based approaches by some small expert interaction of rule-tuning.

#index 838488
#* Situation-aware risk management in autonomous agents
#@ Martin Lorenz;Jan D. Gehrke;Hagen Langer;Ingo J. Timm;Joachim Hammer
#t 2005
#c 1
#% 100324
#% 729449
#% 750572
#! We present a novel approach to enable decision-making in a highly distributed multiagent environment where individual agents need to act in an autonomous fashion. Our architecture framework integrates risk management, knowledge management, and agent deliberation to enable sophisticated, autonomous decision-making. Instead of a centralized knowledge repository, our approach supports a highly distributed knowledge base in which each agent manages a fraction of the knowledge needed by the entire system.

#index 838489
#* Mining officially unrecognized side effects of drugs by combining web search and machine learning
#@ Carlo A. Curino;Yuanyuan Jia;Bruce Lambert;Patricia M. West;Clement Yu
#t 2005
#c 1
#% 54417
#% 55490
#% 219052
#% 280817
#% 311027
#% 316709
#% 375017
#% 376266
#% 387427
#% 766431
#% 766440
#! We consider the problem of finding officially unrecognized side effects of drugs. By submitting queries to the Web involving a given drug name, it is possible to retrieve pages concerning the drug. However, many retrieved pages are irrelevant and some relevant pages are not retrieved. More relevant pages can be obtained by adding the active ingredient of the drug to the query. In order to eliminate irrelevant pages, we propose a machine learning process to filter out the undesirable pages. The process is shown experimentally to be very effective. Since obtaining training data for the machine learning process can be time consuming and expensive, we provide an automatic method to generate the training data. The method is also shown to be very accurate. The side effects of three drugs which are not recognized by FDA are validated by an expert. We believe that the same approach can be applied to many real life problems and will yield high precision. Thus, this could lead a new way to perform retrieval with high accuracy.

#index 838490
#* MailRank: using ranking for spam detection
#@ Paul-Alexandru Chirita;Jörg Diederich;Wolfgang Nejdl
#t 2005
#c 1
#% 220706
#% 577329
#% 577367
#% 753425
#% 779950
#% 803668
#% 807297
#% 1016177
#% 1112900
#! Can we use social networks to combat spam? This paper investigates the feasibility of MailRank, a new email ranking and classification scheme exploiting the social communication network created via email interactions. The underlying email network data is collected from the email contacts of all MailRank users and updated automatically based on their email activities to achieve an easy maintenance. MailRank is used to rate the sender address of arriving emails such that emails from trustworthy senders can be ranked and classified as spam or non-spam. The paper presents two variants: Basic MailRank computes a global reputation score for each email address, whereas in Personalized MailRank the score of each email address is different for each MailRank user. The evaluation shows that MailRank is highly resistant against spammer attacks, which obviously have to be considered right from the beginning in such an application scenario. MailRank also performs well even for rather sparse networks, i.e., where only a small set of peers actually take part in the ranking of email addresses.

#index 838491
#* ViPER: augmenting automatic information extraction with visual perceptions
#@ Kai Simon;Georg Lausen
#t 2005
#c 1
#% 235941
#% 337991
#% 397605
#% 451356
#% 577319
#% 660272
#% 729978
#% 744093
#% 769437
#% 788941
#% 794239
#% 805845
#% 805846
#% 1394469
#! In this paper we address the problem of unsupervised Web data extraction. We show that unsupervised Web data extraction becomes feasible when supposing pages that are made up of repetitive patterns, as it is the case, e.g., for search engine result pages. Hereby the extraction rules are generated automatically without any training or human interaction, by means of operating on the DOM tree respectively the flat tag token sequence of a single page.Our contribution to automatic data extraction through this paper is twofold. First, we identify and rank potential repetitive patterns with respect to the user's visual perception of the Web page, well aware that location and size of matching elements within a Web page constitute important criteria for defining relevance. Second, matching sub-sequences of the pattern with the highest weightiness are aligned with global multiple sequence alignment techniques. Experimental results show that our system is able to achieve high accuracy in distilling and aligning regularly structured objects inside complex Web pages.

#index 838492
#* Interconnection semantics for keyword search in XML
#@ Sara Cohen;Yaron Kanza;Benny Kimelfeld;Yehoshua Sagiv
#t 2005
#c 1
#% 39702
#% 179784
#% 333843
#% 342678
#% 465068
#% 479803
#% 598376
#% 659990
#% 660011
#% 993987
#% 1015258
#% 1016135
#% 1673660
#! A framework for describing semantic relationships among nodes in XML documents is presented. In contrast to earlier work, the XML documents may have ID references (i.e., they correspond to graphs and not just trees). A specific interconnection semantics in this framework can be defined explicitly or derived automatically. The main advantage of interconnection semantics is the ability to pose queries on XML data in the style of keyword search. Several methods for automatically deriving interconnection semantics are presented. The complexity of the evaluation and the satisfiability problems under the derived semantics is analyzed. For many important cases, the complexity is tractable and hence, the proposed interconnection semantics can be efficiently applied to real-world XML documents.

#index 838493
#* Efficient indexing and querying of XML data using modified Prüfer sequences
#@ K. Hima Prasad;P. Sreenivasa Kumar
#t 2005
#c 1
#% 397375
#% 480489
#% 654450
#% 659999
#% 745461
#% 800535
#% 993953
#! With the advent of XML as the new standard for information representation and exchange, indexing and querying of XML data is of major concern. In this paper, we propose a method for representing an XML document as a sequence based on a variation of Prüfer sequences. We incorporate new components in the node encodings such as level, number of a certain kind of descendants and develop methods for holistic processing of tree pattern queries. The query processing involves converting the query also into a sequence and performing subsequence matching on the document sequence. We establish certain interesting properties of the proposed method of sequencing that give rise to a new efficient pattern matching algorithm. The sequence data is stored in a two level B+-trees to support query processing. We also propose an optimization for parent-child axis to speed up the query processing. Our approach does not require any post-processing and guarantees results that are free of false positives and duplicates. Experimental results show that our system performs significantly better than previous systems in a large number of cases.

#index 838494
#* Towards automatic association of relevant unstructured content with structured query results
#@ Prasan Roy;Mukesh Mohania;Bhuvan Bamba;Shree Raman
#t 2005
#c 1
#% 144070
#% 194253
#% 201936
#% 242366
#% 300169
#% 314740
#% 387427
#% 397393
#% 660011
#% 770327
#% 770332
#% 770334
#% 770336
#% 838426
#% 1016176
#! Faced with growing knowledge management needs, enterprises are increasingly realizing the importance of seamlessly integrating critical business information distributed across both structured and unstructured data sources. In existing information integration solutions, the application needs to formulate the SQL logic to retrieve the needed structured data on one hand, and identify a set of keywords to retrieve the related unstructured data on the other. This paper proposes a novel approach wherein the application specifies its information needs using only a SQL query on the structured data, and this query is automatically ``translated'' into a set of keywords that can be used to retrieve relevant unstructured data. We describe the techniques used for obtaining these keywords from (i) the query result, and (ii) additional related information in the underlying database. We further show that these techniques achieve high accuracy with very reasonable overheads.

#index 838495
#* Predicting accuracy of extracting information from unstructured text collections
#@ Eugene Agichtein;Silviu Cucerzan
#t 2005
#c 1
#% 262096
#% 279755
#% 283180
#% 298183
#% 301241
#% 306514
#% 397161
#% 747913
#% 754068
#% 815318
#% 815868
#% 815924
#% 854668
#% 854799
#% 854802
#% 854805
#% 854819
#% 854830
#% 855108
#% 855110
#% 855112
#% 855114
#% 855117
#% 855123
#! Exploiting lexical and semantic relationships in large unstructured text collections can significantly enhance managing, integrating, and querying information locked in unstructured text. Most notably, named entities and relations between entities are crucial for effective question answering and other information retrieval and knowledge management tasks. Unfortunately, the success in extracting these relationships can vary for different domains, languages, and document collections. Predicting extraction performance is an important step towards scalable and intelligent knowledge management, information retrieval and information integration. We present a general language modeling method for quantifying the difficulty of information extraction tasks. We demonstrate the viability of our approach by predicting performance of real world information extraction tasks, Named Entity recognition and Relation Extraction.

#index 838496
#* WAM-Miner: in the search of web access motifs from historical web log data
#@ Qiankun Zhao;Sourav S. Bhowmick;Le Gruenwald
#t 2005
#c 1
#% 273693
#% 342632
#% 378183
#% 433773
#% 443194
#% 463903
#% 464996
#% 466490
#% 577218
#% 580222
#% 586843
#% 629715
#% 630984
#% 723556
#% 727909
#% 729967
#! Existing web usage mining techniques focus only on discovering knowledge based on the statistical measures obtained from the static characteristics of web usage data. They do not consider the dynamic nature of web usage data. In this paper, we focus on discovering novel knowledge by analyzing the change patterns of historical web access sequence data. We present an algorithm called WAM-MINER to discover Web Access Motifs (WAMs). WAMs are web access patterns that never change or do not change significantly most of the time (if not always) in terms of their support values during a specific time period. WAMs are useful for many applications, such as intelligent web advertisement, web site restructuring, business intelligence, and intelligent web caching.

#index 838497
#* A framework for mining topological patterns in spatio-temporal databases
#@ Junmei Wang;Wynne Hsu;Mong Li Lee
#t 2005
#c 1
#% 248790
#% 338580
#% 342635
#% 420078
#% 481281
#% 481290
#% 527021
#% 527188
#% 727910
#% 728302
#% 769914
#! Mining topological patterns in spatial databases has received a lot of attention. However, existing work typically ignores the temporal aspect and suffers from certain efficiency problems. They are not scalable for mining topological patterns in spatio-temporal databases. In this paper, we study the problem for mining topological patterns by incorporating the temporal aspect in the mining process. We introduce a summary-structure that records the instances' count information of a feature in a region within a time window. Using this structure, we design an algorithm, TopologyMiner, to find interesting topological patterns without the need to generate candidates. Experimental results show that TopologyMiner is effective and scalable in finding topological patterns and outperforms Apriori-like algorithm by a few orders of magnitudes.

#index 838498
#* Automated cleansing for spend analytics
#@ Moninder Singh;Jayant R. Kalagnanam;Sudhir Verma;Amit J. Shah;Swaroop K. Chalasani
#t 2005
#c 1
#% 310516
#% 333679
#% 387427
#% 458379
#% 648320
#% 707780
#% 722822
#! The development of an aggregate view of the procurement spend across an enterprise using transactional data is increasingly becoming a very important and strategic activity. Not only does it provide a complete and accurate picture of what the enterprise is buying and from whom, it also allows it to consolidate suppliers, as well as negotiate better prices. The importance, as well as the complexity, of this cleansing exercise is further magnified by the increasing popularity of Business Transformation Outsourcing (BTO) wherein enterprises are turning over non-core activities, such as indirect procurement, to third parties, who now need to develop an integrated view of spend across multiple enterprises in order to optimize procurement and generate maximum savings. However, the creation of such an integrated view of procurement spend requires the creation of a homogeneous data repository from disparate (heterogeneous) data sources across various geographic and functional organizations throughout the enterprise(s). Such repositories get transactional data from various sources such as invoices, purchase orders, account ledgers. As such, the transactions are not cross-indexed, refer to the same suppliers by different names, and use different ways of representing information about the same commodities. Before an aggregated spend view can be developed, this data needs to be cleansed, primarily to normalize the supplier names and correctly map each transaction to the appropriate commodity code. Commodity mapping, in particular, is made more difficult by the fact that it has to be done on the basis of unstructured text descriptions found in the various data sources. We describe an on-demand system to automatically perform this cleansing activity using techniques from information retrieval and machine learning. Built on standard integration and application infrastructure software, this system provides enterprises with a fast, reliable, accurate and on-demand way of cleansing transactional data and generating an integrated view of spend. This system is currently in the process of being deployed by IBM for use in its BTO practice.

#index 838499
#* Feature-based recommendation system
#@ Eui-Hong (Sam) Han;George Karypis
#t 2005
#c 1
#% 173879
#% 202009
#% 202011
#% 220706
#% 220707
#% 220709
#% 220711
#% 280447
#% 280852
#% 301590
#% 308769
#% 310567
#% 310572
#% 314933
#% 330687
#% 342687
#% 465928
#% 722754
#% 734594
#% 1650569
#! The explosive growth of the world-wide-web and the emergence of e-commerce has led to the development of recommender systems--a personalized information filtering technology used to identify a set of N items that will be of interest to a certain user. User-based and model-based collaborative filtering are the most successful technology for building recommender systems to date and is extensively used in many commercial recommender systems. The basic assumption in these algorithms is that there are sufficient historical data for measuring similarity between products or users. However, this assumption does not hold in various application domains such as electronics retail, home shopping network, on-line retail where new products are introduced and existing products disappear from the catalog. Another such application domains is home improvement retail industry where a lot of products (such as window treatments, bathroom, kitchen or deck) are custom made. Each product is unique and there are very little duplicate products. In this domain, the probability of the same exact two products bought together is close to zero. In this paper, we discuss the challenges of providing recommendation in the domains where no sufficient historical data exist for measuring similarity between products or users. We present feature-based recommendation algorithms that overcome the limitations of the existing top-n recommendation algorithms. The experimental evaluation of the proposed algorithms in the real life data sets shows a great promise. The pilot project deploying the proposed feature-based recommendation algorithms in the on-line retail web site shows 75% increase in the recommendation revenue for the first 2 month period.

#index 838500
#* Automatic analysis of call-center conversations
#@ Gilad Mishne;David Carmel;Ron Hoory;Alexey Roytman;Aya Soffer
#t 2005
#c 1
#% 348956
#% 501924
#% 509533
#% 577220
#% 624479
#% 741441
#% 742083
#% 782759
#! We describe a system for automating call-center analysis and monitoring. Our system integrates transcription of incoming calls with analysis of their content; for the analysis, we introduce a novel method of estimating the domain-specific importance of conversation fragments, based on divergence of corpus statistics. Combining this method with Information Retrieval approaches, we provide knowledge-mining tools both for the call-center agents and for administrators of the center.

#index 838501
#* A new approach to intranet search based on information extraction
#@ Hang Li;Yunbo Cao;Jun Xu;Yunhua Hu;Shenjie Li;Dmitriy Meyerzon
#t 2005
#c 1
#% 190581
#% 330616
#% 340928
#% 348163
#% 464612
#% 549563
#% 577326
#% 577339
#% 614036
#% 754059
#% 754067
#% 768898
#% 807296
#% 809426
#% 854668
#% 963890
#! This paper is concerned with 'intranet search'. By intranet search, we mean searching for information on an intranet within an organization. We have found that search needs on an intranet can be categorized into types, through an analysis of survey results and an analysis of search log data. The types include searching for definitions, persons, experts, and homepages. Traditional information retrieval only focuses on search of relevant documents, but not on search of special types of information. We propose a new approach to intranet search in which we search for information in each of the special types, in addition to the traditional relevance search. Information extraction technologies can play key roles in such kind of 'search by type' approach, because we must first extract from the documents the necessary information in each type. We have developed an intranet search system called 'Information Desk'. In the system, we try to address the most important types of search first - finding term definitions, homepages of groups or topics, employees' personal information and experts on topics. For each type of search, we use information extraction technologies to extract, fuse, and summarize information in advance. The system is in operation on the intranet of Microsoft and receives accesses from about 500 employees per month. Feedbacks from users and system logs show that users consider the approach useful and the system can really help people to find information. This paper describes the architecture, features, component technologies, and evaluation results of the system.

#index 838502
#* A novel refinement approach for text categorization
#@ Songbo Tan;Xueqi Cheng;Moustafa M. Ghanem;Bin Wang;Hongbo Xu
#t 2005
#c 1
#% 280817
#% 311034
#% 340904
#% 344447
#% 375017
#% 465754
#% 577232
#! In this paper we present a novel strategy, DragPushing, for improving the performance of text classifiers. The strategy is generic and takes advantage of training errors to successively refine the classification model of a base classifier. We describe how it is applied to generate two new classification algorithms; a Refined Centroid Classifier and a Refined Naïve Bayes Classifier. We present an extensive experimental evaluation of both algorithms on three English collections and one Chinese corpus. The results indicate that in each case, the refined classifiers achieve significant performance improvement over the base classifiers used. Furthermore, the performance of the Refined Centroid Classifier implemented is comparable, if not better, to that of state-of-the-art support vector machine (SVM)-based classifier, but offers a much lower computational cost.

#index 838503
#* Intelligent GP fusion from multiple sources for text classification
#@ Baoping Zhang;Yuxin Chen;Weiguo Fan;Edward A. Fox;Marcos Gonçalves;Marco Cristo;Pável Calado
#t 2005
#c 1
#% 54970
#% 114994
#% 124073
#% 169718
#% 241005
#% 248810
#% 281209
#% 309142
#% 335914
#% 345139
#% 348178
#% 385254
#% 413663
#% 430761
#% 438103
#% 458379
#% 464267
#% 730061
#% 733838
#% 740768
#% 765526
#% 804914
#% 1386610
#% 1387536
#% 1777130
#! This paper shows how citation-based information and structural content (e.g., title, abstract) can be combined to improve classification of text documents into predefined categories. We evaluate different measures of similarity -- five derived from the citation information of the collection, and three derived from the structural content -- and determine how they can be fused to improve classification effectiveness. To discover the best fusion framework, we apply Genetic Programming (GP) techniques. Our experiments with the ACM Computing Classification Scheme, using documents from the ACM Digital Library, indicate that GP can discover similarity functions superior to those based solely on a single type of evidence. Effectiveness of the similarity functions discovered through simple majority voting is better than that of content-based as well as combination-based Support Vector Machine classifiers. Experiments also were conducted to compare the performance between GP techniques and other fusion techniques such as Genetic Algorithms (GA) and linear fusion. Empirical results show that GP was able to discover better similarity functions than GA or other fusion techniques.

#index 838504
#* Time weight collaborative filtering
#@ Yi Ding;Xue Li
#t 2005
#c 1
#% 124010
#% 173879
#% 330687
#% 343141
#% 577368
#% 643007
#% 724548
#% 729932
#% 734590
#% 737417
#% 754126
#% 766449
#% 766517
#% 769888
#% 769915
#% 783531
#% 1016200
#% 1650569
#% 1707866
#! Collaborative filtering is regarded as one of the most promising recommendation algorithms. The item-based approaches for collaborative filtering identify the similarity between two items by comparing users' ratings on them. In these approaches, ratings produced at different times are weighted equally. That is to say, changes in user purchase interest are not taken into consideration. For example, an item that was rated recently by a user should have a bigger impact on the prediction of future user behaviour than an item that was rated a long time ago. In this paper, we present a novel algorithm to compute the time weights for different items in a manner that will assign a decreasing weight to old data. More specifically, the users' purchase habits vary. Even the same user has quite different attitudes towards different items. Our proposed algorithm uses clustering to discriminate between different kinds of items. To each item cluster, we trace each user's purchase interest change and introduce a personalized decay factor according to the user own purchase behaviour. Empirical studies have shown that our new algorithm substantially improves the precision of item-based collaborative filtering without introducing higher order computational complexity.

#index 838505
#* Handling frequent updates of moving objects
#@ Bin Lin;Jianwen Su
#t 2005
#c 1
#% 86950
#% 213975
#% 273706
#% 273715
#% 300174
#% 300194
#% 315005
#% 333853
#% 378405
#% 413597
#% 427199
#% 437913
#% 443181
#% 461923
#% 480093
#% 481304
#% 487880
#% 503869
#% 527166
#% 527174
#% 527187
#% 554884
#% 571296
#% 742059
#% 803125
#% 1015288
#% 1015305
#% 1015320
#! A critical issue in moving object databases is to develop appropriate indexing structures for continuously moving object locations so that queries can still be performed efficiently. However, such location changes typically cause a high volume of updates, which in turn poses serious problems on maintaining index structures. In this paper we propose a Lazy Group Update (LGU) algorithm for disk-based index structures of moving objects. LGU contains two key additional structures to group ``similar'' updates so that they can be performed together: a disk-based insertion buffer (I-Buffer) for each internal node, and a memory-based deletion table (D-Table) for the entire tree. Different strategies of ``pushing down'' an overflow I-Buffer to the next level are studied. Comprehensive empirical studies over uniform and skewed datasets, as well as simulated street traffic data show that LGU achieves a significant improvement on update throughput while allowing a reasonable performance for queries.

#index 838506
#* QED: a novel quaternary encoding to completely avoid re-labeling in XML updates
#@ Changqing Li;Tok Wang Ling
#t 2005
#c 1
#% 58365
#% 236416
#% 325384
#% 333981
#% 378412
#% 397366
#% 462062
#% 479465
#% 480489
#% 745479
#% 765488
#% 800523
#% 823655
#% 838419
#% 1716934
#! The method of assigning labels to the nodes of the XML tree is called a labeling scheme. Based on the labels only, both ordered and un-ordered queries can be processed without accessing the original XML file. One more important point for the labeling scheme is the label update cost in inserting or deleting a node into or from the XML tree. All the current labeling schemes have high update cost, therefore in this paper we propose a novel quaternary encoding approach for the labeling schemes. Based on this encoding approach, we need not re-label any existing nodes when the update is performed. Extensive experimental results on the XML datasets illustrate that our QED works much better than the existing labeling schemes on the label updates when considering either the number of nodes or the time for re-labeling.

#index 838507
#* Detecting changes on unordered XML documents using relational databases: a schema-conscious approach
#@ Erwin Leonardi;Sourav S. Bhowmick
#t 2005
#c 1
#% 25998
#% 479956
#% 745478
#% 1716986
#! Several relational approaches have been proposed to detect the changes to XML documents by using relational databases. These approaches store the XML documents in the relational database and issue SQL queries (whenever appropriate) to detect the changes. All of these relational-based approaches use the schema-oblivious XML storage strategy for detecting the changes. However, there is growing evidence that schema-conscious storage approaches perform significantly better than schema-oblivious approaches as far as XML query processing is concerned. In this paper, we study a relational-based unordered XML change detection technique (called HELIOS) that uses a schema-conscious approach (Shared-Inlining) as the underlying storage strategy. HELIOS is up to 52 times faster than X-Diff [7] for large datasets (more than 1000 nodes). It is also up to 6.7 times faster than XANDY [4]. The result quality of deltas detected by HELIOS is comparable to the result quality of deltas detected by XANDY.

#index 838508
#* Similarity measures for tracking information flow
#@ Donald Metzler;Yaniv Bernstein;W. Bruce Croft;Alistair Moffat;Justin Zobel
#t 2005
#c 1
#% 201935
#% 255137
#% 262096
#% 280851
#% 340948
#% 571725
#% 643014
#% 740915
#% 818332
#! Text similarity spans a spectrum, with broad topical similarity near one extreme and document identity at the other. Intermediate levels of similarity -- resulting from summarization, paraphrasing, copying, and stronger forms of topical relevance -- are useful for applications such as information flow analysis and question-answering tasks. In this paper, we explore mechanisms for measuring such intermediate kinds of similarity, focusing on the task of identifying where a particular piece of information originated. We consider both sentence-to-sentence and document-to-document comparison, and have incorporated these algorithms into RECAP, a prototype information flow analysis tool. Our experimental results with RECAP indicate that new mechanisms such as those we propose are likely to be more appropriate than existing methods for identifying the intermediate forms of similarity.

#index 838509
#* Word sense disambiguation in queries
#@ Shuang Liu;Clement Yu;Weiyi Meng
#t 2005
#c 1
#% 144031
#% 169729
#% 169768
#% 245788
#% 279755
#% 286069
#% 387427
#% 389155
#% 413612
#% 642994
#% 741083
#% 741839
#% 742424
#% 748550
#% 766439
#% 766440
#% 817962
#% 939810
#% 1414358
#! This paper presents a new approach to determine the senses of words in queries by using WordNet. In our approach, noun phrases in a query are determined first. For each word in the query, information associated with it, including its synonyms, hyponyms, hypernyms, definitions of its synonyms and hyponyms, and its domains, can be used for word sense disambiguation. By comparing these pieces of information associated with the words which form a phrase, it may be possible to assign senses to these words. If the above disambiguation fails, then other query words, if exist, are used, by going through exactly the same process. If the sense of a query word cannot be determined in this manner, then a guess of the sense of the word is made, if the guess has at least 50% chance of being correct. If no sense of the word has 50% or higher chance of being used, then we apply a Web search to assist in the word sense disambiguation process. Experimental results show that our approach has 100% applicability and 90% accuracy on the most recent robust track of TREC collection of 250 queries. We combine this disambiguation algorithm to our retrieval system to examine the effect of word sense disambiguation in text retrieval. Experimental results show that the disambiguation algorithm together with other components of our retrieval system yield a result which is 13.7% above that produced by the same system but without the disambiguation, and 9.2% above that produced by using Lesk's algorithm. Our retrieval effectiveness is 7% better than the best reported result in the literature.

#index 838510
#* ERkNN: efficient reverse k-nearest neighbors retrieval with local kNN-distance estimation
#@ Chenyi Xia;Wynne Hsu;Mong Li Lee
#t 2005
#c 1
#% 80995
#% 300163
#% 318703
#% 342828
#% 465009
#% 465017
#% 480661
#% 730019
#% 993999
#% 1016191
#! The Reverse k-Nearest Neighbors (RkNN) queries are important in profile-based marketing, information retrieval, decision support and data mining systems. However, they are very expensive and existing algorithms are not scalable to queries in high dimensional spaces or of large values of k. This paper describes an efficient estimation-based RkNN search algorithm (ERkNN) which answers RkNN queries based on local kNN-distance estimation methods. The proposed approach utilizes estimation-based filtering strategy to lower the computation cost of RkNN queries. The results of extensive experiments on both synthetic and real life datasets demonstrate that ERkNN algorithm retrieves RkNN efficiently and is scalable with respect to data dimensionality, k, and data size.

#index 838511
#* Kalchas: a dynamic XML search engine
#@ Rasmus Kaae;Thanh-Duy Nguyen;Dennis Nørgaard;Albrecht Schmidt
#t 2005
#c 1
#% 1921
#% 118741
#% 122307
#% 169814
#% 172922
#% 214198
#% 243929
#% 249989
#% 332666
#% 342397
#% 465155
#% 481439
#% 654442
#% 659923
#% 747117
#% 810052
#! This paper outlines the system architecture and the core data structures of Kalchas, a fulltext search engine for XML data with emphasis on dynamic indexing, and identifies features worth demonstrating. The concept of dynamic index implies that the aim is to re ect the creation of, deletion of, and updates to relevant files in the search index as early as possible. This is achieved by a number of techniques, including ideas drawn from partitioned B-Trees and inverted indices. The actual ranked retrieval of document is implemented with XML-specific query operators for lowest common ancestor queries.A live demonstration will discuss Kalchas' behaviour in typical use cases, such as interactive editing sessions and bulk loading large amounts of static files as well as querying the contents of the indexed files; it tries to clarify both the short-comings and the advantages of the method.

#index 838512
#* Order checking in a CPOE using event analyzer
#@ Lilian Harada;Yuuji Hotta
#t 2005
#c 1
#% 333850
#% 464058
#% 503878
#% 587746
#% 730030
#% 775886
#% 801694
#! In this paper we present our experience in applying Event Analyzer, a processing engine we have developed to extract patterns from a sequence of events, in the checking of medical orders of a CPOE system. We present some extensions we have implemented in Event Analyzer in order to fulfill the needs of those orders checking, as well as some performance evaluation results. We also outline some problems we are facing now to adapt Event Analyzer's pattern detection engine to support streaming orders in an on-line CPOE checking system.

#index 838513
#* SyynX solutions: practical knowledge management in a medical environment
#@ Christian Herzog;Gianpiero Liuzzi;Mario Diwersy
#t 2005
#c 1
#! In this paper we describe the Knowledge Management approach for the biomedical scientific community developed by SyynX Solutions GmbH [1].

#index 838514
#* Leveraging collective knowledge
#@ Henry Kon;Michael Hoey
#t 2005
#c 1
#% 288211
#! As more organizations begin to deploy taxonomies for categorization and faceted search, the cost of producing these knowledge models is becoming the largest expense on a project. At a cost of 200 - 300 dollars per topic, manually developing subject area taxonomies does not scale for any but the smallest of projects. This paper will discuss an approach called Orthogonal Corpus Indexing ( OCI ). OCI leverages existing published knowledge in the subject area of the taxonomy model. This knowledge is algorithmically mapped into multiple taxonomies via the OCI algorithm. The resulting taxonomy costs are 1/ 100th of the cost of manual methods and are created with embedded rule sets for categorization engines. This paper will discuss the theory of OCI, its practical use as well as examples of knowledge management techniques that are possible when taxonomies are large, detailed and inexpensive.

#index 838515
#* Taxonomies by the numbers: building high-performance taxonomies
#@ Stephen C. Gates;Wilfried Teiken;Keh-Shin F. Cheng
#t 2005
#c 1
#% 91103
#% 280492
#% 311027
#% 413639
#% 729919
#% 730047
#% 770327
#% 771354
#% 780962
#% 782758
#% 782759
#% 783539
#% 787547
#% 800009
#% 805863
#! In this paper, we describe a system for the construction of taxonomies which yield high accuracies with automated categorization systems, even on Web and intranet documents. In particular, we describe the way in which measurement of five key features of the system can be used to predict when categories are sufficiently well defined to yield high accuracy categorization. We describe the use of this system to construct a large (8800-category) general-purpose taxonomy and categorization system.

#index 838516
#* Distributed PageRank computation based on iterative aggregation-disaggregation methods
#@ Yangbo Zhu;Shaozhi Ye;Xing Li
#t 2005
#c 1
#% 66165
#% 193162
#% 224113
#% 290830
#% 472202
#% 475253
#% 577328
#% 754088
#% 754117
#% 769460
#% 769506
#% 853937
#% 1016164
#! PageRank has been widely used as a major factor in search engine ranking systems. However, global link graph information is required when computing PageRank, which causes prohibitive communication cost to achieve accurate results in distributed solution. In this paper, we propose a distributed PageRank computation algorithm based on iterative aggregation-disaggregation (IAD) method with Block Jacobi smoothing. The basic idea is divide-and-conquer. We treat each web site as a node to explore the block structure of hyperlinks. Local PageRank is computed by each node itself and then updated with a low communication cost with a coordinator. We prove the global convergence of the Block Jacobi method and then analyze the communication overhead and major advantages of our algorithm. Experiments on three real web graphs show that our method converges 5-7 times faster than the traditional Power method. We believe our work provides an efficient and practical distributed solution for PageRank on large scale Web graphs.

#index 838517
#* Scalable summary based retrieval in P2P networks
#@ Wolfgang Müller;Martin Eisenhardt;Andreas Henrich
#t 2005
#c 1
#% 172898
#% 194246
#% 340175
#% 340176
#% 496157
#% 577357
#% 593985
#% 636116
#% 741396
#% 963596
#% 1015281
#! Much of the present P2P-IR literature is focused on distributed indexing structures. Within this paper, we present an approach based on the replication of peer data summaries via rumor spreading and multicast in a structured overlay.We will describe Rumorama, a P2P framework for similar-ity queries inspired by GlOSS and CORI and their P2P-adaptation, PlanetP. Rumorama achieves a hierarchization of PlanetP-like summary-based P2P-IR networks. In a Rumorama network, each peer views the network as a small PlanetP network with connections to peers that see other small PlanetP networks. One important aspect is that each peer can choose the size of the PlanetP network it wants to see according to its local processing power and bandwidth. Even in this adaptive environment, Rumorama manages to process a query such that the summary of each peer is considered exactly once in a network without churn. However, the actual number of peers to be contacted for a query is a small fraction of the total number of peers in the network.Within this article, we present the Rumorama base protocol, as well as experiments demonstrating the scalability and viability of the approach under churn.

#index 838518
#* Compact reachability labeling for graph-structured data
#@ Hao He;Haixun Wang;Jun Yang;Philip S. Yu
#t 2005
#c 1
#% 51391
#% 58365
#% 148890
#% 333981
#% 379481
#% 379482
#% 397360
#% 397375
#% 577358
#% 598374
#% 654452
#% 659999
#% 772025
#% 993970
#% 1704059
#! Testing reachability between nodes in a graph is a well-known problem with many important applications, including knowledge representation, program analysis, and more recently, biological and ontology databases inferencing as well as XML query processing. Various approaches have been proposed to encode graph reachability information using node labeling schemes, but most existing schemes only work well for specific types of graphs. In this paper, we propose a novel approach, HLSS(Hierarchical Labeling of Sub-Structures), which identifies different types of substructures within a graph and encodes them using techniques suitable to the characteristics of each of them. We implement HLSS with an efficient two-phase algorithm, where the first phase identifies and encodes strongly connected components as well as tree substructures, and the second phase encodes the remaining reachability relationships by compressing dense rectangular submatrices in the transitive closure matrix. For the important subproblem of finding densest submatrices, we demonstrate the hardness of the problem and propose several practical algorithms. Experiments show that HLSS handles different types of graphs well, while existing approaches fall prey to graphs with substructures they are not designed to handle.

#index 838519
#* A formal characterization of PIVOT/UNPIVOT
#@ Catharine M. Wyss;Edward L. Robertson
#t 2005
#c 1
#% 342957
#% 386623
#% 408396
#% 481944
#% 654445
#% 814652
#% 1016212
#! PIVOT is an important relational operation that allows data in rows to be exchanged for columns. Although most current relational database management systems support PIVOT-type operations, to date a purely formal, algebraic characterization of PIVOT has been lacking. In this paper, we present a characterization in terms of extended relational algebra operators τ (transpose), Π (drop projection), and μ (unique optimal tuple merge). This enables us to (1) draw parallels with PIVOT and existing operators employed in Dynamic Data Mapping Systems (DDMS), (2) formally characterize invertible PIVOT instances, and (3) provide complexity results for PIVOT-type operations. These contributions are an important part of ongoing work on formal models for relational OLAP.

#index 838520
#* A novel approach for privacy-preserving video sharing
#@ Jianping Fan;Hangzai Luo;Mohand-Said Hacid;Elisa Bertino
#t 2005
#c 1
#% 300184
#% 311027
#% 319667
#% 333876
#% 345829
#% 446167
#% 727929
#% 780693
#% 789014
#% 800557
#% 1855648
#! To support privacy-preserving video sharing, we have proposed a novel framework that is able to protect the video content privacy at the individual video clip level and prevent statistical inferences from video collections. To protect the video content privacy at the individual video clip level, we have developed an effective algorithm to automatically detect privacy-sensitive video objects and video events. To prevent the statistical inferences from video collections, we have developed a distributed framework for privacy-preserving classifier training, which is able to significantly reduce the costs of data transmission and reliably limit the privacy breaches by determining the optimal size of blurred test samples for classifier validation. Our experiments on a specific domain of patient training and counseling videos show convincing results.

#index 838521
#* Determining the semantic orientation of terms through gloss classification
#@ Andrea Esuli;Fabrizio Sebastiani
#t 2005
#c 1
#% 465895
#% 577246
#% 577355
#% 722308
#% 723399
#% 746885
#% 755835
#% 779081
#% 815915
#% 854646
#% 855093
#% 855282
#% 938687
#% 939848
#% 1250238
#! Sentiment classification is a recent subdiscipline of text classification which is concerned not with the topic a document is about, but with the opinion it expresses. It has a rich set of applications, ranging from tracking users' opinions about products or about political candidates as expressed in online forums, to customer relationship management. Functional to the extraction of opinions from text is the determination of the orientation of ``subjective'' terms contained in text, i.e. the determination of whether a term that carries opinionated content has a positive or a negative connotation. In this paper we present a new method for determining the orientation of subjective terms. The method is based on the quantitative analysis of the glosses of such terms, i.e. the definitions that these terms are given in on-line dictionaries, and on the use of the resulting term representations for semi-supervised term classification. The method we present outperforms all known methods when tested on the recognized standard benchmarks for this task.

#index 838522
#* Using appraisal groups for sentiment analysis
#@ Casey Whitelaw;Navendu Garg;Shlomo Argamon
#t 2005
#c 1
#% 89589
#% 269218
#% 290482
#% 309208
#% 577355
#% 723399
#% 740411
#% 742368
#% 746885
#% 755835
#% 815915
#% 854646
#% 854664
#% 938687
#% 1250238
#! Little work to date in sentiment analysis (classifying texts by `positive' or `negative' orientation) has attempted to use fine-grained semantic distinctions in features used for classification. We present a new method for sentiment classification based on extracting and analyzing appraisal groups such as ``very good'' or ``not terribly funny''. An appraisal group is represented as a set of attribute values in several task-independent semantic taxonomies, based on Appraisal Theory. Semi-automated methods were used to build a lexicon of appraising adjectives and their modifiers. We classify movie reviews using features based upon these taxonomies combined with standard ``bag-of-words'' features, and report state-of-the-art accuracy of 90.2%. In addition, we find that some types of appraisal appear to be more significant for sentiment classification than others.

#index 838523
#* Effects of web document evolution on genre classification
#@ Elizabeth Sugar Boese;Adele E. Howe
#t 2005
#c 1
#% 266215
#% 290482
#% 311027
#% 480136
#% 657193
#% 718194
#% 742368
#% 757403
#! The World Wide Web is a massive corpus that constantly evolves. Classification experiments usually grab a snapshot (temporally and spatially) of the Web for a corpus. In this paper, we examine the effects of page evolution on genre classification of Web pages. Web genre refers to the type of the page characterized by features such as style, form or presentation layout, and meta-content; Web genre can be used to tune spider crawling re-visits and inform relevance judgments for search engines. We found that pages in some genres change rarely if at all and can be used in present-day research experiments without requiring an updated version. We show that an old corpus can be used for training when testing on new Web pages, with only a marginal drop in accuracy rates on genre classification. We also show that features found to be useful in one corpus do not transfer well to other corpora with different genres.

#index 838524
#* Query workload-aware overlay construction using histograms
#@ Georgia Koloniari;Yannis Petrakis;Evaggelia Pitoura;Thodoris Tsotsos
#t 2005
#c 1
#% 1331
#% 35764
#% 210190
#% 340176
#% 449870
#% 496290
#% 610849
#% 636008
#% 643013
#% 646221
#% 723445
#% 745498
#% 770901
#% 772021
#% 963875
#% 1015256
#% 1711093
#! Peer-to-peer(p2p) systems over an efficient means of data sharing among a dynamically changing set of a large number of a tonomous nodes.Each node in a p2p system is connected with a small number of other nodes thus creating an overlay network of nodes. A query posed at a node is routed through the overlay network towards nodes hosting data items that satisfy it. In this paper, we consider building overlays that exploit the query workload so that nodes are clustered based on their results to a given query workload. The motivation is to create overlays where nodes that match a large number of similar queries are a fewlinks apart. Query frequency is also taken into account so that popular queries have a greater effect on the formation of the overlay than unpopular ones. We focus on range selection queries and se histograms to estimate the query results of each node. Then, nodes are clustered based on the similarity of their histograms. To this end,we introd ce a workload-aware edit distance metric between histograms that takes into account the query workload. Our experimental results show that workload-aware overlays increase the percentage of query results returned for a given number of nodes visited as compared to both random (i.e., unclustered)overlays and non workload-aware clustered overlays (i.e., overlays that cluster nodes based solely on the nodes' content).

#index 838525
#* Optimizing candidate check costs for bitmap indices
#@ Doron Rotem;Kurt Stockinger;Kesheng Wu
#t 2005
#c 1
#% 223781
#% 227861
#% 273904
#% 299982
#% 300185
#% 316523
#% 378404
#% 462217
#% 466953
#% 479808
#% 480329
#% 644560
#% 1016131
#! In this paper, we propose a new strategy for optimizing the placement of bin boundaries to minimize the cost of query evaluation using bitmap indices with binning. For attributes with a large number of distinct values, often the most efficient index scheme is a bitmap index with binning. However, this type of index may not be able to fully resolve some user queries. To fully resolve these queries, one has to access parts of the original data to check whether certain candidate records actually satisfy the specified conditions. We call this procedure the candidate check, which usually dominates the total query processing time. Given a set of user queries, we seek to minimize the total time required to an-swer the queries by optimally placing the bin boundaries. We show that our dynamic programming based algorithm can efficiently determine the bin boundaries. We verify our analysis with some real user queries from the Sloan Digital Sky Survey. For queries that require significant amount of time to perform candidate check, using our optimal bin boundaries reduces the candidate check time by a factor of 2 and the total query processing time by 40%.

#index 838526
#* Towards estimating the number of distinct value combinations for a set of attributes
#@ Xiaohui Yu;Calisto Zuzarte;Kenneth C. Sevcik
#t 2005
#c 1
#% 58348
#% 99463
#% 210190
#% 227883
#% 277347
#% 299989
#% 479648
#% 480805
#% 481749
#% 824682
#! Accurately and efficiently estimating the number of distinct values for some attribute(s) or sets of attributes in a data set is of critical importance to many database operations, such as query optimization and approximation query answering. Previous work has focused on the estimation of the number of distinct values for a single attribute and most existing work adopts a data sampling approach. This paper addresses the equally important issue of estimating the number of distinct value combinations for multiple attributes which we call COLSCARD (for COLumn Set CARDinality). It also takes a different approach that uses existing statistical information (e.g., histograms) available on the individual attributes to assist estimation. We start with cases where exact frequency information on individual attributes is available, and present a pair of lower and upper bounds on COLSCARD that are consistent with the available information, as well as an estimator of COLSCARD based on probability. We then proceed to study the case where only partial information (in the form of histograms) is available on individual attributes, and show how the proposed estimator can be adapted to this case. We consider two types of widely used histograms and show how they can be constructed in order to obtain optimal approximation. An experimental evaluation of the proposed estimation method on synthetic as well as two real data sets is provided.

#index 838527
#* A geometric interpretation and analysis of R-precision
#@ Javed A. Aslam;Emine Yilmaz
#t 2005
#c 1
#% 57485
#% 248074
#% 262276
#% 305866
#% 309093
#% 387427
#% 719991
#% 818277
#! Average precision and R-precision are two of the most commonly cited measures of overall retrieval performance, but their correlation, though well-known, has defied explanation. We recently devised a geometric interpretation of R-precision which suggests that under a reasonable set of assumptions, R-precision approximates the area under the precision-recall curve, as does average precision, thus explaining their correlation. In this paper, we consider these assumptions and our geometric interpretation of R-precision in order to further understand, and make reasonable use of, the information that R-precision provides. Given our geometric interpretation of R-precision, we show that R-precision is highly informative by demonstrating that it can be used to (1) accurately infer precision-recall curves, (2) accurately infer other measures of retrieval performance, and (3) devise new measures of retrieval performance. Through our analysis, we also state the conditions under which R-precision is informative.

#index 838528
#* Regularizing ad hoc retrieval scores
#@ Fernando Diaz
#t 2005
#c 1
#% 54413
#% 54457
#% 65946
#% 65947
#% 86371
#% 109206
#% 144034
#% 153019
#% 169781
#% 280856
#% 719598
#% 765552
#% 766430
#% 766431
#% 789959
#% 818241
#% 818254
#% 827581
#% 842682
#! The cluster hypothesis states: closely related documents tend to be relevant to the same request. We exploit this hypothesis directly by adjusting ad hoc retrieval scores from an initial retrieval so that topically related documents receive similar scores. We refer to this process as score regularization. Score regularization can be presented as an optimization problem, allowing the use of results from semi-supervised learning. We demonstrate that regularized scores consistently and significantly rank documents better than unregularized scores, given a variety of initial retrieval algorithms. We evaluate our method on two large corpora across a substantial number of topics.

#index 838529
#* Incremental test collections
#@ Ben Carterette;James Allan
#t 2005
#c 1
#% 208931
#% 262097
#% 262102
#% 262105
#% 309093
#% 340890
#% 340892
#% 561315
#% 643026
#% 766409
#% 766410
#! Corpora and topics are readily available for information retrieval research. Relevance judgments, which are necessary for system evaluation, are expensive; the cost of obtaining them prohibits in-house evaluation of retrieval systems on new corpora or new topics. We present an algorithm for cheaply constructing sets of relevance judgments. Our method intelligently selects documents to be judged and decides when to stop in such a way that with very little work there can be a high degree of confidence in the result of the evaluation. We demonstrate the algorithm's effectiveness by showing that it produces small sets of relevance judgments that reliably discriminate between two systems. The algorithm can be used to incrementally design retrieval systems by simultaneously comparing sets of systems. The number of additional judgments needed after each incremental design change decreases at a rate reciprocal to the number of systems being compared. To demonstrate the effectiveness of our method, we evaluate TREC ad hoc submissions, showing that with 95% fewer relevance judgments we can reach a Kendall's tau rank correlation of at least 0.9.

#index 838530
#* Query expansion using term relationships in language models for information retrieval
#@ Jing Bai;Dawei Song;Peter Bruza;Jian-Yun Nie;Guihong Cao
#t 2005
#c 1
#% 144029
#% 218978
#% 229348
#% 262096
#% 280851
#% 340895
#% 340899
#% 340901
#% 340948
#% 342707
#% 413579
#% 572500
#% 719598
#% 766428
#% 818240
#! Language Modeling (LM) has been successfully applied to Information Retrieval (IR). However, most of the existing LM approaches only rely on term occurrences in documents, queries and document collections. In traditional unigram based models, terms (or words) are usually considered to be independent. In some recent studies, dependence models have been proposed to incorporate term relationships into LM, so that links can be created between words in the same sentence, and term relationships (e.g. synonymy) can be used to expand the document model. In this study, we further extend this family of dependence models in the following two ways: (1) Term relationships are used to expand query model instead of document model, so that query expansion process can be naturally implemented; (2) We exploit more sophisticated inferential relationships extracted with Information Flow (IF). Information flow relationships are not simply pairwise term relationships as those used in previous studies, but are between a set of terms and another term. They allow for context-dependent query expansion. Our experiments conducted on TREC collections show that we can obtain large and significant improvements with our approach. This study shows that LM is an appropriate framework to implement effective query expansion.

#index 838531
#* Concept-based interactive query expansion
#@ Bruno M. Fonseca;Paulo Golgher;Bruno Pôssas;Berthier Ribeiro-Neto;Nivio Ziviani
#t 2005
#c 1
#% 152934
#% 218992
#% 232713
#% 280840
#% 281174
#% 330617
#% 348155
#% 387427
#% 420135
#% 447947
#% 591792
#% 642985
#% 643033
#% 643069
#% 754125
#% 1603005
#! Despite the recent advances in search quality, the fast increase in the size of the Web collection has introduced new challenges for Web ranking algorithms. In fact, there are still many situations in which the users are presented with imprecise or very poor results. One of the key difficulties is the fact that users usually submit very short and ambiguous queries, and they do not fully specify their information needs. That is, it is necessary to improve the query formation process if better answers are to be provided. In this work we propose a novel concept-based query expansion technique, which allows disambiguating queries submitted to search engines. The concepts are extracted by analyzing and locating cycles in a special type of query relations graph. This is a directed graph built from query relations mined using association rules. The concepts related to the current query are then shown to the user who selects the one concept that he interprets is most related to his query. This concept is used to expand the original query and the expanded query is processed instead. Using a Web test collection, we show that our approach leads to gains in average precision figures of roughly 32%. Further, if the user also provides information on the type of relation between his query and the selected concept, the gains in average precision go up to roughly 52%.

#index 838532
#* Query expansion using random walk models
#@ Kevyn Collins-Thompson;Jamie Callan
#t 2005
#c 1
#% 54413
#% 169729
#% 216150
#% 218978
#% 252328
#% 262067
#% 262096
#% 268079
#% 273699
#% 288202
#% 288577
#% 290830
#% 300542
#% 340899
#% 375017
#% 430757
#% 448843
#% 766406
#% 766525
#% 770864
#% 827581
#! It has long been recognized that capturing term relationships is an important aspect of information retrieval. Even with large amounts of data, we usually only have significant evidence for a fraction of all potential term pairs. It is therefore important to consider whether multiple sources of evidence may be combined to predict term relations more accurately. This is particularly important when trying to predict the probability of relevance of a set of terms given a query, which may involve both lexical and semantic relations between the terms.We describe a Markov chain framework that combines multiple sources of knowledge on term associations. The stationary distribution of the model is used to obtain probability estimates that a potential expansion term reflects aspects of the original query. We use this model for query expansion and evaluate the effectiveness of the model by examining the accuracy and robustness of the expansion methods, and investigate the relative effectiveness of various sources of term evidence. Statistically significant differences in accuracy were observed depending on the weighting of evidence in the random walk. For example, using co-occurrence data later in the walk was generally better than using it early, suggesting further improvements in effectiveness may be possible by learning walk behaviors.

#index 838533
#* Semantic querying of tree-structured data sources using partially specified tree patterns
#@ Dimitri Theodoratos;Theodore Dalamagas;Antonis Koufopoulos;Narain Gehani
#t 2005
#c 1
#% 229827
#% 291299
#% 300143
#% 397379
#% 458861
#% 464724
#% 479465
#% 480646
#% 480822
#% 839148
#% 1015258
#% 1015274
#% 1016135
#% 1698667
#! Nowadays, huge volumes of data are organized or exported in a tree-structured form. Querying capabilities are provided through queries that are based on branching path expression. Even for a single knowledge domain structural differences raise difficulties for querying data sources in a uniform way. In this paper, we present a method for semantically querying tree-structured data sources using partially specified tree patterns. Based on dimensions which are sets of semantically related nodes in tree structures, we define dimension graphs. Dimension graphs can be automatically extracted from trees and abstract their structural information. They are semantically rich constructs that support the formulation of queries and their efficient evaluation. We design a tree-pattern query language to query multiple tree-structured data sources. A central feature of this language is that the structure can be specified fully, partially, or not at all in the queries. Therefore, it can be used to query multiple trees with structural differences. %and We study the derivation of structural expressions in queries by introducing a set of inference rules for structural expressions. We define two types of query unsatisfiability and we provide necessary and sufficient conditions for checking each of them. Our approach is validated through experimental evaluation.

#index 838534
#* Selectivity-based partitioning: a divide-and-union paradigm for effective query optimization
#@ Neoklis Polyzotis
#t 2005
#c 1
#% 554
#% 32889
#% 36117
#% 86949
#% 115661
#% 116043
#% 248793
#% 397354
#% 411554
#% 464700
#% 479920
#% 479938
#% 479984
#% 480306
#% 480419
#% 481266
#% 481915
#% 572311
#% 765435
#% 1016208
#! Modern query optimizers select an efficient join ordering for a physical execution plan based essentially on the average join selectivity factors among the referenced tables. In this paper, we argue that this "monolithic" approach can miss important opportunities for the effective optimization of relational queries. We propose selectivity-based partitioning, a novel optimization paradigm that takes into account the join correlations among relation fragments in order to essentially enable multiple (and more effective) join orders for the evaluation of a single query. In a nutshell, the basic idea is to carefully partition a relation according to the selectivities of the join operations, and subsequently rewrite the query as a union of constituent queries over the computed partitions. We provide a formal definition of the related optimization problem and derive properties that characterize the set of optimal solutions. Based on our analysis, we develop a heuristic algorithm for computing efficiently an effective partitioning of the input query. Results from a preliminary experimental study verify the effectiveness of the proposed approach and demonstrate its potential as an effective optimization technique.

#index 838535
#* Efficient evaluation of parameterized pattern queries
#@ Cédric du Mouza;Philippe Rigaux;Michel Scholl
#t 2005
#c 1
#% 10117
#% 44933
#% 179696
#% 281777
#% 320454
#% 333850
#% 346515
#% 413576
#% 463903
#% 464058
#% 484042
#% 503878
#% 534183
#% 763881
#% 765164
#% 769466
#% 769899
#% 839699
#% 1016170
#! Many applications rely on sequence databases and use extensively pattern-matching queries to retrieve data of interest. This paper extends the traditional pattern-matching expressions to parameterized patterns, featuring variables. Parameterized patterns are more expressive and allow to define concisely regular expressions that would be very complex to describe without variables. They can also be used to express additional constraints on patterns' variables.We show that they can be evaluated without additional cost with respect to traditional techniques (e.g., the Knuth-Morris-Pratt algorithm). We describe an algorithm that enjoys low memory and CPU time requirements, and provide experimental results which illustrate the gain of the optimized solution.

#index 838536
#* Redundant documents and search effectiveness
#@ Yaniv Bernstein;Justin Zobel
#t 2005
#c 1
#% 201935
#% 218992
#% 255137
#% 290703
#% 300176
#% 309093
#% 345087
#% 375017
#% 397133
#% 397163
#% 504572
#% 571725
#% 642975
#% 643014
#% 654447
#% 728115
#% 747116
#% 818222
#! The web contains a great many documents that are content-equivalent, that is, informationally redundant with respect to each other. The presence of such mutually redundant documents in search results can degrade the user search experience. Previous attempts to address this issue, most notably the TREC novelty track, were characterized by difficulties with accuracy and evaluation. In this paper we explore syntactic techniques --- particularly document fingerprinting --- for detecting content equivalence. Using these techniques on the TREC GOV1 and GOV2 corpora revealed a high degree of redundancy; a user study confirmed that our metrics were accurately identifying content-equivalence. We show, moreover, that content-equivalent documents have a significant effect on the search experience: we found that 16.6% of all relevant documents in runs submitted to the TREC 2004 terabyte track were redundant.

#index 838537
#* Novelty detection based on sentence level patterns
#@ Xiaoyan Li;W. Bruce Croft
#t 2005
#c 1
#% 262042
#% 262043
#% 262112
#% 278107
#% 316546
#% 397133
#% 577297
#% 642975
#% 643014
#% 643016
#% 643031
#% 815098
#% 815107
#! The detection of new information in a document stream is an important component of many potential applications. In this paper, a new novelty detection approach based on the identification of sentence level patterns is proposed. Given a user's information need, some patterns in sentences such as combinations of query words, named entities and phrases, may contain more important and relevant information than single words. Therefore, the proposed novelty detection approach focuses on the identification of previously unseen query-related patterns in sentences. Specifically, a query is preprocessed and represented with patterns that include both query words and required answer types. These patterns are used to retrieve sentences, which are then determined to be novel if it is likely that a new answer is present. An analysis of patterns in sentences was performed with data from the TREC 2002 novelty track and experiments on novelty detection were carried out on data from the TREC 2003 and 2004 novelty tracks. The experimental results show that the proposed pattern-based approach significantly outperforms all three baselines in terms of precision at top ranks.

#index 838538
#* Minimal document set retrieval
#@ Wei Dai;Rohini Srihari
#t 2005
#c 1
#% 232650
#% 262045
#% 262112
#% 281186
#% 397133
#% 642975
#% 766433
#% 769968
#! This paper presents a novel formulation and approach to the minimal document set retrieval problem. Minimal Document Set Retrieval (MDSR) is a promising information retrieval task in which each query topic is assumed to have different subtopics; the task is to retrieve and rank relevant document sets with maximum coverage but minimum redundancy of subtopics in each set. For this task, we propose three document set retrieval and ranking algorithms: Novelty Based method, Cluster Based method and Subtopic Extraction Based method. In order to evaluate the system performance, we design a new evaluation framework for document set ranking which evaluates both relevance between set and query topic, and redundancy within each set. Finally, we compare the performance of the three algorithms using the TREC interactive track dataset. Experimental results show the effectiveness of our algorithms.

#index 838539
#* A model for weighting image objects in home photographs
#@ Jean Martinet;Yves Chiaramella;Philippe Mulhem
#t 2005
#c 1
#% 169940
#% 232703
#% 316199
#% 318785
#% 345089
#% 345848
#% 394790
#% 428926
#% 563749
#% 626558
#% 642989
#% 766452
#% 789897
#% 840583
#% 1112584
#% 1855625
#! The paper presents a contribution to image indexing consisting in a weighting model for visible objects -- or image objects -- in home photographs. To improve its effectiveness this weighting model has been designed according to human perception criteria about what is estimated as important in photographs. Four basic hypotheses related to human perception are presented, and their validity is estimated as compared to actual observations from a user study. Finally a formal definition of this weighting model is presented and its consistence with the user study is evaluated.

#index 838540
#* Automatic construction of multifaceted browsing interfaces
#@ Wisam Dakka;Panagiotis G. Ipeirotis;Kenneth R. Wood
#t 2005
#c 1
#% 118771
#% 218992
#% 219052
#% 232683
#% 232926
#% 255137
#% 259633
#% 260001
#% 279755
#% 280849
#% 301263
#% 309131
#% 316527
#% 340951
#% 410276
#% 420487
#% 425007
#% 452641
#% 642989
#% 643068
#% 766433
#% 1264972
#% 1711122
#! Databases of text and text-annotated data constitute a significant fraction of the information available in electronic form. Searching and browsing are the typical ways that users locate items of interest in such databases. Interfaces that use multifaceted hierarchies represent a new powerful browsing paradigm which has been proven to be a successful complement to keyword searching. Thus far, multifaceted hierarchies have been created manually or semi-automatically, making it difficult to deploy multifaceted interfaces over a large number of databases. We present automatic and scalable methods for creation of multifaceted interfaces. Our methods are integrated with traditional relational databases and can scale well for large databases. Furthermore, we present methods for selecting the best portions of the generated hierarchies when the screen space is not sufficient for displaying all the hierarchy at once. We apply our technique to a range of large data sets, including annotated images, television programming schedules, and web pages. The results are promising and suggest directions for future research.

#index 838541
#* Fast on-line index construction by geometric partitioning
#@ Nicholas Lester;Alistair Moffat;Justin Zobel
#t 2005
#c 1
#% 57954
#% 77986
#% 86532
#% 116073
#% 169814
#% 172922
#% 188587
#% 290703
#% 323131
#% 397151
#% 479932
#% 481439
#% 565455
#% 655485
#% 747117
#% 786632
#% 838465
#! Inverted index structures are the mainstay of modern text retrieval systems. They can be constructed quickly using off-line merge-based methods, and provide efficient support for a variety of querying modes. In this paper we examine the task of on-line index construction -- that is, how to build an inverted index when the underlying data must be continuously queryable, and the documents must be indexed and available for search as soon they are inserted. When straightforward approaches are used, document insertions become increasingly expensive as the size of the database grows. This paper describes a mechanism based on controlled partitioning that can be adapted to suit different balances of insertion and querying operations, and is faster and scales better than previous methods. Using experiments on 100GB of web data we demonstrate the efficiency of our methods in practice, showing that they dramatically reduce the cost of on-line index construction.

#index 838542
#* Optimizing cursor movement in holistic twig joins
#@ Marcus Fontoura;Vanja Josifovski;Eugene Shekita;Beverly Yang
#t 2005
#c 1
#% 333981
#% 397375
#% 406493
#% 480489
#% 643566
#% 654442
#% 654450
#% 765406
#% 783546
#% 803121
#% 809253
#% 810046
#% 979743
#% 993953
#% 1015277
#! Holistic twig join algorithms represent the state of the art for evaluating path expressions in XML queries. Using inverted indexes on XML elements, holistic twig joins move a set of index cursors in a coordinated way to quickly find structural matches. Because each cursor move can trigger I/O, the performance of a holistic twig join is largely determined by how many cursor moves it makes, yet, surprisingly, existing join algorithms have not been optimized along these lines. In this paper, we describe TwigOptimal, a new holistic twig join algorithm with optimal cursor movement. We sketch the proof of TwigOptimal's optimality, and describe how TwigOptimal can use information in the return clause of XQuery to boost its performance. Finally, experimental results are presented, showing TwigOptimal's superiority over existing holistic twig join algorithms.

#index 838543
#* Consistent query answering under key and exclusion dependencies: algorithms and experiments
#@ Luca Grieco;Domenico Lembo;Riccardo Rosati;Marco Ruzzi
#t 2005
#c 1
#% 235018
#% 273687
#% 576116
#% 665856
#% 727668
#% 783532
#% 810020
#% 880394
#% 1279213
#% 1279214
#% 1700138
#% 1700140
#% 1705010
#! Research in consistent query answering studies the definition and computation of "meaningful" answers to queries posed to inconsistent databases, i.e., databases whose data do not satisfy the integrity constraints (ICs) declared on their schema. Computing consistent answers to conjunctive queries is generally coNP-hard in data complexity, even in the presence of very restricted forms of ICs (single, unary keys). Recent studies on consistent query answering for database schemas containing only key dependencies have analyzed the possibility of identifying classes of queries whose consistent answers can be obtained by a first-order rewriting of the query, which in turn can be easily formulated in SQL and directly evaluated through any relational DBMS. In this paper we study consistent query answering in the presence of key dependencies and exclusion dependencies. We first prove that even in the presence of only exclusion dependencies the problem is coNP-hard in data complexity, and define a general method for consistent answering of conjunctive queries under key and exclusion dependencies, based on the rewriting of the query in Datalog with negation. Then, we identify a subclass of conjunctive queries that can be first-order rewritten in the presence of key and exclusion dependencies, and define an algorithm for computing the first-order rewriting of a query belonging to such a class of queries. Finally, we compare the relative efficiency of the two methods for processing queries in the subclass above mentioned. Experimental results, conducted on a real and large database of the computer science engineering degrees of the University of Rome "La Sapienza", clearly show the computational advantage of the first-order based technique.

#index 838544
#* Balancing performance and confidentiality in air index
#@ Qingzhao Tan;Wang-Chien Lee;Baihua Zheng;Peng Liu;Dik Lun Lee
#t 2005
#c 1
#% 158911
#% 169835
#% 172876
#% 236779
#% 265208
#% 287258
#% 322884
#% 430426
#% 443127
#% 443224
#% 462771
#% 635819
#% 635901
#! Studies on the performance issues (i.e., access latency and energy conservation) of wireless data broadcast have appeared in the literature. However, the important security issues have not been well addressed. This paper investigates the tradeoff between performance and security of signature-based air index schemes in wireless data broadcast. From the performance perspective, keeping low false drop probability helps clients retrieve the information from a broadcast channel efficiently. Meanwhile, from the security perspective, achieving high false guess probability prevents the hacker from guessing the information easily. There is a tradeoff between these two aspects. An administrator of the wireless broadcast system may balance this tradeoff by carefully configuring the signatures used in broadcast. This study provides a guidance for parameter settings of the signature schemes in order to meet the performance and security requirements. Experiments are performed to validate the analytical results and to obtain optimal signature configuration corresponding to different application criteria.

#index 838545
#* Context modeling and discovery using vector space bases
#@ Massimo Melucci
#t 2005
#c 1
#% 18616
#% 46803
#% 55490
#% 116374
#% 169781
#% 321635
#% 448843
#% 758200
#! In this paper, context is modeled by vector space bases and its evolution is modeled by linear transformations from one base to another. Each document or query can be associated to a distinct base, which corresponds to one context. Also, algorithms are proposed to discover contexts from document, query or groups or them. Linear algebra can thus by employed in a mathematical framework to process context, its evolution and application.

#index 838546
#* Y!Q: contextual search at the point of inspiration
#@ Reiner Kraft;Farzin Maghoul;Chi Chao Chang
#t 2005
#c 1
#% 272917
#% 278107
#% 287217
#% 289340
#% 296646
#% 309792
#% 342963
#% 577300
#% 644704
#% 790458
#% 1715617
#! Contextual search tries to better capture a user's information need by augmenting the user's query with contextual information extracted from the search context (for example, terms from the web page the user is currently reading or a file the user is currently editing).This paper presents Y!Q---a first of its kind large-scale contextual search system---and provides an overview of its system design and architecture. Y!Q solves two major problems. First, how to capture high quality search context. Second, how to use that context in a way to improve the relevancy of search queries. To address the first problem, Y!Q introduces an information widget that captures precise search context and provides convenient access to its functionality at the point of inspiration. For example, Y!Q can be easily embedded into web pages using a web API, or it can be integrated into a web browser toolbar. This paper provides an overview of Y!Q's user interaction design, highlighting its novel aspects for capturing high quality search context.To address the second problem, Y!Q uses a semantic network for analyzing search context, possibly resolving ambiguous terms, and generating a contextual digest comprising its key concepts. This digest is passed through a query planner and rewriting framework for augmenting a user's search query with relevant context terms to improve the overall search relevancy and experience. We show experimental results comparing contextual Y!Q search results side-by-side with regular Yahoo! web search results. This evaluation suggests that Y!Q results are considered significantly more relevant.The paper also identifies interesting research problems and argues that contextual search may represent the next major step in the evolution of web search engines.

#index 838547
#* Implicit user modeling for personalized search
#@ Xuehua Shen;Bin Tan;ChengXiang Zhai
#t 2005
#c 1
#% 218978
#% 262084
#% 281174
#% 284796
#% 308756
#% 320432
#% 340899
#% 340901
#% 406493
#% 577224
#% 577329
#% 643028
#% 731615
#% 754126
#% 766447
#% 807660
#% 818207
#! Information retrieval systems (e.g., web search engines) are critical for overcoming information overload. A major deficiency of existing retrieval systems is that they generally lack user modeling and are not adaptive to individual users, resulting in inherently non-optimal retrieval performance. For example, a tourist and a programmer may use the same word "java" to search for different information, but the current search systems would return the same results. In this paper, we study how to infer a user's interest from the user's search context and use the inferred implicit user model for personalized search. We present a decision theoretic framework and develop techniques for implicit user modeling in information retrieval. We develop an intelligent client-side web search agent (UCAIR) that can perform eager implicit feedback, e.g., query expansion based on previous queries and immediate result reranking based on clickthrough information. Experiments on web search show that our search agent can improve search accuracy over the popular Google search engine.

#index 907482
#* Pair-Wise entity resolution: overview and challenges
#@ Hector Garcia-Molina
#t 2006
#c 1
#! Information integration is one of the oldest and most important computer science problems: Information from diverse sources must be combined, so that users can access and manipulate the information in a unified way. One of the central problems in information integration is that of Entity Resolution (ER) (sometimes referred to as deduplication). ER is the process of identifying and merging incoming records judged to represent the same real-world entity.For example, consider a company that has different customer databases (e.g., one for each subsidiary), and would like to integrate them. Identifying matching records is challenging because there are no unique identifiers across the different sources or databases. A given customer may appear in different ways in each database, and there is a fair amount of guesswork in determining which customers match. Deciding if records match is often computationally expensive, e.g., may involve finding maximal common subsequences in two strings. How to combine matching records is often also application dependent. For example, say different phone numbers appear in two records to be merged. In some cases we may wish to keep both of them, while in others we may want to pick just one as the "consolidated" number.Another source of complexity is that newly merged records may match with other records. For instance, when we combine records r1 and r2 we may obtain a record r12 that now matches r3. The original records, r1 and r2, may not match with r3, but because r12 contains more information about the same real-word entity that r1 and r2 represent, the "connection" to r3 may now be apparent. Such "chained" matches imply that new merged records must be recursively compared to all records.There are many ways to perform ER, but in this talk I will explore only one general approach, where the decision of what records represent the same real-world entity is done in a pair-wise fashion. Furthermore, we assume that the matching is done by a "black-box" function, which makes our approach generic and applicable to many domains. Thus, given two records, r1 and r2, the match function M(r1, r2) returns true if there is enough evidence in the two records that they both refer to the same real-world entity. We also assume a black-box merge function that combines a pair of matching records.In this talk I will discuss the advantages and disadvantages of such a generic, pair-wise approach to ER. And even though the approach is relatively simple, there are still many interesting challenges. For instance, how can one minimize the number of invocations to the match and merge black-boxes? Are there any properties of the functions that can significantly reduce the number of calls? If one has available multiple processors, how can one distribute the computational load? If records have confidences associated with them, how does the problem complexity change, and how can we efficiently find the confidence of the resolved records? In the talk I will address these challenges, and report on some preliminary work we have done at Stanford. (This Stanford work in joint with Omar Benjelloun, Tyson Condie, Johnson (Heng) Gong, Jeff Jonas, Hideki Kawai, Tait E. Larson, David Menestrina, Nicolas Pombourcq, Qi Su, Steven Whang, Jennifer Widom.For additional information on ER and our Stanford SERF Project, please visit http://www-db.stanford.edu/serf/.

#index 907483
#* How I learned to stop worrying and love the imminent internet singularity
#@ Gary William Flake
#t 2006
#c 1
#! In 1993, Verner Vinge [3] introduced the notion of the Singularity -- a step function to nearly unlimited technological capability -- which would be realized if the acceleration of scientific progress continues to produce such things as strong AI, nanotechnology, and super-human intelligence. Since its introduction, the idea of the Singularity has been met with both evangelism (by Ray Kurzweil [2]) and apocalyptic warnings (by Bill Joy [1]).In this talk, I will introduce a more modest version of the idea, which I call the Internet Singularity. Like the original, the Internet Singularity suggests continued acceleration of progress, but makes greater emphasis on our ability to improve science, analytic methods, and engineering on data as opposed to the physical world. I make the case for the Internet Singularity in four steps.First, there is a general trend of more capabilities being more available to more people. These increasing capabilities span content creation, community, and commerce, yielding more power to today's "amateur" than yesterday's "professional". As a result, the boundary between producers and consumers is becoming increasingly blurred over time.Second, in many parts of the Internet we see power law distributions with a heavy tail. One implication of heavy tail distributions is that the aggregate impact of "small" participants can be greater than that of the "large" participants.Third, with the Internet comes entirely new means for authoring new and derivative works: aggregations, mashups, tagging, remixing, etc. The greater emphasis on collaboration and sharing yields direct and indirect network effects. Network effects, can produce entirely new utility, making online activities potentially more efficient or valuable than the offline equivalent.Fourth, on the Internet, advances are effectively decoupled from the physical constraints of the offline world: startups costs are smaller; customer, collaborator, and audience pools are dramatically larger; and improvements happen in more of a continuous rather than discreet manner. As a result the effective "clock cycle" of progress is potentially much faster online.Putting these four pieces together reveals a compelling pattern: more people contribute to the collective pool; the collective pool contains entirely new value that is derived from its data; and the new value from the data increases individual and aggregate capabilities. In combination, these components mutually reinforce one another, forming something of a virtuous cycle. This is the Internet Singularity.Conceptually, if we consider engineering to be the ability to create artifacts, mathematical analysis to be the ability to analyze numerical properties, and science to be the pursuit of knowledge, then each of these activities -- when focused on digital objects as they exist on the Internet -- can be amplified in a manner consistent with the Internet Singularity.The implications for the Internet Singularity are profound as they suggest nothing less than the evolution of the scientific method itself. Moreover, these trends also imply that now may be the best possible moment in the history of the universe to be a computer scientist.

#index 907484
#* The real-time nature and value of homeland security information
#@ Joseph Kielman
#t 2006
#c 1
#! Ensuring the security of our homeland depends in large measure on two quite distinct factors: having the knowledge necessary to prevent, predict, prepare for, or respond, if necessary, to any manner of terrorist attack or a natural or manmade disaster and collaborating or sharing knowledge with a broad range of international, federal, state, local, and tribal agencies, as well as other private or public organizations. The essential problem with adequately addressing these factors, despite the many advancements made in the past decade, is twofold. First, it is not so much the mass but rather the diffuse nature and complexity of the data, information, and knowledge required for understanding terrorism and accounting for the manifold consequences of disasters that make possession of the right knowledge difficult. And, second, that diffuseness and complexity is magnified by the extreme diversity and wide distribution of the many potential homeland security collaborators. Retrospective analysis, and even knowledge discovery, is less useful under these conditions than prospective, real-time synthesis of information for multiple users. Also, privacy is as important as, if not more important than, security. This suggests that database designs and techniques for information retrieval, and knowledge management must take advantage of such technologies as semantic nets, visualization, and discrete mathematics to build knowledge systems capable for homeland security applications.

#index 907485
#* Efficient processing of complex similarity queries in RDBMS through query rewriting
#@ Caetano Traina, Jr.;Agma J. M. Traina;Marcos R. Vieira;Adriano S. Arantes;Christos Faloutsos
#t 2006
#c 1
#% 77648
#% 172931
#% 201876
#% 210172
#% 227856
#% 248797
#% 287461
#% 287466
#% 397378
#% 411554
#% 443482
#% 458742
#% 479462
#% 480652
#% 481947
#% 575361
#% 626971
#% 632009
#% 654471
#% 765150
#% 772438
#! Multimedia and complex data are usually queried by similarity predicates. Whereas there are many works dealing with algorithms to answer basic similarity predicates, there are not generic algorithms able to efficiently handle similarity complex queries combining several basic similarity predicates. In this work we propose a simple and effective set of algorithms that can be combined to answer complex similarity queries, and a set of algebraic rules useful to rewrite similarity query expressions into an adequate format for those algorithms. Those rules and algorithms allow relational database management systems to turn complex queries into efficient query execution plans. We present experiments that highlight interesting scenarios. They show that the proposed algorithms are orders of magnitude faster than the traditional similarity algorithms. Moreover, they are linearly scalable considering the database size.

#index 907486
#* Distributed spatio-temporal similarity search
#@ Demetrios Zeinalipour-Yazti;Song Lin;Dimitrios Gunopulos
#t 2006
#c 1
#% 140617
#% 213981
#% 273706
#% 282343
#% 300174
#% 326529
#% 333854
#% 421124
#% 443444
#% 477479
#% 480817
#% 654443
#% 659961
#% 659993
#% 729931
#% 731404
#% 765453
#% 768521
#% 800509
#% 800572
#% 805466
#% 812755
#% 822531
#% 824724
#% 824730
#% 1698862
#! In this paper we introduce the distributed spatio-temporal similarity search problem: given a query trajectory Q, we want to find the trajectories that follow a motion similar to Q, when each of the target trajectories is segmented across a number of distributed nodes. We propose two novel algorithms, UB-K and UBLB-K, which combine local computations of lower and upper bounds on the matching between the distributed subsequences and Q. Such an operation generates the desired result without pulling together all the distributed subsequences over the fundamentally expensive communication medium. Our solutions find applications in a wide array of domains, such as cellular networks, wild life monitoring and video surveillance. Our experimental evaluation using realistic data demonstrates that our framework is both efficient and robust to a variety of conditions.

#index 907487
#* Structure-based querying of proteins using wavelets
#@ Keith Marsolo;Srinivasan Parthasarathy;Kotagiri Ramamohanarao
#t 2006
#c 1
#% 66927
#% 92533
#% 321455
#% 717359
#% 825074
#% 830883
#% 844319
#% 853017
#% 1188997
#! The ability to retrieve molecules based on structural similarity has use in many applications, from disease diagnosis and treatment to drug discovery and design. In this paper, we present a method to represent protein molecules that allows for the fast, flexible and efficient retrieval of similar structures, based on either global or local attributes. We begin by computing the pair-wise distance between amino acids, transforming each 3D structure into a 2D distance matrix. We normalize this matrix to a specific size and apply a 2D wavelet decomposition to generate a set of approximation coefficients, which serves as our global feature vector. This transformation reduces the overall dimensionality of the data while still preserving spatial features and correlations. We test our method by running queries on three different protein data sets that have been used previously in the literature, basing our comparisons on labels taken from the SCOP database. We find that our method significantly outperforms existing approaches, in terms of retrieval accuracy, memory utilization and execution time. Specifically, using a k-d tree and running a 10-nearest-neighbor search on a dataset of 33,000 proteins against itself, we see an average accuracy of 89% at the SCOP SuperFamily level and a total query time that is up to 350 times faster than previously published techniques. In addition to processing queries based on global similarity, we also propose innovative extensions to effectively match proteins based solely on shared local substructures, allowing for a more flexible query interface.

#index 907488
#* An approximate multi-word matching algorithm for robust document retrieval
#@ Atsuhiro Takasu
#t 2006
#c 1
#% 2324
#% 23985
#% 58593
#% 121278
#% 131061
#% 169778
#% 193204
#% 204297
#% 251405
#% 262039
#% 309102
#% 333679
#% 387427
#% 411375
#% 432569
#% 438325
#% 539297
#% 546093
#% 564599
#% 614037
#% 627852
#% 879571
#! Document generation from low level data and its utilization is one of the most challenging tasks in document engineering. Word occurrence detection is a fundamental problem in the recognized document utilization obtained by a recognizer, such as OCR and speech recognition. Given a set of words, such as a dictionary, this paper proposes an efficient dynamic programming (DP) algorithm to find the occurrences of each word in a text. In this paper, the string similarity is measured by a statistical similarity model that enables a definition of the similarities in the character level as well as edit operation level. The proposed algorithm uses tree structures to measure similarities in order to avoid measuring similarities of the same substrings appearing in different parts of the text and words. The time complexity of the proposed algorithm is O(|W|⋅|S|⋅|Q|), where |W| (resp. |S|) denote the number of nodes in the trees representing the word set (resp. the text), and |Q| donotes the number of the states of the model used for string similarity. This paper shows the proposed algorithm is experimentally about six times faster than a naive DP algorithm.

#index 907489
#* Movie review mining and summarization
#@ Li Zhuang;Feng Jing;Xiao-Yan Zhu
#t 2006
#c 1
#% 529193
#% 577355
#% 722308
#% 746885
#% 769892
#% 791741
#% 805873
#% 815915
#% 854646
#% 855093
#% 855279
#% 855282
#% 938687
#% 939346
#% 939896
#% 1700552
#% 1708349
#! With the flourish of the Web, online review is becoming a more and more useful and important information resource for people. As a result, automatic review mining and summarization has become a hot research topic recently. Different from traditional text summarization, review mining and summarization aims at extracting the features on which the reviewers express their opinions and determining whether the opinions are positive or negative. In this paper, we focus on a specific domain - movie review. A multi-knowledge based approach is proposed, which integrates WordNet, statistical analysis and movie knowledge. The experimental results show the effectiveness of the proposed approach in movie review mining and summarization.

#index 907490
#* Utility scoring of product reviews
#@ Zhu Zhang;Balaji Varadarajan
#t 2006
#c 1
#% 406493
#% 529193
#% 577355
#% 747738
#% 755835
#% 805873
#% 815915
#% 843652
#% 854646
#% 854663
#% 855093
#% 855279
#% 855282
#% 926881
#% 938687
#% 939346
#% 939896
#% 939897
#% 939898
#% 939969
#% 1250238
#% 1269535
#% 1558464
#! We identify a new task in the ongoing research in text sentiment analysis: predicting utility of product reviews, which is orthogonal to polarity classification and opinion extraction. We build regression models by incorporating a diverse set of features, and achieve highly competitive performance for utility scoring on three real-world data sets.

#index 907491
#* Mining blog stories using community-based and temporal clustering
#@ Arun Qamra;Belle Tseng;Edward Y. Chang
#t 2006
#c 1
#% 118771
#% 281186
#% 577220
#% 577360
#% 722904
#% 729980
#% 766433
#% 769897
#% 788094
#% 794513
#% 823344
#% 823367
#% 823373
#% 832271
#% 868088
#% 868089
#! In recent years, weblogs, or blogs for short, have become an important form of online content. The personal nature of blogs, online interactions between bloggers, and the temporal nature of blog entries, differentiate blogs from other kinds of Web content. Bloggers interact with each other by linking to each other's posts, thus forming online communities. Within these communities, bloggers engage in discussions of certain issues, through entries in their blogs. Since these discussions are often initiated in response to online or offline events, a discussion typically lasts for a limited time duration. We wish to extract such temporal discussions, or stories, occurring within blogger communities, based on some query keywords. We propose a Content-Community-Time model that can leverage the content of entries, their timestamps, and the community structure of the blogs, to automatically discover stories. Doing so also allows us to discover hot stories. We demonstrate the effectiveness of our model through several case studies using real-world data collected from the blogosphere.

#index 907492
#* Eigen-trend: trend analysis in the blogosphere based on singular value decompositions
#@ Yun Chi;Belle L. Tseng;Junichi Tatemura
#t 2006
#c 1
#% 290830
#% 316143
#% 316150
#% 577360
#% 577370
#% 731406
#% 754058
#% 754107
#% 763997
#% 823342
#% 844312
#% 869516
#% 978374
#% 1704240
#! The blogosphere - the totality of blog-related Web sites - has become a great source of trend analysis in areas such as product survey, customer relationship, and marketing. Existing approaches are based on simple counts, such as the number of entries or the number of links. In this paper, we introduce a novel concept, coined eigen-trend, to represent the temporal trend in a group of blogs with common interests and propose two new techniques for extracting eigen-trends in blogs. First, we propose a trend analysis technique based on the singular value decomposition. Extracted eigen-trends provide new insights into multiple trends on the same keyword. Second, we propose another trend analysis technique based on a higher-order singular value decomposition. This analyzes the blogosphere as a dynamic graph structure and extracts eigen-trends that reflect the structural changes of the blogosphere over time. Experimental studies based on synthetic data sets and a real blog data set show that our new techniques can reveal a lot of interesting trend information and insights in the blogosphere that are not obtainable from traditional count-based methods.

#index 907493
#* On GMAP: and other transformations
#@ Stephen Robertson
#t 2006
#c 1
#% 309093
#% 397163
#% 879630
#% 879631
#! As an alternative to the usual Mean Average Precision, some use is currently being made of the Geometric Mean Average Precision (GMAP) as a measure of average search effectiveness across topics. GMAP is specifically used to emphasise the lower end of the average precision scale, in order to shed light on poor performance of search engines. This paper discusses the status of this measure and how it should be understood.

#index 907494
#* Investigating the exhaustivity dimension in content-oriented XML element retrieval evaluation
#@ Paul Ogilvie;Mounia Lalmas
#t 2006
#c 1
#% 57485
#% 144074
#% 236052
#% 397163
#% 411762
#% 766415
#% 783526
#% 818222
#% 1674720
#% 1721850
#! INEX, the evaluation initiative for content-oriented XML retrieval, has since its establishment defined the relevance of an element according to two graded dimensions, exhaustivity and specificity. The former measures how exhaustively an XML element discusses the topic of request, whereas specificity measures how focused the element is on the topic of request. The reason for having two dimensions was to provide a more stable measure of relevance than if assessors were asked to rate the relevance of an element on a single scale. However, obtaining relevance assessments is a costly task. as each document must be assessed for relevance by a human assessor. In XML retrieval this problem is exacerbated as the elements of the document must also be assessed with respect to the exhaustivity and specificity dimensions. A continuous discussion in INEX has been whether such a sophisticated definition of relevance, and in particular the exhaustivity dimension, was needed. This paper attempts to answer this question through extensive statistical tests to compare the conclusions about system performance that could be made under different assessment scenarios.

#index 907495
#* Evaluation by comparing result sets in context
#@ Paul Thomas;David Hawking
#t 2006
#c 1
#% 92689
#% 248065
#% 262107
#% 280793
#% 320432
#% 342680
#% 420508
#% 642983
#% 643559
#% 751830
#% 766409
#% 766454
#% 818206
#% 818221
#% 838547
#% 857180
#% 1016176
#! Familiar evaluation methodologies for information retrieval (IR) are not well suited to the task of comparing systems in many real settings. These systems and evaluation methods must support contextual, interactive retrieval over changing, heterogeneous data collections, including private and confidential information.We have implemented a comparison tool which can be inserted into the natural IR process. It provides a familiar search interface, presents a small number of result sets in side-by-side panels, elicits searcher judgments, and logs interaction events. The tool permits study of real information needs as they occur, uses the documents actually available at the time of the search, and records judgments taking into account the instantaneous needs of the searcher.We have validated our proposed evaluation approach and explored potential biases by comparing different whole-of-Web search facilities using a Web-based version of the tool. In four experiments, one with supplied queries in the laboratory and three with real queries in the workplace, subjects showed no discernable left-right bias and were able to reliably distinguish between high- and low-quality result sets. We found that judgments were strongly predicted by simple implicit measures.Following validation we undertook a case study comparing two leading whole-of-Web search engines. The approach is now being used in several ongoing investigations.

#index 907496
#* Estimating average precision with incomplete and imperfect judgments
#@ Emine Yilmaz;Javed A. Aslam
#t 2006
#c 1
#% 184486
#% 248065
#% 262034
#% 262097
#% 262102
#% 309093
#% 340890
#% 340892
#% 453327
#% 643020
#% 643029
#% 730072
#% 748738
#% 766409
#% 818205
#! We consider the problem of evaluating retrieval systems using incomplete judgment information. Buckley and Voorhees recently demonstrated that retrieval systems can be efficiently and effectively evaluated using incomplete judgments via the bpref measure [6]. When relevance judgments are complete, the value of bpref is an approximation to the value of average precision using complete judgments. However, when relevance judgments are incomplete, the value of bpref deviates from this value, though it continues to rank systems in a manner similar to average precision evaluated with a complete judgment set. In this work, we propose three evaluation measures that (1) are approximations to average precision even when the relevance judgments are incomplete and (2) are more robust to incomplete or imperfect relevance judgments than bpref. The proposed estimates of average precision are simple and accurate, and we demonstrate the utility of these estimates using TREC data.

#index 907497
#* Window join approximation over data streams with importance semantics
#@ Adegoke Ojewole;Qiang Zhu;Wen-Chi Hou
#t 2006
#c 1
#% 224218
#% 300179
#% 310488
#% 340635
#% 378388
#% 397352
#% 397353
#% 492932
#% 578391
#% 578560
#% 659972
#% 726621
#% 789000
#% 1015278
#% 1015296
#! Load shedding techniques generate approximate sliding window join results when memory constraints prevent exact computation. The previously proposed random load shedding method drops input tuples without consideration for the number of outputs created, while the recently proposed semantic load shedding technique aims to produce the largest possible result set. We consider a new model in which data stream tuples contain numerical importance values relevant to the query source and seek to maximize the importance of the approximate join result. We show that both random load shedding and semantic load shedding are sub-optimal in this situation, while the techniques presented in this paper satisfy the objective function by considering both tuple importance and join attribute distributions. We extend the existing offline semantic approximation technique to make it compatible with our objective function and show that it is less space and time efficient than our new optimal offline algorithm for small and large join memory allotments. We also introduce four efficient online algorithms, which are quite promising in maximizing the importance of the approximate join result without foreknowledge of input streams.

#index 907498
#* Adaptive non-linear clustering in data streams
#@ Ankur Jain;Zhihua Zhang;Edward Y. Chang
#t 2006
#c 1
#% 116149
#% 190581
#% 210173
#% 248792
#% 266426
#% 340903
#% 397387
#% 594012
#% 654489
#% 729980
#% 743284
#% 769927
#% 1015261
#% 1016200
#! Data stream clustering has emerged as a challenging and interesting problem over the past few years. Due to the evolving nature, and one-pass restriction imposed by the data stream model, traditional clustering algorithms are inapplicable for stream clustering. This problem becomes even more challenging when the data is high-dimensional and the clusters are not linearly separable in the input space. In this paper, we propose a nonlinear stream clustering algorithm that adapts to the stream's evolutionary changes. Using the kernel methods for dealing with the non-linearity of data separation, we propose a novel 2-tier stream clustering architecture. Tier-1 captures the temporal locality in the stream, by partitioning it into segments, using a kernel-based novelty detection approach. Tier-2 exploits this segment structure to continuously project the streaming data nonlinearly onto a low-dimensional space (LDS), before assigning them to a cluster. We demonstrate the effectiveness of our approach through extensive experimental evaluation on various real-world datasets.

#index 907499
#* Classification spanning correlated data streams
#@ Yabo Xu;Ke Wang;Ada Wai-Chee Fu;Rong She;Jian Pei
#t 2006
#c 1
#% 246831
#% 273682
#% 273908
#% 310500
#% 378388
#% 397354
#% 479787
#% 594012
#% 635215
#% 654444
#% 729932
#% 729965
#% 765494
#% 809257
#% 844359
#% 881938
#% 993949
#% 993961
#% 1015296
#% 1016156
#% 1250553
#! In many applications, classifiers need to be built based on multiple related data streams. For example, stock streams and news streams are related, where the classification patterns may involve features from both streams. Thus instead of mining on a single isolated stream, we need to examine multiple related data streams in order to find such patterns and build an accurate classifier. Other examples of related streams include traffic reports and car accidents, sensor readings of different types or at different locations, etc. In this paper, we consider the classification problem defined over sliding-window join of several input data streams. As the data streams arrive in fast pace and the many-to-many join relationship blows up the data arrival rate even more, it is impractical to compute the join and then build the classifier each time the window slides forward. We present an efficient algorithm to build a Naïve Bayesian classifier in such context. Our method does not need to perform the join operations but is still able to build exactly the same classifier as if built on the joined result. It only examines each input tuple twice, independent of the number of tuples it joins in other streams, therefore, is able to keep pace with the fast arriving data streams in the presence of many-to-many join relationships. The experiments confirmed that our classification algorithm is more efficient than conventional methods while maintaining good classification accuracy.

#index 907500
#* Validating associations in biological databases
#@ Francisco M. Couto;Mário J. Silva;Pedro M. Coutinho
#t 2006
#c 1
#% 224713
#% 737247
#% 786497
#% 838478
#% 1275285
#! Erroneous data can often be found in databases, and detecting it is normally a non-trivial task. For example, To cope with the large amount of biological sequences being produced, a significant number of genes and proteins have been annotated by automated tools. A protein annotation is an association between a protein and a term describing its role. These tools have produced a significant number of misannotations that are now present in biological databases. This paper proposes a new method for automatically scoring associations by comparing them to preexisting curated associations. An association is a pair that links two entities. The score can be used to filter incorrect or uncommon associations.We evaluated the method using the automated protein annotations submitted to BioCreAtIvE, an international evaluation of state-of-the-art text-mining systems in Biology. The method scored each of these annotations and those scored below a certain threshold were discarded. The results have shown a small trade-off in recall for a large improvement in precision. For example, we were able to discard 44.6%, 66.8% and 81% of the misannotations, maintaining 96.9%, 84.2%, and 47.8% of the correct annotations, respectively. Moreover, we were able to outperform each individual submission to BioCreAtIvE by proper adjustment of the threshold.

#index 907501
#* Finding highly correlated pairs efficiently with powerful pruning
#@ Jian Zhang;Joan Feigenbaum
#t 2006
#c 1
#% 152934
#% 227917
#% 243166
#% 249321
#% 280456
#% 420073
#% 420112
#% 443393
#% 481290
#% 577214
#% 577215
#% 632028
#% 769909
#% 1650289
#! We consider the problem of finding highly correlated pairs in a large data set. That is, given a threshold not too small, we wish to report all the pairs of items (or binary attributes) whose (Pearson) correlation coefficients are greater than the threshold. Correlation analysis is an important step in many statistical and knowledge-discovery tasks. Normally, the number of highly correlated pairs is quite small compared to the total number of pairs. Identifying highly correlated pairs in a naive way by computing the correlation coefficients for all the pairs is wasteful. With massive data sets, where the total number of pairs may exceed the main-memory capacity, the computational cost of the naive method is prohibitive. In their KDD'04 paper [15], Hui Xiong et al. address this problem by proposing the TAPER algorithm. The algorithm goes through the data set in two passes. It uses the first pass to generate a set of candidate pairs whose correlation coefficients are then computed directly in the second pass. The efficiency of the algorithm depends greatly on the selectivity (pruning power) of its candidate-generating stage.In this work, we adopt the general framework of the TAPER algorithm but propose a different candidate-generation method. For a pair of items, TAPER's candidate-generation method considers only the frequencies (supports) of individual items. Our method also considers the frequency (support) of the pair but does not explicitly count this frequency (support). We give a simple randomized algorithm whose false-negative probability is negligible. The space and time complexities of generating the candidate set in our algorithm are asymptotically the same as TAPER's. We conduct experiments on synthesized and real data. The results show that our algorithm produces a greatly reduced candidate set - one that can be several orders of magnitude smaller than that generated by TAPER. Because of this, our algorithm uses much less memory and can be faster. The former is critical for dealing with massive data.

#index 907502
#* Mining compressed commodity workflows from massive RFID data sets
#@ Hector Gonzalez;Jiawei Han;Xiaolei Li
#t 2006
#c 1
#% 459021
#% 466593
#% 466716
#% 487251
#% 560492
#% 629644
#% 729933
#% 749034
#% 864470
#% 864527
#% 893157
#% 903341
#! Radio Frequency Identification (RFID) technology is fast becoming a prevalent tool in tracking commodities in supply chain management applications. The movement of commodities through the supply chain forms a gigantic workflow that can be mined for the discovery of trends, flow correlations and outlier paths, that in turn can be valuable in understanding and optimizing business processes.In this paper, we propose a method to construct compressed probabilistic workflows that capture the movement trends and significant exceptions of the overall data sets, but with a size that is substantially smaller than that of the complete RFID workflow. Compression is achieved based on the following observations: (1) only a relatively small minority of items deviate from the general trend, (2)only truly non-redundant deviations, ie, those that substantially deviate from the previously recorded ones, are interesting, and (3) although RFID data is registered at the primitive level, data analysis usually takes place at a higher abstraction level. Techniques for workflow compression based on non-redundant transition and emission probabilities are derived; and an algorithm for computing approximate path probabilities is developed. Our experiments demonstrate the utility and feasibility of our design, data structure, and algorithms.

#index 907503
#* Discovering and exploiting keyword and attribute-value co-occurrences to improve P2P routing indices
#@ Sebastian Michel;Matthias Bender;Nikos Ntarmos;Peter Triantafillou;Gerhard Weikum;Christian Zimmer
#t 2006
#c 1
#% 2833
#% 152934
#% 194246
#% 282422
#% 311808
#% 322884
#% 340175
#% 340176
#% 340298
#% 344448
#% 413594
#% 479795
#% 505869
#% 593985
#% 616528
#% 730035
#% 745355
#% 763170
#% 807433
#% 818210
#% 818916
#% 823842
#% 824652
#% 824682
#% 824762
#% 824791
#% 839358
#% 859778
#% 949410
#% 1562244
#% 1704073
#% 1711388
#! Peer-to-Peer (P2P) search requires intelligent decisions for query routing: selecting the best peers to which a given query, initiated at some peer, should be forwarded for retrieving additional search results. These decisions are based on statistical summaries for each peer, which are usually organized on a per-keyword basis and managed in a distributed directory of routing indices. Such architectures disregard the possible correlations among keywords. Together with the coarse granularity of per-peer summaries, which are mandated for scalability, this limitation may lead to poor search result quality.This paper develops and evaluates two solutions to this problem, sk-STAT based on single-key statistics only, and mk-STAT based on additional multi-key statistics. For both cases, hash sketch synopses are used to compactly represent a peer's data items and are efficiently disseminated in the P2P network to form a decentralized directory. Experimental studies with Gnutella and Web data demonstrate the viability and the trade-offs of the approaches.

#index 907504
#* A document-centric approach to static index pruning in text retrieval systems
#@ Stefan Büttcher;Charles L. A. Clarke
#t 2006
#c 1
#% 198335
#% 212665
#% 213786
#% 326522
#% 340887
#% 397150
#% 397151
#% 805862
#% 879611
#% 1742113
#! We present a static index pruning method, to be used in ad-hoc document retrieval tasks, that follows a document-centric approach to decide whether a posting for a given term should remain in the index or not. The decision is made based on the term's contribution to the document's Kullback-Leibler divergence from the text collection's global language model. Our technique can be used to decrease the size of the index by over 90%, at only a minor decrease in retrieval effectiveness. It thus allows us to make the index small enough to fit entirely into the main memory of a single PC, even for large text collections containing millions of documents. This results in great efficiency gains, superior to those of earlier pruning methods, and an average response time around 20 ms on the GOV2 document collection.

#index 907505
#* Pruning strategies for mixed-mode querying
#@ Vo Ngoc Anh;Alistair Moffat
#t 2006
#c 1
#% 194247
#% 198335
#% 212665
#% 228097
#% 274490
#% 292684
#% 319273
#% 340886
#% 340887
#% 730065
#% 805862
#% 818229
#% 818230
#% 818232
#% 857180
#% 867054
#% 879611
#% 1682445
#% 1683906
#! Web information retrieval systems face a range of unique challenges, not the least of which is the sheer scale of the data that must be handled. Also specific to web retrieval is that queries may be a mix of Boolean and ranked features, and documents may have static score components that must also be factored into the ranking process. In this paper we consider a range of query semantics used in web retrieval systems, and show that impact-sorted indexes provide support for dynamic pruning mechanisms and in doing so allow fast document-at-a-time resolution of typical mixed-mode queries, even on relatively large volumes of data. Our techniques also extend to more complex query semantics, including the use of phrase, proximity, and structural constraints.

#index 907506
#* 3DString: a feature string kernel for 3D object classification on voxelized data
#@ Johannes Aßfalg;Karsten M. Borgwardt;Hans-Peter Kriegel
#t 2006
#c 1
#% 173050
#% 190581
#% 269217
#% 269226
#% 304917
#% 397654
#% 416060
#% 420077
#% 479462
#% 527186
#% 587733
#% 662894
#% 664407
#% 833124
#% 840941
#! Classification of 3D objects remains an important task in many areas of data management such as engineering, medicine or biology. As a common preprocessing step in current approaches to classification of voxelized 3D objects, voxel representations are transformed into a feature vector description.In this article, we introduce an approach of transforming 3D objects into feature strings which represent the distribution of voxels over the voxel grid. Attractively, this feature string extraction can be performed in linear runtime with respect to the number of voxels. We define a similarity measure on these feature strings that counts common k-mers in two input strings, which is referred to as the spectrum kernel in the field of kernel methods. We prove that on our feature strings, this similarity measure can be computed in time linear to the number of different characters in these strings. This linear runtime behavior makes our kernel attractive even for large datasets that occur in many application domains. Furthermore, we explain that our similarity measure induces a metric which allows to combine it with an M-tree for handling of large volumes of data. Classification experiments on two published benchmark datasets show that our novel approach is competitive with the best state-of-the-art methods for 3D object classification.

#index 907507
#* Effective and efficient classification on a search-engine model
#@ Aris Anagnostopoulos;Andrei Z. Broder;Kunal Punera
#t 2006
#c 1
#% 136350
#% 144007
#% 190581
#% 246831
#% 280817
#% 348154
#% 376266
#% 387427
#% 420054
#% 443297
#% 458379
#% 465754
#% 571073
#% 615723
#% 730065
#% 763699
#% 763708
#! Traditional document classification frameworks, which apply the learned classifier to each document in a corpus one by one, are infeasible for extremely large document corpora, like the Web or large corporate intranets. We consider the classification problem on a corpus that has been processed primarily for the purpose of searching, and thus our access to documents is solely through the inverted index of a large scale search engine. Our main goal is to build the "best" short query that characterizes a document class using operators normally available within large engines. We show that surprisingly good classification accuracy can be achieved on average over multiple classes by queries with as few as 10 terms. Moreover, we show that optimizing the efficiency of query execution by careful selection of these terms can further reduce the query costs. More precisely, we show that on our set-up the best 10 terms query canachieve 90% of the accuracy of the best SVM classifier (14000 terms), and if we are willing to tolerate a reduction to 86% of the best SVM, we can build a 10 terms query that can be executed more than twice as fast as the best 10 terms query.

#index 907508
#* Multi-evidence, multi-criteria, lazy associative document classification
#@ Adriano Veloso;Wagner Meira, Jr.;Marco Cristo;Marcos Gonçalves;Mohammed Zaki
#t 2006
#c 1
#% 136350
#% 169718
#% 227919
#% 249110
#% 279109
#% 344447
#% 413663
#% 430761
#% 464267
#% 466483
#% 577214
#% 617186
#% 729941
#% 730061
#% 838503
#% 1387536
#% 1499572
#! We present a novel approach for classifying documents that combines different pieces of evidence (e.g., textual features of documents, links, and citations) transparently, through a data mining technique which generates rules associating these pieces of evidence to predefined classes. These rules can contain any number and mixture of the available evidence and are associated with several quality criteria which can be used in conjunction to choose the "best" rule to be applied at classification time. Our method is able to perform evidence enhancement by link forwarding/backwarding (i.e., navigating among documents related through citation), so that new pieces of link-based evidence are derived when necessary. Furthermore, instead of inducing a single model (or rule set) that is good on average for all predictions, the proposed approach employs a lazy method which delays the inductive process until a document is given for classification, therefore taking advantage of better qualitative evidence coming from the document. We conducted a systematic evaluation of the proposed approach using documents from the ACM Digital Library and from a Brazilian Web directory. Our approach was able to outperform in both collections all classifiers based on the best available evidence in isolation as well as state-of-the-art multi-evidence classifiers. We also evaluated our approach using the standard WebKB collection, where our approach showed gains of 1% in accuracy, being 25 times faster. Further, our approach is extremely efficient in terms of computational performance, showing gains of more than one order of magnitude when compared against other multi-evidence classifiers.

#index 907509
#* Knowing a web page by the company it keeps
#@ Xiaoguang Qi;Brian D. Davison
#t 2006
#c 1
#% 248810
#% 269217
#% 281251
#% 309141
#% 309747
#% 348148
#% 348173
#% 348178
#% 413611
#% 451536
#% 466896
#% 729621
#% 730061
#% 766437
#% 769395
#% 838469
#% 879576
#! Web page classification is important to many tasks in information retrieval and web mining. However, applying traditional textual classifiers on web data often produces unsatisfying results. Fortunately, hyperlink information provides important clues to the categorization of a web page. In this paper, an improved method is proposed to enhance web page classification by utilizing the class information from neighboring pages in the link graph. The categories represented by four kinds of neighbors (parents, children, siblings and spouses) are combined to help with the page in question. In experiments to study the effect of these factors on our algorithm, we find that the method proposed is able to boost the classification accuracy of common textual classifiers from around 70% to more than 90% on a large dataset of pages from the Open Directory Project, and outperforms existing algorithms. Unlike prior techniques, our approach utilizes same-host links and can improve classification accuracy even when neighboring pages are unlabeled. Finally, while all neighbor types can contribute, sibling pages are found to be the most important.

#index 907510
#* Improving novelty detection for general topics using sentence level information patterns
#@ Xiaoyan Li;W. Bruce Croft
#t 2006
#c 1
#% 262042
#% 262043
#% 262112
#% 278107
#% 316546
#% 397133
#% 577297
#% 642975
#% 643014
#% 643016
#% 643031
#% 815098
#% 815107
#% 838537
#% 838538
#! The detection of new information in a document stream is an important component of many potential applications. In this work, a new novelty detection approach based on the identification of sentence level information patterns is proposed. First, the information-pattern concept for novelty detection is presented with the emphasis on new information patterns for general topics (queries) that cannot be simply turned into specific questions whose answers are specific named entities (NEs). Then we elaborate a thorough analysis of sentence level information patterns on data from the TREC novelty tracks, including sentence lengths, named entities, sentence level opinion patterns. This analysis provides guidelines in applying those patterns in novelty detection particularly for the general topics. Finally, a unified pattern-based approach is presented to novelty detection for both general and specific topics. The new method for dealing with general topics will be the focus. Experimental results show that the proposed approach significantly improves the performance of novelty detection for general topics as well as the overall performance for all topics from the 2002-2004 TREC novelty tracks.

#index 907511
#* Topic evolution and social interactions: how authors effect research
#@ Ding Zhou;Xiang Ji;Hongyuan Zha;C. Lee Giles
#t 2006
#c 1
#% 220708
#% 249143
#% 285720
#% 310514
#% 311027
#% 329569
#% 387427
#% 466675
#% 577297
#% 578775
#% 722904
#% 730089
#% 754107
#% 760866
#% 769906
#% 771924
#% 823344
#% 869480
#! We propose a method for discovering the dependency relationships between the topics of documents shared in social networks using the latent social interactions, attempting to answer the question: given a seemingly new topic, from where does this topic evolve? In particular, we seek to discover the pair-wise probabilistic dependency in topics of documents which associate social actors from a latent social network, where these documents are being shared. By viewing the evolution of topics as a Markov chain, we estimate a Markov transition matrix of topics by leveraging social interactions and topic semantics. Metastable states in a Markov chain are applied to the clustering of topics. Applied to the CiteSeer dataset, a collection of documents in academia, we show the trends of research topics, how research topics are related and which are stable. We also show how certain social actors, authors, impact these topics and propose new ways for evaluating author impact.

#index 907512
#* A fast and robust method for web page template detection and removal
#@ Karane Vieira;Altigran S. da Silva;Nick Pinto;Edleno S. de Moura;João M. B. Cavalcanti;Juliana Freire
#t 2006
#c 1
#% 103525
#% 121462
#% 262504
#% 268073
#% 289193
#% 319876
#% 340887
#% 340919
#% 348180
#% 349489
#% 376266
#% 480126
#% 729939
#% 754078
#% 754108
#% 805862
#% 807298
#% 810759
#! The widespread use of templates on the Web is considered harmful for two main reasons. Not only do they compromise the relevance judgment of many web IR and web mining methods such as clustering and classification, but they also negatively impact the performance and resource usage of tools that process web pages. In this paper we present a new method that efficiently and accurately removes templates found in collections of web pages. Our method works in two steps. First, the costly process of template detection is performed over a small set of sample pages. Then, the derived template is removed from the remaining pages in the collection. This leads to substantial performance gains when compared to previous approaches that combine template detection and removal. We show, through an experimental evaluation, that our approach is effective for identifying terms occurring in templates - obtaining F-measure values around 0.9, and that it also boosts the accuracy of web page clustering and classification methods.

#index 907513
#* Out-of-context noun phrase semantic interpretation with cross-linguistic evidence
#@ Roxana Girju
#t 2006
#c 1
#% 529645
#% 747893
#% 748531
#% 815894
#% 939515
#% 939540
#% 939600
#% 939965
#% 1249579
#% 1294844
#% 1344852
#! The acquisition of semantic knowledge is paramount for any application that requires a deep understanding of natural language text. Motivated by the problem of building a noun phrase-level semantic parser and adapting it to various applications, such as machine translation and multilingual question answering, in this paper we present a domain-independent model for noun phrase semantic interpretation. We investigate the problem based on cross-linguistic evidence from a set of four Romance languages: Spanish, Italian, French, and Romanian. The focus on Romance languages is well motivated. It is generally the case that English noun phrases translate into constructions of the form "N P N" in Romance languages where, as we will show, the P (preposition) varies in ways that correlate with the semantics. Thus, based on a set of 22 semantic interpretation categories (such as PART-WHOLE, AGENT, POSSESSION) we present empirical observations regarding the distribution of these semantic categories in a cross-lingual corpus and their mapping to various syntactic constructions in English and Romance. Furthermore, given a training set of English noun phrases along with their translations in the four Romance languages, our algorithm automatically learns classification rules and applies them to unseen noun phrase instances for semantic interpretation. Experimental results are compared against a state-of-the-art model reported in the literature.

#index 907514
#* Capturing community search expertise for personalized web search using snippet-indexes
#@ Oisín Boydell;Barry Smyth
#t 2006
#c 1
#% 169803
#% 230432
#% 249110
#% 262084
#% 292151
#% 340916
#% 341964
#% 375017
#% 399057
#% 433674
#% 478627
#% 643028
#% 754126
#% 803556
#% 818221
#% 832349
#% 838547
#% 869500
#% 1289575
#% 1715615
#! We describe and evaluate an approach to capturing and re-using search expertise within a community of like minded searchers, such as the employees of a company or organisation. Within knowledge based industries, search expertise - the ability to quickly and accurately locate information according to a specific information need - is an important corporate asset and in our approach we attempt to capture this knowledge by mining the title and snippet texts of results that have been selected by community members in response to their queries. Our assumption is that the snippet text of a result must play a role in helping users to judge the initial relevance of that result and that the snippet terms of selected results must contain especially informative terms about the goals and preferences of the searchers. In other words, results are selected because the user recognises certain combinations of terms in their snippets which are related to their information needs. Our approach seeks to build a community-based snippet index that reflects the evolving interests of a group of searchers. This index is then used to re-rank the results returned by some underlying search engine by boosting the ranking of key results that have been frequently selected for similar queries by community members in the past.

#index 907515
#* Summarizing local context to personalize global web search
#@ Paul-Alexandru Chirita;Claudiu S. Firan;Wolfgang Nejdl
#t 2006
#c 1
#% 46809
#% 118771
#% 144023
#% 144051
#% 198058
#% 262036
#% 262084
#% 262112
#% 280835
#% 280840
#% 280849
#% 281186
#% 308756
#% 340882
#% 340885
#% 340951
#% 348173
#% 387427
#% 577329
#% 643068
#% 729625
#% 742513
#% 754126
#% 766433
#% 766530
#% 768292
#% 771571
#% 787502
#% 818221
#% 818224
#% 818259
#% 907441
#% 1272053
#! The PC Desktop is a very rich repository of personal information, efficiently capturing user's interests. In this paper we propose a new approach towards an automatic personalization of web search in which the user specific information is extracted from such local desktops, thus allowing for an increased quality of user profiling, while sharing less private information with the search engine. More specifically, we investigate the opportunities to select personalized query expansion terms for web search using three different desktop oriented approaches: summarizing the entire desktop data, summarizing only the desktop documents relevant to each user query, and applying natural language processing techniques to extract dispersive lexical compounds from relevant desktop resources. Our experiments with the Google API showed at least the latter two techniques to produce a very strong improvement over current web search.

#index 907516
#* A study on the effects of personalization and task information on implicit feedback performance
#@ Ryen W. White;Diane Kelly
#t 2006
#c 1
#% 54435
#% 92696
#% 169806
#% 306468
#% 313719
#% 320432
#% 325209
#% 731615
#% 754059
#% 761887
#% 766454
#% 790446
#% 805200
#% 818206
#% 818221
#% 818259
#% 818260
#% 822126
#% 879567
#% 881943
#! While Implicit Relevance Feedback (IRF) algorithms exploit users' interactions with information to customize support offered to users of search systems, it is unclear how individual and task differences impact the effectiveness of such algorithms. In this paper we describe a study on the effect on retrieval performance of using additional information about the user and their search tasks when developing IRF algorithms. We tested four algorithms that use document display time to estimate relevance, and tailored the threshold times (i.e., the time distinguishing relevance from non-relevance) to the task, the user, a combination of both, or neither. Interaction logs gathered during a longitudinal naturalistic study of online information-seeking behavior are used as stimuli for the algorithms. The findings show that tailoring display time thresholds based on task information improves IRF algorithm performance, but doing so based on user information worsens performance. This has implications for the development of effective IRF algorithms.

#index 907517
#* Incorporating query difference for learning retrieval functions in world wide web search
#@ Hongyuan Zha;Zhaohui Zheng;Haoying Fu;Gordon Sun
#t 2006
#c 1
#% 55490
#% 57484
#% 65947
#% 86371
#% 111304
#% 157135
#% 185289
#% 262096
#% 411762
#% 577224
#% 750863
#% 766414
#% 766430
#% 818221
#% 818239
#% 828494
#% 840846
#% 872020
#! We discuss information retrieval methods that aim at serving a diverse stream of user queries such as those submitted to commercial search engines. We propose methods that emphasize the importance of taking into consideration of query difference in learning effective retrieval functions. We formulate the problem as a multi-task learning problem using a risk minimization framework. In particular, we show how to calibrate the empirical risk to incorporate query difference in terms of introducing nuisance parameters in the statistical models, and we also propose an alternating optimization method to simultaneously learn the retrieval function and the nuisance parameters. We work out the details for both L1 and L2 regularization cases, and provide convergence analysis for the alternating optimization method for the special case when the retrieval functions belong to a reproducing kernel Hilbert space. We illustrate the effectiveness of the proposed methods using modeling data extracted from a commercial search engine. We also point out how the current framework can be extended in future research.

#index 907518
#* KDDCS: a load-balanced in-network data-centric storage scheme for sensor networks
#@ Mohamed Aly;Kirk Pruhs;Panos K. Chrysanthis
#t 2006
#c 1
#% 309466
#% 321455
#% 401227
#% 720035
#% 731090
#% 731091
#% 754694
#% 784811
#% 805466
#% 822526
#! We propose an In-Network Data-Centric Storage (INDCS) scheme for answering ad-hoc queries in sensor networks. Previously proposed In-Network Storage (INS) schemes suffered from Storage Hot-Spots that are formed if either the sensors' locations are not uniformly distributed over the coverage area, or the distribution of sensor readings is not uniform over the range of possible reading values. Our K-D tree based Data-Centric Storage (KDDCS) scheme maintains the invariant that the storage of events is distributed reasonably uniformly among the sensors. KDDCS is composed of a set of distributed algorithms whose running time is within a poly-log factor of the diameter of the network. The number of messages any sensor has to send, as well as the bits in those messages, is poly-logarithmic in the number of sensors. Load balancing in KDDCS is based on defining and distributively solving a theoretical problem that we call the Weighted Split Median problem. In addition to analytical bounds on KDDCS individual algorithms, we provide experimental evidence of our scheme's general efficiency, as well as its ability to avoid the formation of storage hot-spots of various sizes, unlike all previous INDCS schemes.

#index 907519
#* Efficient range-constrained similarity search on wavelet synopses over multiple streams
#@ Hao-Ping Hung;Ming-Syan Chen
#t 2006
#c 1
#% 201876
#% 248822
#% 257637
#% 264161
#% 316538
#% 413606
#% 578390
#% 578400
#% 631923
#% 659936
#% 810061
#% 823333
#% 824686
#% 845224
#% 993999
#% 1016196
#! Due to the resource limitation in the data stream environment, it has been reported that answering user queries according to the wavelet synopsis of a stream is an essential ability of a Data Stream Management System (DSMS). In this paper, motivated by the fact that a user may be interested in an arbitrary range of the data streams, we investigate two important types of range-constrained queries in time series streaming environments: the distance queries (which aim at obtaining the Euclidean distance between two streams) and the kNN queries (which aim at discovering k nearest neighbors to a reference stream). To achieve high efficiency in processing these two types of queries, we propose procedure RED (standing for Range-constrained Euclidean Distance) and algorithm EKS (standing for Enhanced KNN Search). Compared to the existing methods in the prior research, the advantageous features of our approaches are in two folds. First, our approaches are capable of processing the queries directly from the wavelet synopses retained in the main memory without using IDWT to reconstruct the data cells. This feature allows us to save the complexity in both memory and time. Moreover, our approaches enable the users to query the DSMS within their range of interest. Unlike the conventional methods which only support the full-range query processing, this feature will enhance the flexibility at the client side. We evaluate procedure RED and algorithm EKS on live and synthetic datasets empirically and show that the proposed approaches are efficient in similarity search and kNN discovery within arbitrary ranges in the time series streaming environments.

#index 907520
#* A data stream language and system designed for power and extensibility
#@ Yijian Bai;Hetal Thakkar;Haixun Wang;Chang Luo;Carlo Zaniolo
#t 2006
#c 1
#% 333850
#% 378388
#% 379444
#% 379445
#% 397414
#% 420053
#% 458843
#% 482088
#% 578560
#% 654497
#% 654510
#% 726621
#% 730046
#% 765404
#% 810007
#% 810033
#% 810063
#% 810092
#% 864437
#% 993949
#% 1016157
#% 1016170
#% 1688279
#! By providing an integrated and optimized support for user-defined aggregates (UDAs), data stream management systems (DSMS) can achieve superior power and generality while preserving compatibility with current SQL standards. This is demonstrated by the Stream Mill system that, through is Expressive Stream Language (ESL), efficiently supports a wide range of applications - including very advanced ones such as data stream mining, streaming XML processing, time-series queries, and RFID event processing. ESL supports physical and logical windows (with optional slides and tumbles) on both built-in aggregates and UDAs, using a simple framework that applies uniformly to both aggregate functions written in an external procedural languages and those natively written in ESL. The constructs introduced in ESL extend the power and generality of DSMS, and are conducive to UDA-specific optimization and efficient execution as demonstrated by several experiments.

#index 907521
#* In search of meaning for time series subsequence clustering: matching algorithms based on a new distance measure
#@ Dina Goldin;Ricardo Mardales;George Nagy
#t 2006
#c 1
#% 24033
#% 36672
#% 114667
#% 359751
#% 534183
#% 727900
#% 729437
#% 814194
#% 837616
#% 844296
#% 844297
#% 922066
#% 1663634
#! Recent papers have claimed that the result of K-means clustering for time series subsequences (STS clustering) is independent of the time series that created it. Our paper revisits this claim. In particular, we consider the following question: Given several time series sequences and a set of STS cluster centroids from one of them (generated by the K-means algorithm), is it possible to reliably determine which of the sequences produced these cluster centroids? While recent results suggest that the answer should be NO, we answer this question in the affirmative.We present cluster shape distance, an alternate distance measure for time series subsequence clusters, based on cluster shapes. Given a set of clusters, its shape is the sorted list of the pairwise Euclidean distances between their centroids. We then present two algorithms based on this distance measure, which match a set of STS cluster centroids with the time series that produced it. While the first algorithm creates DQG reuse this term more smaller "fingerprints" for the sequences, the second is more accurate. In our experiments with a dataset of 10 sequences, it produced a correct match 100% of the time.Furthermore, we offer an analysis that explains why our cluster shape distance provides a reliable way to match STS clusters to the original sequences, whereas cluster set distance fails to do so. Our work establishes for the first time a strong relation between the result of K-means STS clustering and the time series sequence that created it, despite earlier predictions that this is not possible.

#index 907522
#* Incremental hierarchical clustering of text documents
#@ Nachiketa Sahoo;Jamie Callan;Ramayya Krishnan;George Duncan;Rema Padman
#t 2006
#c 1
#% 65440
#% 118771
#% 262043
#% 279755
#% 296738
#% 413610
#% 451052
#% 742513
#% 755463
#% 763708
#% 766430
#% 808789
#% 1783133
#! Incremental hierarchical text document clustering algorithms are important in organizing documents generated from streaming on-line sources, such as, Newswire and Blogs. However, this is a relatively unexplored area in the text document clustering literature. Popular incremental hierarchical clustering algorithms, namely Cobweb and Classit, have not been widely used with text document data. We discuss why, in the current form, these algorithms are not suitable for text clustering and propose an alternative formulation that includes changes to the underlying distributional assumption of the algorithm in order to conform with the data. Both the original Classit algorithm and our proposed algorithm are evaluated using Reuters newswire articles and Ohsumed dataset.

#index 907523
#* Efficiently clustering transactional data with weighted coverage density
#@ Hua Yan;Keke Chen;Ling Liu
#t 2006
#c 1
#% 280419
#% 287285
#% 296738
#% 342621
#% 342659
#% 397597
#% 413618
#% 420081
#% 443468
#% 466675
#% 479659
#% 481290
#% 498852
#% 577296
#% 631985
#% 769883
#% 770826
#% 823086
#% 853068
#% 1705431
#! It is widely recognized that developing efficient and fully automated algorithms for clustering large transactional datasets is a challenging problem. In this paper, we propose a fast, memory-efficient, and scalable clustering algorithm for analyzing transactional data. Our approach has three unique features. First, we use the concept of Weighted Coverage Density as a categorical similarity measure for efficient clustering of transactional datasets. The concept of weighted coverage density is intuitive and allows the weight of each item in a cluster to be changed dynamically according to the occurrences of items. Second, we develop two transactional data clustering specific evaluation metrics based on the concept of large transactional items and the coverage density respectively. Third, we implement the weighted coverage density clustering algorithm and the two clustering validation metrics using a fully automated transactional clustering framework, called SCALE (Sampling, Clustering structure Assessment, cLustering and domain-specific Evaluation). The SCALE framework is designed to combine the weighted coverage density measure for clustering over a sample dataset with self-configuring methods that can automatically tune the two important parameters of the clustering algorithms: (1) the candidates of the best number K of clusters; and (2) the application of two domain-specific cluster validity measures to find the best result from the set of clustering results. We have conducted experimental evaluation using both synthetic and real datasets and our results show that the weighted coverage density approach powered by the SCALE framework can efficiently generate high quality clustering results in a fully automated manner.

#index 907524
#* Ranking web objects from multiple communities
#@ Le Chen;Lei Zhang;Feng Jing;Ke-Feng Deng;Wei-Ying Ma
#t 2006
#c 1
#% 268079
#% 290830
#% 330769
#% 387427
#% 577224
#% 728195
#% 734915
#% 754089
#% 766414
#% 805896
#% 840846
#% 1677518
#! Vertical search is a promising direction as it leverages domain-specific knowledge and can provide more precise information for users. In this paper, we study the Web object-ranking problem, one of the key issues in building a vertical search engine. More specifically, we focus on this problem in cases when objects lack relationships between different Web communities, and take high-quality photo search as the test bed for this investigation. We proposed two score fusion methods that can automatically integrate as many Web communities (Web forums) with rating information as possible. The proposed fusion methods leverage the hidden links discovered by a duplicate photo detection algorithm, and aims at minimizing score differences of duplicate photos in different forums. Both intermediate results and user studies show the proposed fusion methods are practical and efficient solutions to Web object ranking in cases we have described. Though the experiments were conducted on high-quality photo ranking, the proposed algorithms are also applicable to other ranking problems, such as movie ranking and music ranking.

#index 907525
#* Voting for candidates: adapting data fusion techniques for an expert search task
#@ Craig Macdonald;Iadh Ounis
#t 2006
#c 1
#% 118762
#% 232703
#% 290830
#% 312701
#% 340934
#% 340936
#% 340959
#% 341976
#% 342710
#% 411760
#% 413613
#% 413655
#% 642992
#% 662755
#% 730082
#% 783474
#% 818267
#% 838464
#% 869649
#% 879710
#% 1674994
#% 1715628
#% 1742070
#! In an expert search task, the users' need is to identify people who have relevant expertise to a topic of interest. An expert search system predicts and ranks the expertise of a set of candidate persons with respect to the users' query. In this paper, we propose a novel approach for predicting and ranking candidate expertise with respect to a query. We see the problem of ranking experts as a voting problem, which we model by adapting eleven data fusion techniques.We investigate the effectiveness of the voting approach and the associated data fusion techniques across a range of document weighting models, in the context of the TREC 2005 Enterprise track. The evaluation results show that the voting paradigm is very effective, without using any collection specific heuristics. Moreover, we show that improving the quality of the underlying document representation can significantly improve the retrieval performance of the data fusion techniques on an expert search task. In particular, we demonstrate that applying field-based weighting models improves the ranking of candidates. Finally, we demonstrate that the relative performance of the adapted data fusion techniques for the proposed approach is stable regardless of the used weighting models.

#index 907526
#* Bayesian adaptive user profiling with explicit & implicit feedback
#@ Philip Zigoris;Yi Zhang
#t 2006
#c 1
#% 132583
#% 292179
#% 320432
#% 577224
#% 731615
#% 737637
#% 766451
#% 766454
#% 805200
#% 818259
#% 872028
#% 939927
#! Research in information retrieval is now moving into a personalized scenario where a retrieval or filtering system maintains a separate user profile for each user. In this framework, information delivered to the user can be automatically personalized and catered to individual user's information needs. However, a practical concern for such a personalized system is the "cold start problem": any user new to the system must endure poor initial performance until sufficient feedback from that user is provided.To solve this problem, we use both explicit and implicit feedback to build a user's profile and use Bayesian hierarchical methods to borrow information from existing users. We analyze the usefulness of implicit feedback and the adaptive performance of the model on two data sets gathered from user studies where users' interaction with a document, or implicit feedback, were recorded along with explicit feedback. Our results are two-fold: first, we demonstrate that the Bayesian modeling approach effectively trades off between shared and user-specific information, alleviating poor initial performance for each user. Second, we find that implicit feedback has very limited unstable predictive value by itself and only marginal value when combined with explicit feedback.

#index 907527
#* SaLSa: computing the skyline without scanning the whole sky
#@ Ilaria Bartolini;Paolo Ciaccia;Marco Patella
#t 2006
#c 1
#% 2115
#% 465167
#% 654480
#% 731407
#% 824670
#% 864451
#% 864452
#% 954011
#% 993954
#% 993957
#% 994017
#! Skyline queries compute the set of Pareto-optimal tuples in a relation, ie those tuples that are not dominated by any other tuple in the same relation. Although several algorithms have been proposed for efficiently evaluating skyline queries, they either require to extend the relational server with specialized access methods (which is not always feasible) or have to perform the dominance tests on all the tuples in order to determine the result. In this paper we introduce SaLSa (Sort and Limit Skyline algorithm), which exploits the sorting machinery of a relational engine to order tuples so that only a subset of them needs to be examined for computing the skyline result. This makes SaLSa particularly attractive when skyline queries are executed on top of systems that do not understand skyline semantics or when the skyline logic runs on clients with limited power and/or bandwidth.

#index 907528
#* Constrained subspace skyline computation
#@ Evangelos Dellis;Akrivi Vlachou;Ilya Vladimirskiy;Bernhard Seeger;Yannis Theodoridis
#t 2006
#c 1
#% 86950
#% 201876
#% 273941
#% 287466
#% 333854
#% 427199
#% 465167
#% 480671
#% 481749
#% 481956
#% 527328
#% 654480
#% 800512
#% 800555
#% 806212
#% 824671
#% 824672
#% 864452
#% 864453
#% 875011
#% 993954
#% 1688253
#% 1698972
#! In this paper we introduce the problem of Constrained Subspace Skyline Queries. This class of queries can be thought of as a generalization of subspace skyline queries using range constraints. Although both constrained skyline queries and subspace skyline queries have been addressed previously, the implications of constrained subspace skyline queries has not been examined so far. Constrained skyline queries are usually more expensive than regular skylines. In case of constrained subspace skyline queries additional performance degradation is caused through the projection. In order to support constrained skylines for arbitrary subspaces, we present approaches exploiting multiple low-dimensional indexes instead of relying on a single high-dimensional index. Effective pruning strategies are applied to discard points from dominated regions. An important ingredient of our approach is the workload-adaptive strategy for determining the number of indexes and the assignment of dimensions to the indexes. Extensive performance evaluation shows the superiority of our proposed technique compared to its most related competitors.

#index 907529
#* Processing relaxed skylines in PDMS using distributed data summaries
#@ Katja Hose;Christian Lemke;Kai-Uwe Sattler
#t 2006
#c 1
#% 210173
#% 333854
#% 465167
#% 480330
#% 482092
#% 636008
#% 799759
#% 824670
#% 824671
#% 824672
#% 993954
#% 1016183
#% 1688253
#% 1711093
#! Peer Data Management Systems (PDMS) are a natural extension of heterogeneous database systems. One of the main tasks in such systems is efficient query processing. Insisting on complete answers, however, leads to asking almost every peer in the network. Relaxing these completeness requirements by applying approximate query answering techniques can significantly reduce costs. Since most users are not interested in the exact answers to their queries, rank-aware query operators like top-k or skyline play an important role in query processing. In this paper, we present the novel concept of relaxed skylines that combines the advantages of both rank-aware query operators and approximate query processing techniques. Furthermore, we propose a strategy for processing relaxed skylines in distributed environments that allows for giving guarantees for the completeness of the result using distributed data summaries as routing indexes.

#index 907530
#* On the structural properties of massive telecom call graphs: findings and implications
#@ Amit A. Nanavati;Siva Gurumurthy;Gautam Das;Dipanjan Chakraborty;Koustuv Dasgupta;Sougata Mukherjea;Anupam Joshi
#t 2006
#c 1
#% 268079
#% 281214
#% 283833
#% 290830
#% 293721
#% 300079
#% 309749
#% 577219
#% 786841
#% 1777282
#! With ever growing competition in telecommunications markets, operators have to increasingly rely on business intelligence to offer the right incentives to their customers. Toward this end, existing approaches have almost solely focussed on the individual behaviour of customers. Call graphs, that is, graphs induced by people calling each other, can allow telecom operators to better understand the interaction behaviour of their customers, and potentially provide major insights for designing effective incentives.In this paper, we use the Call Detail Records of a mobile operator from four geographically disparate regions to construct call graphs, and analyse their structural properties. Our findings provide business insights and help devise strategies for Mobile Telecom operators. Another goal of this paper is to identify the shape of such graphs. In order to do so, we extend the well-known reachability analysis approach with some of our own techniques to reveal the shape of such massive graphs. Based on our analysis, we introduce the Treasure-Hunt model to describe the shape of mobile call graphs. The proposed techniques are general enough for analysing any large graph. Finally, how well the proposed model captures the shape of other mobile call graphs needs to be the subject of future studies.

#index 907531
#* Heuristic containment check of partial tree-pattern queries in the presence of index graphs
#@ Dimitri Theodoratos;Stefanos Souldatos;Theodore Dalamagas;Pawel Placek;Timos Sellis
#t 2006
#c 1
#% 333989
#% 378393
#% 397360
#% 397374
#% 397379
#% 458861
#% 465051
#% 479465
#% 480646
#% 564264
#% 660000
#% 805867
#% 824662
#% 838533
#% 885390
#% 1015258
#% 1016135
#% 1016139
#% 1016140
#! The wide adoption of XML has increased the interest of the database community on tree-structured data management techniques. Querying capabilities are provided through tree-pattern queries. The need for querying tree-structured data sources when their structure is not fully known, and the need to integrate multiple data sources with different tree structures have driven, recently, the suggestion of query languages that relax the complete specification of a tree pattern. In this paper, we use a query language which allows partial tree-pattern queries (PTPQs). The structure in a PTPQ can be flexibly specified fully, partially or not at all. To evaluate a PTPQ, we exploit index graphs which generate an equivalent set of "complete" tree-pattern queries.In order to process PTPQs, we need to efficiently solve the PTPQ satisfiability and containment problems. These problems become more complex in the context of PTPQs because the partial specification of the structure allows new, non-trivial, structural expressions to be derived from those explicitly specified in a PTPQ. We address the problem of PTPQ satisfiability and containment in the absence and in the presence of index graphs, and we provide necessary and sufficient conditions for each case. To cope with the high complexity of PTPQ containment in the presence of index graphs,we study a family of heuristic approaches for PTPQ containment based on structural information extracted from the index graph in advance and on-the-fly. We implement our approaches and we report on their extensive experimental evaluation and comparison.

#index 907532
#* TRIPS and TIDES: new algorithms for tree mining
#@ Shirish Tatikonda;Srinivasan Parthasarathy;Tahsin Kurc
#t 2006
#c 1
#% 262071
#% 300120
#% 464996
#% 466644
#% 478622
#% 577218
#% 629708
#% 661023
#% 729941
#% 737334
#% 745461
#% 769951
#% 785428
#% 794921
#% 824699
#% 844409
#% 1669921
#! Recent research in data mining has progressed from mining frequent itemsets to more general and structured patterns like trees and graphs. In this paper, we address the problem of frequent subtree mining that has proven to be viable in a wide range of applications such as bioinformatics, XML processing, computational linguistics, and web usage mining. We propose novel algorithms to mine frequent subtrees from a database of rooted trees. We evaluate the use of two popular sequential encodings of trees to systematically generate and evaluate the candidate patterns. The proposed approach is very generic and can be used to mine embedded or induced subtrees that can be labeled, unlabeled, ordered, unordered, or edge-labeled. Our algorithms are highly cache-conscious in nature because of the compact and simple array-based data structures we use. Typically, L1 and L2 hit rates above 99% are observed. Experimental evaluation showed that our algorithms can achieve up to several orders of magnitude speedup on real datasets when compared to state-of-the-art tree mining algorithms.

#index 907533
#* Automatic computation of semantic proximity using taxonomic knowledge
#@ Cai-Nicolas Ziegler;Kai Simon;Georg Lausen
#t 2006
#c 1
#% 198058
#% 325502
#% 330617
#% 330687
#% 375017
#% 387427
#% 447948
#% 465914
#% 641963
#% 754104
#% 765412
#% 783531
#% 805839
#% 805841
#% 805849
#% 805872
#% 818224
#% 1275285
#% 1650569
#! Taxonomic measures of semantic proximity allow us to compute the relatedness of two concepts. These metrics are versatile instruments required for diverse applications, e.g., the Semantic Web, linguistics, and also text mining. However, most approaches are only geared towards hand-crafted taxonomic dictionaries such as WordNet, which only feature a limited fraction of real-world concepts. More specific concepts, and particularly instances of concepts, i.e., names of artists, locations, brand names, etc., are not covered.The contributions of this paper are two fold. First, we introduce a framework based on Google and the Open Directory Project (ODP), enabling us to derive the semantic proximity between arbitrary concepts and instances. Second, we introduce a new taxonomy-driven proximity metric tailored for our framework. Studies with human subjects corroborate our hypothesis that our new metric outperforms benchmark semantic proximity metrics and comes close to human judgement.

#index 907534
#* Exploiting asymmetry in hierarchical topic extraction
#@ Sreenivas Gollapudi;Rina Panigrahy
#t 2006
#c 1
#% 232767
#% 273699
#% 290830
#% 311808
#% 338442
#% 342621
#% 347225
#% 451052
#% 466269
#% 479973
#% 577230
#% 577273
#% 656762
#% 765545
#! Topic or feature extraction is often used as an important step in document classification and text mining. Topics are succinct representation of content in a document collection and hence are very effective when used as content identifiers in peer-to-peer systems and other large scale distributed content management systems. Effective topic extraction is dependent on the accuracy of term clustering that often has to deal with problems like synonymy and polysemy. Retrieval techniques based on spectral analysis like Latent Semantic Indexing (LSI) are often used to effectively solve these problems. Most of the spectral retrieval schemes produce term similarity measures that are symmetric and often, not an accurate characterization of term relationships. Another drawback of LSI is its running time that is polynomial in the dimensions of the m x n matrix, A. This can get prohibitively large for some IR applications. In this paper, we present efficient algorithms using the technique of Locality-Sensitive Hashing (LSH) to extract topics from a document collection based on the asymmetric relationships between terms in a collection. The relationship is characterized by the term co-occurrences and other higher-order similarity measures. Our LSH based scheme can be viewed as a simple alternative to LSI. We show the efficacy of our algorithms via experiments on a set of large documents. An interesting feature of our algorithms is that it produces a natural hierarchical decomposition of the topic space instead of a flat clustering.

#index 907535
#* CP/CV: concept similarity mining without frequency information from domain describing taxonomies
#@ Jong Wook Kim;K. Sel#231/uk Candan
#t 2006
#c 1
#% 149454
#% 224692
#% 252328
#% 290830
#% 319273
#% 325502
#% 387427
#% 452869
#% 465914
#% 641963
#% 654442
#% 744285
#% 747891
#% 748600
#% 757276
#% 805849
#% 818254
#% 824693
#% 881824
#% 1015258
#% 1016176
#% 1683877
#! Domain specific ontologies are heavily used in many applications. For instance, these form the bases on which similarity/dissimilarity between keywords are extracted for various knowledge discovery and retrieval tasks. Existing similarity computation schemes can be categorized as (a) structure- or (b) information-based approaches. Structure based approaches compute dissimilarity between keywords using a (weighted) count of edges between two keywords. Information-base approaches, on the other hand, leverage available corpora to extract additional information, such as keyword frequency, to achieve better performance in similarity computation than structure-based approaches. Unfortunately, in many application domains (such as applications that rely on unique-keys in a relational database), frequency information required by information-based approaches does not exist. In this paper, we note that there is a third way of computing similarity: if each node in a given hierarchy can be represented as a vector of related concepts, these vectors could be compared to compute similarities. This requires mapping concept-nodes in a given hierarchy onto a concept space. In this paper, we propose a concept propagation (CP) scheme, which relies on the semantical relationships between concepts implied by the structure of the hierarchy to annotate each concept-node with a concept-vector (CV). We refer to this approach as CP/CV. Comparison of keyword similarity results shows that CP/CV provides significantly better (upto 33%) results than existing structure-based schemes. Also, even if CP/CV does not assume the availability of an appropriate corpus to extract keyword frequency information, our approach matches (and slightly improves on) the performance of information-based approaches.

#index 907536
#* Secure search in enterprise webs: tradeoffs in efficient implementation for document level security
#@ Peter Bailey;David Hawking;Brett Matson
#t 2006
#c 1
#% 249841
#% 480809
#% 577339
#% 753895
#% 768898
#% 787547
#! Document level security (DLS) -- enforcing permissions prevailing at the time of search -- is specified as a mandatory requirement in many enterprise search applications. Unfortunately, depending upon implementation details and values of key parameters, DLS may come at a high price in increased query processing time, leading to an unacceptably slow search experience. In this paper we present a model and a method for carrying out secure search in the presence of DLS within enterprise webs. We report on two alternative commercial DLS search implementations. Using a 10,000 document experimental DLS environment, we graph the dependence of query processing time on result set size and visibility density for different classes of user. Scaled up to collections of tens of thousands of documents, our results suggest that query times will be unacceptable if exact counts of matching documents are required and also for users who can view only a small proportion of documents. We show that the time to conduct access checks is dramatically increased if requests must be sent off-server, even on a local network, and discuss methods for reducing the cost of security checks. We conclude that enterprises can effectively reduce DLS overheads by organizing documents in such a way that most access checking can be at collection rather than document level, by forgoing accurate match counts, by using caching, batching or hierarchical methods to cut costs of DLS checking and, if applicable, by using a single portal both to access and search documents.

#index 907537
#* Vector and matrix operations programmed with UDFs in a relational DBMS
#@ Carlos Ordonez;Javier García-García
#t 2006
#c 1
#% 152934
#% 248813
#% 273900
#% 280521
#% 300131
#% 300213
#% 342704
#% 413619
#% 443513
#% 464998
#% 654445
#% 765476
#% 766202
#% 771926
#% 797999
#! In general, a relational DBMS provides limited capabilities to perform multidimensional statistical analysis, which requires manipulating vectors and matrices. In this work, we study how to extend a DBMS with basic vector and matrix operators by programming User-Defined Functions (UDFs). We carefully analyze UDF features and limitations to implement vector and matrix operations commonly used in statistics, machine learning and data mining, paying attention to DBMS, operating system and computer architecture constraints. UDFs represent a C programming interface that allows the definition of scalar and aggregate functions that can be used in SQL. UDFs have several advantages and limitations. A UDF allows fast evaluation of arithmetic expressions, memory manipulation, using multidimensional arrays and exploiting all C language control statements. Nevertheless, a UDF cannot perform disk I/O, the amount of heap and stack memory that can be allocated is small and the UDF code must consider specific architecture characteristics of the DBMS. We experimentally compare UDFs and SQL with respect to performance, ease of use, flexibility and scalability. We profile UDFs based on call overhead, memory management and interleaved disk access. We show UDFs are faster than standard SQL aggregations and as fast as SQL arithmetic expressions.

#index 907538
#* POLESTAR: collaborative knowledge management and sensemaking tools for intelligence analysts
#@ Nicholas J. Pioch;John O. Everett
#t 2006
#c 1
#% 452563
#% 740266
#% 868088
#% 881498
#! In this paper, we describe POLESTAR (POLicy Explanation using STories and ARguments), an integrated suite of knowledge management and collaboration tools for intelligence analysts.POLESTAR provides built-in support for analyst workflow, including collection of textual facts from source documents, structured argumentation, and automatic citation in analytic product documents. Underlying POLESTAR is a scalable dependency repository, which provides traceability from product documents to source snippets. The repository's notification engine allows POLESTAR to alert analysts when dependent sources are discredited and aid them in repairing affected arguments. The paper then discusses recent extensions to POLESTAR to support collaborative analysis through community-of-interest finding, portfolio sharing, and peer review of arguments. We conclude with a preview of future research and summary of POLESTAR's primary benefits from the point of view of its deployed users.

#index 907539
#* Task-based process know-how reuse and proactive information delivery in TaskNavigator
#@ Harald Holz;Oleg Rostanin;Andreas Dengel;Takeshi Suzuki;Kaoru Maeda;Katsumi Kanasaki
#t 2006
#c 1
#% 173879
#% 248715
#% 595912
#% 719072
#! Knowledge management approaches for weakly-structured, adhoc knowledge work processes need to be lightweight, i.e., they cannot rely on high upfront modeling efforts. This paper presents TaskNavigator, a novel prototype to support weakly-structured processes by integrating a standard task list application with a state-of-the-art document classification system. The resulting system allows for a task-oriented view on office workers' personal knowledge spaces in order to realize a proactive and contextsensitive information support during daily, knowledge-intensive tasks. Moreover, TaskNavigator supports process know-how reuse by proactively suggesting similar tasks or relevant process models, based on textual similarities. Finally, we report on a feasibility test and a case study that have been conducted in order to evaluate the system in the context of daily research task management and software requirements analysis.

#index 907540
#* Efficient model selection for regularized linear discriminant analysis
#@ Jieping Ye;Tao Xiong;Qi Li;Ravi Janardan;Jinbo Bi;Vladimir Cherkassky;Chandra Kambhamettu
#t 2006
#c 1
#% 80995
#% 190581
#% 224113
#% 235342
#% 309208
#% 793245
#% 829010
#% 857439
#% 876079
#% 881501
#% 1861142
#! Classical Linear Discriminant Analysis (LDA) is not applicable for small sample size problems due to the singularity of the scatter matrices involved. Regularized LDA (RLDA) provides a simple strategy to overcome the singularity problem by applying a regularization term, which is commonly estimated via cross-validation from a set of candidates. However, cross-validation may be computationally prohibitive when the candidate set is large. An efficient algorithm for RLDA is presented that computes the optimal transformation of RLDA for a large set of parameter candidates, with approximately the same cost as running RLDA a small number of times. Thus it facilitates efficient model selection for RLDA.An intrinsic relationship between RLDA and Uncorrelated LDA (ULDA), which was recently proposed for dimension reduction and classification is presented. More specifically, RLDA is shown to approach ULDA when the regularization value tends to zero. That is, RLDA without any regularization is equivalent to ULDA. It can be further shown that ULDA maps all data points from the same class to a common point, under a mild condition which has been shown to hold for many high-dimensional datasets. This leads to the overfitting problem in ULDA, which has been observed in several applications. Thetheoretical analysis presented provides further justification for the use of regularization in RLDA. Extensive experiments confirm the claimed theoretical estimate of efficiency. Experiments also show that, for a properly chosen regularization parameter, RLDA performs favorably in classification, in comparison with ULDA, as well as other existing LDA-based algorithms and Support Vector Machines (SVM).

#index 907541
#* Concept-based document readability in domain specific information retrieval
#@ Xin Yan;Dawei Song;Xue Li
#t 2006
#c 1
#% 308370
#% 605707
#% 766472
#% 882479
#% 1683897
#! Domain specific information retrieval has become in demand. Not only domain experts, but also average non-expert users are interested in searching domain specific (e.g., medical and health) information from online resources. However, a typical problem to average users is that the search results are always a mixture of documents with different levels of readability. Non-expert users may want to see documents with higher readability on the top of the list. Consequently the search results need to be re-ranked in a descending order of readability. It is often not practical for domain experts to manually label the readability of documents for large databases. Computational models of readability needs to be investigated. However, traditional readability formulas are designed for general purpose text and insufficient to deal with technical materials for domain specific information retrieval. More advanced algorithms such as textual coherence model are computationally expensive for re-ranking a large number of retrieved documents. In this paper, we propose an effective and computationally tractable concept-based model of text readability. In addition to textual genres of a document, our model also takes into account domain specific knowledge, i.e., how the domain-specific concepts contained in the document affect the document's readability. Three major readability formulas are proposed and applied to health and medical information retrieval. Experimental results show that our proposed readability formulas lead to remarkable improvements in terms of correlation with users' readability ratings over four traditional readability measures.

#index 907542
#* A probabilistic relevance propagation model for hypertext retrieval
#@ Azadeh Shakery;ChengXiang Zhai
#t 2006
#c 1
#% 25942
#% 54457
#% 73028
#% 129666
#% 186080
#% 214673
#% 224692
#% 255170
#% 262061
#% 268073
#% 288306
#% 290830
#% 300971
#% 311871
#% 340932
#% 342707
#% 342960
#% 420534
#% 641979
#% 643023
#% 648346
#% 769440
#% 769449
#% 799636
#% 818241
#% 818254
#! A major challenge in developing models for hypertext retrieval is to effectively combine content information with the link structure available in hypertext collections. Although several link-based ranking methods have been developed to improve retrieval results, none of them can fully exploit the discrimination power of contents as well as fully exploit all useful link structures. In this paper, we propose a general relevance propagation framework for combining content and link information. The framework gives a probabilistic score to each document defined based on a probabilistic surfing model. Two main characteristics of our framework are our probabilistic view on the relevance propagation model and propagation through multiple sets of neighbors. We compare eight different models derived from the probabilistic relevance propagation framework on two standard TREC Web test collections. Our results show that all the eight relevance propagation models can outperform the baseline content only ranking method for a wide range of parameter values, indicating that the relevance propagation framework provides a general, effective and robust way of exploiting link information. Our experiments also show that using multiple neighbor sets outperforms using just one type of neighbors significantly and taking a probabilistic view of propagation provides guidance on setting propagation parameters.

#index 907543
#* Term context models for information retrieval
#@ Jeremy Pickens;Andrew MacFarlane
#t 2006
#c 1
#% 211044
#% 218978
#% 226495
#% 227819
#% 262096
#% 306497
#% 314739
#% 464434
#% 730140
#% 750863
#% 766414
#% 818262
#% 838412
#! At their heart, most if not all information retrieval models utilize some form of term frequency.The notion is that the more often a query term occurs in a document, the more likely it is that document meets an information need. We examine an alternative. We propose a model which assesses the presence of a term in a document not by looking at the actual occurrence of that term, but by a set of non-independent supporting terms, i.e. context. This yields a weighting for terms in documents which is different from and complementary to tf-based methods, and is beneficial for retrieval.

#index 907544
#* Ranking robustness: a novel framework to predict query performance
#@ Yun Zhou;W. Bruce Croft
#t 2006
#c 1
#% 4430
#% 280864
#% 397161
#% 766408
#% 818267
#% 818298
#% 838472
#! In this paper, we introduce the notion of ranking robustness, which refers to a property of a ranked list of documents that indicates how stable the ranking is in the presence of uncertainty in the ranked documents. We propose a statistical measure called the robustness score to quantify this notion. We demonstrate that the robustness score significantly and consistently correlates with query performance in a variety of TREC test collections including the GOV2 collection. We compare the robustness score with the clarity score method which is the state-of-the-art technique for query performance prediction. Our experimental results show that the robustness score performs better than or at least as good as the clarity score. We find that the clarity score is barely correlated with query performance on the GOV2 collection while the correlation between the robustness score and query performance remains significant. We also notice that a combination of the two usually results in more prediction power.

#index 907545
#* Query result ranking over e-commerce web databases
#@ Weifeng Su;Jiying Wang;Qiong Huang;Fred Lochovsky
#t 2006
#c 1
#% 144070
#% 248801
#% 248857
#% 273901
#% 321635
#% 324129
#% 324192
#% 333947
#% 334001
#% 387427
#% 427219
#% 458860
#% 480302
#% 480418
#% 632060
#% 659990
#% 660011
#% 689389
#% 719598
#% 729437
#% 745519
#% 765464
#% 810013
#% 864432
#% 993957
#% 993987
#% 1015256
#% 1016175
#% 1016176
#% 1016203
#% 1269490
#! To deal with the problem of too many results returned from an E-commerce Web database in response to a user query, this paper proposes a novel approach to rank the query results. Based on the user query, we speculate how much the user cares about each attribute and assign a corresponding weight to it. Then, for each tuple in the query result, each attribute value is assigned a score according to its "desirableness" to the user. These attribute value scores are combined according to the attribute weights to get a final ranking score for each tuple. Tuples with the top ranking scores are presented to the user first. Our ranking method is domain independent and requires no user feedback. Experimental results demonstrate that this ranking method can effectively capture a user's preferences.

#index 907546
#* Optimisation methods for ranking functions with multiple parameters
#@ Michael Taylor;Hugo Zaragoza;Nick Craswell;Stephen Robertson;Chris Burges
#t 2006
#c 1
#% 194284
#% 194301
#% 211820
#% 219050
#% 309095
#% 476873
#% 766409
#% 783474
#% 818239
#% 840846
#% 840882
#! Optimising the parameters of ranking functions with respect to standard IR rank-dependent cost functions has eluded satisfactory analytical treatment. We build on recent advances in alternative differentiable pairwise cost functions, and show that these techniques can be successfully applied to tuning the parameters of an existing family of IR scoring functions (BM25), in the sense that we cannot do better using sensible search heuristics that directly optimize the rank-based cost function NDCG. We also demonstrate how the size of training set affects the number of parameters we can hope to tune this way.

#index 907547
#* Estimating corpus size via queries
#@ Andrei Broder;Marcus Fontura;Vanja Josifovski;Ravi Kumar;Rajeev Motwani;Shubha Nabar;Rina Panigrahy;Andrew Tomkins;Ying Xu
#t 2006
#c 1
#% 268114
#% 281245
#% 298221
#% 309748
#% 319876
#% 340146
#% 375017
#% 413635
#% 480328
#% 805863
#% 807320
#% 869499
#% 1387553
#! We consider the problem of estimating the size of a collection of documents using only a standard query interface. Our main idea is to construct an unbiased and low-variance estimator that can closely approximate the size of any set of documents defined by certain conditions, including that each document in the set must match at least one query from a uniformly sampleable query pool of known size, fixed in advance.Using this basic estimator, we propose two approaches to estimating corpus size. The first approach requires a uniform random sample of documents from the corpus. The second approach avoids this notoriously difficult sample generation problem, and instead uses two fairly uncorrelated sets of terms as query pools; the accuracy of the second approach depends on the degree of correlation among the two sets of terms.Experiments on a large TREC collection and on three major search engines demonstrates the effectiveness of our algorithms.

#index 907548
#* Concept frequency distribution in biomedical text summarization
#@ Lawrence H. Reeve;Hyoil Han;Saya V. Nagori;Jonathan C. Yang;Tamara A. Schwimmer;Ari D. Brooks
#t 2006
#c 1
#% 203462
#% 262112
#% 280835
#% 288614
#% 387427
#% 441307
#% 445287
#% 810743
#% 816173
#% 874084
#% 1291575
#! Text summarization is a data reduction process. The use of text summarization enables users to reduce the amount of text that must be read while still assimilating the core information. The data reduction offered by text summarization is particularly useful in the biomedical domain, where physicians must continuously find clinical trial study information to incorporate into their patient treatment efforts. Such efforts are often hampered by the high-volume of publications. Our contribution is two-fold: 1) to propose the frequency of domain concepts as a method to identify important sentences within a full-text; and 2) propose a novel frequency distribution model and algorithm for identifying important sentences based on term or concept frequency distribution. An evaluation of several existing summarization systems using biomedical texts is presented in order to determine a performance baseline. For domain concept comparison, a recent high-performing frequency-based algorithm using terms is adapted to use concepts and evaluated using both terms and concepts. It is shown that the use of concepts performs closely with the use of terms for sentence selection. Our proposed frequency distribution model and algorithm outperforms a state-of-the-art approach.

#index 907549
#* Describing differences between databases
#@ Heiko Müller;Johann-Christoph Freytag;Ulf Leser
#t 2006
#c 1
#% 84615
#% 201889
#% 227859
#% 248791
#% 273687
#% 350104
#% 479743
#% 481290
#% 481931
#% 729933
#% 765413
#% 768943
#% 810019
#! We study the novel problem of efficiently computing the update distance for a pair of relational databases. In analogy to the edit distance of strings, we define the update distance of two databases as the minimal number of set-oriented insert, delete and modification operations necessary to transform one database into the other. We show how this distance can be computed by traversing a search space of database instances connected by update operations. This insight leads to a family of algorithms that compute the update distance or approximations of it. In our experiments we observed that a simple heuristic performs surprisingly well in most considered cases.Our motivation for studying distance measures for databases stems from the field of scientific databases. There, replicas of a single database are often maintained at different sites, which typically leads to (accidental or planned) divergence of their content. To re-create a consistent view, these differences must be resolved. Such an effort requires an understanding of the process that produced them. We found that minimal update sequences of set-oriented update operations are a proper and concise representation of systematic errors, thus giving valuable clues to domain experts responsible for conflict resolution.

#index 907550
#* A system for query-specific document summarization
#@ Ramakrishna Varadarajan;Vagelis Hristidis
#t 2006
#c 1
#% 194251
#% 211514
#% 230530
#% 262036
#% 280835
#% 288614
#% 309115
#% 316520
#% 330678
#% 387791
#% 397130
#% 397418
#% 464724
#% 479803
#% 539108
#% 654442
#% 741106
#% 742437
#% 746875
#% 754078
#% 755863
#% 757855
#% 766462
#% 783709
#% 815131
#% 824693
#% 838422
#% 854876
#% 855374
#% 863389
#% 993987
#% 1015325
#% 1016176
#% 1272053
#% 1387551
#! There has been a great amount of work on query-independent summarization of documents. However, due to the success of Web search engines query-specific document summarization (query result snippets) has become an important problem, which has received little attention. We present a method to create query-specific summaries by identifying the most query-relevant fragments and combining them using the semantic associations within the document. In particular, we first add structure to the documents in the preprocessing stage and convert them to document graphs. Then, the best summaries are computed by calculating the top spanning trees on the document graphs. We present and experimentally evaluate efficient algorithms that support computing summaries in interactive time. Furthermore, the quality of our summarization method is compared to current approaches using a user survey.

#index 907551
#* Annotation propagation revisited for key preserving views
#@ Gao Cong;Wenfei Fan;Floris Geerts
#t 2006
#c 1
#% 286901
#% 287000
#% 291869
#% 318704
#% 378401
#% 384978
#% 408396
#% 416004
#% 579315
#% 874911
#% 875015
#! This paper revisits the analysis of annotation propagation from source databases to views defined in terms of conjunctive (SPJ) queries. Given a source database D, an SPJ query Q, the view Q(D) and a tuple ΔV in the view, the view (resp. source) side-effect problem is to find a minimal set ΔD of tuples such that the deletion of ΔD from D results in the deletion of ΔV from Q(D) while minimizing the side effects on the view (resp. the source). A third problem, referred to as the annotation placement problem, is to find a single base tuple ΔD such that annotation in a field of ΔD propagates to ΔV while minimizing the propagation to other fields in the view Q(D). These are important for data provenance and the management of view updates. However important, these problems are unfortunately NP-hard for most subclasses of SPJ views [5].To make the annotation propagation analysis feasible in practice, we propose a key preserving condition on SPJ views, which requires that the projection fields of an SPJ view Q retain a key of each base relation involved in Q. While this condition is less restrictive than other proposals [11, 14], it often simplifies the annotation propagation analysis. Indeed, for key-preserving SPJ views the annotation placement problem coincides with the view side-effect problem, and the view and source side-effect problems become tractable. In addition we generalize the setting of [5] by allowing ΔV to be a group of tuples to be deleted, and investigate the insertion of tuples to the view. We show that group updates make the analysis harder: these problems become NP-hard for several subclasses of SPJ views. We also show that for SPJ views the source and view side-effect problems are NP-hard for single-tuple insertion, but are tractable for some subclasses of SPJ for group insertions, in the presence or in the absence of the key preservation condition.

#index 907552
#* Query optimization using restructured views
#@ Rada Chirkova;Fereidoon Sadri
#t 2006
#c 1
#% 102748
#% 213969
#% 223781
#% 248034
#% 273696
#% 300138
#% 333965
#% 342957
#% 464056
#% 479792
#% 479968
#% 480158
#% 481604
#% 481944
#% 482081
#% 495407
#% 564202
#% 572311
#% 599549
#% 772834
#% 810022
#% 1016212
#% 1700143
#! We study optimization of relational queries using materialized views, where views may be regular or restructured. In a restructured view, some data from the base table(s) are represented as metadata - that is, schema information, such as table and attribute names - or vice versa.Using restructured views in query optimization opens up a new spectrum of views that were not previously available, and can result in significant additional savings in query-evaluation costs. These savings can be obtained due to a significantly larger set of views to choose from, and may involve reduced table sizes, elimination of self-joins, clustering produced by restructuring, and horizontal partitioning.In this paper we propose a general query-optimization framework that treats regular and restructured views in a uniform manner and is applicable to SQL select-project-join queries and views with or without aggregation. Within the framework we provide (1) algorithms to determine when a view (regular or restructured) is usable in answering a query, and (2) algorithms to rewrite a query using usable views.Semantic information, such as knowledge of the key of a view, can be used to further optimize a rewritten query. Within our general query-optimization framework, we develop techniques for determining the key of a (regular or restructured) view, and show how this information can be used to further optimize a rewritten query. It is straightforward to integrate all our algorithms and techniques into standard query-optimization algorithms.

#index 907553
#* Improving query I/O performance by permuting and refining block request sequences
#@ Xiaoyu Wang;Mitch Cherniack
#t 2006
#c 1
#% 136740
#% 164748
#% 172052
#% 392275
#% 442705
#% 712361
#% 765417
#! The I/O performance of query processing can be improved using two complementary approaches: improve the buffer and the file system management policies of the DB buffer manager and the OS file system manager (e.g. page replacement), or improve the sequence of requests that are submitted to a file system manager and that lead to actual I/O's (block request sequences). This paper takes the latter approach. Exploiting common file system practices as found in Linux, we propose four techniques for permuting and refining block request sequences: Block-Level I/O Grouping, File-Level I/O Grouping, I/O Ordering, and Block Recycling. To manifest these techniques, we create two new plan operations, MMS and SHJ, each of which adopts some of the block request refinement techniques above. We implement the new plan operations on top of Postgres running on Linux, and show experimental results that demonstrate up to a factor of 4 performance benefit from the use of these techniques.

#index 907554
#* Performance thresholding in practical text classification
#@ Hinrich Schütze;Emre Velipasaoglu;Jan O. Pedersen
#t 2006
#c 1
#% 116165
#% 169717
#% 170649
#% 194284
#% 209021
#% 236729
#% 246831
#% 252011
#% 280817
#% 310503
#% 340941
#% 344447
#% 375017
#% 376266
#% 402289
#% 420054
#% 464268
#% 464466
#% 466419
#% 565531
#% 722797
#% 763708
#% 837668
#% 1558464
#! In practical classification, there is often a mix of learnable and unlearnable classes and only a classifier above a minimum performance threshold can be deployed. This problem is exacerbated if the training set is created by active learning. The bias of actively learned training sets makes it hard to determine whether a class has been learned. We give evidence that there is no general and efficient method for reducing the bias and correctly identifying classes that have been learned. However, we characterize a number of scenarios where active learning can succeed despite these difficulties.

#index 907555
#* Text classification improved through multigram models
#@ Dou Shen;Jian-Tao Sun;Qiang Yang;Zheng Chen
#t 2006
#c 1
#% 67565
#% 103560
#% 279755
#% 311034
#% 318412
#% 458379
#% 465754
#% 568786
#% 642997
#% 719299
#% 735077
#% 748738
#% 763708
#% 939379
#% 939381
#% 1674714
#! Classification algorithms and document representation approaches are two key elements for a successful document classification system. In the past, much work has been conducted to find better ways to represent documents. However, most of the attempts rely on certain extra resources such as WordNet, or they face the problem of extremely high dimension. In this paper, we propose a new document representation approach based on n-multigram language models. This approach can automatically discover the hidden semantic sequences in the documents under each category. Based on n-multigram language models and n-gram language models, we put forward two text classification algorithms. The experiments on RCV1 show that our proposed algorithm based on n-multigram models alone can achieve the similar or even better classification performance compared with the classifier based on n-gram models but the model size of our algorithm is much smaller than that of the latter. Another proposed algorithm based on the combination of n-multigram models and n-gram models improves the micro-F1 and macro-F1 values from 89.5% to 92.6% and 87.2% to 91.1% respectively. All these observations support the validity of our proposed document representation approach.

#index 907556
#* Coupling feature selection and machine learning methods for navigational query identification
#@ Yumao Lu;Fuchun Peng;Xin Li;Nawaaz Ahmed
#t 2006
#c 1
#% 190581
#% 226495
#% 448194
#% 458379
#% 465754
#% 590523
#% 642982
#% 729437
#% 730051
#% 754059
#% 805878
#% 844287
#% 853543
#% 854813
#% 1828377
#! It is important yet hard to identify navigational queries in Web search due to a lack of sufficient information in Web queries, which are typically very short. In this paper we study several machine learning methods, including naive Bayes model, maximum entropy model, support vector machine (SVM), and stochastic gradient boosting tree (SGBT), for navigational query identification in Web search. To boost the performance of these machine techniques, we exploit several feature selection methods and propose coupling feature selection with classification approaches to achieve the best performance. Different from most prior work that uses a small number of features, in this paper, we study the problem of identifying navigational queries with thousands of available features, extracted from major commercial search engine results, Web search user click data, query log, and the whole Web's relational content. A multi-level feature extraction system is constructed.Our results on real search data show that 1) Among all the features we tested, user click distribution features are the most important set of features for identifying navigational queries. 2) In order to achieve good performance, machine learning approaches have to be coupled with good feature selection methods. We find that gradient boosting tree, coupled with linear SVM feature selection is most effective. 3) With carefully coupled feature selection and classification approaches, navigational queries can be accurately identified with 88.1% F1 score, which is 33% error rate reduction compared to the best uncoupled system, and 40% error rate reduction compared to a well tuned system without feature selection.

#index 907557
#* Document re-ranking using cluster validation and label propagation
#@ Lingpeng Yang;Donghong Ji;Guodong Zhou;Yu Nie;Guozheng Xiao
#t 2006
#c 1
#% 218978
#% 262084
#% 298183
#% 329698
#% 398019
#% 766430
#% 783543
#% 818241
#% 818266
#% 819773
#% 838528
#% 848796
#% 939380
#% 1676566
#! This paper proposes a novel document re-ranking approach in information retrieval, which is done by a label propagation-based semi-supervised learning algorithm to utilize the intrinsic structure underlying in the large document data. Since no labeled relevant or irrelevant documents are generally available in IR, our approach tries to extract some pseudo labeled documents from the ranking list of the initial retrieval. For pseudo relevant documents, we determine a cluster of documents from the top ones via cluster validation-based k-means clustering; for pseudo irrelevant ones, we pick a set of documents from the bottom ones. Then the ranking of the documents can be conducted via label propagation. Evaluation on benchmark corpora shows that the approach can achieve significant improvement over standard baselines and performs better than other related approaches.

#index 907558
#* Tracking dragon-hunters with language models
#@ Anton Leuski;Victor Lavrenko
#t 2006
#c 1
#% 217067
#% 340948
#% 342660
#% 397161
#% 458369
#% 781026
#% 848634
#% 850001
#% 918225
#% 995518
#% 1219575
#! We are interested in the problem of understanding the connections between human activities and the content of textual information generated in regard to those activities. Firstly, we define and motivate this problem as an important part in making sense of various life events. Secondly, we introduce the domain of massive online collaborative environments, specifically online virtual worlds, where people meet, exchange messages, and perform actions as a rich data source for such an analysis. Finally, we outline three experimental tasks and show how statistical language modeling and text clustering techniques may allow us to explore those connections successfully.

#index 907559
#* Designing semantics-preserving cluster representatives for scientific input conditions
#@ Aparna S. Varde;Elke A. Rundensteiner;Carolina Ruiz;David C. Brown;Mohammmed Maniruzzaman;Richard D. Sisson
#t 2006
#c 1
#% 316709
#% 330780
#% 340885
#% 449588
#% 461909
#% 729913
#% 769929
#! In scientific domains, knowledge is often discovered from experiments by grouping or clustering them based on the similarity of their output. The causes of similarity are analyzed based on the input conditions characterizing a given type of output, i.e., a given cluster. This analysis helps in applications such as decision support in industry. Cluster representatives form at-a-glance depictions for such applications. Randomly selecting a set of conditions in a cluster as its representative is not sufficient since distinct combinations of inputs could lead to the same cluster. In this paper, an approach called DesCond is proposed to design semantics-preserving cluster representatives for scientific input conditions. We define a notion of distance for conditions to capture semantics based on the types of their attributes and their relative importance. Using this distance, methods of building candidate cluster representatives with different levels of detail are proposed. Candidates are compared using the DesCond Encoding proposed in this paper that assesses their complexity and information loss, given user interests. The candidate with the lowest encoding for each cluster is returned as its designed representative. DesCond is evaluated with real data from Materials Science. Evaluation with domain expert interviews and formal user surveys shows that designed representatives consistently outperform randomly selected ones and different candidates suit different users.

#index 907560
#* Cache-oblivious nested-loop joins
#@ Bingsheng He;Qiong Luo
#t 2006
#c 1
#% 252458
#% 271492
#% 317933
#% 333942
#% 379365
#% 386623
#% 479821
#% 480119
#% 492768
#% 566122
#% 593909
#% 593968
#% 640695
#% 721346
#% 745501
#% 808431
#% 850737
#% 874878
#% 874900
#% 994004
#% 1015288
#! We propose to adapt the newly emerged cache-oblivious model to relational query processing. Our goal is to automatically achieve an overall performance comparable to that of fine-tuned algorithms on a multi-level memory hierarchy. This automaticity is because cache-oblivious algorithms assume no knowledge about any specific parameter values, such as the capacity and block size of each level of the hierarchy. As a first step, we propose recursive partitioning to implement cache-oblivious nested-loop joins (NLJs) without indexes, and recursive clustering and buffering to implement cache-oblivious NLJs with indexes. Our theoretical results and empirical evaluation on three different architectures show that our cache-oblivious NLJs match the performance of their manually optimized, cache-conscious counterparts.

#index 907561
#* A combination of trie-trees and inverted files for the indexing of set-valued attributes
#@ Manolis Terrovitis;Spyros Passas;Panos Vassiliadis;Timos Sellis
#t 2006
#c 1
#% 115466
#% 213786
#% 249989
#% 252608
#% 290703
#% 300120
#% 300169
#% 333950
#% 333981
#% 380546
#% 387427
#% 397151
#% 480926
#% 569755
#% 654454
#% 726628
#% 729418
#% 765463
#% 765466
#! Set-valued attributes frequently occur in contexts like market-basked analysis and stock market trends. Late research literature has mainly focused on set containment joins and data mining without considering simple queries on set valued attributes. In this paper we address superset, subset and equality queries and we propose a novel indexing scheme for answering them on set-valued attributes. The proposed index superimposes a trie-tree on top of an inverted file that indexes a relation with set-valued data. We show that we can efficiently answer the aforementioned queries by indexing only a subset of the most frequent of the items that occur in the indexed relation. Finally, we show through extensive experiments that our approach outperforms the state of the art mechanisms and scales gracefully as database size grows.

#index 907562
#* Efficient join processing over uncertain data
#@ Reynold Cheng;Sarvjeet Singh;Sunil Prabhakar;Rahul Shah;Jeffrey Scott Vitter;Yuni Xia
#t 2006
#c 1
#% 292029
#% 295512
#% 376266
#% 462792
#% 463751
#% 527176
#% 654487
#% 659975
#% 765458
#% 993985
#% 1016178
#% 1016201
#% 1016202
#! In many applications data values are inherently uncertain. This includes moving-objects, sensors and biological databases. There has been recent interest in the development of database management systems that can handle uncertain data. Some proposals for such systems include attribute values that are uncertain. In particular, an attribute value can be modeled as a range of possible values, associated with a probability density function. Previous efforts for this type of data have only addressed simple queries such as range and nearest-neighbor queries. Queries that join multiple relations have not been addressed in earlier work despite the significance of joins in databases. In this paper we address join queries over uncertain data. We propose a semantics for the join operation, define probabilistic operators over uncertain data, and propose join algorithms that provide efficient execution of probabilistic joins. The paper focuses on an important class of joins termed probabilistic threshold joins that avoid some of the semantic complexities of dealing with uncertain data. For this class of joins we develop three sets of optimization techniques: item-level, page-level, and index-level pruning. These techniques facilitate pruning with little space and time overhead, and are easily adapted to most join algorithms. We verify the performance of these techniques experimentally.

#index 907563
#* An integer programming approach for frequent itemset hiding
#@ Aris Gkoulalas-Divanis;Vassilios S. Verykios
#t 2006
#c 1
#% 248791
#% 338609
#% 428404
#% 481290
#% 575966
#% 576214
#% 586838
#% 635220
#% 781938
#% 835074
#% 844335
#% 958757
#! The rapid growth of transactional data brought, soon enough, into attention the need of its further exploitation. In this paper, we investigate the problem of securing sensitive knowledge from being exposed in patterns extracted during association rule mining. Instead of hiding the produced rules directly, we decide to hide the sensitive frequent itemsets that may lead to the production of these rules. As a first step, we introduce the notion of distance between two databases and a measure for quantifying it. By trying to minimize the distance between the original database and its sanitized version (that can safely be released), we propose a novel, exact algorithm for association rule hiding and evaluate it on real world datasets demonstrating its effectiveness towards solving the problem.

#index 907564
#* Privacy preserving sequential pattern mining in distributed databases
#@ V. Kapoor;P. Poncelet;F. Trousset;M. Teisseire
#t 2006
#c 1
#% 54180
#% 310559
#% 329537
#% 346201
#% 452864
#% 459006
#% 463903
#% 464996
#% 477791
#% 477813
#% 575967
#% 575969
#% 577256
#% 577289
#% 635219
#% 751578
#% 823389
#! Research in the areas of privacy preserving techniques in databases and subsequently in privacy enhancement technologies have witnessed an explosive growth-spurt in recent years. This escalation has been fueled by the growing mistrust of individuals towards organizations collecting and disbursing their Personally Identifiable Information (PII). Digital repositories have become increasingly susceptible to intentional or unintentional abuse, resulting in organizations to be liable under the privacy legislations that are being adopted by governments the world over. These privacy concerns have necessitated new advancements in the field of distributed data mining wherein, collaborating parties may be legally bound not to reveal the private information of their customers. In this paper, we present a new algorithm PriPSeP (Privacy Preserving SEquential Patterns) for the mining of sequential patterns from distributed databases while preserving privacy. A salient feature of PriPSeP is that due to its flexibility it is more pertinent to mining operations for real world applications in terms of efficiency and functionality. Under some reasonable assumptions, we prove that our architecture and protocol employed by our algorithm for multi-party computation is secure.

#index 907565
#* A dictionary for approximate string search and longest prefix search
#@ Sreenivas Gollapudi;Rina Panigrahy
#t 2006
#c 1
#% 8391
#% 69350
#% 249321
#% 249322
#% 311808
#% 322884
#% 333679
#% 340175
#% 443055
#% 616528
#% 642409
#% 765262
#% 785131
#% 805746
#% 847168
#! In this paper we propose a dictionary data structure for string search with errors where the query string may didiffer from the expected matching string by a few edits. This data structure can also be used to find the database string with the longest common prefix with few errors. Specifically, with a database of n random strings, each of length of O(m), we show how to perform string search on a query string that differs from its closest match by k edits using a data structure of linear size and query time equal to Õ(log n 2 log n klog a 2m over 2m). This means that if k m over log a 2m log n, then the query time is Õ(1). This is of significant in practice as there are several applications where k is small relative to m. Our approach converts strings into bit vectors so that similar strings can map to similar bit vectors with small hamming distance. A simple reduction can be used to obtain similar results for approximate longest prefix search.

#index 907566
#* A comparative study on classifying the functions of web page blocks
#@ Xiangye Xiao;Qiong Luo;Xing Xie;Wei-Ying Ma
#t 2006
#c 1
#% 330765
#% 754078
#! In this paper, we study the problem of learning block classification models to estimate block functions. We distinguish general models, which are learned across multiple sites, and site-specific models, which are learned within individual sites. We further consider several factors that affect the learning process and model effectiveness. These factors include the layout features, the content features, the classifiers, and the term selection methods. We have empirically evaluated the performance of the models when the factors are varied. Our main results are that layout features do better than content features for learning both general and site-specific models.

#index 907567
#* A neighborhood-based approach for clustering of linked document collections
#@ Ralitsa Angelova;Stefan Siersdorfer
#t 2006
#c 1
#% 338741
#% 879625
#! This paper addresses the problem of automatically structuring linked document collections by using clustering. In contrast to traditional clustering, we study the clustering problem in the light of available link structure information for the data set (e.g., hyperlinks among web documents or co-authorship among bibliographic data entries). Our approach is based on iterative relaxation of cluster assignments, and can be built on top of any clustering algorithm. This technique results in higher cluster purity, better overall accuracy, and make self-organization more robust.

#index 907568
#* A structure-oriented relevance feedback method for XML retrieval
#@ Lobna Hlaoua;Karen Sauvagnat;Mohand Boughanem
#t 2006
#c 1
#% 838388
#! Relevance Feedback (RF) is a technique allowing to enrich an initial query according to the user feedback. The goal is to express more precisily the user's needs. Some open issues appear when considering semi-structured documents like XML documents. Most of the RF approaches proposed in XML retrieval are simple adaptations of traditional RF to the new granularity of information. They enrich queries by adding terms extracted from relevant elements instead of terms extracted from whole documents. In this paper we show how structural constraints can also be used in RF. We propose a new approach that is able to extend the initial query by adding one or more generative structures. This approach is applied to unstructured queries. Experiments are carried out on INEX collection and results show the interest of our method.

#index 907569
#* Adapting association patterns for text categorization: weaknesses and enhancements
#@ Tieyun Qian;Hui Xiong;Yuanzhen Wang;Enhong Chen
#t 2006
#c 1
#% 152934
#% 260001
#% 465895
#% 466483
#% 727897
#% 807396
#% 818217
#! The use of association patterns for text categorization has attracted great interest and a variety of useful methods have been developed. However, the key characteristics of pattern-based text categorization remain unclear. Indeed, there are still no concrete answers for the following two questions: First, what kind of association patterns are the best candidate for pattern-based text categorization? Second, what is the most desirable way to use patterns for text categorization? In this paper, we focus on answering the above two questions. Specifically, we show that hyperclique patterns are more desirable than frequent patterns for text categorization. Along this line, we develop an algorithm for text categorization using hyperclique patterns. The experimental results show that our method provides better performance than state-of-the-art methods in terms of both computational performance and classification accuracy.

#index 907570
#* Amnesic online synopses for moving objects
#@ Michalis Potamias;Kostas Patroumpas;Timos Sellis
#t 2006
#c 1
#% 745486
#% 745513

#index 907571
#* An efficient one-phase holistic twig join algorithm for XML data
#@ Zhewei Jiang;Cheng Luo;Wen-Chi Hou
#t 2006
#c 1
#% 397375
#% 659999
#! In view of the inefficiency of the traditional two-phase Twig-Stack algorithm, we propose a single-phase holistic twig pattern matching method based on the TwigStack algorithm by applying a novel stack structure.

#index 907572
#* Approximate reverse k-nearest neighbor queries in general metric spaces
#@ Elke Achtert;Christian Böhm;Peer Kröger;Peter Kunath;Alexey Pryakhin;Matthias Renz
#t 2006
#c 1
#% 730019
#% 838510
#% 875013
#! In this paper, we propose an approach for efficient approximative RkNN search in arbitrary metric spaces where the value of k is specified at query time. Our method uses an approximation of the nearest-neighbor-distances in order to prune the search space. In several experiments, our solution scales significantly better than existing non-approximative approaches while producing an approximation of the true query result with a high recall.

#index 907573
#* Best-k queries on database systems
#@ Tao Tao;ChengXiang Zhai
#t 2006
#c 1
#% 213981
#% 642975

#index 907574
#* Boosting relevance model performance with query term dependence
#@ Koji Eguchi;W. Bruce Croft
#t 2006
#c 1
#% 340901
#% 789959
#% 818262
#% 1682097
#% 1715627

#index 907575
#* Collaborative filtering in dynamic usage environments
#@ Olfa Nasraoui;Jeff Cerwinske;Carlos Rojas;Fabio Gonzalez
#t 2006
#c 1
#% 220711
#% 584891
#% 630984
#% 727930
#% 823369
#% 1015261

#index 907576
#* Automatically constructing collections of online database directories
#@ Luciano Barbosa;Juliana Freire
#t 2006
#c 1
#% 281251
#% 376266
#% 765409
#% 875039

#index 907577
#* Combining feature selectors for text classification
#@ J. Olsson Scott Olsson;Douglas W. Oard
#t 2006
#c 1
#% 156421
#% 413637
#% 465754
#% 722935
#% 740900
#% 763708
#% 879683
#! We introduce several methods of combining feature selectors for text classification. Results from a large investigation of these combinations are summarized. Easily constructed combinations of feature selectors are shown to improve peak R-precision and F1 at statistically significant levels.

#index 907578
#* Constructing better document and query models with markov chains
#@ Guihong Cao;Jian-Yun Nie;Jing Bai
#t 2006
#c 1
#% 262096
#% 280851
#% 340901
#% 342707
#% 818240
#% 838530
#% 838532
#! Document and query expansions have been used separately in previous studies to enhance the representation of documents and queries. In this paper, we propose a general method that integrates both of them. Expansion is carried out using multi-stage Markov chains. Our experiments show that this method significantly outperforms the existing approaches.

#index 907579
#* Continuous keyword search on multiple text streams
#@ Vagelis Hristidis;Oscar Valdivia;Michail Vlachos;Philip S. Yu
#t 2006
#c 1
#% 397418
#% 654444
#% 993987
#% 1015296
#! In this paper we address the issue of continuous keyword queries on multiple textual streams. This line of work represents a significant departure from previous keyword search models that assumed a static database. In our model the user poses a query comprised by a collection of keywords, which is subsequently applied on multiple text streams (these can be RSS news feeds, TV closed captions, emails, etc). A result to a query is a combination of streams "sufficiently correlated" to each other that collectively contain all query keywords within a specified time span.

#index 907580
#* On subspace clustering with density consciousness
#@ Yi-Hong Chu;Jen-Wei Huang;Kun-Ta Chuang;Ming-Syan Chen
#t 2006
#c 1
#% 248792
#% 280417
#% 300120
#! In this paper, a problem, called "the density divergence problem" is explored. This problem is related to the phenomenon that the densities of the clusters vary in different subspace cardinalities. We take the densities into consideration in subspace clustering and explore an algorithm to adaptively determine different density thresholds to discover clusters in different subspace cardinalities.

#index 907581
#* Direct comparison of commercial and academic retrieval system: an initial study
#@ Yefei Peng;Daqing He
#t 2006
#c 1

#index 907582
#* Effective and efficient similarity search in time series
#@ Sergio Greco;Massimiliano Ruffolo;Andrea Tagarelli
#t 2006
#c 1
#% 36672
#% 137711
#% 310545
#% 375017
#% 659971
#% 809264
#% 810049
#% 1016195
#! We present DSA - Derivative time series Segment Approximation, a novel representation model for time series designed for effective and efficient similarity search. DSA substantially exploits derivative estimation, segmentation and dimensionality reduction to meet at least the requirements of high sensitivity to main features (trends) of time series and robustness to outliers. Experiments show that DSA is drastically faster and still as good or better than the prominent state-of-the-art similarity methods.

#index 907583
#* Efficient mining of max frequent patterns in a generalized environment
#@ Daniel Kunkle;Donghui Zhang;Gene Cooperman
#t 2006
#c 1
#% 248791
#% 481758
#! This poster paper summarizes our solution for mining max frequent generalized itemsets (g-itemsets), a compact representation for frequent patterns in the generalized environment.

#index 907584
#* Estimation, sensitivity, and generalization in parameterized retrieval models
#@ Donald Metzler
#t 2006
#c 1
#% 397129
#% 766412
#! In this work we investigate three important aspects of parameterized retrieval models: estimation, sensitivity, and generalization. Since all parameterized models, even those based on heuristics, have inherent uncertainty, we study these issues using statistical tools.

#index 907585
#* Filtering or adapting: two strategies to exploit noisy parallel corpora for cross-language information retrieval
#@ Lixin Shi;Jian-Yun Nie
#t 2006
#c 1
#% 280826
#% 735135
#% 740901
#% 740915
#% 786575
#% 1698597
#! Noisy parallel corpora have been widely used for Cross-language information retrieval (CLIR). However, the previous studies only focus on truly parallel corpus. In this paper, we examine two possible approaches to exploit noisy corpora: filtering out noise from the corpora or adapting the training process of translation model to the noise corpora. Our experiments show that the second approach is better suited to CLIR.

#index 907586
#* HUX: a schemacentric approach for updating XML views
#@ Ling Wang;Elke A. Rundensteiner;Murali Mani;Ming Jiang
#t 2006
#c 1
#% 318704
#% 333979
#% 1016152

#index 907587
#* Improving query translation with confidence estimation for cross language information retrieval
#@ Youssef Kadri;Jian-Yun Nie
#t 2006
#c 1
#% 217207
#% 561166
#% 807747
#% 939694

#index 907588
#* Information retrieval from relational databases using semantic queries
#@ Anand Ranganathan;Zhen Liu
#t 2006
#c 1

#index 907589
#* Integrated RFID data modeling: an approach for querying physical objects in pervasive computing
#@ Shaorong Liu;Fusheng Wang;Peiya Liu
#t 2006
#c 1
#% 824747
#% 1016228
#% 1688279

#index 907590
#* Integration of cluster ensemble and EM based text mining for microarray gene cluster identification and annotation
#@ Xiaohua Hu;Xiaodan Zhang;Xiaohua Zhou
#t 2006
#c 1
#% 311027
#% 762808
#! In this paper, we design and develop a unified system GE-Miner (Gene Expression Miner) to integrate cluster ensemble, text clustering and multi document summarization and provide an environment for comprehensive gene expression data analysis. We present a novel cluster ensemble approach to generate high quality gene cluster. In our text summarization module, given a gene cluster, our Expectation Maximization (EM) based algorithm can automatically identify subtopics and extract most probable terms for each topic. Then, the extracted top k topical terms from each subtopic are combined to form the biological explanation of each gene cluster. Experimental results demonstrate that our system can obtain high quality clusters and provide informative key terms for the gene clusters.

#index 907591
#* Introduction to a new Farsi stemmer
#@ Alireza Mokhtaripour;Saber Jahanpour
#t 2006
#c 1
#% 804416
#! In this poster, a new Farsi (also called Persian) stemmer which works without dictionary is introduced. Evaluation results show significant improvement in performance (precision/recall) of the Information Retrieval (IR) system using this stemmer.

#index 907592
#* IR principles for content-based indexing and retrieval of functional brain images
#@ Bing Bai;Paul Kantor;Nicu Cornea;Deborah Silver
#t 2006
#c 1
#% 46803
#! In this paper, we explore the concept of a "library of brain images", which implies not only a repository of brain images, but also efficient search and retrieval mechanisms that are based on models derived from IR practice. As a preliminary study, we have worked with a collection of functional MRI brain images assembled in the study of several distinct cognitive tasks. We adapt several classical IR methods (inverted indexing, TFIDF and Latent Semantic Indexing(LSI)) to content-based brain image retrieval. Our results show that efficient and accurate retrieval of brain images is possible, and that representations motivated by the IR perspective are somewhat more effective than are methods based on retaining the full image information.

#index 907593
#* Matching directories and OWL ontologies with AROMA
#@ Jérôme David;Fabrice Guillet;Henri Briand
#t 2006
#c 1
#% 152934
#% 232102
#% 572314
#% 1705177
#! This paper presents a simple and adaptable matching method dealing with web directories, catalogs and OWL ontologies. By using a well-known Knowledge Discovery in Databases model, such as the association rule paradigm, this method has the originality to be both extensional and asymmetric. It works at the terminological level (by selecting concept-relevant terms contained in documents) and permits to discover equivalence and also subsumption relations holding between entities (concepts and properties). This method relies on the implication intensity measure, a probabilistic model of deviation from independence. Selection of significant rules between concepts (or properties) is lead by two criteria permitting to assess respectively the implication quality and the generativity of the rule. Finally, the proposed method is evaluated on two benchmarks. The first contains two conceptual hierarchies populated with textual documents and the second one is composed of OWL ontologies.

#index 907594
#* Matching and evaluation of disjunctive predicates for data stream sharing
#@ Richard Kuntschke;Alfons Kemper
#t 2006
#c 1
#! New optimization techniques, e.g., in data stream management systems (DSMSs), make the treatment of disjunctive predicates a necessity. In this paper, we introduce and compare methods for matching and evaluating disjunctive predicates.

#index 907595
#* Maximizing the sustained throughput of distributed continuous queries
#@ Ioana Stanoi;George Mihaila;Themis Palpanas;Christian Lang
#t 2006
#c 1
#% 397352
#% 654497
#% 654510
#% 809256
#% 864436
#% 1016167
#! Monitoring systems today often involve continuous queries over streaming data, in a distributed collaborative system. The distribution of query operators over a network of processors, and their processing sequence, form a query configuration with inherent constraints on the throughput it can support. In this paper we propose to optimize stream queries with respect to a version of throughput measure, the profiled input throughput. This measure is focused on matching the expected behavior of the input streams. To prune the search space we used hill-climbing techniques that proved to be efficient and effective.

#index 907596
#* Measuring the meaning in time series clustering of text search queries
#@ Bing Liu;Rosie Jones;Kristina Lisa Klinkner
#t 2006
#c 1
#% 805839
#% 869501
#! We use a combination of proven methods from time series analysis and machine learning to explore the relationship between temporal and semantic similarity in web query logs; we discover that the combination of correlation and cycles is a good, but not perfect, sign of semantic relationship.

#index 907597
#* Mining coherent patterns from heterogeneous microarray data
#@ Xiang Zhang;Wei Wang
#t 2006
#c 1
#% 397382
#% 810066
#% 864476
#! Microarray technology is a powerful tool for geneticists to monitor interactions among tens of thousands of genes simultaneously. There has been extensive research on coherent subspace clustering of gene expressions measured under consistent experimental settings. However, these methods assume that all experiments are run using the same batch of microarray chips with similar characteristics of noise. Algorithms developed under this assumption may not be applicable for analyzing data collected from heterogeneous settings, where the set of genes being monitored may be different and expression levels may be not directly comparable even for the same gene. In this paper, we propose a model, F-cluster, for mining subspace coherent patterns from heterogeneous gene expression data. We compare our model with previously proposed models. We analyze the search space of the problem and give a naïve solution for it.

#index 907598
#* k nearest neighbor classification across multiple private databases
#@ Li Xiong;Subramanyam Chitti;Ling Liu
#t 2006
#c 1
#% 575969
#% 742048
#% 812755
#! Distributed privacy preserving data mining tools are critical for mining multiple databases with a minimum information disclosure. We present a framework including a general model as well as multi-round algorithms for mining horizontally partitioned databases using a privacy preserving k Nearest Neighbor (kNN) classifier.

#index 907599
#* Modeling performance-driven workload characterization of web search systems
#@ Claudine Badue;Ricardo Baeza-Yates;Berthier Ribeiro-Neto;Artur Ziviani;Nivio Ziviani
#t 2006
#c 1
#% 578337
#% 730066
#% 752403
#% 778661
#% 801838
#% 818279
#% 851306
#% 943037

#index 907600
#* Multi-query optimization of sliding window aggregates by schedule synchronization
#@ Lukasz Golab;Kumar Gaurav Bijay;M. Tamer Özsu
#t 2006
#c 1

#index 907601
#* Multi-task text segmentation and alignment based on weighted mutual information
#@ Bingjun Sun;Ding Zhou;Hongyuan Zha;John Yen
#t 2006
#c 1
#% 236497
#% 280819
#% 643015
#% 729918
#% 742204
#% 815855
#% 840840
#! Text segmentation is important for text analysis, while text alignment is to determine shared sub-topics among similar documents. Multi-task text segmentation and alignment is the extension of single-task segmentation to utilize information of multi-source documents. In this paper we introduce a novel domain-independent unsupervised method for multi-task segmentation and alignment based on the idea that the optimal segmentation and alignment maximizes weighted mutual information, mutual information with term weights. The experiment results show that our approach works well.

#index 907602
#* PEPX: a query-friendly probabilistic XML database
#@ Te Li;Qihong Shao;Yi Chen
#t 2006
#c 1
#% 397375
#% 480489
#% 765405
#% 800547
#% 993985

#index 907603
#* On progressive sequential pattern mining
#@ Jen-Wei Huang;Chi-Yao Tseng;Jian-Chih Ou;Ming-Syan Chen
#t 2006
#c 1
#% 287242
#% 463903
#% 577256
#% 769931
#! When sequential patterns are generated, the newly arriving patterns may not be identified as frequent sequential patterns due to the existence of old data and sequences. In practice, users are usually more interested in the recent data than the old ones. To capture the dynamic nature of data addition and deletion, we propose a general model of sequential pattern mining with a progressive database. In addition, we present a progressive concept to progressively discover sequential patterns in recent time period of interest.

#index 907604
#* Practical private data matching deterrent to spoofing attacks
#@ Yanjiang Yang;Robert H. Deng;Feng Bao
#t 2006
#c 1
#% 31036
#% 99610
#% 514482
#% 654448
#% 1718378
#! Private data matching between the data sets of two potentially distrusted parties has a wide range of applications. However, existing solutions have substantial weaknesses and do not meet the needs of many practical application scenarios. In particular, practical private data matching applications often require discouraging the matching parties from spoofing their private inputs. In this paper, we address this challenge by forcing the matching parties to "escrow" the data they use for matching to an auditorial agent, and in the "after-the-fact" period, they undertake the liability to attest the genuineness of the escrowed data.

#index 907605
#* Probabilistic document-context based relevance feedback with limited relevance judgments
#@ H. C. Wu;R. W. P. Luk;K. F. Wong;K. L. Kwok
#t 2006
#c 1
#% 118726
#% 169781
#% 169809
#% 176550
#% 194301
#% 309088
#% 340901
#% 342707
#% 578875
#% 766518
#% 818322
#! This paper presents our novel relevance feedback (RF) algorithm that uses the probabilistic document-context based retrieval model with limited relevance judgments for document re-ranking. Probabilities of the document-context based retrieval model are estimated from the top N (=20) documents in the initial retrieval. We use document-context based cosine similarity measure to find similar data for better probability estimation in order to reduce the data scarcity problem and the negative weighting problem. Our RF algorithm is promising because its mean average precision is statistically significantly better than the baseline using TREC-6 and TREC-7 data collections.

#index 907606
#* Processing information intent via weak labeling
#@ Anthony Tomasic;Isaac Simmons;John Zimmerman
#t 2006
#c 1
#% 848638
#% 1289515

#index 907607
#* Pseudo-anchor text extraction for searching vertical objects
#@ Shuming Shi;Fei Xing;Mingjie Zhu;Zaiqing Nie;Ji-Rong Wen
#t 2006
#c 1
#% 249143
#% 268073
#% 309095
#% 805896
#! This paper examines the problem of utilizing pseudo-anchor text to help ranking Web objects in vertical search. We adopt a machine learning based approach to extract pseudo-anchor text for a vertical object from its candidate anchor blocks. Experiments in academic search domain indicate that our approach is able to dramatically improve search performance.

#index 907608
#* Re-ranking search results using query logs
#@ Ziming Zhuang;Silviu Cucerzan
#t 2006
#c 1
#% 284796
#% 309095
#% 348155
#% 643028
#% 754125
#% 769421
#! This work addresses two common problems in search, frequently occurring with underspecified user queries: the top-ranked results for such queries may not contain documents relevant to the user's search intent, and fresh and relevant pages may not get high ranks for an underspecified query due to their freshness and to the large number of pages that match the query, despite the fact that a large number of users have searched for parts of their content recently. We propose a novel method, Q-Rank, to effectively refine the ranking of search results for any given query by constructing the query context from search query logs. Evaluation results show that Q-Rank gains a considerable advantage over the current ranking system of a large-scale commercial Web search engine, being able to improve the relevance of search results for 82% of the queries.

#index 907609
#* Query taxonomy generation for web search
#@ Pu-Jeng Cheng;Ching-Hsiang Tsai;Chen-Ming Hung;Lee-Feng Chien
#t 2006
#c 1
#% 344926
#% 754076
#% 754124
#! We propose an approach that organizes the search-result clusters into a hierarchical structure, called a query taxonomy, from the user's perspective. The proposed approach is based on an unsupervised classification method, which uses the dynamic Web as the training corpus. With query taxonomy, users can browse relevant Web documents more conveniently and comprehensibly. Our experimental results verify the feasibility and the effectiveness of the proposed approach to query taxonomy generation in Web search.

#index 907610
#* Rank synopses for efficient time travel on the web graph
#@ Klaus Berberich;Srikanta Bedathur;Gerhard Weikum
#t 2006
#c 1
#% 466506
#% 754088

#index 907611
#* Ranking in context using vector spaces
#@ Massimo Melucci
#t 2006
#c 1
#% 92696
#% 218982
#% 318415
#% 321635
#% 448843
#% 758200
#% 822126
#% 857180

#index 907612
#* Representing documents with named entities for story link detection (SLD)
#@ Chirag Shah;W. Bruce Croft;David Jensen
#t 2006
#c 1
#% 67565
#% 262096
#% 278107
#% 340901
#% 575570
#% 838532
#! Several information organization, access, and filtering systems can benefit from different kind of document representations than those used in traditional Information Retrieval (IR). Topic Detection and Tracking (TDT) is an example of such an application. In this paper we demonstrate that named entities serve as better choices of units for document representation over all words. In order to test this hypothesis we study the effect of words-based and entity-based representations on Story Link Detection (SLD) - a core task in TDT research. The experiments on TDT corpora show that entity-based representations give significant improvements for SLD. We also propose a mechanism to expand the set of named entities used for document representation, which enhances the performance in some cases. We then take a step further and analyze the limitations of using only named entities for the document representation. Our studies and experiments indicate that adding additional topical terms can help in addressing such limitations.

#index 907613
#* Resource-aware kernel density estimators over streaming data
#@ Christoph Heinz;Bernhard Seeger
#t 2006
#c 1
#% 587754
#% 824795
#% 885375
#! A fundamental building block of many data mining and analysis approaches is density estimation as it provides a comprehensive statistical model of a data distribution. For that reason, its application to transient data streams is highly desirable. A convenient, nonparametric method for density estimation utilizes kernels. However, its computational complexity collides with the rigid processing requirements of data streams. In this work, we present a new approach to this problem that combines linear processing cost with a constant amount of allocated memory. Our approach also supports a dynamic memory adaptation to changing system resources.

#index 907614
#* Retrieval evaluation with incomplete relevance data: a comparative study of three measures
#@ Per Ahlgren;Leif Grönqvist
#t 2006
#c 1
#% 561315
#% 766409

#index 907615
#* Robust periodicity detection algorithms
#@ S. Parthasarathy;S. Mehta;S. Srinivasan
#t 2006
#c 1
#! Periodicity detection is an important pre-processing step for many time series algorithms. It provides important information about the structural properties of a time series. Feature vectors based on periodicity can be used for clustering, classification, abnormality detection, and human motion understanding. The periodicity detection task is not difficult in case of simple and uncontaminated signal. Unfortunately, most of the real datasets exhibit one or more of the following properties: i) non-stationarity, ii) interlaced cyclic patterns and iii) data contamination, which makes the period detection extremely challenging. A seemingly straightforward solution is to develop individual specialized algorithms for handling each case separately. However, determining if a time series is non-stationary or is contaminated in itself is an extremely difficult task. In this article, we propose generic algorithms which can detect periods in complex, noisy and incomplete datasets. The algorithm leverages the frequency characterization and autocorrelation structure inherent in a time series to estimate its periodicity. We extend the methods to handle non-stationary time series by tracking the candidate periods using a Kalman filter. We also address the interesting problem of finding multiple interlaced periodicities.

#index 907616
#* Search result summarization and disambiguation via contextual dimensions
#@ Krishna P Chitrapura;Sachindra Joshi;Raghu Krishnapuram
#t 2006
#c 1
#% 152934
#% 281186
#% 340951
#% 342717
#! Topic hierarchies are a popular method of summarizing the results obtained in response to a query in various search applications. However, topic hierarchies are rigid when they are pre-defined and somewhat unintuitive when they are dynamically generated by statistical techniques. In this paper, we propose an alternative approach to query disambiguation and result summarization by placing the results in set of contextual dimensions which can be viewed as facets. For the generic search scenario, we illustrate our approach by using three types of contextual dimensions, namely, concepts, features, and specializations. We use NLP techniques and a data mining algorithm to select distinct contexts.

#index 907617
#* Semi-automatic annotation and MPEG-7 authoring of dance videos
#@ Balakrishnan Ramadoss;Kannan Rajkumar
#t 2006
#c 1
#% 451603
#% 727493
#% 741409
#! This paper presents a system (DanVideo) that is implemented using J2SE and JMF to annotate manually the macro and micro features of the dance videos by the dance experts. As MPEG-7 has reached a matured state for the description of the multimedia structure and semantics through the Descriptors and Description Schemes, DanVideo generates a MPEG-7 instance that conforms to the MPEG-7 schema, semi-automatically and effortlessly from the dance annotations.

#index 907618
#* The query-vector document model
#@ Diego Puppin;Fabrizio Silvestri
#t 2006
#c 1
#% 194246
#% 255137
#% 729918
#% 878657

#index 907619
#* The visual funding navigator: analysis of the NSF funding information
#@ Shixia Liu;Nan Cao;Hao Lv;Hui Su
#t 2006
#c 1
#% 446242
#% 577025
#% 726086
#% 802001
#% 844509
#! This paper presents an interactive visualization toolkit for navigating and analyzing the National Science Foundation (NSF) funding information. Our design builds upon an improved 2.5D treemap layout and the stacked graph to contribute customized techniques for visually navigating and interacting with the hierarchical data of NSF programs and proposals. Furthermore, an incremental layout method is adopted to handle information on a large scale. The improved treemap visualization will help to visually analyze the static funding related data and the stacked graph is utilized to analyze the time-series data. Through these visual analysis techniques, research trends of NSF, popular NSF programs are quickly identified.

#index 907620
#* Towards interactive indexing for large Chinese calligraphic character databases
#@ Yi Zhuang;Yueting Zhuang;Qing Li;Lei Chen
#t 2006
#c 1
#% 210173
#% 342828
#% 1677496
#! In this paper, based on a novel shape-similarity-based retrieval method, we propose an interactive partial-distance-map (PDM)- based high-dimensional indexing scheme to speed up the retrieval performance of the large Chinese calligraphic character databases. Specifically, we use the approximate minimal bounding hyper- sphere of query character to search the PDM and utilize the users' relevance feedback to refine the search process. We conduct comprehensive experiments to testify the efficiency and effectiveness of the proposed method.

#index 907621
#* Query-specific clustering of search results based on document-context similarity scores
#@ E. K. F. Dang;R. W. P. Luk;D. L. Lee;K. S. Ho;S. C. F. Chan
#t 2006
#c 1
#% 169781
#% 319273
#% 375017
#% 427921
#% 818322
#! This paper presents a pilot study of query-specific clustering that uses our novel document-context based similarity scores as compared with document similarity scores. Clustering is applied to the top 1000 retrieved documents for a given query. Clustering effectiveness is evaluated based on the MK1 score for TREC-2, TREC-6 and TREC-7 test collections. Encouraging results were obtained whereby document-context clustering produces better MK1 scores than document clustering with a 95% confidence level if precision and recall are equally important.

#index 1016296
#* Natural language processing for information retrieval: the time is ripe (again)
#@ Matthew Lease
#t 2007
#c 1
#% 210986
#% 262096
#% 287253
#% 324192
#% 340901
#% 375017
#% 413593
#% 501924
#% 572478
#% 740916
#% 750863
#% 766412
#% 766428
#% 815808
#% 818239
#% 818240
#% 838530
#% 879571
#% 940010
#% 976952
#% 1077150
#% 1250397
#% 1250428
#% 1767698
#% 1916129
#! Paraphrasing van Rijsbergen [37], the time is ripe for another attempt at using natural language processing (NLP) for information retrieval (IR). This paper introduces my dissertation study, which will explore methods for integrating modern NLP with state-of-the-art IR techniques. In addition to text, I will also apply retrieval to conversational speech data, which poses a unique set of considerations in comparison to text. Greater use of NLP has potential to improve both text and speech retrieval.

#index 1016297
#* Reusing relational sources for semantic information access
#@ Lina Lubyte
#t 2007
#c 1
#% 89751
#% 106916
#% 169324
#% 326595
#% 384978
#% 386381
#% 463744
#% 480134
#% 480969
#% 535187
#% 572314
#% 665859
#% 720186
#% 824763
#% 864622
#% 867891
#% 1138514
#% 1269453
#% 1289168
#% 1705177
#% 1725994
#! The rapid growth of available data arises the need for more sophisticated techniques for semantic access to information. It has been proved that using conceptual model or ontology over relational data sources is necessary to overcome many problems related with accessing the structured data. However, the task of wrapping the data residing in a database by means of an ontology is mainly done manually. The research we are carrying out studies the reuse of relational sources in the context of semantics-based access to information. This problem is tackled in two phases: (i) extracting semantics hidden in the relational sources by wrapping them by means of an ontology, (ii) understanding the methodology for semantic extension of such ontologies. In this paper we focus on the first sub-problem and present an automatic procedure for extracting from a relational database schema its conceptual view. The semantic mapping between the database and its conceptualization is captured by associating views over the data source to elements of the extracted ontology. To represent the extracted conceptual model we use an ontology language, rather that a graphical notation, in order to provide precise formal semantics. Our approach uses heuristics based on ideas of standard relational schema design and normalization. Under this we formally prove that our technique preserves the semantics of constraints in the database. Therefore, there is no data loss, and the extracted model constitutes a faithful wrapper of the relational database.

#index 1016298
#* An architecture for xml information retrieval in a peer-to-peer environment
#@ Judith Winter;Oswald Drobnik
#t 2007
#c 1
#% 321635
#% 340176
#% 345708
#% 345712
#% 387427
#% 429749
#% 449870
#% 642993
#% 766417
#% 766595
#% 793374
#% 824794
#% 857107
#% 878624
#% 945788
#% 1077150
#% 1681988
#% 1712582
#! XML has become a widely accepted standard for modelling, storing, and exchanging structured documents. Taking advantage of the document structure can result in improving the retrieval performance of XML-documents notably. A growing number of these documents are stored in Peer-to-Peer networks, which are promising self-organizing infrastructures. Documents are distributed over the Peer-to-Peer network by either being stored locally on individual peers or by being assigned to collections such as Digital Libraries. Current search methods for XML-documents in Peer-to-Peer networks lack the use of Information Retrieval techniques for vague queries and relevance detection. Our work aims for the development of a search engine for XML-documents, where Information Retrieval methods are enhanced by using structural information. Documents and global index are distributed over a Peer-to-Peer Network, building a virtually unlimited storage space. In this paper, a conceptual architecture for XML Information Retrieval in Peer-to-Peer networks is proposed. Based on this general architecture, a component-structured architecture for a concrete search engine is presented, which uses an extension of the Vector Space Model to compute relevance for dynamic XML-documents.

#index 1016299
#* Leveraging semantic technologies for enterprise search
#@ Gianluca Demartini
#t 2007
#c 1
#% 235918
#% 330769
#% 344448
#% 577273
#% 577339
#% 590523
#% 642981
#% 654467
#% 753895
#% 768898
#% 787547
#% 838501
#% 838546
#% 869510
#% 869548
#% 869649
#% 871766
#% 875017
#% 875061
#% 879570
#% 907515
#% 907525
#% 907536
#% 956516
#% 956543
#% 956683
#% 960243
#% 987304
#% 987378
#% 993980
#% 1016182
#% 1098230
#% 1098430
#% 1275180
#% 1392428
#% 1392496
#% 1655403
#% 1704240
#% 1742115
#! Enterprise search is very different from Web search (for example in the link structure or in the user's needs and goal)and some steps have been already done to exploit these differences in order to improve the effectiveness of enterprise search. In this paper we present the state of the art of the enterprise search field with some open issues. We also present a research plan that aims at using Information Retrieval, Semantic Web, and User Modelling techniques to cope with these issues improving the current state of enterprise search.

#index 1016300
#* Hierarchical two-tier ensemble learning: a new paradigm for network intrusion detection
#@ Morteza Analoui;Behrouz Minaei Bidgoli;Mohammad Hossein Rezvani
#t 2007
#c 1
#% 191854
#% 251145
#% 340031
#% 456719
#% 742990
#% 953684
#% 1272397
#! Intrusion detection is a mechanism of providing security to computer networks. Almost all of traditional intelligent intrusion detection systems (IDSs) use a single approach to distinguish normal behavior patterns from attack signatures. Moreover these systems have a high false alarm rate and high cost. The combination of multiple classifiers usually exhibits lower false alarm and overall error rate than individual decisions. On the other hand, the combination of classifiers trained on different feature sets could provide better performances than each single classifier. In this paper, a hierarchical two-level combiner is proposed to detect network intrusions using multiple well-known and efficient base classifiers. The proposed combiner exploits the different recognition capabilities provided by the independent feature representations in the first level as well as the agreement among the classifiers in the second level.

#index 1016301
#* Visualization and analysis of large graphs
#@ Eloïse Loubier;Wahiba Bahsoun;Bernard Dousset
#t 2007
#c 1
#% 122797
#% 193810
#% 258598
#% 538898
#! In Knowledge engineering, synthesized information has often an evolving and relational form. Information representation using graphs may ease data interpretation for non-expert users. However this graph may be complex and simplifications are useful in order to ease analysis. In this article, we present VisuGraph, a powerful tool for graph drawing. This tool gives the possibility to reduce large graph by two techniques: the Markov CLustering algorithm (MCL) application and the global graph division in time-sliced visualizations in order to specify and to simplify temporal analysis.

#index 1016302
#* Mining semantic distance between corpus terms
#@ Ahmad El Sayed;Hakim Hacid;Djamel Zighed
#t 2007
#c 1
#% 198058
#% 279755
#% 465914
#% 708948
#% 741085
#% 747891
#% 748499
#% 748600
#% 748691
#! In this paper, we face two problems in classical semantic similarity measures. Firstly, the context-dependency problem in knowledge-base measures since no one takes into account the context of the target domain. That is, a multisource context-dependent approach is presented. Secondly, the coverage problem with these measures since similarities can only be calculated between concepts included in a taxonomy. Moreover, "pure" corpus-based measures are still way from achieving performance reached by knowledge based measures. We present a more complex corpus-based approach using a taxonomy and data mining techniques in order to compute semantic distances between terms uncovered by the taxonomy. Experiments made show clearly the effectiveness of both proposed approaches.

#index 1016303
#* Cufres: clustering using fuzzy representative eventsselection for the fault recognition problem intelecommunication networks
#@ Jacques H. Bellec;Tahar M. Kechadi
#t 2007
#c 1
#% 210173
#% 248790
#% 480812
#% 564281
#% 726510
#% 794934
#% 823375
#% 823418
#% 899596
#% 1656979
#% 1829875
#! In this paper we introduce an efficient clustering algorithm embedded in a novel approach for solving the problem of faults identification in large telecommunication networks. Our algorithm is especially designed for the event correlation problem taking into account comprehensive information about the system behaviour. Although alarms are usually useful for identifying faults in such systems, their large number overloads the current management systems, making it extremely difficult to provide an answer within a reasonable response time. The alarm flow presents some interesting characteristics like alarm storm and alarm cascade. For instance, a single fault may result in a large number of alarms, and it is often very difficult to isolate the true cause of the fault. One way of overcoming this problem is to analyze, interpret and reduce the number of these alarms before trying to localize the faults. In this paper, we present a new original algorithm, and compare it with some available clustering algorithms by experimenting them with some samples of both simulated and real data from Ericsson's network.

#index 1016304
#* Maxprob and categorization of queries based on linguistic features
#@ Desire Kompaore;Josiane Mothe
#t 2007
#c 1
#% 232703
#% 397161
#% 766497
#% 766525
#% 784148
#! In this paper, we study the use of a probabilistic fusion approach inspired from the probFuse algorithm [14]. Our fusing technique is based on queries that are classified using some automatically extracted linguistic features [11]. In this approach, performances which are estimated during a training phase are used as an indicator of the systems to fuse. The rank list of relevant documents provided by the systems are divided into segments and used in a training/testing process to detect the systems to fuse. Our fusion technique shows better performances than other fusion techniques like CombMNZ [10], ProbFuse [14] and Best_sys [11].

#index 1016305
#* Exploiting web 2.0 forallknowledge-based information retrieval
#@ David N. Milne
#t 2007
#c 1
#% 73048
#% 201992
#% 768903
#% 854577
#% 961685
#% 1019105
#% 1250362
#% 1275012
#% 1707940
#! This paper describes ongoing research into obtaining and using knowledge bases to assist information retrieval. These structures are prohibitively expensive to obtain manually, yet automatic approaches have been researched for decades with limited success. This research investigates a potential shortcut: a way to provide knowledge bases automatically, without expecting computers to replace expert human indexers. Instead we aim to replace the professionals with thousands or even millions of amateurs: with the growing community of contributors who form the core of Web 2.0. Specifically we focus on Wikipedia, which represents a rich tapestry of topics and semantics and a huge investment of human effort and judgment. We show how this can be directly exploited to provide manually-defined yet inexpensive knowledge-bases that are specifically tailored to expose the topics, terminology and semantics of individual document collections. We are also concerned with how best to make these structures available to users, and aim to produce a complete knowledge-based retrieval system-both the knowledge base and the tools to apply it-that can be evaluated by how well it assists real users in performing realistic and practical information retrieval tasks. To this end we have developed Koru, a new search engine that offers concrete evidence of the effectiveness of our Web 2.0 based techniques for assisting information retrieval.

#index 1016306
#* Incorporating quality aspects in sensor data streams
#@ Anja Klein
#t 2007
#c 1
#% 228356
#% 262505
#% 332913
#% 378388
#% 480483
#% 503719
#% 654508
#% 824733
#% 853054
#% 893154
#% 895180
#% 972350
#% 982566
#% 1207174
#% 1669518
#! Sensors are increasingly embedded into physical products in order to capture data about their conditions and usage for decision making in business applications. However, a major issue for such applications is the limited quality of the captured data due to inherently restricted precision and performance of the sensors. Moreover, the data quality is further decreased by data processing to meet resource constraints in streaming environments and ultimately influences business decisions. The issue of how to efficiently provide applications with information about data quality (DQ) is still an open research problem. In my Ph.D. thesis, I address this problem by developing a system to provide business applications with accurate information on data quality. Furthermore, the system will be able to incorporate and guarantee user-defined data quality levels. In this paper, I will present the major results from my research so far. This includes a novel jumping-window-based approach for the efficient transfer of data quality information as well as a flexible metamodel for storage and propagation of data quality. The comprehensive analysis of common data processing operators w.r.t. their impact on data quality allows a fruitful knowledge evaluation and thus diminishes incorrect business decisions.

#index 1016307
#* Mutually beneficial learning with application to on-line news classification
#@ Lei Wu;Zhiwei Li;Mingjing Li;Wei-Ying Ma;Nenghai Yu
#t 2007
#c 1
#% 252011
#% 262050
#% 262059
#% 340948
#% 344447
#% 458379
#% 464465
#% 479462
#% 723391
#% 729940
#% 730039
#% 775249
#% 792603
#% 876010
#% 876039
#% 879626
#% 916788
#! There are three common challenges in real-world classification applications, i.e. how to use domain knowledge, how to resist noisy samples and how to use unlabeled data. To address these problems, a novel classification framework called Mutually Beneficial Learning (MBL) is proposed in this paper. MBL integrates two learning steps together. In the first step, the underlying local structures of feature space are discovered through a learning process. The result provides necessary capability to resist noisy samples and prepare better input for the second step where a consecutive classification process is further applied to the result. These two steps are iteratively performed until a stop condition is met. Different from traditional classifiers, the output of MBL consists of two components: a common classifier and a set of rules corresponding to local structures. In application, a test sample is first matched with the discovered rules. If a matched rule is found, the label of the rule is assigned to the sample; otherwise, the common classifier will be utilized to classify the sample. We applied the MBL to online news classification, and our experimental results showed that MBL is significantly better than Naïve Bayes and SVM, even when the data is noisy or partially labeled.

#index 1016308
#* Temporal constraints for rule-based event processing
#@ Karen Walzer;Alexander Schill;Alexander Löser
#t 2007
#c 1
#% 399
#% 102750
#% 177755
#% 201973
#% 342032
#% 346506
#% 351041
#% 480938
#% 569257
#% 578560
#% 618628
#% 824747
#% 875004
#% 891541
#% 977010
#% 978047
#% 1180857
#% 1180873
#% 1671619
#% 1688279
#% 1688281
#% 1725772
#! Complex event processing (CEP) is an important technology for event-driven systems with a broad application space ranging from supply chain management for RFID, systems monitoring, and stock market analysis to news services. The purpose of CEP is the identification of patterns of events with logical, temporal or causal relationships out of single occurring events. The Rete algorithm is commonly used in rule-based systems to trigger certain actions if a corresponding rule holds. It allows for a high number of rules and is therefore ideally suited for event processing systems. However, traditional Rete networks are limited to operations such as unification and the extraction of predicates from a knowledge base. There is no support for temporal operators. We propose an extension of the Rete algorithm for support of temporal operators. Thereby, we are using interval time semantics. We present the issues created by this extension as well as our pursued methodology to address them. A description language is used to specify the patterns of interest. This specification is also called subscription to a complex event. On the occurrence of the specified event pattern, an action such as the creation of a new event or the execution of certain function is performed.

#index 1016309
#* Managing highly correlated semi-structured data: architectural aspects of a digital archive
#@ Alf-Christian Schering;Holger Meyer;Andreas Heuer
#t 2007
#c 1
#% 272734
#% 479465
#% 480489
#% 480656
#% 488625
#% 631933
#% 811250
#% 893014
#% 1688290
#! XML techniques are well suited to describe, manage, store, and exchange hierarchical, semi-structured data. Information represented beyond hierarchical structures can still be described and exchanged in XML format employing additional concepts such as ID/IDREF or XLink. However, retrieval, manipulation, and storage mechanisms are far away from being the ideal solution for such data. Query languages do not perform efficiently in these cases. Especially in scenarios, such as the Digital Wossidlo Archive (WossiDiA), a project dealing with a huge number of arbitrarily correlated data units, XML query evaluation and retrieval techniques face problems, such as intricate querying and bad efficiency. At this point a solution to manage these data efficiently needs to be devised. This paper introduces a first approach which attempts to find such a solution for the WossiDiA information system.

#index 1016310
#* Towards workload shift detection and prediction for autonomic databases
#@ Marc Holze;Norbert Ritter
#t 2007
#c 1
#% 119485
#% 721346
#% 770344
#% 800385
#% 820356
#% 875027
#% 886477
#% 893130
#% 893178
#% 993933
#% 1016221
#% 1016225
#% 1207103
#! Due to the complexity of industry-scale database systems, the total cost of ownership for these systems is no longer dominated by hardware and software, but by administration expenses. Autonomic databases intend to reduce these costs by providing self-management features. Existing approaches towards this goal are supportive advisors for the database administrator and feedback control loops for online monitoring, analysis and re-configuration. But while advisors are too resource-consuming for continuous operation, feedback control loops suffer from overreaction, oscillation and interference. In this position paper we give a general analysis of the parameters that affect the self-management of a database. Out of these parameters, we identify that the workload has major influence on both physical design of data and DBMS configuration. Hence, we propose to employ a workload model for light-weight, continuous workload monitoring and analysis. This model can be used for the identification and prediction of significant workload shifts, which require autonomic re-configuration of the database.

#index 1016311
#* Personal digital library: pim through a 5s perspective
#@ Yi Ma;Edward A. Fox;Marcos A. Gonçalves
#t 2007
#c 1
#% 335072
#% 342671
#% 750866
#% 766530
#% 790446
#% 802743
#% 845325
#% 845344
#% 848653
#% 967336
#! Information overload and fragmentation problems result from the huge volume of data available in digital formats. This has led to increasing interest in personal information management research. However, current efforts have not fully clarified the thinking in this area due to the lack of theoretical foundations. In this paper, we present a formal framework for personal information management through a personal digital library perspective based upon the 5S approach. This framework includes a formal definition of the components and functionalities for a minimal personal digital library. We also present the primary testing results of our initial implementation of receptor module and behavioral information relevance. Challenges expected in planned future efforts are discussed too, with suggestions for possible solutions. We believe this is one of the first efforts to provide a theoretical foundation for personal information management. Our results and planned work show promise for further doctoral research.

#index 1016312
#* Document retrieval for question answering: a quantitative evaluation of text preprocessing
#@ Gracinda Carvalho;David Martins de Matos;Vitor Rocio
#t 2007
#c 1
#% 309127
#% 336784
#% 642979
#! Question Answering (QA) has been an area of interest for researchers, in part motivated by the international QA evaluation forums, namely the Text REtrieval Conference (TREC), and more recently, the Cross Language Evaluation Forum (CLEF) through QA@CLEF, that since 2004 includes the Portuguese language. In these forums, a collection of written documents is provided, as well as a set of questions, which are to be answered by the participating systems. Each system is evaluated by its capacity to answer the questions, as a whole, and there are relatively few results published that focus on the performance of its different components and their influence on the overall system performance. That is the case of the Information Retrieval (IR) component, which is broadly used in QA systems. Our work concentrates on the different options of preprocessing Portuguese text before feeding it to the IR component, evaluating their impact on the IR performance in the specific context of QA, so that we can make a sustained choice of which options to choose. From this work we conclude the clear advantage of the basic preprocessing techniques: case folding and removal of punctuation marks. For the other techniques considered, stop word removal enhanced the performance of the IR system but that was not the case as far as Stemming and Lemmatization are concerned.

#index 1016313
#* Combining resources with confidence measures for cross language information retrieval
#@ Youssef Kadri;Jian-Yun Nie
#t 2007
#c 1
#% 340948
#% 342707
#% 561166
#% 740915
#% 807747
#% 855102
#! Query translation in Cross Language Information Retrieval (CLIR) can be performed using multiple resources. Previous attempts to combine different translation resources use simple methods such as linear combination. Unfortunately, these approaches are insufficient to combine different types of resources such as bilingual dictionaries and statistical translation models. In this paper, we use confidence measures for this combination for the purpose of English-Arabic CLIR. Confidence measure is used to adjust the original scores of translations and to create a weight of the same nature for translations with different resources. We tested this technique on two test CLIR collections from TREC and obtained encouraging improvements compared to the results of linear combination.

#index 1016314
#* Top-k subgraph matching query in a large graph
#@ Lei Zou;Lei Chen;Yansheng Lu
#t 2007
#c 1
#% 274612
#% 309749
#% 333854
#% 378391
#% 765429
#% 772884
#% 841960
#% 864425
#% 907289
#% 960304
#% 960305
#% 1022280
#% 1673591
#! Recently, due to its wide applications, subgraph search has attracted a lot of attention from database and data mining community. Sub-graph search is defined as follows: given a query graph Q, we report all data graphs containing Q in the database. However, there is little work about sub-graph search in a single large graph, which has been used in many applications, such as biological network and social network. In this paper, we address top-k sub-graph matching query problem, which is defined as follows: given a query graph Q, we locate top-k matchings of Q in a large data graph G according to a score function. The score function is defined as the sum of the pairwise similarity between a vertex in Q and its matching vertex in G. Specifically, we first design a balanced tree (that is G-Tree) to index the large data graph. Then, based on G-Tree, we propose an efficient query algorithm (that is Ranked Matching algorithm). Our extensive experiment results show that, due to efficiency of pruning strategy, given a query with up to 20 vertices, we can locate the top-100 matchings in less than 10 seconds in a large data graph with 100K vertices. Furthermore, our approach outperforms the alternative method by orders of magnitude.

#index 1016315
#* Formalizing ontology reconciliation techniques as a basis for meaningful mediation in service-related tasks
#@ Patrício de Alencar Silva;Cláudia M. F. A. Ribeiro;Ulrich Schiel
#t 2007
#c 1
#% 56174
#% 115390
#% 445169
#% 728755
#% 763218
#% 772435
#% 787631
#% 807393
#% 814212
#% 823935
#% 863905
#% 903358
#% 987494
#% 1713482
#% 1721022
#! With the advent of Semantic Web, the fast dissemination of ontologies to represent and share information causes a deep impact on knowledge retrieval, as a whole. In this context, the use of different ontologies to express meaning on the same application domain leads to a kind of "Tower of Babel Effect" on the Web, bringing new problems to the communication among different applications. This problem still remains in service-oriented architectures, considering that a same service can be described by the use of different ontologies and standards. Therefore, the urge of an ontology reconciliation approach arises in order to enable communication despite the differences. This paper focuses on providing a formal description of ontology reconciliation techniques, such as merging, alignment and integration, to provide better understanding of how these techniques can be used on the scope of Semantic Web Services Architecture.

#index 1016316
#* Possibility and necessity measures for relevance assessment
#@ F. Z. Bessai Mechmache;M. Boughanem;Z. Alimazighi
#t 2007
#c 1
#% 169781
#% 232662
#% 387427
#% 458404
#% 643044
#% 840583
#% 1650272
#% 1739439
#! The major question raised in information retrieval on semi-structured documents relates to the manner of effectively handling the structure and the contents of the document for better answering the user's needs. These needs can be formulated by queries composed of only key words or key words and structural constraints. In this paper, we are interested in Information Retrieval in semi-structured document like XML. For these purposes, we present a model for the semi-structured information retrieval, based on the possibilistic networks. The document - elements and elements - terms relations are modelled by measures of possibility and necessity. In this model, the user's query starts a process of propagation to recover documents or portions of documents necessarily or at least possibly relevant. An example of such a research is proposed in order to illustrate the presented approach.

#index 1016317
#* Collaborative framework for supporting indigenous knowledge management
#@ Theodora Mondo T. Mwebesa;Venansius Baryamureeba;Ddembe Williams
#t 2007
#c 1
#% 310424
#% 384022
#% 794069
#% 814503
#% 826173
#% 837921
#% 837922
#% 837923
#% 837924
#% 837926
#% 1314728
#% 1343849
#% 1349478
#! Indigenous knowledge (IK) is an integral part of the culture and history of local communities. We need to learn from local communities to enrich the development process. IK systems are also dynamic; new knowledge is continuously added. IK is stored in people's memories, activities and is expressed and communicated orally and this posses a serious threat to its preservation and development. However this depicts Indigenous knowledge Management (IKM) as a complex, global and dynamic issue and hence a need for a collaborative framework that will enhance information exchange. Utilization of collaborative frameworks, are increasingly being used in solving problems whose efficiency depends on an interactive nature. Networking or collaboration allows for a more effective pooling of resources and sharing of experiences and information on indigenous knowledge, both among various individuals and organizations.

#index 1016318
#* Webview selection from user access patterns
#@ Samia Saidi;Yahya Slimani;Khedija Arour
#t 2007
#c 1
#% 300177
#% 316969
#% 342870
#% 479950
#% 630984
#% 661023
#% 772329
#% 777934
#% 808600
#! The number of Web Usage Mining (WUM) applications is growing continuously, especially due to the business interest in e-commerce Web sites and the related Web-marketing applications.The application of WUM results goes beyond the subject of our thesis since one important part of our thesis deals with the problem of selection of WebViews using technicsof WUM. View materialization is an important issue ifwe want to improve the efficiency of many applications likeOLAP, Database and Web applications. In this proposal wesuggest a novel approach for selecting webviews to be materialized in order to optimize the response time of web queries since satisfying the needs of users is vital for Web sites. The selection of Webviews to be materialized was mainly based on the estimation of metrics requiring hard collects of multiple statistics [10] that's why, we believe on a solution based on mining an interesting set of webviews to be materialized from realistic data: Web log files. Thus, Web log files will be parsed, analyzed and treated to give a set of webviews, based on frequent closed itemsets.

#index 1297078
#* Patterns in the stream: exploring the interaction of polarity, topic, and discourse in a large opinion corpus
#@ Julian Brooke;Matthew Hurst
#t 2009
#c 1
#% 452450
#% 740329
#% 815915
#% 828958
#% 854646
#% 938687
#% 939896
#% 956510
#% 1346371
#% 1700552
#! A qualitative examination of review texts suggests that there are consistent patterns to how topic and polarity are expressed in discourse. These patterns are visible in the text and paragraph structure, topic depth, and polarity flow. In this paper, we employ sentence-level sentiment classifiers and a hand-built tree ontology to investigate whether these patterns can be quantitatively identified in a large corpus of video game reviews. Our results indicate that the beginning and the end of major textual units (e.g. paragraphs) stand out in the flow of texts, showing a concentration of reliable opinion and key topic aspects, and that there are other important regularities in the expression of opinion and topic relevant to their ordering and the discourse markers with which they appear.

#index 1297079
#* Topic-dependent sentiment analysis of financial blogs
#@ Neil O'Hare;Michael Davy;Adam Bermingham;Paul Ferguson;Páraic Sheridan;Cathal Gurrin;Alan F. Smeaton
#t 2009
#c 1
#% 190581
#% 268073
#% 348165
#% 577355
#% 722935
#% 754125
#% 763708
#% 805873
#% 815915
#% 854646
#% 926881
#% 939897
#% 1074102
#% 1130915
#% 1190865
#% 1200477
#% 1227734
#% 1251645
#! While most work in sentiment analysis in the financial domain has focused on the use of content from traditional finance news, in this work we concentrate on more subjective sources of information, blogs. We aim to automatically determine the sentiment of financial bloggers towards companies and their stocks. To do this we develop a corpus of financial blogs, annotated with polarity of sentiment with respect to a number of companies. We conduct an analysis of the annotated corpus, from which we show there is a significant level of topic shift within this collection, and also illustrate the difficulty that human annotators have when annotating certain sentiment categories. To deal with the problem of topic shift within blog articles, we propose text extraction techniques to create topic-specific sub-documents, which we use to train a sentiment classifier. We show that such approaches provide a substantial improvement over full documentclassification and that word-based approaches perform better than sentence-based or paragraph-based approaches.

#index 1297080
#* Scary films good, scary flights bad: topic driven feature selection for classification of sentiment
#@ Scott Nowson
#t 2009
#c 1
#% 238395
#% 815915
#% 854646
#% 939346
#% 939848
#% 939897
#% 956510
#% 983601
#% 1214749
#% 1256692
#% 1261670
#% 1409618
#! This paper describes preliminary work on feature selection for classification of review text by both sentiment rating and topic. The premise stems from the notion that one size does not fit all; that feature sets for sentiment analysis should be tailored to the topic of a text. Thus it naturally follows that for this to be effective it is also necessary to first determine the topic of a text. Following successful work on classification of texts by author demographics, a corpus of review texts labelled with attributed rating, topic area, and user demographics has been compiled. This collection was divided for this work into different topic groups in order to automatically classify between both text topic and subjective rating. By using a single supervised statistical approach to feature selection, it is shown that improvements can be made to classification accuracy using topic tuned features sets over more generic features.

#index 1297081
#* Towards the definition of requirements for mixed fact and opinion question answering systems
#@ Alexandra Balahur;Estér Boldrini;Andrés Montoyo;Patricio Martínez-Barco
#t 2009
#c 1
#% 939897
#% 939969
#! The growth of the Social Web led to the birth of new textual genres such as blogs, forums or reviews. Such data sources are extremely relevant because texts pertaining to these categories approach a wide range of topics and are written by people with different social backgrounds. As a consequence, they represent a rich resource that can be exploited to carry out different types of analyses by a whole diversity of entities (potential customers, companies, public figures, political parties, etc.). For this research we created a collection of factoid and opinion questions in Spanish, with the purpose of comparing the performance of an open domain and an opinion QA system. Furthermore, we carried out two separate evaluations for each one of them and thoroughly analysed the results in order to understand the reasons for the problematic cases, thus being able to infer the features that an effective QA system for opinions should have. Our conclusion is that this task requires the use of specialised resources, whose creation for languages other than English is highly necessary.

#index 1297082
#* Automatic creation of a reference corpus for political opinion mining in user-generated content
#@ Luís Sarmento;Paula Carvalho;Mário J. Silva;Eugénio de Oliveira
#t 2009
#c 1
#% 854646
#% 855279
#% 855282
#% 1261563
#% 1297085
#% 1700552
#! We propose and evaluate a method for automatically creating a reference corpus for training text classification procedures for mining political opinions in user-generated content. The process starts by compiling a collection of highly opinionated comments posted by users on an on-line newspaper. Then, we define and use a set of manually-crafted high-precision rules supported by a large sentiment-lexicon in order to identify sentences in each comment expressing opinions about political entities. Finally, the opinions found are propagated to the remainder sentences of the comment mentioning the same entities, thus increasing the number and variety of opinion-bearing sentences. Results show that most of the rules can identify negative opinions with very high precision, and these can be safely propagated to the remainder sentences in the comment in almost 100% of the cases. Due to problems arising from irony, the precision of identification drops for positive opinions, but several rules still reach high precision. Propagation of positive opinions is correct in about 77% of the cases, and most errors at this stage result from irony and polarity inversion throughout the comment.

#index 1297083
#* Domain-specific sentiment analysis using contextual feature generation
#@ Yoonjung Choi;Youngho Kim;Sung-Hyon Myaeng
#t 2009
#c 1
#% 577355
#% 722308
#% 746885
#% 815915
#% 817472
#% 838521
#% 855282
#% 871355
#% 938685
#% 938686
#% 939576
#% 939848
#% 939897
#% 987340
#% 1127964
#% 1261566
#! This paper presents a novel framework for sentiment analysis, which exploits sentiment topic information for generating context-driven features. Since the domain-specific nature of sentiment classification led the task more problematic, considering more contextual-information such as topic or domain is essential. In our system, we first automatically extract sentiment clues in different domains by our observation. We identified that a sentiment clue is often syntactically related to a sentiment topic in a sentence, which is defined as a primary subject of sentiment expression, such as event, company, and person. We bootstrap from a small set of seed clues and generate new clues by utilizing linguistic dependencies and collocation information between sentiment clues and sentiment topics. Next, we learn a domain-specific sentiment classifier for each domain with the newly aggregated clues. We ran experiments to see how the bootstrapping algorithm to converge and aggregate new clues and verified that the extracted domain-context features are more effective than generally-used features in sentiment analysis by running them on the same sentiment classifier.

#index 1297084
#* Weakly supervised techniques for domain-independent sentiment classification
#@ Jonathon Read;John Carroll
#t 2009
#c 1
#% 78171
#% 278099
#% 747738
#% 815915
#% 843652
#% 848150
#% 854577
#% 854646
#% 855282
#% 938687
#% 939769
#% 943529
#% 979654
#% 1271268
#% 1277969
#! An important sub-task of sentiment analysis is polarity classification, in which text is classified as being positive or negative. Supervised machine learning techniques can perform this task very effectively. However, they require a large corpus of training data, and a number of studies have demonstrated that the good performance of supervised models is dependent on a good match between the training and testing data with respect to the domain, topic and time-period. Weakly-supervised techniques use a large collection of unlabelled text to determine sentiment, and so their performance may be less dependent on the domain, topic and time-period represented by the testing data. This paper presents experiments that investigate the effectiveness of word similarity techniques when performing weakly-supervised sentiment classification. It also considers the extent to which the performance of each method is independent from the domain, topic and time-period of the testing data. The results indicate that the word similarity techniques are suitable for applications that require sentiment classification across several domains.

#index 1297085
#* Clues for detecting irony in user-generated contents: oh...!! it's "so easy" ;-)
#@ Paula Carvalho;Luís Sarmento;Mário J. Silva;Eugénio de Oliveira
#t 2009
#c 1
#% 757851
#% 1262887
#% 1262890
#% 1297082
#! We investigate the accuracy of a set of surface patterns in identifying ironic sentences in comments submitted by users to an on-line newspaper. The initial focus is on identifying irony in sentences containing positive predicates since these sentences are more exposed to irony, making their true polarity harder to recognize. We show that it is possible to find ironic sentences with relatively high precision (from 45% to 85%) by exploring certain oral or gestural clues in user comments, such as emoticons, onomatopoeic expressions for laughter, heavy punctuation marks, quotation marks and positive interjections. We also demonstrate that clues based on deeper linguistic information are relatively inefficient in capturing irony in user-generated content, which points to the need for exploring additional types of oral clues.

#index 1297086
#* Beyond the stars: exploiting free-text user reviews to improve the accuracy of movie recommendations
#@ Niklas Jakob;Stefan Hagen Weber;Mark Christoph Müller;Iryna Gurevych
#t 2009
#c 1
#% 722904
#% 727877
#% 734590
#% 769892
#% 818234
#% 876018
#% 881487
#% 907489
#% 939896
#% 939897
#% 982677
#% 1055682
#% 1055683
#% 1120983
#% 1275012
#% 1299639
#% 1396093
#% 1650569
#% 1708349
#! In this paper we show that the extraction of opinions from free-text reviews can improve the accuracy of movie recommendations. We present three approaches to extract movie aspects as opinion targets and use them as features for the collaborative filtering. Each of these approaches requires different amounts of manual interaction. We collected a data set of reviews with corresponding ordinal (star) ratings of several thousand movies to evaluate the different features for the collaborative filtering. We employ a state-of-the-art collaborative filtering engine for the recommendations during our evaluation and compare the performance with and without using the features representing user preferences mined from the free-text reviews provided by the users. The opinion mining based features perform significantly better than the baseline, which is based on star ratings and genre information only.

#index 1297087
#* Aspect-based sentence segmentation for sentiment summarization
#@ Jingbo Zhu;Muhua Zhu;Huizhen Wang;Benjamin K. Tsou
#t 2009
#c 1
#% 283180
#% 448786
#% 577355
#% 741058
#% 742204
#% 746885
#% 748442
#% 748550
#% 748631
#% 769892
#% 771916
#% 854646
#% 855093
#% 907489
#% 927762
#% 939896
#% 1026909
#% 1127964
#% 1261576
#% 1264775
#% 1299639
#! Aspect-based sentiment summarization systems generally use sentences associated with relevant aspects extracted from the reviews as the basis for summarization. However, in real reviews, a single sentence often exhibits several aspects for opinions. This paper proposes a two-stage segmentation model to address the challenge of identifying multiple single-aspect and single-polarity units in one sentence, namely aspect-based sentence segmentation. Our model deals with both issues of aspect change and polarity change occurring in the input sentence. Experiments on restaurant reviews show that our model outperforms state-of-the-art linear text segmentation methods.

#index 1297088
#* Locally contextualized smoothing of language models for sentiment sentence retrieval
#@ Takayoshi Okamoto;Tetsuya Honda;Koji Eguchi
#t 2009
#c 1
#% 144034
#% 262096
#% 287253
#% 340901
#% 340948
#% 723399
#% 727877
#% 766409
#% 819213
#% 939897
#% 956510
#% 1127964
#% 1130988
#% 1261565
#! Recently, a number of documents are published on the web. One of the crucial techniques to access to such information is sentiment sentence retrieval. It is very useful to retrieve positive or negative opinions to a specific topic at sentence level. Considering the property that sentiment polarities are often locally consistent in a document, we focus on using local context information for retrieving sentiment-bearing sentences. For this objective, we propose a new smoothing method, extending Dirichlet prior smoothing, to improve effectiveness of the retrieval. We demonstrate through experiments that our proposed smoothing method achieves statistically significant improvements in sentiment sentence retrieval, compared with a conventional smoothing method without local context.

#index 1297089
#* Sentiment analysis of movie reviews on discussion boards using a linguistic approach
#@ Tun Thura Thet;Jin-Cheon Na;Christopher S.G. Khoo;Subbaraj Shakthikumar
#t 2009
#c 1
#% 458379
#% 727877
#% 854646
#% 938687
#% 1125374
#% 1700552
#! We propose a linguistic approach for sentiment analysis of message posts on discussion boards. A sentence often contains independent clauses which can represent different opinions on the multiple aspects of a target object. Therefore, the proposed system provides clause-level sentiment analysis of opinionated texts. For each sentence in a message post, it generates a dependency tree, and splits the sentence into clauses. Then it determines the contextual sentiment score for each clause utilizing grammatical dependencies of words and the prior sentiment scores of the words derived from SentiWordNet and domain specific lexicons. Negation is also delicately handled in this study, for instance, the term "not superb" is assigned a lower negative sentiment score than the term "not good". We have experimented with a dataset of movie review sentences, and the experimental results show the effectiveness of the proposed approach.

#index 1482174
#* Proceedings of the 19th ACM international conference on Information and knowledge management
#@ Jimmy Huang;Nick Koudas;Gareth Jones;Xindong Wu;Kevyn Collins-Thompson;Aijun An
#t 2010
#c 1
#! On behalf of the organizing committee, I wholeheartedly welcome you to the 19th ACM International Conference on Information and Knowledge Management (CIKM 2010). I hope this conference proves to be interesting and beneficial. CIKM is a well-known top tier and premier ACM conference in the areas of information retrieval, knowledge management and database. Since its inception, the CIKM conference has provided a unique international forum for the presentation, discussion, and dissemination of research findings in data management, information retrieval, and knowledge management. The purpose of the conference is to identify challenging problems facing the development of future knowledge and information systems, and to shape future research directions through the publication of high quality, applied and theoretical research findings. The conference has been a leading forum in which experts from academia, industry, and the government gather to exchange ideas, research achievements, and technical developments in multidisciplinary research areas. CIKM has rapidly grown to become one of the world's most recognized conferences in the field. This year CIKM has received a record high number of submissions in the history of CIKM, as can be seen from the following statistics: 1382 abstracts submitted 945 full papers plus 38 demo papers submitted 126 papers accepted for presentation as full papers (13.3% acceptance rate) and an additional 165 were accepted for short papers (17.5%). In addition to regular research tracks, CIKM 2010 features 4 keynote speakers, 4 pre-conference tutorials, 9 workshops, 12 industrial full papers and 20 demo papers. I am proud of our program and acknowledge the tireless efforts of people who materialized this program. First of all, I am honored to have 4 distinguished keynote speakers: Jamie Callan, Susan Dumais, Gregory Grefenstette, and Divesh Srivastava. I deeply appreciate their time and commitment to deliver their speeches and share their cutting-edge research experiences and insightful comments in their research topics.

#index 1482175
#* Search engine support for software applications
#@ Jamie Callan
#t 2010
#c 1
#! Question-answering, computer-assisted language learning, text mining, and other software applications that use a full-search engine to find information in a large text corpus are becoming common. A software application may use metadata and text annotations to reduce the mismatch between the concept-based representations convenient for inference and the word-based representations typically used for text retrieval. Software applications may also be able to specify detailed requirements that retrieved passages must satisfy. This use of text search is very different than the ad-hoc, interactive search that information retrieval research typically studies. Search engine developers are beginning to respond by extending indexing and retrieval models developed for structured (e.g., XML) documents to support multiple representations of document content, text annotations, metadata, and relationships. These new requirements force developers to reconsider basic assumptions about index data structures and ranked retrieval models. How best to use these new capabilities is an open problem. Straightforward transformation of a detailed information need into a complex structured query can produce a query that is effective for exact-match retrieval, but a challenge for the retrieval model to use effectively for best-match retrieval. Bag-of-words retrieval is often disparaged, but its advantage is that it is robust: It works well even when desired documents do not exactly meet expectations. This talk discusses some of the problems encountered when extending a search engine to support queries posed by other software applications and structured documents with derived annotations

#index 1482176
#* Schema extraction
#@ Divesh Srivastava
#t 2010
#c 1
#% 1328162
#% 1523867
#! Understanding the schema of a complex database is a crucial step in exploratory data analysis. However, gaining such an understanding is challenging for new users for many reasons. First, complex databases often have thousands of inter-linked tables, with little indication of the important tables or the main concepts in the database schema. Second, schemas can be inaccurate, e.g., some foreign/primary key relationships are not known to designers but are inherent in the data, while others become invalid due to data inconsistencies. In this talk, we present an approach to effectively address these challenges and automatically extract an understandable schema from a complex database. The first step in our approach is a robust algorithm to discover foreign/primary key relationships between tables. We present a general rule, termed Randomness, that subsumes a variety of other rules proposed in previous work, and develop efficient approximation algorithms for evaluating randomness, using only two passes over the data. The second step is a principled approach to summarize the schema consisting of tables linked using foreign/primary keys, so that a user can easily identify the main concepts and important tables. We present an information theoretic approach to identify important tables, and an intuitive notion of table similarity that can be used to cluster tables into the main concepts of the schema. We validate our approach using real and synthetic datasets. This is based on joint work [1, 2] with Marios Hadjieleftheriou, Beng Chin Ooi, Cecilia M. Procopiuc, Xiaoyan Yang and Meihui Zhang.

#index 1482177
#* Use of semantics in real life applications
#@ Gregory Grefenstette
#t 2010
#c 1
#! Semantics has many different definitions in science. In natural language processing, there has been much research over the past three decades involving extracting the semantics, the meaning, of natural texts. This has led to entity recognition (people, places, companies, prices, dates, and events), and more recently into sentiment analysis, exploring another level of meaning in a text. These techniques are now well understood and robust. Results of this research are beginning to appear in products and online sites, finding their way into practical applications. The stage is set for an explosion of semantically savvy applications, from 3D design, to enhanced web browsing, to social network aware yellowpages. This talk will explore these paths from research to industry, illustrated by current products on the market.

#index 1482178
#* Temporal dynamics and information retrieval
#@ Susan T. Dumais
#t 2010
#c 1
#! Many digital resources, like the Web, are dynamic and ever-changing collections of information. However, most of the tools information retrieval and management that have been developed for interacting with Web content, such as browsers and search engines, focus on a single static snapshot of the information. In this talk, I will present analyses of how Web content changes over time, how people re-visit Web pages over time, and how re-visitation patterns are influenced by changes in user intent and content. These results have implications for many aspects of information retrieval and management including crawling, ranking and information extraction algorithms, result presentation, and evaluation. I will describe a prototype system that supports people in understanding how the information they interact with changes over time, and a new retrieval model that incorporates features about the temporal evolution of content to improve core ranking. Finally, I will conclude with an overview of some general challenges that need to be addressed to fully incorporate temporal dynamics in information retrieval and information management systems.

#index 1482179
#* Components for information extraction: ontology-based information extractors and generic platforms
#@ Daya C. Wimalasuriya;Dejing Dou
#t 2010
#c 1
#% 53705
#% 261694
#% 274523
#% 290482
#% 576214
#% 720483
#% 754104
#% 768896
#% 782761
#% 810014
#% 905122
#% 1083705
#% 1092531
#% 1231245
#% 1275182
#% 1292489
#% 1415657
#% 1428282
#! Information Extraction (IE) has existed as a field for several decades and has produced some impressive systems in the recent past. Despite its success, widespread usage and commercialization remain elusive goals for this field. We identify the lack of effective mechanisms for reuse as one major reason behind this situation. Here, we mean not only the reuse of the same IE technique in different situations but also the reuse of information related to the application of IE techniques (e.g., features used for classification). We have developed a comprehensive component-based approach for information extraction that promotes reuse to address this situation. We designed this approach starting from our previous work on the use of multiple ontologies in information extraction [24]. The key ideas of our approach are "information extractors," which are components of an IE system that make extractions with respect to particular components of an ontology and "platforms for IE," which are domain and corpus independent implementations of IE techniques. A case study has shown that this component-based approach can be successfully applied in practical situations.

#index 1482180
#* Automatically acquiring a semantic network of related concepts
#@ Sean Szumlanski;Fernando Gomez
#t 2010
#c 1
#% 196896
#% 325502
#% 465914
#% 724234
#% 756964
#% 783633
#% 786515
#% 896031
#% 896039
#% 902089
#% 939515
#% 939538
#% 939540
#% 939546
#% 1019082
#% 1019189
#% 1250381
#% 1251606
#% 1275012
#% 1275285
#% 1305622
#! We describe the automatic construction of a semantic network1, in which over 3000 of the most frequently occurring monosemous nouns2 in Wikipedia (each appearing between 1,500 and 100,000 times) are linked to their semantically related concepts in the WordNet noun ontology. Relatedness between nouns is discovered automatically from co-occurrence in Wikipedia texts using an information theoretic inspired measure. Our algorithm then capitalizes on salient sense clustering among related nouns to automatically disambiguate them to their appropriate senses (i.e., concepts). Through the act of disambiguation, we begin to accumulate relatedness data for concepts denoted by polysemous nouns, as well. The resultant concept-to-concept associations, covering 17,543 nouns, and 27,312 distinct senses among them, constitute a large-scale semantic network of related concepts that can be conceived of as augmenting the WordNet noun ontology with related-to links.

#index 1482181
#* Online annotation of text streams with structured entities
#@ Ken Q. Pu;Oktie Hassanzadeh;Richard Drake;Renée J. Miller
#t 2010
#c 1
#% 279755
#% 415957
#% 769884
#% 857163
#% 864415
#% 875017
#% 1077150
#% 1127423
#% 1127426
#% 1166508
#% 1190070
#% 1190092
#% 1217204
#% 1291356
#% 1379016
#% 1467763
#! We propose a framework and algorithm for annotating unbounded text streams with entities of a structured database. The algorithm allows one to correlate unstructured and dirty text streams from sources such as emails, chats and blogs, to entities stored in structured databases. In contrast to previous work on entity extraction, our emphasis is on performing entity annotation in a completely online fashion. The algorithm continuously extracts important phrases and assigns to them top-k relevant entities. Our algorithm does so with a guarantee of constant time and space complexity for each additional word in the text stream, thus infinite text streams can be annotated. Our framework allows the online annotation algorithm to adapt to changing stream rate by self-adjusting multiple run-time parameters to reduce or improve the quality of annotation for fast or slow streams, respectively. The framework also allows the online annotation algorithm to incorporate query feedback to learn the user preference and personalize the annotation for individual users.

#index 1482182
#* Automatic extraction of web data records containing user-generated content
#@ Xinying Song;Jing Liu;Yunbo Cao;Chin-Yew Lin;Hsiao-Wuen Hon
#t 2010
#c 1
#% 103525
#% 273925
#% 289193
#% 330784
#% 397605
#% 480824
#% 577319
#% 654469
#% 660272
#% 729978
#% 754108
#% 769892
#% 779889
#% 805845
#% 805846
#% 838491
#% 881505
#% 889107
#% 936239
#% 989661
#% 1190073
#% 1190153
#% 1292470
#% 1338582
#% 1683908
#! In this paper, we are concerned with the problem of automatically extracting web data records that contain user-generated content (UGC). In previous work, web data records are usually assumed to be well-formed with a limited amount of UGC, and thus can be extracted by testing repetitive structure similarity. However, when a web data record includes a large portion of free-format UGC, the similarity test between records may fail, which in turn results in lower performance. In our work, we find that certain domain constraints (e.g., post-date) can be used to design better similarity measures capable of circumventing the influence of UGC. In addition, we also use anchor points provided by the domain constraints to improve the extraction process, which ends in an algorithm called MiBAT (Mining data records Based on Anchor Trees). We conduct extensive experiments on a dataset consisting of forum thread pages which are collected from 307 sites that cover 219 different forum software packages. Our approach achieves a precision of 98.9% and a recall of 97.3% with respect to post record extraction. On page level, it perfectly handles 91.7% of pages without extracting any wrong posts or missing any golden posts. We also apply our approach to comment extraction and achieve good results as well.

#index 1482183
#* An efficient algorithm for mining time interval-based patterns in large database
#@ Yi-Cheng Chen;Ji-Chiang Jiang;Wen-Chih Peng;Suh-Yin Lee
#t 2010
#c 1
#% 319244
#% 329537
#% 459006
#% 463903
#% 464996
#% 477791
#% 487661
#% 844326
#% 975048
#% 980988
#% 992858
#% 1063499
#! Most studies on sequential pattern mining are mainly focused on time point-based event data. Few research efforts have elaborated on mining patterns from time interval-based event data. However, in many real applications, event usually persists for an interval of time. Since the relationships among event time intervals are intrinsically complex, mining time interval-based patterns in large database is really a challenging problem. In this paper, a novel approach, named as incision strategy and a new representation, called coincidence representation are proposed to simplify the processing of complex relations among event intervals. Then, an efficient algorithm, CTMiner (Coincidence Temporal Miner) is developed to discover frequent time-interval based patterns. The algorithm also employs two pruning techniques to reduce the search space effectively. Furthermore, experimental results show that CTMiner is not only efficient and scalable but also outperforms state-of-the-art algorithms.

#index 1482184
#* What can quantum theory bring to information retrieval
#@ Benjamin Piwowarski;Ingo Frommholz;Mounia Lalmas;Keith van Rijsbergen
#t 2010
#c 1
#% 194298
#% 223810
#% 235342
#% 305867
#% 378462
#% 642975
#% 758200
#% 789959
#% 835027
#% 859500
#% 879587
#% 1051060
#% 1074078
#% 1074133
#% 1194536
#% 1263589
#% 1343447
#% 1537473
#% 1697443
#% 1697454
#% 1697471
#! The probabilistic formalism of quantum physics is said to provide a sound basis for building a principled information retrieval framework. Such a framework can be based on the notion of information need vector spaces where events, such as document relevance or observed user interactions, correspond to subspaces. As in quantum theory, a probability distribution over these subspaces is defined through weighted sets of state vectors (density operators), and used to represent the current view of the retrieval system on the user information need. Tensor spaces can be used to capture different aspects of information needs. Our evaluation shows that the framework can lead to acceptable performance in an ad-hoc retrieval task. Going beyond this, we discuss the potential of the framework for three active challenges in information retrieval, namely, interaction, novelty and diversity.

#index 1482185
#* Entity ranking using Wikipedia as a pivot
#@ Rianne Kaptein;Pavel Serdyukov;Arjen De Vries;Jaap Kamps
#t 2010
#c 1
#% 169784
#% 787549
#% 878916
#% 939376
#% 987276
#% 1019130
#% 1019135
#% 1019189
#% 1052710
#% 1074223
#% 1100822
#% 1206702
#% 1227681
#% 1227754
#% 1263244
#% 1275180
#% 1292546
#% 1483553
#% 1489451
#% 1697440
#! In this paper we investigate the task of Entity Ranking on the Web. Searchers looking for entities are arguably better served by presenting a ranked list of entities directly, rather than a list of web pages with relevant but also potentially redundant information about these entities. Since entities are represented by their web homepages, a naive approach to entity ranking is to use standard text retrieval. Our experimental results clearly demonstrate that text retrieval is effective at finding relevant pages, but performs poorly at finding entities. Our proposal is to use Wikipedia as a pivot for finding entities on the Web, allowing us to reduce the hard web entity ranking problem to easier problem of Wikipedia entity ranking. Wikipedia allows us to properly identify entities and some of their characteristics, and Wikipedia's elaborate category structure allows us to get a handle on the entity's type. Our main findings are the following. Our first finding is that, in principle, the problem of web entity ranking can be reduced to Wikipedia entity ranking. We found that the majority of entity ranking topics in our test collections can be answered using Wikipedia, and that with high precision relevant web entities corresponding to the Wikipedia entities can be found using Wikipedia's 'external links'. Our second finding is that we can exploit the structure of Wikipedia to improve entity ranking effectiveness. Entity types are valuable retrieval cues in Wikipedia. Automatically assigned entity types are effective, and almost as good as manually assigned types. Our third finding is that web entity retrieval can be significantly improved by using Wikipedia as a pivot. Both Wikipedia's external links and the enriched Wikipedia entities with additional links to homepages are significantly better at finding primary web homepages than anchor text retrieval, which in turn significantly improved over standard text retrieval.

#index 1482186
#* Ranking under temporal constraints
#@ Lidan Wang;Donald Metzler;Jimmy Lin
#t 2010
#c 1
#% 262096
#% 303620
#% 340886
#% 340887
#% 340948
#% 420077
#% 766428
#% 818229
#% 818262
#% 840927
#% 879651
#% 987215
#% 987216
#% 987243
#% 1019084
#% 1206832
#% 1227636
#% 1292550
#% 1355019
#% 1355057
#% 1450846
#! This paper introduces the notion of temporally constrained ranked retrieval, which, given a query and a time constraint, produces the best possible ranked list within the specified time limit. Naturally, more time should translate into better results, but the ranking algorithm should always produce some results. This property is desirable from a number of perspectives: to cope with diverse users and information needs, as well as to better manage system load and variance in query execution times. We propose two temporally constrained ranking algorithms based on a class of probabilistic prediction models that can naturally incorporate efficiency constraints: one that makes independent feature selection decisions, and the other that makes joint feature selection decisions. Experiments on three different test collections show that both ranking algorithms are able to satisfy imposed time constraints, although the joint model outperforms the independent model in being able to deliver more effective results, especially under tight time constraints, due to its ability to capture feature dependencies.

#index 1482187
#* Examining the information retrieval process from an inductive perspective
#@ Ronan Cummins;Mounia Lalmas;Colm O'Riordan
#t 2010
#c 1
#% 194275
#% 253191
#% 321635
#% 324129
#% 340146
#% 411760
#% 750863
#% 766412
#% 818263
#% 871571
#% 879579
#% 879607
#% 987229
#% 1227608
#% 1227616
#% 1227646
#% 1227703
#! Term-weighting functions derived from various models of retrieval aim to model human notions of relevance more accurately. However, there is a lack of analysis of the sources of evidence from which important features of these term weighting schemes originate. In general, features pertaining to these term-weighting schemes can be collected from (1) the document, (2) the entire collection and (3) the query. In this work, we perform an empirical analysis to determine the increase in effectiveness as information from these three different sources becomes more accurate. First, we determine the number of documents to be indexed to accurately estimate collection-wide features to obtain near optimal effectiveness for a range of a term-weighting functions. Similarly, we determine the amount of a document and query that must be sampled to achieve near-peak effectiveness. This analysis also allows us to determine the factors that contribute most to the performance of a term-weighting function (i.e. the document, the collection or the query). We use our framework to construct a new model of weighting where we discard the 'bag of words' model and aim to retrieve documents based on the initial physical representation of a document using some basic axioms of retrieval. We show that this is a good first step towards incorporating some more interesting features into a term-weighting function

#index 1482188
#* On identifying representative relevant documents
#@ Fiana Raiber;Oren Kurland
#t 2010
#c 1
#% 118726
#% 218992
#% 223810
#% 262096
#% 262112
#% 268079
#% 280864
#% 340901
#% 340948
#% 342707
#% 375017
#% 397161
#% 411762
#% 427921
#% 642975
#% 719598
#% 742666
#% 766518
#% 818209
#% 818222
#% 818241
#% 838466
#% 838528
#% 879575
#% 881477
#% 952491
#% 987200
#% 1019124
#% 1074072
#% 1292491
#% 1415748
#! Using relevance feedback can significantly improve the effectiveness of ad hoc (query-based) retrieval. However, retrieval performance can significantly vary with respect to the given set of relevant documents. Our goal is to establish a quantitative analysis of what makes a relevant document a good representative of the relevant-documents set regardless of the retrieval approach employed. That is, we would like to estimate the extent to which a relevant document can effectively help in finding (other) relevant documents using some relevance-feedback method employed over the corpus. We present various representativeness estimates; some of which treat documents independently and some utilize inter-document similarities. Empirical evaluation shows that relevant documents that are centrally located within the similarity space of the relevant-documents set tend to be good representatives. In addition, we show that there exist highly representative clusters of similar relevant documents, and devise methods for ranking clusters based on their presumed representativeness. Finally, we study the connection between representativeness and TREC's gradual relevance judgments.

#index 1482189
#* On the selectivity of multidimensional routing indices
#@ Christos Doulkeridis;Akrivi Vlachou;Kjetil Nørvåg;Yannis Kotidis;Michalis Vazirgiannis
#t 2010
#c 1
#% 45753
#% 252304
#% 286237
#% 340176
#% 427199
#% 462059
#% 577357
#% 636008
#% 646239
#% 765701
#% 772021
#% 864421
#% 907436
#% 907529
#% 949410
#% 960252
#% 981628
#% 1022284
#% 1044436
#% 1063526
#% 1091643
#% 1127353
#% 1291217
#% 1291219
#% 1292629
#% 1711093
#% 1851624
#! Recently, the problem of efficiently supporting advanced query operators, such as nearest neighbor or range queries, over multidimensional data in widely distributed environments has attracted much attention. In unstructured peer-to-peer (P2P) networks, peers store data in an autonomous manner, thus multidimensional routing indices (MRI) are required, in order to route user queries efficiently to only those peers that may contribute to the query result set. Focusing on a hybrid unstructured P2P network, in this paper, we analyze the parameters for building MRI of high selectivity. In the case where similar data are located at different parts of the network, MRI exhibit extremely poor performance, which renders them ineffective. We present algorithms that boost the query routing performance by detecting similar peers and reassigning these peers to other parts of the hybrid network in a distributed and scalable way. The resulting MRI are able to eagerly discard routing paths during query processing. We demonstrate the advantages of our approach experimentally and show that our framework enhances a state-of-the-art approach for similarity search in terms of reduced network traffic and number of contacted peers.

#index 1482190
#* Path-hop: efficiently indexing large graphs for reachability queries
#@ Jing Cai;Chung Keung Poon
#t 2010
#c 1
#% 31484
#% 58365
#% 88051
#% 379482
#% 824692
#% 864462
#% 960304
#% 1063514
#% 1217208
#! Graph reachability is a fundamental research problem that finds its use in many applications such as geographic navigation, bioinformatics, web ontologies and XML databases, etc. Given two vertices, u and v, in a directed graph, a reachability query asks if there is a directed path from u to v. Over the last two decades, many indexing schemes have been proposed to support reachability queries on large graphs. Typically, those schemes based on chain or tree covers work well when the graph is sparse. For dense graphs, they still have fast query time but require large storage for their indices. In contrast, the 2-Hop cover and its variations/extensions produce compact indices even for dense graphs but have slower query time than those chain/tree covers. In this paper, we propose a new indexing scheme, called Path-Hop, which is even more space-efficient than those schemes based on 2-Hop cover and yet has query processing speed comparable to those chain/tree covers. We conduct extensive experiments to illustrate the effectiveness of our approach relative to other state-of-the-art methods.

#index 1482191
#* On wavelet decomposition of uncertain time series data sets
#@ Yuchen Zhao;Charu Aggarwal;Philip Yu
#t 2010
#c 1
#% 235023
#% 248822
#% 257637
#% 273902
#% 480465
#% 572308
#% 823402
#% 824709
#% 864394
#% 918001
#% 977008
#% 1016201
#% 1179162
#% 1189215
#% 1206714
#% 1206892
#% 1214624
#! In this paper, we will explore the construction of wavelet decompositions of uncertain data. Uncertain representations of data sets require significantly more space, and it is therefore even more important to construct compressed representations for such cases. We will use a hierarchical optimization technique in order to construct the most effective partitioning for our wavelet representation. We explore two different schemes which optimize the uncertainty in the resulting representation. We will show that the incorporation of uncertainty into the design of the wavelet representations significantly improves the compression rate of the representation. We present experimental results illustrating the effectiveness of our approach.

#index 1482192
#* Efficient set-correlation operator inside databases
#@ Shaoxu Song;Lei Chen
#t 2010
#c 1
#% 67565
#% 227919
#% 248801
#% 280819
#% 375017
#% 466669
#% 577238
#% 577309
#% 643014
#% 729913
#% 747947
#% 763708
#% 766440
#% 766493
#% 769909
#% 818232
#% 818240
#% 838537
#% 840879
#% 863387
#% 864392
#% 864445
#% 869500
#% 893164
#% 907510
#% 939939
#% 987193
#% 987341
#% 1127368
#% 1328164
#% 1392432
#% 1650298
#! Large scale of short text records are now prevalent, such as news highlights, scientific paper citations, and posted messages in a discussion forum, which are often stored as set records in (hidden) databases. Many interesting information retrieval tasks are correspondingly raised on the correlation query over these short text records, such as finding hot topics over news highlights and searching related scientific papers on a certain topic. However, current relational database management systems (RDBMS) do not directly provide support on set correlation query. Thus, in this paper, we address both the effectiveness and efficiency issues of set correlation query over set records in databases. First, we present a framework of set correlation query inside databases. To our best knowledge, only the Pearson's correlation can be implemented to construct token correlations by using RDBMS facilities. Thereby, we propose a novel correlation coefficient to extend Pearson's correlation, and provide a pure-SQL implementation inside databases. We further propose optimal strategies to set up correlation filtering threshold, which can greatly reduce the query time. Our theoretical analysis proves that, with a proper setting of filtering threshold, we can improve the query efficiency with a little effectiveness loss. Finally, we conduct extensive experiments to show the effectiveness and efficiency of proposed correlation query and optimization strategies.

#index 1482193
#* Online update of b-trees
#@ Marina Barsky;Alex Thomo;Zoltan Toth;Calisto Zuzarte
#t 2010
#c 1
#% 86532
#% 172922
#% 208047
#% 213786
#% 268079
#% 443093
#% 479473
#% 481771
#% 570884
#% 577310
#% 604239
#% 857498
#% 867054
#% 879609
#% 1077150
#! Many scenarios impose a heavy update load on B-tree indexes in modern databases. A typical case is when B-trees are used for indexing all the keywords of a text field. For example upon the insertion of a new text record (e.g. a new document arrives), a barrage of new keywords has to be inserted into the index causing many random disk I/Os and interrupting the normal operation of the database. The common approach has been to collect the updates in a separate structure and then perform a batch update of the index. This update "freezes" the database. Many applications, however, require the immediate availability of the new updates without any interruption of the normal database operation. In this paper we present a novel online B-tree update method based on a new buffering data structure we introduce - Dynamic Bucket Tree (DBT). The DBT-buffer serves as a differential index for new updates. The grouping of keys in DBT-buffer is based on the longest common prefixes (LCP) of their binary representations. The LCP is used as a measure of the locality of keys to be transferred to the main B-tree. Our online update system does not slow down concurrent user transactions or lead to degradation of search performance. Experiments confirm that our DBT buffer can be efficiently used for online updates of text fields. As such it represents an effective solution to the notorious problem of handling updates to an Inverted Index.

#index 1482194
#* Wisdom of the ages: toward delivering the children's web with the link-based agerank algorithm
#@ Karl Gyllstrom;Marie-Francine Moens
#t 2010
#c 1
#% 268079
#% 310514
#% 348173
#% 411762
#% 449291
#% 575733
#% 987201
#% 1016177
#% 1023420
#% 1210687
#! Though children frequently use web search engines to learn, interact, and be entertained, modern web search engines are poorly suited to children's needs, requiring relatively complex querying and filtering of results in order to find pages oriented to young audiences. To address this limitation, we designed AgeRank, a link-based algorithm that ranks web pages according their appropriateness for young audiences. We show its effectiveness through a multipart evaluation that demonstrates AgeRank to be accurate in page-labeling, widely-spanning in page coverage, and with high potential to improve children's search. As a fast, scalable, and effective algorithm, AgeRank can be adopted by search engines seeking to more effectively address the needs of young users, or easily fitted to complementary machine-learning based classification approaches.

#index 1482195
#* A cross-lingual framework for monolingual biomedical information retrieval
#@ Dolf Trieschnigg;Djoerd Hiemstra;Franciska de Jong;Wessel Kraaij
#t 2010
#c 1
#% 78171
#% 262047
#% 279755
#% 280851
#% 375017
#% 397145
#% 556836
#% 561312
#% 740915
#% 766429
#% 799695
#% 951636
#% 985825
#% 987336
#% 1126949
#% 1156211
#% 1208119
#! An important challenge for biomedical information retrieval (IR) is dealing with the complex, inconsistent and ambiguous biomedical terminology. Frequently, a concept-based representation defined in terms of a domain-specific terminological resource is employed to deal with this challenge. In this paper, we approach the incorporation of a concept-based representation in monolingual biomedical IR from a cross-lingual perspective. In the proposed framework, this is realized by translating and matching between text and concept-based representations. The approach allows for deployment of a rich set of techniques proposed and evaluated in traditional cross-lingual IR. We compare six translation models and measure their effectiveness in the biomedical domain. We demonstrate that the approach can result in significant improvements in retrieval effectiveness over word-based retrieval. Moreover, we demonstrate increased effectiveness of a CLIR framework for monolingual biomedical IR if basic translations models are combined.

#index 1482196
#* Multi-modal multi-correlation person-centric news retrieval
#@ Zechao Li;Jing Liu;Xiaobin Zhu;Hanqing Lu
#t 2010
#c 1
#% 262042
#% 301241
#% 397133
#% 411762
#% 496116
#% 504443
#% 722926
#% 754068
#% 818215
#% 830520
#% 987253
#% 989618
#% 1130901
#% 1130902
#% 1148252
#% 1190065
#% 1214674
#% 1275182
#% 1289520
#% 1650298
#% 1742126
#! In this paper, we propose a framework of multi-modal multi-correlation person-centric news retrieval, which integrates news event correlations, news entity correlations, and event-entity correlations simultaneously by exploring both text and image information. The proposed framework is confined to a person-name query and enables a more vivid and informative person-centric news retrieval by providing two views of result presentation, namely a query-oriented multi-correlation map and a ranking list of news items with necessary descriptions including news image, news title and summary, central entities and relevant news events. First, we pre-process news articles using natural language techniques, and initialize the three correlations by statistical analysis about events and entities in news articles and face images. Second, a Multi-correlation Probabilistic Matrix Factorization (MPMF) algorithm is proposed to complete and refine the three correlations. Different from traditional Probabilistic Matrix Factorization (PMF), the proposed MPFM additionally considers the event correlations and the entity correlations as well as the event-entity correlations during the factor analysis. Third, the result ranking and visualization are conducted to present search results relevant to a target news topic. Experimental results on a news dataset collected from multiple news websites demonstrate the attractive performance of the proposed solution for news retrieval.

#index 1482197
#* Bringing order to your photos: event-driven classification of flickr images based on social knowledge
#@ Claudiu S. Firan;Mihai Georgescu;Wolfgang Nejdl;Raluca Paiu
#t 2010
#c 1
#% 262042
#% 262043
#% 722902
#% 818215
#% 824666
#% 855601
#% 869482
#% 956515
#% 956564
#% 958000
#% 987205
#% 987218
#% 1055704
#% 1077150
#% 1130827
#% 1176878
#% 1190231
#% 1213444
#% 1213625
#% 1292518
#% 1355045
#! With the rapidly increasing popularity of Social Media sites, a lot of user generated content has been injected in the Web, thus resulting in a large amount of both multimedia items (music - Last.fm, MySpace.com, pictures - Flickr, Picasa, videos - YouTube) and textual data (tags and other text-based documents). As a consequence, especially for multimedia content it has become more and more difficult to find exactly the objects that best match the users' information needs. The methods we propose in this paper try to alleviate this problem and we focus on the domain of pictures, in particular on a subset of Flickr data. Many of the photos posted by users on Flickr have been shot during events and our methods aim to allow browsing and organization of picture collections in a natural way, by events. The algorithms we introduce in this paper exploit the social information produced by users in form of tags, titles and photo descriptions, for classifying pictures into different event categories. The extensive automated experiments demonstrate that our approach is very effective and opens new possibilities for multimedia retrieval, in particular image search. Moreover, the direct comparison with previous event detection algorithms confirm once more the quality of our methods.

#index 1482198
#* Mining topic-level influence in heterogeneous networks
#@ Lu Liu;Jie Tang;Jiawei Han;Meng Jiang;Shiqiang Yang
#t 2010
#c 1
#% 27724
#% 342596
#% 348173
#% 577217
#% 729923
#% 754098
#% 754107
#% 956540
#% 983833
#% 1055737
#% 1083624
#% 1083641
#% 1083672
#% 1083684
#% 1083738
#% 1181261
#% 1214638
#% 1214696
#% 1214701
#% 1214702
#% 1214703
#% 1214714
#% 1214722
#% 1355040
#% 1399993
#% 1400031
#% 1451243
#! Influence is a complex and subtle force that governs the dynamics of social networks as well as the behaviors of involved users. Understanding influence can benefit various applications such as viral marketing, recommendation, and information retrieval. However, most existing works on social influence analysis have focused on verifying the existence of social influence. Few works systematically investigate how to mine the strength of direct and indirect influence between nodes in heterogeneous networks. To address the problem, we propose a generative graphical model which utilizes the heterogeneous link information and the textual content associated with each node in the network to mine topic-level direct influence. Based on the learned direct influence, a topic-level influence propagation and aggregation algorithm is proposed to derive the indirect influence between nodes. We further study how the discovered topic-level influence can help the prediction of user behaviors. We validate the approach on three different genres of data sets: Twitter, Digg, and citation networks. Qualitatively, our approach can discover interesting influence patterns in heterogeneous networks. Quantitatively, the learned topic-level influence can greatly improve the accuracy of user behavior prediction.

#index 1482199
#* Mining interesting link formation rules in social networks
#@ Cane Wing-ki Leung;Ee-Peng Lim;David Lo;Jianshu Weng
#t 2010
#c 1
#% 466644
#% 478274
#% 629708
#% 729938
#% 754098
#% 1268040
#% 1269378
#% 1281961
#% 1384246
#% 1669913
#! Link structures are important patterns one looks out for when modeling and analyzing social networks. In this paper, we propose the task of mining interesting Link Formation rules (LF-rules) containing link structures known as Link Formation patterns (LF-patterns). LF-patterns capture various dyadic and/or triadic structures among groups of nodes, while LF-rules capture the formation of a new link from a focal node to another node as a postcondition of existing connections between the two nodes. We devise a novel LF-rule mining algorithm, known as LFR-Miner, based on frequent subgraph mining for our task. In addition to using a support-confidence framework for measuring the frequency and significance of LF-rules, we introduce the notion of expected support to account for the extent to which LF-rules exist in a social network by chance. Specifically, only LF-rules with higher-than-expected support are considered interesting. We conduct empirical studies on two real-world social networks, namely Epinions and myGamma. We report interesting LF-rules mined from the two networks, and compare our findings with earlier findings in social network analysis.

#index 1482200
#* SHRINK: a structural clustering algorithm for detecting hierarchical communities in networks
#@ Jianbin Huang;Heli Sun;Jiawei Han;Hongbo Deng;Yizhou Sun;Yaguang Liu
#t 2010
#c 1
#% 273890
#% 282905
#% 313959
#% 342596
#% 989654
#% 991977
#% 1914479
#! Community detection is an important task for mining the structure and function of complex networks. Generally, there are several different kinds of nodes in a network which are cluster nodes densely connected within communities, as well as some special nodes like hubs bridging multiple communities and outliers marginally connected with a community. In addition, it has been shown that there is a hierarchical structure in complex networks with communities embedded within other communities. Therefore, a good algorithm is desirable to be able to not only detect hierarchical communities, but also identify hubs and outliers. In this paper, we propose a parameter-free hierarchical network clustering algorithm SHRINK by combining the advantages of density-based clustering and modularity optimization methods. Based on the structural connectivity information, the proposed algorithm can effectively reveal the embedded hierarchical community structure with multiresolution in large-scale weighted undirected networks, and identify hubs and outliers as well. Moreover, it overcomes the sensitive threshold problem of density-based clustering algorithms and the resolution limit possessed by other modularity-based methods. To illustrate our methodology, we conduct experiments with both real-world and synthetic datasets for community detection, and compare with many other baseline methods. Experimental results demonstrate that SHRINK achieves the best performance with consistent improvements.

#index 1482201
#* Outcome aware ranking in interaction networks
#@ Sampath Kameshwaran;Vinayaka Pandit;Sameep Mehta;Nukala Viswanadham;Kashyap Dixit
#t 2010
#c 1
#% 290830
#% 348173
#% 729936
#% 783554
#% 972343
#% 989663
#% 1348087
#! In this paper, we present a novel ranking technique that we developed in the context of an application that arose in a Service Delivery setting. We consider the problem of ranking agents of a service organization. The service agents typically need to interact with other service agents to accomplish the end goal of resolving customer requests. Their ranking needs to take into account two aspects: firstly, their importance in the network structure that arises as a result of their interactions, and secondly, the value generated by the interactions involving them. We highlight several other applications which have the common theme of ranking the participants of a value creation process based on the network structure of their interactions and the value generated by their interactions. We formally present the problem and describe the modeling technique which enables us to encode the value of interaction in the graph. Our ranking algorithm is based on extension of eigen value methods. We present experimental results on real-life, public domain datasets from the Internet Movie DataBase. This makes our experiments replicable and verifiable.

#index 1482202
#* Expansion and search in networks
#@ Arun S. Maiya;Tanya Y. Berger-Wolf
#t 2010
#c 1
#% 217812
#% 256685
#% 300078
#% 636009
#% 748017
#% 823342
#% 878648
#% 902713
#% 957006
#% 1055741
#% 1081531
#% 1400003
#% 1407592
#% 1484200
#! Borrowing from concepts in expander graphs, we study the expansion properties of real-world, complex networks (e.g. social networks, unstructured peer-to-peer or P2P networks) and the extent to which these properties can be exploited to understand and address the problem of decentralized search. We first produce samples that concisely capture the overall expansion properties of an entire network, which we collectively refer to as the expansion signature. Using these signatures, we find a correspondence between the magnitude of maximum expansion and the extent to which a network can be efficiently searched. We further find evidence that standard graph-theoretic measures, such as average path length, fail to fully explain the level of "searchability" or ease of information diffusion and dissemination in a network. Finally, we demonstrate that this high expansion can be leveraged to facilitate decentralized search in networks and show that an expansion-based search strategy outperforms typical search methods.

#index 1482203
#* Improved latent concept expansion using hierarchical markov random fields
#@ Hao Lang;Donald Metzler;Bin Wang;Jin-Tao Li
#t 2010
#c 1
#% 262096
#% 298183
#% 340901
#% 340950
#% 342707
#% 577224
#% 643015
#% 766497
#% 818262
#% 939939
#% 976952
#% 987226
#% 987231
#% 1019093
#% 1074080
#% 1074081
#% 1190106
#% 1195883
#% 1227614
#% 1264966
#% 1268491
#% 1292550
#% 1292759
#% 1355019
#! Most existing query expansion approaches for ad-hoc retrieval adopt overly simplistic textual representations that treat documents as bags of words and ignore inherent document structure. These simple representations often lead to incorrect independence assumptions in the proposed approaches and result in limited retrieval effectiveness. In this paper, we propose a novel query expansion technique that models the various types of dependencies that exist between original query terms and expansion terms within a robust, unified framework. The proposed model is called Hierarchical Markov random fields (HMRFs), based on Latent Concept Expansion (LCE). By exploiting implicit (or explicit) hierarchical structure within documents, HMRFs can incorporate hierarchical interactions which are important for modeling term dependencies in an efficient manner. Our rigorous experimental evaluation carried out using several TREC data sets shows that our proposed query expansion technique consistently and significantly outperforms the current state-of-the-art query expansion approaches, including relevance-based language models and LCE.

#index 1482204
#* Term necessity prediction
#@ Le Zhao;Jamie Callan
#t 2010
#c 1
#% 111304
#% 194283
#% 262037
#% 262096
#% 289340
#% 340901
#% 375017
#% 397161
#% 766503
#% 1019124
#% 1074112
#% 1130847
#% 1156207
#% 1195837
#% 1227647
#% 1292650
#% 1355020
#! The probability that a term appears in relevant documents (P(t | R)) is a fundamental quantity in several probabilistic retrieval models, however it is difficult to estimate without relevance judgments or a relevance model. We call this value term necessity because it measures the percentage of relevant documents retrieved by the term - how necessary a term's occurrence is to document relevance. Prior research typically either set this probability to a constant, or estimated it based on the term's inverse document frequency, neither of which was very effective. This paper identifies several factors that affect term necessity, for example, a term's topic centrality, synonymy and abstractness. It develops term- and query-dependent features for each factor that enable supervised learning of a predictive model of term necessity from training data. Experiments with two popular retrieval models and 6 standard datasets demonstrate that using predicted term necessity estimates as user term weights of the original query terms leads to significant improvements in retrieval accuracy.

#index 1482205
#* Decomposing background topics from keywords by principal component pursuit
#@ Kerui Min;Zhengdong Zhang;John Wright;Yi Ma
#t 2010
#c 1
#% 49501
#% 248027
#% 280819
#% 280822
#% 329569
#% 420480
#% 529316
#% 722904
#% 879587
#% 1227575
#% 1268491
#% 1292484
#% 1292549
#! Low-dimensional topic models have been proven very useful for modeling a large corpus of documents that share a relatively small number of topics. Dimensionality reduction tools such as Principal Component Analysis or Latent Semantic Indexing (LSI) have been widely adopted for document modeling, analysis, and retrieval. In this paper, we contend that a more pertinent model for a document corpus as the combination of an (approximately) low-dimensional topic model for the corpus and a sparse model for the keywords of individual documents. For such a joint topic-document model, LSI or PCA is no longer appropriate to analyze the corpus data. We hence introduce a powerful new tool called Principal Component Pursuit that can effectively decompose the low-dimensional and the sparse components of such corpus data. We give empirical results on data synthesized with a Latent Dirichlet Allocation (LDA) mode to validate the new model. We then show that for real document data analysis, the new tool significantly reduces the perplexity and improves retrieval performance compared to classical baselines.

#index 1482206
#* Document update summarization using incremental hierarchical clustering
#@ Dingding Wang;Tao Li
#t 2010
#c 1
#% 65440
#% 232768
#% 340884
#% 340971
#% 387427
#% 451052
#% 594012
#% 742513
#% 770830
#% 787502
#% 815920
#% 816173
#% 907522
#% 1074088
#% 1074089
#% 1096117
#% 1275040
#% 1275213
#! Document summarization has become a hot topic in recent years. However, most of existing summarization methods work on a batch of documents and do not consider that documents may arrive in a sequence and the corresponding summaries need to be updated in real time. In this paper, we propose a new summarization method based on an incremental hierarchical clustering framework to update summaries as soon as a new document arrives. Extensive experimental results demonstrate the effectiveness and efficiency of our proposed method.

#index 1482207
#* How about utilizing ordinal information from the distribution of unlabeled data
#@ Mingjie Qian;Bo Chen;Hongzhi Xu;Hongwei Qi
#t 2010
#c 1
#% 131165
#% 466263
#% 840853
#% 876068
#% 939346
#% 961218
#% 983815
#% 1073923
#% 1269778
#% 1292673
#% 1318749
#% 1455666
#! Problems of ordinal regression arise in many fields such as information retrieval, data mining and knowledge management. In this paper, we consider ordinal regression in a semi-supervised scenario, i.e., we try to utilize the ordinal information from the distribution of unlabeled data. Semi-supervised ordinal regression is more applicable than traditional supervised ordinal regression, because nowadays labeled data is expensive and time-consuming as it needs human labor, whereas a large amount of unlabeled data are far accessible with the development of internet technology. We construct a general semi-supervised ordinal regression framework to formulate this problem. Based on the framework, we then propose a semi-supervised ordinal regression method called Semi-supervised Ordinal SVM (SOSVM). Additionally, in order to make our proposed method more applicable to problems with large scaled labeled data, we put forward a kernel based dual coordinate descent algorithm to efficiently solve SOSVM. Both rigorous theoretical analysis and promising experimental evaluations on real world datasets show the great performance and remarkable efficiency of SOSVM.

#index 1482208
#* Automatic schema merging using mapping constraints among incomplete sources
#@ Xiang Li;Christoph Quix;David Kensche;Sandra Geisler
#t 2010
#c 1
#% 11284
#% 13048
#% 22948
#% 237190
#% 328429
#% 378409
#% 384978
#% 415979
#% 435102
#% 464717
#% 480969
#% 481290
#% 806215
#% 826032
#% 830529
#% 841959
#% 850730
#% 874881
#% 960233
#% 1015326
#% 1022349
#% 1044441
#% 1063532
#% 1063534
#% 1063710
#% 1063724
#% 1190673
#% 1217195
#% 1232194
#% 1395953
#% 1424597
#% 1426463
#! Schema merging is the process of consolidating multiple schemas into a unified view. The task becomes particularly challenging when the schemas are highly heterogeneous and autonomous. Classical data integration systems rely on a mediated schema created by human experts through an intensive design process. In this paper, we present a novel approach for merging multiple relational data sources related by a collection of mapping constraints in the form of P2P style tuple-generating dependencies (tgds). In the scenario of data integration, we opt for minimal mediated schemas that are complete regarding certain answers of conjunctive queries. Under Open World Assumption (OWA), we characterize the semantics of schema merging by properties of the output mapping system between the source schemas and the mediated schema. We propose a merging algorithm following a redundancy reduction paradigm and prove that the output satisfies the desired logical properties. Recognizing the fact that multiple plausible mediated schemas may co-exist, a variant of the a priori algorithm is employed to enumerate alternative mediated schemas. Output mappings in the form of data dependencies are generated to support the mediated schemas, which enables query processing. We have evaluated our merging approach over a collection of real world data sets, which demonstrate the applicability and effectiveness of our approach in practice.

#index 1482209
#* Preserving location and absence privacy in geo-social networks
#@ Dario Freni;Carmen Ruiz Vicente;Sergio Mascetti;Claudio Bettini;Christian S. Jensen
#t 2010
#c 1
#% 390187
#% 1013611
#% 1125823
#% 1245031
#% 1265776
#% 1298885
#% 1405442
#% 1445732
#! Online social networks often involve very large numbers of users who share very large volumes of content. This content is increasingly being tagged with geo-spatial and temporal coordinates that may then be used in services. For example, a service may retrieve photos taken in a certain region. The resulting geo-aware social networks (GeoSNs) pose privacy threats beyond those found in location-based services. Content published in a GeoSN is often associated with references to multiple users, without the publisher being aware of the privacy preferences of those users. Moreover, this content is often accessible to multiple users. This renders it difficult for GeoSN users to control which information about them is available and to whom it is available. This paper addresses two privacy threats that occur in GeoSNs: location privacy and absence privacy. The former concerns the availability of information about the presence of users in specific locations at given times, while the latter concerns the availability of information about the absence of an individual from specific locations during given periods of time. The challenge addressed is that of supporting privacy while still enabling useful services. We believe this is the first paper to formalize these two notions of privacy and to propose techniques for enforcing them. The techniques offer privacy guarantees, and the paper reports on empirical performance studies of the techniques.

#index 1482210
#* Preference query evaluation over expensive attributes
#@ Justin J. Levandoski;Mohamed F. Mokbel;Mohamed E. Khalefa
#t 2010
#c 1
#% 152940
#% 172931
#% 248795
#% 273911
#% 397378
#% 654480
#% 745519
#% 864453
#% 873881
#% 893150
#% 907527
#% 994017
#% 1016207
#% 1044463
#! Most database systems allow query processing over attributes that are derived at query runtime (e.g., user-defined functions and remote data calls to web services), making them expensive to compute relative to relational data stored in a heap or index. In addition, core support for efficient preference query processing has become an important objective in database systems. This paper addresses an important problem at the intersection of these two query processing objectives: efficient preference query evaluation involving expensive attributes. We explore an efficient framework for processing skyline and multi-objective queries in a database when the data involves a mix of "cheap" and "expensive" attributes. Our solution involves a three-phase approach that evaluates a correct final preference answer while aiming to minimizing the number of expensive attributes computations. Unlike previous works for distributed preference algorithms that assume sorted access over each attribute, our framework assumes expensive attribute requests are stateless, i.e., know nothing previous requests. Thus, the proposed approach is more in line with realistic system architectures. Our framework is implemented inside the query processor of PostgreSQL, and evaluated over both synthetic and real data sets involving computation of expensive attributes over real web-service data (e.g., Microsoft MapPoint).

#index 1482211
#* Energy-efficient top-k query processing in wireless sensor networks
#@ Baichen Chen;Weifa Liang;Rui Zhou;Jeffrey Xu Yu
#t 2010
#c 1
#% 297915
#% 401228
#% 427022
#% 629097
#% 643566
#% 654443
#% 654482
#% 763882
#% 783740
#% 801695
#% 822531
#% 864455
#% 864530
#% 982752
#% 1022217
#% 1075132
#% 1131016
#% 1197114
#% 1831268
#! Technological advances have enabled the deployment of large-scale sensor networks for environmental monitoring and surveillance purposes. The large volume of data generated by sensors needs to be processed to respond to the users queries. However, efficient processing of queries in sensor networks poses great challenges due to the unique characteristics imposed on sensor networks including slow processing capability, limited storage, and energy-limited batteries, etc. Among various queries, top-k query is one of the fundamental operators in many applications of wireless sensor networks for phenomenon monitoring. In this paper we focus on evaluating top-k queries in an energy-efficient manner such that the network lifetime is maximized. To achieve that, we devise a scalable, filter-based localized evaluation algorithm for top-k query evaluation, which is able to filter out as many unlikely top-k results as possible within the network from transmission. We also conduct extensive experiments by simulations to evaluate the performance of the proposed algorithm on real datasets. The experimental results show that the proposed algorithm outperforms existing algorithms significantly in network lifetime prolongation.

#index 1482212
#* StableBuffer: optimizing write performance for DBMS applications on flash devices
#@ Yu Li;Jianliang Xu;Byron Choi;Haibo Hu
#t 2010
#c 1
#% 483048
#% 780903
#% 957869
#% 960238
#% 978505
#% 985755
#% 987738
#% 993385
#% 1053488
#% 1063551
#% 1213385
#% 1217151
#% 1217152
#% 1222046
#% 1222047
#% 1245061
#% 1328139
#% 1426532
#% 1482312
#% 1523901
#! Flash devices have been widely used in embedded systems, laptop computers, and enterprise servers. However, the poor random writes have been an obstacle to running write-intensive DBMS applications on flash devices. In this paper, we exploit the recently discovered, efficient write patterns of flash devices to optimize the performance of DBMS applications. Specifically, motivated by a focused write pattern, we propose to write pages temporarily to a small, pre-allocated storage space on the flash device, called StableBuffer, instead of directly writing to their actual destinations. We then recognize and flush efficient write patterns of the buffer to achieve a better write performance. In contrast to prior log-based techniques, our StableBuffer solution does not require modifying the driver of flash devices and hence works well for commodity flash devices. We discuss the detailed design and implementation of the StableBuffer solution. Performance evaluation based on a TPC-C benchmark trace shows that StableBuffer improves the response time and throughput of write operations by a factor of 1.5-12, in comparison with a direct write-through strategy.

#index 1482213
#* Mr.KNN: soft relevance for multi-label classification
#@ Xiaotong Lin;Xue-wen Chen
#t 2010
#c 1
#% 190581
#% 311034
#% 333820
#% 374537
#% 411762
#% 478470
#% 722924
#% 729437
#% 783478
#% 838412
#% 875967
#% 879446
#% 889101
#% 905168
#% 906025
#% 950571
#% 961181
#% 961192
#% 997067
#% 1093383
#% 1095861
#% 1100077
#% 1105752
#% 1117042
#% 1233357
#% 1274865
#% 1292516
#% 1309606
#% 1663618
#% 1775480
#% 1858650
#! Multi-label classification refers to learning tasks with each instance belonging to one or more classes simultaneously. It arose from real-world applications such as information retrieval, text categorization and functional genomics. Currently, most of the multi-label learning methods use the strategy called binary relevance, which constructs a classifier for each unique label by grouping data into positives (examples with this label) and negatives (examples without this label). With binary relevance, an example with multiple labels is considered as a positive data for each label it belongs to. For some classes, this data point may behave like an outlier confusing classifiers, especially in the cases of well-separated classes. In this paper, we first introduce a new strategy called soft relevance, where each multi-label example is assigned a relevance score to the labels it belongs to. This soft relevance is then employed in a voting function used in a k nearest neighbor classifier. Furthermore, a voting-margin ratio is introduced to the k nearest neighbor classifier for better performance. We compare the proposed method to other multi-label learning methods over three multi-label datasets and demonstrate that the proposed method provides an effective way to multi-label learning.

#index 1482214
#* Collaborative Dual-PLSA: mining distinction and commonality across multiple domains for text classification
#@ Fuzhen Zhuang;Ping Luo;Zhiyong Shen;Qing He;Yuhong Xiong;Zhongzhi Shi;Hui Xiong
#t 2010
#c 1
#% 769967
#% 983828
#% 989592
#% 1019099
#% 1074129
#% 1083655
#% 1100153
#% 1130817
#% 1190064
#% 1211714
#% 1214655
#% 1237826
#% 1270196
#% 1318623
#% 1377374
#% 1650298
#! The distribution difference among multiple data domains has been considered for the cross-domain text classification problem. In this study, we show two new observations along this line. First, the data distribution difference may come from the fact that different domains use different key words to express the same concept. Second, the association between this conceptual feature and the document class may be stable across domains. These two issues are actually the distinction and commonality across data domains. Inspired by the above observations, we propose a generative statistical model, named Collaborative Dual-PLSA (CD-PLSA), to simultaneously capture both the domain distinction and commonality among multiple domains. Different from Probabilistic Latent Semantic Analysis (PLSA) with only one latent variable, the proposed model has two latent factors y and z, corresponding to word concept and document class respectively. The shared commonality intertwines with the distinctions over multiple domains, and is also used as the bridge for knowledge transformation. We exploit an Expectation Maximization (EM) algorithm to learn this model, and also propose its distributed version to handle the situation where the data domains are geographically separated from each other. Finally, we conduct extensive experiments over hundreds of classification tasks with multiple source domains and multiple target domains to validate the superiority of the proposed CD-PLSA model over existing state-of-the-art methods of supervised and transfer learning. In particular, we show that CD-PLSA is more tolerant of distribution differences.

#index 1482215
#* Inferring gender of movie reviewers: exploiting writing style, content and metadata
#@ Jahna Otterbacher
#t 2010
#c 1
#% 1776
#% 124010
#% 279755
#% 318412
#% 376266
#% 406493
#% 740416
#% 787502
#% 810005
#% 835906
#% 907490
#% 946521
#% 956508
#% 1190069
#% 1281821
#% 1292506
#% 1729001
#! Despite differences in the way that men and women experience goods and communicate their perspectives, online review communities typically do not provide participants' gender. We propose to infer author gender, given a set of reviews of a particular item, and experiment on reviews posted at the Internet Movie Database (IMDb). Using logistic regression, we explore the contribution of three types of information: 1) style, 2) content, and 3) metadata (e.g. review age, social feedback). Our results concur with previous research, in that there are salient differences in writing style and content between reviews authored by men versus women. However, in comparison to literary or scientific texts, to which classification tasks are often applied, reviews are brief and occur within the context of an ongoing discourse. Therefore, to compensative for the brevity of reviews, content and stylistic features can be augmented with metadata. We find in particular that the perceived utility of a review is an important correlate of gender. The model incorporating all features has a classification accuracy of 73.7% and is not as sensitive to review length as are those based only on stylistic or content features.

#index 1482216
#* A robust semi-supervised classification method for transfer learning
#@ Akinori Fujino;Naonori Ueda;Masaaki Nagata
#t 2010
#c 1
#% 73441
#% 190581
#% 311027
#% 329562
#% 770847
#% 883830
#% 916788
#% 961134
#% 961195
#% 983814
#% 989599
#% 1031854
#% 1073945
#% 1083678
#% 1261539
#% 1269755
#% 1305444
#% 1305479
#% 1305496
#% 1338586
#% 1338687
#% 1464068
#! The transfer learning problem of designing good classifiers with a high generalization ability by using labeled samples whose distribution is different from that of test samples is an important and challenging research issue in the fields of machine learning and data mining. This paper focuses on designing a semi-supervised classifier trained by using unlabeled samples drawn by the same distribution as test samples, and presents a semi-supervised classification method to deal with the transfer learning problem, based on a hybrid discriminative and generative model. Although JESS-CM is one of the most successful semi-supervised classifier design frameworks and has achieved the best published results in NLP tasks, it has an overfitting problem in transfer learning settings that we consider in this paper. We expect the overfitting problem to be mitigated with the proposed method, which utilizes both labeled and unlabeled samples for the discriminative training of classifiers. We also present a refined objective that formalizes the training algorithm and classifier form. Our experimental results for text classification using three typical benchmark test collections confirmed that the proposed method outperformed the JESS-CM framework with most transfer learning settings.

#index 1482217
#* Multi-view clustering with constraint propagation for learning with an incomplete mapping between views
#@ Eric Eaton;Marie desJardins;Sara Jacob
#t 2010
#c 1
#% 252011
#% 316509
#% 464291
#% 464608
#% 464631
#% 715529
#% 770782
#% 785334
#% 829025
#% 859289
#% 926881
#% 1023420
#% 1211706
#% 1269506
#% 1318663
#% 1408842
#! Multi-view learning algorithms typically assume a complete bipartite mapping between the different views in order to exchange information during the learning process. However, many applications provide only a partial mapping between the views, creating a challenge for current methods. To address this problem, we propose a multi-view algorithm based on constrained clustering that can operate with an incomplete mapping. Given a set of pairwise constraints in each view, our approach propagates these constraints using a local similarity measure to those instances that can be mapped to the other views, allowing the propagated constraints to be transferred across views via the partial mapping. It uses co-EM to iteratively estimate the propagation within each view based on the current clustering model, transfer the constraints across views, and update the clustering model, thereby learning a unified model for all views. We show that this approach significantly improves clustering performance over several other methods for transferring constraints and allows multi-view clustering to be reliably applied when given a limited mapping between the views.

#index 1482218
#* Pricing guaranteed contracts in online display advertising
#@ Vijay Bharadwaj;Wenjing Ma;Michael Schwarz;Jayavel Shanmugasundaram;Erik Vee;Jack Xie;Jian Yang
#t 2010
#c 1
#% 739577
#% 818584
#% 1246501
#% 1336429
#! We consider the problem of pricing guaranteed contracts in online display advertising. This problem has two key characteristics that when taken together distinguish it from related offline and online pricing problems: (1) the guaranteed contracts are sold months in advance, and at various points in time, and (2) the inventory that is sold to guaranteed contracts - user visits - is very high-dimensional, having hundreds of possible attributes, and advertisers can potentially buy any of the very large number (many trillions) of combinations of these attributes. Consequently, traditional pricing methods such as real-time or combinatorial auctions, or optimization-based pricing based on self- and cross-elasticities are not directly applicable to this problem. We hence propose a new pricing method, whereby the price of a guaranteed contract is computed based on the prices of the individual user visits that the contract is expected to get. The price of each individual user visit is in turn computed using historical sales prices that are negotiated between a sales person and an advertiser, and we propose two different variants in this context. Our evaluation using real guaranteed contracts shows that the proposed pricing method is accurate in the sense that it can effectively predict the prices of other (out-of-sample) historical contracts.

#index 1482219
#* Maximum normalized spacing for efficient visual clustering
#@ Zhi-Gang Fan;Yadong Wu;Bo Wu
#t 2010
#c 1
#% 177826
#% 296738
#% 310516
#% 313959
#% 317178
#% 413610
#% 466675
#% 635713
#% 721163
#% 729437
#% 748636
#% 780874
#% 798967
#% 836752
#% 871628
#% 883822
#% 900686
#% 915325
#% 992320
#% 1010466
#% 1828425
#% 1855804
#% 1861495
#! In this paper, for efficient clustering of visual image data that have arbitrary mixture distributions, we propose a simple distance metric learning method called Maximum Normalized Spacing (MNS) which is a generalized principle based on Maximum Spacing [12] and Minimum Spanning Tree (MST). The proposed Normalized Spacing (NS) can be viewed as a kind of adaptive distance metric for contextual dissimilarity measure which takes into account the local distribution of the data vectors. Image clustering is a difficult task because there are multiple nonlinear manifolds embedded in the data space. Many of the existing clustering methods often fail to learn the whole structure of the multiple manifolds and they are usually not very effective. Combining both the internal and external statistics of clusters to capture the density structure of manifolds, MNS is capable of efficient and effective solving the clustering problem for the complex multi-manifold datasets in arbitrary metric spaces. We apply this MNS method into the practical problem of multi-view image clustering and obtain good results which are helpful for image browsing systems. Using the COIL-20 [19] and COIL-100 [18] multi-view image databases, our experimental results demonstrate the effectiveness of the proposed MNS clustering method and this clustering method is more efficient than the traditional clustering methods.

#index 1482220
#* Multilevel manifold learning with application to spectral clustering
#@ Haw-ren Fang;Sophia Sakellaridi;Yousef Saad
#t 2010
#c 1
#% 258598
#% 324288
#% 420083
#% 494539
#% 593047
#% 723241
#% 724227
#% 755463
#% 840933
#% 871628
#% 889151
#% 895895
#% 1159796
#% 1385976
#! In the past decade, a number of nonlinear dimensionality reduction methods using an affinity graph have been developed for manifold learning. This paper explores a multilevel framework with the goal of reducing the cost of unsupervised manifold learning and preserving the embedding quality at the same time. An application to spectral clustering is also presented. Experimental results indicate that our multilevel approach is an appealing alternative to standard techniques.

#index 1482221
#* Accelerating probabilistic frequent itemset mining: a model-based approach
#@ Liang Wang;Reynold Cheng;Sau Dan Lee;David Cheung
#t 2010
#c 1
#% 152934
#% 246002
#% 300120
#% 375017
#% 654487
#% 823402
#% 873104
#% 960257
#% 1016178
#% 1016201
#% 1063521
#% 1063531
#% 1147652
#% 1189215
#% 1206939
#% 1214624
#% 1214633
#% 1217251
#% 1318641
#% 1393138
#% 1408839
#% 1451166
#! Data uncertainty is inherent in emerging applications such as location-based services, sensor monitoring systems, and data integration. To handle a large amount of imprecise information, uncertain databases have been recently developed. In this paper, we study how to efficiently discover frequent itemsets from large uncertain databases, interpreted under the Possible World Semantics. This is technically challenging, since an uncertain database induces an exponential number of possible worlds. To tackle this problem, we propose a novel method to capture the itemset mining process as a Poisson binomial distribution. This model-based approach extracts frequent itemsets with a high degree of accuracy, and supports large databases. We apply our techniques to improve the performance of the algorithms for: (1) finding itemsets whose frequentness probabilities are larger than some threshold; and (2) mining itemsets with the k highest frequentness probabilities. Our approaches support both tuple and attribute uncertainty models, which are commonly used to represent uncertain databases. Extensive evaluation on real and synthetic datasets shows that our methods are highly accurate. Moreover, they are orders of magnitudes faster than previous approaches.

#index 1482222
#* Learning click models via probit bayesian inference
#@ Yuchen Zhang;Dong Wang;Gang Wang;Weizhu Chen;Zhihua Zhang;Botao Hu;Li Zhang
#t 2010
#c 1
#% 411762
#% 715096
#% 818221
#% 823348
#% 879565
#% 891559
#% 956546
#% 1035578
#% 1074092
#% 1166522
#% 1190055
#% 1190056
#% 1810385
#! Recent advances in click models have positioned them as an effective approach to the improvement of interpreting click data, and some typical works include UBM, DBN, CCM, etc. After formulating the knowledge of user search behavior into a set of model assumptions, each click model developed an inference method to estimate its parameters. The inference method plays a critical role in terms of accuracy in interpreting clicks, and we observe that different inference methods for a click model can lead to significant accuracy differences. In this paper, we propose a novel Bayesian inference approach for click models. This approach regards click model under a unified framework, which has the following characteristics and advantages: 1. This approach can be widely applied to existing click models, and we demonstrate how to infer DBN, CCM and UBM through it. This novel inference method is based on the Bayesian framework which is more flexible in characterizing the uncertainty in clicks and brings higher generalization abilities. As a result, it not only excels in the inference methods originally developed in click models, but also provides a valid comparison among different models; 2. In contrast to the previous click models, which are exclusively designed for the position-bias, this approach is capable of capturing more sophisticated information such as BM25 and PageRank score into click models. This makes these models interpret click-through data more accurately. Experimental results illustrate that the click models integrated with more information can achieve significantly better performance on click perplexity and search ranking; 3. Because of the incremental nature of the Bayesian learning, this approach is scalable to process large scale and constantly growing log data.

#index 1482223
#* Document allocation policies for selective searching of distributed indexes
#@ Anagha Kulkarni;Jamie Callan
#t 2010
#c 1
#% 194246
#% 280856
#% 316534
#% 375017
#% 375076
#% 578337
#% 643012
#% 722904
#% 728102
#% 750863
#% 789959
#% 818262
#% 878657
#% 1055108
#% 1227597
#% 1227616
#% 1227629
#% 1292595
#! Indexes for large collections are often divided into shards that are distributed across multiple computers and searched in parallel to provide rapid interactive search. Typically, all index shards are searched for each query. For organizations with modest computational resources the high query processing cost incurred in this exhaustive search setup can be a deterrent to working with large collections. This paper investigates document allocation policies that permit searching only a few shards for each query (selective search) without sacrificing search accuracy. Random, source-based and topic-based document-to-shard allocation policies are studied in the context of selective search. A thorough study of the tradeoff between search cost and search accuracy in a sharded index environment is performed using three large TREC collections. The experimental results demonstrate that selective search using topic-based shards cuts the search cost to less than 1/5th of that of the exhaustive search without reducing search accuracy across all the three datasets. Stability analysis shows that 90% of the queries do as well or improve with selective search. An overlap-based evaluation with an additional 1000 queries for each dataset tests and confirms the conclusions drawn using the smaller TREC query sets.

#index 1482224
#* Rank learning for factoid question answering with linguistic and semantic constraints
#@ Matthew W. Bilotti;Jonathan Elsas;Jaime Carbonell;Eric Nyberg
#t 2010
#c 1
#% 278107
#% 577224
#% 642979
#% 742399
#% 815924
#% 818253
#% 840846
#% 987236
#% 987243
#% 1019124
#% 1035575
#% 1288603
#! This work presents a general rank-learning framework for passage ranking within Question Answering (QA) systems using linguistic and semantic features. The framework enables query-time checking of complex linguistic and semantic constraints over keywords. Constraints are composed of a mixture of keyword and named entity features, as well as features derived from semantic role labeling. The framework supports the checking of constraints of arbitrary length relating any number of keywords. We show that a trained ranking model using this rich feature set achieves greater than a 20% improvement in Mean Average Precision over baseline keyword retrieval models. We also show that constraints based on semantic role labeling features are particularly effective for passage retrieval; when they can be leveraged, an 40% improvement in MAP over the baseline can be realized.

#index 1482225
#* Learning to rank relevant and novel documents through user feedback
#@ Abhimanyu Lad;Yiming Yang
#t 2010
#c 1
#% 169781
#% 262112
#% 309124
#% 342707
#% 397133
#% 581914
#% 642975
#% 742084
#% 766444
#% 879618
#% 987196
#% 1019134
#% 1039845
#% 1073970
#% 1074025
#% 1074133
#% 1166473
#% 1214650
#% 1263584
#% 1263587
#% 1267680
#% 1415709
#! We consider the problem of learning to rank relevant and novel documents so as to directly maximize a performance metric called Expected Global Utility (EGU), which has several desirable properties: (i) It measures retrieval performance in terms of relevant as well as novel information, (ii) gives more importance to top ranks to reflect common browsing behavior of users, as opposed to existing objective functions based on set-coverage, (iii) accommodates different levels of tolerance towards redundancy, which is not taken into account by existing evaluation measures, and (iv) extends naturally to the evaluation of session-based retrieval comprising multiple ranked lists. Our ground truth is defined in terms of "information nuggets", which are obviously not known to the retrieval system when processing a new user query. Therefore, our approach uses observable query and document features (words and named entities) as surrogates for nuggets, whose weights are learned based on user feedback in an iterative search session. The ranked list is produced to maximize the weighted coverage of these surrogate nuggets. The optimization of such coverage-based metrics is known to be NP-hard. Therefore, we use a greedy algorithm and show that it guarantees good performance due to the submodularity of the objective function. Our experiments on Topic Detection and Tracking data show that the proposed approach represents an efficient and effective retrieval strategy for maximizing EGU, as compared to a purely-relevance based ranking approach that uses Indri, as well as a MMR-based approach for non-redundant ranking.

#index 1482226
#* Set cover algorithms for very large datasets
#@ Graham Cormode;Howard Karloff;Anthony Wirth
#t 2010
#c 1
#% 175732
#% 256685
#% 280456
#% 519675
#% 881971
#% 1127381
#% 1373609
#% 1399956
#! The problem of Set Cover - to find the smallest subcollection of sets that covers some universe - is at the heart of many data and analysis tasks. It arises in a wide range of settings, including operations research, machine learning, planning, data quality and data mining. Although finding an optimal solution is NP-hard, the greedy algorithm is widely used, and typically finds solutions that are close to optimal. However, a direct implementation of the greedy approach, which picks the set with the largest number of uncovered items at each step, does not behave well when the input is very large and disk resident. The greedy algorithm must make many random accesses to disk, which are unpredictable and costly in comparison to linear scans. In order to scale Set Cover to large datasets, we provide a new algorithm which finds a solution that is provably close to that of greedy, but which is much more efficient to implement using modern disk technology. Our experiments show a ten-fold improvement in speed on moderately-sized datasets, and an even greater improvement on larger datasets.

#index 1482227
#* The gist of everything new: personalized top-k processing over web 2.0 streams
#@ Parisa Haghani;Sebastian Michel;Karl Aberer
#t 2010
#c 1
#% 158911
#% 218979
#% 219048
#% 262043
#% 300163
#% 378388
#% 397608
#% 463734
#% 479649
#% 730019
#% 797693
#% 875023
#% 894646
#% 987218
#% 1016196
#% 1022217
#% 1127375
#% 1134176
#% 1166524
#% 1181273
#% 1206927
#% 1292554
#% 1655692
#! Web 2.0 portals have made content generation easier than ever with millions of users contributing news stories in form of posts in weblogs or short textual snippets as in Twitter. Efficient and effective filtering solutions are key to allow users stay tuned to this ever-growing ocean of information, releasing only relevant trickles of personal interest. In classical information filtering systems, user interests are formulated using standard IR techniques and data from all available information sources is filtered based on a predefined absolute quality-based threshold. In contrast to this restrictive approach which may still overwhelm the user with the returned stream of data, we envision a system which continuously keeps the user updated with only the top-k relevant new information. Freshness of data is guaranteed by considering it valid for a particular time interval, controlled by a sliding window. Considering relevance as relative to the existing pool of new information creates a highly dynamic setting. We present POL-filter which together with our maintenance module constitute an efficient solution to this kind of problem. We show by comprehensive performance evaluations using real world data, obtained from a weblog crawl, that our approach brings performance gains compared to state-of-the-art.

#index 1482228
#* Fast and accurate estimation of shortest paths in large graphs
#@ Andrey Gubichev;Srikanta Bedathur;Stephan Seufert;Gerhard Weikum
#t 2010
#c 1
#% 338382
#% 379482
#% 548463
#% 572860
#% 823342
#% 906329
#% 956564
#% 960304
#% 1002007
#% 1063514
#% 1206994
#% 1217208
#% 1263175
#% 1265149
#% 1292553
#% 1355056
#% 1366460
#% 1379538
#% 1399997
#% 1409954
#! Computing shortest paths between two given nodes is a fundamental operation over graphs, but known to be nontrivial over large disk-resident instances of graph data. While a number of techniques exist for answering reachability queries and approximating node distances efficiently, determining actual shortest paths (i.e. the sequence of nodes involved) is often neglected. However, in applications arising in massive online social networks, biological networks, and knowledge graphs it is often essential to find out many, if not all, shortest paths between two given nodes. In this paper, we address this problem and present a scalable sketch-based index structure that not only supports estimation of node distances, but also computes corresponding shortest paths themselves. Generating the actual path information allows for further improvements to the estimation accuracy of distances (and paths), leading to near-exact shortest-path approximations in real world graphs. We evaluate our techniques - implemented within a fully functional RDF graph database system - over large real-world social and biological networks of sizes ranging from tens of thousand to millions of nodes and edges. Experiments on several datasets show that we can achieve query response times providing several orders of magnitude speedup over traditional path computations while keeping the estimation errors between 0% and 1% on average.

#index 1482229
#* Fast top-k simple shortest paths discovery in graphs
#@ Jun Gao;Huida Qiu;Xiao Jiang;Tengjiao Wang;Dongqing Yang
#t 2010
#c 1
#% 269922
#% 288139
#% 327432
#% 333854
#% 397358
#% 397366
#% 656784
#% 910180
#% 960304
#% 996635
#% 1022243
#% 1063514
#% 1181254
#! With the wide applications of large scale graph data such as social networks, the problem of finding the top-k shortest paths attracts increasing attention. This paper focuses on the discovery of the top-k simple shortest paths (paths without loops). The well known algorithm for this problem is due to Yen, and the provided worstcase bound O(kn(m + nlogn)), which comes from O(n) times single-source shortest path discovery for each of k shortest paths, remains unbeaten for 30 years, where n is the number of nodes and m is the number of edges. In this paper, we observe that there are shared sub-paths among O(kn) single-source shortest paths. The basic idea behind our method is to pre-compute the shortest paths to the target node, and utilize them to reduce the discovery cost at running time. Specifically, we transform the original graph by encoding the pre-computed paths, and prove that the shortest path discovered over the transformed graph is equivalent to that in the original graph. Most importantly, the path discovery over the transformed graph can be terminated much earlier than before. In addition, two optimization strategies are presented. One is to reduce the total iteration times for shortest path discovery, and the other is to prune the search space in each iteration with an adaptively-determined threshold. Although the worst-case complexity cannot be lowered, our method is proven to be much more efficient in a general case. The final extensive experimental results (on both real and synthetic graphs) also show that our method offers a significant performance improvement over the existing ones.

#index 1482230
#* Factors affecting click-through behavior in aggregated search interfaces
#@ Shanu Sushmita;Hideo Joho;Mounia Lalmas;Robert Villa
#t 2010
#c 1
#% 281186
#% 311866
#% 342680
#% 420475
#% 790841
#% 807371
#% 816601
#% 818221
#% 853649
#% 881570
#% 954949
#% 954997
#% 987256
#% 1014390
#% 1166523
#% 1213419
#% 1227616
#% 1267046
#% 1369422
#% 1376021
#% 1407689
#% 1440958
#! An aggregated search interface is designed to integrate search results from different sources (web, image, video, blog, etc) into a single result page. This paper presents two user studies investigating factors affecting users click-through behavior on aggregated search interfaces. We tested two aggregated search interfaces: one where results from the different sources are blended into a single list (called blended), and another, where results from each source are presented in a separate panel (called non-blended). A total of 1,296 search sessions performed by 48 participants were analysed in our study. Our results suggest that 1) the position of search results is significant only in the blended and not in the non-blended design; 2) participants' click-through behavior on videos is different from other sources; and finally 3) capturing a task's orientation towards particular sources is an important factor for further investigation and research.

#index 1482231
#* Web search solved?: all result rankings the same?
#@ Hugo Zaragoza;B. Barla Cambazoglu;Ricardo Baeza-Yates
#t 2010
#c 1
#% 262107
#% 262276
#% 268114
#% 280041
#% 309095
#% 340888
#% 375017
#% 397203
#% 411762
#% 420508
#% 424260
#% 424333
#% 453464
#% 590523
#% 764560
#% 773040
#% 806713
#% 807457
#% 816377
#% 818257
#% 841618
#% 866256
#% 879738
#% 881955
#% 907546
#% 950658
#% 952823
#% 987321
#% 1021108
#% 1035574
#% 1074069
#% 1074124
#% 1074134
#% 1085059
#% 1095876
#% 1137656
#% 1186531
#% 1187374
#% 1187376
#% 1227631
#% 1292528
#% 1397260
#% 1399990
#% 1406993
#! The objective of this work is to derive quantitative statements about what fraction of web search queries issued to the state-of-the-art commercial search engines lead to excellent results or, on the contrary, poor results. To be able to make such statements in an automated way, we propose a new measure that is based on lower and upper bound analysis over the standard relevance measures. Moreover, we extend this measure to carry out comparisons between competing search engines by introducing the concept of disruptive sets, which we use to estimate the degree to which a search engine solves queries that are not solved by its competitors. We report empirical results on a large editorial evaluation of the three largest search engines in the US market.

#index 1482232
#* Assessor error in stratified evaluation
#@ William Webber;Douglas W. Oard;Falk Scholer;Bruce Hedin
#t 2010
#c 1
#% 217252
#% 262102
#% 262105
#% 857180
#% 907496
#% 1002317
#% 1015625
#% 1074132
#% 1279287
#% 1314931
#% 1494802
#! Several important information retrieval tasks, including those in medicine, law, and patent review, have an authoritative standard of relevance, and are concerned about retrieval completeness. During the evaluation of retrieval effectiveness in these domains, assessors make errors in applying the standard of relevance, and the impact of these errors, particularly on estimates of recall, is of crucial concern. Using data from the interactive task of the TREC Legal Track, this paper investigates how reliably the yield of relevant documents can be estimated from sampled assessments in the presence of assessor error, particularly where sampling is stratified based upon the results of participating retrieval systems. We show that assessor error is in general a greater source of inaccuracy than sampling error. A process of appeal and adjudication, such as used in the interactive task, is found to be effective at locating many assessment errors; but the process is expensive if complete, and biased if incomplete. An unbiased double-sampling method for resolving assessment error is proposed, and shown on representative data to be more efficient and accurate than appeal-based adjudication.

#index 1482233
#* CiteData: a new multi-faceted dataset for evaluating personalized search performance
#@ Abhay Harpale;Yiming Yang;Siddharth Gopal;Daqing He;Zhen Yue
#t 2010
#c 1
#% 340904
#% 348173
#% 397133
#% 413615
#% 495929
#% 577329
#% 642990
#% 643007
#% 763708
#% 770816
#% 771571
#% 788069
#% 799753
#% 818255
#% 987238
#% 1074062
#% 1074070
#! Personalized search systems have evolved to utilize heterogeneous features including document hyperlinks, category labels in various taxonomies and social tags in addition to free-text of the documents. Consequently, classifiers, PageRank algorithms and Collaborative Filtering methods are often used as intermediate steps in such personalized retrieval systems. Thorough comparative evaluation of such complex systems has been difficult due to the lack of appropriate publicly available datasets that provide such diverse feature sets. To remedy the situation, we have created CiteData, a new dataset for benchmark evaluations of personalized search performance, that will be made publicly accessible. CiteData is a collection of academic articles extracted from CiteULike and CiteSeer repositories, with rich feature sets such as authors, author-affiliations, topic labels, social tags and citation information. We further supplement it with personalized queries and relevance judgments which were obtained from volunteer users. This paper starts with a discussion of the design criteria and characteristics of the CiteData dataset in comparison with current benchmark datasets, followed by a set of task-oriented empirical evaluations of popular algorithms in statistical classification, collaborative filtering and link analysis as intermediate steps for personalized search. Our results show significant performance improvement of personalized approaches, over that of unpersonalized approaches. We also observe that a meta personalized search engine that leverages information from multiple sources of features performs better than algorithms that use only one of the constituent source of features.

#index 1482234
#* Learning a user-thread alignment manifold for thread recommendation in online forum
#@ Jun Zhao;Jiajun Bu;Chun Chen;Ziyu Guan;Can Wang;Cheng Zhang
#t 2010
#c 1
#% 766418
#% 813966
#% 943767
#% 989627
#% 1001279
#% 1055691
#% 1055736
#% 1055738
#% 1074109
#% 1083624
#% 1083636
#% 1083720
#% 1083957
#% 1130901
#% 1190130
#% 1214658
#% 1214661
#% 1214668
#% 1214699
#% 1227601
#% 1227602
#% 1227699
#! People are more and more willing to participate in online forums to share their knowledge and experience. However, it may not be easy for them to find their desired threads in online forums due to the information overload problem. Traditional recommendation approaches can not be directly applied to online forums due to two reasons. First, unlike traditional movie or music recommendation problem, there is no rating information in online forums. Second, the sparsity problem is more severe since the users may only read threads but take no actions. To address these limitations, in this paper we propose to make use of the reply relationships among users, as well as thread contents. A learning algorithm is introduced to infer a user-thread alignment manifold in which both users and thread contents can be well represented. Thus, the relatedness between users and threads can be measured on this alignment manifold, and the closest threads which can best meet the corresponding user's information needs are recommended. Experiments on a dataset crawled from digg.com have demonstrated the superiority of our algorithm over traditional recommendation algorithms.

#index 1482235
#* FacetCube: a framework of incorporating prior knowledge into non-negative tensor factorization
#@ Yun Chi;Shenghuo Zhu
#t 2010
#c 1
#% 248010
#% 309095
#% 316143
#% 329569
#% 729918
#% 805877
#% 818291
#% 836675
#% 840934
#% 989585
#% 989587
#% 989618
#% 989640
#% 1055740
#% 1084537
#% 1130902
#% 1298155
#% 1300087
#! Non-negative tensor factorization (NTF) is a relatively new technique that has been successfully used to extract significant characteristics from polyadic data, such as data in social networks. Because these polyadic data have multiple dimensions (e.g., the author, content, and timestamp of a blog post), NTF fits in naturally and extracts data characteristics jointly from different data dimensions. In the standard NTF, all information comes from the observed data and end users have no control over the outcomes. However, in many applications very often the end users have certain prior knowledge, such as the demographic information about individuals in a social network or a pre-constructed ontology on the contents, and therefore prefer the extracted data characteristics being consistent with such prior knowledge. To allow users' prior knowledge to be naturally incorporated into NTF, in this paper we present a novel framework - FacetCube - that extends the standard non-negative tensor factorization. The new framework allows the end users to control the factorization outputs at three different levels for each of the data dimensions. The proposed framework is intuitively appealing in that it has a close connection to the probabilistic generative models. In addition to introducing the framework, we provide an iterative algorithm for computing the optimal solution to the framework. We also develop an efficient implementation of the algorithm that consists of a series of techniques to make our framework scalable to large data sets. Extensive experimental studies on a paper citation data set and a blog data set demonstrate that our new framework is able to effectively incorporate users' prior knowledge, improves performance over the standard NTF on the task of personalized recommendation, and is scalable to large data sets from real-life applications.

#index 1482236
#* Travel route recommendation using geotags in photo sharing sites
#@ Takeshi Kurashima;Tomoharu Iwata;Go Irie;Ko Fujimura
#t 2010
#c 1
#% 333679
#% 643007
#% 722904
#% 723186
#% 734590
#% 849492
#% 874628
#% 903606
#% 967244
#% 989604
#% 997189
#% 1055701
#% 1092274
#% 1190131
#% 1190134
#% 1279780
#% 1305518
#% 1400055
#% 1650298
#% 1728806
#! The ability to create geotagged photos enables people to share their personal experiences as tourists at specific locations and times. Assuming that the collection of each photographer's geotagged photos is a sequence of visited locations, photo-sharing sites are important sources for gathering the location histories of tourists. By following their location sequences, we can find representative and diverse travel routes that link key landmarks. In this paper, we propose a travel route recommendation method that makes use of the photographers' histories as held by Flickr. Recommendations are performed by our photographer behavior model, which estimates the probability of a photographer visiting a landmark. We incorporate user preference and present location information into the probabilistic behavior model by combining topic models and Markov models. We demonstrate the effectiveness of the proposed method using a real-life dataset holding information from 71,718 photographers taken in the United States in terms of the prediction accuracy of travel behavior.

#index 1482237
#* Boosting social network connectivity with link revival
#@ Yuan Tian;Qi He;Qiankun Zhao;Xingjie Liu;Wang-chien Lee
#t 2010
#c 1
#% 343158
#% 730089
#% 813966
#% 835906
#% 1035580
#% 1083672
#% 1117026
#% 1130901
#% 1169572
#% 1183090
#% 1183091
#% 1203761
#% 1211760
#% 1227601
#% 1246431
#% 1287226
#% 1287262
#% 1292592
#% 1366213
#% 1400031
#! Online social networking platforms have become a popular channel of communications among people. However, most people can only keep in touch with a limited number of friends. This phenomenon results in a low-connectivity social network in terms of communications, which is inefficient for information propagation and social engagement. In this paper, we introduce a new recommendation service, called link revival, that suggests users to re-connect with their old friends, such that the resulted connection will improve the social network connectivity. To achieve high connectivity improvement under the dynamic social network evolvement, we propose a graph prediction-based recommendation strategy, which selects proper candidates based on the prediction of their future behaviors. We then develop an effective model that exploits non-homogeneous Poisson process and second-order self-similarity in prediction. Through comprehensive experimental studies on two real datasets (Phone Call Network and Facebook Wall-posts), we demonstrate that our proposed approach can significantly increase the social network connectivity, and that the approach outperforms other baseline solutions. The results also show that our solution is more suitable for online social networks like Facebook, partially due to the stronger long range dependency and lower communication costs in the interactions.

#index 1482238
#* Power in unity: forming teams in large-scale community systems
#@ Aris Anagnostopoulos;Luca Becchetti;Carlos Castillo;Aristides Gionis;Stefano Leonardi
#t 2010
#c 1
#% 1156
#% 124535
#% 187264
#% 232338
#% 261358
#% 341672
#% 456040
#% 531960
#% 730082
#% 879570
#% 956516
#% 1148857
#% 1176887
#% 1214668
#% 1224602
#% 1232240
#% 1275180
#! The internet has enabled the collaboration of groups at a scale that was unseen before. A key problem for large collaboration groups is to be able to allocate tasks effectively. An effective task assignment method should consider both how fit teams are for each job as well as how fair the assignment is to team members, in terms that no one should be overloaded or unfairly singled out. The assignment has to be done automatically or semi-automatically given that it is difficult and time-consuming to keep track of the skills and the workload of each person. Obviously the method to do this assignment must also be computationally efficient. In this paper we present a general framework for task assignment problems. We provide a formal treatment on how to represent teams and tasks. We propose alternative functions for measuring the fitness of a team performing a task and we discuss desirable properties of those functions. Then we focus on one class of task-assignment problems, we characterize the complexity of the problem, and we provide algorithms with provable approximation guarantees, as well as lower bounds. We also present experimental results that show that our methods are useful in practice in several application scenarios.

#index 1482239
#* Who should I cite: learning literature search models from citation behavior
#@ Steven Bethard;Dan Jurafsky
#t 2010
#c 1
#% 73441
#% 137475
#% 268079
#% 722904
#% 817439
#% 874462
#% 874548
#% 940040
#% 987226
#% 987287
#% 987331
#% 1077150
#% 1083684
#% 1130828
#% 1195999
#% 1270714
#% 1532616
#% 1682113
#! Scientists depend on literature search to find prior work that is relevant to their research ideas. We introduce a retrieval model for literature search that incorporates a wide variety of factors important to researchers, and learns the weights of each of these factors by observing citation patterns. We introduce features like topical similarity and author behavioral patterns, and combine these with features from related work like citation count and recency of publication. We present an iterative process for learning weights for these features that alternates between retrieving articles with the current retrieval model, and updating model weights by training a supervised classifier on these articles. We propose a new task for evaluating the resulting retrieval models, where the retrieval system takes only an abstract as its input and must produce as output the list of references at the end of the abstract's article. We evaluate our model on a collection of journal, conference and workshop articles from the ACL Anthology Reference Corpus. Our model achieves a mean average precision of 28.7, a 12.8 point improvement over a term similarity baseline, and a significant improvement both over models using only features from related work and over models without our iterative learning.

#index 1482240
#* A structured approach to query recommendation with social annotation data
#@ Jiafeng Guo;Xueqi Cheng;Gu Xu;Huawei Shen
#t 2010
#c 1
#% 218978
#% 268079
#% 310567
#% 330617
#% 330619
#% 577329
#% 641976
#% 738746
#% 754125
#% 869501
#% 939629
#% 987222
#% 987387
#% 1055677
#% 1074098
#% 1083721
#% 1130854
#% 1130879
#% 1173699
#% 1227592
#% 1712595
#! Query recommendation has been recognized as an important mean to help users search and also improve the usability of search engines. Existing approaches mainly focus on helping users refine their search queries and the recommendations typically stick to users' search intent, named search interests in this paper. However, users may also have some vague or delitescent interests which they are unaware of until they are faced with one, named exploratory interests. These interests may be provoked within a search session when users read a web page from search results or even follow links on the page. By considering exploratory interests in query recommendation, we attract more user clicks on recommendations. This type of query recommendation has not been explicitly addressed in previous work. In this paper, we propose to recommend queries in a structured way for better satisfying both search and exploratory interests of users. Specifically, we construct a query relation graph from query logs and social annotation data which capture two types of interests respectively. Based on the query relation graph, we employ hitting time to rank possible recommendations, leverage a modularity based approach to group top recommendations into clusters, and label each cluster with social tags. Empirical experimental results indicate that our structured approach to query recommendation with social annotation data can better satisfy users' interests and significantly enhance users' click behavior on recommendations.

#index 1482241
#* Using the past to score the present: extending term weighting models through revision history analysis
#@ Ablimit Aji;Yu Wang;Eugene Agichtein;Evgeniy Gabrilovich
#t 2010
#c 1
#% 111304
#% 340899
#% 411762
#% 575570
#% 577360
#% 642992
#% 754106
#% 754107
#% 766409
#% 766430
#% 766431
#% 783474
#% 801831
#% 818240
#% 879567
#% 879587
#% 956535
#% 1021954
#% 1055676
#% 1055715
#% 1055718
#% 1074094
#% 1074103
#% 1074112
#% 1100801
#% 1227636
#% 1292507
#% 1355016
#% 1467831
#% 1489430
#! The generative process underlies many information retrieval models, notably statistical language models. Yet these models only examine one (current) version of the document, effectively ignoring the actual document generation process. We posit that a considerable amount of information is encoded in the document authoring process, and this information is complementary to the word occurrence statistics upon which most modern retrieval models are based. We propose a new term weighting model, Revision History Analysis (RHA), which uses the revision history of a document (e.g., the edit history of a page in Wikipedia) to redefine term frequency - a key indicator of document topic/relevance for many retrieval models and text processing tasks. We then apply RHA to document ranking by extending two state-of-the-art text retrieval models, namely, BM25 and the generative statistical language model (LM). To the best of our knowledge, our paper is the first attempt to directly incorporate document authoring history into retrieval models. Empirical results show that RHA provides consistent improvements for state-of-the-art retrieval models, using standard retrieval tasks and benchmarks.

#index 1482242
#* Language pyramid and multi-scale text analysis
#@ Shuang-Hong Yang;Hongyuan Zha
#t 2010
#c 1
#% 71063
#% 71149
#% 126894
#% 169777
#% 262096
#% 279755
#% 280850
#% 465754
#% 466900
#% 593047
#% 722803
#% 750863
#% 760805
#% 763697
#% 763708
#% 770846
#% 786511
#% 818262
#% 830520
#% 869501
#% 1014683
#% 1055706
#% 1074112
#% 1074127
#% 1186295
#% 1211807
#% 1214758
#% 1227613
#% 1411065
#! The classical Bag-of-Word (BOW) model represents a document as a histogram of word occurrence, losing the spatial information that is invaluable for many text analysis tasks. In this paper, we present the Language Pyramid (LaP) model, which casts a document as a probabilistic distribution over the joint semantic-spatial space and motivates a multi-scale 2D local smoothing framework for nonparametric text coding. LaP efficiently encodes both semantic and spatial contents of a document into a pyramid of matrices that are smoothed both semantically and spatially at a sequence of resolutions, providing a convenient multi-scale imagic view for natural language understanding. The LaP representation can be used in text analysis in a variety of ways, among which we investigate two instantiations in the current paper: (1) multi-scale text kernels for document categorization, and (2) multi-scale language models for ad hoc text retrieval. Experimental results illustrate that: for classification, LaP outperforms BOW by (up to) 4% on moderate-length texts (RCV1 text benchmark) and 15% on short texts (Yahoo! queries); and for retrieval, LaP gains 12% MAP improvement over uni-gram language models on the OHSUMED data set.

#index 1482243
#* Latent interest-topic model: finding the causal relationships behind dyadic data
#@ Noriaki Kawamae
#t 2010
#c 1
#% 769906
#% 876017
#% 879587
#% 881498
#% 956510
#% 989621
#% 1055682
#% 1190055
#% 1211773
#% 1289476
#% 1451015
#! This paper presents a hierarchical generative model that captures the latent relation of cause and effect underlying user behavioral-originated data such as papers, twitter and purchase history. Our proposel, the Latent Interest Topic model (LIT), introduces a latent variable into each document and each author layor in a coherent generative model. We call the former variable the document class, and the latter variable the author class, where these classes are indicator variables that allow the inclusion of different types of probability, and can be shared over documents with similar content and authors with similar interests, respectively. Significantly, unlike other works, LIT differentiates, respectively, document topics and user interests by using these classes. Consequently, LIT is superior to previous models in explaining the causal relationships behind the data by merging similar distributions; it also makes the computation process easier. Experiments on a research paper corpus show that the proposed model can well capture document and author classes, and reduce the dimensionality of documents to a low-dimensional author-document space, making it useful as a generative model.

#index 1482244
#* PROSPECT: a system for screening candidates for recruitment
#@ Amit Singh;Catherine Rose;Karthik Visweswariah;Vijil Chenthamarakshan;Nandakishore Kambhatla
#t 2010
#c 1
#% 95730
#% 162505
#% 309095
#% 348147
#% 413653
#% 451536
#% 464434
#% 544011
#% 643004
#% 816181
#% 870896
#% 939393
#% 968030
#% 983098
#% 987339
#% 1065811
#% 1074112
#% 1166537
#% 1202349
#% 1464208
#! Companies often receive thousands of resumes for each job posting and employ dedicated screeners to short list qualified applicants. In this paper, we present PROSPECT, a decision support tool to help these screeners shortlist resumes efficiently. Prospect mines resumes to extract salient aspects of candidate profiles like skills, experience in each skill, education details and past experience. Extracted information is presented in the form of facets to aid recruiters in the task of screening. We also employ Information Retrieval techniques to rank all applicants for a given job opening. In our experiments we show that extracted information improves our ranking by 30% there by making screening task simpler and more efficient.

#index 1482245
#* Active caching for similarity queries based on shared-neighbor information
#@ Michael E. Houle;Vincent Oria;Umar Qasim
#t 2010
#c 1
#% 90698
#% 314054
#% 340888
#% 413181
#% 481916
#% 571036
#% 763708
#% 778279
#% 787172
#% 800570
#% 805864
#% 840583
#% 978362
#% 1013086
#% 1046509
#% 1055710
#% 1137068
#! Novel applications such as recommender systems, uncertain databases, and multimedia databases are designed to process similarity queries that produce ranked lists of objects as their results. Similarity queries typically result in disk access latency and incur a substantial computational cost. In this paper, we propose an 'active caching' technique for similarity queries that is capable of synthesizing query results from cached information even when the required result list is not explicitly stored in the cache. Our solution, the Cache Estimated Significance (CES) model, is based on shared-neighbor similarity measures, which assess the strength of the relationship between two objects as a function of the number of other objects in the common intersection of their neighborhoods. The proposed method is general in that it does not require that the features be drawn from a metric space, nor does it require that the partial orders induced by the similarity measure be monotonic. Experimental results on real data sets show a substantial cache hit rate when compared with traditional caching approaches.

#index 1482246
#* Searching consumer image collections using web-based concept expansion
#@ Mark D. Wood;Alexander Loui;Stacie Hibino
#t 2010
#c 1
#% 198055
#% 329434
#% 445326
#% 741411
#% 750863
#% 789959
#% 1055704
#% 1190135
#% 1279773
#% 1338171
#% 1338382
#% 1698872
#! As consumers accumulate more and more personal imagery, searching for specific images has become increasingly difficult. Consumers typically provide little or no annotations, and automated classifiers and concept tagging tools are limited in their scope and vocabulary. This work addresses this sparsity of semantic information by leveraging domain-specific information provided by online photo-sharing communities. Such information enables improved search by allowing user-provided search terms to be expanded into a set of semantically related concepts, using relevant semantic relationships provided by millions of users. Our system first extracts metadata using a modest number of image and event-based semantic classifiers, as well as any meaningful file or folder names. When users pose text-based queries, our system retrieves images from their personal image collections by leveraging Flickr's tag dataset for concept expansion. This approach enables users to search their collections without having to manually annotate their pictures. We compare the retrieval performance of using a Flickr-based concept expander with the performance obtained without concept expansion and with using a WordNet-based concept expander. The results demonstrate that common sense knowledge gleaned from online photo sharing communities can enable meaningful image search on consumer image collections, searches that would be impossible using only the available image metadata.

#index 1482247
#* Index structures for efficiently searching natural language text
#@ Pirooz Chubak;Davood Rafiei
#t 2010
#c 1
#% 379390
#% 397150
#% 453572
#% 754068
#% 781169
#% 805883
#% 823464
#% 869535
#% 878624
#% 931332
#% 936965
#% 987259
#% 987275
#% 1077150
#% 1292741
#% 1916419
#! Many existing indexes on text work at the document granularity and are not effective in answering the class of queries where the desired answer is only a term or a phrase. In this paper, we study some of the index structures that are capable of answering the class of queries referred to here as wild card queries and perform an analysis of their performance. Our experimental results on a large class of queries from different sources (including query logs and parse trees) and with various datasets reveal some of the performance barriers of these indexes. We then present Word Permuterm Index (WPI) which is an adaptation of the permuterm index for natural language text applications and show that this index supports a wide range of wild card queries, is quick to construct and is highly scalable. Our experimental resultS comparing WPI to alternative methods on a wide range oF wild card queries show a few orders of magnitude performancE improvements for WPI while the memory usage is kept the same for all compared systems.

#index 1482248
#* Efficient temporal keyword search over versioned text
#@ Avishek Anand;Srikanta Bedathur;Klaus Berberich;Ralf Schenkel
#t 2010
#c 1
#% 273909
#% 297675
#% 479804
#% 570884
#% 571296
#% 800003
#% 960250
#% 987257
#% 1022339
#% 1024551
#! Modern text analytics applications operate on large volumes of temporal text data such as Web archives, newspaper archives, blogs, wikis, and micro-blogs. In these settings, searching and mining needs to use constraints on the time dimension in addition to keyword constraints. A natural approach to address such queries is using an inverted index whose entries are enriched with valid-time intervals. It has been shown that these indexes have to be partitioned along time in order to achieve efficiency. However, when the temporal predicate corresponds to a long time range, requiring the processing of multiple partitions, naive query processing incurs high cost of reading of redundant entries across partitions. We present a framework for efficient approximate processing of keyword queries over a temporally partitioned inverted index which minimizes this overhead, thus speeding up query processing. By using a small synopsis for each partition we identify partitions that maximize the number of final non-redundant results, and schedule them for processing early on. Our approach aims to balance the estimated gains in the final result recall against the cost of index reading required. We present practical algorithms for the resulting optimization problem of index partition selection. Our experiments with three diverse, large-scale text archives reveal that our proposed approach can provide close to 80% result recall even when only about half the index is allowed to be read.

#index 1482249
#* Result-size estimation for information-retrieval subqueries
#@ Guido Sautter;Klemens Böhm;Andranik Khachatryan
#t 2010
#c 1
#% 82346
#% 145196
#% 210189
#% 210190
#% 248010
#% 248821
#% 248822
#% 273705
#% 299984
#% 324129
#% 340934
#% 397123
#% 406493
#% 411554
#% 427219
#% 479816
#% 479931
#% 480283
#% 571046
#% 745489
#% 765425
#% 824684
#% 1015256
#% 1116726
#! Estimating the approximate result size of a query before its execution based on small summary statistics is important for query optimization in database systems and for other facets of query processing. This also holds for queries over text databases. Research on selectivity estimation for such queries has focused on Boolean retrieval, i.e., a document may be relevant for the query or not. But with the coalescence of database and information retrieval (IR) technology, selectivity estimation for other, more sophisticated relevance functions is gaining importance as well. These models generate a query-specific distribution of the documents over the [0, 1]-interval. With document distributions, selectivity estimation means estimating how many documents are how similar to a given query. The problem is much more complex than selectivity estimation in the Boolean context: Beside document frequency, query results also depend on other characteristics such as term frequencies and document lengths. Selectivity estimation must take them into account as well. This paper proposes and evaluates a technique for estimating the result of retrieval queries with non-Boolean relevance functions. It estimates discretized document distributions over the range of the relevance function. Despite the complexity, compared to Boolean selectivity estimation, it requires little additional data, and the additional data can be stored in existing data structures with little extensions. Our evaluation demonstrates the effectiveness of our technique.

#index 1482250
#* FACeTOR: cost-driven exploration of faceted query results
#@ Abhijith Kashyap;Vagelis Hristidis;Michalis Petropoulos
#t 2010
#c 1
#% 232644
#% 345271
#% 452641
#% 458860
#% 765464
#% 857482
#% 894444
#% 960285
#% 994033
#% 1015325
#% 1026960
#% 1130808
#% 1206993
#% 1328119
#% 1914859
#! Faceted navigation is being increasingly employed as an effective technique for exploring large query results on structured databases. This technique of mitigating information-overload leverages metadata of the query results to provide users with facet conditions that can be used to progressively refine the user's query and filter the query results. However, the number of facet conditions can be quite large, thereby increasing the burden on the user. We present the FACeTOR system that proposes a cost-based approach to faceted navigation. At each step of the navigation, the user is presented with a subset of all possible facet conditions that are selected such that the overall expected navigation cost is minimized and every result is guaranteed to be reachable by a facet condition. We prove that the problem of selecting the optimal facet conditions at each navigation step is NP-Hard, and subsequently present two intuitive heuristics employed by FACeTOR. Our user study at Amazon Mechanical Turk shows that FACeTOR reduces the user navigation time compared to the cutting edge commercial and academic faceted search algorithms. The user study also confirms the validity of our cost model. We also present the results of an extensive experimental evaluation on the performance of the proposed approach using two real datasets. FACeTOR is available at http://db.cse.buffalo.edu/facetor/.

#index 1482251
#* A framework for evaluating database keyword search strategies
#@ Joel Coffman;Alfred C. Weaver
#t 2010
#c 1
#% 184486
#% 248065
#% 561315
#% 660011
#% 728195
#% 824693
#% 839172
#% 875017
#% 878624
#% 960243
#% 960259
#% 993987
#% 1015325
#% 1063537
#% 1063539
#% 1074133
#% 1077150
#% 1127445
#% 1206817
#% 1217198
#% 1217235
#% 1348355
#% 1479590
#! With regard to keyword search systems for structured data, research during the past decade has largely focused on performance. Researchers have validated their work using ad hoc experiments that may not reflect real-world workloads. We illustrate the wide deviation in existing evaluations and present an evaluation framework designed to validate the next decade of research in this field. Our comparison of 9 state-of-the-art keyword search systems contradicts the retrieval effectiveness purported by existing evaluations and reinforces the need for standardized evaluation. Our results also suggest that there remains considerable room for improvement in this field. We found that many techniques cannot scale to even moderately-sized datasets that contain roughly a million tuples. Given that existing databases are considerably larger than this threshold, our results motivate the creation of new algorithms and indexing techniques that scale to meet both current and future workloads.

#index 1482252
#* Network growth and the spectral evolution model
#@ Jérôme Kunegis;Damien Fay;Christian Bauckhage
#t 2010
#c 1
#% 87494
#% 252750
#% 420515
#% 464615
#% 503213
#% 730089
#% 805841
#% 881493
#% 915225
#% 1077150
#% 1080078
#% 1083675
#% 1190129
#% 1211760
#% 1246431
#% 1269378
#% 1351060
#! We introduce and study the spectral evolution model, which characterizes the growth of large networks in terms of the eigenvalue decomposition of their adjacency matrices: In large networks, changes over time result in a change of a graph's spectrum, leaving the eigenvectors unchanged. We validate this hypothesis for several large social, collaboration, authorship, rating, citation, communication and tagging networks, covering unipartite, bipartite, signed and unsigned graphs. Following these observations, we introduce a link prediction algorithm based on the extrapolation of a network's spectral evolution. This new link prediction method generalizes several common graph kernels that can be expressed as spectral transformations. In contrast to these graph kernels, the spectral extrapolation algorithm does not make assumptions about specific growth patterns beyond the spectral evolution model. We thus show that it performs particularly well for networks with irregular, but spectral, growth patterns.

#index 1482253
#* Automatic detection of craters in planetary images: an embedded framework using feature selection and boosting
#@ Wei Ding;Tomasz F. Stepinski;Lourenco Bandeira;Ricardo Vilalta;Youxi Wu;Zhenyu Lu;Tianyu Cao
#t 2010
#c 1
#% 116149
#% 520224
#% 543988
#% 718478
#% 736300
#% 792624
#% 913843
#% 983828
#! Identifying impact craters on planetary surfaces is one fundamental task in planetary science. In this paper, we present an embedded framework on auto-detection of craters, using feature selection and boosting strategies. The paradigm aims at building a universal and practical crater detector. This methodology addresses three issues that such a tool must possess: (i) it utilizes mathematical morphology to efficiently identify the regions of an image that can potentially contain craters; only those regions, defined as crater candidates, are the subjects of further processing; (ii) it selects Haar-like image texture features in combination with boosting ensemble supervised learning algorithms to accurately classify candidates into craters and non-craters; (iii) it uses transfer learning, at a minimum additional cost, to enable maintaining an accurate auto-detection of craters on new images, having morphology different from what has been captured by the original training set. All three aforementioned components of the detection methodology are discussed, and the entire framework is evaluated on a large test image of 37,500 x 56,250$ m2 on Mars, showing heavily cratered Martian terrain characterized by nonuniform surface morphology. Our study demonstrates that this methodology provides a robust and practical tool for planetary science, in terms of both detection accuracy and efficiency.

#index 1482254
#* You are where you tweet: a content-based approach to geo-locating twitter users
#@ Zhiyuan Cheng;James Caverlee;Kyumin Lee
#t 2010
#c 1
#% 766441
#% 809461
#% 926881
#% 1055707
#% 1190131
#% 1190207
#% 1227637
#% 1344744
#% 1399939
#% 1400018
#% 1450883
#! We propose and evaluate a probabilistic framework for estimating a Twitter user's city-level location based purely on the content of the user's tweets, even in the absence of any other geospatial cues. By augmenting the massive human-powered sensing capabilities of Twitter and related microblogging services with content-derived location information, this framework can overcome the sparsity of geo-enabled features in these services and enable new location-based personalized information services, the targeting of regional advertisements, and so on. Three of the key features of the proposed approach are: (i) its reliance purely on tweet content, meaning no need for user IP information, private login information, or external knowledge bases; (ii) a classification component for automatically identifying words in tweets with a strong local geo-scope; and (iii) a lattice-based neighborhood smoothing model for refining a user's location estimate. The system estimates k possible locations for each user in descending order of confidence. On average we find that the location estimates converge quickly (needing just 100s of tweets), placing 51% of Twitter users within 100 miles of their actual location.

#index 1482255
#* Partial drift detection using a rule induction framework
#@ Damon Sotoudeh;Aijun An
#t 2010
#c 1
#% 204531
#% 342600
#% 342639
#% 533968
#% 729932
#% 769927
#% 823408
#% 915309
#% 1405144
#! The major challenge in mining data streams is the issue of concept drift, the tendency of the underlying data generation process to change over time. In this paper, we propose a general rule learning framework that can efficiently handle concept-drifting data streams and maintain a highly accurate classification model. The main idea is to focus on partial drifts by allowing individual rules to monitor the stream and detect if there is a drift in the regions they cover. A rule quality measure then decides whether the affected rules are inconsistent with the concept drift. The model is accordingly updated to only include rules that are consistent with the newly arrived concept. A dynamically maintained set of instances deemed relevant to the most recent concept is also kept at memory. Learning a new concept from a larger set of instances reduces the variance of data distribution and allows for a more accurate, stable classification model. Our experiments show that this approach not only handles the drift efficiently, but it also can provide higher classification accuracy compared to other competitive approaches on a variety of real and synthetic data sets.

#index 1482256
#* A method for discovering components of human rituals from streams of sensor data
#@ Athanasios Bamis;Jia Fang;Andreas Savvides
#t 2010
#c 1
#% 420063
#% 464839
#% 464986
#% 478626
#% 481290
#% 578396
#% 631926
#% 806594
#% 975028
#% 1213724
#% 1465201
#% 1671992
#! This paper describes an algorithm for determining if an event occurs persistently within an interval where the interval is periodic but the event is not. The goal of the algorithm is to identify events with this property and also determine the minimum interval in which they occur. This solution is geared towards discovering human routines by considering the triggering of simple sensors over a diverse set of spatial and temporal scales. After describing the problem and the proposed solution, in this paper we demonstrate using testbed data and simulations that this approach uncovers components of routines by identifying which events are parts of the same routine through their temporal properties.

#index 1482257
#* Two-tier similarity model for story link detection
#@ Tadashi Nomoto
#t 2010
#c 1
#% 262080
#% 279755
#% 309096
#% 324887
#% 464996
#% 722904
#% 722925
#% 766457
#% 815159
#% 816125
#% 816196
#% 817448
#% 938668
#% 995450
#% 995518
#% 1348313
#! The paper presents a novel approach to story link detection, where the goal is to determine whether a pair of news stories are linked, i.e., talk about the same event. The present work marks a departure from the prior work in that we measure similarity at two distinct levels of textual organization, the document and its collection, and combine scores at both levels to determine how well stories are linked. Experiments on the TDT-5 corpus show that the present approach, which we call a 'two-tier similarity model,' comfortably beats conventional approaches such as Clarity enhanced KL divergence, while performing robustly across diverse languages.

#index 1482258
#* Selected new training documents to update user profile
#@ Abdulmohsen Algarni;Yuefeng Li;Yue Xu
#t 2010
#c 1
#% 94369
#% 232646
#% 248225
#% 262085
#% 262087
#% 287253
#% 300542
#% 324129
#% 340901
#% 340941
#% 342707
#% 344447
#% 375017
#% 379528
#% 465895
#% 643005
#% 730034
#% 754126
#% 766450
#% 779877
#% 818214
#% 863392
#% 867052
#% 915323
#% 936239
#% 987231
#% 1074065
#% 1074078
#% 1077150
#% 1083679
#% 1130911
#% 1292491
#% 1292658
#% 1349104
#% 1451215
#! Relevance Feedback (RF) has been proven very effective for improving retrieval accuracy. Adaptive information filtering (AIF) technology has benefited from the improvements achieved in all the tasks involved over the last decades. A difficult problem in AIF has been how to update the system with new feedback efficiently and effectively. In current feedback methods, the updating processes focus on updating system parameters. In this paper, we developed a new approach, the Adaptive Relevance Features Discovery (ARFD). It automatically updates the system's knowledge based on a sliding window over positive and negative feedback to solve a nonmonotonic problem efficiently. Some of the new training documents will be selected using the knowledge that the system currently obtained. Then, specific features will be extracted from selected training documents. Different methods have been used to merge and revise the weights of features in a vector space. The new model is designed for Relevance Features Discovery (RFD), a pattern mining based approach, which uses negative relevance feedback to improve the quality of extracted features from positive feedback. Learning algorithms are also proposed to implement this approach on Reuters Corpus Volume 1 and TREC topics. Experiments show that the proposed approach can work efficiently and achieves the encouragement performance.

#index 1482259
#* Collaborative filtering in social tagging systems based on joint item-tag recommendations
#@ Jing Peng;Daniel Dajun Zeng;Huimin Zhao;Fei-yue Wang
#t 2010
#c 1
#% 310572
#% 316143
#% 319705
#% 577214
#% 956515
#% 1052902
#% 1065209
#% 1103333
#% 1156304
#% 1172630
#% 1176933
#% 1190122
#% 1214694
#% 1287228
#% 1287253
#% 1287277
#% 1300087
#% 1327635
#% 1344803
#! Tapping into the wisdom of the crowd, social tagging can be considered an alternative mechanism - as opposed to Web search - for organizing and discovering information on the Web. Effective tag-based recommendation of information items, such as Web resources, is a critical aspect of this social information discovery mechanism. A precise understanding of the information structure of social tagging systems lies at the core of an effective tag-based recommendation method. While most of the existing research either implicitly or explicitly assumes a simple tripartite graph structure for this purpose, we propose a comprehensive information structure to capture all types of co-occurrence information in the tagging data. Based on the proposed information structure, we further propose a unified user profiling scheme to make full use of all available information. Finally, supported by our proposed user profile, we propose a novel framework for collaborative filtering in social tagging systems. In our proposed framework, we first generate joint item-tag recommendations, with tags indicating topical interests of users in target items. These joint recommendations are then refined by the wisdom from the crowd and projected to the item space for final item recommendations. Evaluation using three real-world datasets shows that our proposed recommendation approach significantly outperformed state-of-the-art approaches.

#index 1482260
#* Collaborative future event recommendation
#@ Einat Minkov;Ben Charrow;Jonathan Ledlie;Seth Teller;Tommi Jaakkola
#t 2010
#c 1
#% 118762
#% 301259
#% 577224
#% 722797
#% 722904
#% 734592
#% 840924
#% 1074228
#% 1077150
#% 1190124
#% 1260273
#% 1271961
#% 1287283
#% 1432735
#! We demonstrate a method for collaborative ranking of future events. Previous work on recommender systems typically relies on feedback on a particular item, such as a movie, and generalizes this to other items or other people. In contrast, we examine a setting where no feedback exists on the particular item. Because direct feedback does not exist for events that have not taken place, we recommend them based on individuals' preferences for past events, combined collaboratively with other peoples' likes and dislikes. We examine the topic of unseen item recommendation through a user study of academic (scientific) talk recommendation, where we aim to correctly estimate a ranking function for each user, predicting which talks would be of most interest to them. Then by decomposing user parameters into shared and individual dimensions, we induce a similarity metric between users based on the degree to which they share these dimensions. We show that the collaborative ranking predictions of future events are more effective than pure content-based recommendation. Finally, to further reduce the need for explicit user feedback, we suggest an active learning approach for eliciting feedback and a method for incorporating available implicit user cues.

#index 1482261
#* Hybrid tag recommendation for social annotation systems
#@ Jonathan Gemmell;Thomas Schimoler;Bamshad Mobasher;Robin Burke
#t 2010
#c 1
#% 202011
#% 220711
#% 280852
#% 321635
#% 330687
#% 414514
#% 734594
#% 946524
#% 1100174
#% 1127455
#% 1327635
#% 1355024
#% 1667787
#! Social annotation systems allow users to annotate resources with personalized tags and to navigate large and complex information spaces without the need to rely on predefined hierarchies. These systems help users organize and share their own resources, as well as discover new ones annotated by other users. Tag recommenders in such systems assist users in finding appropriate tags for resources and help consolidate annotations across all users and resources. But the size and complexity of the data, as well as the inherent noise and inconsistencies in the underlying tag vocabularies, have made the design of effective tag recommenders a challenge. Recent efforts have demonstrated the advantages of integrative models that leverage all three dimensions of a social annotation system: users, resources and tags. Among these approaches are recommendation models based on matrix factorization. But, these models tend to lack scalability and often hide the underlying characteristics, or "information channels" of the data that affect recommendation effectiveness. In this paper we propose a weighted hybrid tag recommender that blends multiple recommendation components drawing separately on complementary dimensions, and evaluate it on six large real-world datasets. In addition, we attempt to quantify the strength of the information channels in these datasets and use these results to explain the performance of the hybrid. We find our approach is not only competitive with the state-of-the-art techniques in terms of accuracy, but also has the added benefits of being scalable to large real world applications, extensible to incorporate a wide range of recommendation techniques, easily updateable, and more scrutable than other leading methods.

#index 1482262
#* XML schema computations: schema compatibility testing and subschema extraction
#@ Thomas Yau-tat Lee;David Wai-lok Cheung
#t 2010
#c 1
#% 299944
#% 772031
#% 949370
#% 1021195
#! In this paper, we propose new models and algorithms to perform practical computations on W3C XML Schemas, which are schema minimization, schema equivalence testing, subschema testing and subschema extraction. We have conducted experiments on an e-commerce standard XSD called xCBL to demonstrate the effectiveness of our algorithms. One experiment has refuted the claim that the xCBL 3.5 XSD is compatible with the xCBL 3.0 XSD. Another experiment has shown that the xCBL XSDs can be effectively trimmed into small subschemas for specific applications, which has significantly reduced schema processing time.

#index 1482263
#* Visual cube and on-line analytical processing of images
#@ Xin Jin;Jiawei Han;Liangliang Cao;Jiebo Luo;Bolin Ding;Cindy Xide Lin
#t 2010
#c 1
#% 227880
#% 248865
#% 420053
#% 818916
#% 823345
#% 837616
#% 915249
#% 954969
#% 1016131
#% 1019162
#% 1040539
#% 1090318
#% 1176884
#! On-Line Analytical Processing (OLAP) has shown great success in many industry applications, including sales, marketing, management, financial data analysis, etc. In this paper, we propose Visual Cube and multi-dimensional OLAP of image collections, such as web images indexed in search engines (e.g., Google and Bing), product images (e.g. Amazon) and photos shared on social networks (e.g., Facebook and Flickr). It provides online responses to user requests with summarized statistics of image information and handles rich semantics related to image visual features. A clustering structure measure is proposed to help users freely navigate and explore images. Efficient algorithms are developed to construct Visual Cube. In addition, we introduce the new issue of Cell Overlapping in data cube and present efficient solutions for Visual Cube computation and OLAP operations. Extensive experiments are conducted and the results show good performance of our algorithms.

#index 1482264
#* Pattern discovery for large mixed-mode database
#@ Andrew K.C. Wong;Bin Wu;Gene P.K. Wu;Keith C.C. Chan
#t 2010
#c 1
#% 35065
#% 136350
#% 297463
#% 350287
#% 420146
#% 443172
#% 443314
#% 443880
#% 480940
#% 578406
#% 813902
#% 998568
#% 1010687
#% 1068966
#% 1776473
#% 1777282
#! In business and industry today, large databases with mixed data types (continuous and categorical) are very common. There are great needs to discover patterns from them for knowledge interpretation and understanding. In the past, for classification, this problem is solved as a discrete data problem by first discretizing the continuous data based on the class-attribute interdependence relationship. However, so far no proper solution exists when class information is unavailable. Hence, important pattern post-processing tasks such as pattern clustering and summarization cannot be applied to mixed-mode data. This paper presents a new method for solving the problem. It is based on two essential concepts. (1) Though class information is absent, yet for a correlated dataset, the attribute with the strongest interdependence with others in the group can be used to drive the discretization of the continuous data. (2) For a large database, correlated attribute groups must first be obtained by attribute clustering before (1) can be applied. Based on (1) and (2), pattern discovery methods are developed for mixed-mode data. Extensive experiments using synthetic and real world data were conducted to validate the usefulness and effectiveness of the proposed method.

#index 1482265
#* Constructing classification features using minimal predictive patterns
#@ Iyad Batal;Milos Hauskrecht
#t 2010
#c 1
#% 136350
#% 152934
#% 190581
#% 197394
#% 300120
#% 329537
#% 459020
#% 466483
#% 466644
#% 481290
#% 536291
#% 722803
#% 810064
#% 813990
#% 823356
#% 824710
#% 867057
#% 915347
#% 1083649
#% 1127267
#% 1206650
#! Choosing good features to represent objects can be crucial to the success of supervised machine learning methods. Recently, there has been a great interest in applying data mining techniques to construct new classification features. The rationale behind this approach is that patterns (feature-value combinations) could capture more underlying semantics than single features. Hence the inclusion of some patterns can improve the classification performance. Currently, most methods adopt a two-phases approach by generating all frequent patterns in the first phase and selecting the discriminative patterns in the second phase. However, this approach has limited success because it is usually very difficult to correctly identify important predictive patterns in a large set of highly correlated frequent patterns. In this paper, we introduce the minimal predictive patterns framework to directly mine a compact set of highly predictive patterns. The idea is to integrate pattern mining and feature selection in order to filter out non-informative and redundant patterns while being generated. We propose some pruning techniques to speed up the mining process. Our extensive experimental evaluation on many datasets demonstrates the advantage of our method by outperforming many well known classifiers.

#index 1482266
#* RankSVR: can preference data help regression?
#@ Hwanjo Yu;Sungchul Kim;Seunghoon Na
#t 2010
#c 1
#% 269218
#% 341269
#% 466419
#% 466887
#% 563100
#% 577224
#% 734915
#% 770753
#% 823348
#% 823360
#% 840846
#% 879588
#% 881477
#% 944095
#% 987241
#% 1074021
#% 1195983
#% 1269778
#% 1451223
#% 1538183
#! In some regression applications (e.g., an automatic movie scoring system), a large number of ranking data is available in addition to the original regression data. This paper studies whether and how the ranking data can improve the accuracy of regression task. In particular, this paper first proposes an extension of SVR (Support Vector Regression), RankSVR, which incorporates ranking constraints in the learning of regression function. Second, this paper proposes novel sampling methods for RankSVR, which selectively choose samples of ranking data for training of regression functions in order to maximize the performance of RankSVR. While it is relatively easier to acquire ranking data than regression data, incorporating all the ranking data in the learning of regression doest not always generate the best output. Moreoever, adding too many ranking constraints into the regression problem substantially lengthens the training time. Our proposed sampling methods find the ranking samples that maximize the regression performance. Experimental results on synthetic and real data sets show that, when the ranking data is additionally available, RankSVR significantly performs better than SVR by utilizing ranking constraints in the learning of regression, and also show that our sampling methods improve the RankSVR performance better than the random sampling.

#index 1482267
#* Estimating accuracy for text classification tasks on large unlabeled data
#@ Snigdha Chaturvedi;Tanveer A. Faruquie;L. Venkata Subramaniam;Mukesh K. Mohania
#t 2010
#c 1
#% 170649
#% 191910
#% 269218
#% 458656
#% 466722
#% 816181
#% 876044
#% 1117007
#% 1455666
#% 1510567
#! Rule based systems for processing text data encode the knowledge of a human expert into a rule base to take decisions based on interactions of the input data and the rule base. Similarly, supervised learning based systems can learn patterns present in a given dataset to make decisions on similar and other related data. Performances of both these classes of models are largely dependent on the training examples seen by them, based on which the learning was performed. Even though trained models might fit well on training data, the accuracies they yield on a new test data may be considerably different. Computing the accuracy of the learnt models on new unlabeled datasets is a challenging problem requiring costly labeling, and which is still likely to only cover a subset of the new data because of the large sizes of datasets involved. In this paper, we present a method to estimate the accuracy of a given model on a new dataset without manually labeling the data. We verify our method on large datasets for two shallow text processing tasks: document classification and postal address segmentation, and using both supervised machine learning methods and human generated rule based models.

#index 1482268
#* A probabilistic topic-connection model for automatic image annotation
#@ Xin Chen;Xiaohua Hu;Zhongna Zhou;Caimei Lu;Gail Rosen;Tingting He;E. K. Park
#t 2010
#c 1
#% 318785
#% 375017
#% 642990
#% 724320
#% 740904
#% 760805
#% 824956
#% 836904
#% 852098
#% 860956
#% 905268
#% 945194
#% 975105
#% 990321
#% 996168
#% 1038781
#% 1074073
#% 1074095
#% 1083703
#% 1214625
#% 1214660
#% 1292515
#! The explosive increase of image data on Internet has made it an important, yet very challenging task to index and automatically annotate image data. To achieve that end, sophisticated algorithms and models have been proposed to study the correlation between image content and corresponding text description. Despite the success of previous works, however, researchers are still facing two major difficulties that may undermine their effort of providing reliable and accurate annotations for images. The first difficulty is lacking of comprehensive benchmark image dataset with high quality text descriptions. The second difficulty is lacking of effective way to represent the image content and make it associate with the text descriptions. In our paper, we aim to deal with both problems. To deal with the first problem, we utilize Wikipedia as external knowledge source and enrich the ontology structure of ImageNet database with comprehensive and highly-reliable text descriptions from Wikipedia articles. To address the second problem, we develop a Probabilistic Topic-Connection (PTC) model to represent the connection between latent semantic topic in text description and latent patterns from image feature space. We compare the performance of our model with the currently popular Correspondence LDA (Corr-LDA) model under the same automatic image annotation scenario using cross-validation. Experimental results demonstrate that our model is able to well represent the connection between latent semantic topics and latent patterns in image feature space, thus facilitates knowledge organization and understanding of both image and text descriptions.

#index 1482269
#* Orientation distance-based discriminative feature extraction for multi-class classification
#@ Bo Liu;Yanshan Xiao;Longbing Cao;Philip S. Yu
#t 2010
#c 1
#% 304879
#% 304931
#% 425048
#% 719278
#% 722756
#% 722943
#% 732387
#% 772223
#% 785367
#% 789030
#% 865329
#% 915334
#% 940486
#% 948368
#% 961267
#% 983907
#% 1013679
#% 1130906
#% 1176885
#% 1176928
#% 1292711
#% 1292762
#% 1411129
#% 1767172
#% 1781733
#% 1860424
#% 1860543
#% 1860941
#% 1861020
#% 1861284
#% 1862255
#! Feature extraction is an effective step in data mining and machine learning. While many feature extraction methods have been proposed for clustering, classification and regression, very limited work has been done on multi-class classification problems. In fact, the accuracy of multi-class classification problems relies on well-extracted features, the modeling part aside. This paper proposes a new feature extraction method, namely extracting orientation distance-based discriminative (ODD) features, which is particularly designed for multi-class classification problems. The proposed method works in two steps. In the first step, we extend the Fisher Discriminant idea to determine more appropriate kernel function and map the input data with all classes into a feature space. In the second step, the ODD features are extracted based on the one-vs-all scheme to generate discriminative features between a pattern and each hyperplane. These newly extracted features are treated as the representative features and are further used in the subsequent classification procedure. Substantial experiments on both UCI and real-world datasets have been conducted to investigate the performance of ODD features based multi-class classification. The statistical results show that the classification accuracy based on ODD features outperforms that of the state-of-the-art feature extraction methods.

#index 1482270
#* Evaluating, combining and generalizing recommendations with prerequisites
#@ Aditya G. Parameswaran;Hector Garcia-Molina;Jeffrey D. Ullman
#t 2010
#c 1
#% 330687
#% 414514
#% 734594
#% 805841
#% 813966
#% 1023517
#% 1089739
#% 1127465
#% 1198363
#% 1250331
#% 1287265
#% 1287282
#% 1287283
#% 1328172
#% 1396094
#% 1396104
#% 1482270
#% 1620192
#! We consider the problem of recommending the best set of k items when there is an inherent ordering between items, expressed as a set of prerequisites (e.g., the movie 'Godfather I' is a prerequisite of 'Godfather II'). Since this general problem is computationally intractable, we develop 3 approximation algorithms to solve this problem for various prerequisite structures (e.g., chain graphs, AND graphs, AND-OR graphs). We derive worst-case bounds for these algorithms for these structures, and experimentally evaluate these algorithms on synthetic data. We also develop an algorithm to combine solutions in order to generate even better solutions, and compare the performance of this algorithm with the other three.

#index 1482271
#* Automatically suggesting topics for augmenting text documents
#@ Robert West;Doina Precup;Joelle Pineau
#t 2010
#c 1
#% 722904
#% 790464
#% 868096
#% 1019082
#% 1077150
#% 1130858
#% 1261563
#% 1279276
#% 1292577
#% 1355286
#% 1734219
#! We present a method for automated topic suggestion. Given a plain-text input document, our algorithm produces a ranking of novel topics that could enrich the input document in a meaningful way. It can thus be used to assist human authors, who often fail to identify important topics relevant to the context of the documents they are writing. Our approach marries two algorithms originally designed for linking documents to Wikipedia articles, proposed by Milne and Witten [15] and West et al. [22]. While neither of them can suggest novel topics by itself, their combination does have this capability. The key step towards finding missing topics consists in generalizing from a large background corpus using principal component analysis. In a quantitative evaluation we conclude that our method achieves the precision of human editors when input documents are Wikipedia articles, and we complement this result with a qualitative analysis showing that the approach also works well on other types of input documents.

#index 1482272
#* Detecting product review spammers using rating behaviors
#@ Ee-Peng Lim;Viet-An Nguyen;Nitin Jindal;Bing Liu;Hady Wirawan Lauw
#t 2010
#c 1
#% 309095
#% 314935
#% 769892
#% 838490
#% 939896
#% 956642
#% 1022742
#% 1035590
#% 1190069
#% 1227655
#% 1261574
#% 1355303
#% 1482375
#! This paper aims to detect users generating spam reviews or review spammers. We identify several characteristic behaviors of review spammers and model these behaviors so as to detect the spammers. In particular, we seek to model the following behaviors. First, spammers may target specific products or product groups in order to maximize their impact. Second, they tend to deviate from the other reviewers in their ratings of products. We propose scoring methods to measure the degree of spam for each reviewer and apply them on an Amazon review dataset. We then select a subset of highly suspicious reviewers for further scrutiny by our user evaluators with the help of a web based spammer evaluation software specially developed for user evaluation experiments. Our results show that our proposed ranking and supervised methods are effective in discovering spammers and outperform other baseline method based on helpfulness votes alone. We finally show that the detected spammers have more significant impact on ratings compared with the unhelpful reviewers.

#index 1482273
#* Classical music for rock fans?: novel recommendations for expanding user interests
#@ Makoto Nakatsuji;Yasuhiro Fujiwara;Akimichi Tanaka;Toshio Uchiyama;Ko Fujimura;Toru Ishida
#t 2010
#c 1
#% 173879
#% 279755
#% 330687
#% 342767
#% 734590
#% 783531
#% 805841
#% 860672
#% 915344
#% 975021
#% 1127466
#% 1201363
#% 1211767
#% 1214666
#% 1214687
#% 1227601
#% 1250380
#% 1260273
#% 1274840
#% 1275197
#% 1473317
#% 1650569
#! Most recommender algorithms produce types similar to those the active user has accessed before. This is because they measure user similarity only from the co-rating behaviors against items and compute recommendations by analyzing the items possessed by the users most similar to the active user. In this paper, we define item novelty as the smallest distance from the class the user accessed before to the class that includes target items over the taxonomy. Then, we try to accurately recommend highly novel items to the user. First, our method measures user similarity by employing items rated by users and a taxonomy of items. It can accurately identify many items that may suit the user. Second, it creates a graph whose nodes are users; weighted edges are set between users according to their similarity. It analyzes the user graph and extracts users that are related on the graph though the similarity between the active user and each of those users is not high. The users so extracted are likely to have highly novel items for the active user. An evaluation conducted on several datasets finds that our method accurately identifies items with higher novelty than previous methods.

#index 1482274
#* Improving one-class collaborative filtering by incorporating rich user information
#@ Yanen Li;Jia Hu;ChengXiang Zhai;Ye Chen
#t 2010
#c 1
#% 124010
#% 220709
#% 397155
#% 452563
#% 528182
#% 577224
#% 813966
#% 838547
#% 879567
#% 881540
#% 1083671
#% 1176909
#% 1176959
#% 1190123
#% 1214666
#% 1214688
#% 1260273
#! One-Class Collaborative Filtering (OCCF) is an emerging setup in collaborative filtering in which only positive examples or implicit feedback can be observed. Compared with the traditional collaborative filtering setting where the data has ratings, OCCF is more realistic in many scenarios when no ratings are available. In this paper, we propose to improve OCCF accuracy by exploiting the rich user information that is often naturally available in community-based interactive information systems, including a user's search query history, purchasing and browsing activities. We propose two ways to incorporate such user information into the OCCF models: one is to linearly combine scores from different sources and the other is to embed user information into collaborative filtering. Experimental results on a large-scale retail data set from a major e-commerce company show that the proposed methods are effective and can improve the performance of the One-Class Collaborative Filtering over baseline methods through leveraging rich user information.

#index 1482275
#* Personalized search by tag-based user profile and resource profile in collaborative tagging systems
#@ Yi Cai;Qing Li
#t 2010
#c 1
#% 399057
#% 754126
#% 818224
#% 855601
#% 956544
#% 956552
#% 956579
#% 987193
#% 1074070
#% 1127482
#% 1130827
#% 1152471
#% 1190119
#% 1405129
#% 1409929
#% 1697448
#! With the increase of resource-sharing web sites such as YouTube1 and Flickr2, personalized search becomes more important and challenging, as users demand higher retrieval quality. To achieve this goal, personalized search needs to take users' personalized profiles and information needs into consideration. Collaborative tagging (also known as folksonomy [11]) systems allow users to annotate resources with their own tags, which provide a simple but powerful way for organizing, retrieving and sharing different types of social resources. In this paper, we examine the limitations of previous tag-based personalized search. To handle these limitations, we propose a new method to model user profiles and resource profiles in a collaborative tagging environment. A novel search method using such users' and resources' profiles is proposed to facilitate the desired personalization in resource search. We implement a prototype system named as FMRS. Experiments using FMRS data set and MovieLens data set show that our proposed method outperforms baseline methods.

#index 1482276
#* A comparison of user and system query performance predictions
#@ Claudia Hauff;Diane Kelly;Leif Azzopardi
#t 2010
#c 1
#% 214709
#% 309089
#% 310567
#% 329090
#% 340921
#% 397161
#% 643001
#% 766496
#% 768905
#% 803556
#% 804915
#% 818257
#% 818267
#% 879566
#% 879614
#% 879632
#% 907544
#% 943042
#% 987212
#% 987260
#% 987265
#% 1074069
#% 1130851
#% 1130990
#% 1173701
#% 1213434
#% 1227597
#% 1227623
#% 1263599
#% 1287229
#% 1384095
#% 1415713
#% 1415785
#! Query performance prediction methods are usually applied to estimate the retrieval effectiveness of queries, where the evaluation is largely system sided. However, little work has been conducted to understand query performance prediction from the user's perspective. The question we consider is, whether the predictions of query performance that systems make are in line with the predictions that users make. To this aim, we compare the performance ratings users assign to queries with the performance scores estimated by a range of pre-retrieval and post-retrieval query performance predictors. Two studies are presented that explore the relationship between user ratings and system predictions on two levels: (i) the topic level, and, (ii) the query suggestions level. It is shown that when predicting the performance of query suggestions, user ratings were mostly uncorrelated with system predictions. At the topic level though, where a single query is judged for each information need, we observed moderate correlations between user ratings and a subset of system predictions. As query performance prediction methods are often based on intuitions of how users might rate queries, these findings suggest that such methods are not representative of how users actually rate query suggestions and topics. This motivates further research into understanding the rating process engaged by users, and developing models of query performance prediction in order to bridge the divide between systems and users.

#index 1482277
#* The anatomy of a click: modeling user behavior on web information systems
#@ Kunal Punera;Srujana Merugu
#t 2010
#c 1
#% 277483
#% 290482
#% 400847
#% 737637
#% 752177
#% 754059
#% 946521
#% 954949
#% 956541
#% 956546
#% 1035578
#% 1166521
#% 1173704
#% 1190055
#% 1190056
#% 1275193
#% 1355037
#! The ultimate goal of information retrieval science continues to be providing relevant information to users while placing minimal cognitive load on them. The retrieval and presentation of relevant information (say, search results) as well as any dynamic system behavior (e.g., search engine re-ranking) depends acutely on estimating user intent. Hence, it is critical to use all the available information about user behavior at any stage of a search-session to accurately infer the user intent. However, the simplistic interfaces provided by search engines in order to minimize the user cognitive effort, and intrinsic limits imposed by privacy concerns, latency requirements, and other web instrumentation challenges, result in only a subset of user actions that are predictive of the search intent being captured. In this paper, we present a dynamic Bayesian network (DBN) that models user interaction with general web information systems, taking into account both observed (clicks etc.) as well as hidden (result examinations etc.) user actions. Our model goes beyond the ranked list information access paradigm and gives a solution where arbitrary context information can be incorporated in a principled fashion. To account for heterogeneity in user behavior as well as information access tasks, we further propose a bi-clustering algorithm that partitions users and tasks, and learns separate models for each bicluster. We instantiate this general DBN model for a typical static search interface comprising of a single query box and a ranked list of search results using a set of seven common user actions and various predictive state attributes. Experimental results on real-world web search log data indicate that one can obtain superior predictive performance on various session properties (such as click positions and reformulations) compared to simpler instantiations of the DBN.

#index 1482278
#* Exploring online social activities for adaptive search personalization
#@ Qihua Wang;Hongxia Jin
#t 2010
#c 1
#% 805877
#% 807295
#% 818224
#% 818259
#% 832349
#% 869536
#% 956552
#% 956553
#% 987193
#% 998805
#% 1074070
#% 1074071
#% 1130877
#% 1190074
#% 1227622
#% 1292590
#% 1409929
#! The web has largely become a very social environment and will continue to become even more so. People are not only enjoying their social visibility on the Web but also increasingly participating in various social activities delivered through the Web. In this paper, we propose to explore a user's public social activities, such as blogging and social bookmarking, to personalize Internet services. We believe that public social data provides a more acceptable way to derive user interests than more private data such as search histories and desktop data. We propose a framework that learns about users' preferences from their activities on a variety of online social systems. As an example, we illustrate how to apply the user interests derived by our system to personalize search results. Furthermore, our system is adaptive; it observes users' choices on search results and automatically adjusts the weights of different social systems during the information integration process, so as to refine its interest profile for each user. We have implemented our approach and performed experiments on real-world data collected from three large-scale online social systems. Over two hundred users from worldwide who are active on the three social systems have been tested. Our experimental results demonstrate the effectiveness of our personalized search approach. Our results also show that integrating information from multiple social systems usually leads to better personalized results than relying on the information from a single social system, and our adaptive approach further improves the performance of the personalization solution.

#index 1482279
#* Predicting short-term interests using activity-based search context
#@ Ryen W. White;Paul N. Bennett;Susan T. Dumais
#t 2010
#c 1
#% 399057
#% 577329
#% 771571
#% 805200
#% 807420
#% 818207
#% 818224
#% 818260
#% 832349
#% 835027
#% 857180
#% 879565
#% 879567
#% 881540
#% 919706
#% 956495
#% 1019076
#% 1074071
#% 1083721
#% 1083899
#% 1130852
#% 1227577
#% 1227622
#% 1267762
#% 1355038
#% 1399944
#% 1450885
#% 1456294
#! A query considered in isolation offers limited information about a searcher's intent. Query context that considers pre-query activity (e.g., previous queries and page visits), can provide richer information about search intentions. In this paper, we describe a study in which we developed and evaluated user interest models for the current query, its context (from pre-query session activity), and their combination, which we refer to as intent. Using large-scale logs, we evaluate how accurately each model predicts the user's short-term interests under various experimental conditions. In our study we: (i) determine the extent of opportunity for using context to model intent; (ii) compare the utility of different sources of behavioral evidence (queries, search result clicks, and Web page visits) for building predictive interest models, and; (iii) investigate optimally combining the query and its context by learning a model that predicts the context weight for each query. Our findings demonstrate significant opportunity in leveraging contextual information, show that context and source influence predictive accuracy, and show that we can learn a near-optimal combination of the query and context for each query. The findings can inform the design of search systems that leverage contextual information to better understand, model, and serve searchers' information needs.

#index 1482280
#* Probabilistic first pass retrieval for search advertising: from theory to practice
#@ Hema Raghavan;Rukmini Iyer
#t 2010
#c 1
#% 54435
#% 248214
#% 262096
#% 280851
#% 298183
#% 321635
#% 340901
#% 342707
#% 411762
#% 730065
#% 750863
#% 818262
#% 818265
#% 867054
#% 869484
#% 869501
#% 956546
#% 987214
#% 987221
#% 1040857
#% 1055706
#% 1130910
#% 1190106
#% 1195837
#% 1227648
#% 1355050
#% 1355052
#% 1392432
#! Information retrieval in search advertising, as in other ad-hoc retrieval tasks, aims to find the most appropriate ranking of the ad documents of a corpus for a given query. In addition to ranking the ad documents, we also need to filter or threshold irrelevant ads from participating in the auction to be displayed alongside search results. In this work, we describe our experience in implementing a successful ad retrieval system for a commercial search engine based on the Language Modeling (LM) framework for retrieval. The LM demonstrates significant performance improvements over the baseline vector space model (TF-IDF) system that was in production at the time. From a modeling perspective, we propose a novel approach to incorporate query segmentation and phrases in the LM framework, discuss impact of score normalization for relevance filtering, and present preliminary results of incorporating query expansions using query rewriting techniques. From an implementation perspective, we also discuss real-time latency constraints of a production search engine and how we overcome them by adapting the WAND algorithm to work with language models. In sum, our LM formulation is considerably better in terms of accuracy metrics such as Precision-Recall (10% improvement in AUC) and nDCG (8% improvement in nDCG@5) on editorial data and also demonstrates significant improvements in clicks in live user tests (0.787% improvement in Click Yield, with 8% coverage increase). Finally, we hope that this paper provides the reader with adequate insights into the challenges of building a system that serves millions of users every day.

#index 1482281
#* Faceted search and browsing of audio content on spoken web
#@ Mamadou Diao;Sougata Mukherjea;Nitendra Rajput;Kundan Srivastava
#t 2010
#c 1
#% 201814
#% 268079
#% 394856
#% 399088
#% 452641
#% 755194
#% 783560
#% 838500
#% 860087
#% 860088
#% 939386
#% 949256
#% 993044
#% 1002005
#% 1022757
#% 1055765
#% 1074096
#% 1237728
#% 1350334
#% 1384170
#% 1431116
#% 1499606
#% 1896501
#! Spoken Web is a web of VoiceSites that can be accessed by a phone. The content in a VoiceSite is audio. Therefore Spoken Web provides an alternate to the World Wide Web (WWW) in developing regions where low Internet penetration and low literacy are barriers to accessing the conventional WWW. Searching of audio content in Spoken Web through an audio query-result interface presents two key challenges: indexing of audio content is not accurate, and the presentation of results in audio is sequential, and therefore cumbersome. In this paper, we apply the concepts of faceted search and browsing to the SpokenWeb search problem. We use the concepts of facets to index the meta-data associated with the audio content. We provide a mechanism to rank the facets based on the search results. We develop an interactive query interface that enables easy browsing of search results through the top ranked facets. To our knowledge, this is the first system to use the concepts of facets in audio search, and the first solution that provides an audio search for the rural population. We present quantitative results to illustrate the accuracy and effectiveness of the faceted search and qualitative results to highlight the usability of the interactive browsing system. The experiments have been conducted on more than 4000 audio documents collected from a live SpokenWeb VoiceSite and evaluations were carried out with 40 farmers who are the target users of the VoiceSite.

#index 1482282
#* Predicting product adoption in large-scale social networks
#@ Rushi Bhatt;Vineet Chaoji;Rajesh Parekh
#t 2010
#c 1
#% 342596
#% 577217
#% 729923
#% 881460
#% 956578
#% 1055690
#% 1055737
#% 1055763
#% 1083624
#% 1159217
#% 1190081
#% 1214642
#% 1214692
#% 1399963
#% 1676017
#! Online social networks offer opportunities to analyze user behavior and social connectivity and leverage resulting insights for effective online advertising. We study the adoption of a paid product by members of a large and well-connected Instant Messenger (IM) network. This product is important to the business and poses unique challenges to advertising due to its low baseline adoption rate. We find that adoption by highly connected individuals is correlated with their social connections (friends) adopting after them. However, there is little evidence of social influence by these high degree individuals. Further, the spread of adoption remains mostly local to first-adopters and their immediate friends. We observe strong evidence of peer pressure wherein future adoption by an individual is more likely if the product has been widely adopted by the individual's friends. Social neighborhoods rich in adoptions also continue to add more new adoptions compared to those neighborhoods that are poor in adoption. Using these insights we build predictive models to identify individuals most suited for two types of marketing campaigns - direct marketing where individuals with highest propensity for future adoption are targeted with suitable ads and social neighborhood marketing which involves messaging to members of the social network who are most effective in using the power of their network to convince their friends to adopt. We identify the most desirable features for predicting future adoption of the PC To Phone product which can in turn be leveraged to effectively promote its adoption. Offline analysis shows that building predictive models for direct marketing and social neighborhood marketing outperforms several widely accepted marketing heuristics. Further, these models are able to effectively combine user features and social features to predict adoption better than using either user features or social features in isolation.

#index 1482283
#* Reverted indexing for feedback and expansion
#@ Jeremy Pickens;Matthew Cooper;Gene Golovchinsky
#t 2010
#c 1
#% 144029
#% 262096
#% 340146
#% 340901
#% 342679
#% 411760
#% 730007
#% 818230
#% 878657
#% 987222
#% 1035571
#% 1077150
#% 1130863
#% 1227630
#% 1373774
#% 1715606
#! Traditional interactive information retrieval systems function by creating inverted lists, or term indexes. For every term in the vocabulary, a list is created that contains the documents in which that term occurs and its relative frequency within each document. Retrieval algorithms then use these term frequencies alongside other collection statistics to identify the matching documents for a query. In this paper, we turn the process around: instead of indexing documents, we index query result sets. First, queries are run through a chosen retrieval system. For each query, the resulting document IDs are treated as terms and the score or rank of the document is used as the frequency statistic. An index of documents retrieved by basis queries is created. We call this index a reverted index. With reverted indexes, standard retrieval algorithms can retrieve the matching queries (as results) for a set of documents (used as queries). These recovered queries can then be used to identify additional documents, or to aid the user in query formulation, selection, and feedback.

#index 1482284
#* Improving verbose queries using subset distribution
#@ Xiaobing Xue;Samuel Huston;W. Bruce Croft
#t 2010
#c 1
#% 262096
#% 269217
#% 340948
#% 397161
#% 464434
#% 577224
#% 789959
#% 816181
#% 817472
#% 818262
#% 833890
#% 855119
#% 907493
#% 1074098
#% 1074112
#% 1173692
#% 1195837
#% 1227636
#% 1227647
#% 1227648
#% 1355019
#% 1650403
#! Dealing with verbose (or long) queries poses a new challenge for information retrieval. Selecting a subset of the original query (a "sub-query") has been shown to be an effective method for improving these queries. In this paper, the distribution of sub-queries ("subset distribution") is formally modeled within a well-grounded framework. Specifically, sub-query selection is considered as a sequential labeling problem, where each query word in a verbose query is assigned a label of "keep" or "don't keep". A novel Conditional Random Field model is proposed to generate the distribution of sub-queries. This model captures the local and global dependencies between query words and directly optimizes the expected retrieval performance on a training set. The experiments, based on different retrieval models and performance measures, show that the proposed model can generate high-quality sub-query distributions and can significantly outperform state-of-the-art techniques.

#index 1482285
#* A unified optimization framework for robust pseudo-relevance feedback algorithms
#@ Joshua V. Dillon;Kevyn Collins-Thompson
#t 2010
#c 1
#% 262096
#% 277483
#% 757953
#% 766497
#% 827581
#% 838532
#% 879585
#% 1074104
#% 1074127
#% 1211836
#% 1216776
#% 1263579
#% 1292550
#% 1856555
#! We present a flexible new optimization framework for finding effective, reliable pseudo-relevance feedback models that unifies existing complementary approaches in a principled way. The result is an algorithmic approach that not only brings together different benefits of previous methods, such as parameter self-tuning and risk reduction from term dependency modeling, but also allows a rich new space of model search strategies to be investigated. We compare the effectiveness of a unified algorithm to existing methods by examining iterative performance and risk-reward tradeoffs. We also discuss extensions for generating new algorithms within our framework.

#index 1482286
#* Ranking related entities: components and analyses
#@ Marc Bron;Krisztian Balog;Maarten de Rijke
#t 2010
#c 1
#% 279755
#% 287253
#% 298183
#% 340899
#% 397163
#% 642992
#% 740900
#% 756964
#% 815868
#% 879570
#% 938705
#% 1019135
#% 1100828
#% 1130158
#% 1133171
#% 1263249
#% 1392465
#% 1476276
#! Related entity finding is the task of returning a ranked list of homepages of relevant entities of a specified type that need to engage in a given relationship with a given source entity. We propose a framework for addressing this task and perform a detailed analysis of four core components; co-occurrence models, type filtering, context modeling and homepage finding. Our initial focus is on recall. We analyze the performance of a model that only uses co-occurrence statistics. While this method identifies the potential set of related entities, it fails to rank them effectively. Two types of error emerge: (1) entities of the wrong type pollute the ranking and (2) while somehow associated to the source entity, some retrieved entities do not engage in the right relation with it. To address (1), we add type filtering based on category information available in Wikipedia. To correct for (2), we complement our related entity finding method with contextual information, represented as language models derived from documents in which source and target entities co-occur. To complete the pipeline, we find homepages of top ranked entities by combining a language modeling approach with heuristics based on Wikipedia's external links. Our method achieves very high recall scores on the end-to-end task, providing a solid starting point for expanding our focus to improve precision. Our framework can effectively incorporate additional heuristics and these extensions lead to state-of-the-art performance.

#index 1482287
#* Semantic tags generation and retrieval for online advertising
#@ Roberto Mirizzi;Azzurra Ragone;Tommaso Di Noia;Eugenio Di Sciascio
#t 2010
#c 1
#% 282905
#% 783560
#% 813995
#% 975019
#% 987262
#% 1179122
#% 1190106
#% 1206702
#% 1209656
#% 1288161
#% 1333442
#% 1333446
#% 1409942
#! One of the main problems in online advertising is to display ads which are relevant and appropriate w.r.t. what the user is looking for. Often search engines fail to reach this goal as they do not consider semantics attached to keywords. In this paper we propose a system that tackles the problem by two different angles: help (i) advertisers to create more efficient ads campaigns and (ii) ads providers to properly match ads content to keywords in search engines. We exploit semantic relations stored in the DBpedia dataset and use an hybrid ranking system to rank keywords and to expand queries formulated by the user. Inputs of our ranking system are (i) the DBpedia dataset; (ii) external information sources such as classical search engine results and social tagging systems. We compare our approach with other RDF similarity measures, proving the validity of our algorithm with an extensive evaluation involving real users.

#index 1482288
#* MENTA: inducing multilingual taxonomies from wikipedia
#@ Gerard de Melo;Gerhard Weikum
#t 2010
#c 1
#% 179876
#% 282421
#% 286069
#% 288780
#% 309141
#% 344361
#% 812082
#% 939601
#% 956564
#% 987276
#% 1013696
#% 1019105
#% 1055735
#% 1116141
#% 1155717
#% 1166514
#% 1223597
#% 1227999
#% 1264778
#% 1292483
#% 1292517
#% 1305622
#% 1328332
#% 1397569
#% 1409954
#% 1470572
#% 1471265
#% 1698872
#% 1713449
#! In recent years, a number of projects have turned to Wikipedia to establish large-scale taxonomies that describe orders of magnitude more entities than traditional manually built knowledge bases. So far, however, the multilingual nature of Wikipedia has largely been neglected. This paper investigates how entities from all editions of Wikipedia as well as WordNet can be integrated into a single coherent taxonomic class hierarchy. We rely on linking heuristics to discover potential taxonomic relationships, graph partitioning to form consistent equivalence classes of entities, and a Markov chain-based ranking approach to construct the final taxonomy. This results in MENTA (Multilingual Entity Taxonomy), a resource that describes 5.4 million entities and is presumably the largest multilingual lexical knowledge base currently available.

#index 1482289
#* Ontology emergence from folksonomies
#@ Kaipeng Liu;Binxing Fang;Weizhe Zhang
#t 2010
#c 1
#% 198016
#% 280849
#% 384416
#% 445515
#% 561154
#% 748550
#% 754089
#% 855601
#% 869525
#% 946524
#% 956515
#% 989714
#% 997095
#% 1055739
#% 1119135
#% 1131827
#% 1156304
#% 1190119
#% 1190133
#% 1220736
#! The folksonomies built from the large-scale social annotations made by collaborating users are perfect data sources for bootstrapping Semantic Web applications. In this paper, we develop an ontology induction approach to harvest the emergent semantics from the folksonomies. We propose a latent subsumption hierarchy model to uncover the implicit structure of tag space and develop our ontology induction approach on basis of this model. We identify tag subsumptions with a set-theoretical approach and model the tag space as a tag subsumption graph. While turning this graph into a concept hierarchy, we address the problem of inconsistent subsumptions and propose a random walk based tag generality ranking procedure to settle it. We propose an agglomerative hierarchical clustering algorithm utilizing the result of tag generality ranking to generate the concept hierarchy. We conduct experiments on the Delicious dataset. The results of both qualitative and quantitative evaluation demonstrate the effectiveness of the proposed approach.

#index 1482290
#* Multi-document topic segmentation
#@ Minwoo Jeong;Ivan Titov
#t 2010
#c 1
#% 278106
#% 280838
#% 340950
#% 350859
#% 413592
#% 448786
#% 643015
#% 722904
#% 748583
#% 787502
#% 817489
#% 881529
#% 939503
#% 939539
#% 987217
#% 991230
#% 1264752
#% 1270690
#% 1471367
#! Multiple documents describing the same or closely related sets of events are common and often easy to obtain: for example, consider document clusters on a news aggregator site or multiple reviews of the same product or service. Even though each such document discusses a similar set of topics, they provide alternative views or complimentary information on each of these topics. We argue that revealing hidden relations by jointly segmenting the documents, or, equivalently, predicting links between topically related segments in different documents would help to visualize documents of interest and construct friendlier user interfaces. In this paper, we refer to this problem as multi-document topic segmentation. We propose an unsupervised Bayesian model for the considered problem that models both shared and document-specific topics, and utilizes Dirichlet process priors to determine the effective number of topics. We show that topic segmentation can be inferred efficiently using a simple split-merge sampling algorithm. The resulting method outperforms baseline models on four datasets for multi-document topic segmentation.

#index 1482291
#* Meta-metadata: a metadata semantics language for collection representation applications
#@ Andruid Kerne;Yin Qu;Andrew M. Webb;Sashikanth Damaraju;Nic Lupfer;Abhinav Mathur
#t 2010
#c 1
#% 343769
#% 480309
#% 679857
#% 894555
#% 956568
#% 967276
#% 1019082
#% 1063570
#% 1065244
#% 1065277
#% 1090196
#% 1095879
#% 1130858
#% 1252652
#! Collecting, organizing, and thinking about diverse information resources is the keystone of meaningful digital information experiences, from research to education to leisure. Metadata semantics are crucial for organizing collections, yet their structural diversity exacerbates problems of obtaining and manipulating them, strewing end users and application developers amidst the shadows of a proverbial tower of Babel. We introduce meta-metadata, a language and software architecture addressing a metadata semantics lifecycle: (1) data structures for representation of metadata in programs; (2) metadata extraction from information resources; (3) semantic actions that connect metadata to collection representation applications; and (4) rules for presentation to users. The language enables power users to author metadata semantics wrappers that generalize template-based information sources. The architecture supports development of independent collection representation applications that reuse wrappers. The initial meta-metadata repository of information source wrappers includes Google, Flickr, Yahoo, IMDb, Wikipedia, and the ACM Portal. Case studies validate the approach.

#index 1482292
#* Clickthrough-based translation models for web search: from word models to phrase models
#@ Jianfeng Gao;Xiaodong He;Jian-Yun Nie
#t 2010
#c 1
#% 184488
#% 280819
#% 280851
#% 287253
#% 298183
#% 309095
#% 309126
#% 340948
#% 342961
#% 397128
#% 722904
#% 740915
#% 766428
#% 816170
#% 818239
#% 818262
#% 838398
#% 838530
#% 843645
#% 879567
#% 939939
#% 987194
#% 989578
#% 1074081
#% 1074110
#% 1227621
#% 1292709
#% 1399978
#! Web search is challenging partly due to the fact that search queries and Web documents use different language styles and vocabularies. This paper provides a quantitative analysis of the language discrepancy issue, and explores the use of clickthrough data to bridge documents and queries. We assume that a query is parallel to the titles of documents clicked on for that query. Two translation models are trained and integrated into retrieval models: A word-based translation model that learns the translation probability between single words, and a phrase-based translation model that learns the translation probability between multi-term phrases. Experiments are carried out on a real world data set. The results show that the retrieval systems that use the translation models outperform significantly the systems that do not. The paper also demonstrates that standard statistical machine translation techniques such as word alignment, bilingual phrase extraction, and phrase-based decoding, can be adapted for building a better Web document retrieval system.

#index 1482293
#* Temporal query log profiling to improve web search ranking
#@ Alexander Kotov;Pranam Kolari;Lei Duan;Yi Chang
#t 2010
#c 1
#% 309095
#% 590524
#% 728195
#% 766408
#% 772018
#% 807297
#% 840846
#% 869471
#% 879567
#% 915286
#% 957992
#% 987228
#% 987241
#% 987245
#% 989628
#% 1074083
#% 1125901
#% 1125904
#% 1166523
#% 1190055
#% 1190056
#% 1194310
#% 1194311
#% 1194312
#% 1270766
#% 1355034
#! Temporal information can be leveraged and incorporated to improve web search ranking. In this work, we propose a method to improve the ranking of search results by identifying the fundamental properties of temporal behavior of low-quality hosts and spam-prone queries in search logs and modeling those properties as quantifiable features. In particular, we introduce the concepts of host churn, a measure of changes in host visibility for user queries, and query volatility, a measure of semantic instability of query results, and propose the methods for construction of temporal profiles from search query logs that can be used for estimation of a set of features based on the introduced concepts. The utility of the proposed concepts has been experimentally demonstrated for two language-independent search tasks: the regression-based ranking of search results and a novel classification problem of detecting spam-prone queries introduced in this work.

#index 1482294
#* Improving web search relevance and freshness with content previews
#@ Siva Gurumurthy;Hang Su;Vasileios Kandylas;Vidhyashankar Venkataraman
#t 2010
#c 1
#% 268079
#% 309095
#% 577224
#% 869534
#% 881540
#% 889879
#% 956544
#% 1035588
#% 1074094
#% 1081777
#% 1130913
#% 1214725
#% 1355017
#% 1568714
#! Traditional web search engines find it challenging to achieve good search quality for recency-sensitive queries, as they are prone to delays in discovering, indexing and ranking new web pages. In this paper we introduce PreGen, an adaptive preview generation system, which is run as part of a web search engine to improve search result quality for recency-sensitive queries. PreGen uses a machine learning algorithm to classify and select live web feeds, and generates "previews" of new web pages based on the link descriptions available in these feeds. The search engine can then index and present relevant page previews as part of its search results before the pages are fetched from the web, thereby reducing end-to-end delays. Our experiments show that PreGen improves the search relevance of a state-of-the-art search engine for recency-sensitive queries by 3% and reduces the average latencies of affected documents by 50%.

#index 1482295
#* Organizing query completions for web search
#@ Alpa Jain;Gilad Mishne
#t 2010
#c 1
#% 118771
#% 310567
#% 320839
#% 325001
#% 342961
#% 403341
#% 413621
#% 458630
#% 747738
#% 783483
#% 805839
#% 869500
#% 869501
#% 869651
#% 878454
#% 896031
#% 943837
#% 956570
#% 1130854
#% 1173699
#% 1213625
#% 1217199
#% 1227594
#% 1269907
#% 1280748
#% 1400017
#! All state-of-the-art web search engines implement an auto-completion mechanism - an assistive technology enabling users to effectively formulate their search queries by predicting the next characters or words that they are likely to type. Query completions (or suggestions) are typically mined from past user interactions with the search engine, e.g., from query logs, clickthrough patterns, or query reformulations; they are ranked by some measure of query popularity, e.g., query frequency or clickthrough rate. Current query suggestion tools largely assume that the set of suggestions provided to the users is homogeneous, corresponding to a single real-world interpretation of the query. In this paper, we hypothesize that, in some cases, users would benefit from an alternative presentation of the suggestions, one where suggestions are not only ordered by likelihood but also organized by high-level user intent. Rich search suggestion interaction frameworks that reduce the user effort in identifying the set of relevant suggestions open new and promising directions towards improving user experience. Along these lines, we propose clustering the set of suggestions presented to a search engine user, and assigning an appropriate label to each subset of suggestions to help users quickly identify useful ones. For this, we present a variety of unsupervised clustering techniques for search suggestions, based on the information available to a large-scale web search engine. We evaluate our novel search suggestion presentation techniques on a real-world dataset of query logs. Based on a set of user studies, we show that by extending the existing assistance layer to effectively group suggestions and label them - while accounting for the query popularity - we substantially increase the user's satisfaction.

#index 1482296
#* Selectively diversifying web search results
#@ Rodrygo L.T. Santos;Craig Macdonald;Iadh Ounis
#t 2010
#c 1
#% 92533
#% 243728
#% 262112
#% 397161
#% 590523
#% 642975
#% 642982
#% 818267
#% 879618
#% 928355
#% 944349
#% 987260
#% 1024548
#% 1074065
#% 1074113
#% 1074133
#% 1166473
#% 1174573
#% 1190093
#% 1227591
#% 1227709
#% 1292596
#% 1400021
#% 1417245
#% 1467729
#% 1470617
#% 1537464
#% 1697422
#% 1697424
#! Search result diversification is a natural approach for tackling ambiguous queries. Nevertheless, not all queries are equally ambiguous, and hence different queries could benefit from different diversification strategies. A more lenient or more aggressive diversification strategy is typically encoded by existing approaches as a trade-off between promoting relevance or diversity in the search results. In this paper, we propose to learn such a trade-off on a per-query basis. In particular, we examine how the need for diversification can be learnt for each query - given a diversification approach and an unseen query, we predict an effective trade-off between relevance and diversity based on similar previously seen queries. Thorough experiments using the TREC ClueWeb09 collection show that our selective approach can significantly outperform a uniform diversification for both classical and state-of-the-art diversification approaches.

#index 1482297
#* Building re-usable dictionary repositories for real-world text mining
#@ Shantanu Godbole;Indrajit Bhattacharya;Ajay Gupta;Ashish Verma
#t 2010
#c 1
#% 267027
#% 283180
#% 577285
#% 747738
#% 770858
#% 1045660
#% 1083725
#% 1182904
#% 1214729
#% 1227700
#% 1305475
#% 1318599
#! Text mining, though still a nascent industry, has been growing quickly along with the awareness of the importance of unstructured data in business analytics, customer retention and extension, social media, and legal applications. There has been a recent increase in the number of commercial text mining product and service offerings, but successful or wide-spread deployments are rare, mainly due to a dependence on the expertise and skill of practitioners. Accordingly, there is a growing need for re-usable repositories for text mining. In this paper, we focus on dictionary-based text mining and its role in enabling practitioners in understanding and analyzing large text datasets. We motivate and define the problem of exploratory dictionary construction for capturing concepts of interest, and propose a framework for efficient construction, tuning, and re-use of these dictionaries across datasets. The construction framework offers a range of interaction modes to the user to quickly build concept dictionaries over large datasets. We also show how to adapt one or more dictionaries across domains and tasks, thereby enabling reuse of knowledge and effort in industrial practice. We present results and case studies on real-life CRM analytics datasets, where such repositories and tooling significantly cut down practitioner time and effort for dictionary-based text mining.

#index 1482298
#* OpinionIt: a text mining system for cross-lingual opinion analysis
#@ Honglei Guo;Huijia Zhu;Zhili Guo;Xiaoxun Zhang;Zhong Su
#t 2010
#c 1
#% 78171
#% 280819
#% 722904
#% 769892
#% 769967
#% 805873
#% 876017
#% 879587
#% 907489
#% 939346
#% 956510
#% 1055682
#% 1055683
#% 1055768
#% 1074055
#% 1195050
#% 1214741
#% 1214749
#% 1250237
#% 1292576
#! Opinion mining focuses on extracting customers' opinions from the reviews and predicting their sentiment orientation. Reviewers usually praise a product in some aspects and bemoan it in other aspects. With the business globalization, it is very important for enterprises to extract the opinions toward different aspects and find out cross-lingual/cross-culture difference in opinions. Cross-lingual opinion mining is a very challenging task as amounts of opinions are written in different languages, and not well structured. Since people usually use different words to describe the same aspect in the reviews, product-feature (PF) categorization becomes very critical in cross-lingual opinion mining. Manual cross-lingual PF categorization is time consuming, and practically infeasible for the massive amount of data written in different languages. In order to effectively find out cross-lingual difference in opinions, we present an aspect-oriented opinion mining method with Cross-lingual Latent Semantic Association (CLaSA). We first construct CLaSA model to learn the cross-lingual latent semantic association among all the PFs from multi-dimension semantic clues in the review corpus. Then we employ CLaSA model to categorize all the multilingual PFs into semantic aspects, and summarize cross-lingual difference in opinions towards different aspects. Experimental results show that our method achieves better performance compared with the existing approaches. With CLaSA model, our text mining system OpinionIt can effectively discover cross-lingual difference in opinions.

#index 1482299
#* Hierarchical service analytics for improving productivity in an enterprise service center
#@ Chunye Wang;Ram Akella;Srikant Ramachandran
#t 2010
#c 1
#% 176887
#% 220133
#% 255137
#% 309141
#% 413592
#% 458379
#% 465747
#% 465754
#% 466078
#% 494575
#% 750863
#% 765520
#% 765521
#% 881565
#% 1074079
#% 1214729
#% 1292606
#% 1313899
#! Modern day service centers are the building blocks for highly efficient and productive business systems in a knowledge economy. In these service systems, accurate and timely delivery of pertinent information to service representatives becomes the cornerstone for delivering efficient customer service. There are two main steps in achieving this objective. The first step concerns efficient text mining to extract critical and pertinent information from the very long service request (SR) documents in the historical database. The second step concerns matching new service requests with previously stored service requests. Both lead to efficiencies by minimizing time spent by service personnel in extracting Intellectual Capital (IC). In this paper we present our text analytics system, the Service Request Analyzer and Recommender (SRAR), which is designed to improve the productivity in an enterprise service center for computer network diagnostics and support. SRAR unifies a text preprocessor, a hierarchical classifier, and a service request recommender, to deliver critical, pertinent, and categorized knowledge for improved service efficiency. The novel feature we report here is identifying the components of the diagnostic process underlying the creation of the original text documents. This identification is crucial in the successful design and prototyping of SRAR and its hierarchical classifier element. Equally, the use of domain knowledge and human expertise to generate features are indispensable synergistic elements in improving the accuracy of the text analysis toward identifying the components of the diagnostic process. The evaluation and comparison of SRAR with other benchmark approaches in the literature demonstrate the effectiveness of our framework and algorithms. This framework can be generalized to be applicable in many service industries and business functions that mine textual data to achieve increased efficiency in their service delivery. We observe significant service time responsiveness improvements during the first step of IC extraction in network service center context at Cisco.

#index 1482300
#* VSEncoding: efficient coding and fast decoding of integer lists via dynamic programming
#@ Fabrizio Silvestri;Rossano Venturini
#t 2010
#c 1
#% 230434
#% 290703
#% 324012
#% 420491
#% 453323
#% 657603
#% 766445
#% 786632
#% 864446
#% 865740
#% 903340
#% 1000331
#% 1035571
#% 1190095
#% 1392439
#% 1417245
#! Encoding lists of integers efficiently is important for many applications in different fields. Adjacency lists of large graphs are usually encoded to save space and to improve decoding speed. Inverted indexes of Information Retrieval systems keep the lists of postings compressed in order to exploit the memory hierarchy. Secondary indexes of DBMSs are stored similarly to inverted indexes in IR systems. In this paper we propose Vector of Splits Encoding (VSEncoding), a novel class of encoders that work by optimally partitioning a list of integers into blocks which are efficiently compressed by using simple encoders. In previous works heuristics were applied during the partitioning step. Instead, we find the optimal solution by using a dynamic programming approach. Experiments show that our class of encoders outperform all the existing methods in literature by more than 10% (with the exception of Binary Interpolative Coding with which they, roughly, tie) still retaining a very fast decompression algorithm.

#index 1482301
#* Efficient term proximity search with term-pair indexes
#@ Hao Yan;Shuming Shi;Fan Zhang;Torsten Suel;Ji-Rong Wen
#t 2010
#c 1
#% 198335
#% 212665
#% 268079
#% 278831
#% 280834
#% 290703
#% 292684
#% 340886
#% 397150
#% 397608
#% 463737
#% 480330
#% 643566
#% 730065
#% 805862
#% 805864
#% 867054
#% 879611
#% 879651
#% 907504
#% 987214
#% 987229
#% 1015265
#% 1019139
#% 1019182
#% 1074067
#% 1130876
#% 1166527
#% 1190095
#% 1227595
#% 1355053
#% 1387547
#% 1404894
#% 1415737
#! There has been a large amount of research on early termination techniques in web search and information retrieval. Such techniques return the top-k documents without scanning and evaluating the full inverted lists of the query terms. Thus, they can greatly improve query processing efficiency. However, only a limited amount of efficient top-k processing work considers the impact of term proximity, i.e., the distance between term occurrences in a document, which has recently been integrated into a number of retrieval models to improve effectiveness. In this paper, we propose new early termination techniques for efficient query processing for the case where term proximity is integrated into the retrieval model. We propose new index structures based on a term-pair index, and study new document retrieval strategies on the resulting indexes. We perform a detailed experimental evaluation on our new techniques and compare them with the existing approaches. Experimental results on large-scale data sets show that our techniques can significantly improve the efficiency of query processing.

#index 1482302
#* Improved index compression techniques for versioned document collections
#@ Jinru He;Junyuan Zeng;Torsten Suel
#t 2010
#c 1
#% 118741
#% 146377
#% 227784
#% 321455
#% 342373
#% 420491
#% 459945
#% 482092
#% 570319
#% 654447
#% 656274
#% 766445
#% 768815
#% 805476
#% 864446
#% 867054
#% 956535
#% 960181
#% 987257
#% 1051061
#% 1055710
#% 1190095
#% 1292507
#% 1392437
#% 1392439
#% 1426548
#% 1688264
#% 1715618
#! Current Information Retrieval systems use inverted index structures for efficient query processing. Due to the extremely large size of many data sets, these index structures are usually kept in compressed form, and many techniques for optimizing compressed size and query processing speed have been proposed. In this paper, we focus on versioned document collections, that is, collections where each document is modified over time, resulting in multiple versions of the document. Consecutive versions of the same document are often similar, and several researchers have explored ideas for exploiting this similarity to decrease index size. We propose new index compression techniques for versioned document collections that achieve reductions in index size over previous methods. In particular, we first propose several bitwise compression techniques that achieve a compact index structure but that are too slow for most applications. Based on the lessons learned, we then propose additional techniques that come close to the sizes of the bitwise technique while also improving on the speed of the best previous methods.

#index 1482303
#* Building efficient multi-threaded search nodes
#@ Carolina Bonacic;Carlos García;Mauricio Marin;Manuel Prieto-Matias;Francisco Tirado
#t 2010
#c 1
#% 69503
#% 860861
#% 976948
#% 1099737
#% 1106872
#% 1108001
#% 1131151
#% 1190097
#% 1190098
#% 1207189
#% 1344646
#% 1383607
#! Search nodes are single-purpose components of large Web search engines and their efficient implementation is critical to sustain thousands of queries per second and guarantee individual query response times within a fraction of a second. Current technology trends indicate that search nodes ought to be implemented as multi-threaded multi-core systems. The straightforward solution that system designers can apply in this case is simply to follow standard practice by deploying one asynchronous thread per active query in the node and attaching each thread to a different core. Each concurrent thread is responsible for sequentially processing a single query at a time. The only potential source of read/write conflicts among threads are the accesses to the different application caches present in the search node. However, new Web applications pose much more demanding requirements in terms of read/write conflicts than recent past applications since now data updates must take place concurrently with query processing. Insisting on the same paradigm of concurrent threads now augmented with a transaction concurrency control protocol is a feasible solution. In this paper we propose a more efficient and much simpler solution which has the additional advantage of enabling a very efficient administration of application caches. We propose performing relaxed bulk-synchronous parallelism at multi-core level.

#index 1482304
#* Real-time memory efficient data redundancy removal algorithm
#@ Vikas K. Garg;Ankur Narang;Souvik Bhattacherjee
#t 2010
#c 1
#% 307424
#% 322884
#% 340179
#% 411437
#% 646223
#% 654461
#% 835744
#% 1111952
#% 1171636
#% 1247843
#% 1656854
#! Data intensive computing has become a central theme in research community and industry. There is an ever growing need to process and analyze massive amounts of data from diverse sources such as telecom call data records, telescope imagery, online transaction records, web pages, stock markets, medical records (monitoring critical health conditions of patients), climate warning systems, etc. Removing redundancy in the data is an important problem as it helps in resource and compute efficiency for downstream processing of the massive (1 billion to 10 billion records) datasets. In application domains such as IR, stock markets, telecom and others, there is a strong need for real-time data redundancy removal (referred to as DRR) of enormous amounts of data flowing at the rate of 1 GB/s or more. Real-time scalable data redundancy removal on massive datasets is a challenging problem. We present the design of a novel parallel data redundancy removal algorithm for both in-memory and disk-based execution. We also develop queueing theoretic analysis to optimize the throughput of our parallel algorithm on multi-core architectures. For 500 million records, our parallel algorithm can perform complete de-duplication in 255s, on 16 core Intel Xeon 5570 architecture, with in-memory execution. This gives a throughput of 2M records/s. For 6 billion records, our parallel algorithm can perform complete de-duplication in less than 4.5 hours, using 6 cores of Intel Xeon 5570, with disk-based execution. This gives a throughput of around 370K records/s. To the best of our knowledge, this is the highest real-time throughput for data redundancy removal on such massive datasets. We also demonstrate the scalability of our algorithm with increasing number of cores and data.

#index 1482305
#* Search-log anonymization and advertisement: are they mutually exclusive?
#@ Thorben Burghardt;Klemens Böhm;Achim Guttmann;Chris Clifton
#t 2010
#c 1
#% 576761
#% 801690
#% 804805
#% 864412
#% 867872
#% 878624
#% 1127361
#% 1328187
#! The revenue of search-engine providers strongly depends on targeted advertisement. Targeted advertisement is becoming more reliant on personal data. This puts user privacy at risk. One way to improve privacy is to anonymize search logs, but this reduces usefulness for ad placement. Further, the usefulness depends on the target function used for the anonymization. This paper is the first to study this tradeoff systematically. We quantify the usefulness of an anonymized search log for advertisement purposes, by estimating outcomes such as the number of clicks on ads or the number of ad impressions possible after anonymization. A main result is that anonymized search logs are still useful for advertisement purposes, but the extent strongly depends on the target function.

#index 1482306
#* Automated interaction in social networks with datalog
#@ Royi Ronen;Oded Shmueli
#t 2010
#c 1
#% 11797
#% 55408
#% 384978
#% 599549
#% 801686
#% 1181262
#% 1206916
#! The Query Network [12] is a model for query-based social networks automation features, motivated by the rise of social networks as a central internet application. This work generalizes the model to consist of a proposal query and an acceptance query for each participant. As a result, addition of edges is done by coordination between participants, simulating interactions between participants. We designed, implemented and experimented with evaluation algorithms for this new model. Experiments with both synthetic and real datasets show the high effectiveness of our methods.

#index 1482307
#* Towards a provenance framework for sub-image processing for astronomical data
#@ Johnson Mwebaze;John McFarland;Danny Booxhorn;Edwin Valentijn
#t 2010
#c 1
#% 825663
#% 911553
#% 964141
#% 1013532
#% 1034470
#% 1065655
#% 1081399
#% 1153445
#% 1324909
#! While there has been advances in observational equipment that generate huge high quality images, the processing of these images remains a major bottleneck. We show that provenance data collected during the processing of data can be reused to perform selective processing of data and support network collaboration without clogging distribution networks. We introduce the idea of sub-image processing (SIMP) in the context of processing a subset of pixels of an image and the use of provenance data to assemble pipelines and to select processing metadata for SIMP. We describe an implementation of SIMP in Astro-WISE

#index 1482308
#* Open user schema guided evaluation of streaming RDF queries
#@ Zhihong Chong;Guilin Qi;Hu Shu;Jiajia Bao;Weiwei Ni;Aoying Zhou
#t 2010
#c 1
#% 116065
#% 237204
#% 464056
#% 824755
#% 993960
#% 1016146
#% 1127402
#% 1127431
#% 1190676
#% 1207020
#% 1217194
#% 1366460
#! Performance and scalability are two issues that are becoming increasingly pressing as RDF data model is applied to real-world applications. Because neither vertical nor flat structures of RDF storage can handle frequent schema updates and meanwhile avoid possible long-chain joins, there is no clear winner between these two typical structures. In this paper, we propose an alternative storage schema called open user schema. The open user schema consists of flat tables automatically extracted from RDF query streams. A query is divided into two parts and conquered,respectively, on the flat tables in the open user schema and on the vertical table stored in a backend storage. At the core of this divide and conquer architecture with open user schema, an efficient isomorphism decision algorithm is given to guide a query to related flat tables in the open user schema. Our proposal in essence departs from existing methods in that it can accommodate schema updates without possible long-chain joins. We implement our approach and provide empirical evaluations to demonstrate both efficiency and effectiveness of our approach in evaluating complex RDF queries.

#index 1482309
#* SUMMA: subgraph matching in massive graphs
#@ Shijie Zhang;Shirong Li;Jiong Yang
#t 2010
#c 1
#% 847113
#% 1063472
#% 1181229
#! Graphs can represent a large number of data types, e.g., online social networks, internet links, procedure dependency graphs, etc. The need for indexing massive graphs is an urgent research problem of great practical importance. The main challenge is the size. Each graph may contain at least tens of millions vertices. The working memory may not be able to store the database graph due to its large size, which increases the processing time significantly. We propose a novel index based subgraph matching scheme, namely SUMMA, for graph querying in massive graphs. We devise two novel indices which capture both local and global information of the database graph. SUMMA is further optimized by the use of a matching scheme to reduce redundant calculations and disk accesses. Last but not least, a number of synthetic datasets are used to evaluate the efficiency and scalability of our proposed method.

#index 1482310
#* Automatically weighting tags in XML collection
#@ Dexi Liu;Changxuan Wan;Lei Chen;Xiping Liu
#t 2010
#c 1
#% 1263220
#% 1263225
#! In XML retrieval, nodes with different tags play different roles in XML documents and then tags should be reflected in the relevance ranking. An automatic method is proposed in this paper to infer the weights of tags. We first investigate 15 features about tags, and then select five of them based on the correlations between these features and manual tag weights. Using these features, a tag weight assignment model, ATG, is designed. We evaluate the performance of ATG on two real data sets, IEEECS and Wikipedia from two different perspectives. One is to evaluate the quality of the model by measuring the correlation between weights generated by our model and those given by experts. The other is to test the effectiveness of the model in improving retrieval performance. Experimental results show that the tag weights generated by ATG are highly correlated with the manually assigned weights and the ATG model improves retrieval effectiveness significantly.

#index 1482311
#* Skyline query processing for uncertain data
#@ Mohamed E. Khalefa;Mohamed F. Mokbel;Justin J. Levandoski
#t 2010
#c 1
#% 654487
#% 745519
#% 806212
#% 824670
#% 993954
#% 993957
#% 1022203
#% 1044478
#% 1063485
#% 1127376
#% 1127377
#% 1181270
#% 1206645
#% 1206716
#% 1206905
#! Recently, several research efforts have addressed answering skyline queries efficiently over large datasets. However, this research lacks methods to compute these queries over uncertain data, where uncertain values are represented as a range. In this paper, we define skyline queries over continuous uncertain data, and propose a novel, efficient framework to answer these queries. Query answers are probabilistic, where each object is associated with a probability value of being a query answer. Typically, users specify a probability threshold, that each returned object must exceed, and a tolerance value that defines the allowed error margin in probability calculation to reduce the computational overhead. Our framework employs an efficient two-phase query processing algorithm.

#index 1482312
#* FD-buffer: a buffer manager for databases on flash disks
#@ Sai Tung On;Yinan Li;Bingsheng He;Ming Wu;Qiong Luo;Jianliang Xu
#t 2010
#c 1
#% 902938
#% 960238
#% 1092670
#% 1213385
#% 1222047
#% 1306948
#% 1482212
#% 1523901
#% 1823137
#% 1823467
#! We design and implement FD-Buffer, a buffer manager for database systems running on flash-based disks. Unlike magnetic disks, flash media has an inherent read-write asymmetry: writes involve expensive erase operations and as a result are usually much slower than reads. Therefore, we address this asymmetry in FD-Buffer. Specifically, we use the average I/O cost per page access as opposed to the traditional miss rate as the performance metric for a buffer. We develop a new replacement policy in which we separate clean and dirty pages into two pools. The size ratio of the two pools is automatically adapted to the read-write asymmetry and the runtime workload. We evaluate FD-Buffer with trace-driven experiments on real flash disks. Our evaluation results show that our algorithm achieves up to 33% improvement on the overall performance on commodity flash disks, in comparison with the state-of-the-art flash-aware replacement policy.

#index 1482313
#* Efficiently querying archived data using Hadoop
#@ Rajeev Gupta;Himanshu Gupta;Ullas Nambiar;Mukesh Mohania
#t 2010
#c 1
#% 440518
#% 824697
#% 1022297
#! The need to analyze structured data for various business intelligence applications such as customer churn analysis, social network analysis, telecom network monitoring etc., is well known. However, the potential size to which such data will scale in future will make solutions that revolve around data warehouses hard to scale. As data sizes grow the movement of data from the warehouse to archives becomes more frequent. Current file based archive models make the archived data unusable for any type of insight extraction. In this paper, we present an active archival solution for data warehouses that makes use of Hadoop distributed file system (HDFS) to store the data in an always available and cost-effective manner. We investigate various structured data storage schemes within HDFS and empirical evaluations show that a combination of Universal scheme model and column store is best suited for the active archival solution.

#index 1482314
#* Evaluation of top-k queries in peer-to-peer networks using threshold algorithms
#@ Ioannis Chrysakis;Constantinos Chalkidis;Dimitris Plexousakis
#t 2010
#c 1
#% 213981
#% 768521
#% 1698862
#! In p2p networks, top-k query processing can provide a lot of advantages both in time and bandwidth consumption. Several algorithms have been proposed for the evaluation of top-k queries. A large percentage of them follow the Threshold Approach. We focus on the main adaptations of threshold algorithms fulfilling the requirements of modern p2p applications. We introduce two algorithms optimized for ranking queries in p2p networks and present their characteristics. In the setting of a simulation of large p2p networks, we evaluate the algorithms' performance. Our experiments demonstrate that in some cases a threshold algorithm can improve top-k query processing, while in others it is far more costly. The results show that distributed query processing can be more effective than a simple threshold algorithm in a p2p network.

#index 1482315
#* A metamodel approach to flexible semantic web service discovery
#@ Roberto De Virgilio;Devis Bianchini
#t 2010
#c 1
#% 914650
#% 987502
#% 1058754
#% 1126564
#% 1201364
#% 1217122
#! In this paper we describe an approach for service discovery supported by semantic annotations. We propose a metamodel representation of both the WSDL documents and the associated semantic annotations. Based on this metamodel, effective service discovery is achieved by a Datalog engine implementing flexible matchmaking techniques that allow both exact and partial matches among search results. The metamodel is supported by a storage system that ensures scalability of the entire process. Finally we illustrate experiments on a public dataset of semantic service descriptions.

#index 1482316
#* On top-k social web search
#@ Peifeng Yin;Wang-Chien Lee;Ken C.K. Lee
#t 2010
#c 1
#% 387427
#% 419404
#% 1002007
#% 1019117
#% 1074116
#% 1190127
#% 1227601
#% 1292590
#! To enhance the quality of document search, recent research studies have started to exploit the social networks of users by considering social influence (SI), measurement of the affinity between a query user and the publisher of a retrieved document, in addition to the commonly used textual relevance (TR). We refer to such document search that considers social networks as social web search. In this paper, we focus on efficient top-k social web search and propose two search strategies: (i) TR-based search and (ii) SI-based search that tailor document examination orders upon TR and SI, respectively. We evaluate the proposed strategies through experimentation.

#index 1482317
#* Quantifying uncertainty in multi-dimensional cardinality estimations
#@ Andranik Khachatryan;Klemens Boehm
#t 2010
#c 1
#% 273694
#% 333947
#% 378414
#% 479967
#% 571094
#% 810017
#% 938861
#! We propose a method for predicting the cardinality distribution of a multi-dimensional query. Compared to conventional 'point-based' estimates, distribution-based estimates enable the query optimizer to predict the cost of a query plan more accurately, as we show experimentally. Our method is computationally efficient and works on top of a histogram already in place. It does not store any information additional to the histogram. Our experiments show that the quality of the predictions with the new method is high.

#index 1482318
#* Approximate membership localization (AML) for web-based join
#@ Zhixu Li;Laurianne Sitbon;Liwei Wang;Xiaofang Zhou;Xiaoyong Du
#t 2010
#c 1
#% 279755
#% 411762
#% 875066
#% 1063530
#% 1190105
#% 1292497
#! In this paper, we propose a search-based approach to join two tables in the absence of clean join attributes. Non-structured documents from the web are used to express the correlations between a given query and a reference list. To implement this approach, a major challenge we meet is how to efficiently determine the number of times and the locations of each clean reference from the reference list that is approximately mentioned in the retrieved documents. We formalize the Approximate Membership Localization (AML) problem and propose an efficient partial pruning algorithm to solve it. A study using real-word data sets demonstrates the effectiveness of our search-based approach, and the efficiency of our AML algorithm.

#index 1482319
#* An efficient data-centric storage scheme considering storage and query hot-spots in sensor networks
#@ Yonghun Park;Dongmin Seo;Jonghyeon Yun;Christopher T. Ryu;Jaesoo Yoo
#t 2010
#c 1
#% 321455
#% 443397
#% 576977
#% 731091
#% 907518
#! In wireless sensor networks, various schemes have been proposed to efficiently store and process sensed data. Among them, the Data-Centric Storage (DCS) scheme is one of the most well-known. The DCS scheme distributes data regions and stores the data in the sensor that is responsible for the region. In this paper, we propose a new DCS based scheme, called Time-Parameterized Data-Centric Storage (TPDCS), that avoids the problems of storage hot-spots and query hotspots. To decentralize the skewed data and queries, the data regions are assigned by a time dimension as well as data dimensions in our proposed scheme. Therefore, TPDCS extends the lifetime of sensor networks. It is shown through various experiments that our scheme outperforms the existing schemes.

#index 1482320
#* Formal approach and automated tool for constructing ontology from object-oriented database model
#@ Fu Zhang;Z. M. Ma;Xing Wang;Yu Wang
#t 2010
#c 1
#% 189352
#% 294600
#% 445448
#% 665856
#% 961731
#% 1035134
#% 1092529
#! Extracting domain knowledge from databases can facilitate the development of Web ontologies. In this paper, a formal approach and an automated tool for constructing ontologies from Object-oriented database models (OODMs) are developed. The approach and tool can automatically translate an OODM and its corresponding database instances into the ontology structure and ontology instances, respectively. Case studies show that the approach is feasible and the automated construction tool is efficient.

#index 1482321
#* (k,P)-anonymity: towards pattern-preserving anonymity of time-series data
#@ Xuan Shang;Ke Chen;Lidan Shou;Gang Chen;Tianlei Hu
#t 2010
#c 1
#% 312054
#% 443463
#% 662750
#% 922066
#! The challenges with privacy protection of time series are mainly due to the complex nature of the data and the queries performed on them. We study the anonymization of time series while trying to support complex queries, such as range and pattern similarity queries, on the published data. The conventional k-anonymity cannot effectively address this problem as it may suffer severe pattern loss. We propose a novel anonymization model called (k,P)-anonymity for pattern-rich time series. This model publishes both the attribute values and the patterns of time series in separate data forms. We demonstrate that our model can prevent linkage attacks on the published data while effectively support a wide variety of queries on the anonymized data. We also design an efficient algorithm for enforcing (k,P)-anonymity on time series data.

#index 1482322
#* Concurrent atomic protocols for making and changing decisions in social networks
#@ Royi Ronen;Oded Shmueli
#t 2010
#c 1
#% 9241
#% 336201
#% 415943
#% 754082
#% 1181262
#% 1206916
#% 1471598
#% 1482306
#! We study a novel data management scenario, in which social networks participants use protocols in order to manage their activities and the ever-growing data available to them in the network. In particular, we study protocols which operate on a consistent network (that we define), and transform it into another consistent state by atomically performing a set of changes. Multiple protocol instances, which work on intersecting parts of the network graphs are able to operate concurrently.

#index 1482323
#* Extending dictionary-based entity extraction to tolerate errors
#@ Guoliang Li;Dong Deng;Jianhua Feng
#t 2010
#c 1
#% 480654
#% 1063530
#% 1127425
#% 1206665
#% 1217204
#% 1328142
#! Entity extraction (also known as entity recognition) extracts entities (e.g., person names, locations, companies) from text. Approximate (dictionary-based) entity extraction is a recent trend to improve extraction quality, which extracts substrings in text that approximately match predefined entities in a given dictionary. In this paper, we study the problem of approximate entity extraction with edit-distance constraints. A straightforward method first extracts all substrings from the text and then for each substring identifies its similar entities from the dictionary using existing methods for approximate string search. However many substrings of the text have overlaps, and we have an opportunity to utilize the shared computation across the overlaps to avoid unnecessary duplicate computations. To this end, we propose a heap-based framework to efficiently extract entities. We have implemented our techniques, and the experimental results show that our method achieves high performance and outperforms existing studies significantly.

#index 1482324
#* Yet another write-optimized DBMS layer for flash-based solid state storage
#@ Hongchan Roh;Daewook Lee;Sanghyun Park
#t 2010
#c 1
#% 960238
#% 1222046
#! Flash-based Solid State Storage (flashSSS) has write-oriented problems such as low write throughput, and limited life-time. Especially, flashSSDs have a characteristic vulnerable to random-writes, due to its control logic utilizing parallelism between the flash memory chips. In this paper, we present a write-optimized layer of DBMSs to address the write-oriented problems of flashSSS in on-line transaction processing environments. The layer consists of a write-optimized buffer, a corresponding log space, and an in-memory mapping table, closely associated with a novel logging scheme called InCremental Logging (ICL). The ICL scheme enables DBMSs to reduce page-writes at the least expense of additional page-reads, while replacing random-writes into sequential-writes. Through experiments, our approach demonstrated up-to an order of magnitude performance enhancement in I/O processing time compared to the original DBMS, increasing the longevity of flashSSS by approximately a factor of two.

#index 1482325
#* Print: a provenance model to support integration processes
#@ Bruno Tomazela;Carmem S. Hara;Ricardo R. Ciferri;Cristina D.A. Ciferri
#t 2010
#c 1
#% 875015
#% 893089
#% 1036075
#% 1081399
#% 1134504
#% 1189368
#% 1292534
#% 1718170
#! In some integration applications, users are allowed to import data from heterogeneous sources, but are not allowed to update source data directly. Imported data may be inconsistent, and even when inconsistencies are detected and solved, these changes may not be propagated to the sources due to their update policies. Therefore, they continue to provide the same inconsistent data in the future until the proper authority updates them. In this paper, we propose PrInt, a model that supports user's decisions on cleaning data to be automatically reapplied in subsequent integration processes. By reproducing previous decisions, the user may focus only on new inconsistencies originated from source modified data. The reproducibility provided by PrInt is based on logging, and by incorporating data provenance in the integration process.

#index 1482326
#* OLAP-based query recommendation
#@ Carlos Garcia-Alvarado;Zhibo Chen;Carlos Ordonez
#t 2010
#c 1
#% 728105
#% 754125
#% 1063528
#% 1181246
#% 1292776
#% 1296950
#% 1297348
#% 1512993
#! Query recommendation is an invaluable tool for enabling users to speed up their searches. In this paper, we present algorithms for generating query suggestions, assuming no previous knowledge of the collection. We developed an online OLAP algorithm to generate query suggestions for the users based on the frequency of the keywords in the selected documents and the correlation between the keywords in the collection. In addition, performance and scalability experiments of these algorithms are presented as proof of their feasibility. We also present sampling as an additional approach for improving performance by using approximate results. We show valid recommendations as a result of combinations generated using the correlations between the keywords. The online OLAP algorithm is also compared with the well-known Apriori algorithm and found to be faster only when simple computations were performed in smaller collections with a few keywords. On the other hand, OLAP showed a more stable behavior between collections, and allows us to have more complex policies during the aggregation and term combinations. Additionally, sampling showed improvement in the time without a significant change on the suggested queries, and proved to be an accurate alternative with a few small samples.

#index 1482327
#* Selective data acquisition for probabilistic K-NN query
#@ Yu-Chieh Lin;De-Nian Yang;Ming-Syan Chen
#t 2010
#c 1
#% 654488
#% 1016202
#% 1103008
#% 1127377
#% 1127408
#% 1181270
#% 1189215
#% 1206640
#% 1292524
#% 1292531
#% 1408794
#! Recently, management of uncertain data draws lots of attention to consider the granularity of devices and noises in collection and delivery of data. Previous works directly model and handle uncertain data to find the required results. However, when data uncertainty is not small or limited, users are not able to obtain useful insights and thereby tend to provide more resources to improve the solution, by reducing the uncertainty of data. In light of this issue, this paper formulates a new problem of choosing a given number of uncertain data objects for acquiring their attribute values to improve the solutions of Probabilistic k-Nearest-Neighbor (k-PNN) query. We prove that solutions must be better after data acquisition, and we devise algorithms to maximize expected improvement. Our experiment results demonstrate that the probability can be significantly improved with only a small number of data acquisitions.

#index 1482328
#* Support elements in graph structured schema reintegration
#@ Xun Sun;Rachel A. Pottinger;Michael K. Lawrence
#t 2010
#c 1
#% 173990
#% 259646
#% 271710
#% 328429
#% 654457
#% 810021
#% 960233
#% 1015326
#% 1206580
#% 1409948
#! Manipulating graph-structured schemas (ontologies, models, etc.) requires the result to remain fully connected. In certain cases, e.g., calculating the difference of two schemas, support structures may be needed in the result. We describe our engine to process support structures in the context of a schema management system and describe schema reintegration experiments which validate the performance and correctness of our system

#index 1482329
#* BP-tree: an efficient index for similarity search in high-dimensional metric spaces
#@ Jurandy Almeida;Ricardo da S. Torres;Neucimar J. Leite
#t 2010
#c 1
#% 120270
#% 281750
#% 294634
#% 322309
#% 342827
#% 443482
#% 479462
#% 481460
#% 572265
#% 592155
#% 778279
#% 826098
#% 891559
#% 1134266
#! Similarity search in high-dimensional metric spaces is a key operation in many applications, such as multimedia databases, image retrieval, object recognition, and others. The high dimensionality of the data requires special index structures to facilitate the search. Most of existing indexes are constructed by partitioning the data set using distance-based criteria. However, those methods either produce disjoint partitions, but ignore the distribution properties of the data; or produce non-disjoint groups, which greatly affect the search performance. In this paper, we study the performance of a new index structure, called Ball-and-Plane tree (BP-tree), which overcomes the above disadvantages. BP-tree is constructed by recursively dividing the data set into compact clusters. Distinctive from other techniques, it integrates the advantages of both disjoint and non-disjoint paradigms in order to achieve a structure of tight and low overlapping clusters, yielding significantly improved performance. Results obtained from an extensive experimental evaluation with real-world data sets show that BP-tree consistently outperforms state-of-the-art solutions.

#index 1482330
#* Query optimization for ontology-based information integration
#@ Yingjie Li;Jeff Heflin
#t 2010
#c 1
#% 411554
#% 1048524
#% 1120996
#% 1127431
#% 1269903
#% 1288163
#% 1518002
#! In recent years, there has been an explosion of publicly available RDF and OWL data sources. In order to effectively and quickly answer queries in such an environment, we present an approach to identifying the potentially relevant Semantic Web data sources using query rewritings and a term index. We demonstrate that such an approach must carefully handle query goals that lack constants; otherwise the algorithm may identify many sources that do not contribute to eventual answers. This is because the term index only indicates if URIs are present in a document, and specific answers to a subgoal cannot be calculated until the source is physically accessed - an expensive operation given disk/network latency. We present an algorithm that, given a set of query rewritings that accounts for ontology heterogeneity, incrementally selects and processes sources in order to maintain selectivity. Once sources are selected, we use an OWL reasoner to answer queries over these sources and their corresponding ontologies. We present the results of experiments using both a synthetic data set and a subset of the real-world Billion Triple Challenge data.

#index 1482331
#* Data aspects in a relational database
#@ Curtis Dyreson;Omar U. Florez
#t 2010
#c 1
#% 168773
#% 237303
#% 327230
#% 361445
#% 378553
#% 392374
#% 447946
#% 463424
#% 480129
#% 552019
#% 654457
#% 742561
#% 745525
#% 803468
#% 875015
#% 998907
#% 1015307
#% 1016204
#! Data has cross-cutting concerns such as versioning, privacy, and reliability. In this paper we sketch support such concerns by adapting the aspect-oriented programming (AOP) paradigm to data. Our goal, shared by AOP, is to re-engineer applications to support cross-cutting concerns without directly modifying the application's data or queries. We propose modeling a cross-cutting data concern as a data aspect. A data aspect weaves metadata around an application's data and queries, imbuing them with additional semantics for constraint and query processing.

#index 1482332
#* A hierarchical approach to reachability query answering in very large graph databases
#@ Saikat K. Dey;Hasan Jamil
#t 2010
#c 1
#% 47573
#% 58365
#% 88051
#% 864462
#% 960304
#! The cost of reachability query computation using traditional algorithms such as depth first search or transitive closure has been found to be prohibitive and unacceptable in massive graphs such as biological interaction networks, or pathways. Contemporary solutions mainly take two distinct approaches - precompute reachability in the form of transitive closure (trade space for time) or use state space search (trade time for space). A middle ground among the two approaches has recently gained popularity. It precomputes part of the reachability information as a complex index so that most queries can be answered within a reasonable time. In this approach, the main cost now is creation of the index, and response generation using it as well as the space needed to materialize the structure. Most contemporary solutions favor a combination of these costs to be efficient for a class of applications. In this paper, we propose a hierarchical index based on graph segmentation to reduce index size without sacrificing query efficiency. We present experimental evidence to show that our approach can achieve significant space savings, and improve efficiency. We also show that our index need not be rebuilt for a large class of updates, a feature missing in all other contemporary systems.

#index 1482333
#* Computing the top-k maximal answers in a join of ranked lists
#@ Mirit Shalem;Yaron Kanza
#t 2010
#c 1
#% 777931
#% 1063713
#% 1733680
#! Complex search tasks that utilize information from several data sources, are answered by integrating the results of distinct basic search queries. In such integration, each basic query returns a ranked list of items, and the main task is to compute the join of these lists, returning the top-k combinations. Computing the top-k join of ranked lists has been studied extensively for the case where the answer comprises merely complete combinations. However, a join is a lossy operation, and over heterogeneous data sources some highly-ranked items, from the results of the basic queries, may not appear in any combination. Yet, such items and the partial combinations in which they appear may still be relevant answers and should not be discarded categorically. In this paper we consider a join where combinations are padded by nulls for missing items. A combination is maximal if it cannot be extended by replacing a null by an item. We present algorithms for computing the top-k maximal combinations and provide an experimental evaluation.

#index 1482334
#* PruSM: a prudent schema matching approach for web forms
#@ Thanh Hoang Nguyen;Hoa Nguyen;Juliana Freire
#t 2010
#c 1
#% 287463
#% 333990
#% 387427
#% 572314
#% 765409
#% 866989
#% 956538
#% 1015284
#% 1016163
#% 1063534
#% 1127405
#% 1127557
#% 1705177
#% 1730010
#! There has been a substantial increase in the number of Web data sources whose contents are hidden and can only be accessed through form interfaces. To leverage this data, several applications have emerged that aim to automate and simplify the access to these data sources, from hidden-Web crawlers and meta-searchers to Web information integration systems. A requirement shared by these applications is the ability to understand these forms, so that they can automatically fill them out. In this paper, we address a key problem in form understanding: how to match elements across distinct forms. Although this problem has been studied in the literature, existing approaches have important limitations. Notably, they only handle small form collections and assume that form elements are clean and normalized, often through manual pre-processing. When a large number of forms is automatically gathered, matching form schemata presents new challenges: data heterogeneity is compounded with the Web-scale and noise introduced by automated processes. We propose PruSM, a prudent schema matching strategy the determines matches for form elements in a prudent fashion, with the goal of minimizing error propagation. A experimental evaluation of PruSM using widely available data sets shows that the approach effective and able to accurately match a large number of form schemata and without requiring any manual pre-processing.

#index 1482335
#* Anonymizing data with quasi-sensitive attribute values
#@ Pu Shi;Li Xiong;Benjamin C.M. Fung
#t 2010
#c 1
#% 864406
#% 864412
#% 1022247
#% 1083709
#% 1130892
#% 1381029
#! We study the problem of anonymizing data with quasi-sensitive attributes. Quasi-sensitive attributes are not sensitive by themselves, but certain values or their combinations may be linked to external knowledge to reveal indirect sensitive information of an individual. We formalize the notion of l-diversity and t-closeness for quasi-sensitive attributes, which we call QS l-diversity and QS t-closeness, to prevent indirect sensitive attribute disclosure. We propose a two-phase anonymization algorithm that combines quasi-identifying value generalization and quasi-sensitive value suppression to achieve QS l-diversity and QS t-closeness.

#index 1482336
#* Exploiting site-level information to improve web search
#@ Andrei Broder;Evgeniy Gabrilovich;Vanja Josifovski;George Mavromatis;Donald Metzler;Jane Wang
#t 2010
#c 1
#% 268079
#% 290830
#% 411762
#% 720475
#% 766430
#% 766431
#% 818254
#% 818262
#% 879587
#% 1041735
#% 1056627
#% 1227604
#% 1292528
#! Ranking Web search results has long evolved beyond simple bag-of-words retrieval models. Modern search engines routinely employ machine learning ranking that relies on exogenous relevance signals. Yet the majority of current methods still evaluate each Web page out of context. In this work, we introduce a novel source of relevance information for Web search by evaluating each page in the context of its host Web site. For this purpose, we devise two strategies for compactly representing entire Web sites. We formalize our approach by building two indices, a traditional page index and a new site index, where each "document" represents the an entire Web site. At runtime, a query is first executed against both indices, and then the final page score for a given query is produced by combining the scores of the page and its site. Experimental results carried out on a large-scale Web search test collection from a major commercial search engine confirm the proposed approach leads to consistent and significant improvements in retrieval effectiveness.

#index 1482337
#* Exploration-exploitation tradeoff in interactive relevance feedback
#@ Maryam Karimzadehgan;ChengXiang Zhai
#t 2010
#c 1
#% 262112
#% 340899
#% 342707
#% 397161
#% 642975
#% 818209
#% 1074078
#% 1392451
#! We study an interesting optimization problem in interactive feedback that aims at optimizing the tradeoff between presenting search results with the highest immediate utility to a user (but not necessarily most useful for collecting feedback information) and presenting search results with the best potential for collecting useful feedback information (but not necessarily the most useful documents from a user's perspective). Optimizing such an exploration-exploitation tradeoff is key to the optimization of the overall utility of relevance feedback to a user in the entire session of relevance feedback. We frame this tradeoff as a problem of optimizing the diversification of search results. We propose a machine learning approach to adaptively optimizing the diversification of search results for each query so as to optimize the overall utility in an entire session. Experiment results show that the proposed learning approach can effectively optimize the exploration-exploitation tradeoff and outperforms the traditional relevance feedback approach which only does exploitation without exploration.

#index 1482338
#* Ranking social bookmarks using topic models
#@ Morgan Harvey;Ian Ruthven;Mark Carman
#t 2010
#c 1
#% 329569
#% 722904
#% 855601
#% 879587
#% 1280261
#! Ranking of resources in social tagging systems is a difficult problem due to the inherent sparsity of the data and the vocabulary problems introduced by having a completely unrestricted lexicon. In this paper we propose to use hidden topic models as a principled way of reducing the dimensionality of this data to provide more accurate resource rankings with higher recall. We first describe Latent Dirichlet Allocation (LDA) and then show how it can be used to rank resources in a social bookmarking system. We test the LDA tagging model and compare it with 3 non-topic model baselines on a large data sample obtained from the Delicious social bookmarking site. Our evaluations show that our LDA-based method significantly outperforms all of the baselines.

#index 1482339
#* A fine-grained taxonomy of tables on the web
#@ Eric Crestan;Patrick Pantel
#t 2010
#c 1
#% 348147
#% 658628
#% 755816
#% 956500
#% 1083721
#! We propose a classification taxonomy over a large crawl of HTML tables on the Web, focusing primarily on those tables that express structured knowledge. The taxonomy separates tables into two top-level classes: a) those used for layout purposes, including navigational and formatting; and b) those containing relational knowledge, including listings, attribute/value, matrix, enumeration, and form. We then propose a classification algorithm for automatically detecting a subset of the classes in our taxonomy, namely layout tables and attribute/value tables. We report on the performance of our system over a large sample of manually annotated HTML tables on the Web.

#index 1482340
#* Challenges in personalized authority flow based ranking of social media
#@ Hassan Sayyadi;John Edmonds;Vagelis Hristidis;Louiqa Raschid
#t 2010
#c 1
#% 290830
#% 348173
#% 805848
#% 1016176
#% 1026960
#% 1166508
#% 1206684
#% 1742093
#! As the social interaction of Internet users increases, so does the need to effectively rank social media. We study the challenges of personalized ranking of blog posts. Web search techniques are inadequate since social media lack many of the characteristics of the Web such as rich document content and an extensive hyperlink graph. Further, user behavior in social media has moved beyond keyword based search and must support users who follow a particular blog or theme. In this research, we extend a social media dataset to exploit the associations between authors, blog posts, and categories (topics) of the posts. We then apply personalized authority flow based ranking algorithms based on the random surfer model. We evaluate our personalization approaches through an extensive study on a range of virtual users whose preferences are defined based on intuitive criteria. Our evaluation shows that the accuracy of our personalized recommendations ranges from good to very good for a majority of users, and outperforms reasonable baseline approaches.

#index 1482341
#* A new mathematics retrieval system
#@ Shahab Kamali;Frank Wm. Tompa
#t 2010
#c 1
#% 198621
#% 322302
#% 412335
#% 556294
#% 879713
#% 1099072
#% 1655249
#% 1728015
#! The Web contains a large collection of documents, some with mathematical expressions. Because mathematical expressions are objects with complex structures and rather few distinct symbols, conventional text retrieval systems are not very successful in mathematics retrieval. The lack of a definition for similarity between mathematical expressions, and the inadequacy of searching for exact matches only, makes the problem of mathematics retrieval even harder. As a result, the few existing mathematics retrieval systems are not very helpful in addressing users' needs. We propose a powerful query language for mathematical expressions that augments exact matching with approximate matching, but in a way that is controlled by the user. We also introduce a novel indexing scheme that scales well for large collections of expressions. Based on this indexing scheme, an efficient lookup algorithm is proposed.

#index 1482342
#* Explore click models for search ranking
#@ Dong Wang;Weizhu Chen;Gang Wang;Yuchen Zhang;Botao Hu
#t 2010
#c 1
#% 309095
#% 840846
#% 1035578
#% 1074092
#% 1190055
#% 1190056
#% 1355048
#! Recent advances in click model have positioned it as an effective approach to estimate document relevance based on user behavior in web search. Yet, few works have been conducted to explore the use of click model to help web search ranking. In this paper, we focus on learning a ranking function by taking the results from a click model into account. Thus, besides the editorial relevance data arising from the explicit manually labeled search result by experts, we also have the estimated relevance data that is automatically inferred from click models based on user search behavior. We carry out extensive experiments on large-scale commercial datasets and demonstrate the effectiveness of the proposed methods.

#index 1482343
#* Generating advertising keywords from video content
#@ Michael J. Welch;Junghoo Cho;Walter Chang
#t 2010
#c 1
#% 46803
#% 722904
#% 818265
#% 838532
#% 869484
#% 956505
#% 1289633
#% 1677673
#! With the proliferation of online distribution methods for videos, content owners require easier and more effective methods for monetization through advertising. Matching advertisements with related content has a significant impact on the effectiveness of the ads, but current methods for selecting relevant advertising keywords for videos are limited by reliance on manually supplied metadata. In this paper we study the feasibility of using text available from video content to obtain high quality keywords suitable for matching advertisements. In particular, we tap into three sources of text for ad keyword generation: production scripts, closed captioning tracks, and speech-to-text transcripts. We address several challenges associated with using such data. To overcome the high error rates prevalent in automatic speech recognition and the lack of an explicit structure to provide hints about which keywords are most relevant, we use statistical and generative methods to identify dominant terms in the source text. To overcome the sparsity of the data and resulting vocabulary mismatches between source text and the advertiser's chosen keywords, these terms are then expanded into a set of related keywords using related term mining methods. Our evaluations present a comprehensive analysis of the relative performance for these methods across a range of videos, including professionally produced films and popular videos from YouTube.

#index 1482344
#* Web page classification on child suitability
#@ Carsten Eickhoff;Pavel Serdyukov;Arjen P. de Vries
#t 2010
#c 1
#% 308363
#% 344923
#% 805873
#% 807299
#% 829975
#% 939396
#% 987245
#% 1014680
#% 1194330
#% 1227578
#% 1260690
#% 1264737
#% 1709423
#! Children spend significant amounts of time on the Internet. Recent studies showed, that during these periods they are often not under adult supervision. This work presents an automatic approach to identifying suitable web pages for children based on topical and non-topical web page aspects. We discuss the characteristics of children's web sites with respect to recent findings in children's psychology and cognitive sciences. We finally evaluate our approach in a large-scale user study, finding, that it compares favourably to state of the art methods while approximating human performance.

#index 1482345
#* Rough sets based reasoning and pattern mining for a two-stage information filtering system
#@ Xujuan Zhou;Yuefeng Li;Peter David Bruza;Yue Xu;Raymond Y.K. Lau
#t 2010
#c 1
#% 248791
#% 387427
#% 584888
#% 863392
#% 879595
#% 915323
#% 987227
#% 1026882
#% 1130911
#! This paper presents a novel two-stage information filtering model which combines the merits of term-based and pattern- based approaches to effectively filter sheer volume of infor- mation. In particular, the first filtering stage is supported by a novel rough analysis model which efficiently removes a large number of irrelevant documents, thereby addressing the overload problem. The second filtering stage is empow- ered by a semantically rich pattern taxonomy mining model which effectively fetches incoming documents according to the specific information needs of a user, thereby addressing the mismatch problem. The experiments have been conducted to compare the proposed two-stage filtering (T-SM) model with other possible "term-based + pattern-based" or "term-based + term-based" IF models. The results based on the RCV1 corpus show that the T-SM model significantly outperforms other types of "two-stage" IF models.

#index 1482346
#* A late fusion approach to cross-lingual document re-ranking
#@ Dong Zhou;Séamus Lawless;Jinming Min;Vincent Wade
#t 2010
#c 1
#% 27049
#% 1051798
#% 1305532
#% 1338692
#% 1415756
#% 1544178
#! The field of information retrieval still strives to develop models which allow semantic information to be integrated in the ranking process to improve performance in comparison to standard bag-of-words based models. Cross-lingual information retrieval is an example of where such a model is required, as content or concepts often need to be matched across languages. To overcome this problem, a conceptual model has been adopted in ranking an entire corpus which normally exploits latent/implicit features of the text. One of the drawbacks of this model is that the computational cost is significant and often intractable in modern test collections. Therefore, approaches utilizing concept-based models for re-ranking initial retrieval results have attracted a considerable amount of study, in particular the latent concept model. However, fitting such a model to a smaller collection is less meaningful than fitting it into the whole corpus. This paper proposes a late fusion method which incorporates scores generated by using external knowledge to enhance the space produced by the latent concept method. This method is further demonstrated to be suitable for multilingual re-ranking purposes. To illustrate the effectiveness of the proposed method, experiments were conducted over test collections across three languages. The results demonstrate that the method can comfortably achieve improvements in retrieval performance over several re-ranking methods.

#index 1482347
#* Learning to generate summary as structured output
#@ Hiroya Takamura;Manabu Okumura
#t 2010
#c 1
#% 297675
#% 829043
#% 939705
#% 1074025
#% 1260752
#% 1264133
#% 1275040
#! We propose to use a structured output learning for summary generation based on the maximum coverage problem. Our method learns a function that outputs the benefit of each conceptual unit in the document cluster for this summarization model. In the training, we iteratively run a greedy algorithm that accepts items (sentences) with different costs (length) in order to generate a summary within the given maximum length limit. We empirically show that the structured output learning works well for this task and also examine its behavior in several dierent settings.

#index 1482348
#* Group ranking with application to image retrieval
#@ Ou Wu;Weiming Hu;Bing Li
#t 2010
#c 1
#% 232703
#% 330769
#% 654466
#% 1130994
#% 1190089
#% 1190132
#! Many existing ranking-related information processing applications can be summarized into one theoretical problem called group ranking (GR). A simple average-ranking approach is usually applied to GR. Although the approach seems reasonable, no theoretical analysis about its intrinsic mechanism has been presented, increasing the difficulty of evaluating the ranking results. This study provides a formal analysis for GR. We first construct an objective function for the GR problem, and discover that each GR problem can be transformed into a rank aggregation problem whose objective function is proved to be equal to the objective function of GR. As a consequence, the average-ranking approach can be explained by two well-known rank aggregation techniques. We incorporate two other effective rank aggregation methods into the GR problem and obtain two new GR algorithms. We apply the GR algorithms into image retrieval to diversify the image search results returned by search engines. Experimental results show the effectiveness of the proposed GR algorithms.

#index 1482349
#* Unifying explicit and implicit feedback for collaborative filtering
#@ Nathan N. Liu;Evan W. Xiang;Min Zhao;Qiang Yang
#t 2010
#c 1
#% 528156
#% 734592
#% 840924
#% 1083696
#% 1102242
#% 1116993
#% 1176909
#% 1214688
#% 1260273
#% 1291600
#! Most collaborative filtering algorithms are based on certain statistical models of user interests built from either explicit feedback (eg: ratings, votes) or implicit feedback (eg: clicks, purchases). Explicit feedbacks are more precise but more difficult to collect from users while implicit feedbacks are much easier to collect though less accurate in reflecting user preferences. In the existing literature, separate models have been developed for either of these two forms of user feedbacks due to their heterogeneous representation. However in most real world recommended systems both explicit and implicit user feedback are abundant and could potentially complement each other. It is desirable to be able to unify these two heterogeneous forms of user feedback in order to generate more accurate recommendations. In this work, we developed matrix factorization models that can be trained from explicit and implicit feedback simultaneously. Experimental results of multiple datasets showed that our algorithm could effectively combine these two forms of heterogeneous user feedback to improve recommendation quality.

#index 1482350
#* Directly optimizing evaluation measures in learning to rank based on the clonal selection algorithm
#@ Qiang He;Jun Ma;Shuaiqiang Wang
#t 2010
#c 1
#% 734915
#% 983820
#% 987241
#% 987242
#% 1035577
#% 1349072
#% 1456844
#! One fundamental issue of learning to rank is the choice of loss function to be optimized. Although the evaluation measures used in Information Retrieval (IR) are ideal ones, in many cases they can't be used directly because they do not satisfy the smooth property needed in conventional machine learning algorithms. In this paper a new method named RankCSA is proposed, which tries to use IR evaluation measure directly. It employs the clonal selection algorithm to learn an effective ranking function by combining various evidences in IR. Experimental results on the LETOR benchmarh datasets demonstrate that RankCSA outperforms the baseline methods in terms of P@n, MAP and NDCG@n.

#index 1482351
#* Query model refinement using word graphs
#@ Yunping Huang;Le Sun;Jian-Yun Nie
#t 2010
#c 1
#% 218978
#% 262096
#% 268079
#% 340899
#% 342707
#% 818240
#% 838530
#% 1074081
#% 1074127
#! Pseudo relevance feedback method is an effective method for query model refinement. Most existing pseudo relevance feedback methods only take into consideration the term distribution of the feedback documents, but omit the term's context information. This paper presents a graph-based method to improve query models, in which a word graph is constructed to encode terms and their co-occurrence dependencies within the feedback documents. Using a random walk, the weight of each term in the graph can be determined in a context-dependent manner, i.e. the weight of a term is strongly dependent on the weights of the connected context terms. Our experimental results on four TREC collections show that our proposed approach is more effective than the existing state-of-the-art approaches.

#index 1482352
#* Building recommendation systems using peer-to-peer shared content
#@ Yuval Shavitt;Ela Weinsberg;Udi Weinsberg
#t 2010
#c 1
#% 220706
#% 663686
#% 1475113
#! Peer-to-Peer (p2p) networks are used for sharing content by millions of users. Often, meta-data used for searching is missing or wrong, making it difficult for users to find content. Moreover, searching for new content is almost impossible. Recommender systems are unable to handle p2p data due to inherent difficulties, such as implicit ranking, noise and the extreme dimensions and sparseness of the network. This paper introduces methods for using p2p data in recommender systems. We present a method for creating content-similarity graph while overcoming inherent noise. Using this graph, a clustering method is presented for detecting proximity between files using the "wisdom-of-the-crowds". Evaluation using songs shared by over 1.2 million users in the Gnutella network, shows that these techniques can leverage p2p data for building efficient recommender systems.

#index 1482353
#* Threshold behavior of incentives in social networks
#@ Nagaraj Kota;Y. Narahari
#t 2010
#c 1
#% 643087
#% 754144
#% 836506
#% 838067
#% 956578
#% 963336
#% 1002007
#% 1024728
#% 1300556
#! The advent of large scale online social networks has resulted in a spurt of studies on the user participation in the networks. We consider a query incentive model on social networks, where user's queries are answered through her friendship network and there are `rewards' or `incentives' in the system to answer the queries utilizing ones community. We model the friendship network as a random graph with power-law degree distribution, and show that the reward function exhibits a theoretic threshold behavior on the scaling exponent α, a network parameter. Specifically, there exists a threshold on α above which the reward is exponential in the average path length in the network and below the threshold, the reward is proportional to the average path length. We demonstrate this finding on simulated power-law networks and real world data gathered from online social media such as Flickr, Digg, YouTube and Yahoo! Answers.

#index 1482354
#* Image retrieval at memory's edge: known image search based on user-drawn sketches
#@ Michael Springmann;Ihab Al Kabary;Heiko Schuldt
#t 2010
#c 1
#% 982791
#% 1132472
#% 1375828
#% 1784776
#! With the increasingly growing size of digital image collections, known image search is gaining more and more importance. Especially in collections where individual objects are not tagged with metadata describing their content, content-based image retrieval (CBIR) is a promising approach, but usually suffers from the unavailability of query images that are good enough to express the user's information need. In this paper, we present the QbS system that provides CBIR based on user-drawn sketches. The QbS system combines angular radial partitioning for the extraction of features in the user-provided sketch, taking into account the spatial distribution of edges, and the image distortion model. This combination offers several highly relevant invariances that allow the query sketch to slightly deviate from the searched image in terms of rotation, translation, relative size, and/or unknown objects in the background. To illustrate the benefits of the approach, we present search results from the evaluation of the QbS system on the basis of the MIRFLICKR collection with 25,000 objects and compare the retrieval results of pure metadata-driven approaches, pure content-based retrieval using different sketches, and combinations thereof.

#index 1482355
#* Utilizing re-finding for personalized information retrieval
#@ Sarah K. Tyler;Jian Wang;Yi Zhang
#t 2010
#c 1
#% 194299
#% 233808
#% 247268
#% 411762
#% 818259
#% 859488
#% 946521
#% 954970
#% 987211
#% 1047436
#% 1355035
#! Individuals often use search engines to return to web pages they have previously visited. This behaviour, called re-finding, accounts for about 38% of all queries. While researchers have shown how re-finding differs from traditionally studied new-findings, research on how to predict and utilize re-finding is limited. In this paper we explore re-finding for personalized search. We compared three machine learning algorithms (decision trees, Bayesian multinomial regression and support vector machines) to identify re-findings. We then propose several re-ranking methods to utilize the prediction, including promoting predicted re-finding URLs and combining re-finding prediction with relevance estimation. The experimental results demonstrate that using re-finding predictions can improve retrieval performance for personalized search.

#index 1482356
#* User behavior driven ranking without editorial judgments
#@ Taesup Moon;Georges Dupret;Shihao Ji;Ciya Liao;Zhaohui Zheng
#t 2010
#c 1
#% 818221
#% 987228
#% 1190055
#% 1227581
#% 1355034
#! We explore the potential of using users click-through logs where no editorial judgment is available to improve the ranking function of a vertical search engine. We base our analysis on the Cumulate Relevance Model, a user behavior model recently proposed as a way to extract relevance signal from click-through logs. We propose a novel way of directly learning the ranking function, effectively by-passing the need to have explicit editorial relevance label for each query-document pair. This approach potentially adjusts more closely the ranking function to a variety of user behaviors both at the individual and at the aggregate levels. We investigate two ways of using behavioral model; First, we consider the parametric approach where we learn the estimates of document relevance and use them as targets for the machine learned ranking schemes. In the second, functional approach, we learn a function that maximizes the behavioral model likelihood, effectively by-passing the need to estimate a substitute for document labels. Experiments using user session data collected from a commercial vertical search engine demonstrate the potential of our approach. While in terms of DCG, the editorial model out-perform the behavioral one, online experiments show that the behavioral model is on par --if not superior-- to the editorial model. To our knowledge, this is the first report in the Literature of a competitive behavioral model in a commercial setting

#index 1482357
#* Alignment of short length parallel corpora with an application to web search
#@ Jitendra Ajmera;Hema Swetha Koppula;Krishna P. Leela;Shibnath Mukherjee;Mehul Parsana
#t 2010
#c 1
#% 411762
#% 464434
#% 869501
#% 1130855
#% 1190106
#% 1227648
#! With evolving Web, short length parallel corpora is becoming very common and some of these include user queries, web snippets etc. This paper concerns situations where short length parallel corpora has to be analyzed in order to find meaningful unit-alignment. This is similar to dealing with parallel corpora where a sentence level alignment of translations is required, but differs in that the alignment is to be inferred at unit (word or phrase) level. A Conditional Random Field (CRF) based approach is proposed to discover this unit alignment. Given pairs of semantically or syntactically similar entities, the problem is formulated as that of mutual segmentation and sequence alignment problem. The mutual segmentation refers to the process of segmenting the first entity based on units (or labels) in the second entity and vice-versa. The process of optimizing this mutual segmentation also results in optimal unit alignment. Since our training data is not segmented and unit-aligned, we modify the CRF objective function to accommodate unsupervised data and iterative learning. We have applied this framework to Web Search domain and specifically for query reformulation task. Finally, our experiments suggest that the proposed approach indeed results in meaningful alternatives of the original query.

#index 1482358
#* A feature-word-topic model for image annotation
#@ Cam-Tu Nguyen;Natsuda Kaothanthong;Xuan-Hieu Phan;Takeshi Tokuyama
#t 2010
#c 1
#% 329569
#% 457912
#% 642990
#% 780862
#% 839975
#% 975105
#% 1013668
#% 1071121
#% 1224331
#! Image annotation is to automatically associate semantic labels with images in order to obtain a more convenient way for indexing and searching images on the Web. This paper proposes a novel method for image annotation based on feature-word and word-topic distributions. The introduction of topics enables us to efficiently take word associations, such as {ocean, fish, coral}, into image annotation. Feature-word distributions are utilized to define weights in computation of topic distributions for annotation. By doing so, topic models in text mining can be applied directly in our method. Experiments show that our method is able to obtain promising improvements over the state-of-the-art method - Supervised Multiclass Labeling (SML)

#index 1482359
#* Weighting common syntactic structures for natural language based information retrieval
#@ Chang Liu;Hui Wang;Sally McClean;Epaminondas Kapetanios;Denis Carroll
#t 2010
#c 1
#% 324129
#% 1074247
#% 1077150
#% 1665151
#! Natural Language Processing (NLP) techniques are believed to hold the potential to assist "bag-of-words" Information Retrieval (IR) in terms of retrieval accuracy. In this paper, we report a natural language based IR approach where the common syntactic structures between documents and the query is regarded to as a query-dependent feature for documents. Specifically, a "structural weight" is proposed for query terms, which can be seen as a weight to model the degree of term's involvement in the common syntactic structures. This structural weight is used together with the TF-IDF weighting scheme, which results in a new ranking function. The accumulation of this structural weight of all the query terms in the new ranking function will be seen as a measure of how much a document and a query share the common syntactic structures. The experimental results show that by using this ranking function, significant improvements in the retrieval performance are achieved.

#index 1482360
#* Ranking with auxiliary data
#@ Bo Long;Yi Chang;Srinivas Vadrevu;Shuang Hong Yang;Zhaohui Zheng
#t 2010
#c 1
#! Learning to rank arises in many information retrieval applications, ranging from Web search engine, online advertising to recommendation system. In learning to rank, the performance of a ranking function heavily depends on the number of labeled examples in the training set; on the other hand, obtaining labeled examples for training data is very expensive and time-consuming. This presents a great need for making use of available auxiliary data, i.e., the within-domain unlabeled data and the out-of-domain labeled data. In this paper, we propose a general framework for ranking with auxiliary data, which is applicable to various ranking applications. Under this framework, we derive a generic ranking algorithm to effectively make use of both the within-domain unlabeled data and the out-of-domain labeled data. The proposed algorithm iteratively learns ranking functions for target domain and source domains and enforces their consensus on the unlabeled data in the target domain.

#index 1482361
#* Using various term dependencies according to their utilities
#@ Lixin Shi;Jian-Yun Nie
#t 2010
#c 1
#% 35937
#% 86371
#% 109190
#% 134878
#% 219047
#% 287253
#% 397205
#% 413593
#% 766414
#% 766428
#% 818239
#% 818262
#% 976952
#% 987229
#% 1227613
#% 1227614
#% 1355019
#! In this paper, we propose a model to integrate term dependencies. Different from previous studies, each pair of terms is assigned a different weight of dependency according to their utility to IR. The experiments show that our model can significantly outperform the previous dependency models using fixed weights.

#index 1482362
#* Modeling reformulation using passage analysis
#@ Xiaobing Xue;W. Bruce Croft;David A. Smith
#t 2010
#c 1
#% 262096
#% 298183
#% 340901
#% 340948
#% 413592
#% 818262
#% 869501
#% 987231
#% 987272
#% 1055706
#% 1074081
#% 1074098
#% 1130855
#% 1227747
#% 1355020
#% 1715627
#! Query reformulation modifies the original query with the aim of better matching the vocabulary of the relevant documents, and consequently improving ranking effectiveness. Previous techniques typically generate words and phrases related to the original query, but do not consider how these words and phrases would fit together in new queries. In this paper, we focus on an implementation of an approach that models reformulation as a distribution of queries, where each query is a variation of the original query. This approach considers a query as a basic unit and can capture important dependencies between words and phrases in the query. The implementation discussed here is based on passage analysis of the target corpus. Experiments on the TREC collection show that the proposed model for query reformulation significantly outperforms state-of-the-art methods.

#index 1482363
#* Online learning for recency search ranking using real-time user feedback
#@ Taesup Moon;Lihong Li;Wei Chu;Ciya Liao;Zhaohui Zheng;Yi Chang
#t 2010
#c 1
#% 989628
#% 1190055
#% 1211840
#% 1214666
#% 1318590
#% 1355017
#% 1355034
#% 1399999
#! Traditional machine-learned ranking algorithms for web search are trained in batch mode, which assume static relevance of documents for a given query. Although such a batch-learning framework has been tremendously successful in commercial search engines, in scenarios where relevance of documents to a query changes over time, such as ranking recent documents for a breaking news query, the batch-learned ranking functions do have limitations. Users' real-time click feedback becomes a better and timely proxy for the varying relevance of documents rather than the editorial judgments provided by human editors. In this paper, we propose an online learning algorithm that can quickly learn the best re-ranking of the top portion of the original ranked list based on real-time users' click feedback. In order to devise our algorithm and evaluate it accurately, we collected exploration bucket data that removes positional biases on clicks on the documents for recency-classified queries. Our initial experimental result shows that our scheme is more capable of quickly adjusting the ranking to track the varying relevance of documents reflected in the click feedback, compared to batch-trained ranking functions.

#index 1482364
#* Expert identification in community question answering: exploring question selection bias
#@ Aditya Pal;Joseph A. Konstan
#t 2010
#c 1
#% 956516
#% 1019165
#% 1083720
#% 1130900
#! Community Question Answering (CQA) services enables users to ask and answer questions. In these communities, there are typically a small number of experts amongst the large population of users. We study which questions a user select for answering and show that experts prefer answering questions where they have a higher chance of making a valuable contribution. We term this preferential selection as question selection bias and propose a mathematical model to estimate it. Our results show that using Gaussian classification models we can effectively distinguish experts from ordinary users over their selection biases. In order to estimate these biases, only a small amount of data per user is required, which makes an early identification of expertise a possibility. Further, our study of bias evolution reveals that they do not show significant changes over time indicating that they emanates from the intrinsic characteristics of users.

#index 1482365
#* On the relationship between novelty and popularity of user-generated content
#@ David Carmel;Haggai Roitman;Elad Yom-Tov
#t 2010
#c 1
#% 748017
#% 786842
#% 939867
#% 955229
#% 983833
#% 1035587
#% 1190088
#% 1252937
#% 1266225
#% 1281981
#% 1292698
#% 1404878
#% 1815525
#! This work deals with the task of predicting the popularity of user-generated content. We demonstrate how the novelty of newly published content plays an important role in affecting its popularity. We study three dimensions of novelty: contemporaneous novelty, self novelty, and discussion novelty. We demonstrate the contribution of the new novelty measures to estimating blog-post popularity by predicting the number of comments expected for a fresh post. We further demonstrate how novelty based measures can be utilized for predicting the citation volume of academic papers.

#index 1482366
#* Focused crawling using navigational rank
#@ Shicong Feng;Li Zhang;Yuhong Xiong;Conglei Yao
#t 2010
#c 1
#% 281251
#% 348173
#% 466250
#% 480309
#% 956551
#! The goal of focused crawling is to use limited resources to effectively discover web pages related to a specific topic rather than downloading all accessible web documents. The major challenge in focused crawling is how to effectively determine each hyperlink's capability of leading to target pages. To compute this capability, we present a novel approach, called Navigational Rank (NR). In general, NR is a kind of two-step and two-direction credit propagation approach. Compared to existing methods, NR mainly has three advantages. First, NR is dynamically updated during the crawling progress, which can adapt to different website structures very well. Second, when the crawling seed is far away from the target pages, and the target pages only constitute a small portion of the whole website, NR shows a significant performance advantage. Third, NR computes each link's capability of leading to target pages by considering both the target and non-target pages it leads to. This global knowledge causes a better performance than only using target pages. We have performed extensive experiments for performance evaluation of the proposed approach using two groups of large-scale, real-world datasets from two different domains. The experimental results show that our approach is domain-independent and significantly outperforms the state-of-arts.

#index 1482367
#* TAER: time-aware entity retrieval-exploiting the past to find relevant entities in news articles
#@ Gianluca Demartini;Malik Muhammad Saad Missen;Roi Blanco;Hugo Zaragoza
#t 2010
#c 1
#% 879570
#% 1019189
#% 1052710
#% 1415743
#% 1450969
#% 1489451
#! Retrieving entities instead of just documents has become an important task for search engines. In this paper we study entity retrieval for news applications, and in particular the importance of the news trail history (i.e., past related articles) in determining the relevant entities in current articles. This is an important problem in applications that display retrieved entities to the user, together with the news article. We analyze and discuss some statistics about entities in news trails, unveiling some unknown findings such as the persistence of relevance over time. We focus on the task of query dependent entity retrieval over time. For this task we evaluate several features, and show that their combinations significantly improves performance.

#index 1482368
#* Demographic information flows
#@ Ingmar Weber;Alejandro Jaimes
#t 2010
#c 1
#% 754107
#% 765412
#% 766447
#% 832271
#% 878624
#% 1019163
#% 1214671
#% 1355051
#% 1450894
#% 1562244
#! In advertising and content relevancy prediction it is important to understand whether, over time, information that reaches one demographic group spreads to others. In this paper we analyze the query log of a large U.S. web search engine to determine whether the same queries are performed by different demographic groups at different times, particularly when there are query bursts. We obtain aggregate demographic features from user-provided registration information (gender, birth year, ZIP code), U.S. census data, and election results. Given certain queries, we examine trends (from high to low and vice versa) and changes in the statistical spread of the demographic features of users that issue the queries over time periods that include query bursts. Our analysis shows that for certain types of queries (movies and news) distinct demographic groups perform searches at different times, suggesting that information related to such queries flows between them. Queries of movie titles, for instance, tend to be issued first by young and then by older users, where a sudden jump in age occurs upon the movie's release. To the best of our knowledge, this is the first time this problem has been studied using search query logs.

#index 1482369
#* Detecting periodic changes in search intentions in a search engine
#@ Masaya Murata;Hiroyuki Toda;Yumiko Matsuura;Ryoji Kataoka;Takayoshi Mochizuki
#t 2010
#c 1
#% 590523
#% 765412
#% 766408
#% 805839
#% 869517
#% 1348355
#% 1355017
#! Information needs expressed by using the same query for a search engine might be totally different, whether on week days or weekends, or during the day or at night. For queries having no temporal changes in search intentions, the same search results ranking may be returned regardless of the time, but for those with temporal changes the ranking must be suitably altered depending on the time of input. To achieve time-dependent search results rankings, we focus on the temporal changes in the search intentions. We present the results obtained by analyzing a commercial search engine log and propose a method of detecting queries showing periodic changes in the search intentions.

#index 1482370
#* Recommendation based on object typicality
#@ Yi Cai;Ho-fung Leung;Qing Li;Jie Tang;Juanzi Li
#t 2010
#c 1
#% 220707
#% 220709
#% 220711
#% 234992
#% 280447
#% 280852
#% 301259
#% 330687
#% 578684
#% 813966
#% 818216
#% 879627
#% 987197
#% 1083734
#% 1145231
#% 1305617
#% 1650298
#! Current recommendation methods are mainly classified into content-based, collaborative filtering and hybrid methods. These methods are based on similarity measurements among items or users. In this paper, we investigate recommendation systems from a new perspective based on object typicality and propose a novel typicality-based recommendation approach. Experiments show that our method outperforms compared methods on recommendation quality.

#index 1482371
#* Selecting keywords for content based recommendation
#@ Christian Wartena;Wout Slakhorst;Martin Wibbels
#t 2010
#c 1
#% 304425
#% 428265
#% 722904
#% 722935
#% 817576
#% 1055785
#% 1396094
#% 1507035
#! The continued growth of online content makes personalized recommendation an increasingly important tool for media consumption. While collaborative filtering techniques have shown to be very successful in stable collections, content based approaches are necessary for recommending new items. Content based recommendation uses the similarity between new items and consumed items to predict whether a new item is interesting for the user. The similarity is computed by comparing the content or the meta-data of the items. In this paper we consider recommendation of TV-broadcasts for which meta-data and synopses are available. We thereby concentrate on the new item problem. We investigate the value of different types of meta-data provided by the broadcaster or extracted from synopsis. We show that extracted keywords are better suited for recommendation than manually assigned keywords. Furthermore we show that the number of keywords used is of great importance. Using a rather small number of keywords to present an item yields the best results for recommendation.

#index 1482372
#* Structural annotation of search queries using pseudo-relevance feedback
#@ Michael Bendersky;W. Bruce Croft;David A. Smith
#t 2010
#c 1
#% 279755
#% 340901
#% 643057
#% 869501
#% 987221
#% 1019130
#% 1035584
#% 1055706
#% 1074098
#% 1074170
#% 1173692
#% 1227610
#% 1264824
#% 1330534
#% 1338596
#% 1483085
#! Marking up queries with annotations such as part-of-speech tags, capitalization, and segmentation, is an important part of many approaches to query processing and understanding. Due to their brevity and idiosyncratic structure, search queries pose a challenge to existing annotation tools that are commonly trained on full-length documents. To address this challenge, we view the query as an explicit representation of a latent information need, which allows us to use pseudo-relevance feedback, and to leverage additional information from the document corpus, in order to improve the quality of query annotation.

#index 1482373
#* Search as if you were in your home town: geographic search by regional context and dynamic feature-space selection
#@ Makoto P. Kato;Hiroaki Ohshima;Satoshi Oyama;Katsumi Tanaka
#t 2010
#c 1
#% 443261
#% 479788
#% 993935
#! We propose a query-by-example geographic object search method for users that do not know well about the place they are in. Geographic objects, such as restaurants, are often retrieved using an attribute-based or keyword query. These queries, however, are difficult to use for users that have little knowledge on the place where they want to search. The proposed query-by-example method allows users to query by selecting examples in familiar places for retrieving objects in unfamiliar places. One of the challenges is to predict an effective distance metric, which varies for individuals. Another challenge is to calculate the distance between objects in heterogeneous domains considering the feature gap between them, for example, restaurants in Japan and China. Our proposed method is used to robustly estimate the distance metric by amplifying the difference between selected and non-selected examples. By using the distance metric, each object in a familiar domain is evenly assigned to one in an unfamiliar domain to eliminate the difference between those domains. We developed a restaurant search using data obtained from a Japanese restaurant Web guide to evaluate our method.

#index 1482374
#* Topic aspect analysis for multi-document summarization
#@ Chao Shen;Dingding Wang;Tao Li
#t 2010
#c 1
#% 340884
#% 787502
#% 816173
#% 939539
#% 1074074
#% 1074089
#% 1261539
#% 1270689
#% 1275220
#% 1292659
#% 1354495
#% 1362503
#% 1728504
#! Query-based multi-document summarization aims to create a short summary given a collection of documents and a query. Most of the existing methods treat the query as one single sentence and rank the sentences in the documents based on their similarities with the query sentence. However, these methods lack of intensive analysis on the given query which typically consist of several topic aspects. In this paper, we propose a topic aspect extraction method to discover the aspect words and sentences contained in the query narrative texts and the input documents, and then incorporate these aspect words and sentences into a cross propagation model based on the sentence-term bipartite graph for document summarization. Experiments on DUC benchmark data show the effectiveness of our proposed approach on the topic-driven document summarization task.

#index 1482375
#* Finding unusual review patterns using unexpected rules
#@ Nitin Jindal;Bing Liu;Ee-Peng Lim
#t 2010
#c 1
#% 136350
#% 869469
#% 869471
#% 907490
#% 1035590
#% 1482272
#! In recent years, opinion mining attracted a great deal of research attention. However, limited work has been done on detecting opinion spam (or fake reviews). The problem is analogous to spam in Web search [1, 9 11]. However, review spam is harder to detect because it is very hard, if not impossible, to recognize fake reviews by manually reading them [2]. This paper deals with a restricted problem, i.e., identifying unusual review patterns which can represent suspicious behaviors of reviewers. We formulate the problem as finding unexpected rules. The technique is domain independent. Using the technique, we analyzed an Amazon.com review dataset and found many unexpected rules and rule groups which indicate spam activities.

#index 1482376
#* Visual-semantic graphs: using queries to reduce the semantic gap in web image retrieval
#@ Barbara Poblete;Benjamin Bustos;Marcelo Mendoza;Juan Manuel Barrios
#t 2010
#c 1
#% 251485
#% 411762
#% 754089
#% 780873
#% 818218
#% 839851
#% 857113
#% 916102
#% 1130921
#% 1279874
#% 1857842
#! We explore the application of a graph representation to model similarity relationships that exist among images found on the Web. The resulting similarity-induced graph allows us to model in a unified way different types of content-based similarities, as well as semantic relationships. Content-based similarities include different image descriptors, and semantic similarities can include relevance user feedback from search engines. The goal of our representation is to provide an experimental framework for combining apparently unrelated metrics into a unique graph structure, which allows us to enhance the results of Web image retrieval. We evaluate our approach by re-ranking Web image search results.

#index 1482377
#* Novel local features with hybrid sampling technique for image retrieval
#@ Leszek Kaliciak;Dawei Song;Nirmalie Wiratunga;Jeff Pan
#t 2010
#c 1
#% 635689
#% 824956
#% 996168
#% 1106212
#% 1132472
#% 1410878
#% 1667698
#% 1697432
#! In image retrieval, most existing approaches that incorporate local features produce high dimensional vectors, which lead to a high computational and data storage cost. Moreover, when it comes to the retrieval of generic real-life images, randomly generated patches are often more discriminant than the ones produced by corner/blob detectors. In order to tackle these problems, we propose a novel method incorporating local features with a hybrid sampling (a combination of detector-based and random sampling). We take three large data collections for the evaluation: MIRFlickr, ImageCLEF, and a collection from British National Geological Survey. The overall performance of the proposed approach is better than the performance of global features and comparable with the current state-of-the-art methods in content-based image retrieval. One of the advantages of our method when compared with others is its easy implementation and low computational cost. Another is that hybrid sampling can improve the performance of other methods based on the ``bag of visual words'' approach.

#index 1482378
#* Expected browsing utility for web search evaluation
#@ Emine Yilmaz;Milad Shokouhi;Nick Craswell;Stephen Robertson
#t 2010
#c 1
#% 411762
#% 1095876
#% 1166517
#% 1190055
#% 1190056
#% 1227640
#% 1292528
#% 1355034
#% 1366523
#! Most information retrieval evaluation metrics are designed to measure the satisfaction of the user given the results returned by a search engine. In order to evaluate user satisfaction, most of these metrics have underlying user models, which aim at modeling how users interact with search engine results. Hence, the quality of an evaluation metric is a direct function of the quality of its underlying user model. This paper proposes EBU, a new evaluation metric that uses a sophisticated user model tuned by observations over many thousands of real search sessions. We compare EBU with a number of state of the art evaluation metrics and show that it is more correlated with real user behavior captured by clicks.

#index 1482379
#* Community-based topic modeling for social tagging
#@ Daifeng Li;Bing He;Ying Ding;Jie Tang;Cassidy Sugimoto;Zheng Qin;Erjia Yan;Juanzi Li;Tianxi Dong
#t 2010
#c 1
#% 722904
#% 788094
#% 1083734
#% 1176930
#% 1399996
#! Exploring community is fundamental for uncovering the connections between structure and function of complex networks and for practical applications in many disciplines such as biology and sociology. In this paper, we propose a TTR-LDA-Community model which combines the Latent Dirichlet Allocation model (LDA) and the Girvan-Newman community detection algorithm with an inference mechanism. The model is then applied to data from Delicious, a popular social tagging system, over the time period of 2005-2008. Our results show that 1) users in the same community tend to be interested in similar set of topics in all time periods; and 2) topics may divide into several sub-topics and scatter into different communities over time. We evaluate the effectiveness of our model and show that the TTR-LDA-Community model is meaningful for understanding communities and outperforms TTR-LDA and LDA models in tag prediction.

#index 1482380
#* Discriminative factored prior models for personalized content-based recommendation
#@ Lanbo Zhang;Yi Zhang
#t 2010
#c 1
#% 766451
#% 987198
#% 1128931
#! Most existing content-based filtering approaches including Rocchio, Language Models, SVM, Logistic Regression, Neural Networks, etc. learn user profiles independently without capturing the similarity among users. The Bayesian hierarchical models learn user profiles jointly and have the advantage of being able to borrow information from other users through a Bayesian prior. The standard Bayesian hierarchical model assumes all user profiles are generated from the same prior. However, considering the diversity of user interests, this assumption might not be optimal. Besides, most existing content-based filtering approaches implicitly assume that each user profile corresponds to exactly one user interest and fail to capture a user's multiple interests (information needs). In this paper, we present a flexible Bayesian hierarchical modeling approach to model both commonality and diversity among users as well as individual users' multiple interests. We propose two models each with different assumptions, and the proposed models are called Discriminative Factored Prior Models (DFPM). In our models, each user profile is modeled as a discriminative classifier with a factored model as its prior, and different factors contribute in different levels to each user profile. Compared with existing content-based filtering models, DFPM are interesting because they can 1) borrow discriminative criteria of other users while learning a particular user profile through the factored prior; 2) trade off well between diversity and commonality among users; and 3) handle the challenging classification situation where each class contains multiple concepts. The experimental results on a dataset collected from real users on digg.com show that our models significantly outperform the baseline models of L-2 regularized logistic regression and the standard Bayesian hierarchical model with logistic regression

#index 1482381
#* Fast query expansion using approximations of relevance models
#@ Marc-Allen Cartright;James Allan;Victor Lavrenko;Andrew McGregor
#t 2010
#c 1
#% 340901
#% 411762
#% 879611
#% 1019124
#% 1215321
#% 1216713
#% 1227596
#! Pseudo-relevance feedback (PRF) improves search quality by expanding the query using terms from high-ranking documents from an initial retrieval. Although PRF can often result in large gains in effectiveness, running two queries is time consuming, limiting its applicability. We describe a PRF method that uses corpus pre-processing to achieve query-time speeds that are near those of the original queries. Specifically, Relevance Modeling, a language modeling based PRF method, can be recast to benefit substantially from finding pairwise document relationships in advance. Using the resulting Fast Relevance Model (fastRM), we substantially reduce the online retrieval time and still benefit from expansion. We further explore methods for reducing the preprocessing time and storage requirements of the approach, allowing us to achieve up to a 10% increase in MAP over unexpanded retrieval,vwhile only requiring 1% of the time of standard expansion.

#index 1482382
#* Mining rules to explain activities in videos
#@ Omar U. Florez;Curtis Dyreson
#t 2010
#c 1
#% 152934
#% 1014847
#% 1856612
#! We present a novel approach to mining dependency rules that explain the scenes present during a video sequence. The approach first characterizes activities based on their most important events. Next, an HMM-based approach finds the mixture components that best describe the clustering dependencies between events and activities in video data. The dependencies among activities are taken as association patterns with temporal precedence and analyzed using their co-occurrence relationships in time windows. This technique is meant to understand the multiple actions taken in a video or to predict future occurrences of certain activities.

#index 1482383
#* Online stratified sampling: evaluating classifiers at web-scale
#@ Paul N. Bennett;Vitor R. Carvalho
#t 2010
#c 1
#% 194284
#% 207677
#% 642988
#% 770847
#% 956455
#% 960411
#% 1074126
#% 1090784
#% 1190175
#! Deploying a classifier to large-scale systems such as the web requires careful feature design and performance evaluation. Evaluation is particularly challenging because these large collections frequently change. In this paper we adapt stratified sampling techniques to evaluate the precision of classifiers deployed in large-scale systems. We investigate different types of stratification strategies, and then we derive a new online sampling algorithm that incrementally approximates the theoretical optimal disproportionate sampling strategy. In experiments, the proposed algorithm significantly outperforms both simple random sampling as well as other types of stratified sampling, with an average reduction of about 20% in labeling effort to reach the same confidence and interval-bounds on precision

#index 1482384
#* Routing questions to appropriate answerers in community question answering services
#@ Baichuan Li;Irwin King
#t 2010
#c 1
#% 406493
#% 750863
#% 838464
#% 879627
#% 1190249
#% 1399976
#! Community Question Answering (CQA) service provides a platform for increasing number of users to ask and answer for their own needs but unanswered questions still exist within a fixed period. To address this, the paper aims to route questions to the right answerers who have a top rank in accordance of their previous answering performance. In order to rank the answerers, we propose a framework called Question Routing (QR) which consists of four phases: (1) performance profiling, (2) expertise estimation, (3) availability estimation, and (4) answerer ranking. Applying the framework, we conduct experiments with Yahoo! Answers dataset and the results demonstrate that on average each of 1,713 testing questions obtains at least one answer if it is routed to the top 20 ranked answerers.

#index 1482385
#* Learning to rank with groups
#@ Yuan Lin;Hongfei Lin;Zheng Ye;Song Jin;Xiaoling Sun
#t 2010
#c 1
#% 1074021
#% 1268491
#! An essential issue in document retrieval is ranking, and the documents are ranked by their expected relevance to a given query. Multiple labels are used to represent different level of relevance for documents to a given query, and the corresponding label values are used to quantify the relevance of the documents. According to the training set for a given query, the documents can be divided into several groups. Specifically, the documents with the same label are assigned to the same group. If the documents in the group with higher relevance label can always be ranked higher over the ones in groups with lower relevance label by a ranking model, it is reasonable to expect perfect ranking performance. Inspired by this idea, we propose a novel framework for learning to rank, which depends on two new samples. The first one is one-group constituted by one document with higher level label and a group of documents with lower level label; the second one is group-group constituted by a group of documents with higher level label and a group of documents with lower level label. A novel loss function is proposed based on the likelihood loss similar to ListMLE. We demonstrate the advantages of our approaches on the Letor 3.0 data set. Experimental results show that our approaches are effective in improving the ranking performance.

#index 1482386
#* Optimizing unified loss for web ranking specialization
#@ Fan Li;Xin Li;Jiang Bian;Zhaohui Zheng
#t 2010
#c 1
#% 411762
#% 805878
#% 818281
#% 829975
#% 987228
#% 987326
#% 1074065
#% 1399946
#! In this paper, we proposed a novel divide-and-conquer approach to optimize the overall relevance in an unified framework for query clustering and query-based ranking. In our model, latent topics and specialized ranking models are learned iteratively so that an unified objective function, which lower-bounds the conditional probability of observed grades annotated by human editors on training data, is maximized. We conducted experiments comparing the proposed method with several baseline approaches on two data-sets. Experimental results illustrate that our method can significantly improve the ranking relevance over these baselines

#index 1482387
#* Hypergraph-based multilevel matrix approximation for text information retrieval
#@ Haw-ren Fang;Yousef Saad
#t 2010
#c 1
#% 46803
#% 262217
#% 268018
#% 280059
#% 319234
#% 329562
#% 801612
#% 995168
#% 1504830
#! In Latent Semantic Indexing (LSI), a collection of documents is often pre-processed to form a sparse term-document matrix, followed by a computation of a low-rank approximation to the data matrix. A multilevel framework based on hypergraph coarsening is presented which exploits the hypergraph that is canonically associated with the sparse term-document matrix representing the data. The main goal is to reduce the cost of the matrix approximation without sacrificing accuracy. Because coarsening by multilevel hypergraph techniques is a form of clustering, the proposed approach can be regarded as a hybrid of factorization-based LSI and clustering-based LSI. Experimental results indicate that our method achieves good improvement of the retrieval performance at a reduced cost

#index 1482388
#* A peer-selection algorithm for information retrieval
#@ Yosi Mass;Yehoshua Sagiv;Michal Shmueli-Scheuer
#t 2010
#c 1
#% 309095
#% 519953
#% 907503
#% 960250
#% 1074091
#% 1292751
#! A novel method for creating collection summaries is developed, and a fully decentralized peer-selection algorithm is described. This algorithm finds the most promising peers for answering a given query. Specifically, peers publish per-term synopses of their documents. The synopses of a peer for a given term are divided into score intervals and for each interval, a KMV (K Minimal Values) synopsis of its documents is created. The synopses are used to effectively rank peers by their relevance to a multi-term quer. The proposed approach is verified by experiments on a large real-world dataset. In particular, two collections were created from this dataset, each with a different number of peers. Compared to the state-of-the-art approaches, the proposed method is effective and efficient even when documents are randomly distributed among peers

#index 1482389
#* Exploring domain-specific term weight in archived question search
#@ Zhao-Yan Ming;Tat-Seng Chua;Gao Cong
#t 2010
#c 1
#% 750863
#% 818299
#% 838397
#% 838398
#% 867054
#% 1074110
#% 1227600
#% 1292492
#% 1399953
#! Community Question Answering services, e.g., Yahoo! Answers, have accumulated large archives of question answer (QA) pairs for information and answer retrieval. An effective question retrieval model is essential to increase the accessibility of the QA archives. QA archives are usually organized into categories and question search can be performed within the whole collection or within a certain category.. In this paper, we explore domain-specific term weight for archived question search. Specifically, we propose a novel light-weighted term weighting scheme that exploits multiple aspects of the domain information. We also introduce a framework to seamlessly integrate domain-specific term weight into the existing retrieval models. Extensive experiments conducted on real Archived QA data demonstrate the utility of the proposed techniques.

#index 1482390
#* Multi-information fusion for uncertain semantic representations of videos
#@ Bo Lu;Guoren Wang;Xiaofeng Gong
#t 2010
#c 1
#% 588223
#% 903632
#% 990321
#! Concept-Based Semantic Video Retrieval(CBSVR) usually uses semantic representations of videos to handle user's retrieval requests. It is obvious that the accuracy of semantic video retrieval depends on results of concept detectors, but the detection results are usually imprecise and uncertain . In this paper, we propose a multi-information fusion approach (MIF) which is dedicated to solving the problem of uncertain semantic representations of videos for improving retrieval accuracy. This approach is based on a novel two-phase framework that involves the inferring phase and the fusing phase. In the inferring phase, the most relevant concepts to the user's query are chosen by exploring both contextual correlation among concepts and temporal correlation among shots. In the fusing phase, the inferred probabilities of the related concepts are fused together with the detection results via minimization of potential function to refine the detector prediction. Experiments on the widely used TRECVID datasets demonstrate that our approach can effectively improve the accuracy of semantic concept detection.

#index 1482391
#* Proceedings of the 19th ACM international conference on Information and knowledge management
#@ Jimmy Huang;Nick Koudas;Gareth Jones;Xindong Wu;Kevyn Collins-Thompson;Aijun An
#t 2010
#c 1
#! On behalf of the organizing committee, I wholeheartedly welcome you to the 19th ACM International Conference on Information and Knowledge Management (CIKM 2010). I hope this conference proves to be interesting and beneficial. CIKM is a well-known top tier and premier ACM conference in the areas of information retrieval, knowledge management and database. Since its inception, the CIKM conference has provided a unique international forum for the presentation, discussion, and dissemination of research findings in data management, information retrieval, and knowledge management. The purpose of the conference is to identify challenging problems facing the development of future knowledge and information systems, and to shape future research directions through the publication of high quality, applied and theoretical research findings. The conference has been a leading forum in which experts from academia, industry, and the government gather to exchange ideas, research achievements, and technical developments in multidisciplinary research areas. CIKM has rapidly grown to become one of the world's most recognized conferences in the field. This year CIKM has received a record high number of submissions in the history of CIKM, as can be seen from the following statistics: 1382 abstracts submitted 945 full papers plus 38 demo papers submitted 126 papers accepted for presentation as full papers (13.3% acceptance rate) and an additional 165 were accepted for short papers (17.5%). In addition to regular research tracks, CIKM 2010 features 4 keynote speakers, 4 pre-conference tutorials, 9 workshops, 12 industrial full papers and 20 demo papers. I am proud of our program and acknowledge the tireless efforts of people who materialized this program. First of all, I am honored to have 4 distinguished keynote speakers: Jamie Callan, Susan Dumais, Gregory Grefenstette, and Divesh Srivastava. I deeply appreciate their time and commitment to deliver their speeches and share their cutting-edge research experiences and insightful comments in their research topics.

#index 1482392
#* A topical link model for community discovery in textual interaction graph
#@ Guoqing Zheng;Jinwen Guo;Lichun Yang;Shengliang Xu;Shenghua Bao;Zhong Su;Dingyi Han;Yong Yu
#t 2010
#c 1
#% 310514
#% 313959
#% 840964
#% 1055681
#% 1117074
#% 1399996
#! This paper is concerned with community discovery in textual interaction graph, where the links between entities are indicated by textual documents. Specifically, we propose a Topical Link Model(TLM), which leverages Hierarchical Dirichlet Process(HDP) to introduce hidden topical variable of the links. Other than the use of links, TLM can look into the documents on the links in detail to recover sound communities. Moreover, TLM is a nonparametric model, which is able to learn the number of communities from the data. Extensive experiments on two real world corpora show TLM outperforms two state-of-the-art baseline models, which verify the effectiveness of TLM in determining the proper number of communities and generating sound communities.

#index 1482393
#* Taxonomic clustering of web service for efficient discovery
#@ Sourish Dasgupta;Satish Bhat;Yugyung Lee
#t 2010
#c 1
#% 449870
#% 519428
#% 1044192
#% 1190197
#% 1241154
#% 1275285
#% 1327642
#% 1349293
#% 1776653
#! The World Wide Web (WWW) has become a major platform for hosting, discovering, and composing web services. Web service clustering is a technique for efficiently facilitating web service discovery. Most web service clustering approaches are based on suitable semantic similarity distance measure and a threshold. Threshold selection is essentially difficult and often leads to unsatisfactory accuracy. In this paper we propose a taxonomic clustering algorithm for grouping functionally similar web services. We have tested the algorithm on both simulation based randomly generated test data and the standard OWL-S TC test data set. We have observed promising results both in terms of accuracy and performance.

#index 1482394
#* Active learning in parallel universes
#@ Nicolas Cebron;Michael R. Berthold
#t 2010
#c 1
#% 170649
#% 236729
#% 376266
#% 385564
#% 1272126
#% 1447586
#! This work addresses two challenges in combination: learning with a very limited number of labeled training examples (active learning) and learning in the presence of multiple views for each object where the global model to be learned is spread out over some or all of these views (learning in parallel universes). We propose a new active learning approach which selects the best samples to query the label with the goal of improving overall model accuracy and determining which universe contributes most to the local model. The resulting combination and class-specific weighting of universes provides a significantly better classification accuracy than traditional active learning methods.

#index 1482395
#* TAGME: on-the-fly annotation of short text fragments (by wikipedia entities)
#@ Paolo Ferragina;Ugo Scaiella
#t 2010
#c 1
#% 1019082
#% 1034802
#% 1130858
#% 1202162
#% 1214667
#! We designed and implemented TAGME, a system that is able to efficiently and judiciously augment a plain-text with pertinent hyperlinks to Wikipedia pages. The specialty of TAGME with respect to known systems [5,8] is that it may annotate texts which are short and poorly composed, such as snippets of search-engine results, tweets, news, etc.. This annotation is extremely informative, so any task that is currently addressed using the bag-of-words paradigm could benefit from using this annotation to draw upon (the millions of) Wikipedia pages and their inter-relations.

#index 1482396
#* Adaptive outlierness for subspace outlier ranking
#@ Emmanuel Müller;Matthias Schiffer;Thomas Seidl
#t 2010
#c 1
#% 300136
#% 1083673
#% 1117035
#% 1196030
#% 1207241
#% 1318668
#% 1328215
#% 1496814
#! Outlier mining is an important data analysis task to distinguish exceptional outliers from regular objects. However, in recent applications traditional outlier mining approaches miss outliers as they are hidden in subspace projections. In this work, we propose a novel outlier ranking based on the degree of deviation in subspaces. Object deviation is measured only in a selection of relevant subspaces and is based on adaptive neighborhoods in these subspaces. We show that our approach outperforms competing outlier ranking approaches by detecting outliers in arbitrary subspaces.

#index 1482397
#* Understanding retweeting behaviors in social networks
#@ Zi Yang;Jingyi Guo;Keke Cai;Jie Tang;Juanzi Li;Li Zhang;Zhong Su
#t 2010
#c 1
#% 1083672
#% 1214702
#% 1379671
#% 1595769
#% 1810385
#! Retweeting is an important action (behavior) on Twitter, indicating the behavior that users re-post microblogs of their friends. While much work has been conducted for mining textual content that users generate or analyzing the social network structure, few publications systematically study the underlying mechanism of the retweeting behaviors. In this paper, we perform an interesting analysis for the problem on Twitter. We have found that almost 25.5% of the tweets posted by users are actually retweeted from friends' blog spaces. Our investigation unveils that for the retweet behaviors, some statistics still follows the power law distribution, while some others violate the state-of-the-art distribution for Web. Based on these important observations, we propose a factor graph model to predict users' retweeting behaviors. Experimental results on the Twitter data set show that our method can achieve a precision of 28.81% and recall of 37.33% for prediction of the retweet behaviors.

#index 1482398
#* Mapping web pages to database records via link paths
#@ Tim Weninger;Fabio Fumarola;Jiawei Han;Donato Malerba
#t 2010
#c 1
#% 268079
#% 340928
#% 503216
#! In this paper we propose a new knowledge management task which aims to map Web pages to their corresponding records in a structured database. For example, the DBLP database contains records for many computer scientists, and most of these persons have public Web pages; if we can map the database record with the appropriate Web page then the new information could be used to further describe the person's database record. To accomplish this goal we employ link paths which contain anchor texts from multiple paths through the Web ending at the Web page in question. We hypothesize that the information from these link paths can be used to generate an accurate Web page to database record mapping. Experiments on two large, real world data sets, DBLP and IMDB for the structured data and computer science faculty members' Web pages and official movie homepages for the Web page data, show that our method does provide an accurate mapping. Finally, we conclude by issuing a call for further research on this promising new task.

#index 1482399
#* Personalized recommender system based on item taxonomy and folksonomy
#@ Huizhi Liang;Yue Xu;Yuefeng Li;Richi Nayak
#t 2010
#c 1
#% 783531
#% 813966
#% 1130827
#% 1153537
#% 1169573
#% 1280708
#% 1291235
#% 1366619
#% 1429408
#! Item folksonomy or tag information is popularly available on the web now. However, since tags are arbitrary words given by users, they contain a lot of noise such as tag synonyms, semantic ambiguities and personal tags. Such noise brings difficulties to improve the accuracy of item recommendations. In this paper, we propose to combine item taxonomy and folksonomy to reduce the noise of tags and make personalized item recommendations. The experiments conducted on the dataset collected from Amazon.com demonstrated the effectiveness of the proposed approaches. The results suggested that the recommendation accuracy can be further improved if we consider the viewpoints and the vocabularies of both experts and users.

#index 1482400
#* Communication motifs: a tool to characterize social communications
#@ Qiankun Zhao;Yuan Tian;Qi He;Nuria Oliver;Ruoming Jin;Wang-Chien Lee
#t 2010
#c 1
#% 1083672
#% 1214668
#! Social networks mediate not only the relations between entities, but also the patterns of information propagation among them and their communication behavior. In this paper, we extensively study the temporal annotations (e.g., time stamps and duration) of historical communications in social networks and propose two novel tools -- communication motifs and maximum-flow communication motifs -- for characterizations of the patterns of information propagation in social networks. Using these motifs, we verify the following hypothesis in social communication network: 1) the functional behavioral patterns of information propagation within both social networks are stable over time; 2) the patterns of information propagation in synchronous and asynchronous social networks are different and sensitive to the cost of communication; and 3) the speed and the amount of information that is propagated through a network are correlated and dependent on individual profiles.

#index 1482401
#* Improving taxonomies for large-scale hierarchical classifiers of web documents
#@ Kiyoshi Nitta
#t 2010
#c 1
#% 465747
#% 829975
#% 881494
#% 996818
#% 1021186
#! We focused on taxonomy modification algorithms for gradually improving the relevance performances of large-scale hierarchical classifiers of web documents. Considering the research results of Tang et al. [5,4], who took the same approach, we investigated and implemented two heuristic taxonomy modification algorithms for performing practical classification processes for large-scale taxonomies. Although a taxonomy modification algorithm continuously improves the relevance performances of hierarchical classifiers, it increases the computational costs of those classifiers for training and predicting processes. We developed an improved taxonomy modification algorithm for reducing computational costs by preventing child node concentration. Although the relevance performances of the algorithm-modified taxonomy classifiers improved without increasing computational costs until the fourth generation by spreading the set of predicted classes, their relevance performances and behaviors went in opposite directions from the fifth generation.

#index 1482402
#* PTM: probabilistic topic mapping model for mining parallel document collections
#@ Duo Zhang;Jimeng Sun;ChengXiang Zhai;Abhijit Bose;Nikos Anerousis
#t 2010
#c 1
#% 329569
#% 340899
#% 342707
#% 722904
#% 766491
#% 769906
#% 862153
#% 939776
#% 1055743
#% 1074109
#% 1074110
#% 1083684
#% 1166510
#% 1338620
#! Many applications generate a large volume of parallel document collections. A parallel document collection consists of two sets of documents where the documents in each set correspond to each other and form semantic pairs (e.g., pairs of problem and solution descriptions in a help-desk setting). Although much work has been done on text mining, little previous work has attempted to mine such a novel kind of text data. In this paper, we propose a new probabilistic topic model, called Probabilistic Topic Mapping (PTM) model, to mine parallel document collections to simultaneously discover latent topics in both sets of documents as well as the mapping of topics in one set to those in the other. We evaluate the PTM model on one real parallel document collection in IT service domain. We show that PTM can effectively discover meaningful topics, as well as their mappings, and it's also useful for improving text matching and retrieval when there's a vocabulary gap.

#index 1482403
#* Hierarchical auto-tagging: organizing Q&A knowledge for everyone
#@ Kyosuke Nishida;Ko Fujimura
#t 2010
#c 1
#% 232653
#% 465754
#% 466266
#% 722935
#% 747946
#% 765527
#% 869608
#% 1026895
#! We propose a hierarchical auto-tagging system, TagHats, to improve users' knowledge sharing. Our system assigns three different levels of tags to Q&A documents: category, theme, and keyword. Multiple category tags can organize a document according to multiple viewpoints, and multiple theme and keyword tags can identify what the document is about clearly. Moreover, these hierarchical tags will be helpful in organizing documents to support everyone because different users have different demands in terms of tag specificity. Our system consists of a hierarchical classification method for assigning category and theme tags, a new keyword extraction method that considers the structure of Q&A documents, and a new method for selecting theme tag candidates from each category. Experiments with the documents of Oshiete! goo demonstrate that our system is able to assign hierarchical tags to the documents appropriately and is capable of outperforming baseline methods significantly.

#index 1482404
#* Extracting structured information from Wikipedia articles to populate infoboxes
#@ Dustin Lange;Christoph Böhm;Felix Naumann
#t 2010
#c 1
#% 464434
#% 1019061
#% 1083705
#% 1166537
#! Roughly every third Wikipedia article contains an infobox - a table that displays important facts about the subject in attribute-value form. The schema of an infobox, i.e., the attributes that can be expressed for a concept, is defined by an infobox template. Often, authors do not specify all template attributes, resulting in incomplete infoboxes. With iPopulator, we introduce a system that automatically populates infoboxes of Wikipedia articles by extracting attribute values from the article's text. In contrast to prior work, iPopulator detects and exploits the structure of attribute values to independently extract value parts. We have tested iPopulator on the entire set of infobox templates and provide a detailed analysis of its effectiveness. For instance, we achieve an average extraction precision of 91% for 1,727 distinct infobox template attributes.

#index 1482405
#* Automatic metadata extraction from multilingual enterprise content
#@ Melike Şah;Vincent Wade
#t 2010
#c 1
#% 388471
#% 805894
#% 1333477
#! Enterprises provide professionally authored content about their products/services in different languages for use in web sites and customer care. For customer care, personalization/personalized information delivery is becoming important since it re-encourages users to return to the service provider. Personalization usually requires both contextual and descriptive metadata. But current metadata authored by content developers is usually quite simple. In this paper, we introduce an automatic metadata extraction framework, which can extract multilingual metadata from the enterprise content, for a personalized information retrieval system. We introduce two new ontologies for metadata creation and a novel semi-automatic topic vocabulary extraction algorithm. We demonstrate and evaluate our approach on the English and German Symantec Norton 360 technical content. Evaluations indicate that the proposed approach produces rich and high quality metadata for a personalized information retrieval system.

#index 1482406
#* Intelligent sales forecasting engine using genetic algorithms
#@ M. Vijayalakshmi;Bernard Menezes;Rohit Menon;Aniket Divecha;Rajesh Ravindran;Kamal Mehta
#t 2010
#c 1
#% 207535
#! Times series techniques have been extensively used for Sales forecasting. Research has established that a combination forecast works better than a single forecast. Our research attempts to design an Intelligent Forecasting Engine which will use a combination forecasting technique. This design is based on use of Genetic Algorithms, for selecting the best methods to combine for forecasting. Early results demonstrate that Genetic Algorithms have the potential to become a powerful tool for time series sales forecasting.

#index 1482407
#* Identifying new categories in community question answering archives: a topic modeling approach
#@ Yajie Miao;Chunping Li;Jie Tang;Lili Zhao
#t 2010
#c 1
#% 280819
#% 769967
#% 879593
#% 1019165
#% 1055683
#% 1074110
#% 1074111
#% 1190068
#% 1190249
#% 1227599
#% 1227600
#% 1227698
#! Community Question Answering (CQA) services have evolved into a popular way of information seeking and providing. User-posted questions in CQA are generally organized into hierarchical categories. In this paper, we define and study a novel problem which is referred to as New Category Identification (NCI) in CQA question archives. New Category Identification is primarily concerned with detecting and characterizing new or emerging categories which are not included in the existing category hierarchy. We define this problem formally, and propose both unsupervised and semi-supervised topic modeling methods to solve it. Experiments with a ground-truth set built from Yahoo! Answers show that our methods identify and interpret new categories effectively.

#index 1482408
#* An effective approach for mining mobile user habits
#@ Huanhuan Cao;Tengfei Bao;Qiang Yang;Enhong Chen;Jilei Tian
#t 2010
#c 1
#% 201894
#% 300120
#% 481290
#! The user interaction with the mobile device plays an important role in user habit understanding, which is crucial for improving context-aware services. In this paper, we propose to mine the associations between user interactions and contexts captured by mobile devices, or behavior patterns for short, from context logs to characterize the habits of mobile users. Though several state-of-the-art studies have been reported for association mining, they cannot apply to behavior pattern mining due to the unbalanced occurrences of contexts and user interaction records. To this end, we propose a novel approach for behavior pattern mining which takes context logs as time ordered sequences of context records and takes into account the co-occurrences of contexts and interaction records in the whole time ranges of contexts. Moreover, we develop an Apriori-like algorithm for behavior pattern mining and improve the original algorithm in terms of efficiency by introducing the context hash tree. Last, we build a data collection system and collect the rich context data and interaction records of 50 recruited volunteers from their mobile devices. The extensive experiments on the collected real life data clearly validate the ability of our approach for mining effective behavior patterns.

#index 1482409
#* Mining networks with shared items
#@ Jun Sese;Mio Seki;Mutsumi Fukuzaki
#t 2010
#c 1
#% 300120
#% 478274
#% 481290
#% 629708
#% 840892
#% 989636
#! Recent advances in data processing have enabled the generation of large and complex graphs. Many researchers have developed techniques to investigate informative structures within these graphs. However, the vertices and edges of most real-world graphs are associated with its features, and only a few studies have considered their combination. In this paper, we specifically examine a large graph in which each vertex has associated items. From the graph, we extract subgraphs with common itemsets, which we call itemset-sharing subgraphs (ISSes). The problem has various potential applications such as the detection of gene networks affected by drugs or the findings of popular research areas of contributing researchers. We propose an efficient algorithm to enumerate ISSes in large graphs. This algorithm enumerates ISSes with two efficient data structures: a DFS itemset tree and a visited itemset table. In practive, the combination of these two structures enables us to compute optimal solutions efficiently. We demonstrate the efficiency of our algorithm in mining ISSes from synthetic graphs with more than one million edges. We also present experiments performed using two real biological networks and a citation network. The experiments show that our algorithm can find interesting patterns in real datasets

#index 1482410
#* Learning sentiment classification model from labeled features
#@ Yulan He
#t 2010
#c 1
#% 855226
#% 1074125
#% 1074166
#% 1214749
#% 1251720
#% 1292560
#% 1328330
#% 1338589
#% 1481483
#! We propose a novel framework where an initial classifier is learned by incorporating prior information extracted from an existing sentiment lexicon. Preferences on expectations of sentiment labels of those lexicon words are expressed using generalized expectation criteria. Documents classified with high confidence are then used as pseudo-labeled examples for automatical domain-specific feature acquisition. The word-class distributions of such self-learned features are estimated from the pseudo-labeled examples and are used to train another classifier by constraining the model's predictions on unlabeled instances. Experiments on both the movie review data and the multi-domain sentiment dataset show that our approach attains comparable or better performance than exiting weakly-supervised sentiment classification methods despite using no labeled documents.

#index 1482411
#* Embedding tolerance relations in formal concept analysis: an application in information fusion
#@ Mehdi Kaytoue;Zainab Assaghir;Amedeo Napoli;Sergei O. Kuznetsov
#t 2010
#c 1
#% 384416
#% 565484
#% 1217563
#% 1705155
#! This paper shows how to embed a similarity relation between complex descriptions in concept lattices. We formalize similarity by a tolerance relation: objects are grouped within a same concept when having similar descriptions, extending the ability of FCA to deal with complex data. We propose two different approaches.~A first classical manner defines a discretization procedure. A second way consists in representing data by pattern structures, from which a pattern concept lattice can be constructed directly. In this case, considering a tolerance relation can be mathematically defined by a projection in a meet-semi-lattice. This allows to use concept lattices for their knowledge representation and reasoning abilities without transforming data. We show finally that resulting lattices are useful for solving information fusion problems.

#index 1482412
#* Online learning for multi-task feature selection
#@ Haiqin Yang;Irwin King;Michael R. Lyu
#t 2010
#c 1
#% 723239
#% 769886
#% 1074348
#% 1128929
#% 1193366
#% 1232034
#% 1379069
#% 1386006
#% 1417091
#% 1674792
#! Multi-task feature selection (MTFS) is an important tool to learn the explanatory features across multiple related tasks. Previous MTFS methods fulfill this task in batch-mode training. This makes them inefficient when data come in sequence or when the number of training data is so large that they cannot be loaded into the memory simultaneously. To tackle these problems, we propose the first online learning framework for MTFS. A main advantage of the online algorithms is the efficiency in both time complexity and memory cost due to the closed-form solutions in updating the model weights at each iteration. Experimental results on a real-world dataset attest to the merits of the proposed algorithms.

#index 1482413
#* Exploiting user interests for collaborative filtering: interests expansion via personalized ranking
#@ Qi Liu;Enhong Chen;Hui Xiong;Chris H.Q. Ding
#t 2010
#c 1
#% 173879
#% 577329
#% 722904
#% 805841
#% 975021
#% 1127466
#% 1211828
#% 1409615
#! In real applications, a given user buys or rates an item based on his/her interests. Learning to leverage this interest information is often critical for recommender systems. However, in existing recommender systems, the information about latent user interests are largely under-explored. To that end, in this paper, we propose an interest expansion strategy via personalized ranking based on the topic model, named iExpand, for building an interest-oriented collaborative filtering framework. The iExpand method introduces a three-layer, user-interest-item, representation scheme, which leads to more interpretable recommendation results and helps the understanding of the interactions among users, items, and user interests. Moreover, iExpand strategically deals with many issues, such as the overspecialization and the cold-start problems. Finally, we evaluate iExpand on benchmark data sets, and experimental results show that iExpand outperforms state-of-the-art methods.

#index 1482414
#* K-farthest-neighbors-based concept boundary determination for support vector data description
#@ Yanshan Xiao;Bo Liu;Longbing Cao
#t 2010
#c 1
#% 209021
#% 302406
#% 479462
#% 730041
#% 732387
#% 940340
#% 1097909
#% 1292710
#! Support vector data description (SVDD) is very useful for one-class classification. However, it incurs high time complexity in handling large scale data. In this paper, we propose a novel and efficient method, named K-Farthest-Neighbors-based Concept Boundary Detection (KFN-CBD for short), to improve the SVDD learning efficiency on large datasets. This work is motivated by the observation that SVDD classifier is determined by support vectors (SVs), and removing the non-support vectors (non-SVs) will not change the classifier but will reduce computational costs. Our approach consists of two steps. In the first step, we propose the K-farthest-neighbors method to identify the samples around the hyper-sphere surface, which are more likely to be SVs. At the same time, a new tree search strategy of M-tree is presented to speed up the K-farthest neighbor query. In the second step, the non-SVs are eliminated from the training set, and only the identified boundary samples are used to train the SVDD classifier. By removing the non-SVs, the training time of SVDD can be substantially reduced.Extensive experiments have shown that KFN-CBD achieves around 6 times speedup compared to the standard SVDD, and obtains the comparable classification quality as the entire dataset used.

#index 1482415
#* Relational feature engineering of natural language processing
#@ Hamidreza Kobdani;Hinrich Schütze;Andre Burkovski;Wiltrud Kessler;Gunther Heidemann
#t 2010
#c 1
#% 224702
#% 376266
#% 385564
#% 385686
#% 480286
#% 815329
#% 939857
#% 1472114
#! We present a new framework for feature engineering of natural language processing that is based on a relational data model of text. It includes fast and flexible methods for implementing and extracting new features and thereby reduces the effort of creating an NLP system for a particular task. In an instantiation and evaluation of the framework for the problem of coreference resolution in multiple languages, we were able to obtain competitive results in a short implementation period. This demonstrates the potential power of our framework for feature engineering.

#index 1482416
#* Transfer incremental learning for pattern classification
#@ Zhenfeng Zhu;Xingquan Zhu;Yue-Fei Guo;Xiangyang Xue
#t 2010
#c 1
#% 236497
#% 342598
#% 983828
#% 1464068
#! Traditional machine learning methods, such as Support Vector Machines (SVMs), usually assume that training and test data share the same distributions. Due to the inherent dynamic data nature, it is often observed that (1) the volumes of the training data may gradually grow; and (2) the existing and the newly arrived samples may be subject to different distributions or learning tasks. In this paper, we propose a Transfer Incremental Support Vector Machine(TrISVM), with the objective of tackling changes in data volumes and learning tasks at the same time. By using new updating rules to calculate the inverse matrix, TrISVM solves the existing incremental learning problem more efficiently, especially for high dimensional data. Furthermore, when using new samples to update the existing models, TrISVM employs sample-based weight adjustment procedures to ensure that the concept transferring between auxiliary and target samples can be leveraged to fulfill the transfer learning goal. Experimental results on real-world data sets demonstrate that TrISVM achieves better efficiency and prediction accuracy than both incremental-learning and transfer-learning based methods. In addition, the results also show that TrISVM is able to achieve bidirectional knowledge transfer between two similar tasks.

#index 1482417
#* Learning ontology resolution for document representation and its applications in text mining
#@ Lidong Bing;Bai Sun;Shan Jiang;Yan Zhang;Wai Lam
#t 2010
#c 1
#% 198058
#% 727861
#% 881552
#% 1002315
#! It is well known that synonymous and polysemous terms often bring in some noises when calculating the similarity between documents. Existing ontology-based document representation methods are static, hence, the chosen semantic concept set for representing a document has a fixed resolution and it is not adaptable to the characteristics of a document collection and the text mining problem in hand. We propose an Adaptive Concept Resolution (ACR) model to overcome this issue. ACR can learn a concept border from an ontology taking into consideration of the characteristics of a particular document collection. Then this border can provide a tailor-made semantic concept representation for a document coming from the same domain. Another advantage of ACR is that it is applicable in both classification task where the groups are given in the training document set, and clustering task where no group information is available. Furthermore, the result of this model is not sensitive to the model parameter. The experimental results show that ACR outperforms an existing static method significantly.

#index 1482418
#* Supervised identification and linking of concept mentions to a domain-specific ontology
#@ Gabor Melli;Martin Ester
#t 2010
#c 1
#% 452449
#% 532186
#% 742218
#% 816181
#% 855119
#% 975019
#% 1019082
#% 1130858
#% 1214667
#! We propose a pipelined supervised learning approach named SDOI to the task of interlinking the concepts mentioned within a document to the concepts within an ontology. Concept mention identification is performed by training a sequential tagging model. Each identified concept mention is then associated with a set of candidate ontology concepts along with a feature vector based on features proposed in the literature and novel ones based on new data sources, such as from the training corpus itself. An iterative algorithm is defined for handling collective features. We show a lift in performance over applicable baselines against the ability to identify the concept mentions within the 139 KDD-2009 conference paper abstracts, and to link these concept mentions to a domain-specific ontology for the field of data mining. Additional experiments of 22 ICDM-2009 abstracts suggest that the trained models are portable both in terms of accuracy and in their ability to reduce annotation time.

#index 1482419
#* Relevance-index size tradeoff in contextual advertising
#@ Pavan Kumar GM;Krishna P. Leela;Mehul Parsana;Sachin Garg
#t 2010
#c 1
#% 411762
#% 449588
#% 818265
#% 869484
#% 956504
#% 987262
#% 1019092
#% 1040857
#% 1127356
#% 1190059
#% 1190099
#% 1355054
#! In Contextual advertising, textual ads relevant to the content in a webpage are embedded in the page. Content keywords are extracted offline by crawling webpages and then stored in an index for fast serving. Given a page, ad selection involves index lookup, computing similarity between the keywords of the page and those of candidate ads and returning the top-k scoring ads. In this approach, there is a tradeoff between relevance and index size where better relevance can be achieved if there are no limits on the index size. However, the assumption of unlimited index size is not practical due to the large number of pages on the Web and stringent requirements on the serving latency. Secondly, page visits on the web follows power-law distribution where a significant proportion of the pages are visited infrequently, also called the tail pages. Indexing tail pages is not efficient given that these pages are accessed very infrequently. We propose a novel mechanism to mitigate these problems in the same framework. The basic idea is to index the same keyword vector for a set of similar pages. The scheme involves learning a website specific hierarchy from (page, URL) pairs of the website. Next, keywords are populated on the nodes via bottom-up traversal over the hierarchy. We evaluate our approach on a human labeled dataset where our approach has higher nDCG compared to a recent approach even though the index size of our approach is 7 times less than index size of the recent approach.

#index 1482420
#* CasJoin: a cascade chain for text similarity joins
#@ Xiaoxun Zhang;Zhili Guo;Honglei Guo;Huijia Zhu;Zhong Su
#t 2010
#c 1
#% 736300
#% 765463
#% 864392
#% 893164
#% 956506
#% 1055678
#% 1055684
#! We are concerned with the problem of similarity joins of text data, where the task is to find all pairs of documents above an expected similarity. Such a problem often serves as an indispensable step in many web applications. A crucial issue is to preclude unnecessary candidate pairs as many as possible ahead of expensive similarity evaluation. In this paper, we initiate an idea of adopting a cascade structure in text joins for a large speedup, where a latter stage can exclude a considerable number of invalid pairs survived in former stages. The proposed algorithm is shortly referred to as CasJoin. We further adopt a prefix filter to build the stage of CasJoin by introducing a novel vision to the dynamic generation of document vector. Specifically, a vector is partitioned into a chain of multiple prefixes that are appended one by one for cascade joining. We evaluate our CasJoin on a typical web corpus, ODP. Experiments indicate that, comparing to the state-of-the-art prefix algorithms, CasJoin can achieve a drastic reduction of candidates by as much as 98.15% and a dramatic speedup of joining by up to 13.34x.

#index 1482421
#* Learning naïve bayes transfer classifier throughclass-wise test distribution estimation
#@ Jeong-Woo Son;Seong-Bae Park;Hyun-Je Song
#t 2010
#c 1
#% 311027
#% 1269755
#! Text classification is a well-known problem for various applications. For last decades, it is beleived that a large corpus is one of the most important aspects for better classification. However, even though a great number of documents is available for training a classifier, it is practically impossible to achieve an ideal performance, since the distributions of labeled and unlabeled documents are often different. To overcome this problem, this paper describes a novel Naïve Bayes classifier for text classification under distribution difference between training and test data. The proposed method approximates test distribution by weighting labeled documents to cope with the distribution difference. Unlike other transfer learning which estimates the weights of labeled documents, the proposed method considerd both the documents and their estimated class labels. Therefore, the proposed method naturally combines the advantage of semi-supervised learning with those of transfer learning.

#index 1482422
#* Top-Eye: top-k evolving trajectory outlier detection
#@ Yong Ge;Hui Xiong;Zhi-hua Zhou;Hasan Ozdemir;Jannite Yu;K. C. Lee
#t 2010
#c 1
#% 1206639
#% 1207011
#% 1214637
#% 1214710
#% 1698368
#! The increasing availability of large-scale location traces creates unprecedent opportunities to change the paradigm for identifying abnormal moving activities. Indeed, various aspects of abnormality of moving patterns have recently been exploited, such as wrong direction and wandering. However, there is no recognized way of combining different aspects into an unified evolving abnormality score which has the ability to capture the evolving nature of abnormal moving trajectories. To that end, in this paper, we provide an evolving trajectory outlier detection method, named TOP-EYE, which continuously computes the outlying score for each trajectory in an accumulating way. Specifically, in TOP-EYE, we introduce a decay function to mitigate the influence of the past trajectories on the evolving outlying score, which is defined based on the evolving moving direction and density of trajectories. This decay function enables the evolving computation of accumulated outlying scores along the trajectories. An advantage of TOP-EYE is to identify evolving outliers at very early stage with relatively low false alarm rate. Finally, experimental results on real-world location traces show that TOP-EYE can effectively capture evolving abnormal trajectories.

#index 1482423
#* Multi task learning on multiple related networks
#@ Prakash Mandayam Comar;Pang-ning Tan;Anil Kumar Jain
#t 2010
#c 1
#% 592143
#% 1055685
#% 1292708
#% 1318663
#! With the rapid proliferation of online social networks, the need for newer class of learning algorithm to simultaneously deal with multiple related networks has become increasingly important. This paper proposes an approach for multi-task learning in multiple related networks, where in we perform different tasks such as classification on one network and clustering on the other. We show that the framework can be extended to incorporate prior information about the correspondences between the clusters and classes in different networks. We have performed experiments on real-world data sets to demonstrate the effectiveness of the proposed framework.

#index 1482424
#* Building a semantic representation for personal information
#@ Jinyoung Kim;Anton Bakalov;David A. Smith;W. Bruce Croft
#t 2010
#c 1
#% 577224
#% 751830
#% 1047409
#% 1083900
#% 1152462
#% 1292539
#% 1667783
#! A typical collection of personal information contains many documents and mentions many concepts (e.g., person names, events, etc.). In this environment, associative browsing between these concepts and documents can be useful as a complement for search. Previous approaches in the area of semantic desktops aimed at addressing this task. However, they were not practical because they require tedious manual annotation by the user. In this work, we suggest a methodology and a prototype system for building a semantic representation of personal information based on click feedback from the user. We employed a feature-based model of associations between the concepts and documents. Our initial evaluation shows that the suggested semantic representation can play an important role in the known-item finding task and that the system can learn to predict such associations with a small amount of click data.

#index 1482425
#* Affinity-driven prediction and ranking of products in online product review sites
#@ Hui Li;Sourav S. Bhowmick;Aixin Sun
#t 2010
#c 1
#% 411762
#% 1190130
#% 1286356
#! Large online product review websites (e.g., Epinions, Blippr)to various types of products. Typically, each product in these sites is associated with a group of members who have provided ratings and comments on it. These people form a product community. A potential member can join a produce community by giving a new rating to the product. We refer to this phenomenon of a product community's ability to "attract" new members as product affinity. The knowledge of a ranked list of products based on product affinity is of much importance to be utilized for implementing policies, marketing research, online advertisement, and other applications. In this paper, we identify and analyze an array of features that exert effect on product affinity and propose a novel model, called AffRank, that utilizes these features to predict the future rank of products according to their affinities. Evaluated on a real-world dataset, we demonstrate the effectiveness and superior prediction quality of AffRank compared to baseline methods. Our experiments show that features such as affinity rank history, affinity evolution distance, and average rating are the most important factors affecting future rank of products.

#index 1482426
#* Topic-driven web search result organization by leveraging wikipedia semantic knowledge
#@ Xianpei Han;Jun Zhao
#t 2010
#c 1
#% 218992
#% 262045
#% 643068
#% 768632
#% 1200254
#% 1387574
#! Effective organization of web search results can greatly improve the utility of search engine and enhance the quality of search results. However, the organization of search results is difficult because the sub-topics of a query are usually not explicitly given. In this paper, we propose a novel topic-driven search result organization method, which can first detect the sub-topics of a query by finding the coherent Wikipedia concept groups from its search results; then organize these results using a topic-driven clustering algorithm; in the end we score and rank the topics using the support vector regression model. Empirical results show that our method can achieve competitive performance.

#index 1482427
#* Fast dimension reduction for document classification based on imprecise spectrum analysis
#@ Hu Guan;Bin Xiao;Jingyu Zhou;Minyi Guo;Tao Yang
#t 2010
#c 1
#% 109918
#% 129316
#% 248027
#% 794857
#% 1042491
#% 1131829
#% 1190075
#% 1292821
#! This paper proposes an algorithm called Imprecise Spectrum Analysis (ISA) to carry out fast dimension reduction for document classification. ISA is designed based on the one-sided Jacobi method for Singular Value Decomposition (SVD). To speedup dimension reduction, it simplifies the orthogonalization process of Jacobi computation and introduces a new mapping formula for transforming original document-term vectors. To improve classification accuracy using ISA, a feature selection method is further developed to make inter-class feature vectors more orthogonal in building the initial weighted term-document matrix. Our experimental results show that ISA is extremely fast in handling large term-document matrices and delivers better or competitive classification accuracy compared to SVD-based LSI.

#index 1482428
#* Manifold ranking with sink points for update summarization
#@ Pan Du;Jiafeng Guo;Jin Zhang;Xueqi Cheng
#t 2010
#c 1
#% 987368
#% 1251647
#% 1252672
#% 1275220
#! Update summarization aims to create a summary over a topic-related multi-document dataset based on the assumption that the user has already read a set of earlier documents of the same topic. Beyond the problems (i.e., topic relevance, salience, and diversity in extracted information) tackled by topic-focused multi-document summarization, the update summarization must address the novelty problem as well. In this paper, we propose a novel extractive approach based on manifold ranking with sink points for update summarization. Specifically, our approach leverages a manifold ranking process over the sentence manifold to find topic relevant and salient sentences. More important, by introducing the sink points into sentence manifold, the ranking process can further capture the novelty and diversity based on the intrinsic sentence manifold. Therefore, we are able to address the four challenging problems above for update summarization in a unified way. Experiments on benchmarks of TAC are performed and the evaluation results show that our approach can achieve comparative performance to the existing best performing systems in TAC tasks.

#index 1482429
#* Construction of a sentimental word dictionary
#@ Eduard C. Dragut;Clement Yu;Prasad Sistla;Weiyi Meng
#t 2010
#c 1
#% 78171
#% 722308
#% 755835
#% 838521
#% 939897
#% 1019145
#% 1292562
#% 1788189
#! The Web has plenty of reviews, comments and reports about products, services, government policies, institutions, etc. The opinions expressed in these reviews influence how people regard these entities. For example, a product with consistently good reviews is likely to sell well, while a product with numerous bad reviews is likely to sell poorly. Our aim is to build a sentimental word dictionary, which is larger than existing sentimental word dictionaries and has high accuracy. We introduce rules for deduction, which take words with known polarities as input and produce synsets (a set of synonyms with a definition) with polarities. The synsets with deduced polarities can then be used to further deduce the polarities of other words. Experimental results show that for a given sentimental word dictionary with D words, approximately an additional 50% of D words with polarities can be deduced. An experiment is conducted to find the accuracy of a random sample of the deduced words. It is found that the accuracy is about the same as that of comparing the judgment of one human with that of another.

#index 1482430
#* Exploiting novelty, coverage and balance for topic-focused multi-document summarization
#@ Xuan Li;Yi-Dong Shen;Liang Du;Chen-Yan Xiong
#t 2010
#c 1
#% 262112
#% 340885
#% 397137
#% 1074086
#% 1074088
#% 1074133
#% 1190062
#% 1275220
#! Novelty, coverage and balance are important requirements in topic-focused summarization, which to a large extent determine the quality of a summary. In this paper, we propose a novel method that incorporates these requirements into a sentence ranking probability model. It differs from the existing methods in that the novelty, coverage and balance requirements are all modeled w.r.t. a given topic, so that summaries are highly relevant to the topic and at the same time comply with topic-aware novelty, coverage and balance. Experimental results on the DUC 2005, 2006 and 2007 benchmark data sets demonstrate the effectiveness of our method.

#index 1482431
#* Context modeling for ranking and tagging bursty features in text streams
#@ Wayne Xin Zhao;Jing Jiang;Jing He;Dongdong Shan;Hongfei Yan;Xiaoming Li
#t 2010
#c 1
#% 342707
#% 643520
#% 722904
#% 765412
#% 824666
#% 987218
#% 989620
#% 1214669
#! Bursty features in text streams are very useful in many text mining applications. Most existing studies detect bursty features based purely on term frequency changes without taking into account the semantic contexts of terms, and as a result the detected bursty features may not always be interesting or easy to interpret. In this paper we propose to model the contexts of bursty features using a language modeling approach. We then propose a novel topic diversity-based metric using the context models to find newsworthy bursty features. We also propose to use the context models to automatically assign meaningful tags to bursty features. Using a large corpus of a stream of news articles, we quantitatively show that the proposed context language models for bursty features can effectively help rank bursty features based on their newsworthiness and to assign meaningful tags to annotate bursty features.

#index 1482432
#* Comparison of six aggregation strategies to compute users' trustworthiness
#@ Pierpaolo Dondio;Stephen Barrett
#t 2010
#c 1
#% 348818
#% 607996
#! The decision to grant trust in virtual societies is often an evidence based process. The evidence for such decision derives from a diverse set, where mutual relationships and contradictions might occur. This paper compares and evaluates six aggregation strategies to compute users' trustworthiness. Our evaluation performed over a large online-community, shows how a rule-based strategy based on an argumentation semantic outperforms strategies where mutual relationships among evidence are ignored.

#index 1482433
#* Visualization and clustering of crowd video content in MPCA subspace
#@ Haiping Lu;How-Lung Eng;Myo Thida;Konstantinos N. Plataniotis
#t 2010
#c 1
#% 593047
#% 724326
#% 783517
#% 1066707
#% 1136824
#% 1373835
#% 1862054
#! This paper presents a novel approach for the visualization and clustering of crowd video contents by using multilinear principal component analysis (MPCA). In contrast to feature-point-based approach and frame-based dimensionality reduction approach, the proposed method maps each short video segment to a point in MPCA subspace to take temporal information into account naturally through tensorial representations. Specifically, MPCA projects each short segment of a video to a low-dimensional tensor first. A few MPCA features are then selected according to the variance captured as the final representation. Thus, a video is visualized as a trajectory in MPCA subspace. The trajectory generated enables visual interpretation of video content in a compact space as well as visual clustering of video events. The proposed method is evaluated on the PETS 2009 datasets through comparison with three existing methods for video visualization. The MPCA visualization shows superior performance in clustering segments of the same event as well as identifying the transitions between events.

#index 1482434
#* ANITA: a narrative interpretation of taxonomies for their adaptation to text collections
#@ Mario Cataldi;K. Selçuk Candan;Maria Luisa Sapino
#t 2010
#c 1
#% 169768
#% 262045
#% 280849
#% 413610
#% 474625
#% 643068
#% 907535
#% 1008083
#% 1269899
#% 1272078
#% 1290789
#! Taxonomies embody formalized knowledge and define aggregations between concepts/categories in a given domain, facilitating the organization of the data and making the contents easily accessible to the users. Since taxonomies have significant roles in the data annotation, search and navigation, they are often carefully engineered. However, especially in very dynamic content, they do not necessarily reflect the content knowledge. Thus, in this paper, we propose A Narrative Interpretation of Taxonomies for their Adaptation (ANITA) for re-structuring existing taxonomies to varying application contexts and we evaluate the proposed scheme by user studies that show that the proposed algorithm is able to adapt the taxonomy in a new compact and understandable structure from a human point of view.

#index 1482435
#* Yes we can: simplex volume maximization for descriptive web-scale matrix factorization
#@ Christian Thurau;Kristian Kersting;Christian Bauckhage
#t 2010
#c 1
#% 1119142
#% 1318694
#% 1327693
#! Matrix factorization methods are among the most common techniques for detecting latent components in data. Popular examples include the Singular Value Decomposition or Non-negative Matrix Factorization. Unfortunately, most methods suffer from high computational complexity and therefore do not scale to massive data. In this paper, we present a linear time algorithm for the factorization of gigantic matrices that iteratively yields latent components. We consider a constrained matrix factorization s.t.~the latent components form a simplex that encloses most of the remaining data. The algorithm maximizes the volume of that simplex and thereby reduces the displacement of data from the space spanned by the latent components. Hence, it also lowers the Frobenius norm, a common criterion for matrix factorization quality. Our algorithm is efficient, well-grounded in distance geometry, and easily applicable to matrices with billions of entries. In addition, the resulting factors allow for an intuitive interpretation of data: every data point can now be expressed as a convex combination of the most extreme and thereby often most descriptive instances in a collection of data. Extensive experimental validations on web-scale data, including 80 million images and 1.5 million twitter tweets, demonstrate superior performance compared to related factorization or clustering techniques.

#index 1482436
#* Incorporating terminology evolution for query translation in text retrieval with association rules
#@ Amal C. Kaluarachchi;Aparna S. Varde;Srikanta Bedathur;Gerhard Weikum;Jing Peng;Anna Feldman
#t 2010
#c 1
#% 286069
#% 287242
#% 746871
#% 938705
#% 939810
#! Time-stamped documents such as newswire articles, blog posts and other web-pages are often archived online. When these archives cover long spans of time, the terminology within them could undergo significant changes. Hence, when users pose queries pertaining to historical information, over such documents, the queries need to be translated, taking into account these temporal changes, to provide accurate responses to users. For example, a query on Sri Lanka should automatically retrieve documents with its former name Ceylon. We call such concepts SITACs, i.e., Semantically Identical Temporally Altering Concepts. In order to discover SITACs, we propose an approach based on a novel framework constituting an integration of natural language processing, association rule mining, and contextual similarity as a learning technique. The proposed approach has been experimented with real data and has been found to yield good results with respect to efficiency and accuracy.

#index 1482437
#* Exploiting co-occurrence and information quality metrics to recommend tags in web 2.0 applications
#@ Fabiano Muniz Belém;Eder Ferreira Martins;Jussara Marques Almeida;Marcos André Gonçalves;Gisele Lobo Pappa
#t 2010
#c 1
#% 481290
#% 974033
#% 1055704
#% 1074117
#% 1127458
#% 1287227
#% 1292558
#! This work addresses the task of recommending high quality tags by exploiting not only previously assigned tags, but also terms extracted from other textual features (e.g., title and description) associated with the target object.To estimate the quality of a candidate tag recommendation, we use several metrics related to both tag co-occurrence and information quality. We also propose a heuristic function to combine the metrics to produce a final ranking of the recommended tags. We evaluate our heuristic function in various scenarios, for three popular Web 2.0 applications. Our experimental results indicate that our heuristic function significantly outperforms two state-of-the-art tag recommendation algorithms.

#index 1482438
#* Elusive vandalism detection in wikipedia: a text stability-based approach
#@ Qinyi Wu;Danesh Irani;Calton Pu;Lakshmish Ramaswamy
#t 2010
#c 1
#% 751850
#% 956520
#% 1016107
#% 1288485
#% 1288486
#% 1383192
#% 1400176
#% 1415780
#! The open collaborative nature of wikis encourages participation of all users, but at the same time exposes their content to vandalism. The current vandalism-detection techniques, while effective against relatively obvious vandalism edits, prove to be inadequate in detecting increasingly prevalent sophisticated (or elusive) vandal edits. We identify a number of vandal edits that can take hours, even days, to correct and propose a text stability-based approach for detecting them. Our approach is focused on the likelihood of a certain part of an article being modified by a regular edit. In addition to text-stability, our machine learning-based technique also takes into account edit patterns. We evaluate the performance of our approach on a corpus comprising of 15000 manually labeled edits from the Wikipedia Vandalism PAN corpus. The experimental results show that text-stability is able to improve the performance of the selected machine-learning algorithms significantly.

#index 1482439
#* Feature subspace transformations for enhancing k-means clustering
#@ Anirban Chatterjee;Sanjukta Bhowmick;Padma Raghavan
#t 2010
#c 1
#% 122797
#% 191910
#% 1013696
#! Unsupervised classification typically concerns identifying clusters of similar entities in an unlabeled dataset. Popular methods include clustering based on (i) distance-based metrics between the entities in the feature space (K-Means), and (ii) combinatorial properties in a weighted graph representation of the dataset (Multilevel K-Means). In this paper, we present a force-directed graph layout based feature subspace transformation (FST) scheme to transform the dataset before the application of K-Means. Our FST-K-Means method utilizes both distance-based and combinatorial attributes of the original dataset to seek improvements in the internal and external quality metrics of unsupervised classification. We demonstrate the effectiveness of FST-K-Means in improving classification quality relative to K-Means and Multilevel K-Means (GraClus). The quality of classification is measured by observing internal and external quality metrics on a test suite of datasets. Our results indicate that on average, the internal quality metric (cluster cohesiveness) is 20.2% better than K-Means, and 6.6% better than GraClus. More significantly, FST-K-Means improves the external quality metric (accuracy) of classification on average by 14.9% relative to K-Means and 23.6% relative to GraClus.

#index 1482440
#* On bootstrapping recommender systems
#@ Nadav Golbandi;Yehuda Koren;Ronny Lempel
#t 2010
#c 1
#% 124010
#% 330687
#% 342767
#% 452563
#% 734590
#% 1200881
#% 1260273
#% 1291600
#! Recommender systems perform much better on users for which they have more information. This gives rise to a problem of satisfying users new to a system. The problem is even more acute considering that some of these hard to profile new users judge the unfamiliar system by its ability to immediately provide them with satisfying recommendations, and may be the quickest to abandon the system when disappointed. Rapid profiling of new users is often achieved through a bootstrapping process - a kind of an initial interview - that elicits users to provide their opinions on certain carefully chosen items or categories. This work offers a new bootstrapping method, which is based on a concrete optimization goal, thereby handily outperforming known approaches in our tests.

#index 1482441
#* Using Wikipedia categories for compact representations of chemical documents
#@ Benjamin Köhncke;Wolf-Tilo Balke
#t 2010
#c 1
#% 961697
#% 961725
#% 1019081
#% 1214660
#% 1434147
#% 1682486
#! Today, Web pages are usually accessed using text search engines, whereas documents stored in the deep Web are accessed through domain-specific Web portals. These portals rely on external knowledge bases, respectively ontologies, mapping documents to more general concepts allowing for suitable classifications and navigational browsing. Since automatically generated ontologies are still not satisfactory for advanced information retrieval tasks, most portals heavily rely on hand-crafted domain-specific ontologies. This, however, also leads to high creation and maintaining costs. On the other hand, a freely available community maintained, if somewhat general, knowledge base is offered by Wikipedia. During the last years the coverage of Wikipedia has reached a large pool of information including articles from almost all domains. In this paper, we investigate the use of Wikipedia categories to describe the content of chemical documents in a compact form. We compare the results to the domain-specific ChEBI ontology and the results show that Wikipedia categories indeed allow useful descriptions for chemical documents that are even better than descriptions from the ChEBI ontology.

#index 1482442
#* Efficient wikipedia-based semantic interpreter by exploiting top-k processing
#@ Jong Wook Kim;Ashwin Kashyap;Dekai Li;Sandilya Bhamidipati
#t 2010
#c 1
#% 321635
#% 643566
#% 1016183
#% 1022278
#% 1272267
#! Proper representation of the meaning of texts is crucial to enhancing many data mining and information retrieval tasks, including clustering, computing semantic relatedness between texts, and searching. Representing of texts in the concept space derived from Wikipedia has received growing attention recently, due to its comprehensiveness and expertise, This concept-based representation is capable of extracting semantic relatedness between texts that cannot be deduced with the bag of words model. A key obstacle, however, for using Wikipedia as a semantic interpreter is that the sheer size of the concepts derived from Wikipedia makes it hard to efficiently map texts into concept-space. In this paper, we develop an efficient algorithm which is able to represent the meaning of a text by using the concepts that best match it. In particular, our approach first computes the approximate top-k concepts that are most relevant to the given text. We then leverage these concepts for representing the meaning of the given text. The experimental results show that the proposed technique provides significant gains in execution time over current solutions to the problem.

#index 1482443
#* A study of rumor control strategies on social networks
#@ Rudra M. Tripathy;Amitabha Bagchi;Sameep Mehta
#t 2010
#c 1
#% 729923
#! In this paper we study and evaluate rumor-like methods for combating the spread of rumors on a social network. We model rumor spread as a diffusion process on a network and suggest the use of an "anti-rumor" process similar to the rumor process. We study two natural models by which these anti-rumors may arise. The main metrics we study are the belief time, i.e., the duration for which a person believes the rumor to be true and point of decline, i.e., point after which anti-rumor process dominates the rumor process. We evaluate our methods by simulating rumor spread and anti-rumor spread on a data set derived from the social networking site Twitter and on a synthetic network generated according to the Watts and Strogatz model. We find that the lifetime of a rumor increases if the delay in detecting it increases, and the relationship is at least linear. Further our findings show that coupling the detection and anti-rumor strategy by embedding agents in the network, we call them beacons, is an effective means of fighting the spread of rumor, even if these beacons do not share information.

#index 1482444
#* Domain-independent entity coreference in RDF graphs
#@ Dezhao Song;Jeff Heflin
#t 2010
#c 1
#% 503213
#% 747890
#% 855094
#% 1696287
#% 1696307
#! In this paper, we present a novel entity coreference algorithm for Semantic Web instances. The key issues include how to locate context information and how to utilize the context appropriately. To collect context information, we select a neighborhood (consisting of triples) of each instance from the RDF graph. To determine the similarity between two instances, our algorithm computes the similarity between comparable property values in the neighborhood graphs. The similarity of distinct URIs and blank nodes is computed by comparing their outgoing links. To provide the best possible domain-independent matches, we examine an appropriate way to compute the discriminability of triples. To reduce the impact of distant nodes, we explore a distance-based discounting approach. We evaluated our algorithm using different instance categories in two datasets. Our experiments show that the best results are achieved by including both our triple discrimination and discounting approaches.

#index 1482445
#* Opinion digger: an unsupervised opinion miner from unstructured product reviews
#@ Samaneh Moghaddam;Martin Ester
#t 2010
#c 1
#% 248218
#% 459006
#% 769892
#% 805873
#% 815915
#% 1292576
#% 1292706
#% 1440454
#! Mining customer reviews (opinion mining) has emerged as an interesting new research direction. Most of the reviewing websites such as Epinions.com provide some additional information on top of the review text and overall rating, including a set of predefined aspects and their ratings, and a rating guideline which shows the intended interpretation of the numerical ratings. However, the existing methods have ignored this additional information. We claim that using this information, which is freely available, along with the review text can effectively improve the accuracy of opinion mining. We propose an unsupervised method, called Opinion Digger, which extracts important aspects of a product and determines the overall consumer's satisfaction for each, by estimating a rating in the range from 1 to 5. We demonstrate the improved effectiveness of our methods on a real life dataset that we crawled from Epinions.com.

#index 1482446
#* Combining link and content for collective active learning
#@ Lixin Shi;Yuhang Zhao;Jie Tang
#t 2010
#c 1
#% 464434
#% 769942
#% 842682
#% 961278
#% 1030882
#% 1214681
#% 1400012
#% 1673032
#! In this paper, we study a novel problem Collective Active Learning, in which we aim to select a batch set of "informative" instances from a networking data set to query the user in order to improve the accuracy of the learned classification model. We perform a theoretical investigation of the problem and present three criteria (i.e., minimum redundancy, maximum uncertainty and maximum impact) to quantify the informativeness of a set of selected instances. We define an objective function based on the three criteria and present an efficient algorithm to optimize the objective function with a bounded approximation rate. Experimental results on a real-world data sets demonstrate the effectiveness of our proposed approach.

#index 1482447
#* Classifying sentiment in microblogs: is brevity an advantage?
#@ Adam Bermingham;Alan F. Smeaton
#t 2010
#c 1
#% 938687
#% 939769
#% 939897
#% 1022355
#% 1117092
#% 1127964
#% 1297079
#% 1297085
#% 1384224
#% 1707816
#! Microblogs as a new textual domain offer a unique proposition for sentiment analysis. Their short document length suggests any sentiment they contain is compact and explicit. However, this short length coupled with their noisy nature can pose difficulties for standard machine learning document representations. In this work we examine the hypothesis that it is easier to classify the sentiment in these short form documents than in longer form documents. Surprisingly, we find classifying sentiment in microblogs easier than in blogs and make a number of observations pertaining to the challenge of supervised learning for sentiment analysis in microblogs.

#index 1482448
#* Identifying hotspots on the real-time web
#@ Krishna Yeswanth Kamath;James Caverlee
#t 2010
#c 1
#% 823395
#% 972357
#% 1085750
#! We study the problem of automatically identifying ``hotspots'' on the real-time web. Concretely, we propose to identify highly-dynamic ad-hoc collections of users -- what we refer to as crowds -- in massive social messaging systems like Twitter and Facebook. The proposed approach relies on a message-based communication clustering approach over time-evolving graphs that captures the natural conversational nature of social messaging systems. One of the salient features of the proposed approach is an efficient locality-based clustering approach for identifying crowds of users in near real-time compared to more heavyweight static clustering algorithms. Based on a three month snapshot of Twitter consisting of 711,612 users and 61.3 million messages, we show how the proposed approach can efficiently and effectively identify Twitter-based crowds relative to static graph clustering techniques at a fraction of the computational cost.

#index 1482449
#* Discovery of numerous specific topics via term co-occurrence analysis
#@ Omid Madani;Jiye Yu
#t 2010
#c 1
#% 158687
#% 229348
#% 279755
#% 722904
#% 1273829
#% 1344853
#% 1385996
#% 1650298
#! We describe efficient techniques for construction of large term co-occurrence graphs, and investigate an application to the discovery of numerous fine-grained (specific) topics. A topic is a small dense subgraph discovered by a random walk initiated at a term (node) in the graph. We observe that the discovered topics are highly interpretable, and reveal the different meanings of terms in the corpus. We show the information-theoretic utility of the topics when they are used as features in supervised learning. Such features lead to consistent improvements in classification accuracy over the standard bag-of-words representation, even at high training proportions. We explain how a layered pyramidal view of the term distribution helps in understanding the algorithms and in visualizing and interpreting the topics.

#index 1482450
#* Digging for knowledge with information extraction: a case study on human gene-disease associations
#@ Markus Bundschus;Anna Bauer-Mehren;Volker Tresp;Laura Furlong;Hans-Peter Kriegel
#t 2010
#c 1
#% 1019061
#% 1072321
#% 1166537
#! We present the information extraction system Text2SemRel. The system (semi-) automatically constructs knowledge bases from textual data consisting of facts about entities using semantic relations. An integral part of the system is a graph-based interactive visualization and search layer. The second contribution in this paper is the presentation of a case study on the (semi-) automatic construction of a knowledge base consisting of gene-disease associations. The resulting knowledge base, the Literature-derived Human Gene-Disease Network (LHGDN), is now an integral part of the Linked Life Data initiative and represents currently the largest publicly available gene-disease repository. The LHGDN is compared against several curated state of the art databases. A unique feature of the LHGDN is that the semantics of the associations constitute a wide variety of biomolecular conditions.

#index 1482451
#* Towards query log based personalization using topic models
#@ Mark J. Carman;Fabio Crestani;Morgan Harvey;Mark Baillie
#t 2010
#c 1
#% 577224
#% 722904
#% 881544
#% 956552
#! We investigate the utility of topic models for the task of personalizing search results based on information present in a large query log. We define generative models that take both the user and the clicked document into account when estimating the probability of query terms. These models can then be used to rank documents by their likelihood given a particular query and user pair.

#index 1482452
#* Choosing your own adventure: automatic taxonomy generation to permit many paths
#@ Xiaoguang Qi;Dawei Yin;Zhenzhen Xue;Brian D. Davison
#t 2010
#c 1
#% 280849
#% 341672
#% 657201
#% 754124
#! A taxonomy organizes concepts or topics in a hierarchical structure and can be created manually or via automated systems. A major drawback of taxonomies is that they require users to have the same view of the topics as the taxonomy creator. Users who do not share that mental taxonomy are likely to have difficulty in finding the desired topic. In this paper, we propose a new approach to taxonomy expansion which is able to provide more flexible views. Based on an existing taxonomy, our algorithm finds possible alternative paths and generates an expanded taxonomy with flexibility in user browsing choices. In experiments on the dmoz Open Directory Project, the rebuilt taxonomies provide more alternative paths and shorter paths to information. User studies show that our expanded taxonomies are preferred compared to the original

#index 1482453
#* Robust prediction from multiple heterogeneous data sources with partial information
#@ Mohammad S. Aziz;Chandan K. Reddy
#t 2010
#c 1
#% 471586
#% 551723
#% 551745
#% 832903
#% 1131263
#% 1271973
#% 1590138
#! Significant research efforts for robust integration of information from multiple sources are being pursued at a rapid pace. However, the information in heterogeneous sources is often incomplete and hence making the maximum use of all the available information is a challenging problem. Most of the recent research on data integration have been primarily focused on the cases where the information is available across all the different sources and can not effectively integrate sources in the presence of partial information. We develop an ensemble method that boosts the decisions made from different models on individual sources and obtain robust results for the task of class prediction. We propose a heterogeneous boosting framework that uses all the available information even if some of the sources do not provide any information about some objects. We demonstrate the effectiveness of the proposed framework for the problem of gene function prediction and compare to the state-of-the-art methods using several real-world biological datasets. We also show that the proposed method outperforms any kind of imputation schemes that are widely used while integrating data with partial information

#index 1482454
#* Collaboration analytics: mining work patterns from collaboration activities
#@ Qihua Wang;Hongxia Jin;Yan Liu
#t 2010
#c 1
#% 722904
#% 956570
#% 987328
#% 1055680
#% 1292590
#! People are increasingly using more and more social softwares, generating flooding communications. User analytics may be performed to mine a person's activities on different social systems and extract patterns, be it interest patterns, social patterns, or work patterns. Such patterns may benefit both the individuals and the organizations the users associated with, as the information is valuable in numerous tasks, including recommendation, evaluation, management, and so on. In this article, we present an actionable solution of user analytics, namely collaboration analytics, by focusing on mining a person's work patterns from her collaboration activities. Our solution effectively makes use of a user's heterogeneous data collected from various collaboration tools to derive an integrated description of the user's collaborative work. A number of ``work areas'', each of which contains its work topics and people involved, are generated for every user. The challenges we face include the clustering of items with short texts and prioritizing/weighting data items based on importance/relevance. Our solutions to those issues will be described in this article. In particular, we mine users' background information from various types of data and use such information to enrich the semantics of the short texts contained in the activity instances on collaboration tools before clustering those instances into work areas. Finally, we have developed a prototype of our collaboration analytics solution and evaluated it with real-world data and people.

#index 1482455
#* Adapting cost-sensitive learning for reject option
#@ Jun Du;Eileen A. Ni;Charles X. Ling
#t 2010
#c 1
#% 280437
#% 466760
#% 477640
#% 727925
#% 876045
#% 926881
#% 961196
#% 992620
#% 1117689
#% 1250582
#% 1289281
#! Traditional cost-sensitive learning algorithms always deterministically predict examples as either positive or negative (in binary setting), to minimize the total misclassification cost. However, in more advanced real-world settings, the algorithms can also have another option to reject examples of high uncertainty. In this paper, we assume that cost-sensitive learning algorithms can reject the examples and obtain their true labels by paying reject cost. We therefore analyse three categories of popular cost-sensitive learning approaches, and provide generic methods to adapt them for reject option.

#index 1482456
#* SKIF: a data imputation framework for concept drifting data streams
#@ Peng Zhang;Xingquan Zhu;Jianlong Tan;Li Guo
#t 2010
#c 1
#% 857113
#% 918001
#% 1015261
#% 1030848
#% 1211775
#% 1292554
#% 1318705
#% 1318708
#% 1408853
#% 1520207
#! Missing data commonly occurs in many applications. While many data imputation methods exist to handle the missing data problem for large scale databases, when applied to concept drifting data streams, these methods face some common difficulties. First, due to large and continuous data volumes, we are unable to maintain all stream records to form a candidate pool and estimate missing values, as most existing methods commonly do. Second, even if we could maintain all complete stream records using a summary structure, the concept drifting problem would make some information obsolete, and thus deteriorate the imputation accuracy. Third, in data streams, it is necessary to develop a fast yet accurate algorithm to find the most similar data for imputation. Fourth, due to the dynamic and sophisticated data collection environments, the missing rate of most stream data may be much higher than that in generic static databases, so the imputation method should be able to accommodate high missing rate in the data. To tackle these challenges, we propose, in this paper, a Streaming k-Nearest-Neighbors Imputation Framework (SKIF) for concept drifting data streams. To handle concept drifting and large volume problems in data streams, SKIF first summarizes historical complete records in some micro-resources (which are high-level statistical data structures), and maintains these micro-resources in a candidate pool as benchmark data. After that, SKIF employs a novel hybrid-kNN imputation procedure, which uses a hybrid similarity search mechanism, to find the most similar micro-resources from the large scale candidate pool efficiently. Experimental results demonstrate the effectiveness of the proposed SKIF framework for data stream imputation tasks.

#index 1482457
#* Detecting controversial events from twitter
#@ Ana-Maria Popescu;Marco Pennacchiotti
#t 2010
#c 1
#% 196896
#% 939896
#% 1400111
#! Social media provides researchers with continuously updated information about developments of interest to large audiences. This paper addresses the task of identifying controversial events using Twitter as a starting point: we propose 3 models for this task and report encouraging initial results.

#index 1482458
#* Topic detection and organization of mobile text messages
#@ Ye Tian;Wendong Wang;Xueli Wang;Jinghai Rao;Canfeng Chen;Jian Ma
#t 2010
#c 1
#% 262042
#% 575570
#% 824531
#! How to organize and visualize big amount of text messages stored on one's mobile phone is a challenging problem, since they can hardly be organized by threads as we do for emails due to lack of necessary metadata such as "subject" and "reply-to". In this paper, we propose an innovative approach based on clustering algorithms and natural language processing methods. We first cluster the text messages into candidate conversations based on their temporal attributes, and then do further analysis using a semantic model based on Latent Dirichlet Allocation (LDA). Considering that the text messages are usually short and sparse, we trained the model using a large scale external data collected from twitter-like web sites, and applied the model to text messages. In the end, the text messages are organized as conversations based on their topics. We evaluated our approach based on 122,359 text messages collected from 50 university students during 6 months.

#index 1482459
#* Unsupervised public health event detection for epidemic intelligence
#@ Marco Fisichella;Avaré Stewart;Kerstin Denecke;Wolfgang Nejdl
#t 2010
#c 1
#% 262042
#% 262043
#% 613977
#% 818215
#% 987218
#% 1314743
#% 1355060
#% 1650298
#! Recent pandemics such as Swine Flu have caused concern for public health officials. Given the ever increasing pace at which infectious diseases can spread globally, officials must be prepared to react sooner and with greater epidemic intelligence gathering capabilities. However, state-of-the-art systems for Epidemic Intelligence have not kept the pace with the growing need for more robust public health event detection. In this paper, we propose a game-changing approach where public health events are detected in an unsupervised manner. We address the problems associated with adapting an unsupervised learner to the medical domain and in doing so, propose an approach which combines aspects from different feature-based event detection methods. We evaluate our approach with a real world dataset with respect to the quality of article clusters. Our results show that we are able to achieve a precision of 66% and a recall of 81% when evaluated using manually annotated, real-world data. This shows promising results for the use of such techniques in this new problem setting.

#index 1482460
#* Pattern based keyword extraction for contextual advertising
#@ Kushal S. Dave;Vasudeva Varma
#t 2010
#c 1
#% 309093
#% 420487
#% 818222
#% 855293
#% 869484
#% 1055862
#% 1273825
#! Contextual Advertising (CA) refers to the placement of ads that are contextually related to the web page content. The science of CA deals with the task of finding advertising keywords from web pages. We present a different candidate selection method to extract advertising keywords from a web page. This method makes use of Part-of-Speech (POS) patterns that restrict the number of potential candidates a classifier has to handle. It fetches words/phrases that belong to the selected set of POS patterns. We design four systems based on chunking method and the features they use. These systems are trained on a naive Bayes classifier with a set of web pages annotated with 'advertising' keywords. The systems can then find advertising keywords from previously unseen web pages. Empirical evaluation shows that systems using the proposed chunking method perform better than the systems using N-Gram based chunking. All improvements in the systems are found statistically significant at a 99% confidence interval.

#index 1482461
#* Mixture model label propagation
#@ Mingmin Chi;Xisheng He;Shipeng Yu
#t 2010
#c 1
#% 466263
#% 771841
#% 784995
#% 840967
#% 879624
#% 1455666
#! Usually, we can use a classification or clustering machine learning algorithm to manage knowledge and information retrieval. If we have a small size of known information with a large scale of unknown data, a semi-supervised learning (SSL) algorithm is often preferred. Under the cluster or manifold assumption, usually, the larger amount of unlabeled data are used for learning, the bigger gains of the SSL approaches are achieved. In the paper, we adopt the graph-based SSL algorithm to solve the problem. However the graph-based SSL algorithms are unable to be learnt with large-scale unlabeled samples and originally can only work in a transductive setting. In the paper, we propose a scalable graph-based SSL algorithm to attack the problems aforementioned by Gaussian mixture model label propagation. Experiments conducted on the real dataset illustrate the effectiveness of the proposed algorithm.

#index 1482462
#* Regularization and feature selection for networked features
#@ Hongliang Fei;Brian Quanz;Jun Huan
#t 2010
#c 1
#% 830371
#% 1060798
#% 1211744
#! In the standard formalization of supervised learning problems, a datum is represented as a vector of features without prior knowledge about relationships among features. However, for many real world problems, we have such prior knowledge about structure relationships among features. For instance, in Microarray analysis where the genes are features, the genes form biological pathways. Such prior knowledge should be incorporated to build a more accurate and interpretable model, especially in applications with high dimensionality and low sample sizes. Towards an efficient incorporation of the structure relationships, we have designed a classification model where we use an undirected graph to capture the relationship of features. In our method, we combine both L1 norm and Laplacian based L2 norm regularization with logistic regression. In this approach, we enforce model sparsity and smoothness among features to identify a small subset of grouped features. We have derived efficient optimization algorithms based on coordinate decent for the new formulation. Using comprehensive experimental study, we have demonstrated the effectiveness of the proposed learning methods.

#index 1482463
#* BagBoo: a scalable hybrid bagging-the-boosting model
#@ Dmitry Yurievich Pavlov;Alexey Gorodilov;Cliff A. Brunk
#t 2010
#c 1
#% 400847
#% 1100070
#% 1211826
#% 1268491
#! In this paper, we introduce a novel machine learning approach for regression based on the idea of combining bagging and boosting that we call BagBoo. Our BagBoo model borrows its high accuracy potential from. Friedman's gradient boosting [2], and high efficiency and scalability through parallelism from Breiman's bagging [1]. We run empirical evaluations on large scale Web ranking data, and demonstrate that BagBoo is not only showing superior relevance than standalone bagging or boosting, but also outperforms most previously published results on these data sets. We also emphasize that BagBoo is intrinsically scalable and parallelizable, allowing us to train order of half a million trees on 200 nodes in 2 hours CPU time and beat all of the competitors in the Internet Mathematics relevance competition sponsored by Yandex and be one of the top algorithms in both tracks of Yahoo ICML-2010 challenge. We conclude the paper by stating that while impressive experimental evaluation results are presented here in the context of regression trees, the hybrid BagBoo model is applicable to other domains, such as classification, and base training models.

#index 1482464
#* Exploiting sequential relationships for familial classification
#@ Lee S. Jensen;James G. Shanahan
#t 2010
#c 1
#% 136350
#% 376266
#% 464434
#% 816181
#% 833890
#% 1301004
#% 1502489
#! The pervasive nature of the internet has caused a significant transformation in the field of genealogical research. This has impacted not only how research is conducted, but has also dramatically increased the number of people discovering their family history. Recent market research (Maritz Marketing 2000, Harris Interactive 2009) indicates that general interest in the United States has increased from 45% in 1996, to 60% in 2000, and 87% in 2009. Increased popularity has caused a dramatic need for improvements in algorithms related to extracting, accessing, and processing genealogical data for use in building family trees. This paper presents one approach to algorithmic improvement in the family history domain, where we infer the familial relationships of households found in human transcribed United States census data. By applying advances made in natural language processing, exploiting the sequential nature of the census, and using state of the art machine learning algorithms, we were able to decrease the error by 35% over a hand coded baseline system. The resulting system is immediately applicable to hundreds of millions of other genealogical records where families are represented, but the familial relationships are missing.

#index 1482465
#* Massive structured data management solution
#@ Ullas Nambiar;Rajeev Gupta;Himanshu Gupta;Mukesh Mohania
#t 2010
#c 1
#% 227880
#% 273917
#% 993967
#% 1023420
#% 1217159
#! The need to analyze structured data for various business intelligence applications such as customer churn analysis, social network analysis, etc. is well known. However, the potential size to which such data will scale in future will make solutions that revolve around data warehouses hard to scale. We begin by presenting a business case that prompted us to look at building a distributed analytics platform that is leveraging the MapReduce framework pioneered by Google. We present the results of the study and highlight issues with the current structured data access techniques for MapReduce platforms. Finally, we present a distributed and scalable data platform that leverages Apache Hadoop to enable business analysts to seamlessly query archived data along with data stored in the warehouse.

#index 1482466
#* Ranking of evolving stories through meta-aggregation
#@ Juozas Gordevicius;Francisco J. Estrada;Hyun Chul Lee;Periklis Andritsos;Johann Gamper
#t 2010
#c 1
#% 268079
#% 290830
#% 387427
#% 397685
#% 728195
#% 748636
#% 788593
#% 818241
#% 879575
#% 879602
#% 912461
#% 1081574
#% 1214671
#% 1222115
#% 1328216
#! In this paper we focus on the problem of ranking news stories within their historical context by exploiting their content similarity. We observe that news stories evolve and thus have to be ranked in a time and query dependent manner. We do this in two steps. First, the mining step discovers metastories, which constitute meaningful groups of similar stories that occur at arbitrary points in time. Second, the ranking step uses well known measures of content similarity to construct implicit links among all metastories, and uses them to rank those metastories that overlap the time interval provided in a user query. We use real data from conventional and social media sources (weblogs) to study the impact of different meta-aggregation techniques and similarity measures in the final ranking. We evaluate the framework using both objective and subjective criteria, and discuss the selection of clustering method and similarity measure that lead to the best ranking results.

#index 1482467
#* Injecting domain knowledge into a granular database engine: a position paper
#@ Dominik Ślezak;Graham Toppin
#t 2010
#c 1
#% 451767
#% 576207
#% 864446
#% 893130
#% 1127565
#% 1127968
#% 1130241
#% 1217210
#% 1334336
#% 1507636
#! We discuss how to use techniques from such fields as text processing and knowledge management to better handle text attributes in the Infobright's RDBMS engine. Our approach leads to a rich interface for domain experts who wish to share their knowledge about data content and, on the other hand, it remains unnoticeable to data users. It enables to improve data storage, data access, and data compression, with no changes required at the database schema level.

#index 1482468
#* Experiences with using SVM-based learning for multi-objective ranking
#@ Linh Thai Nguyen;Wai Gen Yee;Roger Liew;Ophir Frieder
#t 2010
#c 1
#% 577224
#% 840846
#% 840882
#% 879565
#% 926881
#% 1268491
#% 1355031
#! We describe our experiences in applying learning-to-rank techniques to improving the quality of search results of an online hotel reservation system. The search result quality factors we use are average booking position and distribution of margin in top-ranked results. (We expect that total revenue will increase with these factors.) Our application of the SVMRank technique improves booking position by up to 25% and margin distribution by up to 14%.

#index 1482469
#* Learning to blend rankings: a monotonic transformation to blend rankings from heterogeneous domains
#@ Zhenzhen Kou;Yi Chang;Zhaohui Zheng;Hongyuan Zha
#t 2010
#c 1
#% 330769
#% 411762
#% 564279
#% 577224
#% 840846
#% 956542
#% 983820
#% 983825
#% 987241
#% 1074083
#% 1355031
#! There have been great needs to develop effective methods for combining multiple rankings from heterogeneous domains into one single rank list arising from many recent web search applications, such as integrating web search results from multiple engines, facets, or verticals. We define this problem as Learning to blend rankings from multiple domains. We propose a class of learning-to-blend methods that learn a monotonically increasing transformation for each ranking so that the rank order in each domain is preserved and the transformed values are comparable across multiple rankings. The transformation learning can be tackled by solving a quadratic programming problem. The novel machine learning method for blending multiple ranking lists is evaluated with queries sampled from a commercial search engine and a promising improvement of Discounted Cumulative Gain has been observed.

#index 1482470
#* EntityEngine: answering entity-relationship queries using shallow semantics
#@ Xiaonan Li;Chengkai Li;Cong Yu
#t 2010
#c 1
#% 869535
#% 956564
#% 1022234
#% 1355028
#! We introduce EntityEngine, a system for answering entity-relationship queries over text. Such queries combine SQL-like structures with IR-style keyword constraints and therefore, can be expressive and flexible in querying about entities and their relationships. EntityEngine consists of various offline and online components, including a position-based ranking model for accurate ranking of query answers and a novel entity-centric index for efficient query evaluation.

#index 1482471
#* Facetedpedia: enabling query-dependent faceted search for wikipedia
#@ Ning Yan;Chengkai Li;Senjuti B. Roy;Rakesh Ramegowda;Gautam Das
#t 2010
#c 1
#% 857482
#% 1399998
#! Facetedpedia is a faceted search system that dynamically discovers query-dependent faceted interfaces for Wikipedia search result articles. In this paper, we give an overview of Facetedpedia, present the system architecture and implementation techniques, and elaborate on a demonstration scenario.

#index 1482472
#* Discovering, ranking and annotating cross-document relationships between concepts
#@ Wei Jin;Xin Wu
#t 2010
#c 1
#% 1032042
#% 1055870
#% 1117013
#! This paper presents CDRMiner, a system for automatically discovering, ranking and annotating cross-document links between concepts. Specifically, we focus on detecting hidden associations between two concepts and further generating annotations for each discovered hypothesis. We interpret such a relationship query as finding the most meaningful concept chains and evidence trails across multiple documents that potentially connect them. These functionalities are implemented using an interactive visualization paradigm which assists users for a better understanding and interpretation of discovered hypotheses, and matching their domain knowledge with the algorithmic power of text mining techniques.

#index 1482473
#* WikiPop: personalized event detection system based on Wikipedia page view statistics
#@ Marek Ciglan;Kjetil Nørvåg
#t 2010
#c 1
#% 252328
#% 1269107
#% 1443432
#! In this paper, we describe WikiPop service, a system designed to detect significant increase of popularity of topics related to users' interests. We exploit Wikipedia page view statistics to identify concepts with significant increase of the interest from the public. Daily, there are thousands of articles with increased popularity; thus, a personalization is in order to provide the user only with results related to his/her interest. The WikiPop system allows a user to define a context by stating a set of Wikipedia articles describing topics of interest. The system is then able to search, for the given date, for popular topics related to the user defined context.

#index 1482474
#* XReal: an interactive XML keyword searching
#@ Zhifeng Bao;Jiaheng Lu;Tok Wang Ling
#t 2010
#c 1
#% 810052
#% 1206957
#! Keyword search over XML data usually brings irrelevant results especially when the keywords in a user query have ambiguities. We demonstrate a statistic-based approach to identify the search targets and constraints of a user query in the presence of keyword ambiguities, and come out a relevance oriented result ranking scheme called XML TF*IDF. Since the search intention of a same query may even vary from user to user, we provide an interactive search strategy by allowing user to simply tick their desired search targets from a list of suggestions recommended by the search engine. In this way, we can acquire more precise results and also take the burden of learning the schema of XML data off users.

#index 1482475
#* EUI: an embedded engine for understanding user intents from mobile devices
#@ JongWoo Ha;Jung-Hyun Lee;Kyu-Sun Shim;SangKeun Lee
#t 2010
#c 1
#! We design and implement a novel embedded software engine, called EUI, to understand user intents from usage data within mobile devices. By developing the EUI engine in mobile devices, we expect to move towards "proactive" devices for mobile personalized services. To this end, we seek to embed the Open Directory Project (ODP) into mobile devices, and build a robust classifier with the embedded ODP. Thus, the EUI engine classifies the usage data within mobile devices into some ODP categories. Our implementation handles some challenging issues in embedding the ODP and building a robust classifier. The demonstration shows that our implementation understands the semantics of the usage data effectively.

#index 1482476
#* TC-DCA: a system for text classification based on document's content allocation
#@ Wenbo Li;Le Sun;Zhenzhong Zhang;Xue Jiang;Weiru Zhang
#t 2010
#c 1
#% 458379
#% 722904
#! The text classification methods heavily depend on machine learning algorithms with abstract mathematic metrics, which obstruct the direct observation and intuitive understanding of the text-specific classification. In this paper, we model a document as a Document-Classes-Topics top-down hierarchical structure. Furthermore, by running the document generation procedure, we can obtain each class's content share, which not only can be used to make the classification decision but also can provide a natural visualization approach for text classification. We implement this idea by a new tool named TC-DCA, which provides the visualization of text classification result, where the target document is expressed graphically as its content's allocation on every class. TC-DCA can also perform the drilling down operation to reveal the classification effect of each word of the document.

#index 1482477
#* Crawling the web for structured documents
#@ Julián Urbano;Juan Loréns;Yorgos Andreadakis;Mónica Marrero
#t 2010
#c 1
#% 445848
#% 879646
#% 1292778
#! Structured Information Retrieval is gaining a lot of interest in recent years, as this kind of information is becoming an invaluable asset for professional communities such as Software Engineering. Most of the research has focused on XML documents, with initiatives like INEX to bring together and evaluate new techniques focused on structured information. Despite the use of XML documents is the immediate choice, the Web is filled with several other types of structured information, which account for millions of other documents. These documents may be collected directly using standard Web search engines like Google and Yahoo, or following specific search patterns in online repositories like SourceForge. This demo describes a distributed and focused web crawler for any kind of structured documents, and we show with it how to exploit general-purpose resources to gather large amounts of real-world structured documents off the Web. This kind of tool could help building large test collections of other types of documents, such as Java source code for software-oriented search engines or RDF for semantic searching.

#index 1482478
#* Connecting the local and the online in information management
#@ Gabriella Kazai;Natasa Milic-Frayling;Tim Haughton;Natalia Manola;Katerina Iatropoulou;Antonis Lempesis;Paolo Manghi;Marko Mikulicic
#t 2010
#c 1
#% 1157483
#% 1183099
#% 1495115
#! With the popularity of social media sites, digital content is increasingly stored and managed online. At the same time, the desktop and local storage continues to provide a personal environment in which users perform their daily tasks. Thus, to accomplish their tasks, users need to continuously switch between local and remote resources and applications, often carrying the burden of coordinating and synchronizing these in a consistent way. In this demonstration, we describe a system, called ScholarLynk, that bridges the local and online worlds and allows users to manage both local and online resources in a uniform way and in collaboration with others.

#index 1482479
#* LiquidXML: adaptive XML content redistribution
#@ Jesús Camacho-Rodríguez;Asterios Katsifodimos;Ioana Manolescu;Alexandra Roatis
#t 2010
#c 1
#% 210182
#% 654452
#% 1034835
#! We propose to demonstrate LiquidXML, a platform for managing large corpora of XML documents in large-scale P2P networks. All LiquidXML peers may publish XML documents to be shared with all the network peers. The challenge then is to efficiently (re-)distribute the published content in the network, possibly in overlapping, redundant fragments, to support efficient processing of queries at each peer. The novelty of LiquidXML relies in its adaptive method of choosing which data fragments are stored where, to improve performance. The "liquid" aspect of XML management is twofold: XML data flows from many sources towards many consumers, and its distribution in the network continuously adapts to improve query performance.

#index 1482480
#* Brown dwarf: a P2P data-warehousing system
#@ Katerina Doka;Dimitrios Tsoumakos;Nectarios Koziris
#t 2010
#c 1
#% 397388
#% 453194
#% 1127579
#% 1464886
#! In this demonstration we present the Brown Dwarf, a distributed system designed to efficiently store, query and update multidimensional data. Deployed on any number of commodity nodes, our system manages to distribute large volumes of data over network peers on-the-fly and process queries and updates on-line through cooperating nodes that hold parts of a materialized cube. Moreover, it adapts its resources according to demand and hardware failures and is cost-effective both over the required hardware and software components. All the aforementioned functionality will be tested using various datasets and query loads.

#index 1482481
#* RDFViewS: a storage tuning wizard for RDF applications
#@ François Goasdoué;Konstantinos Karanasos;Julien Leblay;Ioana Manolescu
#t 2010
#c 1
#% 397432
#% 1022236
#% 1127402
#% 1127431
#% 1217194
#% 1250923
#! In recent years, the significant growth of RDF data used in numerous applications has made its efficient and scalable manipulation an important issue. In this paper, we present RDFViewS, a system capable of choosing the most suitable views to materialize, in order to minimize the query response time for a specific SPARQL query workload, while taking into account the view maintenance cost and storage space constraints. Our system employs practical algorithms and heuristics to navigate through the search space of potential view configurations, and exploits the possibly available semantic information - expressed via an RDF Schema - to ensure the completeness of the query evaluation.

#index 1482482
#* WS-GraphMatching: a web service tool for graph matching
#@ Qiong Cheng;Mitsunori Ogihara;Jinpeng Wei;Alexander Zelikovsky
#t 2010
#c 1
#% 408396
#% 629708
#% 867880
#% 1139397
#! Some emerging applications deal with graph data and relie on graph matching and mining. The service-oriented graph matching and mining tool has been required. In this demo we present the web service tool WS-GraphMatching which supports the efficient and visualized matching of polytrees, series-parallel graphs, and arbitrary graphs with bounded feedback vertex set. Its embedded matching algorithms take in account the similarity of vertex-to-vertex and graph structures, allowing path contraction, vertex deletion, and vertex insertions. It provides one-to-one matching queries as well as queries in batch modes including one-to-many matching mode and many-to-many matching mode. It can be used for predicting unknown structured information, comparing and finding conserved patterns, and resolving ambiguous identification of vertices. WS-GraphAligner is available as web-server at: http://odysseus.cs.fiu.edu:8080/MinePW/pages/gmapping/GMMain.html.

#index 1482483
#* FALCON: seamless access to meeting data from the inbox and calendar
#@ Peter Bjellerup;Karl J. Cama;Mukundan Desikan;Yi Guo;Ajinkya G. Kale;Jennifer C. Lai;Nizar Lethif;Jie Lu;Mercan Topkara;Stephan H. Wissel
#t 2010
#c 1
#! We present a system that supports seamless access to information contained in recorded meetings from the cornerstone points of a knowledge worker's daily life: mailbox and calendar. The solution supports granular search of meeting content from an enterprise email system and automatically displays recordings of meetings related to the message the user is currently viewing. Additionally thumbnail summaries of the meetings are added to the user's calendar entries after the meetings have taken place. Lastly our system supports easy sharing of videos associated with recorded meetings through the use of hot-linked thumbnail summaries which can be sent via email.

#index 1482484
#* SPac: a distributed, peer-to-peer, secure and privacy-aware social space
#@ Angela Bonifati;Hui (Wendy) Wang;Ruilin Liu
#t 2010
#c 1
#% 319849
#% 960253
#% 1246361
#% 1328097
#% 1486150
#! To support privacy-aware management of data in social spaces, the user personal data needs to be stored at each user device, and shared only with a trusted subset of other users. To date, social spaces only have fairly limited access control capabilities, that do not protect the possibly sensitive data of the users. In this demonstration, we showcase our SPAC system, a distributed, peer-to-peer, secure and privacy-aware social space system. SPAC is equipped with: (i) an SQL-based declarative distributed query language to specify which data to share and whom to share with. Such a language guarantees the fine-grained access to the data, (ii) a fully-decentralized authorization that relies on classic cryptographic protocols to provide robust and resilient key-based encryption for access control enforcement, and (iii) an update-friendly access control mechanism, that also addresses the updates on both the network and the access control policies.

#index 1482485
#* SEQUEL: query completion via pattern mining on multi-column structural data
#@ Chuancong Gao;Qingyan Yang;Jianyong Wang
#t 2010
#c 1
#% 464996
#% 879610
#% 964841
#% 1026924
#% 1130854
#% 1190092
#! In this demonstration, we propose an interactive query completion system on structural data like DBLP, called SEQUEL. It is novel in several aspects: with patterns mined on the structural data using newly devised algorithm, SEQUEL offers high-utility completions composed with not only words but also phrases, and requires no explicit indications of corresponding columns. Instead of using query logs exploited previously for unstructured data, more effective completions are provided based on patterns mined directly from the records. Moreover, an effective index structure helps SEQUEL respond fast at millisecond level for each keystroke.

#index 1482486
#* MI-WDIS: web data integration system for market intelligence
#@ Zhongmin Yan;Qingzhong Li;Shidong Zhang;Zhaohui Peng;Yongquan Dong;Yanhui Ding;Yongxin Zhang;Xiuxing Xu
#t 2010
#c 1
#% 1491825
#% 1512592
#% 1545579
#! As an important supporting technology of Market Intelligence (MI), Web data integration is facing new challenges, such as the integrity of data acquisition, the quality of data extraction and data consolidation. To solve such problems, we propose an MI-oriented web data integration system (MI-WDIS), which achieves excellent performances in integrating Surface Web and Deep Web data with much less manual work. Based on MI-WDIS, we have developed a platform for intelligent analysis of job data. The platform collects tens of thousands of job data daily and provides personalized services for job seekers through diversified channels. Besides, it provides other advanced services, including intelligence analysis, automatic monitoring and alerting, for various organizations, such as enterprises, training institutions and recruitment agencies.

#index 1482487
#* i-SEE: integrated stream execution environment over on-line data streams
#@ Se Jung Shin;Hong Kyu Park;Ho Jin Woo;Won Suk Lee
#t 2010
#c 1
#% 729959
#% 765494
#% 1063555
#% 1131041
#% 1170840
#! The primary objective of various data stream applications is to monitor the on-going variations of data elements newly generated in data streams, so that appropriate services can be delivered to users timely. Basically, such monitoring activities can be divided into three categories: instant event monitoring, frequent behavior monitoring and analytic OLAP measures monitoring. Several prototype systems for data streams have been introduced but they are designated to support only the operations of one or two categories. However, many real-world data stream applications often require mixed operations of the three categories. This demonstration introduces an Integrated Stream Execution Environment (i-SEE) that can support the mixed execution of the operations of the three categories seamlessly, so that an i-SEE system can provide the multifaceted analytic results of on-line data streams continuously for various emerging applications.

#index 1482488
#* Summarizing biological literature with BioSumm
#@ Elena Baralis;Alessandro Fiori
#t 2010
#c 1
#% 321635
#% 905819
#% 1132061
#! BioSumm is a summarization environment that supports user queries on online repositories of scientific publications by providing abstract descriptions of focused document groups. The summarization approach is driven by a grading function which evaluates the occurrences of domain dictionary terms. The demonstrated system enables users to query and download research papers from online databases (e.g., PubMed) and local repositories. The (possibly large) retrieved document collection is then partitioned into document clusters devoted to homogeneous topics. Finally, documents in a cluster are summarized by extracting sentences relevant for a specific application domain. In the demo the considered domain is the interaction of human genes and proteins.

#index 1482489
#* Exploring and visualizing academic social networks
#@ Veselin Ganev;Zhaochen Guo;Diego Serrano;Denilson Barbosa;Eleni Stroulia
#t 2010
#c 1
#% 451514
#% 937552
#% 1082200
#% 1290798
#! We demonstrate the ReaSoN portal, consisting of interactive web-based tools for visualizing, exploring, querying, and integrating academic social networks. We describe how these networks are automatically extracted from bibliographic and citation databases, discuss notions of visibility in such networks which enable a rich set of social network analysis, and demonstrate our novel tools for the visualization and exploration of social networks.

#index 1482490
#* Summary of the 4th workshop on analytics for noisy unstructured text data (AND)
#@ Roberto Basili;Daniel Lopresti;Christoph Ringlestetter;Shourya Roy;Klaus U. Schulz;L. Venkata Subramaniam
#t 2010
#c 1

#index 1482491
#* 3rd BooksOnline workshop: research advances in large digital book repositories and complementary media
#@ Gabriella Kazai;Peter Brusilovsky
#t 2010
#c 1
#% 1131070
#% 1482491
#% 1482550
#% 1482552
#% 1482553
#% 1482554
#% 1482555
#% 1482556
#% 1482557
#% 1482558
#% 1482559
#% 1482560
#! The goal of the 3rd BooksOnline Workshop is to bring together researchers and industry practitioners in information retrieval, digital libraries, e-books, human computer interaction, publishing industry, and online book services to foster progress on addressing challenges and exploring opportunities around large collections of digital books and complementary media. Towards this goal, the workshop programme consists of contributions both from academia and industry, including two keynote talks: James Crawford from Google Books and John Mark Ockerbloom from the University of Pennsylvania.

#index 1482492
#* Report on the second international workshop on cloud data management (CloudDB 2010)
#@ Xiaofeng Meng;Ying Chen;Jiaheng Lu;Jianliang Xu
#t 2010
#c 1

#index 1482493
#* DTMBIO workshop summary
#@ Hagit Shatkay;Doheon Lee;Min Song;Shamkant Navathe
#t 2010
#c 1

#index 1482494
#* DOLAP 2010 workshop summary
#@ Carlos Ordonez;Il-Yeol Song
#t 2010
#c 1
#% 1482617
#% 1482618
#% 1482619
#% 1482620
#% 1482622
#% 1482623
#% 1482624
#% 1482625
#% 1482626
#% 1482627
#% 1482628
#% 1482629
#% 1482630
#% 1482631
#% 1482632
#! The ACM DOLAP workshop presents research on data warehousing and On-Line Analytical Processing (OLAP). The program has three interesting sessions on modeling, query processing and new trends, as well as a keynote talk on OLAP query processing and a panel comparing relational and non-relational technology for data warehousing.

#index 1482495
#* Third workshop on exploiting semantic annotations in information retrieval (ESAIR): CIKM 2010 workshop
#@ Jaap Kamps;Jussi Karlgren;Ralf Schenkel
#t 2010
#c 1
#% 1482636
#% 1482637
#% 1482638
#% 1482639
#% 1482640
#% 1482641
#% 1482642
#% 1482643
#% 1482644
#% 1482645
#% 1482646
#% 1482647
#% 1482648
#% 1482649
#% 1482650
#% 1482651
#! There is an increasing amount of structure on the Web as a result of modern Web languages, user tagging and annotation, and emerging robust NLP tools. These meaningful, semantic, annotations hold the promise to significantly enhance information access, by enhancing the depth of analysis of today's systems. Currently, we have only started exploring the possibilities and only begin to understand how these valuable semantic cues can be put to fruitful use. Unleashing the potential of semantic annotations requires us to think outside the box, by combining the insights of natural language processing (NLP) to go beyond bags of words, the insights of databases (DB) to use structure efficiently even when aggregating over millions of records, the insights of information retrieval (IR) in effective goal-directed search and evaluation, and the insights of knowledge management (KM) to get grips on the greater whole. The Workshop aims to bring together researchers from these different disciplines and work together on one of the greatest challenges in the years to come. The desired result of the workshop will be concrete insight into the potential of semantic annotations, and in concrete steps to take this research forward; synchronize related research happening in NLP, DB, IR, and KM, in ways that combine the strengths of each discipline; and have a lively, interactive workshop were everyone contributes and that inspires attendees to think "outside the box".

#index 1482496
#* 3rd international workshop on patent information retrieval (PaIR'10)
#@ Mihai Lupu;John Tait;Katja Mayer;Christopher Harris
#t 2010
#c 1
#! The 3rd International Workshop on Patent Information Retrieval builds on the experiences of the first two workshops, to provide its participants an exciting, scientifically challenging and interactive event, where the specific issues of patent retrieval may be put into the general context of Information Retrieval and Knowledge Management, in order to explore innovative solutions to new and old problems, but also to evaluate and adapt traditional or classic approaches to new problems. Between the scientific presentations and posters, distinguished keynote speakers and a panel discussion, PaIR 2010 shapes itself into a significant landmark in the field of domain specific information retrieval.

#index 1482497
#* PIKM 2010: ACM workshop for ph.d. students in information and knowledge management
#@ Anisoara Nica;Aparna S. Varde
#t 2010
#c 1
#% 1016295
#% 1077041
#% 1301009
#% 1482497
#% 1482586
#% 1482587
#% 1482588
#% 1482589
#% 1482590
#% 1482591
#% 1482592
#% 1482593
#% 1482594
#% 1482595
#% 1482596
#% 1482598
#% 1482599
#% 1482600
#! The PIKM workshop focuses on papers consisting mainly of the Ph.D. dissertation proposals of doctoral students. A wide range of topics on any area in databases, information retrieval and knowledge management are presented at this workshop. The areas of interest are similar to those at the CIKM main conference in the three respective tracks. Interdisciplinary work across these tracks is encouraged.

#index 1482498
#* Overview of the 2nd international workshop on search and mining user-generated contents
#@ Jose Carlos Cortizo;Francisco M. Carrero;Ivan Cantador;Jose Antonio Troyano;Paolo Rosso
#t 2010
#c 1
#! This overview introduces the aim of the SMUC 2010 workshop, as well as the list of papers presented in the workshop.

#index 1482536
#* Proceedings of the fourth workshop on Analytics for noisy unstructured text data
#@ Roberto Basili;Daniel Lopresti;Christoph Ringlstetter;Shourya Roy;Klaus U. Schulz;L. Venkata Subramaniam
#t 2010
#c 1
#! Thank you for attending AND 2010! The Fourth workshop in the AND series contains highquality work spanning a large array of disciplines related to the treatment of Noisy text. This year AND is collocated with CIKM so the flavor of IR and KM shows through in the AND program as well. The final program consists of 11 papers selected from 21 submissions. Each paper was carefully reviewed by three Program Committee members. We would like to thank our Program Committee for selecting this high-quality program for AND 2010. Papers in which a student is the primary author (first author/presenter) will be eligible for the IAPR Best Student Paper Award. The initial short listing for this award has been done based on the reviews. The final decision will be made during the workshop based on the presentations. Also the best papers from the workshop will appear in a special issue of the International Journal on Document Analysis and Recognition after going through further reviewing. The selection for this is based on the reviews of the AND PC members. We are excited to have Randy Geobel, University of Alberta, Canada, as the key note speaker. His talk is very interestingly titled "The nature of noise in linguistic corpora."

#index 1482549
#* Proceedings of the third workshop on Research advances in large digital book repositories and complementary media
#@ Gabriella Kazai;Peter Brusilovsky
#t 2010
#c 1
#! It is our great pleasure to welcome you to the BooksOnline'10 Workshop on Research Advances in Large Digital Book Repositories and Complementary Media. In its third year, the workshop series is gaining increased momentum benefiting from the rapid advance of digital books and eReaders, encouraging research in several related areas. The aim of the BooksOnline workshops is to bring together researchers and industry practitioners in information retrieval, digital libraries, ebooks, human computer interaction, publishing industry, and on-line book services to foster progress on addressing challenges and exploring opportunities around large collections of digital books and complementary media. This year's workshop continues to provide a forum to follow community progress, share results, highlight and address issues, and evolve the research agenda. BooksOnline'10 boasts a high quality programme, including keynote addresses by James Crawford from Google Books and by John Mark Ockerbloom from the University of Pennsylania. From the accepted papers two main themes emerged: 1) Issues relating to building suitable infrastructure for online book collections and for the evaluation of the applications and services that make use of such collections, and 2) Opportunities relating to the social or collaborative reading and annotation of books and issues relating to the design of suitable eReaders. We hope that these proceedings will serve as a valuable reference for researchers and practitioners.

#index 1482561
#* Proceedings of the ACM fourth international workshop on Data and text mining in biomedical informatics
#@ Doheon Kim;Hagit Shatkay;Min Song;Shamkant Navathe
#t 2010
#c 1
#! It is our great pleasure to welcome you to the ACM Fourth International Workshop on Data and Text Mining in Biomedical Informatics (DTMBIO'10), in conjunction with the 19th ACM International Conference on Information and Knowledge Management (CIKM'10). The rapid development of bioinformatics techniques is tightly coupled with development within several fields in computer science, including data mining, information retrieval, and database management systems, among others. A fundamental topic of research within bioinformatics is how to make effective use of the tremendous amount of biological and biomedical data to improve the understanding of biological systems. Such data include, but are not limited to, gene and protein sequences, gene expression profiles from microarray experiments, protein structure predictions resulting from high-throughput computational methods, proteinprotein interactions from proteomic studies, single nucleotide polymorphisms profiles from SNP arrays, and much bibliographic information from electronic medical journals. The need to automatically and effectively extract, understand, integrate, and make use of information embedded in such heterogeneous unstructured data drives the current research in bioinformatics. This year's workshop continues the tradition of bringing together researchers that work in the field of data mining, text mining, and computational biology and providing a forum to present and discuss current research topics at the interface of the related fields. The mission of DTMBIO is to promote a tighter connection between literature search and data analysis within biomedical informatics. In particular, this year we focus on the following two themes: (1) Data and text mining for biomedical applications, and the identification of relevant background knowledge in database annotations or in text documents, such as scientific publications.. And (2) Knowledge discovery through the integration of heterogeneous biomedical data collected from electronic bulletin boards, scientific publications, and any type of experiments. Furthermore, we put more focus this year on the integration of bioinformatics and medical informatics toward translational research. The papers accepted for presentation and publication in this volume cover a variety of topics, including bio-text mining, translational bioinformatics, systems bioinformatics, bio-ontology management, sequence analysis for massively parallel sequencing, protein-protein interactions, biomedical data classification, and biomedical information retrieval. We hope that these proceedings will serve as a valuable and up-to-date reference about the application of data- and text-mining techniques within biomedical informatics.

#index 1482574
#* Proceedings of the 3rd international workshop on Patent information retrieval
#@ John Tait;Christopher Harris;Mihai Lupu
#t 2010
#c 1
#! On behalf of the PaIR workshop organizing committee, we welcome you to the 3rd workshop on Patent Information Retrieval (PaIR'10), organized by the Information Retrieval Facility (IRF) and the University of Iowa. Previous PaIR workshops were held in Napa Valley, California (PaIR'08) and Hong Kong (PaIR'09). This year's workshop continues our examination of many of the most challenging aspects of patent-related information retrieval. Despite the enormous recent progress in Information Retrieval techniques, advanced search tools for patent professionals are yet still in the early stages of development - thus, the research in patent retrieval discussed here today may become key components in the patent search tools of tomorrow. Patents are not only crucial in protecting intellectual property but also serve as a strategic business factor in all modern economies. Patent search is a particular challenge to information retrieval and access systems for many reasons that are obvious and some reasons that are far more subtle. Looking forward, successful patent search systems of the future will need to address the following aspects: a vast amount of highly-complex structured documents; a highly heterogeneous document collection (scientific papers, legal public disclosure as well as patents); multiple languages; ambiguous and conflicting technical jargon; complex technological concepts; sophisticated legal jargon; harmonization issues between patent-issuing bodies; evaluation of numerical ranges and other complex query types; tracking temporal issues like publication data and patent priority dates; tabular and graphical information embedded and referred to through placeholders in the patent text; and many others. The objective of this workshop is to provide a forum for Information Retrieval and Knowledge Management scientists, as well as Patent Retrieval experts from industry to exchange ideas, discuss the state-of-the-art and to study the next generation of patent search tools. This year the workshop received 11 submissions, from which 4 full papers were accepted. Furthermore 5 papers have been invited to do a short presentation and display posters, especially to generate discussion on future directions of Patent Information Retrieval. All submitted papers for this year's workshop were of high quality, demonstrating considerable merit in the field of Patent IR. The 4 full papers cover some of the most significant issues in Patent IR. Linda Andersson's A Vector Space Analysis of Swedish Patent Claims with Different Linguistic Indices examines if the vector space model can be used to retrieve Swedish patent claims. In Genre and Domain in Patent Texts, Oostdijk et al apply an "aboutness"-based dependency parser to the text mining of patents. Next, in Preliminary Study into Query Translation for Patent Retrieval, Jochim et al, investigates the use of query term translation for multi-lingual retrieval. The authors use both generic dictionaries and dictionaries extracted from the patent corpus. Finally, in Search for Patents Using Treatment and Causal Relationships, Krishnan et al present the way linguistic methods can be used to find relationships in patents in order to improve the recall and precision of search results. The authors apply modifications they bring to an already known corpus harvesting algorithm in order to improve retrieval results. In addition to the 4 full papers, 5 research groups will describe their ideas in a booster session, followed by poster presentations during the workshop's break. The topics covered by these papers cover patent classification and categorization, translation, natural language processing and, of course, new ways of performing prior art search. We hope that these papers will initiate interesting conversations and spark future development in intellectual property search, fostering further collaboration between researchers and industry representatives. Our primary goal for this workshop is to serve as a springboard for many future discussions and research in Patent IR. Additionally, we hope that it will lead to the recognition of patent searching and retrieval as one of the fundamental areas of research in and application of Information Retrieval and Knowledge Management.

#index 1482585
#* Proceedings of the 3rd workshop on Ph.D. students in information and knowledge management
#@ Anisoara Nica;Aparna S. Varde
#t 2010
#c 1
#! Welcome to PIKM 2010, the 3rd Ph.D. Workshop on Information and Knowledge Management, held in conjunction with ACM CIKM 2010 in Toronto, Canada. PIKM 2010 is a forum for doctoral students in databases, information retrieval or knowledge management to present their preliminary work and get feedback from experienced researchers. This workshop builds on the success of the PIKM 2007 and PIKM 2008 workshops, held in conjunction with ACM CIKM 2007 in Lisboa, Portugal, and ACM CIKM 2008 in Napa Valley, California, USA, respectively. The call for papers for PIKM 2010 attracted 23 submissions from 11 countries across Asia, Europe, and North America. The program committee accepted 10 high quality submissions as full papers keeping a good balance of the three tracks of databases, information retrieval and knowledge management. Considering that this is a Ph.D. workshop with research at an early stage, we wanted to give some more students a chance to present their work and get feedback. Accordingly, another 7 submissions were accepted as short poster papers covering a variety of areas. The topics in the full and poster papers included database systems, text mining, semantic web, social networks, and human computer interaction. Short review articles based on the outcomes of PIKM 2007 and PIKM 2008 were published in ACM SIGIR Forum 2008 and ACM SIGKDD Explorations 2009 respectively. Likewise, we expect to submit a review article based on the outcome of PIKM 2010 to a suitable ACM venue in 2011. Moreover, this year we have a special journal issue dedicated to the expanded versions of selected papers presented at PIKM 2010, in the International Journal of Knowledge and Web Intelligence (IJKWI). The papers will be selected by a committee of guest editors through a highly competitive double blind review process. We hope that the PIKM 2010 Ph.D. workshop will serve its purpose of providing the participants the opportunity to showcase their research with helpful feedback. We also hope that this forum will enable the sharing of ideas with fellow students and researchers from around the world leading to further interesting work.

#index 1482607
#* Proceedings of the second international workshop on Cloud data management
#@ Xiaofeng Meng;Ying Chen;Jianliang Xu;Jiaheng Lu
#t 2010
#c 1
#! It is our great pleasure to welcome you to the Second International Workshop on Cloud Data Management (CloudDB 2010). Cloud computing is set to revolutionize the IT industry, and database services are at the core of cloud computing since most information systems are built on database services. Cloud data management holds many promises and challenges. CloudDB 2010 is intended to address the challenges of large-scale data management based on cloud computing infrastructure. This workshop brings together researchers and practitioners in cloud computing and data-intensive system designs, parallel algorithms, data management, scientific applications, and information-based applications to maximize performance, minimize cost and improve the scale of their endeavors. This workshop attracted 11 submissions from Asia, Europe, and North America. Due to the high quality of the submissions received, the program committee decided to accept 8 full papers. These papers cover a variety of topics, including cloud data security, cloud data management and cloud system applications. We hope that they will serve as a valuable starting point for much brilliant thinking in cloud data management. Paper "Towards a Data-centric View of Cloud Security" discusses data management challenges in the areas of secure distributed query processing, system analysis and forensics, and query correctness assurance. Zhou et al. proposed a data-centric view of cloud security. Paper "ESQP: An Efficient SQL Query Processing for Cloud Data Management" by Zhao et al. proposed an efficient algorithm about query processing on structured data. They also demonstrated the efficiency and scalability of the algorithm with kinds of experiments. In the paper "Benchmarking Cloud-based Data Management Systems", Shi et al. conducted comprehensive experiments on several representative cloud-based data management systems to explore relative performance of different implementation approaches. In the paper "Towards Bipartite Graph Data Management", Zhao et al. fully studied the BGDM and developed a logic graph structure for indexing bipartite graphs to improve common operations efficiently. In "Contract-based Cloud Architecture", Alnemr et al. studied several issues such as security and legality that should be considered before entering the cloud. They showed that companies have to comply with diverse laws across jurisdictions and are accountable to various national regulators. Security requirements may not be compatible with those offered by existing providers. In the paper "Comparing SQL and MapReduce to Compute Naive Bayes in a Single Table Scan", Ordonez et al. presented some novel techniques to handle the data mining which is performed on flat files inside the DBMS, and considered two phases of the classifier: building the model and scoring a new data set. In the paper "Adaptive Query Execution for Data Management in the Cloud", Popescu et al. focused on analyzing the "light" queries processed on the cloud by using an adaptive scheme, which uses a cost model to switch between MapReduce and a DBMS. In the paper "Dynamic Data Replication through Virtualization", Daudjee et al. investigated data replication in a virtualized environment, focused on how to provision services when the master database server is heavily loaded or when it fails.

#index 1482616
#* Proceedings of the ACM 13th international workshop on Data warehousing and OLAP
#@ Il-Yeol Song;Carlos Ordonez
#t 2010
#c 1
#! It is our great pleasure to welcome you to the 13th ACM International Workshop on Data Warehousing and OLAP -- DOLAP 2010. The DOLAP workshop continues its tradition of being the premier forum where both researchers and practitioners in data warehousing and On-Line Analytical Processing (OLAP) share their findings in theoretical foundations, current methodologies, new trends and practical experiences. The mission of the DOLAP workshop is to identify and explore new directions for future research and development, as well as emerging application domains in the areas of data warehousing and OLAP. In recent years, research in these areas have addressed many topics, ranging from conceptual-level and methodological issues, which help designers to build effective decision-support applications, to physicallevel and query processing issues, aiming at increasing the performance of these applications in order to deal with vast amounts of data. However, the successful use of data warehousing and OLAP technologies within organizations brings up new requirements and research opportunities, in particular to cope with nontraditional application domains, such as text, biological, imaging, and spatio-temporal applications. The call for papers attracted 27 submissions, from 16 different countries and 4 continents. After careful review and discussion, the program committee accepted 8 full papers and 6 short papers, for a competitive acceptance rate of 29% for full papers and about 50% overall. The accepted papers cover a wide variety of topics, including conceptual modeling, multidimensional design, query processing, exploiting new hardware and privacy, among other themes. Papers are grouped into three sessions covering modeling, query processing and new trends. This year we also have an invited keynote talk from the DBMS industry on advanced query processing and a thought-provoking panel discussing and comparing relational and nonrelational technology to manage and analyze data warehouses.

#index 1482633
#* Proceedings of the third workshop on Exploiting semantic annotations in information retrieval
#@ Jaap Kamps;Jussi Karlgren;Ralf Schenkel
#t 2010
#c 1
#! These proceedings contain the invited talks and contributed posters of the Third Workshop on Exploiting Semantic Annotations in Information Retrieval (ESAIR 2010), held at CIKM 2010, Toronto Canada on October 30, 2010. After successful workshops at ECIR 2008 in Glasgow, and at WSDM 2009 in Barcelona, both chaired by Omar Alonso and Hugo Zaragoza, this year's workshop will focus on the problem from the wider CIKM perspective. Unleashing the potential of semantic annotations requires us to transcend earlier technologies, by combining the insights of natural language processing (NLP) to go beyond bags of words, the insights of databases (DB) to use structure efficiently even when aggregating over millions of records, the insights of information retrieval (IR) in effective goaldirected search and evaluation, and the insights of knowledge management (KM) to get grips on the greater whole. ESAIR 2010 will be a real workshop where researchers from these different disciplines will work together to identify natural use cases, barriers to success, and work on ways of addressing them: Use Cases: Are we looking at the right applications? What are use cases that make obvious the need for semantic annotation of information? What tasks cannot be solved by document retrieval using the traditional bag-of-words algorithms? What characterizes a successful application? Annotations: Are we using the right types of data and annotation? Are there crucial differences between author, software, user, and machine generated annotations? Named entities, temporal expressions on the one hand and sentiment and hedging on the other are examples of analyses which have moved to profitable application. Are there other analyses within our grasp? Searcher/Queries: Are we considering the right search requests? With shallow 2.4 word navigational queries, how much benefit can semantic annotations provide? What expressive power is hidden in semantic annotations? How can searchers find ways to use and explore analyses? Result Aggregation: Are we using the right types of results? Whereas IR focuses almost exclusively at finding individual chunks of information, DB naturally focuses on results that combine information and produce aggregated results (think of OLAP queries), and KM naturally deals with the whole information space. How can we fruitfully combine these strengths? Grand Challenge: Is there a grand challenge for the application of semantic annotation? The workshop will consist of three main parts: Keynote talks by Liz Liddy and Maarten Marx to help us formulate the challenges. A boaster and poster session with 16 papers selected by the program committee from 19 submissions (84%). Each paper was reviewed by at least two members of the program committee. Break out groups on different aspects of exploiting semantic annotations, with reports being discussed in the final session.

#index 1482652
#* Proceedings of the 2nd international workshop on Search and mining user-generated contents
#@ Jose Carlos Cortizo;Francisco M. Carrero;Ivan Cantador;Jose Antonio Troyano;Paolo Rosso
#t 2010
#c 1
#! It is our great pleasure to welcome you to the 2nd International Workshop on Search and Mining User-generated Contents -- SMUC 2010. SMUC 2010 aims to become a forum for researchers from several Information and Knowledge Management areas like data/text mining, information retrieval, semantics, etc. that apply their work into the fields of Social Media and Opinion/Sentiment Analysis where the main goal is to process user generated contents. User generated content provides an excellent scenario to apply the metaphor of mining any kind of information. In a social media context, users create a huge amount of data where we can look for valuable nuggets of knowledge by applying several search techniques (information retrieval) or mining techniques (data mining, text mining, web mining, opinion mining, etc.). In this kind of data we can find both structured information (ratings, tags, links, etc.) and unstructured information (text, audio, video, etc.), and we must learn to combine existing techniques in order to take advantage of this heterogeneity while extracting useful knowledge. The call for papers attracted 25 submissions from Asia, North America, Europe and Africa. The program committee accepted 8 full papers and 7 posters that cover a variety of topics, including data and text mining, opinion mining, search, spam filtering and tagging. The accepted papers use several information sources for experimentation like Twitter data, Product Reviews, Webpages, Wikipedia, Programable Web, Delicious and Blogs. In addition, the program includes a panel on Industrial Applications of Search and Mining User-generated Contents technologies, and a keynote speech by Bing Liu. We hope that these proceedings will serve as a valuable reference for researchers and developers.

#index 1641910
#* Proceedings of the 20th ACM international conference on Information and knowledge management
#@ Bettina Berendt;Arjen de Vries;Wenfei Fan;Craig Macdonald;Iadh Ounis;Ian Ruthven
#t 2011
#c 1
#! On behalf of the organizing committee, it is our great pleasure to welcome you to the 20th ACM Conference on Information and Knowledge Management in Glasgow! Since its inception, the CIKM conference has provided a unique international forum for the presentation, discussion and dissemination of research findings in data management, information retrieval and knowledge management. The purpose of the conference is to identify challenging problems facing the development of future knowledge and information systems and to shape future research directions through the publication of high quality, applied and theoretical research findings. The conference has been a leading forum in which experts from academic, industry and the public sector gather to exchange ideas, research achievements and technical developments in multidisciplinary research areas. CIKM is one of the world's most recognized conferences in the field. This year CIKM received 918 full paper submissions, 220 poster submissions, and 56 demonstration submissions. This is a great demonstration of the lively research areas that contribute to the CIKM area. In addition, CIKM 2011 will host 10 tutorials from leading researchers, 15 workshops on cutting-edge areas of research, a panel session on Social and Collaborative Search and a dedicated Industry Day featuring leading industrial practitioners. We are grateful to all authors who chose to submit their research to CIKM 2011 and are very excited by the final program. CIKM values interdisciplinary research and we are proud to present three keynote speakers, Professor Justin Zobel, Professor Maurizio Lenzerini and Professor David Karger, all of whom will give presentations that cross discipline boundaries.

#index 1641911
#* Creating user interfaces that entice people to manage better information
#@ David R. Karger
#t 2011
#c 1
#% 956568
#% 1271808
#% 1384198
#% 1400028
#% 1573684
#! Much research in information management begins by asking how to manage a given information corpus. But information management systems can only be as good as the information they manage. They struggle and often fail to correctly infer meaning from large blobs of text and the mysterious actions and demands of users. And they are useless for managing information that is never captured. Instead of accepting the existing information as an immutable condition, I will argue that there are significant opportunities to help and motivate people to improve the quality and quantity of information their tools manage, and to exploit that better information to benefit its users. The greatest challenge in doing so is developing systems, and particularly user interfaces, that overcome humans' perverse reluctance to invest small present-moment effort for large future payoffs. Effective systems must minimize the effort needed to record high-quality information and maximize the perceived future benefits of that information investment. I will support these ideas with examples covering structured data management and presentation, notetaking, collaborative filtering, and social media.

#index 1641912
#* Data, health, and algorithmics: computational challenges for biomedicine
#@ Justin Zobel
#t 2011
#c 1

#index 1641913
#* Ontology-based data management
#@ Maurizio Lenzerini
#t 2011
#c 1
#% 992962
#% 1063534
#% 1065125
#! Ontology-based data management aims at accessing and using data by means of an ontology, i.e., a conceptual representation of the domain of interest in the underlying information system. This new paradigm provides several interesting features, many of which have been already proved effective in managing complex information systems. On the other hand, several important issues remain open, and constitute stimulating challenges for the research community. In this talk we first provide an introduction to ontology-based data management, illustrating the main ideas and techniques for using an ontology to access the data layer of an information system, and then we discuss several important issues that are still the subject of extensive investigations, including the need of inconsistency tolerant query answering methods, and the need of supporting update operations expressed over the ontology.

#index 1641914
#* Lower-bounding term frequency normalization
#@ Yuanhua Lv;ChengXiang Zhai
#t 2011
#c 1
#% 120104
#% 169781
#% 218982
#% 262096
#% 321635
#% 340948
#% 411760
#% 766412
#% 818263
#% 879579
#% 987229
#% 1154026
#% 1305656
#% 1450858
#% 1558471
#% 1598452
#% 1642160
#! In this paper, we reveal a common deficiency of the current retrieval models: the component of term frequency (TF) normalization by document length is not lower-bounded properly; as a result, very long documents tend to be overly penalized. In order to analytically diagnose this problem, we propose two desirable formal constraints to capture the heuristic of lower-bounding TF, and use constraint analysis to examine several representative retrieval functions. Analysis results show that all these retrieval functions can only satisfy the constraints for a certain range of parameter values and/or for a particular set of query terms. Empirical results further show that the retrieval performance tends to be poor when the parameter is out of the range or the query term is not in the particular set. To solve this common problem, we propose a general and efficient method to introduce a sufficiently large lower bound for TF normalization which can be shown analytically to fix or alleviate the problem. Our experimental results demonstrate that the proposed method, incurring almost no additional computational cost, can be applied to state-of-the-art retrieval functions, such as Okapi BM25, language models, and the divergence from randomness approach, to significantly improve the average precision, especially for verbose queries.

#index 1641915
#* A quasi-synchronous dependence model for information retrieval
#@ Jae Hyun Park;W. Bruce Croft;David A. Smith
#t 2011
#c 1
#% 109190
#% 218978
#% 262096
#% 279755
#% 280851
#% 397129
#% 397205
#% 756103
#% 766428
#% 816186
#% 817472
#% 818262
#% 987282
#% 1019084
#% 1074112
#% 1131677
#% 1227647
#% 1292594
#% 1299651
#% 1328356
#% 1355019
#% 1450986
#% 1482284
#! Incorporating syntactic features in a retrieval model has had very limited success in the past, with the exception of binary term dependencies. This paper presents a new term dependency modeling approach based on syntactic dependency parsing for both queries and documents. Our model is inspired by a quasi-synchronous stochastic process for machine translation[21]. We model four different types of relationships between syntactically dependent term pairs to perform inexact matching between documents and queries. We also propose a machine learning technique for predicting optimal parameter settings for a retrieval model incorporating syntactic relationships. The results on TREC collections show that the quasi-synchronous dependence model can improve retrieval performance and outperform a strong state-of-art sequential dependence baseline when we use predicted optimal parameters.

#index 1641916
#* Improving retrieval accuracy of difficult queries through generalizing negative document language models
#@ Maryam Karimzadehgan;ChengXiang Zhai
#t 2011
#c 1
#% 57896
#% 115608
#% 118726
#% 144029
#% 169729
#% 218978
#% 229348
#% 232646
#% 262096
#% 289079
#% 340899
#% 340901
#% 340948
#% 342707
#% 375017
#% 397161
#% 766440
#% 766525
#% 790839
#% 818207
#% 818240
#% 818267
#% 838530
#% 838532
#% 879585
#% 879613
#% 1019183
#% 1074078
#% 1292550
#% 1450869
#% 1482285
#% 1482337
#% 1697461
#! When a query topic is difficult and the search results are very poor, negative feedback is a very useful method to improve the retrieval accuracy and user experience. One challenge in negative feedback is that negative documents tend to be distracting in different ways, thus as training examples, negative examples are sparse. In this paper, we solve the problem of data sparseness in the language modeling framework. We propose an optimization framework, in which we learn from a few top-ranked non-relevant examples, and search in a large space of all language models to build a more general negative language model. This general negative language model has more power in pruning the non-relevant documents, thus potentially improving the performance for difficult queries. Experiment results on representative TREC collections show that the proposed optimization framework can improve negative feedback performance over the state-of-the-art negative feedback method through generalizing negative language models.

#index 1641918
#* Finding relevant information of certain types from enterprise data
#@ Xitong Liu;Hui Fang;Cong-Lei Yao;Min Wang
#t 2011
#c 1
#% 279755
#% 323131
#% 375017
#% 659990
#% 768898
#% 818263
#% 879570
#% 879579
#% 879663
#% 879678
#% 907536
#% 987375
#% 987402
#% 993987
#% 1015325
#% 1063537
#% 1077150
#% 1124990
#% 1130856
#% 1130922
#% 1181246
#% 1217114
#% 1227616
#% 1292756
#% 1392493
#% 1450916
#% 1471314
#% 1482405
#% 1697465
#! Search over enterprise data is essential to every aspect of an enterprise because it helps users fulfill their information needs. Similar to Web search, most queries in enterprise search are keyword queries. However, enterprise search is a unique research problem because, compared with the data in traditional IR applications (e.g., text data), enterprise data includes information stored in different formats. In particular, enterprise data include both unstructured and structured information, and all the data center around a particular enterprise. As a result, the relevant information from these two data sources could be complementary to each other. Intuitively, such integrated data could be exploited to improve the enterprise search quality. Despite its importance, this problem has received little attention so far. In this paper, we demonstrate the feasibility of leveraging the integrated information in enterprise data to improve search quality through a case study, i.e., finding relevant information of certain types from enterprise data. Enterprise search users often look for different types of relevant information other than documents, e.g., the contact information of per- sons working on a product. When formulating a keyword query, search users may specify both content requirements, i.e., what kind of information is relevant, and type requirements, i.e., what type of information is relevant. Thus, the goal is to find information relevant to both requirements specified in the query. Specifically, we formulate the problem as keyword search over structured or semistructured data, and then propose to leverage the complementary unstructured information in the enterprise data to solve the problem. Experiment results over real world enterprise data and simulated data show that the proposed methods can effectively exploit the unstructured information to find relevant information of certain types from structured and semistructured information in enterprise data.

#index 1641919
#* Unsupervised transactional query classification based on webpage form understanding
#@ Yuchen Liu;Xiaochuan Ni;Jian-Tao Sun;Zheng Chen
#t 2011
#c 1
#% 262061
#% 269217
#% 290830
#% 330782
#% 331011
#% 340928
#% 397126
#% 480479
#% 590523
#% 642982
#% 754059
#% 765409
#% 769890
#% 805878
#% 810110
#% 824775
#% 866989
#% 1015284
#% 1043040
#% 1055676
#% 1328136
#% 1399933
#% 1560359
#% 1676560
#% 1682107
#! Query type classification aims to classify search queries into categories like navigational, informational and transactional, etc., according to the type of information need behind the queries. Although this problem has drawn many research attentions, previous methods usually require editors to label queries as training data or need domain knowledge to edit rules for predicting query type. Also, the existing work has been mainly focusing on the classification of informational and navigational query types. Transactional query classification has not been well addressed. In this work, we propose an unsupervised approach for transactional query classification. This method is based on the observation that, after the transactional queries are issued to a search engine, many users will click the search result pages and then have interactions with Web forms on these pages. The interactions, e.g., typing in text box, making selections from dropdown list, clicking on a button to execute actions, are used to specify detailed information of the transaction. By mining toolbar search log data, which records the associations between queries and Web forms clicked by users, we can get a set of good quality transactional queries without using manual labeling efforts. By matching these automatically acquired transactional queries and their associated Web form contents, we can generalize these queries into patterns. These patterns can be used to classify queries which are not covered by search log. Our experiments indicate that transactional queries produced by this method have good quality. The pattern based classifier achieves 83% F1 classification result. This is very effective considering the fact that we do not adopt any labeling efforts to train the classifier.

#index 1641920
#* Assigning documents to master sites in distributed search
#@ Roi Blanco;B. Barla Cambazoglu;Flavio P. Junqueira;Ivan Kelly;Vincent Leroy
#t 2011
#c 1
#% 194246
#% 249153
#% 280833
#% 342707
#% 411762
#% 435115
#% 578337
#% 616169
#% 617222
#% 750863
#% 869491
#% 1055149
#% 1227628
#% 1292508
#% 1450839
#% 1450840
#% 1536562
#! An appealing solution to scale Web search with the growth of the Internet is the use of distributed architectures. Distributed search engines rely on multiple sites deployed in distant regions across the world, where each site is specialized to serve queries issued by the users of its region. This paper investigates the problem of assigning each document to a master site. We show that by leveraging similarities between a document and the activity of the users, we can accurately detect which site is the most relevant to place a document. We conduct various experiments using two document assignment approaches, showing performance improvements of up to 20.8% over a baseline technique which assigns the documents to search sites based on their language.

#index 1641921
#* Discovering URLs through user feedback
#@ Xiao Bai;B. Barla Cambazoglu;Flavio P. Junqueira
#t 2011
#c 1
#% 268087
#% 298221
#% 330604
#% 330609
#% 348136
#% 348137
#% 424292
#% 480136
#% 480479
#% 500899
#% 659994
#% 731406
#% 754058
#% 754088
#% 756015
#% 766472
#% 783693
#% 794132
#% 805879
#% 809418
#% 823348
#% 836100
#% 956536
#% 956538
#% 1035588
#% 1055714
#% 1127557
#% 1132154
#% 1153719
#% 1166533
#% 1183221
#% 1227650
#% 1369421
#% 1399989
#! Search engines rely upon crawling to build their Web page collections. A Web crawler typically discovers new URLs by following the link structure induced by links on Web pages. As the number of documents on the Web is large, discovering newly created URLs may take arbitrarily long, and depending on how a given page is connected to others, such a crawler may miss the pages altogether. In this paper, we evaluate the benefits of integrating a passive URL discovery mechanism into a Web crawler. This mechanism is passive in the sense that it does not require the crawler to actively fetch documents from the Web to discover URLs. We focus here on a mechanism that uses toolbar data as a representative source for new URL discovery. We use the toolbar logs of Yahoo! to characterize the URLs that are accessed by users via their browsers, but not discovered by Yahoo! Web crawler. We show that a high fraction of URLs that appear in toolbar logs are not discovered by the crawler. We also reveal that a certain fraction of URLs are discovered by the crawler later than the time they are first accessed by users. One important conclusion of our work is that web search engines can highly benefit from user feedback in the form of toolbar logs for passive URL discovery.

#index 1641922
#* User browsing behavior-driven web crawling
#@ Minghai Liu;Rui Cai;Ming Zhang;Lei Zhang
#t 2011
#c 1
#% 268079
#% 268087
#% 281251
#% 290830
#% 577330
#% 783439
#% 805879
#% 807302
#% 835231
#% 879601
#% 1022233
#% 1035570
#% 1055716
#% 1074108
#% 1227650
#% 1369421
#% 1399994
#! To optimize the performance of web crawlers, various page importance measures have been studied to select and order URLs in crawling. Most sophisticated measures (e.g. breadth-first and PageRank) are based on link structure. In this paper, we treat the problem from another perspective and propose to measure page importance through mining user interest and behaviors from web browse logs. Unlike most existing approaches which work on single URL, in this paper, both the log mining and the crawl ordering are performed at the granularity of URL pattern. The proposed URL pattern-based crawl orderings are capable to properly predict the importance of newly created (unseen) URLs. Promising experimental results proved the feasibility of our approach.

#index 1641923
#* Diversifying search results of controversial queries
#@ Mouna Kacimi;Johann Gamper
#t 2011
#c 1
#% 290482
#% 642975
#% 815915
#% 854646
#% 879686
#% 907489
#% 1035591
#% 1074133
#% 1166473
#% 1190093
#% 1227591
#% 1250238
#% 1292596
#% 1348342
#% 1450945
#% 1482296
#% 1536539
#% 1587424
#% 1845364
#! Diversifying search results of queries seeking for different view points about controversial topics is key to improving satisfaction of users. The challenge for finding different opinions is how to maximize the number of discussed arguments without being biased against specific sentiments. This paper addresses the issue by first introducing a new model that represents the patterns occurring in documents about controversial topics. Second, proposing an opinion diversification model that uses (1) relevance of documents, (2) semantic diversification to capture different arguments and (3) sentiment diversification to identify positive, negative and neutral sentiments about the query topic. We have conducted our experiments using queries on various controversial topics and applied our diversification model on the set of documents returned by Google search engine. The results show that our model outperforms the native ranking of Web pages about controversial topics by a significant margin.

#index 1641924
#* Relevance weighting using within-document term statistics
#@ Kai Hui;Ben He;Tiejian Luo;Bin Wang
#t 2011
#c 1
#% 169781
#% 194245
#% 340948
#% 411760
#% 857180
#% 1043051
#% 1077150
#% 1166534
#% 1305656
#! With the rapid development of the information technology, there exists the difficulty in deploying state-of-the-art retrieval models in environments such as peer-to-peer networks and pervasive computing, where it is expensive or even infeasible to maintain the global statistics. To this end, this paper presents an investigation in the validity of different statistical assumptions of term distributions. Based on the findings in this investigation, a variety of weighting models, called NG (standing for "no global statistics") models, are derived from the Divergence from Randomness framework, in which only the within-document statistics are used in the relevance weighting. Compared to the state-of-the-art weighting models in extensive experiments on various standard TREC test collections, our proposed NG models can provide acceptable retrieval performance in ad-hoc search, without the use of global statistics.

#index 1641925
#* Suggestion set utility maximization using session logs
#@ Umut Ozertem;Emre Velipasaoglu;Larry Lai
#t 2011
#c 1
#% 262112
#% 348155
#% 411762
#% 805863
#% 869500
#% 869501
#% 879618
#% 879686
#% 987212
#% 987222
#% 989578
#% 1055706
#% 1074133
#% 1083721
#% 1083899
#% 1130852
#% 1130854
#% 1166473
#% 1173699
#% 1190093
#% 1292653
#% 1348342
#% 1712595
#! Assistance technology is undoubtedly one of the important elements in the commercial search engines, and routing the user towards the right direction throughout the search sessions is of great importance for providing a good search experience. Most search assistance methods in the literature that involve query generation, query expansion and other techniques consider each suggestion candidate individually, which implies an independence assumption. We challenge this independence assumption and give a method to maximize the utility of a given set of suggestions. For this, we will define a measure of conditional utility for query pairs using query-URL bipartite graphs based on the session logs (clicked and viewed URLs). Afterwards, we remove the redundant queries from the suggestion set using a greedy algorithm to be able to replace them with more useful ones. Both offline (based on user studies and session log analysis) and online (based on millions of user interactions) evaluations show that modeling the conditional utility and maximizing the utility of the set of queries (by eliminating redundant ones) significantly increases the effectiveness of the search assistance both for the presubmit and postsubmit modes.

#index 1641926
#* Improving context-aware query classification via adaptive self-training
#@ Minmin Chen;Jian-Tao Sun;Xiaochuan Ni;Yixin Chen
#t 2011
#c 1
#% 1257
#% 186340
#% 252011
#% 296646
#% 303498
#% 348155
#% 420077
#% 464434
#% 466580
#% 748550
#% 770763
#% 816181
#% 844287
#% 939527
#% 950658
#% 983878
#% 1074093
#% 1130878
#% 1227577
#% 1321568
#% 1455666
#% 1470674
#! Topical classification of user queries is critical for general-purpose web search systems. It is also a challenging task, due to the sparsity of query terms and the lack of labeled queries. On the other hand, search contexts embedded in query sessions and unlabeled queries free on the web have not been fully utilized in most query classification systems. In this work, we leverage these information to improve query classification accuracy. We first incorporate search contexts into our framework using a Conditional Random Field (CRF) model. Discriminative training of CRFs is favored over the traditional maximum likelihood training because of its robustness to noise. We then adapt self-training with our model to exploit the information in unlabeled queries. By investigating different confidence measurements and model selection strategies, we effectively avoid the error-reinforcing nature of self-training. In extensive experiments on real search logs, we have averaged around 20% improvement in classification accuracy over other state-of-the-art baselines.

#index 1641927
#* A task level metric for measuring web search satisfaction and its application on improving relevance estimation
#@ Ahmed Hassan;Yang Song;Li-wei He
#t 2011
#c 1
#% 281174
#% 411762
#% 577224
#% 590523
#% 766472
#% 805200
#% 823348
#% 879565
#% 879567
#% 943049
#% 956495
#% 983921
#% 987263
#% 989628
#% 1074092
#% 1130811
#% 1130878
#% 1166517
#% 1264133
#% 1272000
#% 1301405
#% 1355034
#% 1355038
#% 1450912
#! Understanding the behavior of satisfied and unsatisfied Web search users is very important for improving users search experience. Collecting labeled data that characterizes search behavior is a very challenging problem. Most of the previous work used a limited amount of data collected in lab studies or annotated by judges lacking information about the actual intent. In this work, we performed a large scale user study where we collected explicit judgments of user satisfaction with the entire search task. Results were analyzed using sequence models that incorporate user behavior to predict whether the user ended up being satisfied with a search or not. We test our metric on millions of queries collected from real Web search traffic and show empirically that user behavior models trained using explicit judgments of user satisfaction outperform several other search quality metrics. The proposed model can also be used to optimize different search engine components. We propose a method that uses task level success prediction to provide a better interpretation of clickthrough data. Clickthough data has been widely used to improve relevance estimation. We use our user satisfaction model to distinguish between clicks that lead to satisfaction and clicks that do not. We show that adding new features derived from this metric allowed us to improve the estimation of document relevance.

#index 1641928
#* Multi-view random walk framework for search task discovery from click-through log
#@ Jianwei Cui;Hongyan Liu;Jun Yan;Lei Ji;Ruoming Jin;Jun He;Yingqin Gu;Zheng Chen;Xiaoyong Du
#t 2011
#c 1
#% 590523
#% 629672
#% 754059
#% 805878
#% 879581
#% 983949
#% 987212
#% 1190102
#% 1214758
#% 1264812
#% 1318687
#% 1399955
#% 1400017
#% 1400033
#% 1450893
#% 1566937
#! Search engine users often have clear search tasks hidden behind their queries. Inspired by this, the modern search engines are providing an increasing number of services to help users simplify their key tasks. However, the problem of what are the major user search tasks with high traffic for which search engines should design special services is still underexplored. In this paper, we propose a novel Multi-view Random Walk (MRW) algorithm to measure the search task oriented similarity between queries, and then group search queries with similar tasks so that the major search tasks of users can be identified from search engine click-through log. The proposed MRW, which is a general framework to combine knowledge from different views in a random walk process, allows the random surfer to walk across different views to integrate information for search task discovery. Experimental results on click-through log of a commonly used commercial search engine show that our proposed MRW algorithm can effectively discover user search tasks.

#index 1641929
#* Query sampling for learning data fusion
#@ Ting-Chu Lin;Pu-Jen Cheng
#t 2011
#c 1
#% 132697
#% 169774
#% 184496
#% 243727
#% 302391
#% 340936
#% 413613
#% 520224
#% 840002
#% 879582
#% 879605
#% 1227635
#% 1278067
#% 1292721
#% 1392446
#% 1415738
#% 1450872
#% 1587347
#% 1673566
#% 1704840
#! Data fusion is to merge the results of multiple independent retrieval models into a single ranked list. Several earlier studies have shown that the combination of different models can improve the retrieval performance better than using any of the individual models. Although many promising results have been given by supervised fusion methods, training data sampling has attracted little attention in previous work of data fusion. By observing some evaluations on TREC and NTCIR datasets, we found that the performance of one model varied largely from one training example to another, so that not all training examples were equivalently effective. In this paper, we propose two novel approaches: greedy and boosting approaches, which select effective training data by query sampling to improve the performance of supervised data fusion algorithms such as BayesFuse, probFuse and MAPFuse. Extensive experiments were conducted on five data sets including TREC-3,4,5 and NTCIR-3,4. The results show that our sampling approaches can significantly improve the retrieval performance of those data fusion methods.

#index 1641930
#* Query session detection as a cascade
#@ Matthias Hagen;Benno Stein;Tino Rüb
#t 2011
#c 1
#% 401407
#% 783483
#% 823348
#% 838547
#% 869500
#% 878624
#% 950658
#% 974108
#% 1130878
#% 1187376
#% 1227677
#% 1275012
#% 1275193
#% 1292473
#% 1348356
#% 1392432
#% 1536532
#! We propose a cascading method for query session detection, the problem of identifying series of consecutive queries a user submits with the same information need. While the existing session detection research mostly deals with effectiveness, our focus also is on efficiency, and we investigate questions related to the analysis trade-off: How expensive (in terms of runtime) is a certain improvement in F-Measure? In this regard, we distinguish two major scenarios where query session knowledge is important: (1) In an online setting, the search engine tries to incorporate knowledge of the preceding queries for an improved retrieval performance. Obviously, the efficiency of the session detection method is a crucial issue as the overall retrieval time should not be influenced too much. (2) In an offline post-retrieval setting, search engine logs are divided into sessions in order to examine what causes users to fail or to identify typical reformulation patterns etc. Here, efficiency might not be as important as in the online scenario but the accuracy of the detected sessions is essential. Our cascading method provides a sensible treatment for both scenarios. It involves different steps that form a cascade in the sense that computationally costly and hence time-consuming features are applied only after cheap features "failed." This is different to previous session detection methods, most of which involve many features simultaneously. Experiments on a standard test corpus show the cascading method to save runtime compared to the state of the art while the detected sessions' accuracy is even superior.

#index 1641931
#* Discovering missing click-through query language information for web search
#@ Xing Yi;James Allan
#t 2011
#c 1
#% 262096
#% 340899
#% 340901
#% 411762
#% 577224
#% 642992
#% 677148
#% 730090
#% 766430
#% 783482
#% 840846
#% 879567
#% 879575
#% 879584
#% 879587
#% 987222
#% 989628
#% 1074093
#% 1074127
#% 1130855
#% 1173692
#% 1227621
#% 1450844
#% 1450882
#% 1587372
#! The click-through information in web query logs has been widely used for web search tasks. However, it usually suffers from the data sparseness problem, known as the missing/incomplete click problems, where large volume of pages receive few or no clicks. In this paper, we adapt two language modeling based approaches to address this issue in the context of using web query logs for web search. The first approach discovers missing click-through query language features for web pages with no or few clicks from their similar pages' click-associated queries in the query logs, to help search. We further propose combining this content based approach with the random walk approach on the click graph to further reduce click-through sparseness for search. The second approach follows the query expansion method and utilizes the queries and their clicked web pages in the query logs to reconstruct a structured variant of the relevance based language models for each user-input query for search. We design experiments with a publicly available query log excerpt and two TREC web search tasks on the GOV2 and ClueWeb09 corpora to evaluate the search performance of different approaches. Our results show that using discovered semantic click-through query language features can statistically significantly improve search performance, compared with the baselines that do not use the discovered information. The combination approach that uses discovered click-through features from both random walk and the content based approach can further improve search performance.

#index 1641932
#* Interactive sense feedback for difficult queries
#@ Alexander Kotov;ChengXiang Zhai
#t 2011
#c 1
#% 131434
#% 144029
#% 144031
#% 152968
#% 169768
#% 280840
#% 280847
#% 280849
#% 286069
#% 292686
#% 309088
#% 340899
#% 342660
#% 342707
#% 397159
#% 420471
#% 427921
#% 572500
#% 577285
#% 642994
#% 742666
#% 766430
#% 766439
#% 766440
#% 838531
#% 879621
#% 987225
#% 1074078
#% 1074113
#% 1227584
#% 1263579
#% 1292739
#! Ambiguity of query terms is a common cause of inaccurate retrieval results. Existing work has mostly focused on studying how to improve retrieval accuracy by automatically resolving word sense ambiguity. However, fully automatic sense identification and disambiguation is a very challenging task. In this work, we propose to involve a user in the process of disambiguation through interactive sense feedback and study the potential effectiveness of this novel feedback strategy. We propose several general methods to automatically identify the major senses of query terms based on global analysis of document collection and generate concise representations of the discovered senses to the users. This feedback strategy does not rely on initial retrieval results, and thus can be especially useful for improving the results of difficult queries. We evaluated the effectiveness of the proposed methods for sense identification and presentation through simulation experiments and user studies, which both indicate that sense feedback strategy is a promising alternative to the existing interactive feedback techniques such as relevance feedback and term feedback.

#index 1641933
#* Reranking search results for sparse queries
#@ Elif Aktolga;James Allan
#t 2011
#c 1
#% 262096
#% 411762
#% 783482
#% 818221
#% 823348
#% 879567
#% 879738
#% 946521
#% 987222
#% 987321
#% 1074112
#% 1107086
#% 1130814
#% 1190106
#% 1227621
#% 1292473
#% 1292653
#% 1399990
#% 1450904
#% 1482231
#% 1482283
#% 1682059
#% 1715625
#! It is well known that clickthrough data can be used to improve the effectiveness of search results: broadly speaking, a query's past clicks are a predictor of future clicks on documents. However, when a new or unusual query appears, or when a system is not as widely used as a mainstream web search system, there may be little to no click data available to improve the results. Existing methods to boost query performance for sparse queries extend the query-document click relationship to more documents or queries, but require substantial clickthrough data from other queries. In this work we describe a way to boost rarely-clicked queries in a system where limited clickthrough data is available for all queries. We describe a probabilistic approach for carrying out that estimation and use it to rerank retrieved documents. We utilize information from co-click queries, subset queries, and synonym queries to estimate the clickthrough for a sparse query. Our experiments on a query log from a medical informatics company demonstrate that when overall clickthrough data is sparse, reranking search results using clickthrough information from related queries significantly outperforms reranking that employs clickthrough information from the query alone.

#index 1641934
#* Searching microblogs: coping with sparsity and document quality
#@ Nasir Naveed;Thomas Gottron;Jérôme Kunegis;Arifah Che Alhadi
#t 2011
#c 1
#% 218982
#% 722904
#% 1355042
#% 1399992
#% 1472943
#% 1517969
#% 1536506
#% 1560174
#% 1587349
#% 1587367
#% 1587369
#! Two of the main challenges in retrieval on microblogs are the inherent sparsity of the documents and difficulties in assessing their quality. The feature sparsity is immanent to the restriction of the medium to short texts. Quality assessment is necessary as the microblog documents range from spam over trivia and personal chatter to news broadcasts, information dissemination and reports of current hot topics. In this paper we analyze how these challenges can influence standard retrieval models and propose methods to overcome the problems they pose. We consider the sparsity's effect on document length normalization and introduce "interestingness" as static quality measure. Our results show that deliberately ignoring length normalization yields better retrieval results in general and that interestingness improves retrieval for underspecified queries.

#index 1641935
#* Finding images of difficult entities in the long tail
#@ Bilyana Taneva;Mouna Kacimi;Gerhard Weikum
#t 2011
#c 1
#% 262096
#% 750863
#% 760805
#% 987229
#% 1038781
#% 1119142
#% 1227608
#% 1227720
#% 1273825
#% 1275012
#% 1292688
#% 1355059
#% 1409954
#% 1450912
#% 1857848
#! While images of famous people and places are abundant on the Internet, they are much harder to retrieve for less popular entities such as notable computer scientists or regionally interesting churches. Querying the entity names in image search engines yields large candidate lists, but they often have low precision and unsatisfactory recall. In this paper, we propose a principled model for finding images of rare or ambiguous named entities. We propose a set of efficient, light-weight algorithms for identifying entity-specific keyphrases from a given textual description of the entity, which we then use to score candidate images based on the matches of keyphrases in the underlying Web pages. Our experiments show the high precision-recall quality of our approach.

#index 1641936
#* Learning to rank user intent
#@ Giorgos Giannopoulos;Ulf Brefeld;Theodore Dalamagas;Timos Sellis
#t 2011
#c 1
#% 348173
#% 577224
#% 754126
#% 783482
#% 805200
#% 818207
#% 823348
#% 824716
#% 829039
#% 879567
#% 879588
#% 881477
#% 881540
#% 907515
#% 959870
#% 987227
#% 987228
#% 989628
#% 1020103
#% 1074071
#% 1217155
#% 1246167
#% 1399946
#% 1682072
#! Personalized retrieval models aim at capturing user interests to provide personalized results that are tailored to the respective information needs. User interests are however widely spread, subject to change, and cannot always be captured well, thus rendering the deployment of personalized models challenging. We take a different approach and study ranking models for user intent. We exploit user feedback in terms of click data to cluster ranking models for historic queries according to user behavior and intent. Each cluster is finally represented by a single ranking model that captures the contained search interests expressed by users. Once new queries are issued, these are mapped to the clustering and the retrieval process diversifies possible intents by combining relevant ranking functions. Empirical evidence shows that our approach significantly outperforms baseline approaches on a large corporate query log.

#index 1641937
#* Learning to aggregate vertical results into web search results
#@ Jaime Arguello;Fernando Diaz;Jamie Callan
#t 2011
#c 1
#% 278107
#% 577224
#% 983874
#% 1074093
#% 1077150
#% 1166523
#% 1227616
#% 1227617
#% 1227620
#% 1268491
#% 1399990
#% 1450915
#% 1536576
#% 1587348
#! Aggregated search is the task of integrating results from potentially multiple specialized search services, or verticals, into the Web search results. The task requires predicting not only which verticals to present (the focus of most prior research), but also predicting where in the Web results to present them (i.e., above or below the Web results, or somewhere in between). Learning models to aggregate results from multiple verticals is associated with two major challenges. First, because verticals retrieve different types of results and address different search tasks, results from different verticals are associated with different types of predictive evidence (or features). Second, even when a feature is common across verticals, its predictiveness may be vertical-specific. Therefore, approaches to aggregating vertical results require handling an inconsistent feature representation across verticals, and, potentially, a vertical-specific relationship between features and relevance. We present 3 general approaches that address these challenges in different ways and compare their results across a set of 13 verticals and 1070 queries. We show that the best approaches are those that allow the learning algorithm to learn a vertical-specific relationship between features and relevance.

#index 1641938
#* Coreference aware web object retrieval
#@ Jeffrey Dalton;Roi Blanco;Peter Mika
#t 2011
#c 1
#% 262112
#% 818262
#% 838536
#% 881510
#% 913783
#% 987272
#% 1074133
#% 1083704
#% 1166473
#% 1190105
#% 1206662
#% 1217235
#% 1217265
#% 1227589
#% 1227591
#% 1312812
#% 1333456
#% 1355026
#% 1400010
#% 1400011
#! As user demands become increasingly sophisticated, search engines today are competing in more than just returning document results from the Web. One area of competition is providing web object results from structured data extracted from a multitude of information sources. We address the problem of performing keyword retrieval over a collection of objects containing a large degree of duplication as different Web-based information sources provide descriptions of the same object. We develop a method for coreference aware retrieval that performs topic-specific coreference resolution on retrieved objects in order to improve object search results. Our results demonstrate that coreference has a significant impact on the effectiveness of retrieval in the domain of local search. Our results show that a coreference aware system outperforms naive object retrieval by more than 20% in P5 and P10.

#index 1641939
#* Tag clouds revisited
#@ Dimitrios Skoutas;Mohammad Alrifai
#t 2011
#c 1
#% 340936
#% 857482
#% 956649
#% 975019
#% 987205
#% 1055719
#% 1055808
#% 1074161
#% 1130808
#% 1183299
#% 1190090
#% 1190091
#% 1190093
#% 1227411
#% 1227644
#% 1266423
#% 1356643
#% 1399998
#% 1400088
#% 1472964
#% 1536516
#% 1696323
#! Tagging has become a very common feature in Web 2.0 applications, providing a simple and effective way for users to freely annotate resources to facilitate their discovery and management. Subsequently, tag clouds have become popular as a summarized representation of a collection of tagged resources. A tag cloud is typically a visualization of the top-k most frequent tags in the underlying collection. In this paper, we revisit tag clouds, to examine whether frequency is the most suitable criterion for tag ranking. We propose alternative tag ranking strategies, based on methods for random walk on graphs, diversification,and rank aggregation. To enable the comparison of different tag selection and ranking methods, we propose a set of evaluation metrics that consider the use of tag clouds for search, navigation and recommendations. We apply these tag ranking methods and evaluation metrics to empirically compare alternative tag clouds in a dataset obtained from Flickr, comprising 488,112 tagged photos organized in 451 groups, and 112,514 distinct tags.

#index 1641940
#* Ranking-based processing of SQL queries
#@ Hany Azzam;Thomas Roelleke;Sirvan Yahyaei
#t 2011
#c 1
#% 41230
#% 77650
#% 184490
#% 215225
#% 262096
#% 768898
#% 810018
#% 824703
#% 894444
#% 960263
#% 1016201
#% 1021951
#% 1022206
#% 1195848
#% 1296979
#% 1333456
#% 1392431
#% 1482175
#! A growing number of applications are built on top of search engines and issue complex structured queries. This paper contributes a customisable ranking-based processing of such queries, specifically SQL. Similar to how term-based statistics are exploited by term-based retrieval models, ranking-aware processing of SQL queries exploits tuple-based statistics that are derived from sources or, more precisely, derived from the relations specified in the SQL query. To implement this ranking-based processing, we leverage PSQL, a probabilistic variant of SQL, to facilitate probability estimation and the generalisation of document retrieval models to be used for tuple retrieval. The result is a general-purpose framework that can interpret any SQL query and then assign a probabilistic retrieval model to rank the results of that query. The evaluation on the IMDB and Monster benchmarks proves that the PSQL-based approach is applicable to (semi-)structured and unstructured data and structured queries.

#index 1641941
#* Keyword search over RDF graphs
#@ Shady Elbassuoni;Roi Blanco
#t 2011
#c 1
#% 262096
#% 309095
#% 340948
#% 413594
#% 654442
#% 660011
#% 766431
#% 810052
#% 824693
#% 956501
#% 977142
#% 1022234
#% 1026960
#% 1063537
#% 1063539
#% 1074072
#% 1092530
#% 1166537
#% 1183369
#% 1190118
#% 1195848
#% 1206817
#% 1206910
#% 1292565
#% 1409954
#% 1712243
#! Large knowledge bases consisting of entities and relationships between them have become vital sources of information for many applications. Most of these knowledge bases adopt the Semantic-Web data model RDF as a representation model. Querying these knowledge bases is typically done using structured queries utilizing graph-pattern languages such as SPARQL. However, such structured queries require some expertise from users which limits the accessibility to such data sources. To overcome this, keyword search must be supported. In this paper, we propose a retrieval model for keyword queries over RDF graphs. Our model retrieves a set of subgraphs that match the query keywords, and ranks them based on statistical language models. We show that our retrieval model outperforms the-state-of-the-art IR and DB models for keyword search over structured data using experiments over two real-world datasets.

#index 1641942
#* Frequency-aware similarity measures: why Arnold Schwarzenegger is always a duplicate
#@ Dustin Lange;Felix Naumann
#t 2011
#c 1
#% 124073
#% 252473
#% 577238
#% 729913
#% 760866
#% 913783
#% 937552
#% 1127558
#% 1211086
#% 1301004
#% 1328067
#% 1434125
#% 1538763
#! Measuring the similarity of two records is a challenging problem, but necessary for fundamental tasks, such as duplicate detection and similarity search. By exploiting frequencies of attribute values, many similarity measures can be improved: In a person table with U.S. citizens, Arnold Schwarzenegger is a very rare name. If we find several Arnold Schwarzeneggers in it, it is very likely that these are duplicates. We are then less strict when comparing other attribute values, such as birth date or address. We put this intuition to use by partitioning compared record pairs according to frequencies of attribute values. For example, we could create three partitions from our data: Partition 1 contains all pairs with rare names, Partition 2 all pairs with medium frequent names, and Partition 3 all pairs with frequent names. For each partition, we learn a different similarity measure: we apply machine learning techniques to combine a set of base similarity measures into an overall measure. To determine a good partitioning, we compare different partitioning strategies. We achieved best results with a novel algorithm inspired by genetic programming. We evaluate our approach on real-world data sets from a large credit rating agency and from a bibliography database. We show that our learning approach works well for logistic regression, SVM, and decision trees with significant improvements over (i) learning models that ignore frequencies and (ii) frequency-enriched models without partitioning.

#index 1641943
#* A probabilistic method for inferring preferences from clicks
#@ Katja Hofmann;Shimon Whiteson;Maarten de Rijke
#t 2011
#c 1
#% 320432
#% 384911
#% 411762
#% 577224
#% 766454
#% 805200
#% 818207
#% 822126
#% 879567
#% 943049
#% 1035578
#% 1073938
#% 1073970
#% 1074134
#% 1130811
#% 1166517
#% 1173704
#% 1190055
#% 1292763
#% 1415711
#% 1450912
#% 1450952
#% 1587358
#% 1830389
#! Evaluating rankers using implicit feedback, such as clicks on documents in a result list, is an increasingly popular alternative to traditional evaluation methods based on explicit relevance judgments. Previous work has shown that so-called interleaved comparison methods can utilize click data to detect small differences between rankers and can be applied to learn ranking functions online. In this paper, we analyze three existing interleaved comparison methods and find that they are all either biased or insensitive to some differences between rankers. To address these problems, we present a new method based on a probabilistic interleaving process. We derive an unbiased estimator of comparison outcomes and show how marginalizing over possible comparison outcomes given the observed click data can make this estimator even more effective. We validate our approach using a recently developed simulation framework based on a learning to rank dataset and a model of click behavior. Our experiments confirm the results of our analysis and show that our method is both more accurate and more robust to noise than existing methods.

#index 1641944
#* Intent-aware query similarity
#@ Jiafeng Guo;Xueqi Cheng;Gu Xu;Xiaofei Zhu
#t 2011
#c 1
#% 132779
#% 232713
#% 277483
#% 280819
#% 310567
#% 342961
#% 641976
#% 722904
#% 769906
#% 783475
#% 853542
#% 869500
#% 869501
#% 869651
#% 875959
#% 987222
#% 1055681
#% 1074101
#% 1127383
#% 1130854
#% 1130868
#% 1130879
#% 1130899
#% 1173699
#% 1211703
#% 1227610
#% 1227619
#% 1292589
#% 1450893
#% 1482240
#% 1712595
#! Query similarity calculation is an important problem and has a wide range of applications in IR, including query recommendation, query expansion, and even advertisement matching. Existing work on query similarity aims to provide a single similarity measure without considering the fact that queries are ambiguous and usually have multiple search intents. In this paper, we argue that query similarity should be defined upon search intents, so-called intent-aware query similarity. By introducing search intents into the calculation of query similarity, we can obtain more accurate and also informative similarity measures on queries and thus help a variety of applications, especially those related to diversification. Specifically, we first identify the potential search intents of queries, and then measure query similarity under different intents using intent-aware representations. A regularized topic model is employed to automatically learn the potential intents of queries by using both the words from search result snippets and the regularization from query co-clicks. Experimental results confirm the effectiveness of intent-aware query similarity on ambiguous queries which can provide significantly better similarity scores over the traditional approaches. We also experimentally verified the utility of intent-aware similarity in the application of query recommendation, which can suggest diverse queries in a structured way to search users.

#index 1641945
#* Semi-supervised learning to rank with preference regularization
#@ Martin Szummer;Emine Yilmaz
#t 2011
#c 1
#% 309095
#% 324129
#% 375017
#% 466263
#% 577224
#% 734915
#% 840846
#% 875948
#% 956506
#% 961218
#% 1002316
#% 1074063
#% 1074082
#% 1193635
#% 1227634
#% 1227635
#% 1415710
#% 1456843
#% 1536584
#% 1564411
#% 1642153
#! We propose a semi-supervised learning to rank algorithm. It learns from both labeled data (pairwise preferences or absolute labels) and unlabeled data. The data can consist of multiple groups of items (such as queries), some of which may contain only unlabeled items. We introduce a preference regularizer favoring that similar items are similar in preference to each other. The regularizer captures manifold structure in the data, and we also propose a rank-sensitive version designed for top-heavy retrieval metrics including NDCG and mean average precision. The regularizer is employed in SSLambdaRank, a semi-supervised version of LambdaRank. This algorithm directly optimizes popular retrieval metrics and improves retrieval accuracy over LambdaRank, a state-of-the-art ranker that was used as part of the winner of the Yahoo! Learning to Rank challenge 2010. The algorithm runs in linear time in the number of queries, and can work with huge datasets.

#index 1641946
#* Simultaneous clustering of multi-type relational data via symmetric nonnegative matrix tri-factorization
#@ Hua Wang;Heng Huang;Chris Ding
#t 2011
#c 1
#% 342621
#% 342659
#% 464291
#% 876018
#% 879594
#% 881468
#% 1214657
#% 1327693
#% 1595860
#% 1598427
#% 1885609
#! The rapid growth of Internet and modern technologies has brought data involving objects of multiple types that are related to each other, called as multi-type relational data. Traditional clustering methods for single-type data rarely work well on them, which calls for more advanced clustering techniques to deal with multiple types of data simultaneously to utilize their interrelatedness. A major challenge in developing simultaneous clustering methods is how to effectively use all available information contained in a multi-type relational data set including inter-type and intra-type relationships. In this paper, we propose a Symmetric Nonnegative Matrix Tri-Factorization (S-NMTF) framework to cluster multi-type relational data at the same time. The proposed S-NMTF approach employs NMTF to simultaneously cluster different types of data using their inter-type relationships, and incorporate the intra-type information through manifold regularization. In order to deal with the symmetric usage of the factor matrix in S-NMTF, we present a new generic matrix inequality to derive the solution algorithm, which involves a fourth-order matrix polynomial, in a principled way. Promising experimental results have validated the proposed approach.

#index 1641947
#* Collaborative online learning of user generated content
#@ Guangxia Li;Kuiyu Chang;Steven C.H. Hoi;Wenting Liu;Ramesh Jain
#t 2011
#c 1
#% 302390
#% 722903
#% 723239
#% 765519
#% 829014
#% 916788
#% 961152
#% 1117691
#! We study the problem of online classification of user generated content, with the goal of efficiently learning to categorize content generated by individual user. This problem is challenging due to several reasons. First, the huge amount of user generated content demands a highly efficient and scalable classification solution. Second, the categories are typically highly imbalanced, i.e., the number of samples from a particular useful class could be far and few between compared to some others (majority class). In some applications like spam detection, identification of the minority class often has significantly greater value than that of the majority class. Last but not least, when learning a classification model from a group of users, there is a dilemma: A single classification model trained on the entire corpus may fail to capture personalized characteristics such as language and writing styles unique to each user. On the other hand, a personalized model dedicated to each user may be inaccurate due to the scarcity of training data, especially at the very beginning; when users have written just a few articles. To overcome these challenges, we propose learning a global model over all users' data, which is then leveraged to continuously refine the individual models through a collaborative online learning approach. The class imbalance problem is addressed via a cost-sensitive learning approach. Experimental results show that our method is effective and scalable for timely classification of user generated content.

#index 1641948
#* Structured learning of two-level dynamic rankings
#@ Karthik Raman;Thorsten Joachims;Pannaga Shivaswamy
#t 2011
#c 1
#% 262112
#% 642975
#% 829043
#% 879618
#% 1074025
#% 1166473
#% 1536529
#% 1641948
#! For ambiguous queries, conventional retrieval systems are bound by two conflicting goals. On the one hand, they should diversify and strive to present results for as many query intents as possible. On the other hand, they should provide depth for each intent by displaying more than a single result. Since both diversity and depth cannot be achieved simultaneously in the conventional static retrieval model, we propose a new dynamic ranking approach. In particular, our proposed two-level dynamic ranking model allows users to adapt the ranking through interaction, thus overcoming the constraints of presenting a one-size-fits-all static ranking. In this model, a user's interactions with the first-level ranking are used to infer this user's intent, so that second-level rankings can be inserted to provide more results relevant to this intent. Unlike previous dynamic ranking models, we provide an algorithm to efficiently compute dynamic rankings with provable approximation guarantees. We also propose the first principled algorithm for learning dynamic ranking functions from training data. In addition to the theoretical results, we provide empirical evidence demonstrating the gains in retrieval quality over conventional approaches.

#index 1641949
#* Efficiency optimizations for interpolating subqueries
#@ Marc-Allen Cartright;James Allan
#t 2011
#c 1
#% 86371
#% 111303
#% 169781
#% 194247
#% 198335
#% 262096
#% 321635
#% 340901
#% 783474
#% 818229
#% 818262
#% 879611
#% 944348
#% 1019124
#% 1074112
#% 1216713
#% 1292597
#% 1482301
#% 1482381
#% 1536517
#! A large class of queries can be viewed as linear combinations of smaller subqueries. Additionally, many situations arise when part or all of one subquery has been preprocessed or has cached information, while another subquery requires full processing. This type of query is common, for example, in relevance feedback settings where the original query has been run to produce a set of expansion terms, but the expansion terms still need to be processed. We investigate mechanisms to reduce the time needed to process queries of this nature. We use RM3, a variant of the Relevance Model scoring algorithm, as our instantiation of this arrangement. We examine the different scenarios that can arise when we have access to the internal structure of each subquery. Given this additional information, we investigate methods to utilize this information, reducing processing costs substantially. Depending on the amount of accessibility we have into the subqueries, we can reduce processing costs over 80% without affecting the score of the final results.

#index 1641950
#* Efficiently encoding term co-occurrences in inverted indexes
#@ Marcus Fontoura;Maxim Gurevich;Vanja Josifovski;Sergei Vassilvitskii
#t 2011
#c 1
#% 198335
#% 268079
#% 330706
#% 340886
#% 387427
#% 387508
#% 655485
#% 730065
#% 781169
#% 805864
#% 818229
#% 867054
#% 878624
#% 987215
#% 1015265
#% 1016222
#% 1166469
#% 1166527
#% 1292495
#% 1328112
#% 1404881
#% 1482301
#% 1538767
#% 1667821
#! Precomputation of common term co-occurrences has been successfully applied to improve query performance in large scale search engines based on inverted indexes. The results of such precomputations are traditionally stored as additional posting lists in the index. During query evaluation, these precomputed lists are used to reduce the number of query terms, as the results for multiple terms can be accessed through a single precomputed list. In this paper, we expand this paradigm by considering an alternative method for storing term co-occurrences in inverted indexes. For a selected set of terms in the index, we store bitmaps that encode term co-occurrences. A bitmap of size k for term t augments each posting to store the co-occurrences of t with k other terms, across every document in the index. At query evaluation, size k bitmaps can be used to answer queries that involve any of the 2^k combinations of the additional terms. In contrast, a precomputed list, although typically shorter, can only be used to evaluate queries containing all of its terms. We evaluate the bitmaps technique we propose, and the baseline of adding precomputed posting lists and show that they are complementary, as they capture different aspects of the query evaluation cost. We perform an experimental evaluation on the TREC WT10g corpus and show that a hybrid strategy combining both methods significantly lowers the cost of query evaluation compared to each method separately.

#index 1641951
#* SIMD-based decoding of posting lists
#@ Alexander A. Stepanov;Anil R. Gangolli;Daniel E. Rose;Ryan J. Ernst;Paramjit S. Oberoi
#t 2011
#c 1
#% 86532
#% 214789
#% 322412
#% 786632
#% 864446
#% 1077150
#% 1166469
#% 1181094
#% 1480464
#% 1480887
#% 1482300
#! Powerful SIMD instructions in modern processors offer an opportunity for greater search performance. In this paper, we apply these instructions to decoding search engine posting lists. We start by exploring variable-length integer encoding formats used to represent postings. We define two properties, byte-oriented and byte-preserving, that characterize many formats of interest. Based on their common structure, we define a taxonomy that classifies encodings along three dimensions, representing the way in which data bits are stored and additional bits are used to describe the data. Using this taxonomy, we discover new encoding formats, some of which are particularly amenable to SIMD-based decoding. We present generic SIMD algorithms for decoding these formats. We also extend these algorithms to the most common traditional encoding format. Our experiments demonstrate that SIMD-based decoding algorithms are up to 3 times faster than non-SIMD algorithms.

#index 1641952
#* Factorization-based lossless compression of inverted indices
#@ George Beskales;Marcus Fontoura;Maxim Gurevich;Sergei Vassilvitskii;Vanja Josifovski
#t 2011
#c 1
#% 340887
#% 397151
#% 643566
#% 722904
#% 730065
#% 778215
#% 793248
#% 878624
#% 996753
#% 1077150
#% 1190095
#% 1400001
#% 1815246
#! Many large-scale Web applications that require ranked top-k retrieval are implemented using inverted indices. An inverted index represents a sparse term-document matrix, where non-zero elements indicate the strength of term-document associations. In this work, we present an approach for lossless compression of inverted indices. Our approach maps terms in a document corpus to a new term space in order to reduce the number of non-zero elements in the term-document matrix, resulting in a more compact inverted index. We formulate the problem of selecting a new term space as a matrix factorization problem, and prove that finding the optimal solution is an NP-hard problem. We develop a greedy algorithm for finding an approximate solution. A side effect of our approach is increasing the number of terms in the index, which may negatively affect query evaluation performance. To eliminate such effect, we develop a methodology for modifying query evaluation algorithms by exploiting specific properties of our compression approach.

#index 1641953
#* TOPSIG: topology preserving document signatures
#@ Shlomo Geva;Christopher M. De Vries
#t 2011
#c 1
#% 249989
#% 290703
#% 318437
#% 375017
#% 411760
#% 750863
#% 879566
#% 907525
#% 1077150
#% 1095876
#% 1312832
#% 1450831
#% 1489449
#% 1622409

#index 1641954
#* Implementation techniques for large-scale latent semantic indexing applications
#@ Roger B. Bradford
#t 2011
#c 1
#% 664048
#% 678676
#% 854886
#% 1077150
#% 1130822
#! The technique of latent semantic indexing (LSI) has wide applicability in information retrieval and data mining tasks. To date, however, most applications of LSI have addressed relatively small collections of data. This has been due partly to hardware and software limitations and partly to overly pessimistic estimates of the processing requirements of the singular value decomposition (SVD) process. In recent years, advances in hardware capabilities and software implementations have enabled much larger LSI applications. Moreover, experience with large LSI indexes has shown that the SVD is not the limitation on scalability that it was long thought to be. This paper describes techniques applicable to creating large-scale (multi-million document) LSI indexes. Detailed data regarding the LSI index creation process is presented for collections of up to 100 million documents. Four key factors are shown to contribute to the scalability of LSI. First, in most situations, the time required for calculation of the singular value decomposition (SVD) of the term-document matrix is not the dominant factor determining the overall time required to build an LSI index. Second, the time required to calculate the SVD in LSI is linear in the number of objects indexed. Third, incremental index creation greatly facilitates use of LSI in dynamic environments. Fourth, distributed query processing can be employed to support large numbers of users. It is shown that LSI is well-suited for implementation in modern distributed computing environments. This paper provides the first measurements of the execution time for large-scale LSI build processes in a cloud environment.

#index 1641955
#* Statistical source expansion for question answering
#@ Nico Schlaefer;Jennifer Chu-Carroll;Eric Nyberg;James Fan;Wlodek Zadrozny;David Ferrucci
#t 2011
#c 1
#% 255170
#% 262112
#% 278106
#% 280815
#% 340928
#% 340953
#% 397160
#% 397177
#% 575578
#% 741058
#% 815828
#% 879636
#% 939971
#% 987236
#% 1221010
#% 1223708
#% 1269107
#% 1292768
#% 1987074
#! A source expansion algorithm automatically extends a given text corpus with related content from large external sources such as the Web. The expanded corpus is not intended for human consumption but can be used in question answering (QA) and other information retrieval or extraction tasks to find more relevant information and supporting evidence. We propose an algorithm that extends a corpus of seed documents with web content, using a statistical model to select text passages that are both relevant to the topics of the seeds and complement existing information. In an evaluation on 1,500 hand-labeled web pages, our algorithm ranked text passages by relevance with 81% MAP, compared to 43% when relying on web search engine ranks alone and 75% when using a multi-document summarization algorithm. Applied to QA, the proposed method yields consistent and significant performance gains. We evaluated the impact of source expansion on over 6,000 questions from the Jeopardy! quiz show and TREC evaluations using Watson, a state-of-the-art QA system. Accuracy increased from 66% to 71% on Jeopardy! questions and from 59% to 64% on TREC questions.

#index 1641956
#* Passage retrieval for incorporating global evidence in sequence labeling
#@ Jeffrey Dalton;James Allan;David A. Smith
#t 2011
#c 1
#% 158687
#% 340901
#% 464434
#% 643004
#% 763708
#% 786528
#% 815178
#% 818262
#% 879584
#% 938708
#% 939376
#% 939641
#% 940002
#% 1249541
#% 1249546
#% 1328359
#% 1471218
#% 1482372
#% 1591965
#! Many forms of linguistic analysis, such as part of speech tagging, named entity recognition, and other sequence labeling tasks are performed on short spans of text and assume statistical dependence within a window of only a few tokens. We propose using passage retrieval to induce non-local dependencies in structured classification that generalizes earlier work in context aggregation for named-entity recognition. We introduce a new method for feature expansion inspired by psuedo-relevance feedback (PRF). Our results on the CoNLL 2003 task show that features from cross-document feature expansion improves NER effectiveness over previous aggregation models. Utilizing all the tokens in a sentence for query context consistently perform best on both intrinsic and extrinsic evaluations. Tagging models incorporating feature expansion outperform the leading NER system when evaluated on out of domain data, a collection of publicly available scanned books on the topic of historic Deerfield, MA. Finally, the results show that retrieval based feature expansion using an external collection of unlabeled text can result in further effectiveness improvements.

#index 1641957
#* Effective and efficient polarity estimation in blogs based on sentence-level evidence
#@ Jose M. Chenlo;David E. Losada
#t 2011
#c 1
#% 818255
#% 854646
#% 855279
#% 938687
#% 939897
#% 943811
#% 1074158
#% 1127964
#% 1130915
#% 1195856
#% 1450879
#% 1700552
#! One of the core tasks in Opinion Mining consists of estimating the polarity of the opinionated documents found. In some scenarios (e.g. blogs), this estimation is severely affected by sentences that are off-topic or that simply do not express any opinion. In fact, the key sentiments in a blog post often appear in specific locations of the text. In this paper we propose several effective and robust polarity detection methods based on different sentence features. We show that we can successfully determine the polarity of documents guided by a sentence-level analysis that takes into account topicality and the location in the blog post of the subjective sentences. Our experimental results show that some of our proposed variants are both highly effective and computationally-lightweight.

#index 1641958
#* Sentiment classification based on supervised latent n-gram analysis
#@ Dmitriy Bespalov;Bing Bai;Yanjun Qi;Ali Shokoufandeh
#t 2011
#c 1
#% 280819
#% 722308
#% 722904
#% 722928
#% 746885
#% 833913
#% 1073892
#% 1127964
#% 1215859
#% 1250356
#% 1292484
#% 1434468
#% 1451218
#% 1457039
#% 1470684
#% 1471319
#% 1481572
#! In this paper, we propose an efficient embedding for modeling higher-order (n-gram) phrases that projects the n-grams to low-dimensional latent semantic space, where a classification function can be defined. We utilize a deep neural network to build a unified discriminative framework that allows for estimating the parameters of the latent space as well as the classification function with a bias for the target classification task at hand. We apply the framework to large-scale sentimental classification task. We present comparative evaluation of the proposed method on two (large) benchmark data sets for online product reviews. The proposed method achieves superior performance in comparison to the state of the art.

#index 1641959
#* Legal document clustering with built-in topic segmentation
#@ Qiang Lu;Jack G. Conrad;Khalid Al-Kofahi;William Keenan
#t 2011
#c 1
#% 262045
#% 280819
#% 310516
#% 342668
#% 413619
#% 466495
#% 588466
#% 722904
#% 741058
#% 742204
#% 744285
#% 746910
#% 748482
#% 815855
#% 894072
#% 963669
#% 1412740
#% 1467704
#! Clustering is a useful tool for helping users navigate, summarize, and organize large quantities of textual documents available on the Internet, in news sources, and in digital libraries. A variety of clustering methods have also been applied to the legal domain, with various degrees of success. Some unique characteristics of legal content as well as the nature of the legal domain present a number of challenges. For example, legal documents are often multi-topical, contain carefully crafted, professional, domain-specific language, and possess a broad and unevenly distributed coverage of legal issues. Moreover, unlike widely accessible documents on the Internet, where search and categorization services are generally free, the legal profession is still largely a fee-for-service field that makes the quality (e.g., in terms of both recall and precision) a key differentiator of provided services. This paper introduces a classification-based recursive soft clustering algorithm with built-in topic segmentation. The algorithm leverages existing legal document metadata such as topical classifications, document citations, and click stream data from user behavior databases, into a comprehensive clustering framework. Techniques associated with the algorithm have been applied successfully to very large databases of legal documents, which include judicial opinions, statutes, regulations, administrative materials and analytical documents. Extensive evaluations were conducted to determine the efficiency and effectiveness of the proposed algorithm. Subsequent evaluations conducted by legal domain experts have demonstrated that the quality of the resulting clusters based upon this algorithm is similar to those created by domain experts.

#index 1641960
#* What and how children search on the web
#@ Sergio Duarte Torres;Ingmar Weber
#t 2011
#c 1
#% 306468
#% 320342
#% 575733
#% 590523
#% 643000
#% 987289
#% 1019163
#% 1130852
#% 1130878
#% 1210687
#% 1355035
#% 1355038
#% 1450894
#% 1450995
#% 1455269
#% 1497569
#% 1536504
#! The Internet has become an important part of the daily life of children as a source of information and leisure activities. Nonetheless, given that most of the content available on the web is aimed at the general public, children are constantly exposed to inappropriate content, either because the language goes beyond their reading skills, their attention span differs from grown-ups or simple because the content is not targeted at children as is the case of ads and adult content. In this work we employed a large query log sample from a commercial web search engine to identify the struggles and search behavior of children of the age of 6 to young adults of the age of 18. Concretely we hypothesized that the large and complex volume of information to which children are exposed leads to ill-defined searches and to disorientation during the search process. For this purpose, we quantified their search difficulties based on query metrics (e.g. fraction of queries posed in natural language), session metrics (e.g. fraction of abandoned sessions) and click activity (e.g. fraction of ad clicks). We also used the search logs to retrace stages of child development. Concretely we looked for changes in the user interests (e.g. distribution of topics searched), language development (e.g. readability of the content accessed) and cognitive development (e.g. sentiment expressed in the queries) among children and adults. We observed that these metrics clearly demonstrate an increased level of confusion and unsuccessful search sessions among children. We also found a clear relation between the reading level of the clicked pages and the demographics characteristics of the users such as age and average educational attainment of the zone in which the user is located.

#index 1641961
#* Personalizing web search results by reading level
#@ Kevyn Collins-Thompson;Paul N. Bennett;Ryen W. White;Sebastian de la Chica;David Sontag
#t 2011
#c 1
#% 290679
#% 328524
#% 642975
#% 766507
#% 818259
#% 838442
#% 987209
#% 1166518
#% 1214757
#% 1227621
#% 1292474
#% 1338622
#% 1384136
#% 1399944
#% 1442577
#% 1455269
#% 1482194
#% 1482279
#% 1536555
#! Traditionally, search engines have ignored the reading difficulty of documents and the reading proficiency of users in computing a document ranking. This is one reason why Web search engines do a poor job of serving an important segment of the population: children. While there are many important problems in interface design, content filtering, and results presentation related to addressing children's search needs, perhaps the most fundamental challenge is simply that of providing relevant results at the right level of reading difficulty. At the opposite end of the proficiency spectrum, it may also be valuable for technical users to find more advanced material or to filter out material at lower levels of difficulty, such as tutorials and introductory texts. We show how reading level can provide a valuable new relevance signal for both general and personalized Web search. We describe models and algorithms to address the three key problems in improving relevance for search using reading difficulty: estimating user proficiency, estimating result difficulty, and re-ranking based on the difference between user and result reading level profiles. We evaluate our methods on a large volume of Web query traffic and provide a large-scale log analysis that highlights the importance of finding results at an appropriate reading level for the user.

#index 1641962
#* Location-aware click prediction in mobile local search
#@ Dimitrios Lymberopoulos;Peixiang Zhao;Christian Konig;Klaus Berberich;Jie Liu
#t 2011
#c 1
#% 448194
#% 860086
#% 949163
#% 956622
#% 987203
#% 1004298
#% 1055697
#% 1089478
#% 1190135
#% 1266460
#% 1338581
#% 1369276
#% 1450894
#% 1450953
#% 1476142
#% 1549002
#% 1728811
#! Users increasingly rely on their mobile devices to search, locate and discover places and activities around them while on the go. Their decision process is driven by the information displayed on their devices and their current context (e.g. traffic, driving or walking etc.). Even though recent research efforts have already examined and demonstrated how different context parameters such as weather, time and personal preferences affect the way mobile users click on local businesses, little has been done to study how the location of the user affects the click behavior. In this paper we follow a data-driven methodology where we analyze approximately 2 million local search queries submitted by users across the US, to visualize and quantify how differently mobile users click across locations. Based on the data analysis, we propose new location-aware features for improving local search click prediction and quantify their performance on real user query traces. Motivated by the results, we implement and evaluate a data-driven technique where local search models at different levels of location granularity (e.g. city, state, and country levels) are combined together at run-time to further improve click prediction accuracy. By applying the location-aware features and the multiple models at different levels of location granularity on real user query streams from a major, commercially available search engine, we achieve anywhere from 5% to 47% higher Precision than a single click prediction model across the US can achieve.

#index 1641963
#* Text vs. space: efficient geo-search query processing
#@ Maria Christoforaki;Jinru He;Constantinos Dimopoulos;Alexander Markowetz;Torsten Suel
#t 2011
#c 1
#% 143306
#% 235114
#% 252304
#% 285932
#% 318437
#% 321455
#% 387427
#% 427199
#% 527181
#% 818938
#% 838407
#% 867054
#% 874993
#% 982560
#% 987214
#% 1055710
#% 1125716
#% 1190095
#% 1206801
#% 1328137
#% 1523828
#% 1550750
#% 1555383
#% 1720754
#! Many web search services allow users to constrain text queries to a geographic location (e.g., yoga classes near Santa Monica). Important examples include local search engines such as Google Local and location-based search services for smart phones. Several research groups have studied the efficient execution of queries mixing text and geography; their approaches usually combine inverted lists with a spatial access method such as an R-tree or space-filling curve. In this paper, we take a fresh look at this problem. We feel that previous work has often focused on the spatial aspect at the expense of performance considerations in text processing, such as inverted index access, compression, and caching. We describe new and existing approaches and discuss their different perspectives. We then compare their performance in extensive experiments on large document collections. Our results indicate that a query processor that combines state-of-the-art text processing techniques with a simple coarse-grained spatial structure can outperform existing approaches by up to two orders of magnitude. In fact, even a naive approach that first uses a simple inverted index and then filters out any documents outside the query range outperforms many previous methods.

#index 1641964
#* One is enough: distributed filtering for duplicate elimination
#@ Georgia Koloniari;Nikos Ntarmos;Evaggelia Pitoura;Dimitris Souravlias
#t 2011
#c 1
#% 307424
#% 322884
#% 642409
#% 654461
#% 805840
#% 874972
#% 942873
#% 1140554
#% 1181214
#% 1326701
#% 1449043
#% 1483635
#% 1676151
#! The growth of online services has created the need for duplicate elimination in high-volume streams of events. The sheer volume of data in applications such as pay-per-click clickstream processing, RSS feed syndication and notification services in social sites such Twitter and Facebook makes traditional centralized solutions hard to scale. In this paper, we propose an approach based on distributed filtering. To this end, we introduce a suite of distributed Bloom filters that exploit different ways of partitioning the event space. To address the continuous nature of event delivery, the filters are extended to support sliding window semantics. Moreover, we examine locality-related tradeoffs and propose a tree-based architecture to allow for duplicate elimination across geographic locations. We cast the design space and present experimental results that demonstrate the pros and cons of our various solutions in different settings.

#index 1641965
#* Duplicate detection through structure optimization
#@ Luís Leitão;Pável Calado
#t 2011
#c 1
#% 44876
#% 190581
#% 201889
#% 269217
#% 376266
#% 387427
#% 463445
#% 577238
#% 577263
#% 729887
#% 729913
#% 800590
#% 810014
#% 810044
#% 819550
#% 830275
#% 870902
#% 871766
#% 937552
#% 993980
#% 1019088
#% 1090217
#% 1195991
#% 1217163
#% 1292587
#% 1537096
#% 1673578
#% 1688289
#! Detecting and eliminating duplicates in databases is a task of critical importance in many applications. Although solutions for traditional models, such as relational data, have been widely studied, recently there has been some focus on solutions for more complex hierarchical structures as, for instance, XML data. Such data presents many different challenges, among which is the issue of how to exploit the schema structure to determine if two objects are duplicates. In this paper, we argue that structure can indeed have a significant impact on the process of duplicate detection. We propose a novel method that automatically restructures database objects in order to take full advantage of the relations between its attributes. This new structure reflects the relative importance of the attributes in the database and avoids the need to perform a manual selection. To test our approach we applied it to an existing duplicate detection system. Experiments performed on several datasets show that, using the new learned structure, we consistently outperform both the results obtained with the original database structure and those obtained by letting a knowledgeable user manually choose the attributes to compare.

#index 1641966
#* SISP: a new framework for searching the informative subgraph based on PSO
#@ Chen Chen;Guoren Wang;Huilin Liu;Junchang Xin;Ye Yuan
#t 2011
#c 1
#% 282905
#% 404719
#% 425081
#% 577273
#% 599545
#% 769887
#% 853538
#% 868094
#% 881496
#% 915344
#% 983805
#% 1040833
#% 1052710
#% 1063501
#% 1263246
#% 1268739
#% 1292670
#! A significant number of applications on graph require the key relations among a group of query nodes. Given a relational graph such as social network or biochemical interaction, an informative subgraph is urgent, which can best explain the relationships among a group of given query nodes. Based on Particle Swarm Optimization (PSO), a new framework of SISP (Searching the Informative Subgraph based on PSO) is proposed. SISP contains three key stages. In the initialization stage, a random spreading method is proposed, which can effectively guarantee the connectivity of the nodes in each particle; In the calculating stage of fitness, a fitness function is designed by incorporating a sign function with the goodness score; In the update stage, the intersection-based particle extension method and rule-based particle compression method are proposed. To evaluate the qualities of returned subgraphs, the appropriate calculating of goodness score is studied. Considering the importance and relevance of a node together, we present the PNR method, which makes the definition of informativeness more reliable and the returned subgraph more satisfying. At last, we present experiments on a real dataset and a synthetic dataset separately. The experimental results confirm that the proposed methods achieve increased accuracy and are efficient for any query set.

#index 1641967
#* Indexes for highly repetitive document collections
#@ Francisco Claude;Antonio Fariña;Miguel A. Martínez-Prieto;Gonzalo Navarro
#t 2011
#c 1
#% 118741
#% 311799
#% 516361
#% 544211
#% 754755
#% 786632
#% 864446
#% 867054
#% 936965
#% 1099209
#% 1153118
#% 1266886
#% 1292507
#% 1399964
#% 1404881
#% 1418499
#% 1459015
#% 1482302
#% 1604066
#% 1667821
#% 1688264
#! We introduce new compressed inverted indexes for highly repetitive document collections. They are based on run-length, Lempel-Ziv, or grammar-based compression of the differential inverted lists, instead of gap-encoding them as is the usual practice. We show that our compression methods significantly reduce the space achieved by classical compression, at the price of moderate slowdowns. Moreover, many of our methods are universal, that is, they do not need to know the versioning structure of the collection. We also introduce compressed self-indexes in the comparison. We show that techniques can compress much further, using a small fraction of the space required by our new inverted indexes, yet they are orders of magnitude slower.

#index 1641968
#* Partial duplicate detection for large book collections
#@ Ismet Zeki Yalniz;Ethem F. Can;R. Manmatha
#t 2011
#c 1
#% 201935
#% 255137
#% 320220
#% 345087
#% 347225
#% 413577
#% 465914
#% 504572
#% 571725
#% 654447
#% 874470
#% 879600
#% 967298
#% 978157
#% 1074122
#% 1446934
#% 1450881
#% 1455223
#! A framework is presented for discovering partial duplicates in large collections of scanned books with optical character recognition (OCR) errors. Each book in the collection is represented by the sequence of words (in the order they appear in the text) which appear only once in the book. These words are referred to as "unique words" and they constitute a small percentage of all the words in a typical book. Along with the order information the set of unique words provides a compact representation which is highly descriptive of the content and the flow of ideas in the book. By aligning the sequence of unique words from two books using the longest common subsequence (LCS) one can discover whether two books are duplicates. Experiments on several datasets show that DUPNIQ is more accurate than traditional methods for duplicate detection such as shingling and is fast. On a collection of 100K scanned English books DUPNIQ detects partial duplicates in 30 min using 350 cores and has precision 0.996 and recall 0.833 compared to shingling with precision 0.992 and recall 0.720. The technique works on other languages as well and is demonstrated for a French dataset.

#index 1641969
#* This image smells good: effects of image information scent in search engine results pages
#@ Faidon Loumakis;Simone Stumpf;David Grayson
#t 2011
#c 1
#% 201991
#% 297554
#% 324984
#% 325198
#% 325209
#% 343160
#% 344930
#% 452654
#% 590523
#% 717134
#% 720198
#% 802822
#% 874487
#% 954948
#% 1124986
#% 1137804
#% 1389359
#% 1450834
#% 1450891
#% 1742107
#! Users are confronted with an overwhelming amount of web pages when they look for information on the Internet. Current search engines already aid the user in their information seeking tasks by providing textual results but adding images to results pages could further help the user in judging the relevance of a result. We investigated this problem from an Information Foraging perspective and we report on two empirical studies that focused on the information scent of images. Our results show that images have their own distinct "smell" which is not as strong as that of text. We also found that combining images and text cues leads to a stronger overall scent. Surprisingly, when images were added to search engine results pages, this did not lead our participants to behave significantly differently in terms of effectiveness or efficiency. Even when we added images that could confuse the participants' scent, this had no significantly detrimental impact on their behaviour. However, participants expressed a preference for results pages which included images. We discuss potential challenges and point to future research to ensure the success of adding images to textual results in search engine results pages.

#index 1641970
#* Retrieving and ranking unannotated images through collaboratively mining online search results
#@ Songhua Xu;Hao Jiang;Francis Chi-Moon Lau
#t 2011
#c 1
#% 15462
#% 290703
#% 318785
#% 435065
#% 724559
#% 760805
#% 769243
#% 780874
#% 780875
#% 784995
#% 836904
#% 840455
#% 940440
#% 990307
#% 1055702
#% 1058303
#% 1085939
#% 1131878
#% 1132472
#% 1176901
#% 1178985
#% 1275012
#% 1279773
#% 1375784
#% 1378794
#% 1555363
#% 1560062
#! We present a new image search and ranking algorithm for retrieving unannotated images by collaboratively mining online search results which consist of online image and text search results. The online image search results are leveraged as reference examples to perform content-based image search over unannotated images. The online text search results are utilized to estimate the reference images' relevance to the search query. The key feature of our method is its capability to deal with unreliable online image search results through jointly mining visual and textual aspects of online search results. Through such collaborative mining, our algorithm infers the relevance of an online search result image to a text query. Once we obtain the estimate of query relevance score for each online image search result, we can selectively use query specific online search result images as reference examples for retrieving and ranking unannotated images. We tested our algorithm both on the standard public image datasets and several modestly sized personal photo collections. We also compared our method with two well-known peer methods. The results indicate that our algorithm is superior to existing content-based image search algorithms for retrieving and ranking unannotated images.

#index 1641971
#* Adaptive parallel approximate similarity search for responsive multimedia retrieval
#@ George Teodoro;Eduardo Valle;Nathan Mariano;Ricardo Torres;Wagner Meira, Jr.
#t 2011
#c 1
#% 114706
#% 291632
#% 299997
#% 318785
#% 333854
#% 337051
#% 342377
#% 342828
#% 402834
#% 435392
#% 465015
#% 479649
#% 654466
#% 760805
#% 818938
#% 824956
#% 896768
#% 919460
#% 940440
#% 1011225
#% 1015900
#% 1074684
#% 1090214
#% 1117433
#% 1130826
#% 1130882
#% 1130883
#% 1333257
#% 1421115
#% 1442467
#% 1464852
#% 1482189
#% 1711039
#% 1823795
#! This paper introduces Hypercurves, a flexible framework for pro- viding similarity search indexing to high throughput multimedia services. Hypercurves efficiently and effectively answers k-nearest neighbor searches on multigigabyte high-dimensional databases. It supports massively parallel processing and adapts at runtime its parallelization regimens to keep answer times optimal for either low and high demands. In order to achieve its goals, Hypercurves introduces new techniques for selecting parallelism configurations and allocating threads to computation cores, including hyperthreaded cores. Its efficiency gains are throughly validated on a large database of multimedia descriptors, where it presented near linear speedups and superlinear scaleups. The adaptation reduces query response times in 43% and 74% for both platforms tested, when compared to the best static parallelism regimens.

#index 1641972
#* A linear-time approximation of the earth mover's distance
#@ Min-Hee Jang;Sang-Wook Kim;Christos Faloutsos;Sunju Park
#t 2011
#c 1
#% 227706
#% 243035
#% 325683
#% 329791
#% 345848
#% 443397
#% 479462
#% 840455
#% 864398
#% 940440
#% 975141
#% 1022225
#% 1023420
#% 1046510
#% 1063484
#% 1132050
#% 1688294
#% 1730622
#% 1855134
#% 1855403
#% 1857842
#% 1858487
#! Color descriptors are one of the important features used in content-based image retrieval. The dominant color descriptor (DCD) represents a few perceptually dominant colors in an image through color quantization. For image retrieval based on DCD, the earth mover's distance and the optimal color composition distance are proposed to measure the dissimilarity between two images. Although providing good retrieval results, both methods are too time-consuming to be used in a large image database. To solve the problem, we propose a new distance function that calculates an approximate earth mover's distance in linear time. To calculate the dissimilarity in linear time, the proposed approach employs the space-filling curve for multidimensional color space. To improve the accuracy, the proposed approach uses multiple curves and adjusts the color positions. As a result, our approach achieves order-of-magnitude time improvement but incurs small errors. We have performed extensive experiments to show the effectiveness and efficiency of the proposed approach. The results reveal that our approach achieves almost the same results with the EMD in linear time.

#index 1641973
#* Towards a framework for attribute retrieval
#@ Arlind Kopliku;Mohand Boughanem;Karen Pinel-Sauvagnat
#t 2011
#c 1
#% 345754
#% 480824
#% 755816
#% 756964
#% 785365
#% 817419
#% 889107
#% 939896
#% 956501
#% 956564
#% 1035573
#% 1083705
#% 1127393
#% 1136348
#% 1275208
#% 1291356
#% 1409954
#% 1450836
#% 1483554
#% 1588407
#% 1631312
#% 1712125
#! In this paper, we propose an attribute retrieval approach which extracts and ranks attributes from HTML tables. We distinguish between class attribute retrieval and instance attribute retrieval. On one hand, given an instance (e.g. University of Strathclyde) we retrieve from the Web its attributes (e.g. principal, location, number of students). On the other hand, given a class (e.g. universities) represented by a set of instances, we retrieve common attributes of its instances. Furthermore, we show we can reinforce instance attribute retrieval if similar instances are available. Our approach uses HTML tables which are probably the largest source for attribute retrieval. Three recall oriented filters are applied over tables to check the following three properties: (i) is the table relational, (ii) has the table a header, and (iii) the conformity of its attributes and values. Candidate attributes are extracted from tables and ranked with a combination of relevance features. Our approach is shown to have a high recall and a reasonable precision. Moreover, it outperforms state of the art techniques.

#index 1641974
#* Building directories for social tagging systems
#@ Denis Helic;Markus Strohmaier
#t 2011
#c 1
#% 300078
#% 946524
#% 956589
#% 1053979
#% 1065406
#% 1152471
#% 1281994
#% 1355043
#% 1399985
#% 1429421
#% 1512435
#% 1560396
#% 1603812
#! Today, a number of algorithms exist for constructing tag hierarchies from social tagging data. While these algorithms were designed with ontological goals in mind, we know very little about their properties from an information retrieval perspective, such as whether these tag hierarchies support efficient navigation in social tagging systems. The aim of this paper is to investigate the usefulness of such tag hierarchies (sometimes also called folksonomies - from folk-generated taxonomy) as directories that aid navigation in social tagging systems. To this end, we simulate navigation of directories as decentralized search on a network of tags using Kleinberg's model. In this model, a tag hierarchy can be applied as background knowledge for decentralized search. By constraining the visibility of nodes in the directories we aim to mimic typical constraints imposed by a practical user interface (UI), such as limiting the number of displayed subcategories or related categories. Our experiments on five different social tagging datasets show that existing tag hierarchy algorithms can support navigation in theory, but our results also demonstrate that they face tremendous challenges when user interface (UI) restrictions are taken into account. Based on this observation, we introduce a new algorithm that constructs efficiently navigable directories on our datasets. The results are relevant for engineers and scientists aiming to improve navigability of social tagging systems.

#index 1641975
#* Workload-aware indexing for keyword search in social networks
#@ Truls A. Bjørklund;Michaela Götz;Johannes Gehrke;Nils Grimsmo
#t 2011
#c 1
#% 195456
#% 198335
#% 213786
#% 303072
#% 397151
#% 410276
#% 864446
#% 867054
#% 874702
#% 879609
#% 956544
#% 960298
#% 993947
#% 1024514
#% 1055710
#% 1070892
#% 1074116
#% 1127407
#% 1221039
#% 1227628
#% 1292509
#% 1292511
#% 1404881
#% 1426571
#% 1479593
#% 1667787
#% 1675941
#% 1730131
#! More and more data is accumulated inside social networks. Keyword search provides a simple interface for exploring this content. However, a lot of the content is private, and a search system must enforce the privacy settings of the social network. In this paper, we present a workload-aware keyword search system with access control based on a social network. We make two technical contributions: (1) HeapUnion, a novel union operator that improves processing of search queries with access control by up to a factor of two compared to the best previous solution; and (2) highly accurate cost models that vary in sophistication and accuracy; these cost models provide input to an optimization algorithm that selects the most efficient organization of access control meta-data for a given workload. Our experimental results with real and synthetic data show that our approach outperforms previous work by up to a factor of three.

#index 1641976
#* Effective retrieval of resources in folksonomies using a new tag similarity measure
#@ Giovanni Quattrone;Licia Capra;Pasquale De Meo;Emilio Ferrara;Domenico Ursino
#t 2011
#c 1
#% 224113
#% 577273
#% 946524
#% 1017565
#% 1052902
#% 1053979
#% 1077150
#% 1099462
#% 1100174
#% 1127456
#% 1202825
#% 1667787
#! Social (or folksonomic) tagging has become a very popular way to describe content within Web 2.0 websites. However, as tags are informally defined, continually changing, and ungoverned, it has often been criticised for lowering, rather than increasing, the efficiency of searching. To address this issue, a variety of approaches have been proposed that recommend users what tags to use, both when labeling and when looking for resources. These techniques work well in dense folksonomies, but they fail to do so when tag usage exhibits a power law distribution, as it often happens in real-life folksonomies. To tackle this issue, we propose an approach that induces the creation of a dense folksonomy, in a fully automatic and transparent way: when users label resources, an innovative tag similarity metric is deployed, so to enrich the chosen tag set with related tags already present in the folksonomy. The proposed metric, which represents the core of our approach, is based on the mutual reinforcement principle. Our experimental evaluation proves that the accuracy and coverage of searches guaranteed by our metric are higher than those achieved by applying classical metrics.

#index 1641977
#* Content-driven detection of campaigns in social media
#@ Kyumin Lee;James Caverlee;Zhiyuan Cheng;Daniel Z. Sui
#t 2011
#c 1
#% 255137
#% 345087
#% 754097
#% 937814
#% 982680
#% 1063503
#% 1074121
#% 1360738
#% 1482272
#% 1487804
#! We study the problem of detecting coordinated free text campaigns in large-scale social media. These campaigns -- ranging from coordinated spam messages to promotional and advertising campaigns to political astro-turfing -- are growing in significance and reach with the commensurate rise of massive-scale social systems. Often linked by common "talking points", there has been little research in detecting these campaigns. Hence, we propose and evaluate a content-driven framework for effectively linking free text posts with common "talking points" and extracting campaigns from large-scale social media. One of the salient aspects of the framework is an investigation of graph mining techniques for isolating coherent campaigns from large message-based graphs. Through an experimental study over millions of Twitter messages we identify five major types of campaigns -- Spam, Promotion, Template, News, and Celebrity campaigns -- and we show how these campaigns may be extracted with high precision and recall.

#index 1641978
#* Exploring categorization property of social annotations for information retrieval
#@ Peng Li;Bin Wang;Wei Jin;Jian-Yun Nie;Zhiwei Shi;Ben He
#t 2011
#c 1
#% 262096
#% 280819
#% 313959
#% 342621
#% 722904
#% 750863
#% 766430
#% 879587
#% 956515
#% 956544
#% 995140
#% 1035588
#% 1055743
#% 1074070
#% 1130827
#% 1130996
#% 1166510
#% 1227592
#% 1227754
#% 1338553
#% 1667787
#! User generated social annotations provide extra information for describing document contents. In this paper, we propose an effective method to model the categorization property of social annotations and explore the potential of combining it with classical language models for improving retrieval performance. Specifically, a novel TR-LDA model is presented to take annotations as an additional source for generating document contents apart from the document itself. We provide strategies for representing and weighting the categorization property and develop an efficient inference algorithm, where space saving is taken into account. Experiments are carried out on synthetic datasets, where documents and queries come from the standard evaluation conference TREC and annotations come from the website Delicious.com. Our results demonstrate the effectiveness of the proposed method on the ad-hoc retrieval task, which significantly outperforms state-of-art baselines.

#index 1641979
#* Context-aware search personalization with concept preference
#@ Di Jiang;Kenneth Wai-Ting Leung;Wilfred Ng
#t 2011
#c 1
#% 577224
#% 818207
#% 823348
#% 857180
#% 879737
#% 946521
#% 1119129
#% 1130877
#% 1130878
#% 1166492
#% 1187376
#% 1292473
#% 1446821
#% 1450864
#% 1450885
#! As the size of the web is growing rapidly, a well-recognized challenge for developing web search engines is to optimize the search result towards each user's preference. In this paper, we propose and develop a new personalization framework that captures the user's preference in the form of concepts obtained by mining web search contexts. The search context consists of both the user's clickthroughs and query reformulations that satisfy some specific information need, which is able to provide more information than each individual query in a search session. We also propose a method that discovers search contexts by one-pass of raw search query log. Using the information of the search context, we develop eight strategies that derive conceptual preference judgment. A learning-to-rank approach is employed to combine the derived preference judgments and then a Context-Aware User Profile (CAUP) is created. We further employ CAUP to adapt a personalized ranking function. Experimental results demonstrate that our approach captures accurate and comprehensive user's preference and, in terms of Top-N results quality, outperforms those existing concept-based personalization approaches without using search contexts.

#index 1641980
#* A framework for personalized and collaborative clustering of search results
#@ David C. Anastasiu;Byron J. Gao;David Buttler
#t 2011
#c 1
#% 36672
#% 214711
#% 218992
#% 262045
#% 281186
#% 375017
#% 427921
#% 464291
#% 590523
#% 754059
#% 754124
#% 766433
#% 769881
#% 801383
#% 813043
#% 879615
#% 956515
#% 987203
#% 1055719
#% 1074070
#% 1077150
#% 1200254
#% 1202162
#% 1214742
#% 1274862
#% 1400132
#% 1450916
#! How to organize and present search results plays a critical role in the utility of search engines. Due to the unprecedented scale of the Web and diversity of search results, the common strategy of ranked lists has become increasingly inadequate, and clustering has been considered as a promising alternative. Clustering divides a long list of disparate search results into a few topic-coherent clusters, allowing the user to quickly locate relevant results by topic navigation. While many clustering algorithms have been proposed that innovate on the automatic clustering procedure, we introduce ClusteringWiki, the first prototype and framework for personalized clustering that allows direct user editing of the clustering results. Through a Wiki interface, the user can edit and annotate the membership, structure and labels of clusters for a personalized presentation. In addition, the edits and annotations can be shared among users as a mass-collaborative way of improving search result organization and search engine utility.

#index 1641981
#* Using query log and social tagging to refine queries based on latent topics
#@ Lidong Bing;Wai Lam;Tak-Lam Wong
#t 2011
#c 1
#% 348155
#% 449294
#% 577224
#% 722904
#% 730007
#% 754125
#% 823348
#% 869501
#% 878624
#% 879567
#% 955496
#% 987193
#% 987212
#% 987272
#% 1074052
#% 1074098
#% 1074170
#% 1083721
#% 1117083
#% 1130852
#% 1130854
#% 1130855
#% 1130868
#% 1173699
#% 1227619
#% 1292473
#% 1355020
#% 1400017
#% 1482240
#% 1482275
#% 1482284
#% 1482292
#! An important way to improve users' satisfaction in Web search is to assist them to issue more effective queries. One such approach is query refinement (reformulation), which generates new queries according to the current query issued by users. A common procedure for conducting refinement is to generate some candidate queries first, and then a scoring method is designed to assess the quality of these candidates. Currently, most of the existing methods are context based. They rely heavily on the context relation of terms in the historical queries, and cannot detect and maintain the semantic consistency of queries. In this paper, we propose a graphical model to score queries. The proposed model exploits a latent topic space, which is automatically derived from the query log, to assess the semantic dependency of terms in a query. In the graphical model, both term context dependency and topic context dependency are considered. This also makes it feasible to score some queries which do not have much available historical term context information. We also utilize social tagging data in the candidate query generation process. Based on the observation that different users may tag the same resource with different tags of similar meaning, we propose a method to mine these term pairs for new candidate query construction.

#index 1641982
#* Retrieval models for audience selection in display advertising
#@ Sarah K. Tyler;Sandeep Pandey;Evgeniy Gabrilovich;Vanja Josifovski
#t 2011
#c 1
#% 342707
#% 465895
#% 879633
#% 881544
#% 956546
#% 987194
#% 1055713
#% 1077150
#% 1190081
#% 1214642
#% 1356185
#% 1405661
#% 1450847
#! Web applications often rely on user profiles of observed user actions, such as queries issued, page views, etc. In audience selection for display advertising, the audience that is likely to be responsive to a given ad campaign is identified via such profiles. We formalize the audience selection problem as a ranked retrieval task over an index of known users. We focus on the common case of audience selection where a small seed set of users who have previously responded positively to the campaign is used to identify a broader target audience. The actions of the users in the seed set are aggregated to construct a query, the query is then executed against an index of other user profiles to retrieve the highest scoring profiles. We validate our approach on a real-world dataset, demonstrating the trade-offs of different user and query models and that our approach is particularly robust for small campaigns. The proposed user modeling framework is applicable to many other applications requiring user profiles such as content suggestion and personalization.

#index 1641983
#* A language model approach to capture commercial intent and information relevance for sponsored search
#@ Lei Wang;Mingjiang Ye;Yu Zou
#t 2011
#c 1
#% 46803
#% 262096
#% 280819
#% 340901
#% 342707
#% 413573
#% 722904
#% 1055681
#% 1074101
#% 1130909
#% 1166534
#% 1227742
#% 1355050
#% 1355052
#% 1450899
#% 1482280
#% 1650298
#! A fundamental task of sponsored search is how to find the best match between web search queries and textual advertisements. To address this problem, we explicitly characterize the criteria for an advertisement to be a 'good match' to a query from two aspects (it should be relevant with the query from information perspective, and it should be able to capture and satisfy the commercial intent in the query). Correspondingly, we introduce in this paper a mixture language model of two parts: a commercial model which characterizes language bias of commercial intent leveraging on users' clicks on advertisements, and an informational model which is a traditional language model with consideration of the entropy of each word to capture informational relevance. We then introduce a regularized expectation-maximization (EM) algorithm model for parameters estimation, and integrate query commercial intent into the scoring function to boost overall click efficiency. Empirical evaluation shows that our model achieves better performance as compared to a well tuned classical language model and deliberated TFIDF-pLSI model (6% and 5% precision improvement at our operating point in production environment of 30% recall, and 5.3% and 6.3% AUC improvement), and performs superior to the KL Divergence language model for tail queries (0.5% nDCG improvement). Live traffic test shows over 2% CTR lift and 2.5% RPS lift as well.

#index 1641984
#* Learning to rank audience for behavioral targeting in display ads
#@ Jian Tang;Ning Liu;Jun Yan;Yelong Shen;Shaodan Guo;Bin Gao;Shuicheng Yan;Ming Zhang
#t 2011
#c 1
#% 277396
#% 705252
#% 722904
#% 1190081
#% 1211773
#% 1214642
#% 1214758
#% 1400052
#% 1450931
#! Behavioral targeting (BT), which aims to sell advertisers those behaviorally related user segments to deliver their advertisements, is facing a bottleneck in serving the rapid growth of long tail advertisers. Due to the small business nature of the tail advertisers, they generally expect to accurately reach a small group of audience, which is hard to be satisfied by classical BT solutions with large size user segments. In this paper, we propose a novel probabilistic generative model named Rank Latent Dirichlet Allocation (RANKLDA) to rank audience according to their ads click probabilities for the long tail advertisers to deliver their ads. Based on the basic assumption that users who clicked the same group of ads will have a higher probability of sharing similar latent search topical interests, RANKLDA combines topic discovery from users' search behaviors and learning to rank users from their ads click behaviors together. In computation, the topic learning could be enhanced by the supervised information of the rank learning and simultaneously, the rank learning could be better optimized by considering the discovered topics as features. This co-optimization scheme enhances each other iteratively. Experiments over the real click-through log of display ads in a public ad network show that the proposed RANKLDA model can effectively rank the audience for the tail advertisers.

#index 1641985
#* Simulating simple user behavior for system effectiveness evaluation
#@ Ben Carterette;Evangelos Kanoulas;Emine Yilmaz
#t 2011
#c 1
#% 262105
#% 411762
#% 823348
#% 879566
#% 879567
#% 936215
#% 987203
#% 1074092
#% 1074133
#% 1095876
#% 1166473
#% 1263584
#% 1292527
#% 1292528
#% 1366523
#% 1450904
#% 1482378
#% 1598424
#% 1598439
#! Information retrieval effectiveness evaluation typically takes one of two forms: batch experiments based on static test collections, or lab studies measuring actual users interacting with a system. Test collection experiments are sometimes viewed as introducing too many simplifying assumptions to accurately predict the usefulness of a system to its users. As a result, there is great interest in creating test collections and measures that better model user behavior. One line of research involves developing measures that include a parameterized user model; choosing a parameter value simulates a particular type of user. We propose that these measures offer an opportunity to more accurately simulate the variance due to user behavior, and thus to analyze system effectiveness to a simulated user population. We introduce a Bayesian procedure for producing sampling distributions from click data, and show how to use statistical tools to quantify the effects of variance due to parameter selection.

#index 1641986
#* Click the search button and be happy: evaluating direct and immediate information access
#@ Tetsuya Sakai;Makoto P. Kato;Young-In Song
#t 2011
#c 1
#% 232685
#% 411762
#% 818257
#% 840846
#% 893431
#% 940047
#% 950043
#% 987196
#% 987201
#% 1227582
#% 1227640
#% 1263584
#% 1455256
#% 1536510
#% 1536523
#% 1560357
#% 1598438
#! We define Direct Information Access as a type of information access where there is no user operation such as clicking or scrolling between the user's click on the search button and the user's information acquisition; we define Immediate Information Access as a type of information access where the user can locate the relevant information within the system output very quickly. Hence, a Direct and Immediate Information Access (DIIA) system is expected to satisfy the user's information need very quickly with its very first response. We propose a nugget-based evaluation framework for DIIA, which takes nugget positions into account in order to evaluate the ability of a system to present important nuggets first and to minimise the amount of text the user has to read. To demonstrate the integrity, usefulness and limitations of our framework, we built a Japanese DIIA test collection with 60 queries and over 2,800 nuggets as well as an offset-based nugget match evaluation interface, and conducted experiments with manual and automatic runs. The results suggest our proposal is a useful complement to traditional ranked retrieval evaluation based on document relevance.

#index 1641987
#* Local computation of PageRank: the ranking side
#@ Marco Bressan;Luca Pretto
#t 2011
#c 1
#% 268079
#% 268087
#% 309151
#% 402266
#% 430190
#% 577367
#% 754117
#% 783528
#% 794132
#% 799242
#% 799636
#% 810581
#% 818241
#% 840722
#% 878224
#% 921816
#% 967278
#% 1016175
#% 1016177
#% 1023380
#% 1068351
#% 1100986
#% 1130835
#% 1299272
#% 1381953
#% 1392481
#! Imagine you are a social network user who wants to search, in a list of potential candidates, for the best candidate for a job on the basis of their PageRank-induced importance ranking. Is it possible to compute this ranking for a low cost, by visiting only small subnetworks around the nodes that represent each candidate? The fundamental problem underpinning this question, i.e. computing locally the PageRank ranking of k nodes in an $n$-node graph, was first raised by Chen et al. (CIKM 2004) and then restated by Bar-Yossef and Mashiach (CIKM 2008). In this paper we formalize and provide the first analysis of the problem, proving that any local algorithm that computes a correct ranking must take into consideration Ω(√(kn)) nodes -- even when ranking the top $k$ nodes of the graph, even if their PageRank scores are "well separated", and even if the algorithm is randomized (and we prove a stronger Ω(n) bound for deterministic algorithms). Experiments carried out on large, publicly available crawls of the web and of a social network show that also in practice the fraction of the graph to be visited to compute the ranking may be considerable, both for algorithms that are always correct and for algorithms that employ (efficient) local score approximations.

#index 1641988
#* Prioritizing relevance judgments to improve the construction of IR test collections
#@ Mehdi Hosseini;Ingemar J. Cox;Natasa Milic-Frayling;Trevor Sweeting;Vishwa Vinay
#t 2011
#c 1
#% 262097
#% 262102
#% 818222
#% 879598
#% 879632
#% 907496
#% 1019126
#% 1074126
#% 1227632
#% 1278067
#% 1355039
#% 1450897
#% 1622345
#! We consider the problem of optimally allocating a fixed budget to construct a test collection with associated relevance judgements, such that it can (i) accurately evaluate the relative performance of the participating systems, and (ii) generalize to new, previously unseen systems. We propose a two stage approach. For a given set of queries, we adopt the traditional pooling method and use a portion of the budget to evaluate a set of documents retrieved by the participating systems. Next, we analyze the relevance judgments to prioritize the queries and remaining pooled documents for further relevance assessments. The query prioritization is formulated as a convex optimization problem, thereby permitting efficient solution and providing a flexible framework to incorporate various constraints. Query-document pairs with the highest priority scores are evaluated using the remaining budget. We evaluate our resource optimization approach on the TREC 2004 Robust track collection. We demonstrate that our optimization techniques are cost efficient and yield a significant improvement in the reusability of the test collections.

#index 1641989
#* Evaluating an associative browsing model for personal information
#@ Jinyoung Kim;W. Bruce Croft;David Smith;Anton Bakalov
#t 2011
#c 1
#% 115437
#% 577224
#% 677746
#% 722904
#% 751830
#% 879622
#% 987195
#% 1047409
#% 1065099
#% 1185582
#% 1292493
#% 1292539
#% 1450835
#% 1482424
#! Recent studies suggest that associative browsing can be beneficial for personal information access. Associative browsing is intuitive for the user and complements other methods of accessing personal information, such as keyword search. In our previous work, we proposed an associative browsing model of personal information in which users can navigate through the space of documents and concepts (e.g., person names, events, etc.). Our approach differs from other systems in that it presented a ranked list of associations by combining multiple measures of similarity, whose weights are improved based on click feedback from the user. In this paper, we evaluate the associative browsing model we proposed in the context of known-item finding task. We performed game-based user studies as well as a small scale instrumentation study using a prototype system that helped us to collect a large amount of usage data from the participants. Our evaluation results show that the associative browsing model can play an important role in known-item finding. We also found that the system can learn to improve suggestions for browsing with a small amount of click data.

#index 1641990
#* Semi-supervised SVMs for classification with unknown class proportions and a small labeled dataset
#@ Sathiya Keerthi Selvaraj;Bigyan Bhar;Sundararajan Sellamanickam;Shirish Shevade
#t 2011
#c 1
#% 382680
#% 466263
#% 593097
#% 875222
#% 879624
#% 881477
#% 983814
#% 1074345
#% 1386130
#% 1455666
#! In the design of practical web page classification systems one often encounters a situation in which the labeled training set is created by choosing some examples from each class; but, the class proportions in this set are not the same as those in the test distribution to which the classifier will be actually applied. The problem is made worse when the amount of training data is also small. In this paper we explore and adapt binary SVM methods that make use of unlabeled data from the test distribution, viz., Transductive SVMs (TSVMs) and expectation regularization/constraint (ER/EC) methods to deal with this situation. We empirically show that when the labeled training data is small, TSVM designed using the class ratio tuned by minimizing the loss on the labeled set yields the best performance; its performance is good even when the deviation between the class ratios of the labeled training set and the test set is quite large. When the labeled training data is sufficiently large, an unsupervised Gaussian mixture model can be used to get a very good estimate of the class ratio in the test set; also, when this estimate is used, both TSVM and EC/ER give their best possible performance, with TSVM coming out superior. The ideas in the paper can be easily extended to multi-class SVMs and MaxEnt models.

#index 1641991
#* A pairwise ranking based approach to learning with positive and unlabeled examples
#@ Sundararajan Sellamanickam;Priyanka Garg;Sathiya Keerthi Selvaraj
#t 2011
#c 1
#% 464641
#% 722811
#% 722935
#% 727883
#% 729621
#% 730039
#% 734915
#% 765527
#% 840846
#% 840882
#% 881477
#% 1000760
#% 1083647
#% 1215874
#% 1279298
#% 1442579
#! A large fraction of binary classification problems arising in web applications are of the type where the positive class is well defined and compact while the negative class comprises everything else in the distribution for which the classifier is developed; it is hard to represent and sample from such a broad negative class. Classifiers based only on positive and unlabeled examples reduce human annotation effort significantly by removing the burden of choosing a representative set of negative examples. Various methods have been proposed in the literature for building such classifiers. Of these, the state of the art methods are Biased SVM and Elkan & Noto's methods. While these methods often work well in practice, they are computationally expensive since hyperparameter tuning is very important, particularly when the size of labeled positive examples set is small and class imbalance is high. In this paper we propose a pairwise ranking based approach to learn from positive and unlabeled examples (LPU) and we give a theoretical justification for it. We present a pairwise RankSVM (RSVM) based method for our approach. The method is simple, efficient, and its hyperparameters are easy to tune. A detailed experimental study using several benchmark datasets shows that the proposed method gives competitive classification performance compared to the mentioned state of the art methods, while training 3-10 times faster. We also propose an efficient AUC based feature selection technique in the LPU setting and demonstrate its usefulness on the datasets. To get an idea of the goodness of the LPU methods we compare them against supervised learning (SL) methods that also make use of negative examples in training. SL methods give a slightly better performance than LPU methods when there is a rich set of negative examples; however, they are inferior when the number of negative training examples is not large enough.

#index 1641992
#* Robust nonnegative matrix factorization using L21-norm
#@ Deguang Kong;Chris Ding;Heng Huang
#t 2011
#c 1
#% 336073
#% 875980
#% 881468
#% 1117063
#% 1176865
#% 1214657
#% 1250561
#% 1305478
#% 1327693
#% 1495611
#! Nonnegative matrix factorization (NMF) is widely used in data mining and machine learning fields. However, many data contain noises and outliers. Thus a robust version of NMF is needed. In this paper, we propose a robust formulation of NMF using L21 norm loss function. We also derive a computational algorithm with rigorous convergence analysis. Our robust NMF approach, (1) can handle noises and outliers; (2) provides very efficient and elegant updating rules; (3) incurs almost the same computational cost as standard NMF, thus potentially to be used in more real world application tasks. Experiments on 10 datasets show that the robust NMF provides more faithful basis factors and consistently better clustering results as compared to standard NMF.

#index 1641993
#* TAKES: a fast method to select features in the kernel space
#@ Ye Xu;Furao Shen;Wei Ping;Jinxi Zhao
#t 2011
#c 1
#% 126894
#% 224113
#% 266426
#% 324288
#% 465754
#% 577242
#% 722943
#% 769964
#% 783523
#% 786615
#% 818202
#% 857439
#% 865329
#% 883875
#% 899958
#% 915289
#% 975162
#% 983819
#% 983876
#% 987243
#% 1034774
#% 1041316
#% 1074066
#% 1117692
#% 1292711
#! Feature selection is an effective tool to deal with the "curse of dimensionality". To cope with the non-separable problem, feature selection in the kernel space has been investigated. However, previous study cannot adequately estimate the intrinsic dimensionality of the kernel space. Thus, it is difficult to accurately preserve the sketch of the kernel space using the learned basis, and the feature selection performance is affected. Moreover, the computing load of the algorithm reaches at least cubic with the number of training data. In this paper, we propose a fast framework to conduct feature selection in the kernel space. By designing a fast kernel subspace learning method, we automatically learn the intrinsic dimensionality and construct an orthogonal basis set of kernel space. The learned basis can accurately preserve the sketch of kernel space. Then backed by the constructed basis, we directly select features in kernel space. The whole proposed framework has a quadratic complexity with the number of training data, which is faster than existing kernel methods for feature selection. We evaluate our work under several typical datasets and find it not only preserves the sketch of the kernel space more accurately but also achieves better classification performance compared with many state-of-the-art methods.

#index 1641994
#* Designing an ensemble classifier over subspace classifiers using iterative convergence routine
#@ Bhanukiran Vinzamuri;Kamalakar Karlapalem
#t 2011
#c 1
#% 132938
#% 251145
#% 296375
#% 400847
#% 551723
#% 770763
#% 818916
#% 871026
#% 915244
#% 1275295
#% 1328215
#% 1411034
#% 1535357
#! There can be multiple classifiers for a given data set. One way to generate multiple classifiers is to use subspaces of the attribute sets. In this paper, we generate subspace classifiers by an iterative convergence routine to build an ensemble classifier. Experimental evaluation covers the cases of both labelled and unlabelled (blind) data separately. We evaluate our approach on many benchmark UC Irvine datasets to assess the robustness of our approach with varying induced noise levels. We explicitly compare and present the utility of the clusterings generated for classification using several diverse clustering dissimilarity metrics. Results show that our ensemble classifier is a more robust classifier in comparison to different multi-class classification approaches.

#index 1641995
#* Bayesian latent variable models for collaborative item rating prediction
#@ Morgan Harvey;Mark J. Carman;Ian Ruthven;Fabio Crestani
#t 2011
#c 1
#% 124010
#% 329569
#% 428272
#% 452563
#% 642990
#% 722904
#% 734592
#% 769906
#% 813966
#% 987198
#% 1083671
#% 1190066
#% 1190123
#% 1650569
#! Collaborative filtering systems based on ratings make it easier for users to find content of interest on the Web and as such they constitute an area of much research. In this paper we first present a Bayesian latent variable model for rating prediction that models ratings over each user's latent interests and also each item's latent topics. We describe a Gibbs sampling procedure that can be used to estimate its parameters and show by experiment that it is competitive with the gradient descent SVD methods commonly used in state-of-the-art systems. We then proceed to make an important and novel extension to this model, enhancing it with user-dependent and item-dependant biases to significantly improve rating estimation. We show by experiment on a large set of real ratings data that these models are able to outperform 3 common baselines, including a very competitive and modern SVD-based model. Furthermore we illustrate other advantages of our approach beyond simply its ability to provide more accurate ratings and show that it is able to perform better on the common and important case where the user profile is short.

#index 1641996
#* Timing when to buy
#@ Rakesh Agrawal;Samuel Ieong;Raja Velu
#t 2011
#c 1
#% 330687
#% 394984
#% 414514
#% 729921
#% 1396094
#% 1541748
#% 1605952
#! Most e-commerce sites to-date have focused on helping consumers decide what to buy and where to buy. We study the complementary question of helping consumers decide when to buy, focusing on consumer durables. We introduce a utility-based model for evaluating different approaches to this question. We focus on how best to make use of forecasts in making recommendations, and propose three natural strategies. We establish a relationship between these strategies, and show that one of them is optimal. We conduct a large-scale experimental study to test the performance and robustness of these strategies. Across a wide range of conditions, the best strategy obtains 90% of the maximum possible gains.

#index 1641997
#* Assisting web search users by destination reachability
#@ Chi-Hoon Lee;Alpa Jain;Larry Lai
#t 2011
#c 1
#% 280819
#% 330617
#% 641976
#% 642985
#% 643001
#% 643057
#% 728105
#% 783475
#% 869501
#% 869651
#% 987212
#% 987222
#% 1055677
#% 1130852
#% 1130854
#% 1450975
#% 1560359
#% 1712595
#! Search engine users are increasingly performing complex tasks based on the simple keyword-in document-out paradigm. To assist users in accomplishing their tasks effectively, search engines provide query recommendations based on the user's current query. These are suggestions for follow-up queries given the user-provided query. A large number of techniques have been proposed in the past on mining such query recommendations which include past user sessions (e.g., sequence of queries within a specified window of time) to identify most frequently occurring pairs, using click-through graphs (e.g., a bipartite graph of queries and the urls on which users clicked) and rank these suggestions using some form of frequency counts from the past query logs. Given the limited number of queries that are offered (typically 5) it is important to effectively rank them. In this paper, we present a novel approach to ranking query recommendations which not only consider relevance to the original query but also take into account efficiency of a query at accomplishing a user search task at hand. We formalize the notion of query efficiency and show how our objective function effectively captures this as determined by a human study and eliminates biases introduced by click-through based metrics. To compute this objective function, we present a pseudosupervised learning technique where no explicit human experts are required to label samples. In addition, our techniques effectively characterize preferred url destinations and project each query into a higher dimension space where each sub-spaces represents user intent using these characteristics. Finally, we present an extensive evaluation of our proposed methods against production systems and show our method to increase task completion efficiency by 15%.

#index 1641998
#* Modeling personalized email prioritization: classification-based and regression-based approaches
#@ Shinjae Yoo;Yiming Yang;Jaime Carbonell
#t 2011
#c 1
#% 315510
#% 319261
#% 413589
#% 456718
#% 458623
#% 818214
#% 829028
#% 840853
#% 939346
#% 987227
#% 1214718
#% 1650300
#% 1702628
#% 1860941
#% 1861694
#! Email overload, even after spam filtering, presents a serious productivity challenge for busy professionals and executives. One solution is automated prioritization of incoming emails to ensure the most important are read and processed quickly, while others are processed later as/if time permits in declining priority levels. This paper presents a study of machine learning approaches to email prioritization into discrete levels, comparing ordinal regression versus classifier cascades. Given the ordinal nature of discrete email priority levels, SVM ordinal regression would be expected to perform well, but surprisingly a cascade of SVM classifiers significantly outperforms ordinal regression for email prioritization. In contrast, SVM regression performs well -- better than classifiers -- on selected UCI data sets. This unexpected performance inversion is analyzed and results are presented, providing core functionality for email prioritization systems.

#index 1641999
#* Diversification and refinement in collaborative filtering recommender
#@ Rubi Boim;Tova Milo;Slava Novgorodov
#t 2011
#c 1
#% 805841
#% 813966
#% 875957
#% 960287
#% 1127465
#% 1181244
#% 1190093
#% 1207001
#% 1321509
#% 1328120
#% 1358747
#% 1471597
#% 1472964
#% 1549873
#% 1594669
#! This paper considers a popular class of recommender systems that are based on Collaborative Filtering (CF) and proposes a novel technique for diversifying the recommendations that they give to users. Items are clustered based on a unique notion of priority-medoids that provides a natural balance between the need to present highly ranked items vs. highly diverse ones. Our solution estimates items diversity by comparing the rankings that different users gave to the items, thereby enabling diversification even in common scenarios where no semantic information on the items is available. It also provides a natural zoom-in mechanism to focus on items (clusters) of interest and recommending diversified similar items. We present DiRec a plug-in that implements the above concepts and allows CF Recommender systems to diversify their recommendations. We illustrate the operation of DiRec in the context of a movie recommendation system and present a thorough experimental study that demonstrates the effectiveness of our recommendation diversification technique and its superiority over previous solutions.

#index 1642000
#* Emerging topic detection using dictionary learning
#@ Shiva Prasad Kasiviswanathan;Prem Melville;Arindam Banerjee;Vikas Sindhwani
#t 2011
#c 1
#% 263387
#% 274586
#% 329562
#% 350859
#% 444064
#% 722904
#% 812400
#% 855200
#% 1077150
#% 1164188
#% 1299092
#% 1301020
#% 1386100
#% 1432574
#% 1455009
#% 1470583
#% 1481749
#% 1482205
#% 1650298
#% 1745124
#% 1759275
#% 1817229
#! Streaming user-generated content in the form of blogs, microblogs, forums, and multimedia sharing sites, provides a rich source of data from which invaluable information and insights maybe gleaned. Given the vast volume of such social media data being continually generated, one of the challenges is to automatically tease apart the emerging topics of discussion from the constant background chatter. Such emerging topics can be identified by the appearance of multiple posts on a unique subject matter, which is distinct from previous online discourse. We address the problem of identifying emerging topics through the use of dictionary learning. We propose a two stage approach respectively based on detection and clustering of novel user-generated content. We derive a scalable approach by using the alternating directions method to solve the resulting optimization problems. Empirical results show that our proposed approach is more effective than several baselines in detecting emerging topics in traditional news story and newsgroup data. We also demonstrate the practical application to social media analysis, based on a study on streaming data from Twitter.

#index 1642001
#* Focusing on novelty: a crawling strategy to build diverse language models
#@ Luciano Barbosa;Srinivas Bangalore
#t 2011
#c 1
#% 281251
#% 330599
#% 342398
#% 348138
#% 466250
#% 480309
#% 624428
#% 642994
#% 956538
#% 1019761
#% 1130819
#% 1166473
#% 1193315
#% 1237597
#% 1263581
#% 1292510
#% 1301004
#% 1399978
#! Word prediction performed by language models has an important role in many tasks as e.g. word sense disambiguation, speech recognition, hand-writing recognition, query spelling and query segmentation. Recent research has exploited the textual content of the Web to create language models. In this paper, we propose a new focused crawling strategy to collect Web pages that focuses on novelty in order to create diverse language models. In each crawling cycle, the crawler tries to ll the gaps present in the current language model built from previous cycles, by avoiding visiting pages whose vocabulary is already well represented in the model. It relies on an information theoretic measure to identify these gaps and then learns link patterns to pages in these regions in order to guide its visitation policy. To handle constantly evolving domains, a key feature of our crawler approach is its ability to adjust its focus as the crawl progresses. We evaluate our approach in two different scenarios in which our solution can be useful. First, we demonstrate that our approach produces more effective language models than the ones created by a baseline crawler in the context of a speech recognition task of broadcast news. In fact, in some cases, our crawler was able to obtain similar results to the baseline by crawling only 12.5% of the pages collected by the latter. Secondly, since in the news domain avoiding well-represented content might lead to novelty, i.e. up-to-date pages, we show that our diversity-based crawler can also be helpful to guide the crawler for the most recent content in the news. The results show that our approach was able to obtain on average 50% more up-to-date pages than the baseline crawler.

#index 1642002
#* Natural event summarization
#@ Yexi Jiang;Chang-Shing Perng;Tao Li
#t 2011
#c 1
#% 116390
#% 232136
#% 259633
#% 322619
#% 342633
#% 463903
#% 464986
#% 479785
#% 570889
#% 575972
#% 577226
#% 729999
#% 823418
#% 832572
#% 934581
#% 989678
#% 1021491
#% 1083670
#% 1206639
#% 1268047
#% 1268739
#% 1279663
#% 1306056
#% 1426517
#% 1697259
#! Event mining is a useful way to understand computer system behaviors. The focus of recent works on event mining has been shifted to event summarization from discovering frequent patterns. Event summarization seeks to provide a comprehensible explanation of the event sequence on certain aspects. Previous methods have several limitations such as ignoring temporal information, generating the same set of boundaries for all event patterns, and providing a summary which is difficult for human to understand. In this paper, we propose a novel framework called natural event summarization that summarizes an event sequence using inter-arrival histograms to capture the temporal relationship among events. Our framework uses the minimum description length principle to guide the process in order to balance between accuracy and brevity. Also, we use multi-resolution analysis for pruning the problem space. We demonstrate how the principles can be applied to generate summaries with periodic patterns and correlation patterns in the framework. Experimental results on synthetic and real data show our method is capable of producing usable event summary, robust to noises, and scalable.

#index 1642003
#* Transferring topical knowledge from auxiliary long texts for short text clustering
#@ Ou Jin;Nathan N. Liu;Kai Zhao;Yong Yu;Qiang Yang
#t 2011
#c 1
#% 869500
#% 876034
#% 983899
#% 1055680
#% 1073897
#% 1074129
#% 1214660
#% 1224331
#% 1250629
#% 1269907
#% 1292559
#% 1338553
#% 1450992
#% 1464068
#% 1498610
#% 1561559
#% 1583279
#% 1633202
#! With the rapid growth of social Web applications such as Twitter and online advertisements, the task of understanding short texts is becoming more and more important. Most traditional text mining techniques are designed to handle long text documents. For short text messages, many of the existing techniques are not effective due to the sparseness of text representations. To understand short messages, we observe that it is often possible to find topically related long texts, which can be utilized as the auxiliary data when mining the target short texts data. In this article, we present a novel approach to cluster short text messages via transfer learning from auxiliary long text data. We show that while some previous work exists that enhance short text clustering with related long texts, most of them ignore the semantic and topical inconsistencies between the target and auxiliary data and hurt the clustering performance. To accommodate the possible inconsistency between source and target data, we propose a novel topic model - Dual Latent Dirichlet Allocation (DLDA) model, which jointly learns two sets of topics on short and long texts and couples the topic parameters to cope with the potential inconsistency between data sets. We demonstrate through large-scale clustering experiments on both advertisements and Twitter data that we can obtain superior performance over several state-of-art techniques for clustering short text documents.

#index 1642004
#* LogSig: generating system events from raw textual logs
#@ Liang Tang;Tao Li;Chang-Shing Perng
#t 2011
#c 1
#% 289130
#% 406493
#% 464291
#% 722803
#% 770782
#% 818916
#% 823418
#% 835018
#% 848846
#% 897239
#% 922066
#% 965295
#% 989678
#% 1083670
#% 1176904
#% 1214747
#% 1238937
#% 1268047
#% 1306057
#% 1373703
#% 1426517
#% 1468578
#% 1535456
#! Modern computing systems generate large amounts of log data. System administrators or domain experts utilize the log data to understand and optimize system behaviors. Most system logs are raw textual and unstructured. One main fundamental challenge in automated log analysis is the generation of system events from raw textual logs. Log messages are relatively short text messages but may have a large vocabulary, which often result in poor performance when applying traditional text clustering techniques to the log data. Other related methods have various limitations and only work well for some particular system logs. In this paper, we propose a message signature based algorithm logSig to generate system events from textual log messages. By searching the most representative message signatures, logSig categorizes log messages into a set of event types. logSig can handle various types of log data, and is able to incorporate human's domain knowledge to achieve a high performance. We conduct experiments on five real system log data. Experiments show that logSig outperforms other alternative algorithms in terms of the overall performance.

#index 1642005
#* Coupling or decoupling for KNN search on road networks?: a hybrid framework on user query patterns
#@ Ying-Ju Chen;Kun-Ta Chuang;Ming-Syan Chen
#t 2011
#c 1
#% 274612
#% 300132
#% 527187
#% 654478
#% 824723
#% 893162
#% 1015321
#% 1016199
#% 1063472
#% 1181299
#% 1688257
#% 1692266
#% 1720746
#% 1720757
#% 1728229
#! We explore in this paper a new KNN algorithm, called the SQUARE algorithm, for searching spatial objects on road networks. Recent works in the literature discussed the necessity to support object updates for promising location-based services. Among them, the decoupling spatial search algorithms, which separate the handle of the network traversal and the object lookup, has been recognized as the most effective approach to cut the maintenance overhead from updates. However, the queue-based network traversal needs to be performed from scratch for each KNN query until the KNN objects are exactly identified, indicating that the query complexity is in proportion to the number of visited network nodes. The query efficiency is concerned for online LBS applications since they only allow lightweight operations for minimizing the query latency. To improve the query scalability while supporting data updates, SQUARE constructs the network index similar to the way used in decoupling models, and meanwhile exploit the coupling idea to maintain the KNN information relative to hot regions in the network index. The hot region denotes the area with frequent queries discovered in the query history. Inspired from the prevalently observed 80-20 rule, SQUARE can maximize the query throughput by returning KNN results in the quasi-constant time for 80% queries that are roughly issued within 20% area (hot regions). As validated in our experimental results, SQUARE outperforms previous works and achieves the significant performance improvement without sacrifice on the maintenance overhead for object updates.

#index 1642006
#* Toward traffic-driven location-based web search
#@ Zhiyuan Cheng;James Caverlee;Krishna Yeswanth Kamath;Kyumin Lee
#t 2011
#c 1
#% 46803
#% 172949
#% 290830
#% 330677
#% 480467
#% 572480
#% 577224
#% 783482
#% 805839
#% 836094
#% 855307
#% 876074
#% 1055694
#% 1055914
#% 1378458
#% 1480830
#% 1503356
#% 1536521
#% 1536522
#% 1560388
#% 1573621
#% 1676080
#! The emergence of location sharing services is rapidly accelerating the convergence of our online and offline activities. In one direction, Foursquare, Google Latitude, Facebook Places, and related services are enriching real-world venues with the social and semantic connections among online users. In analogy to how clickstreams have been successfully incorporated into traditional web ranking based on content and link analysis, we propose to mine traffic patterns revealed through location sharing services to augment traditional location-based search. Concretely, we study location-based traffic patterns revealed through location sharing services and find that these traffic patterns can identify semantically related locations. Based on this observation, we propose and evaluate a traffic-driven location clustering algorithm that can group semantically related locations with high confidence. Through experimental study of 12 million locations from Foursquare, we extend this result through supervised location categorization, wherein traffic patterns can be used to accurately predict the semantic category of uncategorized locations. Based on these results, we show how traffic-driven semantic organization of locations may be naturally incorporated into location-based web search.

#index 1642007
#* CLUES: a unified framework supporting interactive exploration of density-based clusters in streams
#@ Di Yang;Zhenyu Guo;Elke A. Rundensteiner;Matthew O. Ward
#t 2011
#c 1
#% 434614
#% 479658
#% 576113
#% 654489
#% 810047
#% 844509
#% 878299
#% 881538
#% 881938
#% 889089
#% 989584
#% 1015261
#% 1016157
#% 1083627
#% 1176888
#% 1181258
#! Although various mining algorithms have been proposed in the literature to efficiently compute clusters, few strides have been made to date in helping analysts to interactively explore such patterns in the stream context. We present a framework called CLUES to both computationally and visually support the process of real-time mining of density-based clusters. CLUES is composed of three major components. First, as foundation of CLUES, we develop an evolution model of density-based clusters in data streams that captures the complete spectrum of cluster evolution types across streaming windows. Second, to equip CLUES with the capability of efficiently tracking cluster evolution, we design a novel algorithm to piggy-back the evolution tracking process into the underlying cluster detection process. Third, CLUES organizes the detected clusters and their evolution interrelationships into a multidimensional pattern space - presenting clusters at different time horizons and across different abstraction levels. It provides a rich set of visualization and interaction techniques to allow the analyst to explore this multi-dimensional pattern space in real-time. Our experimental evaluation, including performance studies and a user study, using real streams from ground group movement monitoring and from stock transaction domains confirm both the efficiency and effectiveness of our proposed CLUES framework.

#index 1642008
#* e-NSP: efficient negative sequential pattern mining based on identified positive patterns without database rescanning
#@ Xiangjun Dong;Zhigang Zheng;Longbing Cao;Yanchang Zhao;Chengqi Zhang;Jinjiu Li;Wei Wei;Yuming Ou
#t 2011
#c 1
#% 463903
#% 767654
#% 1040761
#% 1114499
#% 1156596
#% 1196009
#% 1442149
#% 1737785
#% 1960014
#! Mining Negative Sequential Patterns (NSP) is much more challenging than mining Positive Sequential Patterns (PSP) due to the high computational complexity and huge search space required in calculating Negative Sequential Candidates (NSC). Very few approaches are available for mining NSP, which mainly rely on re-scanning databases after identifying PSP. As a result, they are very inefficient. In this paper, we propose an efficient algorithm for mining NSP, called e-NSP, which mines for NSP by only involving the identified PSP, without re-scanning databases. First, negative containment is defined to determine whether or not a data sequence contains a negative sequence. Second, an efficient approach is proposed to convert the negative containment problem to a positive containment problem. The supports of NSC are then calculated based only on the corresponding PSP. Finally, a simple but efficient approach is proposed to generate NSC. With e-NSP, mining NSP does not require additional database scans, and the existing PSP mining algorithms can be integrated into e-NSP to mine for NSP efficiently. e-NSP is compared with two currently available NSP mining algorithms on 14 synthetic and real-life datasets. Intensive experiments show that e-NSP takes as little as 3% of the runtime of the baseline approaches and is applicable for efficient mining of NSP in large datasets.

#index 1642009
#* Optimising ontology stream reasoning with truth maintenance system
#@ Yuan Ren;Jeff Z. Pan
#t 2011
#c 1
#% 152928
#% 1190165
#% 1289408
#% 1413157
#% 1655410
#% 1705032
#% 1737582
#! So far researchers in the Description Logics / Ontology communities mainly consider ontology reasoning services for static ontologies. The rapid development of the Semantic Web and its emerging data ask for reasoning technologies for dynamic knowledge streams. Existing work on stream reasoning is focused on lightweight languages such as RDF and RDFS. In this paper, we introduce the notion of Ontology Stream Management System (OSMS) and present a stream-reasoning approach based on Truth Maintenance System (TMS). We present optimised EL++ algorithm to reduce memory consumption. Our evaluations show that the optimisation improves TMS-enabled EL++ reasoning to deal with relatively large volumes of data and update efficiently.

#index 1642010
#* Harvesting facts from textual web sources by constrained label propagation
#@ Yafang Wang;Bin Yang;Lizhen Qu;Marc Spaniol;Gerhard Weikum
#t 2011
#c 1
#% 783552
#% 891549
#% 938706
#% 939517
#% 943834
#% 956564
#% 1052713
#% 1055735
#% 1055761
#% 1089602
#% 1190065
#% 1267783
#% 1330550
#% 1346154
#% 1372745
#% 1409954
#% 1471327
#% 1481634
#% 1536527
#% 1586117
#% 1697416
#! There have been major advances on automatically constructing large knowledge bases by extracting relational facts from Web and text sources. However, the world is dynamic: periodic events like sports competitions need to be interpreted with their respective timepoints, and facts such as coaching a sports team, holding political or business positions, and even marriages do not hold forever and should be augmented by their respective timespans. This paper addresses the problem of automatically harvesting temporal facts with such extended time-awareness. We employ pattern-based gathering techniques for fact candidates and construct a weighted pattern-candidate graph. Our key contribution is a system called PRAVDA based on a new kind of label propagation algorithm with a judiciously designed loss function, which iteratively processes the graph to label good temporal facts for a given set of target relations. Our experiments with online news and Wikipedia articles demonstrate the accuracy of this method.

#index 1642011
#* Towards a top-down and bottom-up bidirectional approach to joint information extraction
#@ Xiaofeng Yu;Irwin King;Michael R. Lyu
#t 2011
#c 1
#% 303620
#% 464434
#% 840966
#% 915340
#% 940028
#% 946811
#% 961269
#% 987235
#% 989662
#% 1117681
#% 1166535
#% 1214745
#% 1251719
#% 1261597
#% 1265116
#% 1267781
#% 1269815
#% 1269895
#% 1292498
#% 1328349
#% 1451217
#% 1544164
#% 1650488
#% 1810385
#! Most high-level information extraction (IE) consists of compound and aggregated subtasks. Such IE problems are generally challenging and they have generated increasing interest recently. We investigate two representative IE tasks: (1) entity identification and relation extraction from Wikipedia, and (2) citation matching, and we formally define joint optimization of information extraction. We propose a joint paradigm integrating three factors -- segmentation, relation, and segmentation-relation joint factors, to solve all relevant subtasks simultaneously. This modeling offers a natural formalism for exploiting bidirectional rich dependencies and interactions between relevant subtasks to capture mutual benefits. Since exact parameter estimation is prohibitively intractable, we present a general, highly-coupled learning algorithm based on variational expectation maximization (VEM) to perform parameter estimation approximately in a top-down and bottom-up manner, such that information can flow bidirectionally and mutual benefits from different subtasks can be well exploited. In this algorithm, both segmentation and relation are optimized iteratively and collaboratively using hypotheses from each other. We conducted extensive experiments using two real-world datasets to demonstrate the promise of our approach.

#index 1642012
#* From names to entities using thematic context distance
#@ Anja Pilz;Gerhard Paaß
#t 2011
#c 1
#% 318412
#% 402289
#% 577224
#% 577318
#% 722904
#% 747890
#% 830275
#% 1092530
#% 1183237
#% 1642232
#% 1696287
#! Name ambiguity arises from the polysemy of names and causes uncertainty about the true identity of entities referenced in unstructured text. This is a major problem in areas like information retrieval or knowledge management, for example when searching for a specific entity or updating an existing knowledge base. We approach this problem of named entity disambiguation (NED) using thematic information derived from Latent Dirichlet Allocation (LDA) to compare the entity mention's context with candidate entities in Wikipedia represented by their respective articles. We evaluate various distances over topic distributions in a supervised classification setting to find the best suited candidate entity, which is either covered in Wikipedia or unknown. We compare our approach to a state of the art method and show that it achieves significantly better results in predictive performance, regarding both entities covered in Wikipedia as well as uncovered entities. We show that our approach is in general language independent as we obtain equally good results for named entity disambiguation using the English, the German and the French Wikipedia.

#index 1642013
#* Learning conditional random fields with latent sparse features for acronym expansion finding
#@ Jie Liu;Jimeng Chen;Yi Zhang;Yalou Huang
#t 2011
#c 1
#% 190581
#% 464434
#% 770759
#% 905089
#% 1535431
#% 1650403
#% 1713162
#! The ever increasing usage of acronyms in many kinds of documents, including web pages, is becoming an obstacle for average readers. This paper studies the task of finding expansions in documents for a given set of acronyms. We cast the expansion finding problem as a sequence labeling task and adapt Conditional Random Fields (CRF) to solve it. While adapting CRFs, we enhance the performance from two aspects. First, we introduce nonlinear hidden layers to learn better representations of the input data. Second, we design simple and effective features. We create a hand labeled evaluation data based on Wikipedia.org and web crawling. We evaluate the effectiveness of several algorithms in solving the expansion finding problem. The experimental results demonstrate that the new method achieves performs better than Support Vector Machine and standard Conditional Random Fields.

#index 1642014
#* Accounting for data dependencies within a hierarchical dirichlet process mixture model
#@ Dongwoo Kim;Alice Oh
#t 2011
#c 1
#% 722904
#% 788094
#% 983833
#% 1292503
#% 1305565
#% 1451248
#! We propose a hierarchical nonparametric topic model, based on the hierarchical Dirichlet process (HDP), that accounts for dependencies among the data. The HDP mixture models are useful for discovering an unknown semantic structure (i.e., topics) from a set of unstructured data such as a corpus of documents. For simplicity, HDP makes an exchangeability assumption that any permutation of the data points would result in the same joint probability of the data being generated. This exchangeability assumption poses a problem for some domains where there are clear and strong dependencies among the data. A model that allows for non-exchangeability of data can capture these dependencies and assign higher probabilities to clusters that account for data dependencies, for example, inferring topics that reflect the temporal patterns of the data. Our model incorporates the distance dependent Chinese restaurant process (ddCRP), which clusters data with an inherent bias toward clusters of data points that are near to one another, into a hierarchical construction analogous to the HDP, and we call this new prior the distance dependent Chinese restaurant franchise (ddCRF). When tested with temporal datasets, the ddCRF mixture model shows clear improvements in data fit compared to the HDP in terms of heldout likelihood and complexity. The resulting set of topics shows the sequential emergence and disappearance patterns of topics.

#index 1642015
#* Summarizing web forum threads based on a latent topic propagation process
#@ Zhaochun Ren;Jun Ma;Shuaiqiang Wang;Yang Liu
#t 2011
#c 1
#% 296738
#% 397137
#% 641979
#% 787502
#% 875959
#% 876067
#% 881498
#% 939368
#% 1074086
#% 1211773
#% 1227593
#% 1270689
#% 1292559
#% 1305518
#% 1632478
#! With an increasingly amount of information in web forums, quick comprehension of threads in web forums has become a challenging research problem. To handle this issue, this paper investigates the task of Web Forum Thread Summarization (WFTS), aiming to give a brief statement of each thread that involving multiple dynamic topics. When applied to the task of WFTS, traditional summarization methods are cramped by topic dependencies, topic drifting and text sparseness. Consequently, we explore an unsupervised topic propagation model in this paper, the Post Propagation Model (PPM), to burst through these problems by simultaneously modeling the semantics and the reply relationship existing in each thread. Each post in PPM is considered as a mixture of topics, and a product of Dirichlet distributions in previous posts is employed to model each topic dependencies during the asynchronous discussion. Based on this model, the task of WFTS is accomplished by extracting most significant sentences in a thread. The experimental results on two different forum data sets show that WFTS based on the PPM outperforms several state-of-the-art summarization methods in terms of ROUGE metrics.

#index 1642016
#* Cloning for privacy protection in multiple independent data publications
#@ Muzammil M. Baig;Jiuyong Li;Jixue Liu;Hua Wang
#t 2011
#c 1
#% 576761
#% 801690
#% 810011
#% 824727
#% 864406
#% 874989
#% 881546
#% 883236
#% 893100
#% 937550
#% 960291
#% 993943
#% 1019452
#% 1022247
#% 1022266
#% 1044457
#% 1083631
#% 1083653
#% 1117733
#% 1206582
#% 1217156
#% 1426564
#% 1451174
#% 1451188
#% 1451189
#% 1496012
#% 1538423
#% 1670071
#% 1700134
#% 1706196
#% 1740518
#% 1960025
#! Data anonymization has become a major technique in privacy preserving data publishing. Many methods have been proposed to anonymize one dataset and a series of datasets of a data owner. However, no method has been proposed for the anonymization of data of multiple independent data publications. A data owner publishes a dataset, which contains overlapping population with other datasets published by other independent data owners. In this paper we analyze the privacy risk in the such scenario and vulnerability of partitioned based anonymization methods. We show that no partitioned based anonymization methods can protect privacy in arbitrary data distributions, and identify a case that the privacy can be protected in the scenario. We propose a new generalization principle ε-cloning to protect privacy for multiple independent data publications. We also develop an effective algorithm to achieve the ε-cloning. We experimentally show that the proposed algorithm anonymizes data to satisfy the privacy requirement and preserves good data utility.

#index 1642017
#* Privacy-aware querying over sensitive trajectory data
#@ Nikos Pelekis;Aris Gkoulalas-Divanis;Marios Vodas;Despina Kopanaki;Yannis Theodoridis
#t 2011
#c 1
#% 67453
#% 421124
#% 443463
#% 480473
#% 576761
#% 824722
#% 863155
#% 864406
#% 960283
#% 1015102
#% 1063572
#% 1080161
#% 1092320
#% 1170188
#% 1206713
#% 1592778
#! Existing approaches for privacy-aware mobility data sharing aim at publishing an anonymized version of the mobility dataset, operating under the assumption that most of the information in the original dataset can be disclosed without causing any privacy violations. In this paper, we assume that the majority of the information that exists in the mobility dataset must remain private and the data has to stay in-house to the hosting organization. To facilitate privacy-aware sharing of the mobility data we develop a trajectory query engine that allows subscribed users to gain restricted access to the database to accomplish various analysis tasks. The proposed engine (i) audits queries for trajectory data to block potential attacks to user privacy, (ii) supports range, distance, and k-nearest neighbors spatial and spatiotemporal queries, and (iii) preserves user anonymity in answers to queries by (a) augmenting the real trajectories with a set of carefully crafted, realistic fake trajectories, and (b) ensuring that no user-specific sensitive locations are reported as part of the returned trajectories.

#index 1642018
#* Privacy preserving indexing for eHealth information networks
#@ Yuzhe Tang;Ting Wang;Ling Liu;Shicong Meng;Balaji Palanisamy
#t 2011
#c 1
#% 23638
#% 31041
#% 124016
#% 233480
#% 261357
#% 319849
#% 346201
#% 401897
#% 730035
#% 812755
#% 840677
#% 864414
#% 981651
#% 999957
#% 1044459
#% 1134414
#% 1263931
#% 1426541
#% 1523848
#! The past few years have witnessed an increasing demand for the next generation health information networks (e.g., NHIN[1]), which hold the promise of supporting large-scale information sharing across a network formed by autonomous healthcare providers. One fundamental capability of such information network is to support efficient, privacy-preserving (for both users and providers) search over the distributed, access controlled healthcare documents. In this paper we focus on addressing the privacy concerns of content providers; that is, the search should not reveal the specific association between contents and providers (a.k.a. content privacy). We propose SS-PPI, a novel privacy-preserving index abstraction, which, in conjunction of distributed access control-enforced search protocols, provides theoretically guaranteed protection of content privacy. Compared with existing proposals (e.g., flipping privacy-preserving index[2]), our solution highlights with a series of distinct features: (a) it incorporates access control policies in the privacy-preserving index, which improves both search efficiency and attack resilience; (b) it employs a fast index construction protocol via a novel use of the secrete-sharing scheme in a fully distributed manner (without trusted third party), requiring only constant (typically two) round of communication; (c) it provides information-theoretic security against colluding adversaries during index construction as well as query answering. We conduct both formal analysis and experimental evaluation of SS-PPI and show that it outperforms the state-of-the-art solutions in terms of both privacy protection and execution efficiency.

#index 1642019
#* Recommendation in the end-to-end encrypted domain
#@ Jyh-Ren Shieh;Ching-Yung Lin;Ja-Ling Wu
#t 2011
#c 1
#% 319994
#% 330687
#% 344551
#% 397153
#% 557517
#% 616944
#% 767656
#% 1001277
#% 1001278
#% 1198205
#% 1669942
#% 1673616
#% 1726710
#% 1727965
#! In recommendation systems, a central host typically requires access to user profiles in order to generate useful recommendations. This access, however, undermines user privacy; the more information is revealed to the host, the more the user's privacy is compromised. In this paper, we propose a novel end-to-end encrypted recommendation mechanism which encrypts sensitive private data at the user end, without ever exposing plaintext private data to the host server. Unlike previously proposed privacy-preserving recommendation mechanisms, the data in this proposed system are lossless - a pivotal feature to many applications, e.g., in health informatics, business analytics, cyber security, etc. We achieve this goal by developing encrypted-domain polynomial ring homomorphism cryptographic algorithms to compute similarity of encrypted scores on the server, so that collaborative recommendations can be computed in the encryption domain and only an authorized person can decrypt the exact results. We also propose a novel key management system to make sure private information retrieval and recommendation computations can be executed in the encrypted domain in practice. Our experiments show that the proposed scheme offers robust security and lossless accurate recommendation, as well as high efficiency. Our preliminary results show the recommendation accuracy is 21% better than the existing statistical lossy privacy-preserving mechanisms based on random perturbation and user profile distribution. This new approach can potentially be applied to various data mining and cloud computing environments and significantly alleviates the privacy concerns of users.

#index 1642020
#* Privacy preservation by independent component analysis and variance control
#@ Chih-Ming Hsu;Ming-Syan Chen
#t 2011
#c 1
#% 1868
#% 3084
#% 67453
#% 297186
#% 300184
#% 466457
#% 466458
#% 727904
#% 810010
#% 844360
#% 959212
#! The primary objective of privacy preservation is to protect an individual's confidential information in released data sets. In recent years, several simulation-based approaches for privacy preservation have been proposed. The idea is to generate a synthetic data set with the constraint that the probability distribution is as close as possible to that of the original set. In this paper, we propose two frameworks for simulation-based privacy preservation of multivariate numerical data. The first framework, called PRIMP (PRivacy preserving by Independent coMPonents), is based on independent component analysis (ICA). It is shown empirically that PRIMP outperforms other simulation-based approaches in terms of Spearman's rank correlation and Kendall's tau correlation. The second approach proposed is a hybrid method that combines PRIMP and Cholesky's decomposition technique. It is shown empirically that the hybrid method preserves the covariance matrix of the original data exactly. The method also resolves the problem of generating good seeds for the Cholesky-based approach. Although the empirical results show that the hybrid approach is not always better than the PRIMP in terms of Spearman's rank correlation and Kendall's tau correlation, in theory, the risk of information leakage under the hybrid approach is much less than that under PRIMP.

#index 1642021
#* Can irrelevant data help semi-supervised learning, why and how?
#@ Haiqin Yang;Shenghuo Zhu;Irwin King;Michael R. Lyu
#t 2011
#c 1
#% 100384
#% 190581
#% 304876
#% 311027
#% 466263
#% 576520
#% 875970
#% 876071
#% 903334
#% 916788
#% 961195
#% 961218
#% 1074167
#% 1176910
#% 1232031
#% 1269777
#% 1354495
#% 1464068
#% 1473497
#! Previous semi-supervised learning (SSL) techniques usually assume unlabeled data are relevant to the target task. That is, they follow the same distribution as the targeted labeled data. In this paper, we address a different and very difficult scenario in SSL, where the unlabeled data may be a mixture of data relevant or irrelevant to the target binary classification task. In our framework, we do not require explicitly prior knowledge on the relatedness of the unlabeled data to the target data. In order to alleviate the effect of the irrelevant unlabeled data and utilize the implicit knowledge among all available data, we develop a novel maximum margin classifier, named the tri-class support vector machine (3C-SVM), to seek an inductive rule to separate the target binary classification task well while finding out the irrelevant data by-product. To attain this goal, we introduce a new min loss function, which can relieve the impact of the irrelevant data while relying more on the labeled data and the relevant unlabeled data. This loss function can therefore achieve the maximum entropy principle. The 3C-SVM can then generalize standard SVMs, Semi-supervised SVMs, and SVMs learned from the universum as its special cases. We further analyze the property of 3C-SVM on why the irrelevant data can help to improve the model performance. For implementation, we make relaxation and approximate the objective by the convex-concave procedure, which turns the original optimization from integral programming problem to a problem by just solving a finite number of quadratic programming problems. Empirical results are reported to demonstrate the advantages of our 3C-SVM model.

#index 1642022
#* Toward interactive training and evaluation
#@ Gregory Druck;Andrew McCallum
#t 2011
#c 1
#% 464434
#% 879616
#% 1074125
#% 1074126
#% 1211770
#% 1249532
#% 1291461
#% 1338536
#% 1472297
#% 1482383
#% 1711859
#% 1917474
#! Machine learning often relies on costly labeled data, and this impedes its application to new classification and information extraction problems. This has motivated the development of methods for leveraging abundant prior knowledge about these problems, including methods for lightly supervised learning using model expectation constraints. Building on this work, we envision an interactive training paradigm in which practitioners perform evaluation, analyze errors, and provide and refine expectation constraints in a closed loop. In this paper, we focus on several key subproblems in this paradigm that can be cast as selecting a representative sample of the unlabeled data for the practitioner to inspect. To address these problems, we propose stratified sampling methods that use model expectations as a proxy for latent output variables. In classification and sequence labeling experiments, these sampling strategies reduce accuracy evaluation effort by as much as 53%, provide more reliable estimates of $F_1$ for rare labels, and aid in the specification and refinement of constraints.

#index 1642023
#* Semi-supervised multi-task learning of structured prediction models for web information extraction
#@ Paramveer S. Dhillon;Sundararajan Sellamanickam;Sathiya Keerthi Selvaraj
#t 2011
#c 1
#% 236497
#% 333943
#% 431536
#% 464434
#% 480824
#% 769877
#% 769886
#% 769942
#% 770759
#% 805846
#% 829014
#% 864416
#% 881505
#% 889107
#% 916788
#% 939527
#% 1022260
#% 1022353
#% 1100145
#% 1131145
#% 1166537
#% 1190153
#% 1211726
#% 1214639
#% 1269815
#% 1328133
#% 1400030
#% 1406981
#% 1426569
#% 1481552
#! Extracting information from web pages is an important problem; it has several applications such as providing improved search results and construction of databases to serve user queries. In this paper we propose a novel structured prediction method to address two important aspects of the extraction problem: (1) labeled data is available only for a small number of sites and (2) a machine learned global model does not generalize adequately well across many websites. For this purpose, we propose a weight space based graph regularization method. This method has several advantages. First, it can use unlabeled data to address the limited labeled data problem and falls in the class of graph regularization based semi-supervised learning approaches. Second, to address the generalization inadequacy of a global model, this method builds a local model for each website. Viewing the problem of building a local model for each website as a task, we learn the models for a collection of sites jointly; thus our method can also be seen as a graph regularization based multi-task learning approach. Learning the models jointly with the proposed method is very useful in two ways: (1) learning a local model for a website can be effectively influenced by labeled and unlabeled data from other websites; and (2) even for a website with only unlabeled examples it is possible to learn a decent local model. We demonstrate the efficacy of our method on several real-life data; experimental results show that significant performance improvement can be obtained by combining semi-supervised and multi-task learning in a single framework.

#index 1642024
#* Memory-less unsupervised clustering for data streaming by versatile ellipsoidal function
#@ Niwan Wattanakitrungroj;Chidchanok Lursinsap
#t 2011
#c 1
#% 835018
#% 1015261
#% 1016200
#% 1114033
#% 1136344
#% 1321538
#% 1332141
#% 1702194
#% 1707828
#! The challenge of clustering on data stream is the ability to deal with the continuous incoming data which are unlimited and unable to store all of them. To manage the storage crisis, the data must be processed in a single pass or only once after the arrival and are thrown away outer. All previously clustered data must be mathematically captured in terms of group features since those data are already non-existent. The proposed data stream clustering algorithm is divided into two main phases, namely on-line and off-line. In the on-line phase, new micro-cluster features are proposed. Our micro-cluster features better represent the arriving data than the traditional micro-cluster features. In the off-line phase, the prepared micro-clusters are categorized by their densities. The proposed method can generate the final clusters with different shapes and densities. Based on entropy, purity, Jaccard coefficient, and Rand statistic measures, our algorithm being applied on synthetic and real data outperforms the other previous data stream clustering algorithms.

#index 1642025
#* Coupled nominal similarity in unsupervised learning
#@ Can Wang;Longbing Cao;Mingchun Wang;Jinjiu Li;Wei Wei;Yuming Ou
#t 2011
#c 1
#% 140588
#% 478120
#% 837604
#% 986227
#% 995140
#% 1000422
#% 1272304
#% 1482245
#% 1848056
#! The similarity between nominal objects is not straightforward, especially in unsupervised learning. This paper proposes coupled similarity metrics for nominal objects, which consider not only intra-coupled similarity within an attribute (i.e., value frequency distribution) but also inter-coupled similarity between attributes (i.e. feature dependency aggregation). Four metrics are designed to calculate the inter-coupled similarity between two categorical values by considering their relationships with other attributes. The theoretical analysis reveals their equivalent accuracy and superior efficiency based on intersection against others, in particular for large-scale data. Substantial experiments on extensive UCI data sets verify the theoretical conclusions. In addition, experiments of clustering based on the derived dissimilarity metrics show a significant performance improvement.

#index 1642026
#* Feature selection using hierarchical feature clustering
#@ Huawen Liu;Xindong Wu;Shichao Zhang
#t 2011
#c 1
#% 115608
#% 126894
#% 296375
#% 425002
#% 444044
#% 722929
#% 729437
#% 793239
#% 793250
#% 796212
#% 814023
#% 889150
#% 1041316
#% 1058802
#% 1085531
#% 1108900
#% 1165665
#% 1182803
#% 1302079
#% 1352989
#% 1378511
#% 1408092
#% 1411478
#% 1532427
#! One of the challenges in data mining is the dimensionality of data, which is often very high and prevalent in many domains, such as text categorization and bio-informatics. The high-dimensionality of data may bring many adverse situations to traditional learning algorithms. To cope with this issue, feature selection has been put forward. Currently, many efforts have been attempted in this field and lots of feature selection algorithms have been developed. In this paper we propose a new selection method to pick discriminative features by using information measurement. The main characteristic of our selection method is that the selection procedure works like feature clustering in a hierarchically agglomerative way, where each feature is considered as a cluster and the between-cluster and within-cluster distances are measured by mutual information and the coefficient of relevancy respectively. Consequently, the final aggregated cluster is the selection result, which has the minimal redundancy among its members and the maximal relevancy with the class labels. The simulation experiments on seven datasets show that the proposed method outperforms other popular feature selection algorithms in classification performance.

#index 1642027
#* Discovering top-k teams of experts with/without a leader in social networks
#@ Mehdi Kargar;Aijun An
#t 2011
#c 1
#% 479803
#% 881460
#% 1063539
#% 1075875
#% 1178476
#% 1207007
#% 1214668
#% 1297295
#% 1482238
#% 1512399
#% 1606349
#! We study the problem of discovering a team of experts from a social network. Given a project whose completion requires a set of skills, our goal is to find a set of experts that together have all of the required skills and also have the minimal communication cost among them. We propose two communication cost functions designed for two types of communication structures. We show that the problem of finding the team of experts that minimizes one of the proposed cost functions is NP-hard. Thus, an approximation algorithm with an approximation ratio of two is designed. We introduce the problem of finding a team of experts with a leader. The leader is responsible for monitoring and coordinating the project, and thus a different communication cost function is used in this problem. To solve this problem, an exact polynomial algorithm is proposed. We show that the total number of teams may be exponential with respect to the number of required skills. Thus, two procedures that produce top-k teams of experts with or without a leader in polynomial delay are proposed. Extensive experiments on real datasets demonstrate the effectiveness and scalability of the proposed methods.

#index 1642028
#* Content based social behavior prediction: a multi-task learning approach
#@ Hongliang Fei;Ruoyi Jiang;Yuhao Yang;Bo Luo;Jun Huan
#t 2011
#c 1
#% 190581
#% 425048
#% 723239
#% 729923
#% 757953
#% 829014
#% 956512
#% 1211727
#% 1211771
#% 1318585
#% 1417091
#% 1451258
#% 1535333
#% 1560424
#! Information Flow Studies analyze the principles and mechanisms of social information distribution and is an essential research topic in social networks. Traditional approaches are primarily based on the social network graph topology. However, topology itself can not accurately reflect the user interests or activities. In this paper, we adopt a "microeconomics" approach to study social information diffusion and aim to answer the question that how social information flow and socialization behaviors are related to content similarity and user interests. In particular, we study content-based social activity prediction, i.e., to predict a user's response (e.g. comment or like) to their friends' postings (e.g. blogs) w.r.t. message content. In our solution, we cast the social behavior prediction problem as a multi-task learning problem, in which each task corresponds to a user. We have designed a novel multi-task learning algorithm that is specifically designed for learning information flow in social networks. In our model, we apply l1 and Tikhonov regularization to obtain a sparse and smooth model in a linear multi-task learning framework. Using comprehensive experimental study, we have demonstrated the effectiveness of the proposed learning method.

#index 1642029
#* Improving user interest inference from social neighbors
#@ Zhen Wen;Ching-Yung Lin
#t 2011
#c 1
#% 152934
#% 769942
#% 847102
#% 853532
#% 1081487
#% 1083624
#% 1083641
#% 1117695
#% 1190207
#% 1214638
#% 1355041
#% 1451176
#! Prior research has provided some evidence of social correlation (i.e., "you are who you know"), which makes it possible to infer one's interests from his or her social neighbors. However, it is also shown to be challenging to consistently obtain high quality inference. This challenge can be partially attributed to the fact that people usually maintain diverse social relationships, in order to tap into diverse information and knowledge. It is unlikely that a person would possess all interests of his/her social neighbors. Instead, s/he may selectively acquire just a subset of them. This paper intends to improve inferring interests from neighbors given this observation. We conduct this study by implementing a privacy-preserving large distributed social sensor system in a large global IT company to capture the multifaceted activities (e.g., emails, instant messaging, social bookmarking, etc.) of 25K+ people. These activities occupy the majority of employees' time, and thus, provide a higher quality view of the diverse aspects of their professional interests compared to the friending activity on online social networking sites. In this paper, we propose a technique that exploits the correlation among the attributes that a person possesses to improve social-correlation-based inference quality. Our technique offers two unique contributions. First, we demonstrate that the proposed technique can significantly improve inference quality by as much as 76.1%. Second, we study the interaction between the two factors: social correlation and attribute correlation under different situations. The results can inform practical applications how the inference quality would change in various scenarios.

#index 1642030
#* CASINO: towards conformity-aware social influence analysis in online social networks
#@ Hui Li;Sourav S. Bhowmick;Aixin Sun
#t 2011
#c 1
#% 269217
#% 729923
#% 1399997
#% 1536569
#% 1536586
#! Social influence analysis in online social networks is the study of people's influence by analyzing the social interactions between individuals. There have been increasing research efforts to understand the influence propagation phenomenon due to its importance to information dissemination among others. Despite the progress achieved by state-of-the-art social influence analysis techniques, a key limitation of these techniques is that they only utilize positive interactions (e.g., agreement, trust) between individuals, ignoring two equally important factors, namely, negative relationships (e.g., distrust, disagreement) between individuals and conformity of people, which refers to a person's inclination to be influenced. In this paper, we propose a novel algorithm CASINO (Conformity-Aware Social INfluence cOmputation) to study the interplay between influence and conformity of each individual. Given a social network, CASINO first extracts a set of topic-based subgraphs where each subgraph depicts the social interactions associated with a specific topic. Then it optionally labels the edges (relationships) between individuals with positive or negative signs. Finally, it computes the influence and conformity indices of each individual in each signed topic-based subgraph. Our empirical study with several real-world social networks demonstrates superior effectiveness and accuracy of CASINO compared to state-of-the-art methods. Furthermore, we revealed several interesting characteristics of "influentials" and "conformers" in these networks.

#index 1642031
#* Mining direct antagonistic communities in explicit trust networks
#@ David Lo;Didi Surian;Kuan Zhang;Ee-Peng Lim
#t 2011
#c 1
#% 443350
#% 481290
#% 629708
#% 729933
#% 745515
#% 989617
#% 1013604
#% 1015228
#% 1035586
#% 1206864
#% 1737765
#! There has been a recent increase of interest in analyzing trust and friendship networks to gain insights about relationship dynamics among users. Many sites such as Epinions, Facebook, and other social networking sites allow users to declare trusts or friendships between different members of the community. In this work, we are interested in extracting direct antagonistic communities (DACs) within a rich trust network involving trusts and distrusts. Each DAC is formed by two subcommunities with trust relationships among members of each sub-community but distrust relationships across the sub-communities. We develop an efficient algorithm that could analyze large trust networks leveraging the unique property of direct antagonistic community. We have experimented with synthetic and real data-sets (myGamma and Epinions) to demonstrate the scalability of our proposed solution.

#index 1642032
#* Connecting users with similar interests via tag network inference
#@ Xufei Wang;Huan Liu;Wei Fan
#t 2011
#c 1
#% 464615
#% 730089
#% 1183090
#% 1355021
#% 1399976
#% 1425621
#! The popularity of social networking greatly increases interaction among people. However, one major challenge remains --- how to connect people who share similar interests. In a social network, the majority of people who share similar interests with given a user are in the long tail that accounts for 80% of total population. Searching for similar users by following links in social network has two limitations: it is inefficient and incomplete. Thus, it is desirable to design new methods to find like-minded people. In this paper, we propose to use collective wisdom from the crowd or tag networks to solve the problem. In a tag network, each node represents a tag as described by some words, and the weight of an undirected edge represents the co-occurrence of two tags. As such, the tag network describes the semantic relationships among tags. In order to connect to other users of similar interests via a tag network, we use diffusion kernels on the tag network to measure the similarity between pairs of tags. The similarity of people's interests are measured on the basis of similar tags they share. To recommend people who are alike, we retrieve top k people sharing the most similar tags. Compared to two baseline methods triadic closure and LSI, the proposed tag network approach achieves 108% and 27% relative improvements on the BlogCatalog dataset, respectively.

#index 1642033
#* Do all birds tweet the same?: characterizing twitter around the world
#@ Barbara Poblete;Ruth Garcia;Marcelo Mendoza;Alejandro Jaimes
#t 2011
#c 1
#% 1002007
#% 1040837
#% 1399992
#% 1560422
#! Social media services have spread throughout the world in just a few years. They have become not only a new source of information, but also new mechanisms for societies world-wide to organize themselves and communicate. Therefore, social media has a very strong impact in many aspects -- at personal level, in business, and in politics, among many others. In spite of its fast adoption, little is known about social media usage in different countries, and whether patterns of behavior remain the same or not. To provide deep understanding of differences between countries can be useful in many ways, e.g.: to improve the design of social media systems (which features work best for which country?), and influence marketing and political campaigns. Moreover, this type of analysis can provide relevant insight into how societies might differ. In this paper we present a summary of a large-scale analysis of Twitter for an extended period of time. We analyze in detail various aspects of social media for the ten countries we identified as most active. We collected one year's worth of data and report differences and similarities in terms of activity, sentiment, use of languages, and network structure. To the best of our knowledge, this is the first on-line social network study of such characteristics.

#index 1642034
#* Topic sentiment analysis in twitter: a graph-based hashtag sentiment classification approach
#@ Xiaolong Wang;Furu Wei;Xiaohua Liu;Ming Zhou;Ming Zhang
#t 2011
#c 1
#% 197394
#% 430761
#% 723399
#% 769892
#% 854646
#% 854650
#% 855282
#% 879625
#% 889849
#% 907489
#% 938687
#% 939897
#% 956510
#% 987340
#% 1019145
#% 1268503
#% 1292503
#% 1481479
#% 1544009
#% 1544032
#% 1591944
#% 1650403
#% 1815596
#! Twitter is one of the biggest platforms where massive instant messages (i.e. tweets) are published every day. Users tend to express their real feelings freely in Twitter, which makes it an ideal source for capturing the opinions towards various interesting topics, such as brands, products or celebrities, etc. Naturally, people may anticipate an approach to receiving the common sentiment tendency towards these topics directly rather than through reading the huge amount of tweets about them. On the other side, Hashtags, starting with a symbol "#" ahead of keywords or phrases, are widely used in tweets as coarse-grained topics. In this paper, instead of presenting the sentiment polarity of each tweet relevant to the topic, we focus our study on hashtag-level sentiment classification. This task aims to automatically generate the overall sentiment polarity for a given hashtag in a certain time period, which markedly differs from the conventional sentence-level and document-level sentiment analysis. Our investigation illustrates that three types of information is useful to address the task, including (1) sentiment polarity of tweets containing the hashtag; (2) hashtags co-occurrence relationship and (3) the literal meaning of hashtags. Consequently, in order to incorporate the first two types of information into a classification framework where hashtags can be classified collectively, we propose a novel graph model and investigate three approximate collective classification algorithms for inference. Going one step further, we show that the performance can be remarkably improved using an enhanced boosting classification setting in which we employ the literal meaning of hashtags as semi-supervised information. Experimental results on a real-life data set consisting of 29,195 tweets and 2,181 hashtags show the effectiveness of the proposed model and algorithms.

#index 1642035
#* Language-independent sentiment classification using three common words
#@ Zheng Lin;Songbo Tan;Xueqi Cheng
#t 2011
#c 1
#% 815915
#% 854646
#% 1019180
#% 1195857
#% 1200477
#% 1261539
#% 1328329
#% 1471292
#% 1481638
#% 1650665
#! Many methods for cross-lingual processing tasks are resource-dependent, which will not work without machine translation system or bilingual lexicon. In this paper, we propose a novel approach for multilingual sentiment classification just by few seed words. For a given language, the proposed approach learns a sentiment classifier from the initial seed words instead of any labeled data. We employ our method both in supervised learning and unsupervised learning. Experimental results demonstrate that our method relies less on external resource but performs as well as or better than the baseline.

#index 1642036
#* A cross-domain adaptation method for sentiment classification using probabilistic latent analysis
#@ Sheng Gao;Haizhou Li
#t 2011
#c 1
#% 280819
#% 722904
#% 769892
#% 854646
#% 916788
#% 989592
#% 1127964
#% 1190064
#% 1211714
#% 1214639
#% 1227700
#% 1261539
#% 1400008
#% 1464068
#% 1481560
#! Sentiment classification is becoming attractive in recent years because of its potential commercial applications. It exploits supervised learning methods to learn the classifiers from the annotated training documents. The challenge in sentiment classification lies in that the sentiment domains are diverse, heterogeneous and fast-growing. The classifiers trained on one domain (source domain) could not classify a document from another domain (target domain). The domain adaptation technique is to address the problem by making use of labeled samples in the source domain, and unlabeled samples in the target domain. This paper presents a new solution, a cross-domain topic indexing (CDTI) method, with which a common semantic space is found from the prior between-domain term correspondences and the term co-occurrences in the cross-domain documents. These observations are characterized with the mixture model in CDTI, with each component being a possible topic shared by the source and target domains. Such common topics are found to index the cross-domain content. We evaluate the algorithms on a multi-domain sentiment classification task, which shows that CDTI outperforms the state-of-the-art domain adaptation method, i.e. spectral feature alignment (SFA), and the traditional latent semantic indexing method.

#index 1642037
#* Using games with a purpose and bootstrapping to create domain-specific sentiment lexicons
#@ Albert Weichselbraun;Stefan Gindl;Arno Scharl
#t 2011
#c 1
#% 311034
#% 722308
#% 746885
#% 815915
#% 835045
#% 885438
#% 939634
#% 939848
#% 939897
#% 1060610
#% 1087227
#% 1200477
#% 1215457
#% 1260667
#% 1268503
#% 1270650
#% 1473349
#! Sentiment detection analyzes the positive or negative polarity of text. The field has received considerable attention in recent years, since it plays an important role in providing means to assess user opinions regarding an organization's products, services, or actions. Approaches towards sentiment detection include machine learning techniques as well as computationally less expensive methods. Both approaches rely on the use of language-specific sentiment lexicons, which are lists of sentiment terms with their corresponding sentiment value. The effort involved in creating, customizing, and extending sentiment lexicons is considerable, particularly if less common languages and domains are targeted without access to appropriate language resources. This paper proposes a semi-automatic approach for the creation of sentiment lexicons which assigns sentiment values to sentiment terms via crowd-sourcing. Furthermore, it introduces a bootstrapping process operating on unlabeled domain documents to extend the created lexicons, and to customize them according to the particular use case. This process considers sentiment terms as well as sentiment indicators occurring in the discourse surrounding a articular topic. Such indicators are associated with a positive or negative context in a particular domain, but might have a neutral connotation in other domains. A formal evaluation shows that bootstrapping considerably improves the method's recall. Automatically created lexicons yield a performance comparable to professionally created language resources such as the General Inquirer.

#index 1642038
#* Polarity analysis of texts using discourse structure
#@ Bas Heerschop;Frank Goossen;Alexander Hogenboom;Flavius Frasincar;Uzay Kaymak;Franciska de Jong
#t 2011
#c 1
#% 286069
#% 449588
#% 814007
#% 815915
#% 816183
#% 838522
#% 854646
#% 938687
#% 1127964
#% 1179734
#% 1301020
#% 1330512
#% 1471319
#% 1529652
#% 1590302
#% 1599374
#! Sentiment analysis has applications in many areas and the exploration of its potential has only just begun. We propose Pathos, a framework which performs document sentiment analysis (partly) based on a document's discourse structure. We hypothesize that by splitting a text into important and less important text spans, and by subsequently making use of this information by weighting the sentiment conveyed by distinct text spans in accordance with their importance, we can improve the performance of a sentiment classifier. A document's discourse structure is obtained by applying Rhetorical Structure Theory on sentence level. When controlling for each considered method's structural bias towards positive classifications, weights optimized by a genetic algorithm yield an improvement in sentiment classification accuracy and macro-level F1 score on documents of 4.5% and 4.7%, respectively, in comparison to a baseline not taking into account discourse structure.

#index 1642039
#* A query-based multi-document sentiment summarizer
#@ Maria Soledad Pera;Rani Qumsiyeh;Yiu-Kai Ng
#t 2011
#c 1
#% 766442
#% 940039
#% 992311
#% 995507
#% 1693254
#! Review websites, such as Epinions.com, which offer users a platform to share their opinions on diverse products and services, provide a valuable source of opinion-rich information. Browsing through archived reviews to locate different opinions on a product or service, however, is a time-consuming and tedious task, and in most cases, the large amount of available information is difficult for users to absorb. To facilitate the process of synthesizing opinions expressed in reviews on a product or service P specified in a user query/question Q, we introduce QMSS, a query-based multi-document sentiment summarizer. QMSS creates a summary for Q, which either reflects the general opinions on P or is tailored to specific facets (i.e., features) and/or sentiment of P as specified in Q. QMSS (i) identifies the facets addressed in reviews retrieved for Q, (ii) employs a sentence-based, sentiment classifier to determine the polarity of each sentence in each review, and (iii) clusters sentences in reviews according to the facets captured in the sentences, which are identified using a keyword-label extraction algorithm. This process dictates which sentences in the reviews should be included in the summary for Q. Empirical studies have verified that QMSS is highly effective in generating summaries that satisfy users' information needs and ranks on top among the state-of-the-art query-based multi-document sentiment summarizers

#index 1642040
#* Scalable density-based subspace clustering
#@ Emmanuel Müller;Ira Assent;Stephan Günnemann;Thomas Seidl
#t 2011
#c 1
#% 248792
#% 273891
#% 280417
#% 300120
#% 300131
#% 316709
#% 397384
#% 464888
#% 481290
#% 727868
#% 765518
#% 785355
#% 844313
#% 885366
#% 915305
#% 1083683
#% 1117035
#% 1130918
#% 1165480
#% 1176982
#% 1206694
#% 1318668
#% 1328215
#! For knowledge discovery in high dimensional databases, subspace clustering detects clusters in arbitrary subspace projections. Scalability is a crucial issue, as the number of possible projections is exponential in the number of dimensions. We propose a scalable density-based subspace clustering method that steers mining to few selected subspace clusters. Our novel steering technique reduces subspace processing by identifying and clustering promising subspaces and their combinations directly. Thereby, it narrows down the search space while maintaining accuracy. Thorough experiments on real and synthetic databases show that steering is efficient and scalable, with high quality results. For future work, our steering paradigm for density-based subspace clustering opens research potential for speeding up other subspace clustering approaches as well.

#index 1642041
#* Correlated multi-label feature selection
#@ Quanquan Gu;Zhenhui Li;Jiawei Han
#t 2011
#c 1
#% 318412
#% 413637
#% 465754
#% 577224
#% 729437
#% 757953
#% 763699
#% 763708
#% 818234
#% 881477
#% 961190
#% 983901
#% 983905
#% 1073889
#% 1073923
#% 1083666
#% 1083698
#% 1264133
#% 1270338
#% 1292601
#% 1298987
#% 1305490
#% 1451240
#% 1451260
#% 1495503
#! Multi-label learning studies the problem where each instance is associated with a set of labels. There are two challenges in multi-label learning: (1) the labels are interdependent and correlated, and (2) the data are of high dimensionality. In this paper, we aim to tackle these challenges in one shot. In particular, we propose to learn the label correlation and do feature selection simultaneously. We introduce a matrix-variate Normal prior distribution on the weight vectors of the classifier to model the label correlation. Our goal is to find a subset of features, based on which the label correlation regularized loss of label ranking is minimized. The resulting multi-label feature selection problem is a mixed integer programming, which is reformulated as quadratically constrained linear programming (QCLP). It can be solved by cutting plane algorithm, in each iteration of which a minimax optimization problem is solved by dual coordinate descent and projected sub-gradient descent alternatively. Experiments on benchmark data sets illustrate that the proposed methods outperform single-label feature selection method and many other state-of-the-art multi-label learning methods.

#index 1642042
#* Pattern change discovery between high dimensional data sets
#@ Yi Xu;Zhongfei Zhang;Philips Yu;Bo Long
#t 2011
#c 1
#% 342600
#% 729932
#% 823396
#% 881487
#% 907492
#% 916785
#% 989638
#% 989648
#% 998561
#% 1016144
#% 1063629
#% 1083714
#% 1108878
#% 1108903
#% 1176889
#% 1214635
#% 1292518
#% 1314738
#% 1405144
#% 1411043
#% 1451184
#! This paper investigates the general problem of pattern change discovery between high-dimensional data sets. Current methods either mainly focus on magnitude change detection of low-dimensional data sets or are under supervised frameworks. In this paper, the notion of the principal angles between the subspaces is introduced to measure the subspace difference between two high-dimensional data sets. Principal angles bear a property to isolate subspace change from the magnitude change. To address the challenge of directly computing the principal angles, we elect to use matrix factorization to serve as a statistical framework and develop the principle of the dominant subspace mapping to transfer the principal angle based detection to a matrix factorization problem. We show how matrix factorization can be naturally embedded into the likelihood ratio test based on the linear models. The proposed method is of an unsupervised nature and addresses the statistical significance of the pattern changes between high-dimensional data sets. We have showcased the different applications of this solution in several specific real-world applications to demonstrate the power and effectiveness of this method.

#index 1642043
#* MTopS: scalable processing of continuous top-k multi-query workloads
#@ Avani Shastri;Yang Di;Elke A. Rundensteiner;Matthew O. Ward
#t 2011
#c 1
#% 300180
#% 333854
#% 397378
#% 399762
#% 631988
#% 654443
#% 659921
#% 733373
#% 766671
#% 875023
#% 878299
#% 1022217
#% 1127375
#% 1292554
#% 1549841
#! A continuous top-k query retrieves the k most preferred objects in a data stream according to a given preference function. These queries are important for a broad spectrum of applications ranging from web-based advertising to financial analysis. In various streaming applications, a large number of such continuous top-k queries need to be executed simultaneously against a common popular input stream. To efficiently handle such top-k query workload, we present a comprehensive framework, called MTopS.Within this MTopS framework, several computational components work collaboratively to first analyze the commonalities across the workload; organize the workload for maximized sharing opportunities; execute the workload queries simultaneously in a shared manner; and output query results whenever any input query requires. In particular, MTopS supports two proposed algorithms, MTopBand and MTopList, which both incrementally maintain the top-k objects over time for multiple queries. As the foundation, we first identify the minimal object set from the data stream that is both necessary and sufficient for accurately answering all top-k queries in the workload. Then, the MTopBand algorithm is presented to incrementally maintain such minimum object set and eliminate the need for any recomputation from scratch. To further optimize MTop-Band, we design the second algorithm, MTopList which organizes the progressive top-k results of workload queries in a compact structure. MTopList is shown to be memory optimal and also more efficient in terms of CPU time usage than MTopBand. Our experimental study, using real data streams from domains of stock trades and moving object monitoring, demonstrates that both the efficiency and scalability of our proposed techniques are clearly superior to the state-of-the-art solutions.

#index 1642044
#* Probabilistic near-duplicate detection using simhash
#@ Sadhan Sood;Dmitri Loguinov
#t 2011
#c 1
#% 86786
#% 249238
#% 249321
#% 255137
#% 326812
#% 345087
#% 347225
#% 427199
#% 479658
#% 544011
#% 632029
#% 805905
#% 879600
#% 956507
#% 987347
#% 1055714
#% 1215859
#% 1275181
#! This paper offers a novel look at using a dimensionality-reduction technique called simhash to detect similar document pairs in large-scale collections. We show that this algorithm produces interesting intermediate data, which is normally discarded, that can be used to predict which of the bits in the final hash are more susceptible to being flipped in similar documents. This paves the way for a probabilistic search technique in the Hamming space of simhashes that can be significantly faster and more space-efficient than the existing simhash approaches. We show that with 95% recall compared to deterministic search of prior work, our method exhibits 4-14 times faster lookup and requires 2-10 times less RAM on our collection of 70M web pages.

#index 1642045
#* Collective prediction with latent graphs
#@ Xiaoxiao Shi;Yao Li;Philip Yu
#t 2011
#c 1
#% 416810
#% 420495
#% 464434
#% 769942
#% 840947
#% 949164
#% 1328169
#% 1650403
#% 1815596
#! Collective classification in relational data has become an important and active research topic in the last decade. It exploits the dependencies of instances in a network to improve predictions. Related applications include hyperlinked document classification, social network analysis and collaboration network analysis. Most of the traditional collective classification models mainly study the scenario that there exists a large amount of labeled examples (labeled nodes). However, in many real-world applications, labeled data are extremely difficult to obtain. For example, in network intrusion detection, there may be only a limited number of identified intrusions whereas there are a huge set of unlabeled nodes. In this situation, most of the data have no connection to labeled nodes; hence, no supervision knowledge can be obtained from the local connections. In this paper, we propose to explore various latent linkages among the nodes and judiciously integrate the linkages to generate a latent graph. This is achieved by finding a graph that maximizes the linkages among the training data with the same label, and maximizes the separation among the data with different labels. The objective is further cast into an optimization problem and is solved with quadratic programming. Finally, we apply label propagation on the latent graph to make prediction. Experiments show that the proposed model LNP (Latent Network Propagation) can improve the learning accuracy significantly. For instance, when there are only 10% of labeled examples, the accuracies of all the comparison models are less than 63%, while that of the proposed model is 74%.

#index 1642046
#* Who will follow you back?: reciprocal relationship prediction
#@ John Hopcroft;Tiancheng Lou;Jie Tang
#t 2011
#c 1
#% 464434
#% 955712
#% 1035581
#% 1117026
#% 1192930
#% 1214702
#% 1355042
#% 1399992
#% 1399997
#% 1400018
#% 1425621
#% 1426611
#% 1451159
#% 1451163
#% 1451245
#% 1482397
#% 1536568
#% 1560425
#% 1607319
#% 1650318
#% 1673048
#! We study the extent to which the formation of a two-way relationship can be predicted in a dynamic social network. A two-way (called reciprocal) relationship, usually developed from a one-way (parasocial) relationship, represents a more trustful relationship between people. Understanding the formation of two-way relationships can provide us insights into the micro-level dynamics of the social network, such as what is the underlying community structure and how users influence each other. Employing Twitter as a source for our experimental data, we propose a learning framework to formulate the problem of reciprocal relationship prediction into a graphical model. The framework incorporates social theories into a machine learning model. We demonstrate that it is possible to accurately infer 90% of reciprocal relationships in a dynamic network. Our study provides strong evidence of the existence of the structural balance among reciprocal relationships. In addition, we have some interesting findings, e.g., the likelihood of two "elite" users creating a reciprocal relationships is nearly 8 times higher than the likelihood of two ordinary users. More importantly, our findings have potential implications such as how social structures can be inferred from individuals' behaviors.

#index 1642047
#* Link prediction: the power of maximal entropy random walk
#@ Rong-Hua Li;Jeffrey Xu Yu;Jianquan Liu
#t 2011
#c 1
#% 464615
#% 577273
#% 577329
#% 823388
#% 878207
#% 878224
#% 955712
#% 975021
#% 1083711
#% 1178359
#% 1211760
#% 1214743
#% 1428692
#% 1451162
#% 1451163
#% 1482252
#% 1536568
#! Link prediction is a fundamental problem in social network analysis. The key technique in unsupervised link prediction is to find an appropriate similarity measure between nodes of a network. A class of wildly used similarity measures are based on random walk on graph. The traditional random walk (TRW) considers the link structures by treating all nodes in a network equivalently, and ignores the centrality of nodes of a network. However, in many real networks, nodes of a network not only prefer to link to the similar node, but also prefer to link to the central nodes of the network. To address this issue, we use maximal entropy random walk (MERW) for link prediction, which incorporates the centrality of nodes of the network. First, we study certain important properties of MERW on graph $G$ by constructing an eigen-weighted graph G. We show that the transition matrix and stationary distribution of MERW on G are identical to the ones of TRW on G. Based on G, we further give the maximal entropy graph Laplacians, and show how to fast compute the hitting time and commute time of MERW. Second, we propose four new graph kernels and two similarity measures based on MERW for link prediction. Finally, to exhibit the power of MERW in link prediction, we compare 27 various link prediction methods over 3 synthetic and 8 real networks. The results show that our newly proposed MERW based methods outperform the state-of-the-art method on most datasets.

#index 1642048
#* Exploiting longer cycles for link prediction in signed networks
#@ Kai-Yang Chiang;Nagarajan Natarajan;Ambuj Tewari;Inderjit S. Dhillon
#t 2011
#c 1
#% 754098
#% 955712
#% 1384246
#% 1399997
#% 1535418
#! We consider the problem of link prediction in signed networks. Such networks arise on the web in a variety of ways when users can implicitly or explicitly tag their relationship with other users as positive or negative. The signed links thus created reflect social attitudes of the users towards each other in terms of friendship or trust. Our first contribution is to show how any quantitative measure of social imbalance in a network can be used to derive a link prediction algorithm. Our framework allows us to reinterpret some existing algorithms as well as derive new ones. Second, we extend the approach of Leskovec et al. (2010) by presenting a supervised machine learning based link prediction method that uses features derived from longer cycles in the network. The supervised method outperforms all previous approaches on 3 networks drawn from sources such as Epinions, Slashdot and Wikipedia. The supervised approach easily scales to these networks, the largest of which has 132k nodes and 841k edges. Most real-world networks have an overwhelmingly large proportion of positive edges and it is therefore easy to get a high overall accuracy at the cost of a high false positive rate. We see that our supervised method not only achieves good accuracy for sign prediction but is also especially effective in lowering the false positive rate.

#index 1642049
#* Structural link analysis and prediction in microblogs
#@ Dawei Yin;Liangjie Hong;Brian D. Davison
#t 2011
#c 1
#% 577273
#% 730089
#% 915292
#% 955712
#% 1026857
#% 1083671
#% 1214666
#% 1260273
#% 1305450
#% 1355042
#% 1366213
#% 1399992
#% 1400119
#% 1451163
#% 1506202
#% 1512414
#% 1536568
#% 1598518
#! With hundreds of millions of participants, social media services have become commonplace. Unlike a traditional social network service, a microblogging network like Twitter is a hybrid network, combining aspects of both social networks and information networks. Understanding the structure of such hybrid networks and predicting new links are important for many tasks such as friend recommendation, community detection, and modeling network growth. We note that the link prediction problem in a hybrid network is different from previously studied networks. Unlike the information networks and traditional online social networks, the structures in a hybrid network are more complicated and informative. We compare most popular and recent methods and principles for link prediction and recommendation. Finally we propose a novel structure-based personalized link prediction model and compare its predictive performance against many fundamental and popular link prediction methods on real-world data from the Twitter microblogging network. Our experiments on both static and dynamic data sets show that our methods noticeably outperform the state-of-the-art.

#index 1642050
#* Temporal link prediction by integrating content and structure information
#@ Sheng Gao;Ludovic Denoyer;Patrick Gallinari
#t 2011
#c 1
#% 730089
#% 853532
#% 853534
#% 987253
#% 1117026
#% 1176865
#% 1199830
#% 1318728
#! In this paper we address the problem of temporal link prediction, i.e., predicting the apparition of new links, in time-evolving networks. This problem appears in applications such as recommender systems, social network analysis or citation analysis. Link prediction in time-evolving networks is usually based on the topological structure of the network only. We propose here a model which exploits multiple information sources in the network in order to predict link occurrence probabilities as a function of time. The model integrates three types of information: the global network structure, the content of nodes in the network if any, and the local or proximity information of a given vertex. The proposed model is based on a matrix factorization formulation of the problem with graph regularization. We derive an efficient optimization method to learn the latent factors of this model. Extensive experiments on several real world datasets suggest that our unified framework outperforms state-of-the-art methods for temporal link prediction tasks.

#index 1642051
#* Towards feature selection in network
#@ Quanquan Gu;Jiawei Han
#t 2011
#c 1
#% 190581
#% 329569
#% 465754
#% 593047
#% 722929
#% 729437
#% 757953
#% 840965
#% 913838
#% 961218
#% 987253
#% 1211747
#% 1305469
#% 1417091
#% 1482446
#! Traditional feature selection methods assume that the data are independent and identically distributed (i.i.d.). However, in real world, there are tremendous amount of data which are distributing in a network. Existing features selection methods are not suited for networked data because the i.i.d. assumption no longer holds. This motivates us to study feature selection in a network. In this paper, we present a supervised feature selection method based on Laplacian Regularized Least Squares (LapRLS) for networked data. In detail, we use linear regression to utilize the content information, and adopt graph regularization to consider the link information. The proposed feature selection method aims at selecting a subset of features such that the empirical error of LapRLS is minimized. The resultant optimization problem is a mixed integer programming, which is difficult to solve. It is relaxed into a $L_{2,1}$-norm constrained LapRLS problem and solved by accelerated proximal gradient descent algorithm. Experiments on benchmark networked data sets show that the proposed feature selection method outperforms traditional feature selection method and the state of the art learning in network approaches.

#index 1642052
#* Practical representations for web and social graphs
#@ Francisco Claude;Susana Ladra
#t 2011
#c 1
#% 453572
#% 754117
#% 794132
#% 847077
#% 954624
#% 957998
#% 1014808
#% 1080906
#% 1153123
#% 1173147
#% 1214643
#% 1267017
#% 1267026
#% 1394202
#% 1451193
#% 1456295
#% 1730133
#! In this paper we focus on representing Web and social graphs. Our work is motivated by the need of mining information out of these graphs, thus our representations do not only aim at compressing the graphs, but also at supporting efficient navigation. This allows us to process bigger graphs in main memory, avoiding the slowdown brought by resorting on external memory. We first show how by just partitioning the graph and combining two existing techniques for Web graph compression, k2-trees [Brisaboa, Ladra and Navarro, SPIRE 2009] and RePair-Graph [Claude and Navarro, TWEB 2010], exploiting the fact that most links are intra-domain, we obtain the best time/space trade-off for direct and reverse navigation when compared to the state of the art. In social networks, splitting the graph to achieve a good decomposition is not easy. For this case, we explore a new proposal for indexing MPK linearizations [Maserrat and Pei, KDD 2010], which have proven to be an effective way of representing social networks in little space by exploiting common dense subgraphs. Our proposal offers better worst case bounds in space and time, and is also a competitive alternative in practice.

#index 1642053
#* Determining the diameter of small world networks
#@ Frank W. Takes;Walter A. Kosters
#t 2011
#c 1
#% 282056
#% 300078
#% 314844
#% 577219
#% 823342
#% 881526
#% 937549
#% 1002007
#% 1128428
#% 1288161
#% 1294660
#! In this paper we present a novel approach to determine the exact diameter (longest shortest path length) of large graphs, in particular of the nowadays frequently studied small world networks. Typical examples include social networks, gene networks, web graphs and internet topology networks. Due to complexity issues, the diameter is often calculated based on a sample of only a fraction of the nodes in the graph, or some approximation algorithm is applied. We instead propose an exact algorithm that uses various lower and upper bounds as well as effective node selection and pruning strategies in order to evaluate only the critical nodes which ultimately determine the diameter. We will show that our algorithm is able to quickly determine the exact diameter of various large datasets of small world networks with millions of nodes and hundreds of millions of links, whereas before only approximations could be given.

#index 1642054
#* Detecting anomalies in graphs with numeric labels
#@ Michael Davis;Weiru Liu;Paul Miller;George Redpath
#t 2011
#c 1
#% 300136
#% 300183
#% 445369
#% 574604
#% 629708
#% 729983
#% 1490160
#% 1710593
#! This paper presents Yagada, an algorithm to search labelled graphs for anomalies using both structural data and numeric attributes. Yagada is explained using several security-related examples and validated with experiments on a physical Access Control database. Quantitative analysis shows that in the upper range of anomaly thresholds, Yagada detects twice as many anomalies as the best-performing numeric discretization algorithm. Qualitative evaluation shows that the detected anomalies are meaningful, representing a combination of structural irregularities and numerical outliers.

#index 1642055
#* Extracting multi-dimensional relations: a generative model of groups of entities in a corpus
#@ Ching-man Au Yeung;Tomoharu Iwata
#t 2011
#c 1
#% 722904
#% 842499
#% 868092
#% 869502
#% 876017
#% 903660
#% 1117023
#% 1214638
#% 1214702
#% 1214703
#% 1272187
#% 1374378
#% 1399948
#% 1400031
#% 1536554
#! Extracting relations among different entities from various data sources has been an important topic in data mining. While many methods focus only on a single type of relations, real world entities maintain relations that contain much richer information. We propose a hierarchical Bayesian model for extracting multi-dimensional relations among entities from a text corpus. Using data from Wikipedia, we show that our model can accurately predict the relevance of an entity given the topic of the document as well as the set of entities that are already mentioned in that document.

#index 1642056
#* Distributed social graph embedding
#@ Anne-Marie Kermarrec;Vincent Leroy;Gilles Trédan
#t 2011
#c 1
#% 122797
#% 300078
#% 576111
#% 765310
#% 770874
#% 789241
#% 955712
#% 985402
#% 1102544
#% 1706166
#! Distributed recommender systems are becoming increasingly important for they address both scalability and the Big Brother syndrome. Link prediction is one of the core mechanism in recommender systems and relies on extracting some notion of proximity between entities in a graph. Applied to social networks, defining a proximity metric between users enable to predict potential relevant future relationships. In this paper, we propose SoCS (Social Coordinate Systems}, a fully distributed algorithm that embeds any social graph in an Euclidean space, which can easily be used to implement link prediction. To the best of our knowledge, SoCS is the first system explicitly relying on graph embedding. Inspired by recent works on non-isomorphic embeddings, the SoCS embedding preserves the community structure of the original graph, while being easy to decentralize. Nodes thus get assigned coordinates that reflect their social position. We show through experiments on real and synthetic data sets that these coordinates can be exploited for efficient link prediction.

#index 1642057
#* Classification and annotation in social corpora using multiple relations
#@ Yann Jacob;Ludovic Denoyer;Patrick Gallinari
#t 2011
#c 1
#% 840965
#% 875948
#% 881557
#% 905823
#% 961218
#% 1030874
#% 1055712
#% 1131844
#% 1187392
#% 1192929
#% 1267758
#% 1288794
#% 1506188
#! We consider the problem of learning to annotate documents with concepts or keywords in content information networks, where the documents may share multiple relations. The concepts associated to a document will depend both on its content and on its neighbors in the network through the different relations. We formalize this problem as single and multi-label classification in a multi-graph, the nodes being the documents and the edges representing the different relations. The proposed algorithm learns to weight the different relations according to their importance for the annotation task. We perform experiments on different corpora corresponding to different annotation tasks on scientific articles, emails and Flickr images and show how the model may take advantage of the rich relational information.

#index 1642058
#* Plagiarism detection based on structural information
#@ Efstathios Stamatatos
#t 2011
#c 1
#% 571725
#% 616528
#% 654447
#% 757403
#% 838508
#% 882871
#% 1065411
#% 1074121
#% 1074122
#% 1077150
#% 1155644
#% 1166531
#% 1195895
#% 1261349
#% 1321312
#% 1450913
#% 1544119
#% 1558415
#! In this paper a novel method for detecting plagiarized passages in document collections is presented. In contrast to previous work in this field that uses mainly content terms to represent documents, the proposed method is based on structural information provided by occurrences of a small list of stopwords (i.e., very frequent words). We show that stopword n-grams are able to capture local syntactic similarities between suspicious and original documents. Moreover, an algorithm for detecting the exact boundaries of plagiarized and source passages is proposed. Experimental results on a publicly-available corpus demonstrate that the performance of the proposed approach is competitive when compared with the best reported results. More importantly, it achieves significantly better results when dealing with difficult plagiarism cases where the plagiarized passages are highly modified by replacing most of the words or phrases with synonyms to hide the similarity with the source documents.

#index 1642059
#* Studying how the past is remembered: towards computational history through large scale text mining
#@ Ching-man Au Yeung;Adam Jatowt
#t 2011
#c 1
#% 279755
#% 350859
#% 722904
#% 817550
#% 824531
#% 875959
#% 881498
#% 907491
#% 1120965
#% 1127964
#% 1251781
#% 1264755
#% 1292475
#% 1434129
#% 1434130
#% 1451202
#% 1523942
#% 1576176
#% 1586581
#! History helps us understand the present and even to predict the future to certain extent. Given the huge amount of data about the past, we believe computer science will play an increasingly important role in historical studies, with computational history becoming an emerging interdisciplinary field of research. We attempt to study how the past is remembered through large scale text mining. We achieve this by first collecting a large dataset of news articles about different countries and analyzing the data using computational and statistical tools. We show that analysis of references to the past in news articles allows us to gain a lot of insight into the collective memories and societal views of different countries. Our work demonstrates how various computational tools can assist us in studying history by revealing interesting topics and hidden correlations. Our ultimate objective is to enhance history writing and evaluation with the help of algorithmic support.

#index 1642060
#* Combining machine learning and human judgment in author disambiguation
#@ Yanan Qian;Yunhua Hu;Jianling Cui;Qinghua Zheng;Zaiqing Nie
#t 2011
#c 1
#% 760866
#% 809459
#% 809460
#% 870896
#% 874510
#% 956629
#% 1121280
#% 1131005
#% 1213413
#% 1663664
#! Author disambiguation in digital libraries becomes increasingly difficult as the number of publications and consequently the number of ambiguous author names keep growing. The fully automatic author disambiguation approach could not give satisfactory results due to the lack of signals in many cases. Furthermore, human judgment on the basis of automatic algorithms is also not suitable because the automatically disambiguated results are often mixed and not understandable for humans. In this paper, we propose a Labeling Oriented Author Disambiguation approach, called LOAD, to combine machine learning and human judgment together in author disambiguation. LOAD exploits a framework which consists of high precision clustering, high recall clustering, and top dissimilar clusters selection and ranking. In the framework, supervised learning algorithms are used to train the similarity functions between publications and a clustering algorithm is further applied to generate clusters. To validate the effectiveness and efficiency of the proposed LOAD approach, comprehensive experiments are conducted. Comparing to conventional author disambiguation algorithms, the LOAD yields much more accurate results to assist human labeling. Further experiments show that the LOAD approach can save labeling time dramatically.

#index 1642061
#* Citation count prediction: learning to estimate future citations for literature
#@ Rui Yan;Jie Tang;Xiaobing Liu;Dongdong Shan;Xiaoming Li
#t 2011
#c 1
#% 722904
#% 768632
#% 809424
#% 907511
#% 955712
#% 983833
#% 1083734
#% 1214701
#% 1214702
#% 1392484
#% 1399975
#% 1482198
#% 1482239
#! In most of the cases, scientists depend on previous literature which is relevant to their research fields for developing new ideas. However, it is not wise, nor possible, to track all existed publications because the volume of literature collection grows extremely fast. Therefore, researchers generally follow, or cite merely a small proportion of publications which they are interested in. For such a large collection, it is rather interesting to forecast which kind of literature is more likely to attract scientists' response. In this paper, we use the citations as a measurement for the popularity among researchers and study the interesting problem of Citation Count Prediction (CCP) to examine the characteristics for popularity. Estimation of possible popularity is of great significance and is quite challenging. We have utilized several features of fundamental characteristics for those papers that are highly cited and have predicted the popularity degree of each literature in the future. We have implemented a system which takes a series of features of a particular publication as input and produces as output the estimated citation counts of that article after a given time period. We consider several regression models to formulate the learning process and evaluate their performance based on the coefficient of determination (R-square). Experimental results on a real-large data set show that the best predictive model achieves a mean average predictive performance of 0.740 measured in R-square, which significantly outperforms several alternative algorithms.

#index 1642062
#* Extracting cross references from life science databases for search result ranking
#@ Anja Bachmann;Rene Schult;Matthias Lange;Myra Spiliopoulou
#t 2011
#c 1
#% 253188
#% 869534
#% 879567
#! Scholars in life sciences have to process huge amounts of data in a disciplined and efficient way. These data are spread among thousands of databases which overlap in content but differ substantially with respect to interface, formats and data structure. Search engines have the potential of assisting in data retrieval from these structured sources but fall short of providing a relevance ranking of the results that reflects the needs of life science scholars. One such need is to acquire insights to cross-references among entities in the databases, whereby search hits with many cross-references are expected to be more informative than those with few cross-references. In this work, we investigate to what extend this expectation holds. We propose BioXREF, a method that extracts cross-references from multiple life science databases by combining targeted crawling, pointer chasing, sampling and information extraction. We study the retrieval quality of our method and the relationship between manually crafted relevance ranking and relevance ranking based on cross-references, and report on first, promising results.

#index 1642063
#* Extracting collective expectations about the future from large text collections
#@ Adam Jatowt;Ching-man Au Yeung
#t 2011
#c 1
#% 817550
#% 907491
#% 1213423
#% 1292475
#% 1598409
#% 1632467
#! News articles often contain information about the future. Given the huge volume of information available nowadays, an automatic way for extracting and summarizing future-related information is desirable. Such information will allow people to obtain a collective image of the future, to recognize possible future scenarios and be prepared for the future events. We propose a model-based clustering algorithm for detecting future events based on information extracted from a text corpus. The algorithm takes into account both textual and temporal similarity of sentences. We demonstrate that our algorithm can be used to discover future events and estimate their probabilities over time.

#index 1642064
#* Towards a unified solution: data record region detection and segmentation
#@ Lidong Bing;Wai Lam;Yuan Gu
#t 2011
#c 1
#% 248808
#% 273925
#% 275915
#% 299488
#% 300288
#% 312860
#% 330784
#% 424931
#% 431536
#% 480824
#% 577319
#% 632051
#% 654469
#% 660272
#% 729978
#% 769437
#% 794239
#% 805845
#% 805846
#% 805847
#% 838491
#% 881505
#% 902460
#% 956500
#% 976684
#% 989660
#% 989661
#% 1190153
#% 1200332
#% 1292470
#% 1328069
#% 1364949
#! Although the task of data record extraction from Web pages has been studied extensively, yet it fails to handle many pages due to their complexity in format or layout. In this paper, we propose a unified method to tackle this task by addressing several key issues in a uniform manner. A new search structure, named as Record Segmentation Tree (RST), is designed, and several efficient search pruning strategies on the RST structure are proposed to identify the records in a given Web page. Another characteristic of our method which is significantly different from previous works is that it can effectively handle complicated and challenging data record regions. It is achieved by generating subtree groups dynamically from the RST structure during the search process. Furthermore, instead of using string edit distance or tree edit distance, we propose a token-based edit distance which takes each DOM node as a basic unit in the cost calculation. Extensive experiments are conducted on four data sets, including flat, nested, and intertwine records. The experimental results demonstrate that our method achieves higher accuracy compared with three state-of-the-art methods.

#index 1642065
#* Fast metadata-driven multiresolution tensor decomposition
#@ Claudio Schifanella;K. Selçuk Candan;Maria Luisa Sapino
#t 2011
#c 1
#% 274612
#% 805877
#% 910167
#% 989585
#% 989640
#% 1042588
#% 1089780
#% 1147645
#% 1176933
#% 1294079
#% 1300087
#% 1396474
#% 1482235
#! Tensors (multi-dimensional arrays) are widely used for representing high-order dimensional data, in applications ranging from social networks, sensor data, and Internet traffic. Multi-way data analysis techniques, in particular tensor decompositions, allow extraction of hidden correlations among multi-way data and thus are key components of many data analysis frameworks. Intuitively, these algorithms can be thought of as multi-way clustering schemes, which consider multiple facets of the data in identifying clusters, their weights, and contributions of each data element. Unfortunately, algorithms for fitting multi-way models are, in general, iterative and very time consuming. In this paper, we observe that, in many applications, there is a priori background knowledge (or metadata) about one or more domain dimensions. This metadata is often in the form of a hierarchy that clusters the elements of a given data facet (or mode). In this paper, we investigate whether such single-mode data hierarchies can be used to boost the efficiency of tensor decomposition process, without significant impact on the final decomposition quality. We consider each domain hierarchy as a guide to help provide higher- or lower-resolution views of the data in the tensor on demand and we rely on these metadata-induced multi-resolution tensor representations to develop a multiresolution approach to tensor decomposition. In this paper, we focus on an alternating least squares (ALS) based implementation of the PARAllel FACtors (PARAFAC) decomposition (which decomposes a tensor into a diagonal tensor and a set of factor matrices). Experiment results show that, when the available metadata is used as a rough guide, the proposed multiresolution method helps fit PARAFAC models with consistent (for both dense and sparse tensor representations, under different parameters settings) savings in execution time and memory consumption, while preserving the quality of the decomposition.

#index 1642066
#* Enabling information extraction by inference of regular expressions from sample entities
#@ Falk Brauer;Robert Rieger;Adrian Mocan;Wojciech M. Barczynski
#t 2011
#c 1
#% 278109
#% 283163
#% 300157
#% 311037
#% 464434
#% 480499
#% 833912
#% 864416
#% 867052
#% 1063579
#% 1166537
#% 1178389
#% 1181316
#% 1249461
#% 1264720
#% 1289318
#% 1370257
#! Regular expressions are the dominant technique to extract business relevant entities (e.g., invoice numbers or product names) from text data (e.g., invoices), since these entity types often follow a strict underlying syntactical pattern. However, the manual construction of regular expressions that guarantee a high recall and precision is a tedious manual task and requires expert knowledge. In this paper, we propose an approach that automatically infers regular expressions from a set of (positive) sample entities, which in turn can be derived either from enterprise databases (e.g., a product catalog) or annotated documents (e.g., historical invoices). The main innovation of our approach is that it learns effective regular expressions that can be easily interpreted and modified by a user. The effectiveness is obtained by a novel method that weights dependent entity features of different granularity (i.e. on character and token level) against each other and selects the most suitable ones to form a regular expression.

#index 1642067
#* Mining entity translations from comparable corpora: a holistic graph mapping approach
#@ Jinhan Kim;Long Jiang;Seung-won Hwang;Young-In Song;Ming Zhou
#t 2011
#c 1
#% 643022
#% 741114
#% 741892
#% 747831
#% 747947
#% 748444
#% 786574
#% 817453
#% 855296
#% 938673
#% 939737
#% 1264746
#% 1275016
#% 1330535
#% 1330593
#% 1481577
#% 1484310
#! This paper addresses the problem of mining named entity translations from comparable corpora, specifically, mining English and Chinese named entity translation. We first observe that existing approaches use one or more of the following named entity similarity metrics: entity, entity context, and relationship. Inspired by this observation, in this paper, we propose a new holistic approach, by (1) combining all similarity types used and (2) additionally considering relationship context similarity between pairs of named entities, a missing quadrant in the taxonomy of similarity metrics. We abstract the named entity translation problem as the matching of two named entity graphs extracted from the comparable corpora. Specifically, named entity graphs are first constructed from comparable corpora to extract relationship between named entities. Entity similarity and entity context similarity are then calculated from every pair of bilingual named entities. A reinforcing method is utilized to reflect relationship similarity and relationship context similarity between named entities. According to our experimental results, our holistic graph-based approach significantly outperforms previous approaches.

#index 1642068
#* Max margin learning on domain-independent web information extraction
#@ Bin Zhao;Xiaoxin Yin;Eric P. Xing
#t 2011
#c 1
#% 301241
#% 458379
#% 722816
#% 744210
#% 829043
#% 856762
#% 1019082
#% 1187347
#% 1560405
#% 1650318
#% 1810398
#% 1814768
#! Domain-independent web information extraction can be addressed as a structured prediction problem where we learn a mapping function from an input web page to the structured and interdependent output variables, labeling each block on the page. In this paper, built upon an HTML parser of Internet Explorer that parses and renders a web page based on HTML tags and visual appearance, we propose a max margin learning approach for web information extraction. Specifically, the output of the parser is a vision tree, which is similar to a DOM tree but with visual information, i.e., how each node is displayed. Based on this hierarchical structure, we develop a max margin learning method for labeling each of its nodes. Due to the rich connections between blocks on the web page, we further introduce edges that connect spatially adjacent nodes on the vision tree, complicating the problem into a cyclic graph labeling task. A max margin learning method on cyclic graphs is developed for this problem, where loopy belief propagation is used for approximate inference. Experimental results on web data extraction show the feasibility and promise of our approach.

#index 1642069
#* Finding dimensions for queries
#@ Zhicheng Dou;Sha Hu;Yulong Luo;Ruihua Song;Ji-Rong Wen
#t 2011
#c 1
#% 194298
#% 218978
#% 223781
#% 262084
#% 393530
#% 642985
#% 754068
#% 787502
#% 869651
#% 956649
#% 960356
#% 987212
#% 1035573
#% 1077150
#% 1127393
#% 1130807
#% 1176941
#% 1206746
#% 1251678
#% 1270281
#% 1328355
#% 1399998
#% 1450864
#% 1458885
#% 1475763
#% 1482281
#% 1482286
#% 1484352
#! We address the problem of finding multiple groups of words or phrases that explain the underlying query facets, which we refer to as query dimensions. We assume that the important aspects of a query are usually presented and repeated in the query's top retrieved documents in the style of lists, and query dimensions can be mined out by aggregating these significant lists. Experimental results show that a large number of lists do exist in the top results, and query dimensions generated by grouping these lists are useful for users to learn interesting knowledge about the queries.

#index 1642070
#* Large-scale question classification in cQA by leveraging Wikipedia semantic knowledge
#@ Li Cai;Guangyou Zhou;Kang Liu;Jun Zhao
#t 2011
#c 1
#% 73441
#% 198058
#% 211044
#% 344447
#% 757306
#% 766444
#% 907555
#% 956564
#% 961685
#% 1055679
#% 1055680
#% 1074073
#% 1074110
#% 1074128
#% 1083703
#% 1117027
#% 1214660
#% 1227600
#% 1250362
#% 1269899
#% 1292559
#% 1399953
#% 1450829
#% 1591994
#! With the flourishing of community-based question answering (cQA) services like Yahoo! Answers, more and more web users seek their information need from these sites. Understanding user's information need expressed through their search questions is crucial to information providers. Question classification in cQA is studied for this purpose. However, there are two main difficulties in applying traditional methods (question classification in TREC QA and text classification) to cQA: (1) Traditional methods confine themselves to classify a text or question into two or a few predefined categories. While in cQA, the number of categories is much larger, such as Yahoo! Answers, there contains 1,263 categories. Our empirical results show that with the increasing of the number of categories to moderate size, the performance of the classification accuracy dramatically decreases. (2) Unlike the normal texts, questions in cQA are very short, which cannot provide sufficient word co-occurrence or shared information for a good similarity measure due to the data sparseness. In this paper, we propose a two-stage approach for question classification in cQA that can tackle the difficulties of the traditional methods. In the first stage, we preform a search process to prune the large-scale categories to focus our classification effort on a small subset. In the second stage, we enrich questions by leveraging Wikipedia semantic knowledge to tackle the data sparseness. As a result, the classification model is trained on the enriched small subset. We demonstrate the performance of our proposed method on Yahoo! Answers with 1,263 categories. The experimental results show that our proposed method significantly outperforms the baseline method (with error reductions of 23.21%).

#index 1642071
#* Hierarchical tag visualization and application for tag recommendations
#@ Yang Song;Baojun Qiu;Umer Farooq
#t 2011
#c 1
#% 299119
#% 855601
#% 869482
#% 956579
#% 1074115
#% 1074117
#% 1131128
#% 1183281
#% 1190090
#% 1190119
#% 1202825
#% 1268491
#% 1272078
#! Social bookmarking sites typically visualize user-generated tags as tag clouds. While tag clouds effectively show the relative frequency and thus popularity of tags, they fail to convey two aspects to the users: (1) the similarity between tags, and (2) the abstractness of tags. We suggest an alternative to tag clouds known as tag hierarchies. Tag hierarchies are based on a minimum evolution-based greedy algorithm for tag hierarchy construction, which iteratively includes optimal tags into the tree that introduce minimum changes to the existing taxonomy. Our algorithm also uses a global tag ranking method to order tags according to their levels of abstractness as well as popularity such that more abstract tags will appear at higher levels in the taxonomy. Based on the tag hierarchy, we derive a new tag recommendation algorithm, which is a structure-based approach that does not require heavily trained models and thus is highly efficient. User studies and quantitative analysis suggest that (1) the tag hierarchy can potentially reduce the user's tagging time in comparison to tag clouds and other tag tree structures, and (2) the tag recommendation algorithm significantly outperforms existing content-based methods in quality.

#index 1642072
#* Perspective hierarchical dirichlet process for user-tagged image modeling
#@ Xin Chen;Xiaohua Hu;Yuan An;Zunyan Xiong;Tingting He;E. K. Park
#t 2011
#c 1
#% 318785
#% 642990
#% 724320
#% 905319
#% 913845
#% 1055743
#% 1130827
#% 1214625
#% 1292515
#% 1451208
#! In this paper, we proposed a perspective Hierarchical Dirichlet Process (pHDP) model to deal with user-tagged image modeling. The contribution is two-fold. Firstly, we associate image features with image tags. Secondly, we incorporate the user's perspectives into the image tag generation process and introduce new latent variables to determine if an image tag is generated from user's perspectives or from the image content. Therefore, the model is able to extract both embedded semantic components and user's perspectives from user-tagged images. Based on the proposed pHDP model, we achieve automatic image tagging with users' perspective. Experimental results show that the pHDP model achieves better image tagging performance compared to state-of-the-art topic models.

#index 1642073
#* Asking what no one has asked before: using phrase similarities to generate synthetic web search queries
#@ Marius Pasca
#t 2011
#c 1
#% 309127
#% 742092
#% 786511
#% 815297
#% 867884
#% 869501
#% 989578
#% 1043040
#% 1083721
#% 1130854
#% 1130855
#% 1190102
#% 1264760
#% 1328353
#% 1330553
#% 1338626
#% 1450865
#% 1470648
#% 1560359
#! This paper introduces a method for automatically inferring meaningful, not-yet-submitted queries. The inferred queries fill some of the knowledge gaps between documents, on one hand, and known (i.e., already-submitted) queries, on the other hand. Thus, the inferred queries expand query logs and increase their coverage. New candidate queries are over-generated from known queries via phrase similarity data, then filtered against the set of known queries. The accuracy of the generated queries is computed using open-domain questions from standard question answering evaluation sets. Over the ranked lists of questions inferred for each of the evaluation questions, the precision reaches 0.9 at rank 50. The set of inferred queries is more than twice as large as the set of input queries.

#index 1642074
#* Simultaneous joint and conditional modeling of documents tagged from two perspectives
#@ Pradipto Das;Rohini Srihari;Yun Fu
#t 2011
#c 1
#% 642990
#% 722904
#% 939349
#% 989639
#% 1166510
#% 1250278
#% 1299681
#% 1318713
#% 1338553
#% 1451208
#% 1470574
#% 1471304
#! This paper explores correspondence and mixture topic modeling of documents tagged from two different perspectives. There has been ongoing work in topic modeling of documents with tags (tag-topic models) where words and tags typically reflect a single perspective, namely document content. However, words in documents can also be tagged from different perspectives, for example, syntactic perspective as in part-of-speech tagging or an opinion perspective as in sentiment tagging. The models proposed in this paper are novel in: (i) the consideration of two different tag perspectives -- a document level tag perspective that is relevant to the document as a whole and a word level tag perspective pertaining to each word in the document; (ii) the attribution of latent topics with word level tags and labeling latent topics with images in case of multimedia documents; and (iii) discovering the possible correspondence of the words to document level tags. The proposed correspondence tag-topic model shows better predictive power i.e. higher likelihood on heldout test data than all existing tag topic models and even a supervised topic model. To evaluate the models in practical scenarios, quantitative measures between the outputs of the proposed models and the ground truth domain knowledge have been explored. Manually assigned (gold standard) document category labels in Wikipedia pages are used to validate model-generated tag suggestions using a measure of pairwise concept similarity within an ontological hierarchy like WordNet. Using a news corpus, automatic relationship discovery between person names was performed and compared to a robust baseline.

#index 1642075
#* External evaluation measures for subspace clustering
#@ Stephan Günnemann;Ines Färber;Emmanuel Müller;Ira Assent;Thomas Seidl
#t 2011
#c 1
#% 248792
#% 273891
#% 300131
#% 397384
#% 727868
#% 765518
#% 871026
#% 915305
#% 1083683
#% 1116995
#% 1117035
#% 1165480
#% 1176982
#% 1213625
#% 1214709
#% 1310270
#% 1318668
#% 1328215
#% 1535416
#! Knowledge discovery in databases requires not only development of novel mining techniques but also fair and comparable quality assessment based on objective evaluation measures. Especially in young research areas where no common measures are available, researchers are unable to provide a fair evaluation. Typically, publications glorify the high quality of one approach only justified by an arbitrary evaluation measure. However, such conclusions can only be drawn if the evaluation measures themselves are fully understood. In this paper, we provide the basis for systematic evaluation in the emerging research area of subspace clustering. We formalize general quality criteria for subspace clustering measures not yet addressed in the literature. We compare the existing external evaluation methods based on these criteria and pinpoint limitations. We propose a novel external evaluation measure which meets the requirements in form of quality properties. In thorough experiments we empirically show characteristic properties of evaluation measures. Overall, we provide a set of evaluation measures that fulfill the general quality criteria as recommendation for future evaluations. All measures and datasets are provided on our website and are integrated in our evaluation framework.

#index 1642076
#* Behavior-driven clustering of queries into topics
#@ Luca Maria Aiello;Debora Donato;Umut Ozertem;Filippo Menczer
#t 2011
#c 1
#% 194299
#% 232713
#% 233648
#% 280819
#% 290482
#% 310567
#% 342961
#% 406493
#% 642985
#% 766433
#% 783483
#% 823348
#% 907534
#% 946523
#% 989578
#% 1051181
#% 1055741
#% 1083721
#% 1096052
#% 1130868
#% 1130878
#% 1190162
#% 1254275
#% 1292768
#% 1292771
#% 1313996
#% 1399965
#% 1403567
#% 1450893
#% 1544138
#! Categorization of web-search queries in semantically coherent topics is a crucial task to understand the interest trends of search engine users and, therefore, to provide more intelligent personalization services. Query clustering usually relies on lexical and clickthrough data, while the information originating from the user actions in submitting their queries is currently neglected. In particular, the intent that drives users to submit their requests is an important element for meaningful aggregation of queries. We propose a new intent-centric notion of topical query clusters and we define a query clustering technique that differs from existing algorithms in both methodology and nature of the resulting clusters. Our method extracts topics from the query log by merging missions, i.e., activity fragments that express a coherent user intent, on the basis of their topical affinity. Our approach works in a bottom-up way, without any a-priori knowledge of topical categorization, and produces good quality topics compared to state-of-the-art clustering techniques. It can also summarize topically-coherent missions that occur far away from each other, thus enabling a more compact user profiling on a topical basis. Furthermore, such a topical user profiling discriminates the stream of activity of a particular user from the activity of others, with a potential to predict future user search activity.

#index 1642077
#* Discovering customer intent in real-time for streamlining service desk conversations
#@ Ullas Nambiar;Tanveer Faruquie;L. Venkata Subramaniam;Sumit Negi;Ganesh Ramakrishnan
#t 2011
#c 1
#% 190581
#% 204531
#% 310500
#% 376266
#% 466408
#% 769927
#% 969348
#% 1019069
#% 1063557
#% 1117092
#! Businesses require the contact center agents to meet pre-specified customer satisfaction levels while keeping the cost of operations low or meeting sales targets, objectives that end up being complementary and difficult to achieve in real-time. In this paper, we describe a speech enabled real-time conversation management system that tracks customer-agent conversations to detect user intent (e.g. gathering information, likely to buy, etc.) that can help agents to then decide the best sequence of actions for that call. We present an entropy based decision support system that parses a text stream generated in real-time during a audio conversation and identifies the first instance at which the intent becomes distinct enough for the agent to then take subsequent actions. We provide evaluation results displaying the efficiency and effectiveness of our system.

#index 1642078
#* Sparse structured probabilistic projections for factorized latent spaces
#@ Xinquan Qu;Xinlei Chen
#t 2011
#c 1
#% 1109848
#% 1136254
#% 1411729
#% 1587862
#! Building a common representation for several related data sets is an important problem in multi-view learning. CCA and its extensions have shown that they are effective in finding the shared variation among all data sets. However, these models generally fail to exploit the common structure of the data when the views are with private information. Recently, methods explicitly modeling the information into shared part and private parts have been proposed, but they presume to know the prior knowledge about the latent space, which is usually impossible to obtain. In this paper, we propose a probabilistic model, which could simultaneously learn the structure of the latent space whilst factorize the information correctly, therefore the prior knowledge of the latent space is unnecessary. Furthermore, as a probabilistic model, our method is able to deal with missing data problem in a natural way. We show that our approach attains the performance of state-of-art methods on the task of human pose estimation when the motion capture view is completely missing, and significantly improves the inference accuracy with only a few observed data.

#index 1642079
#* Automated feature generation from structured knowledge
#@ Weiwei Cheng;Gjergji Kasneci;Thore Graepel;David Stern;Ralf Herbrich
#t 2011
#c 1
#% 144031
#% 169729
#% 719299
#% 793419
#% 956564
#% 961134
#% 1044003
#% 1112388
#% 1183373
#% 1206702
#% 1264044
#% 1338596
#% 1409954
#% 1674714
#% 1693364
#! The prediction accuracy of any learning algorithm highly depends on the quality of the selected features; but often, the task of feature construction and selection is tedious and nonscalable. In recent years, however, there have been numerous projects with the goal of constructing general-purpose or domain-specific knowledge bases with entity-relationship-entity triples extracted from various Web sources or collected from user communities, e.g. YAGO, DBpedia, Freebase, UMLS, etc. This paper advocates the simple and yet far-reaching idea that the structured knowledge contained in such knowledge bases can be exploited to automatically extract features for general learning tasks. We introduce an expressive graph-based language for extracting features from such knowledge bases and a theoretical framework for constructing feature vectors from the extracted features. Our experimental evaluation on different learning scenarios provides evidence that the features derived through our framework can considerably improve the prediction accuracy, especially when the labeled data at hand is sparse.

#index 1642080
#* Filtering and clustering relations for unsupervised information extraction in open domain
#@ Wei Wang;Romaric Besançon;Olivier Ferret;Brigitte Grau
#t 2011
#c 1
#% 269217
#% 301241
#% 397597
#% 756964
#% 815325
#% 833928
#% 938705
#% 939699
#% 940029
#% 956506
#% 983614
#% 1002100
#% 1019100
#% 1077150
#% 1261583
#% 1275182
#% 1289456
#% 1318595
#% 1330550
#% 1330552
#% 1471230
#% 1492142
#! Information Extraction has recently been extended to new areas by loosening the constraints on the strict definition of the extracted information and allowing to design more open information extraction systems. In this new domain of unsupervised information extraction, we focus on the task of extracting and characterizing a priori unknown relations between a given set of entity types. One of the challenges of this task is to deal with the large amount of candidate relations when extracting them from a large corpus. We propose in this paper an approach for the filtering of such candidate relations based on heuristics and machine learning models. More precisely, we show that the best model for achieving this task is a Conditional Random Field model according to evaluations performed on a manually annotated corpus of about one thousand relations. We also tackle the problem of identifying semantically similar relations by clustering large sets of them. Such clustering is achieved by combining a classical clustering algorithm and a method for the efficient identification of highly similar relation pairs. Finally, we evaluate the impact of our filtering of relations on this semantic clustering with both internal measures and external measures. Results show that the filtering procedure doubles the recall of the clustering while keeping the same precision.

#index 1642081
#* Facilitating pattern discovery for relation extraction with semantic-signature-based clustering
#@ Yunyao Li;Vivian Chu;Sebastian Blohm;Huaiyu Zhu;Howard Ho
#t 2011
#c 1
#% 283180
#% 301241
#% 481290
#% 504443
#% 721139
#% 722926
#% 756964
#% 815868
#% 875064
#% 915277
#% 938705
#% 938763
#% 939378
#% 939383
#% 939909
#% 1019100
#% 1022227
#% 1108875
#% 1206687
#% 1250367
#% 1264744
#% 1264970
#% 1275182
#% 1288554
#% 1291356
#% 1301004
#% 1426638
#% 1471192
#% 1478137
#% 1481633
#% 1581974
#! Hand-crafted textual patterns have been the mainstay device of practical relation extraction for decades. However, there has been little work on reducing the manual effort involved in the discovery of effective textual patterns for relation extraction. In this paper, we propose a clustering-based approach to facilitate the pattern discovery for relation extraction. Specifically, we define the notion of semantic signature to represent the most salient features of a textual fragment. We then propose a novel clustering algorithm based on semantic signature, S2C, and its enhancement S2C+. Experiments on two real-world data sets show that, when compared with k-means clustering, S2C and S2C+ are at least an order of magnitude faster, while generating high quality clusters that are at least comparable to the best clusters generated by k-means without requiring any manual tuning. Finally, a user study confirms that our clustering-based approach can indeed help users discover effective textual patterns for relation extraction with only a fraction of the manual effort required by the conventional approach.

#index 1642082
#* Finding all justifications of OWL entailments using TMS and MapReduce
#@ Gang Wu;Guilin Qi;Jianfeng Du
#t 2011
#c 1
#% 3460
#% 154456
#% 444998
#% 1153273
#% 1279264
#% 1333439
#% 1374370
#% 1374374
#% 1374393
#% 1409922
#% 1737596
#! Finding all justifications of an OWL entailment is an important reasoning service for explaining logical inconsistencies. In this paper, we consider finding all justifications of an entailment in OWL pD* fragment, which is a fragment of OWL that makes possible decidable rule extensions of OWL. We first propose a novel approach to find all justifications of OWL pD* entailments using TMS and show the complexity of this approach. This approach is limited by the hardware capabilities of standalone systems. In order to improve its scalability to handle large scale semantic data, we optimize the proposed approach by exploiting the MapReduce technology. We implement our approach and the optimization, and do experiments on synthetic and real world data sets. Evaluation results show that our approach has the ability to scale to more than one billion triples.

#index 1642083
#* Estimating selectivity for joined RDF triple patterns
#@ Hai Huang;Chengfei Liu
#t 2011
#c 1
#% 44876
#% 129987
#% 197387
#% 240222
#% 273909
#% 322884
#% 824755
#% 956564
#% 996377
#% 1015256
#% 1016149
#% 1055731
#% 1127402
#% 1217194
#% 1272363
#! A fundamental problem related to RDF query processing is selectivity estimation, which is crucial to query optimization for determining a join order of RDF triple patterns. In this paper we focus research on selectivity estimation for SPARQL graph patterns. The previous work takes the join uniformity assumption when estimating the joined triple patterns. This assumption would lead to highly inaccurate estimations in the cases where properties in SPARQL graph patterns are correlated. We take into account the dependencies among properties in SPARQL graph patterns and propose a more accurate estimation model. Since star and chain query patterns are common in SPARQL graph patterns, we first focus on these two basic patterns and propose to use Bayesian network and chain histogram respectively for estimating the selectivity of them. Then, for estimating the selectivity of an arbitrary SPARQL graph pattern, we design algorithms for maximally using the precomputed statistics of the star paths and chain paths. The experiments show that our method outperforms existing approaches in accuracy.

#index 1642084
#* Efficient resource attribute retrieval in RDF triple stores
#@ Andreas Brodt;Oliver Schiller;Bernhard Mitschang
#t 2011
#c 1
#% 411554
#% 442850
#% 838519
#% 1016212
#% 1022236
#% 1092530
#% 1127402
#% 1127431
#% 1217194
#% 1409954
#% 1594602
#! The W3C Resource Description Framework (RDF) is gaining popularity for its ability to manage semi-structured data without a predefined database schema. So far, most RDF query processors have concentrated on finding complex graph patterns in RDF, which typically involves a high number of joins. This works very well to query resources by the relations between them. Yet, obtaining a record-like view on the attributes of resources, as natively supported by RDBMS, imposes unnecessary performance burdens, as the individual attributes must be joined to assemble the final result records. We present an approach to retrieve the attributes of resources efficiently. We first determine the resources in question and then retrieve all their attributes efficiently at once, exploiting contiguous storage in RDF indexes. In addition, we present an index structure which is specifically designed for RDF attribute retrieval. Our measurements show that our approach is clearly superior for larger numbers of attributes.

#index 1642085
#* Effective stratification for low selectivity queries on deep web data sources
#@ Fan Wang;Gagan Agrawal
#t 2011
#c 1
#% 198466
#% 227883
#% 277347
#% 333955
#% 465162
#% 481101
#% 956455
#% 960286
#% 960294
#% 1201871
#% 1206653
#% 1206906
#% 1426573
#! We study the problem of estimating the result of an aggregation query with low selectivity when a data source only supports limited data accesses. Existing stratified sampling techniques cannot be applied to such a problem since either it is very hard, if not impossible, to gather certain critical statistics from such a data source, or more importantly, the selective attribute of the query may not be queriable on the data source. In such cases, we need an effective mechanism to stratify the data and form homogeneous strata with respect to the selective attribute of the query, despite not being able to query the data source with the selective attribute. This paper presents and evaluates a stratification method for this problem utilizing a queriable auxiliary attribute. The breaking points for the stratification are computed based on a novel Bayesian Adaptive Harmony Search algorithm. This method derives from the existing Harmony search method, but includes novel objective function, and introduces a technique for dynamically adapting key parameters of this method. Our experiments show that the estimation accuracy achieved using our method is consistently higher than 95% even for 0.01% selectivity query, even when there is only a low correlation between the auxiliary attribute and the selective attribute. Furthermore, our method achieves at least a five fold reduction in estimation error over three other methods, for the same sampling cost.

#index 1642086
#* Finding information nebula over large networks
#@ Lijun Chang;Jeffrey Xu Yu;Lu Qin;Yuanyuan Zhu;Haixun Wang
#t 2011
#c 1
#% 278831
#% 348173
#% 577273
#% 643566
#% 730089
#% 769887
#% 805904
#% 881496
#% 956564
#% 1026960
#% 1063539
#% 1127383
#% 1127384
#% 1127445
#% 1206817
#% 1207007
#% 1292521
#% 1292670
#% 1328067
#% 1372721
#% 1426574
#% 1426635
#% 1451191
#% 1451242
#! Social and information networks have been extensively studied over years. In this paper, we concentrate ourselves on a large information network that is composed of entities and relationships, where entities are associated with sets of keyword terms (kterms) to specify what they are, and relationships describe the link structure among entities which can be very complex. Our work is motivated but is different from the existing works that find a best subgraph to describe how user-specified entities are connected. We compute information nebula (cloud) which is a set of top-K kterms P that are most correlated to a set of user-specified kterms Q, over a large information network. Our goal is to find how kterms are correlated given the complex information network among entities. The information nebula computing requests us to take all possible kterms into consideration for the top-K kterms selection, and needs to measure the similarity between kterms by considering all possible subgraphs that connect them instead of the best single one. In this work, we compute information nebula using a global structural-context similarity, and our similarity measure is independent of connection subgraphs. To the best of our knowledge, among the link-based similarity methods, none of the existing work considers similarity between two sets of nodes or two kterms. We propose new algorithms to find top-K kterms P for a given set of kterms Q based on the global structural-context similarity, without computing all the similarity scores of kterms in the large information network. We performed extensive performance studies using large real datasets, and confirmed the effectiveness and efficiency of our approach.

#index 1642087
#* Efficient methods for finding influential locations with adaptive grids
#@ Da Yan;Raymond Chi-Wing Wong;Wilfred Ng
#t 2011
#c 1
#% 86950
#% 217815
#% 480661
#% 824730
#% 1022250
#% 1063470
#% 1328203
#% 1426505
#% 1426530
#% 1720751
#! Given a set S of servers and a set C of clients, an optimal-location query returns a location where a new server can attract the greatest number of clients. Optimal-location queries are important in a lot of real-life applications, such as mobile service planning or resource distribution in an area. Previous studies assume that a client always visits its nearest server, which is too strict to be true in reality. In this paper, we relax this assumption and propose a new model to tackle this problem. We further generalize the problem to finding top-k optimal locations. The main challenge is that, even the fastest approach in existing studies needs to take hours to answer an optimal-location query on a typical real world dataset, which significantly limits the applications of the query. Using our relaxed model, we design an efficient grid-based approximation algorithm called FILM (Fast Influential Location Miner) to the queries, which is orders of magnitude faster than the best-known previous work and the number of clients attracted by a new server in the result location often exceeds 98% of the optimal. The algorithm is extended to finding k influential locations. Extensive experiments are conducted to show the efficiency and effectiveness of FILM on both real and synthetic datasets.

#index 1642088
#* Semi-indexing semi-structured data in tiny space
#@ Giuseppe Ottaviano;Roberto Grossi
#t 2011
#c 1
#% 288893
#% 593766
#% 835496
#% 956602
#% 963669
#% 994015
#% 996633
#% 1013630
#% 1044439
#% 1080906
#% 1131312
#% 1193640
#% 1264695
#% 1412873
#% 1484078
#! Semi-structured textual formats are gaining increasing popularity for the storage of document collections and rich logs. Their flexibility comes at the cost of having to load and parse a document entirely even if just a small part of it needs to be accessed. For instance, in data analytics massive collections are usually scanned sequentially, selecting a small number of attributes from each document. We propose a technique to attach to a raw, unparsed document (even in compressed form) a "semi-index": a succinct data structure that supports operations on the document tree at speed comparable with an in-memory deserialized object, thus bridging textual formats with binary formats. After describing the general technique, we focus on the JSON format: our experiments show that avoiding the full loading and parsing step can give speedups of up to 12 times for on-disk documents using a small space overhead.

#index 1642089
#* Evaluation of set-based queries with aggregation constraints
#@ Quoc Trung Tran;Chee-Yong Chan;Guoping Wang
#t 2011
#c 1
#% 1015319
#% 1269866
#% 1426572
#% 1433984
#% 1476463
#% 1594608
#! Many applications often require finding a set of items of interest with respect to some aggregation constraints. For example, a tourist might want to find a set of places of interest to visit in a city such that the total expected duration is no more than six hours and the total cost is minimized. We refer to such queries as SAC queries for ``set-based with aggregation constraints'' queries. The usefulness of SAC queries is evidenced by the many variations of SAC queries that have been studied which differ in the number and types of constraints supported. In this paper, we make two contributions to SAC query evaluation. We first establish the hardness of evaluating SAC queries with multiple count constraints and presented a novel, pseudo-polynomial time algorithm for evaluating a non-trivial fragment of SAC queries with multiple sum constraints and at most one of either count, group-by, or content constraint. We also propose a heuristic approach for evaluating general SAC queries. The effectiveness of our proposed solutions is demonstrated by an experimental performance study.

#index 1642090
#* Index structures and top-k join algorithms for native keyword search databases
#@ Günter Ladwig;Thanh Tran
#t 2011
#c 1
#% 379482
#% 824693
#% 875017
#% 875063
#% 960243
#% 960259
#% 994013
#% 1015325
#% 1063537
#% 1075132
#% 1181254
#% 1206910
#% 1217198
#% 1523800
#! For supporting keyword search on structured data, current solutions require large indexes to be built that redundantly store subgraphs called neighborhoods. Further, for exploring keyword search results, large graphs have to be loaded into memory. We propose a solution, which employs much more compact index structures for neighborhood lookups. Using these indexes, we reduce keyword search result exploration to the traditional database problem of top-k join processing, enabling results to be computed efficiently. In particular, this computation can be performed on data streams successively loaded from disk (i.e., does not require the entire input to be loaded at once into memory). For supporting this, we propose a top-k procedure based on the rank join operator, which not only computes the k-best results, but also selects query plans in a top-k fashion during the process. In experiments using large real-world datasets, our solution reduced storage requirements and also outperformed the state-of-the-art in terms of performance and scalability.

#index 1642091
#* Optimized processing of multiple aggregate continuous queries
#@ Shenoda Guirguis;Mohamed A. Sharaf;Panos K. Chrysanthis;Alexandros Labrinidis
#t 2011
#c 1
#% 36117
#% 237203
#% 300166
#% 333962
#% 397352
#% 397353
#% 404719
#% 565457
#% 654507
#% 726621
#% 745434
#% 803602
#% 810032
#% 810033
#% 875022
#% 893139
#% 907600
#% 913788
#% 960274
#% 960278
#% 1022231
#% 1456848
#% 1594582
#! Data Streams Management Systems are designed to support monitoring applications, which require the processing of hundreds of Aggregate Continuous Queries (ACQs). These ACQs typically have different time granularities, with possibly different selection predicates and group-by attributes. In order to achieve scalability in the presence of heavy workloads, in this paper, we introduce the concept of 'Weaveability' as an indicator of the potential gains of sharing the processing of ACQs. We then propose Weave Share, a cost-based optimizer that exploits weaveability to optimize the shared processing of ACQs. Our experimental analysis shows that Weave Share outperforms the alternative sharing schemes generating up to four orders of magnitude better quality plans. Finally, we describe a practical implementation of the Weave Share optimizer.

#index 1642092
#* XQuery optimization based on program slicing
#@ Jesus M. Almendros-Jimenez;Josep Silva;Salvador Tamarit
#t 2011
#c 1
#% 19622
#% 598747
#% 824777
#% 827135
#% 839158
#% 893111
#% 960780
#% 1015272
#% 1015354
#% 1022330
#% 1023963
#% 1130861
#! XQuery has become the standard query language for XML. The efforts put on this language have produced mature and efficient implementations of XQuery processors. However, in practice the efficiency of XQuery programs is strongly dependent on the ability of the programmer to combine different queries which often affect several XML sources that in turn can be distributed in different branches of the organization. Therefore, techniques to reduce the amount of data loaded and also to reduce the intermediate structures computed by queries is a necessity. In this work we propose a novel technique that allows the programmer to automatically optimize a query in such a way that unnecessary intermediate computations are avoided, and, in addition, it identifies the paths in the source XML documents that are really required to resolve the query.

#index 1642093
#* Learning-based relevance feedback for web-based relation completion
#@ zhixu li;Laurianne Sitbon;Xiaofang Zhou
#t 2011
#c 1
#% 340901
#% 340948
#% 823312
#% 1450901
#% 1482318
#! In a pilot application based on web search engine called Web-based Relation Completion (WebRC), we propose to join two columns of entities linked by a predefined relation by mining knowledge from the web through a web search engine. To achieve this, a novel retrieval task Relation Query Expansion (RelQE) is modelled: given an entity (query), the task is to retrieve documents containing entities in predefined relation to the given one. Solving this problem entails expanding the query before submitting it to a web search engine to ensure that mostly documents containing the linked entity are returned in the top K search results. In this paper, we propose a novel Learning-based Relevance Feedback (LRF) approach to solve this retrieval task. Expansion terms are learned from training pairs of entities linked by the predefined relation and applied to new entity-queries to find entities linked by the same relation. After describing the approach, we present experimental results on real-world web data collections, which show that the LRF approach always improves the precision of top-ranked search results to up to 8.6 times the baseline. Using LRF, WebRC also shows performances way above the baseline.

#index 1642094
#* Categorising logical differences between OWL ontologies
#@ Rafael S. Gonçalves;Bijan Parsia;Ulrike Sattler
#t 2011
#c 1
#% 578767
#% 1108116
#% 1136066
#% 1272206
#% 1379024
#% 1409922
#% 1503462
#! The analysis of changes between OWL ontologies (in the form of a diff ) is an important service for ontology engineering. A purely syntactic analysis of changes is insufficient to distinguish between changes that have logical impact and those that do not. The current state of the art in semantic diffing ignores logically ineffectual changes and lacks any further characterisation of even significant changes. We present a diff method based on an exhaustive categorisation of effectual and ineffectual changes between ontologies. In order to verify the applicability of our approach we apply it to 88 versions of the National Cancer Institute (NCI) Thesaurus (NCIt), and demonstrate that all categories are realized throughout the corpus. Based on the outcome of the NCIt study we argue that the devised categorisation of changes is helpful for ontology engineers and their understanding of changes carried out between ontologies.

#index 1642095
#* ReDRIVE: result-driven database exploration through recommendations
#@ Marina Drosou;Evaggelia Pitoura
#t 2011
#c 1
#% 227919
#% 579314
#% 729934
#% 893105
#% 894444
#% 915254
#% 1031998
#% 1217187
#% 1217203
#% 1218714
#% 1328119
#% 1426503
#% 1482250
#! Typically, users interact with database systems by formulating queries. However, many times users do not have a clear understanding of their information needs or the exact content of the database, thus, their queries are of an exploratory nature. In this paper, we propose assisting users in database exploration by recommending to them additional items that are highly related with the items in the result of their original query. Such items are computed based on the most interesting sets of attribute values (or faSets) that appear in the result of the original user query. The interestingness of a faSet is defined based on its frequency both in the query result and in the database instance. Database frequency estimations rely on a novel approach that employs an e-tolerance closed rare faSets representation. We report evaluation results of the efficiency and effectiveness of our approach on both real and synthetic datasets.

#index 1642096
#* Information re-finding by context: a brain memory inspired approach
#@ Tangjian Deng;Liang Zhao;Ling Feng;Wenwei Xue
#t 2011
#c 1
#% 233808
#% 642983
#% 987211
#% 998797
#% 1047435
#% 1047436
#% 1231242
#% 1292539
#% 1292616
#% 1783054
#! Re-finding what we have accessed before is a common behavior in real life. Psychological studies show that context under which information was accessed can serve as a powerful cue for information recall. "Finding the sweet recipe that I read at the hotel on the trip to Africa last year" is a context-based re-finding request example. Inspired by users' recall characteristics and human memory, we present a context memory model, where each context unit links to the data created/accessed before. Context units are organized in a clustering and associative manner, and evolve dynamically in life cycles. Based on the context memory, we build a recall-by-context query model. Two methods are devised to evaluate context-based recall queries. Our experiments with synthetic and real data show that evaluation exploring the use of context associations can get the best response time.

#index 1642097
#* Semantic data markets: a flexible environment for knowledge management
#@ Roberto De Virgilio;Giorgio Orsi;Letizia Tanca;Riccardo Torlone
#t 2011
#c 1
#% 191611
#% 237189
#% 442661
#% 442883
#% 824755
#% 992962
#% 1126564
#% 1206804
#% 1217122
#% 1269632
#% 1305620
#% 1357868
#% 1594576
#% 1605087
#% 1667768
#% 1713480
#! We present Nyaya, a system for the management of Semantic-Web data which couples a general-purpose and extensible storage mechanism with efficient ontology reasoning and querying capabilities. Nyaya processes large Semantic-Web datasets, expressed in multiple formalisms, by transforming them into a collection of Semantic Data Kiosks. Nyaya uniformly exposes the native meta-data of each kiosk using the datalog+- language, a powerful rule-based modelling language for ontological databases. The kiosks form a Semantic Data Market where the data in each kiosk can be uniformly accessed using conjunctive queries and where users can specify user-defined constraints over the data. Nyaya is easily extensible and robust to updates of both data and meta-data in the kiosk and can readily adapt to different logical organization of the persistent storage. The approach has been experimented using well-known benchmarks, and compared to state-of-the-art research prototypes and commercial systems.

#index 1642098
#* Advancing the discovery of unique column combinations
#@ Ziawasch Abedjan;Felix Naumann
#t 2011
#c 1
#% 481290
#% 791004
#% 893145
#% 936820
#! Unique column combinations of a relational database table are sets of columns that contain only unique values. Discovering such combinations is a fundamental research problem and has many different data management and knowledge discovery applications. Existing discovery algorithms are either brute force or have a high memory load and can thus be applied only to small datasets or samples. In this paper, the well-known Gordian algorithm [9] and "Apriori-based" algorithms [4] are compared and analyzed for further optimization. We greatly improve the Apriori algorithms through efficient candidate generation and statistics-based pruning methods. A hybrid solution HCA-Gordian combines the advantages of Gordian and our new algorithm HCA, and it outperforms all previous work in many situations.

#index 1642099
#* Continuously monitoring the correlations of massive discrete streams
#@ Yueguo Chen;Wei Wang;Xiaoyong Du;Xiaofang Zhou
#t 2011
#c 1
#% 172949
#% 333679
#% 397381
#% 421124
#% 810058
#% 824709
#% 893164
#% 1063530
#% 1206665
#% 1206677
#% 1328106
#% 1581890
#! The problem of monitoring the correlations of discrete streams is to continuously monitor the temporal correlations among massive discrete streams. A temporal correlation of two streams is defined as a tracking behavior, i.e., the most recent pattern of one stream is very similar to a historical pattern of another stream. The challenge is that both the tracking stream and the tracked stream are evolving, which causes the frequent updates of the correlation-ships. The straightforward way of monitoring correlations by brute-force subsequence matching will be very expensive for massive streams. We propose techniques that are able to significantly reduce the number of expensive subsequence matching calls, by continuously pruning and refining the correlated streams. Extensive experiments on the streaming trajectories show the significant performance improvement achieved by the proposed algorithms.

#index 1642100
#* Multiple keyword-based queries over XML streams
#@ Felipe C. Hummel;Altigran S. da Silva;Mirella M. Moro;Alberto H.F. Laender
#t 2011
#c 1
#% 397366
#% 654442
#% 765432
#% 960261
#% 1015258
#% 1016180
#% 1022274
#% 1147659
#% 1292478
#% 1372727
#! In this paper, we propose that various keyword-based queries be processed over XML streams in a multi-query processing way. Our algorithms rely on parsing stacks designed for simultaneously matching terms from several distinct queries and use new query indexes to speed up search operations when processing a large number of queries. Besides defining a new problem and novel solutions, we perform experiments in which aspects related to performance and scalability are examined.

#index 1642101
#* Authentication of location-based skyline queries
#@ Xin Lin;Jianliang Xu;Haibo Hu
#t 2011
#c 1
#% 654480
#% 810042
#% 874980
#% 893150
#% 1058620
#% 1080130
#% 1211647
#% 1594676
#! In outsourced spatial databases, the location-based service (LBS) provides query services to the clients on behalf of the data owner. However, if the LBS is not trustworthy, it may return incorrect or incomplete query results. Thus, authentication is needed to verify the soundness and completeness of query results. In this paper, we study the authentication problem for location-based skyline queries, which have recently been receiving increasing attention in LBS applications. We propose two authentication methods: one based on the traditional MR-tree index and the other based on a newly developed MR-Sky-tree. Experimental results demonstrate the efficiency of our proposed methods in terms of the authentication cost.

#index 1642102
#* Matching query processing in high-dimensional space
#@ Chunyang Ma;Yongluan Zhou;Lidan Shou;Dan Dai;Gang Chen
#t 2011
#c 1
#% 248796
#% 299978
#% 300162
#% 479649
#% 745466
#% 814646
#% 1022250
#% 1063470
#% 1328197
#% 1426417
#! In many applications, such as online dating or job hunting websites, users often need to search for potential matches based on the requirements or preferences imposed by both sides.We refer to this type of queries as matching queries. In spite of their wide applicabilities, there has been little attention devoted to improve their performance. As matching queries often appear in various forms even within a single application, we, in this paper, propose a general processing framework, which can efficiently process various forms of matching queries. Moreover, we elaborate the detailed processing algorithms for two particular forms of matching queries to illustrate the applicability of this framework. We conduct an extensive experimental study with both synthetic and real datasets. The results indicate that, for various matching queries, our techniques can dramatically improve the query performance, especially when the dimensionality is high.

#index 1642103
#* Answering label-constraint reachability in large graphs
#@ Kun Xu;Lei Zou;Jeffery Xu Yu;Lei Chen;Yanghua Xiao;Dongyan Zhao
#t 2011
#c 1
#% 88051
#% 722530
#% 864462
#% 960304
#% 991744
#% 1044451
#% 1063514
#% 1190065
#% 1217208
#% 1270470
#% 1380974
#% 1426512
#% 1594585
#! In this paper, we study a variant of reachability queries, called label-constraint reachability (LCR) queries, specifically,given a label set S and two vertices u1 and u2 in a large directed graph G, we verify whether there exists a path from u1 to u2 under label constraint S. Like traditional reachability queries, LCR queries are very useful, such as pathway finding in biological networks, inferring over RDF (resource description f ramework) graphs, relationship finding in social networks. However, LCR queries are much more complicated than their traditional counterpart.Several techniques are proposed in this paper to minimize the search space in computing path-label transitive closure. Furthermore, we demonstrate the superiority of our method by extensive experiments.

#index 1642104
#* The list Viterbi training algorithm and its application to keyword search over databases
#@ Silvia Rota;Sonia Bergamaschi;Francesco Guerra
#t 2011
#c 1
#% 269195
#% 1215247
#% 1467763
#% 1482251
#% 1523959
#% 1581860
#% 1581893
#% 1651549
#! Hidden Markov Models (HMMs) are today employed in a variety of applications, ranging from speech recognition to bioinformatics. In this paper, we present the List Viterbi training algorithm, a version of the Expectation-Maximization (EM) algorithm based on the List Viterbi algorithm instead of the commonly used forward-backward algorithm. We developed the batch and online versions of the algorithm, and we also describe an interesting application in the context of keyword search over databases, where we exploit a HMM for matching keywords into database terms. In our experiments we tested the online version of the training algorithm in a semi-supervised setting that allows us to take into account the feedbacks provided by the users.

#index 1642105
#* Context-based people search in labeled social networks
#@ Cheng-Te Li;Man-Kwan Shan;Shou-De Lin
#t 2011
#c 1
#% 818218
#% 875001
#% 879563
#% 879594
#% 956544
#% 967260
#% 989665
#% 1019117
#% 1019119
#% 1074116
#% 1214668
#% 1215458
#! In online social networking services, there are a range of scenarios in which users want to search a particular person given the targeted person one's name. The challenge of such people search is namesake, which means that there are many people possess the same names in the social network. In this paper, we propose to leverage the query contexts to tackle such problems. For example, given the information of one's graduation year and city, the last names of some individuals, one may wish to find classmates from his/her high school. We formulate such problem as the context-based people search. Given a social network in which each node is associated with a set of labels and given a query set of labels consisting of a targeted name label and other context labels, our goal is to return a ranking list of persons who possess the targeted name label and connects to other context labels with minimum communication costs through an effective subgraph in the social network. We consider the interactions among query labels to propose a grouping-based method to solve the context-based people search. Our method consists of three major parts. First, we model those nodes with query labels into a group graph which is able to reduce the search space to enhance the time efficiency. Second, we identify three different kinds of connectors which connecting different groups, and exploit connectors to find the corresponding detailed graph topology from the group graph. Third, we propose a Connector-Steiner Tree algorithm to retrieve a resulting ranked list of individuals who possess the targeted label. Experimental results on the DBLP bibliography data show that our grouping-based method can reach the good quality of returned persons as a greedy search algorithm at a considerable outperformance on the time efficiency.

#index 1642106
#* On benchmarking data translation systems for semantic-web ontologies
#@ Carlos R. Rivero;Inma Hernández;David Ruiz;Rafael Corchuelo
#t 2011
#c 1
#% 810078
#% 826032
#% 885468
#% 960233
#% 993981
#% 1019068
#% 1060776
#% 1065125
#% 1077467
#% 1127370
#% 1201361
#% 1206804
#% 1206875
#% 1374374
#% 1651528
#! Data translation, also known as data exchange, is an integration task that aims at populating a target model using data from a source model. This task is gaining importance in the context of semantic-web ontologies due to the increasing interest in graph databases and semantic-web agents. Currently, there are a variety of semantic-web technologies that can be used to implement data translation systems. This makes it difficult to assess them from an empirical point of view. In this paper, we present a benchmark that provides a catalogue of seven data translation patterns that can be instantiated by means of seven parameters. This allows us to create a variety of synthetic, domain-independent scenarios one can use to test existing data translation systems. We also illustrate how to analyse three such systems using our benchmark. The main benefit of our benchmark is that it allows to compare data translation systems side by side within a homogeneous framework.

#index 1642107
#* I/O-efficient algorithms for answering pattern-based aggregate queries in a sequence OLAP system
#@ Chun Kit Chui;Ben Kao;Eric Lo;Reynold Cheng
#t 2011
#c 1
#% 172950
#% 300179
#% 333850
#% 338609
#% 378388
#% 482088
#% 503878
#% 578560
#% 824747
#% 864470
#% 893157
#% 993958
#% 1063518
#% 1292567
#% 1732543
#! Many kinds of real-life data exhibit logical ordering among their data items and are thus sequential in nature. In recent years, the concept of Sequence OLAP (S-OLAP) has been proposed. The biggest distinguishing feature of SOLAP from traditional OLAP is that data sequences managed by an S-OLAP system are characterized by the subsequence/substring patterns they possess. An S-OLAP system thus supports pattern-based grouping and aggregation. Conceptually, an S-OLAP system maintains a sequence data cube which is composed of sequence cuboids. Each sequence cuboid presents the answer of a pattern-based aggregate (PBA) query. This paper focuses on the I/O aspects of evaluating PBA queries. We study the problems of joining plan selection and execution planning, which are the core issues in the design of I/O-efficient cuboid materialization algorithms. Through an empirical study, we show that our algorithms lead to a very I/O-efficient strategy for sequence cuboid materialization.

#index 1642108
#* Tractable XML data exchange via relations
#@ Rada Chirkova;Leonid Libkin;Juan L. Reutter
#t 2011
#c 1
#% 332166
#% 333981
#% 397366
#% 428146
#% 479956
#% 570875
#% 570877
#% 765432
#% 800004
#% 800576
#% 806215
#% 809239
#% 826032
#% 865766
#% 866986
#% 893094
#% 960233
#% 993981
#% 1013630
#% 1016139
#% 1039061
#% 1044476
#% 1215806
#% 1217117
#% 1217196
#% 1408529
#% 1424599
#% 1426460
#! We consider data exchange for XML documents: given source and target schemas, a mapping between them, and a document conforming to the source schema, construct a target document and answer target queries in a way that is consistent with source information. The problem has primarily been studied in the relational context, in which data-exchange systems have also been built. Since many XML documents are stored in relations, it is natural to consider using a relational system for XML data exchange. However, there is a complexity mismatch between query answering in relational and XML data exchange, which indicates that restrictions have to be imposed on XML schemas and mappings, and on XML shredding schemes, to make the use of relational systems possible. We isolate a set of five requirements that must be fulfilled in order to have a faithful representation of the XML data-exchange problem by a relational translation. We then demonstrate that these requirements naturally suggest the inlining technique for dataexchange tasks. Our key contribution is to provide shredding algorithms for schemas, documents, mappings and queries, and demonstrate that they enable us to correctly perform XML data-exchange tasks using a relational system.

#index 1642109
#* A parallel algorithm for computing borders
#@ Nicolas Hanusse;Sofian Maabout
#t 2011
#c 1
#% 248791
#% 310507
#% 420062
#% 448482
#% 458762
#% 459020
#% 465003
#% 465035
#% 481290
#% 487843
#% 579314
#% 629708
#% 729418
#% 824931
#% 841959
#% 844302
#% 887530
#% 903010
#% 1022310
#% 1089465
#% 1181293
#% 1202826
#% 1206698
#% 1262948
#% 1426572
#% 1573139
#! The border concept has been introduced by Mannila and Toivonen in their seminal paper [20]. This concept finds many applications, e.g maximal frequent itemsets, minimal functional dependencies, emerging patterns between consecutive database instances and materialized view selection. For large transactions and relational databases defined on n items or attributes, the running time of any border computations are mainly dominated by the time T (for standard sequential algorithms) required to test the interestingness, in general the frequencies, of sets of candidates. In this paper we propose a general parallel algorithm for computing borders whatever the application is. We prove the efficiency of our algorithm by showing that: (i) it generates exactly the same number of candidates as the standard sequential algorithm and, (ii) if the interestingness test time of a candidate is bounded by Δ then for a multi-processor shared memory machine with p cores, we prove that the total interestingness time Tp

#index 1642110
#* Supporting queries spanning across phases of evolving artifacts using Steiner forests
#@ Siarhei Bykau;John Mylopoulos;Flavio Rizzolo;Yannis Velegrakis
#t 2011
#c 1
#% 13016
#% 32903
#% 94589
#% 201898
#% 291869
#% 315025
#% 397349
#% 462212
#% 480429
#% 480827
#% 488624
#% 660011
#% 800499
#% 824693
#% 824736
#% 960237
#% 960259
#% 992830
#% 1072638
#% 1217114
#% 1333840
#% 1381948
#% 1523833
#! The problem of managing evolving data has attracted considerable research attention. Researchers have focused on the modeling and querying of schema/instance-level structural changes, such as, addition, deletion and modification of attributes. Databases with such a functionality are known as temporal databases. A limitation of the temporal databases is that they treat changes as independent events, while often the appearance (or elimination) of some structure in the database is the result of an evolution of some existing structure. We claim that maintaining the causal relationship between the two structures is of major importance since it allows additional reasoning to be performed and answers to be generated for queries that previously had no answers. We present here a novel framework for exploiting the evolution relationships between the structures in the database. In particular, our system combines different structures that are associated through evolution relationships into virtual structures to be used during query answering. The virtual structures define "possible" database instances, in a fashion similar to the possible worlds in the probabilistic databases. The framework includes a query answering mechanism that allows queries to be answered over these possible databases without materializing them. Evaluation of such queries raises many interesting technical challenges, since it requires the discovery of Steiner forests on the evolution graphs. On this problem we have designed and implemented a new dynamic programming algorithm with exponential complexity in the size of the input query and polynomial complexity in terms of both the attribute and the evolution data sizes.

#index 1642111
#* Provenance-based refresh in data-oriented workflows
#@ Robert Ikeda;Semih Salihoglu;Jennifer Widom
#t 2011
#c 1
#% 13016
#% 318704
#% 413119
#% 464891
#% 577523
#% 800499
#% 803468
#% 825661
#% 889107
#% 893095
#% 893167
#% 976987
#% 1022258
#% 1063593
#% 1231247
#% 1523811
#! We consider a general workflow setting in which input data sets are processed by a graph of transformations to produce output results. Our goal is to perform efficient selective refresh of elements in the output data, i.e., compute the latest values of specific output elements when the input data may have changed. We explore how data provenance can be used to enable efficient refresh. Our approach is based on capturing one-level data provenance at each transformation when the workflow is run initially. Then at refresh time provenance is used to determine (transitively) which input elements are responsible for given output elements, and the workflow is rerun only on that portion of the data needed for refresh. Our contributions are to formalize the problem setting and the problem itself, to specify properties of transformations and provenance that are required for efficient refresh, and to provide algorithms that apply to a wide class of transformations and workflows. We have built a prototype system supporting the features and algorithms presented in the paper. We report preliminary experimental results on the overhead of provenance capture, and on the crossover point between selective refresh and full workflow recomputation.

#index 1642112
#* Ranking support for keyword search on structured data using relevance models
#@ Veli Bicer;Thanh Tran;Radoslav Nedkov
#t 2011
#c 1
#% 218982
#% 262096
#% 340901
#% 420498
#% 766431
#% 824693
#% 875017
#% 940042
#% 956501
#% 960243
#% 960259
#% 994033
#% 1015325
#% 1063537
#% 1074127
#% 1130925
#% 1195848
#% 1206910
#% 1348342
#% 1482251
#! Keyword query processing over structured data has gained a lot of interest as keywords have proven to be an intuitive mean for accessing complex results in databases. While there is a large body of work that provides different mechanisms for computing keyword search results efficiently, a recent study has shown that the problem of ranking is much neglected. Existing strategies employ heuristics that perform only in ad-hoc experiments but fail to consistently and repeatedly deliver results across different information needs. We provide a principled approach for ranking that focuses on a well-established notion of what constitutes relevant keyword search results. In particular, we adopt relevance-based language models to consider the structure and semantics of keyword search results, and introduce novel strategies for smoothing probabilities in this structured data setting. Using a standardized evaluation framework, we show that our work largely and consistently outperforms all existing systems across datasets and various information needs.

#index 1642113
#* Efficient similarity search: arbitrary similarity measures, arbitrary composition
#@ Dustin Lange;Felix Naumann
#t 2011
#c 1
#% 243163
#% 248010
#% 333679
#% 342827
#% 342828
#% 420072
#% 577238
#% 729913
#% 857113
#% 913783
#% 915242
#% 1022229
#% 1044465
#% 1250576
#% 1292648
#% 1426578
#% 1455643
#! Given a (large) set of objects and a query, similarity search aims to find all objects similar to the query. A frequent approach is to define a set of base similarity measures for the different aspects of the objects, and to build light-weight similarity indexes on these measures. To determine the overall similarity of two objects, the results of these base measures are composed, e.g., using simple aggregates or more involved machine learning techniques. We propose the first solution to this search problem that does not place any restrictions on the similarity measures, the composition technique, or the data set size. We define the query plan optimization problem to determine the best query plan using the similarity indexes. A query plan must choose which individual indexes to access and which thresholds to apply. The plan result should be as complete as possible within some cost threshold. We propose the approximative top neighborhood algorithm, which determines a near-optimal plan while significantly reducing the amount of candidate plans to be considered. An exact version of the algorithm determines the optimal solution. Evaluation on real-world data indicates that both versions clearly outperform a complete search of the query plan space.

#index 1642114
#* Learning to rank results in relational keyword search
#@ Joel Coffman;Alfred C. Weaver
#t 2011
#c 1
#% 243728
#% 268079
#% 344447
#% 479803
#% 539108
#% 577224
#% 660011
#% 783474
#% 818242
#% 824693
#% 840846
#% 869534
#% 875017
#% 879588
#% 881477
#% 907546
#% 956564
#% 960243
#% 960259
#% 987243
#% 993987
#% 1015325
#% 1063537
#% 1063539
#% 1127445
#% 1206702
#% 1206817
#% 1217198
#% 1290045
#% 1407689
#% 1467763
#% 1479590
#% 1482251
#% 1721860
#! Keyword search within databases has become a hot topic within the research community as databases store increasing amounts of information. Users require an effective method to retrieve information from these databases without learning complex query languages (viz. SQL). Despite the recent research interest, performance and search effectiveness have not received equal attention, and scoring functions in particular have become increasingly complex while providing only modest benefits with regards to the quality of search results. An analysis of the factors appearing in existing scoring functions suggests that some factors previously deemed critical to search effectiveness are at best loosely correlated with relevance. We consider a number of these different scoring factors and use machine learning to create a new scoring function that provides significantly better results than existing approaches. We simplify our scoring function by systematically removing the factors with the lowest weight and show that this version still outperforms the previous state-of-the-art in this area.

#index 1642115
#* Adding structure to top-k: from items to expansions
#@ Xueyao Liang;Min Xie;Laks V.S. Lakshmanan
#t 2011
#c 1
#% 643566
#% 857482
#% 875001
#% 891559
#% 1015256
#% 1074115
#% 1075132
#% 1328119
#% 1399975
#% 1399998
#% 1400029
#% 1476464
#% 1482239
#% 1563362
#% 1710013
#! Keyword based search interfaces are extremely popular as a means for efficiently discovering items of interest from a huge collection, as evidenced by the success of search engines like Google and Bing. However, most of the current search services still return results as a flat ranked list of items. Considering the huge number of items which can match a query, this list based interface can be very difficult for the user to explore and find important items relevant to their search needs. In this work, we consider a search scenario in which each item is annotated with a set of keywords. E.g., in Web 2.0 enabled systems such as flickr and del.icio.us, it is common for users to tag items with keywords. Based on this annotation information, we can automatically group query result items into different expansions of the query corresponding to subsets of keywords. We formulate and motivate this problem within a top-k query processing framework, but as that of finding the top-k most important expansions. Then we study additional desirable properties for the set of expansions returned, and formulate the problem as an optimization problem of finding the best k expansions satisfying all the desirable properties. We propose several efficient algorithms for this problem. Our problem is similar in spirit to recent works on automatic facets generation, but has the important difference and advantage that we don't need to assume the existence of pre-defined categorical hierarchy which is critical for these works. Through extensive experiments on both real and synthetic datasets, we show our proposed algorithms are both effective and efficient.

#index 1642116
#* TEXplorer: keyword-based object search and exploration in multidimensional text databases
#@ Bo Zhao;Xide Lin;Bolin Ding;Jiawei Han
#t 2011
#c 1
#% 399056
#% 459025
#% 643566
#% 874975
#% 875001
#% 960285
#% 1015294
#% 1022234
#% 1035573
#% 1055719
#% 1074094
#% 1130311
#% 1130807
#% 1176884
#% 1181222
#% 1181290
#% 1217235
#% 1399998
#% 1482250
#! We propose a novel system TEXplorer that integrates keyword-based object ranking with the aggregation and exploration power of OLAP in a text database with rich structured attributes available, e.g., a product review database. TEXplorer can be implemented within a multi-dimensional text database, where each row is associated with structural dimensions (attributes) and text data (e.g., a document). The system utilizes the text cube data model, where a cell aggregates a set of documents with matching values in a subset of dimensions. Cells in a text cube capture different levels of summarization of the documents, and can represent objects at different conceptual levels. Users query the system by submitting a set of keywords. Instead of returning a ranked list of all the cells, we propose a keyword-based interactive exploration framework that could offer flexible OLAP navigational guides and help users identify the levels and objects they are interested in. A novel significance measure of dimensions is proposed based on the distribution of IR relevance of cells. During each interaction stage, dimensions are ranked according to their significance scores to guide drilling down; and cells in the same cuboids are ranked according to their relevance to guide exploration. We propose efficient algorithms and materialization strategies for ranking top-k dimensions and cells. Finally, extensive experiments on real datasets demonstrate the efficiency and effectiveness of our approach.

#index 1642117
#* The quality of the XML web
#@ Steven Grijzenhout;Maarten Marx
#t 2011
#c 1
#% 255212
#% 298221
#% 322415
#% 330702
#% 351927
#% 464719
#% 480479
#% 487275
#% 504573
#% 577353
#% 772031
#% 781545
#% 801668
#% 805911
#% 830529
#% 836149
#% 845589
#% 847340
#% 867280
#% 893098
#% 894435
#% 912094
#% 913783
#% 1055754
#% 1081462
#% 1181461
#% 1426465
#% 1700122
#! We collect evidence to answer the following question: Is the quality of the XML documents found on the web sufficient to apply XML technology like XQuery, XPath and XSLT? XML collections from the web have been previously studied statistically, but no detailed information about the quality of the XML documents on the web is available to date. We address this shortcoming in this study. We gathered 180K XML documents from the web. Their quality is surprisingly good; 85.4% is well-formed and 99.5% of all specified encodings is correct. Validity needs serious attention. Only 25% of all files contain a reference to a DTD or XSD, of which just one third is actually valid. Errors are studied in detail. Automatic error repair seems promising. Our study is well documented and easily repeatable. This paves the way for a periodic quality assessment of the XML web.

#index 1642118
#* Context-based entity description rule for entity resolution
#@ Lingli Li;Jianzhong Li;Hongzhi Wang;Hong Gao
#t 2011
#c 1
#% 248801
#% 460812
#% 654467
#% 875066
#% 1022229
#% 1055684
#% 1206821
#% 1328152
#% 1498542
#! In this paper, we consider the entity resolution(ER) problem, which is to identify objects referring to the same real-world entity. Prior work of ER involves expensive similarity comparison and clustering approaches. Additionally, the quality of entity resolution may be low due to insufficient information. To address these problems, by adopting context information of data objects, we present a novel framework of entity resolution, context-based entity description (CED), to make context information help entity resolution. In our framework, each entity is described by a set of CEDs. During entity resolution, objects are only compared with CEDs to determine its corresponding entity. Additionally, we propose efficient algorithms for CED discovery and CED-based entity resolution. We experimentally evaluated our CED-based ER algorithm on the real DBLP datasets, and the experimental results show that our algorithm can achieve both high precision and recall as well as outperform existing methods.

#index 1642119
#* Cost-efficient repair in inconsistent probabilistic databases
#@ Xiang Lian;Yincheng Lin;Lei Chen
#t 2011
#c 1
#% 273687
#% 442758
#% 778320
#% 810019
#% 810020
#% 810098
#% 824764
#% 833132
#% 864417
#% 893167
#% 992830
#% 1022228
#% 1063521
#% 1063725
#% 1127378
#% 1347336
#% 1426527
#! Due to the ubiquitous data uncertainty in many emerging real applications, efficient management of probabilistic databases has become an increasingly important yet challenging problem. In particular, one fundamental task of data management is to identify those unreliable data in the probabilistic database that violate integrity constraints (e.g., functional dependencies), and then quickly resolve data inconsistencies. In this paper, we formulate and tackle an important problem of repairing inconsistent probabilistic databases efficiently by value modification. Specifically, we propose a repair semantic, namely possible-world-oriented repair (PW-repair), which partitions possible worlds into several disjoint groups, and repairs these groups individually with minimum repair costs. Due to the intractable result that finding such a PW-repair strategy is NP-complete, we carefully design a heuristic-based greedy approach for PW-repair, which can efficiently obtain an effective repair of the inconsistent probabilistic database. Through extensive experiments, we show that our approach can achieve the efficiency and effectiveness of the repair on inconsistent probabilistic data.

#index 1642120
#* Approximate tensor decomposition within a tensor-relational algebraic framework
#@ Mijung Kim;Kasim Selçuk Candan
#t 2011
#c 1
#% 321635
#% 322880
#% 1176933
#% 1300087
#% 1482235
#! In this paper, we first introduce a tensor-based relational data model and define algebraic operations on this model. We note that, while in traditional relational algebraic systems the join operation tends to be the costliest operation of all, in the tensor-relational framework presented here, tensor decomposition becomes the computationally costliest operation. Therefore, we consider optimization of tensor decomposition operations within a relational algebraic framework. This leads to a highly efficient, effective, and easy-to-parallelize join-by-decomposition approach and a corresponding KL-divergence based optimization strategy. Experimental results provide evidence that minimizing KL-divergence within the proposed join-by-decomposition helps approximate the conventional join-then-decompose scheme well, without the associated time and space costs.

#index 1642121
#* RFID data analysis using tensor calculus for supply chain management
#@ Roberto De Virgilio;Franco Milicchio
#t 2011
#c 1
#% 43756
#% 317358
#% 864470
#% 918017
#% 1063491
#% 1150708
#! In current trends of consumer products market, there is a growing significance of the role of retailers in the governance of supply chains. RFID is a promising infrastructure-less technology, allowing to connect an object with its virtual counterpart, i.e., its representation within information systems. However, the amount of RFID data in supply chain management is vast, posing significant challenges for attaining acceptable performance on their analysis. Current approaches provide hard-coded solutions, with high consumption of resources; moreover, these exhibit very limited flexibility dealing with multidimensional queries, at various levels of granularity and complexity. In this paper we propose a general model for supply chain management based on the first principles of linear algebra, in particular on tensorial calculus. Leveraging our abstract algebraic framework, our technique allows both quick decentralized on-line processing, and centralized off-line massive business logic analysis, according to needs and requirements of supply chain actors. Experimental results show that our approach, utilizing recent linear algebra techniques can process analysis efficiently, when compared to recent approaches. In particular, we are able to carry out the required computations even in high memory constrained environments, such as on mobile devices. Moreover, when dealing with massive amounts of data, we are capable of exploiting recent parallel and distributed technologies, subdividing our tensor objects into sub-blocks, and processing them independently.

#index 1642122
#* Spreadsheet-based complex data transformation
#@ Vu Hung;Boualem Benatallah;Regis Saint-Paul
#t 2011
#c 1
#% 786388
#% 801413
#% 832195
#% 881740
#% 893094
#% 1013546
#% 1127370
#% 1206925
#% 1217196
#% 1384174
#% 1426518
#! Spreadsheets are used by millions of users as a routine all-purpose data management tool. It is now increasingly necessary for external applications and services to consume spreadsheet data. In this paper, we investigate the problem of transforming spreadsheet data to structured formats required by these applications and services. Unlike prior methods, we propose a novel approach in which transformation logic is embedded into a familiar and expressive spreadsheet-like formula mapping language. Popular transformation patterns provided by transformation languages and mapping tools, that are relevant to spreadsheet-based data transformation, are supported in the language via formulas. Consequently, the language avoids cluttering the source spreadsheets with transformations and turns out to be helpful when multiple schemas are targeted. We implemented a prototype and evaluated the benefits of our approach via experiments in a real application. The experimental results confirmed the benefits of our approach.

#index 1642123
#* High efficiency and quality: large graphs matching
#@ Yuanyuan Zhu;Lu Qin;Jeffrey Xu Yu;Yiping Ke;Xuemin Lin
#t 2011
#c 1
#% 45753
#% 288990
#% 318330
#% 443663
#% 737997
#% 772183
#% 775355
#% 787098
#% 940178
#% 948251
#% 1154676
#% 1202773
#% 1214474
#% 1252774
#% 1286835
#% 1387891
#% 1682600
#! Graph matching plays an essential role in many real applications. In this paper, we study how to match two large graphs by maximizing the number of matched edges, which is known as maximum common subgraph matching and is NP-hard. To find exact matching, it cannot handle a graph with more than 30 nodes. To find an approximate matching, the quality can be very poor. We propose a novel two-step approach which can efficiently match two large graphs over thousands of nodes with high matching quality. In the first step, we propose an anchor-selection/expansion approach to compute a good initial matching. In the second step, we propose a new approach to refine the initial matching. We give the optimality of our refinement and discuss how to randomly refine the matching with different combinations. We conducted extensive testing using real and synthetic datasets, and will report our findings.

#index 1642124
#* DELTA: indexing and querying multi-labeled graphs
#@ Jiong Yang;Shijie Zhang;Wei Jin
#t 2011
#c 1
#% 68108
#% 70370
#% 378391
#% 427199
#% 765429
#% 864425
#% 960305
#% 989645
#% 1022236
#% 1022280
#% 1063500
#% 1145935
#% 1181229
#% 1206699
#% 1206703
#% 1328183
#% 1482309
#% 1523825
#% 1523900
#! With the emergence of social networks and computational biology, more data are in the forms of multi-labeled graphs, where a vertex has multiple labels. Since most algorithms focus only on single labeled graphs, these algorithms perform inefficiently when applied to multi-labeled graphs. In this paper, we investigate the problem of subgraph indexing and matching in the multi-labeled graphs. The label set on a vertex is transformed into a high dimensional box. The R-tree is employed to store and index these boxes. The vertex matching problem can be transformed into spatial range queries on the high dimensional space. In addition, we study two types of queries: location and existence queries. In this paper, detailed algorithms are provided to process these two types of queries. Real and synthetic data sets are employed to demonstrate the efficiency and effectiveness of our subgraph indexing and query processing methods.

#index 1642125
#* Skynets: searching for minimum trees in graphs with incomparable edge weights
#@ Huiping Cao;K. Selçuk Candan;Maria Luisa Sapino
#t 2011
#c 1
#% 34368
#% 288976
#% 303076
#% 330678
#% 410276
#% 465167
#% 480671
#% 505433
#% 539108
#% 576762
#% 660011
#% 680294
#% 824693
#% 854034
#% 907527
#% 960243
#% 960259
#% 993987
#% 1063501
#% 1063539
#% 1126560
#% 1217198
#% 1357953
#% 1426512
#% 1482332
#% 1542528
#% 1704004
#! Query processing over weighted data graphs often involves searching for a minimum weighted subgraph --a tree-- which covers the nodes satisfying the given query criteria (such as a given set of keywords). Existing works often focus on graphs where the edges have scalar valued weights. In many applications, however, edge weights need to be represented as ranges (or intervals) of possible values. In this paper, we introduce the problem of skynets, for searching minimum weighted subgraphs, covering the nodes satisfying given query criteria, over interval-weighted graphs. The key challenge is that, unlike scalars which are often totally ordered, depending on the application specific semantics of the ≤ operator, intervals may be partially ordered. Naturally, the need to maintain alternative, incomparable solutions can push the computational complexity of the problem (which is already high for the case with totally ordered scalar edge weights) even higher. In this paper, we first provide alternative definitions of the ≤ operator for intervals and show that some of these lend themselves to efficient solutions. To tackle the complexity challenge in the remaining cases, we propose two optimization criteria that can be used to constrain the solution space. We also discuss how to extend existing approximation algorithms for Steiner trees to discover solutions to the skynet problem. For efficient calculation of the results, we introduce a novel skyline union operator. Experiments show that the proposed approach achieves significant gains in efficiency, while providing close to optimal results.

#index 1642126
#* Fast fully dynamic landmark-based estimation of shortest path distances in very large graphs
#@ Konstantin Tretyakov;Abel Armas-Cervantes;Luciano García-Bañuelos;Jaak Vilo;Marlon Dumas
#t 2011
#c 1
#% 338382
#% 548463
#% 743640
#% 785120
#% 813718
#% 1002007
#% 1019117
#% 1180668
#% 1292553
#% 1355056
#% 1399992
#% 1404186
#% 1475163
#% 1482228
#! Computing the shortest path between a pair of vertices in a graph is a fundamental primitive in graph algorithmics. Classical exact methods for this problem do not scale up to contemporary, rapidly evolving social networks with hundreds of millions of users and billions of connections. A number of approximate methods have been proposed, including several landmark-based methods that have been shown to scale up to very large graphs with acceptable accuracy. This paper presents two improvements to existing landmark-based shortest path estimation methods. The first improvement relates to the use of shortest-path trees (SPTs). Together with appropriate short-cutting heuristics, the use of SPTs allows to achieve higher accuracy with acceptable time and memory overhead. Furthermore, SPTs can be maintained incrementally under edge insertions and deletions, which allows for a fully-dynamic algorithm. The second improvement is a new landmark selection strategy that seeks to maximize the coverage of all shortest paths by the selected landmarks. The improved method is evaluated on the DBLP, Orkut, Twitter and Skype social networks.

#index 1642127
#* CP-index: on the efficient indexing of large graphs
#@ Yan Xie;Philip S. Yu
#t 2011
#c 1
#% 431105
#% 466644
#% 481779
#% 629708
#% 765429
#% 823347
#% 824693
#% 841960
#% 869492
#% 881466
#% 894441
#% 960305
#% 994157
#% 1022280
#% 1044450
#% 1044451
#% 1063500
#% 1063503
#% 1117006
#% 1127380
#% 1147657
#% 1181229
#% 1206699
#% 1328171
#% 1523818
#% 1523825
#% 1523835
#! Graph search, i.e., finding all graphs in a database D that contain the query graph q, is a classical primitive prevalent in various graph database applications. In the past, there has been an abundance of studies devoting to this topic; however, with the recent emergence of large information networks, it places new challenges to the research community. Most of the traditional graph search schemes utilize the strategy of graph feature based indexing, whereas the index construction step that often involves frequent subgraph mining becomes a bottleneck for large graphs due to the high computational complexity. Although there have been several methods proposed to solve this mining bottleneck such as summarization of database graphs, the frequent subgraphs thus generated as indexing features are still unsatisfactory because the feature set is in general not only inadequate or deficient for the large graph scenario, but also with many redundant features. Furthermore, the large size of the graphs makes it too easy for a small feature to be contained in many of them, severely impacting its selectivity and pruning power. Motivated by all the above issues we identify, in this paper we propose a novel CP-Index (Contact Preservation) for efficient indexing of large graphs. To overcome the low selectivity issue, we reap further pruning opportunities by leveraging each feature's location information in the database graphs. Specifically, we look at how features are touching upon each other in the query, and check whether this contact pattern is preserved in the target graphs. Then, to tackle the deficiency and redundancy problems associated with features, new feature generation and selection methods such as dual feature generation and size-increasing bootstrapping feature selection are introduced to complete our design. Experiment results show that CP-Index is much more effective in indexing large graphs.

#index 1642128
#* Learning to target: what works for behavioral targeting
#@ Sandeep Pandey;Mohamed Aly;Abraham Bagherjeiran;Andrew Hatch;Peter Ciccolo;Adwait Ratnaparkhi;Martin Zinkevich
#t 2011
#c 1
#% 1114743
#% 1190081
#% 1214642
#% 1214692
#% 1399936
#% 1400100
#% 1431624
#% 1450847
#% 1535253
#! Understanding what interests and delights users is critical to effective behavioral targeting, especially in information-poor contexts. As users interact with content and advertising, their passive behavior can reveal their interests towards advertising. Two issues are critical for building effective targeting methods: what metric to optimize for and how to optimize. More specifically, we first attempt to understand what the learning objective should be for behavioral targeting so as to maximize advertiser's performance. While most popular advertising methods optimize for user clicks, as we will show, maximizing clicks does not necessarily imply maximizing purchase activities or transactions, called conversions, which directly translate to advertiser's revenue. In this work we focus on conversions which makes a more relevant metric but also the more challenging one. Second is the issue of how to represent and combine the plethora of user activities such as search queries, page views, ad clicks to perform the targeting. We investigate several sources of user activities as well as methods for inferring conversion likelihood given the activities. We also explore the role played by the temporal aspect of user activities for targeting, e.g., how recent activities compare to the old ones. Based on a rigorous offline empirical evaluation over 200 individual advertising campaigns, we arrive at what we believe are best practices for behavioral targeting. We deploy our approach over live user traffic to demonstrate its superiority over existing state-of-the-art targeting methods.

#index 1642129
#* Large-scale behavioral targeting with a social twist
#@ Kun Liu;Lei Tang
#t 2011
#c 1
#% 889273
#% 949164
#% 961278
#% 1055737
#% 1117691
#% 1159217
#% 1190081
#% 1214642
#% 1214692
#% 1214703
#% 1355041
#% 1451176
#% 1482282
#% 1536509
#% 1618917
#! Behavioral targeting (BT) is a widely used technique for online advertising. It leverages information collected on an individual's web-browsing behavior, such as page views, search queries and ad clicks, to select the ads most relevant to user to display. With the proliferation of social networks, it is possible to relate the behavior of individuals and their social connections. Although the similarity among connected individuals are well established (i.e., homophily), it is still not clear whether and how we can leverage the activities of one's friends for behavioral targeting; whether forecasts derived from such social information are more accurate than standard behavioral targeting models. In this paper, we strive to answer these questions by evaluating the predictive power of social data across 60 consumer domains on a large online network of over 180 million users in a period of two and a half months. To our best knowledge, this is the most comprehensive study of social data in the context of behavioral targeting on such an unprecedented scale. Our analysis offers interesting insights into the value of social data for developing the next generation of targeting services.

#index 1642130
#* Evolving social search based on bookmarks and status messages from social networks
#@ Bastian Karweg;Christian Huetter;Klemens Böhm
#t 2011
#c 1
#% 956544
#% 967260
#% 1035588
#% 1043040
#% 1074116
#% 1132901
#% 1227602
#% 1399976
#% 1400065
#% 1400144
#% 1479593
#% 1667787
#! Social search is a variant of information retrieval where a document or website is considered relevant if individuals from the searcher's social network have interacted with it. Our ranking metric Social Relevance Score (SRS) is based on two factors. First, the engagement intensity quantifies the effort a user has made during an interaction. Second, users can assign a trust score to each person from their social network, which is then refined using social network analysis. We have tested our hypotheses with our search engine www.social-search.com, which extends the existing social bookmarking platform folkd.com. Our search engine integrates information the folkd.com users share through the popular social networks Twitter and Facebook. With permission of 2,385 testers, we have connected to their social graphs to generate a large-scale real-world dataset. Over the course of a two-month field study, 468,889 individuals have generated 24,854,281 website recommendations. We have used those links to enhance their search results while measuring the impact on the search behavior. We have found that social results are available for most queries and usually lead to more satisfying results.

#index 1642131
#* Social ranking for spoken web search
#@ Shrey Sahay;Nitendra Rajput;Niketan Pansare
#t 2011
#c 1
#% 452641
#% 755194
#% 838500
#% 860088
#% 939386
#% 1022757
#% 1055765
#% 1074096
#% 1292485
#% 1384170
#% 1431116
#% 1482201
#% 1560310
#% 1586878
#% 1896501
#! Spoken Web is an alternative Web for low-literacy users in the developing world. People can create audio content over phone and share on the Spoken Web. This enables easy creation of locally relevant content. Even on the World Wide Web in developed regions, the recent increase in traffic is due to the locally relevant content created on social networking sites. This paper argues that content search and ranking in the new scenario needs a re-look. The generic model of using in-links for ranking such content is not an appropriate measure of the content relevance in such a collaborative Web 2.0 world. This paper aims to bring the social context in Spoken Web ranking. We formulate a relationship function between the query-creator and the content-creator and use this as one measure of the content relevance to the user. The relationship function uses the geographical location of the two people and their prior browsing preferences as parameters to determine the relationship between the two users. Further we also determine the trustability of the content based on the content creator's acceptance measure by the social network. We use these two features in addition to the term-frequency - inverse-term-frequency match to rank the search results in context of the social network of the query-creator and provide a more specific and socially relevant result to the user.

#index 1642132
#* Effects of search success on search engine re-use
#@ Victor Hu;Maria Stone;Jan Pedersen;Ryen W. White
#t 2011
#c 1
#% 309095
#% 572780
#% 734050
#% 805200
#% 956495
#% 987224
#% 989668
#% 1055851
#% 1074056
#% 1292474
#% 1355038
#% 1384363
#% 1450833
#% 1451140
#% 1537504
#% 1598367
#% 1684547
#% 1715212
#! People's experiences when interacting with online services affects their decisions on reuse. Users of Web search engines are primarily focused on obtaining relevant information pertaining to their query. Search engines that fail to satisfy users' information needs may find their market share to be negatively affected. However, despite its importance to search providers, the relationship be-tween search success and search engine reuse is poorly understood. In this paper, we present a longitudinal log-based study with a large cohort of search engine users that quantifies the relationship between success and re-use of search engines. We use time series analysis to define two groups of users: stationary and non-stationary. We find that recent changes in satisfaction rate do correlate moderately with changes in rate of return for stationary users. For non-stationary users, we find that satisfaction and rate of return change together and in the same direction. We also find that some effects are stronger for a smaller player on the market than for a clear market leader, but both are affected. This is the first study to explore these issues in the context of Web search, and our findings have implications for search providers seeking to better understand their users and improving their experience.

#index 1642133
#* Enriching textbooks with images
#@ Rakesh Agrawal;Sreenivas Gollapudi;Anitha Kannan;Krishnaram Kenthapadi
#t 2011
#c 1
#% 340405
#% 351983
#% 722927
#% 751818
#% 780136
#% 793417
#% 935763
#% 1019082
#% 1040539
#% 1074098
#% 1166508
#% 1211636
#% 1220596
#% 1227647
#% 1269923
#% 1417787
#% 1450865
#% 1470687
#% 1480225
#% 1480880
#% 1482284
#% 1493413
#% 1528118
#% 1560308
#% 1857921
#% 1896512
#! Textbooks have a direct bearing on the quality of education imparted to the students. Therefore, it is of paramount importance that the educational content of textbooks should provide rich learning experience to the students. Recent studies on understanding learning behavior suggest that the incorporation of digital visual material can greatly enhance learning. However, textbooks used in many developing regions are largely text-oriented and lack good visual material. We propose techniques for finding images from the web that are most relevant for augmenting a section of the textbook, while respecting the constraint that the same image is not repeated in different sections of the same chapter. We devise a rigorous formulation of the image assignment problem and present a polynomial time algorithm for solving the problem optimally. We also present two image mining algorithms that utilize orthogonal signals and hence obtain different sets of relevant images. Finally, we provide an ensembling algorithm for combining the assignments. To empirically evaluate our techniques, we use a corpus of high school textbooks in use in India. Our user study utilizing the Amazon Mechanical Turk platform indicates that the proposed techniques are able to obtain images that can help increase the understanding of the textbook material.

#index 1642134
#* Exploring the corporate ecosystem with a semi-supervised entity graph
#@ Hassan H. Malik;Ian MacGillivray;Måns Olof-Ors;Siming Sun;Shailesh Saroha
#t 2011
#c 1
#% 333943
#% 464434
#% 479726
#% 577214
#% 769877
#% 782759
#% 783535
#% 818328
#% 823344
#% 956551
#% 1121044
#% 1166537
#% 1269349
#% 1288161
#% 1291461
#% 1356190
#% 1447584
#% 1451202
#% 1478714
#% 1486655
#% 1537003
#% 1696287
#% 1735563
#! Investment decisions in the financial markets require careful analysis of information available from multiple data sources. In this paper, we present Atlas, a novel entity-based information analysis and content aggregation platform that uses heterogeneous data sources to construct and maintain the "ecosystem" around tangible and logical entities such as organizations, products, industries, geographies, commodities and macroeconomic indicators. Entities are represented as vertices in a directed graph, and edges are generated using entity co-occurrences in unstructured documents and supervised information from structured data sources. Significance scores for the edges are computed using a method that combines supervised, unsupervised and temporal factors into a single score. Important entity attributes from the structured content and the entity neighborhood in the graph are automatically summarized as the entity "fingerprint". A highly interactive user interface provides exploratory access to the graph and supports common business use cases. We present results of experiments performed on five years of news and broker research data, and show that Atlas is able to accurately identify important and interesting connections in real-world entities. We also demonstrate that Atlas entity fingerprints are particularly useful in entity similarity queries, with a quality that rivals existing human maintained databases.

#index 1642135
#* Generating links to background knowledge: a case study using narrative radiology reports
#@ Jiyin He;Maarten de Rijke;Merlijn Sevenster;Rob van Ommering;Yuechen Qian
#t 2011
#c 1
#% 197394
#% 212363
#% 217248
#% 262096
#% 279755
#% 286069
#% 400847
#% 464434
#% 709630
#% 751850
#% 755821
#% 830902
#% 854173
#% 854903
#% 868096
#% 1019082
#% 1074394
#% 1130858
#% 1223735
#% 1227691
#% 1291576
#% 1301004
#% 1333455
#% 1489456
#% 1624266
#! Automatically annotating texts with background information has recently received much attention. We conduct a case study in automatically generating links from narrative radiology reports to Wikipedia. Such links help users understand the medical terminology and thereby increase the value of the reports. Direct applications of existing automatic link generation systems trained on Wikipedia to our radiology data do not yield satisfactory results. Our analysis reveals that medical phrases are often syntactically regular but semantically complicated, e.g., containing multiple concepts or concepts with multiple modifiers. The latter property is the main reason for the failure of existing systems. Based on this observation, we propose an automatic link generation approach that takes into account these properties. We use a sequential labeling approach with syntactic features for anchor text identification in order to exploit syntactic regularities in medical terminology. We combine this with a sub-anchor based approach to target finding, which is aimed at coping with the complex semantic structure of medical phrases. Empirical results show that the proposed system effectively improves the performance over existing systems.

#index 1642136
#* Information extraction from pathology reports in a hospital setting
#@ David Martinez;Yue Li
#t 2011
#c 1
#% 926881
#% 1277464
#% 1671156
#! As more health data becomes available, information extraction aims to make an impact on the workflows of hospitals and care centers. One of the targeted areas is the management of pathology reports, which are employed for cancer diagnosis and staging. In this work we integrate text mining tools in the workflow of the Royal Melbourne Hospital, to extract information from pathology reports with minimal expert intervention. Our framework relies on coarse-grained annotation (at document level), making it highly portable. Our evaluation shows that the kind of language used in these reports makes it feasible to extract information with high precision and recall, by means of state-of-the-art classification methods, and feature engineering.

#index 1642137
#* Extract knowledge from semi-structured websites for search task simplification
#@ Yingqin Gu;Jun Yan;Hongyan Liu;Jun He;Lei Ji;Ning Liu;Zheng Chen
#t 2011
#c 1
#% 348146
#% 480824
#% 504443
#% 654469
#% 730038
#% 805845
#% 989672
#% 1275182
#% 1291356
#! Simplifying the key tasks of search engine users by directly retrieving to them structured knowledge according to their queries is attracting much attention from both industry and academia. A bottleneck of this challenging problem is how to extract the structured knowledge from the noisy and complex Web scale websites automatically. In this paper, we propose an unsupervised automatic wrapper induction algorithm, named as Scalable Knowledge Extractor from webSites (SKES). SKES induces the wrapper in a divide and conquer mode, i.e., it divides the general wrapper into several sub-wrappers to learn from the data independently. Moreover, through employing techniques such as tag path representation of Web pages, SKES is verified to be efficient and noise-tolerant by the experimental results. Furthermore, based on our automatically extracted knowledge, we also built a prototype to serve structured knowledge to end users for simplifying their key search tasks. Very positive feedbacks were received on the prototype.

#index 1642138
#* Privacy protected knowledge management in services with emphasis on quality data
#@ Debapriyo Majumdar;Rose Catherine;Shajith Ikbal;Karthik Visweswariah
#t 2011
#c 1
#% 269217
#% 309119
#% 425039
#% 509533
#% 729939
#% 789855
#% 1074831
#% 1083691
#% 1127581
#% 1265050
#% 1381029
#% 1464199
#% 1490286
#% 1598413
#! Improving productivity of practitioners through effective knowledge management and delivering high quality service in Application Management Services (AMS) domain, are key focus areas for all IT services organizations. One source of historical knowledge in AMS is the large amount of resolved problem ticket data which are often confidential, immensely valuable, but majority of it is of very bad quality. In this paper, we present a knowledge management tool that detects the quality of information present in problem tickets and enables effective knowledge search in tickets by prioritizing quality data in the search ranking. The tool facilitates leveraging of knowledge across different AMS accounts, while preserving data privacy, by masking client confidential information. It also extracts several relevant entities contained in the noisy unstructured text entered in the tickets and presents them to the users. We present several experimental evaluations and a pilot study conducted with an AMS account which show that our tool is effective and leads to substantial improvement in productivity of the practitioners.

#index 1642139
#* Search result diversification for enterprise data
#@ Wei Zheng;Hui Fang;Conglei Yao;Min Wang
#t 2011
#c 1
#% 340948
#% 879579
#% 960287
#% 1166473
#% 1292596
#% 1400021
#% 1450870
#% 1482296
#% 1536552
#% 1588402
#% 1650298
#! Search result diversification aims to return a list of diversified relevant documents in order to satisfy different user information needs. Most of the efforts focused on Web Search, and few studies have considered another important search domain, i.e., enterprise search. Unlike Web search, enterprise search deals with both unstructured and structured data. In this paper, we propose to integrate the structured and unstructured data to discover meaningful query subtopics in search result diversification. Experimental results show that integrating structured and unstructured information allows us to discover high quality query, which are effective in diversifying the retrieval results.

#index 1642140
#* Diversification for multi-domain result sets
#@ Alessandro Bozzon;Marco Brambilla;Piero Fraternali;Marco Tagliasacchi
#t 2011
#c 1
#% 262112
#% 642975
#% 1074133
#% 1166473
#% 1190093
#% 1206662
#% 1399949
#% 1450870
#% 1472964
#% 1523826
#% 1536552
#% 1577256
#! Multi-domain search answers to queries spanning multiple entities, like "Find an affordable house in a city with low criminality index, good schools and medical services", by producing ranked sets of entity combinations that maximize relevance, measured by a function expressing the user's preferences. Due to the combinatorial nature of results, good entity instances (e.g., inexpensive houses) tend to appear repeatedly in top-ranked combinations. To improve the quality of the result set, it is important to balance relevance (i.e., high values of the ranking function) with diversity, which promotes different, yet almost equally relevant, entities in the top-k combinations. This paper explores two different notions of diversity for multi-domain result sets, compares experimentally alternative algorithms for the trade-off between relevance and diversity, and performs a user study for evaluating the utility of diversification in multi-domain queries.

#index 1642141
#* A peer's-eye view: network term clouds in a peer-to-peer system
#@ Raynor Vliegendhart;Martha Larson;Christoph Kofler;Johan Pouwelse
#t 2011
#c 1
#% 1016639
#% 1025695
#% 1026299
#% 1468339
#% 1697459
#% 1768420
#! We investigate term clouds that represent the content available in a peer-to-peer (P2P) network. Such network term clouds are non-trivial to generate in distributed settings. Our term cloud generator was implemented and released in Tribler--a widely-used, server-free P2P system--to support users in understanding the sorts of content available. Our evaluation and analysis focuses on three aspects of the clouds: coverage, usefulness and accumulation speed. A live experiment demonstrates that individual peers accumulate substantial network-level information, indicating good coverage of the overall content of the system. The results of a user study carried out on a crowdsourcing platform confirm the usefulness of clouds, showing that they succeed in conveying to users information on the type of content available in the network. An analysis of five example peers reveals that accumulation speeds of terms at new peers can support the development of a semantically diverse term set quickly after a cold start. This work represents the first investigation of term clouds in a live, 100% server-free P2P setting.

#index 1642142
#* RerankEverything: a reranking interface for exploring search results
#@ Takehiro Yamamoto;Satoshi Nakamura;Katsumi Tanaka
#t 2011
#c 1
#% 329090
#% 1185582
#! This paper proposes a system called "RerankEverything", which enables users to rerank search results in any search service, such as a Web search engine, an e-commerce site, a hotel reservation site, and so on. This system helps users explore diverse search results. In conventional search services, interactions between users and systems are quite limited and complicated. By using RerankEverything, users can interactively explore search results in accordance with their interests by reranking search results from various viewpoints. Experimental results show that our system potentially help users search more proactively. When using our system, users were more likely to click search results that were initially low ranked. Users also browsed through more diverse search results by reranking search results after giving various types of feedback with our system.

#index 1642143
#* HealthTrust: trust-based retrieval of you tube's diabetes channels
#@ Luis Fernandez-Luque;Randi Karlsen;Genevieve B. Melton
#t 2011
#c 1
#% 290830
#% 309779
#% 1016177
#% 1262208
#% 1400022
#% 1476492
#! The Internet has become one of the main sources of consumer health information. Health consumers have access to ever-growing health information resources, especially since the rise of the Social Media. For example, over 20.000 videos have been uploaded by American hospitals on to YouTube. To find health videos is challenging because of factors like tags spamming and misleading information. Previous studies have found difficulties when searching for good health videos in YouTube, including false information (e.g., herbal cures for diabetes or cancer). Our objective was to extract information about the trustworthiness of the diabetes YouTube's channels using link analysis of the diabetes online community by developing an algorithm, called HealthTrust, based on Hyperlink-Induced Topic Search (HITS) for ranking the most authoritative diabetes channels. The ranked list of channels from HealthTrust was compared with the list of the most relevant diabetes channels from YouTube. Two healthcare professionals made a blinded classification of channels based on whether they would recommend the channel to a patient. HealthTrust performed better for retrieving channels recommended by the professional reviewers. HealthTrust performed several times better than YouTube for filtering out the worst channels (i.e., those not recommended by any expert reviewer).

#index 1642144
#* Item categorization in the e-commerce domain
#@ Dan Shen;Jean David Ruvini;Manas Somaiya;Neel Sundaresan
#t 2011
#c 1
#% 280817
#% 309141
#% 344447
#% 420466
#% 465747
#% 770796
#% 783478
#% 1117691
#! Hierarchical classification is a challenging problem yet bears a broad application in real-world tasks. Item categorization in the ecommerce domain is such an example. In a large-scale industrial setting such as eBay, a vast amount of items need to be categorized into a large number of leaf categories, on top of which a complex topic hierarchy is defined. Other than the scale challenges, item data is extremely sparse and skewed distributed over categories, and exhibits heterogeneous characteristics across categories. A common strategy for hierarchical classification is the "gates-and-experts" methods, where a high-level classification is made first (the gates), followed by a low-level distinction (the experts). In this paper, we propose to leverage domain-specific feature generation and modeling techniques to greatly enhance the classification accuracy of the experts. In particular, we innovatively derive features to encode various rich domain knowledge and linguistic hints, and then adapt a SVM-based model to distinguish several very confusing category groups appeared as the performance bottleneck of a currently deployed live system at eBay. We use illustrative examples and empirical results to demonstrate the effectiveness of our approach, particularly the merit of smartly designed domain-specific features.

#index 1642145
#* An efficient method for using machine translation technologies in cross-language patent search
#@ Walid Magdy;Gareth J.F. Jones
#t 2011
#c 1
#% 879590
#% 1450905
#! Topics in prior-art patent search are typically full patent applications and relevant items are patents often taken from sources in different languages. Cross language patent retrieval (CLPR) technologies support searching for relevant patents across multiple languages. As such, CLPR requires a translation process between topic and document languages. The most popular method for crossing the language barrier in cross language information retrieval (CLIR) in general is machine translation (MT). High quality MT systems are becoming widely available for many language pairs and generally have higher effectiveness for CLIR than dictionary based methods. However for patent search, using MT for translation of the very long search queries requires significant time and computational resources. We present a novel MT approach specifically designed for CLIR in general and CLPR in particular. In this method information retrieval (IR) text pre-processing in the form of stop word removal and stemming are applied to the MT training corpus prior to the training phase of the MT system. Applying this step leads to a significant decrease in the MT computational and resource requirements in both the training and translation phases. Experiments on the CLEF-IP 2010 CLPR task show the new technique to be 5 to 23 times faster than standard MT for query translation, while maintaining statistically indistinguishable IR effectiveness. Furthermore the new method is significantly better than standard MT when only limited translation training resources are available.

#index 1642146
#* Understanding the types of information humans associate with geographic objects
#@ Ahmet Aker;Robert Gaizauskas
#t 2011
#c 1
#% 194252
#% 741106
#! In this paper we investigate what sorts of information humans request about geographical objects of the same type. For example, Edinburgh Castle and the Bodiam Castle are two objects of the same type - castle. The question is whether specific information is requested for the object type castle and how this information differs for objects of other types, e.g. church, museum or lake. We aim to answer this question using an online survey. In the survey we showed 184 participants 200 images pertaining to urban and rural objects and asked them to write questions for which they would like to know the answers when seeing those objects. Our analysis of 7644 questions collected in the survey shows that humans have shared ideas of what to ask about geographical objects. When the object types resemble each other (e.g. church, temple) the requested information is similar for the objects of these types. Otherwise, the information is specific to an object type. Our results can guide tasks involving automatic generation of templates for image descriptions, and their assessment as well as image indexing and organization.

#index 1642147
#* Google, bing and a new perspective on ranking similarity
#@ Bruno Cardoso;João Magalhães
#t 2011
#c 1
#% 115608
#% 298221
#% 306468
#% 956622
#% 1348089
#% 1465385
#% 1482231
#! In this paper, we propose a framework to characterize and compare two search engine results. Typical user-queries are ambiguous and, consequentially, each search engine will compute ranks in different manners, attempting to answer them in the best possible way. Thus, each search engine will have its own bias. Given the importance of the first page results in Web Search Engines, in this paper we propose a framework to assess the information presented in the first page by measuring the information entropy and the correlations between two ranks. Employing the recently proposed Rank-Biased Overlap measure [2] we compare to which extent do Bing and Google rankings in fact differ. We also extend this measure and propose a measure for comparing the information entropy present in two ranks. The proposed measure is based on the correlation of two ranks and the application of Jensen-Shannon's divergence among two document sets. Our methodology starts with 40,000 user queries and crawls the search results for these queries on both search engines. The results allow us to determine the search engines correlations, crawling coverage, information overlap, and information entropy.

#index 1642148
#* Effectiveness beyond the first crawl tier
#@ Rodrygo L.T. Santos;Craig Macdonald;Iadh Ounis
#t 2011
#c 1
#% 253188
#% 330609
#% 807302
#% 879598
#% 1621236
#! Modern Web crawlers seek to visit quality documents first, and re-visit them more frequently than other documents. As a result, the first-tier crawl of a Web corpus is typically of higher quality compared to subsequent crawls. In this paper, we investigate the impact of first-tier documents on adhoc retrieval performance. In particular, we analyse the retrieval performance of runs submitted to the adhoc task of the TREC 2009 Web track in terms of how they rank first-tier documents and how these documents contribute to the performance of each run. Our results show that the performance of these runs is heavily dependent on their ability to rank first-tier documents. Moreover, we show that, different from leading Web search engines, their attempt to go beyond the first tier almost always results in decreased performance. Finally, we show that selectively removing spam from different tiers can be a direction for fully exploiting documents beyond the first tier.

#index 1642149
#* Worker types and personality traits in crowdsourcing relevance labels
#@ Gabriella Kazai;Jaap Kamps;Natasa Milic-Frayling
#t 2011
#c 1
#% 1047347
#% 1150163
#% 1151011
#% 1264744
#% 1384364
#% 1384503
#% 1450896
#% 1478132
#% 1480225
#% 1587349
#% 1587350
#% 1598354
#% 1622383
#! Crowdsourcing platforms offer unprecedented opportunities for creating evaluation benchmarks, but suffer from varied output quality from crowd workers who possess different levels of competence and aspiration. This raises new challenges for quality control and requires an in-depth understanding of how workers' characteristics relate to the quality of their work. In this paper, we use behavioral observations (HIT completion time, fraction of useful labels, label accuracy) to define five worker types: Spammer, Sloppy, Incompetent, Competent, Diligent. Using data collected from workers engaged in the crowdsourced evaluation of the INEX 2010 Book Track Prove It task, we relate the worker types to label accuracy and personality trait information along the `Big Five' personality dimensions. We expect that these new insights about the types of crowd workers and the quality of their work will inform how to design HITs to attract the best workers to a task and explain why certain HIT designs are more effective than others.

#index 1642150
#* A nugget-based test collection construction paradigm
#@ Shahzad Rajput;Virgil Pavlu;Peter B. Golbus;Javed A. Aslam
#t 2011
#c 1
#% 312689
#% 544011
#% 879598
#% 879641
#% 1074132
#% 1074134
#% 1130811
#! The problem of building test collections is central to the development of information retrieval systems such as search engines. Starting with a few relevant "nuggets" of information manually extracted from existing TREC corpora, we implement and test a methodology that finds and correctly assesses the vast majority of relevant documents found by TREC assessors - as well as up to four times more additional relevant documents. Our methodology produces highly accurate test collections that hold the promise of addressing the issues of scalability, reusability, and applicability.

#index 1642151
#* Recency ranking by diversification of result set
#@ Andrey Styskin;Fedor Romanenko;Fedor Vorobyev;Pavel Serdyukov
#t 2011
#c 1
#% 989668
#% 1074133
#% 1130811
#% 1166473
#% 1227616
#% 1292528
#% 1355017
#% 1399966
#! In this paper, we propose a web search retrieval approach which automatically detects recency sensitive queries and increases the freshness of the ordinary document ranking by a degree proportional to the probability of the need in recent content. We propose to solve the recency ranking problem by using result diversification principles and deal with the query's non-topical ambiguity appearing when the need in recent content can be detected only with uncertainty. Our offine and online experiments with millions of queries from real search engine users demonstrate the significant increase in satisfaction of users presented with a search result generated by our approach.

#index 1642152
#* Patent query reduction using pseudo relevance feedback
#@ Debasis Ganguly;Johannes Leveling;Walid Magdy;Gareth J.F. Jones
#t 2011
#c 1
#% 298183
#% 340901
#% 766518
#% 783506
#% 838466
#% 855221
#% 987331
#% 1227746
#% 1450905
#% 1494803
#% 1587413
#! Queries in patent prior art search are full patent applications and much longer than standard ad hoc search and web search topics. Standard information retrieval (IR) techniques are not entirely effective for patent prior art search because of ambiguous terms in these massive queries. Reducing patent queries by extracting key terms has been shown to be ineffective mainly because it is not clear what the focus of the query is. An optimal query reduction algorithm must thus seek to retain the useful terms for retrieval favouring recall of relevant patents, but remove terms which impair IR effectiveness. We propose a new query reduction technique decomposing a patent application into constituent text segments and computing the Language Modeling (LM) similarities by calculating the probability of generating each segment from the top ranked documents. We reduce a patent query by removing the least similar segments from the query, hypothesising that removal of these segments can increase the precision of retrieval, while still retaining the useful context to achieve high recall. Experiments on the patent prior art search collection CLEF-IP 2010 show that the proposed method outperforms standard pseudo-relevance feedback (PRF) and a naive method of query reduction based on removal of unit frequency terms (UFTs).

#index 1642153
#* Relevance feedback exploiting query-specific document manifolds
#@ Chang Wang;Emine Yilmaz;Martin Szummer
#t 2011
#c 1
#% 94369
#% 340899
#% 340901
#% 840846
#% 1074079
#% 1074198
#% 1227635
#! We incorporate relevance feedback into a learning to rank framework by exploiting query-specific document similarities. Given a few judged feedback documents and many retrieved but unjudged documents for a query, we learn a function that adjusts the initial ranking score of each document. Scores are fit so that documents with similar term content get similar scores, and scores of judged documents are close to their labels. By such smoothing along the manifold of retrieved documents, we avoid overfitting, and can therefore learn a detailed query-specific scoring function with several dozen term weights.

#index 1642154
#* Insights into explicit semantic analysis
#@ Thomas Gottron;Maik Anderka;Benno Stein
#t 2011
#c 1
#% 228088
#% 1227677
#% 1270267
#% 1275012
#% 1305532
#% 1415756
#% 1697467
#! Since its debut the Explicit Semantic Analysis (ESA) has received much attention in the IR community. ESA has been proven to perform surprisingly well in several tasks and in different contexts. However, given the conceptual motivation for ESA, recent work has observed unexpected behavior. In this paper we look at the foundations of ESA from a theoretical point of view and employ a general probabilistic model for term weights which reveals how ESA actually works. Based on this model we explain some of the phenomena that have been observed in previous work and support our findings with new experiments. Moreover, we provide a theoretical grounding on how the size and the composition of the index collection affect the ESA-based computation of similarity values for texts.

#index 1642155
#* On bias problem in relevance feedback
#@ Qianli Xing;Yi Zhang;Lanbo Zhang
#t 2011
#c 1
#% 342707
#% 406493
#% 818209
#% 1074080
#% 1392451
#% 1482188
#! Relevance feedback is an effective approach to improve retrieval quality over the initial query. Typical relevance feedback methods usually select top-ranked documents for relevance judgments, then query expansion or model updating are carried out based on the feedback documents. However, the number of feedback documents is usually limited due to expensive human labeling. Thus relevant documents in the feedback set are hardly representative of all relevant documents and the feedback set is actually biased. As a result, the performance of relevance feedback will get hurt. In this paper, we first show how and where the bias problem exists through experiments. Then we study how the bias can be reduced by utilizing the unlabeled documents. After analyzing the usefulness of a document to relevance feedback, we propose an approach that extends the feedback set with carefully selected unlabeled documents by heuristics. Our experiment results show that the extended feedback set has less bias than the original feedback set and better performance can be achieved when the extended feedback set is used for relevance feedback.

#index 1642156
#* Selecting related terms in query-logs using two-stage SimRank
#@ Yunlong Ma;Hongfei Lin;Yuan Lin
#t 2011
#c 1
#% 340901
#% 577273
#% 879585
#% 1074080
#% 1127383
#% 1173699
#% 1181094
#! It is commonly believed that query logs from Web search are a gold mine for search business, because they reflect users' preference over Web pages presented by search engines, so a lot of studies based on query logs have been carried out in the last few years. In this study, we assume that two queries are relevant to each other when they have same clicked page in their result lists, and we also consider the queries' topics of user's need. Thus, we propose a Two-Stage SimRank (called TSS in this paper) algorithm based on SimRank and some clustering algorithms to compute the similarity among queries, and then use it to discover relevant terms for query expansion, considering the information of topics and the global relationships of queries concurrently, with a query log collected by a practical search engine. Experimental results on two TREC test collections show that our approach can discover qualified terms effectively and improve retrieval performance.

#index 1642157
#* On relevance, time and query expansion
#@ Giuseppe Amodeo;Giambattista Amati;Giorgio Gambosi
#t 2011
#c 1
#% 262096
#% 730070
#% 755899
#% 987257
#% 1074094
#% 1155729
#% 1174574
#% 1292475
#% 1482369
#% 1536521
#% 1697416
#! We present the results of our exploratory analysis on the relationship that exists between relevance and time. We observe how the amount of documents published in a given interval of time is related to the probability of relevance, and, using the time series analysis, we show the existence of a correlation between time and relevance. As an initial application of this analysis, we study query expansion exploiting the detection of publication time peaks over the Blog06 collection. We finally propose an effective approach for the query expansion in the blog search domain. Our approach is based on the documents publication trend being so completely independent of any external resource.

#index 1642158
#* Diverse retrieval via greedy optimization of expected 1-call@k in a latent subtopic relevance model
#@ Scott Sanner;Shengbo Guo;Thore Graepel;Sadegh Kharazmi;Sarvnaz Karimi
#t 2011
#c 1
#% 169781
#% 262112
#% 309093
#% 406493
#% 642975
#% 722904
#% 771841
#% 879618
#% 1074025
#% 1074133
#% 1166473
#% 1227591
#% 1292528
#% 1400021
#% 1450857
#% 1450988
#! It has been previously observed that optimization of the 1-call@k relevance objective (i.e., a set-based objective that is 1 if at least one document is relevant, otherwise 0) empirically correlates with diverse retrieval. In this paper, we proceed one step further and show theoretically that greedily optimizing expected 1-call@k w.r.t. a latent subtopic model of binary relevance leads to a diverse retrieval algorithm sharing many features of existing diversification approaches. This new result is complementary to a variety of diverse retrieval algorithms derived from alternate rank-based relevance criteria such as average precision and reciprocal rank. As such, the derivation presented here for expected 1-call@k provides a novel theoretical perspective on the emergence of diversity via a latent subtopic model of relevance --- an idea underlying both ambiguous and faceted subtopic retrieval that have been used to motivate diverse retrieval.

#index 1642159
#* Hybrid models for future event prediction
#@ Giuseppe Amodeo;Roi Blanco;Ulf Brefeld
#t 2011
#c 1
#% 169781
#% 805839
#% 956509
#% 1155729
#% 1174574
#% 1292475
#% 1426683
#% 1482369
#% 1536521
#! We present a hybrid method to turn off-the-shelf information retrieval (IR) systems into future event predictors. Given a query, a time series model is trained on the publication dates of the retrieved documents to capture trends and periodicity of the associated events. The periodicity of historic data is used to estimate a probabilistic model to predict future bursts. Finally, a hybrid model is obtained by intertwining the probabilistic and the time-series model. Our empirical results on the New York Times corpus show that autocorrelation functions of time-series suffice to classify queries accurately and that our hybrid models lead to more accurate future event predictions than baseline competitors.

#index 1642160
#* Adaptive term frequency normalization for BM25
#@ Yuanhua Lv;ChengXiang Zhai
#t 2011
#c 1
#% 169781
#% 218982
#% 324129
#% 411760
#% 766412
#% 783474
#% 960413
#% 1598452
#% 1641914
#! A key component of BM25 contributing to its success is its sub linear term frequency (TF) normalization formula. The scale and shape of this TF normalization component is controlled by a parameter k1, which is generally set to a term-independent constant. We hypothesize and show empirically that in order to optimize retrieval performance, this parameter should be set in a term-specific way. Following this intuition, we propose an information gain measure to directly estimate the contributions of repeated term occurrences, which is then exploited to fit the BM25 function to predict a term-specific k1. Our experiment results show that the proposed approach, without needing any training data, can efficiently and automatically estimate a term-specific k1, and is more effective and robust than the standard BM25.

#index 1642161
#* An unsupervised ranking method based on a technical difficulty terrain
#@ Shoaib Jameel;Wai Lam;Ching-man Au Yeung;Sheaujiun Chyan
#t 2011
#c 1
#% 309095
#% 838442
#% 861988
#% 907541
#% 1166518
#% 1292770
#! Users look for information that can suit their level of expertise, but it often takes a mammoth effort to trace such information. One has to sift through multiple pages to look for one that fits the appropriate technical background. In this paper, a query-independent ranking system is proposed for technical web pages. The pages returned by the system are sorted by their relative technical difficulty in either ascending or descending order specified by the user. The technical difficulty of a document i.e. terms in sequence, is first computed by the combination of each individual term's geometry in the low-dimensional latent semantic indexing (LSI) space, which can be visualized as a conceptual terrain. Then the pages are ranked based on the expected cost to get over the terrain. Results indicate that our terrain based method outperforms traditional readability measures.

#index 1642162
#* When close enough is good enough: approximate positional indexes for efficient ranked retrieval
#@ Tamer Elsayed;Jimmy Lin;Donald Metzler
#t 2011
#c 1
#% 109190
#% 290703
#% 340887
#% 397205
#% 648308
#% 818262
#% 879651
#% 976952
#! Previous research has shown that features based on term proximity are important for effective retrieval. However, they incur substantial costs in terms of larger inverted indexes and slower query execution times as compared to term-based features. This paper explores whether term proximity features based on approximate term positions are as effective as those based on exact term positions. We introduce the novel notion of approximate positional indexes based on dividing documents into coarse-grained buckets and recording term positions with respect to those buckets. We propose different approaches to defining the buckets and compactly encoding bucket ids. In the context of linear ranking functions, experimental results show that features based on approximate term positions are able to achieve effectiveness comparable to exact term positions, but with smaller indexes and faster query evaluation.

#index 1642163
#* Index tuning for query-log based on-line index maintenance
#@ Sairam Gurajada;Sreenivasa Kumar P.
#t 2011
#c 1
#% 86532
#% 172922
#% 290703
#% 481439
#% 747117
#% 838465
#% 838541
#% 878624
#% 879609
#% 1019137
#% 1292509
#! The existing query-log based on-line index maintenance approaches rely on frequency distribution of terms in the static query-log. Though these approaches are proved to be efficient, but in real world, the frequency distribution of the terms changes over a period of time. This negatively affects the efficiency of the static query-log based approaches. To overcome this problem, we propose an index tuning strategy for reorganizing the indexes according to the latest frequency distribution of the terms captured from query-logs.Experimental results show that the proposed tuning strategy improves the performance of static query-log based approaches.

#index 1642164
#* Efficient phrase querying with flat position index
#@ Dongdong Shan;Wayne Xin Zhao;Jing He;Rui Yan;Hongfei Yan;Xiaoming Li
#t 2011
#c 1
#% 296646
#% 397150
#% 781169
#% 786632
#% 987214
#% 1019138
#% 1166469
#% 1536506
#% 1682445
#! A large proportion of search engine queries contain phrases,namely a sequence of adjacent words. In this paper, we propose to use flat position index (a.k.a schema-independent index) for phrase query evaluation. In the flat position index, the entire document collection is viewed as a huge sequence of tokens. Each token is represented by one flat position, which is a unique position offset from the beginning of the collection. Each indexed term is associated with a list of the flat positions about that term in the sequence. To recover DocID from flat positions efficiently, we propose a novel cache sensitive look-up table (CSLT), which is much faster than existing search algorithms. Experiments on TREC GOV2 data collection show that flat position index can reduce the index size and speed up phrase querying substantially, compared with traditional word-level index.

#index 1642165
#* Trained trigger language model for sentence retrieval in QA: bridging the vocabulary gap
#@ Saeedeh Momtazi;Dietrich Klakow
#t 2011
#c 1
#% 158687
#% 280851
#% 1292734
#% 1450869
#% 1470608
#! We propose a novel language model for sentence retrieval in Question Answering (QA) systems called trained trigger language model. This model addresses the word mismatch problem in information retrieval. The proposed model captures pairs of trigger and target words while training on a large corpus. The word pairs are extracted based on both unsupervised and supervised approaches while different notions of triggering are used. In addition, we study the impact of corpus size and domain for a supervised model. All notions of the trained trigger model are finally used in a language model-based sentence retrieval framework. Our experiments on TREC QA collection verify that the proposed model significantly improves the sentence retrieval performance compared to the state-of-the-art translation model and class model which address the same problem.

#index 1642166
#* Topic modeling for named entity queries
#@ Xiaobing Xue;Xiaoxin Yin
#t 2011
#c 1
#% 342707
#% 722904
#% 765412
#% 805839
#% 1083721
#% 1130868
#% 1214708
#% 1227610
#% 1292768
#% 1400017
#% 1400033
#% 1650298
#! Named entities are observed in a large portion of web search queries (named entity queries), where each entity can be associated with many different query terms that refer to various aspects of this entity. Organizing these query terms into topics helps understand major search intents about entities and the discovered topics are useful for applications such as query suggestion. Furthermore, we notice that named entities can often be organized into categories and those from the same category share many generic topics. Therefore, working on a category of named entities instead of individual ones helps avoid the problems caused by the sparsity and noise in the data. In this paper, Named Entity Topic Model (NETM) is proposed to discover generic topics for a category of named entities, where the quality of the generic topics is improved through the model design and the parameter initialization. Experiments based on query log data show that NETM discovers high-quality topics and outperforms the state-of-the-art techniques by 12.8% based on F1 measure.

#index 1642167
#* Semantic convolution kernels over dependency trees: smoothed partial tree kernel
#@ Danilo Croce;Alessandro Moschitti;Roberto Basili
#t 2011
#c 1
#% 466759
#% 742218
#% 815303
#% 815896
#% 1019151
#% 1130832
#% 1249504
#% 1392456
#% 1481475
#% 1482224
#% 1484299
#% 1592162
#% 1665151
#% 1711790
#% 1711819
#! In recent years, natural language processing techniques have been used more and more in IR. Among other syntactic and semantic parsing are effective methods for the design of complex applications like for example question answering and sentiment analysis. Unfortunately, extracting feature representations suitable for machine learning algorithms from linguistic structures is typically difficult. In this paper, we describe one of the most advanced piece of technology for automatic engineering of syntactic and semantic patterns. This method merges together convolution dependency tree kernels with lexical similarities. It can efficiently and effectively measure the similarity between dependency structures, whose lexical nodes are in part or completely different. Its use in powerful algorithm such as Support Vector Machines (SVMs) allows for fast design of accurate automatic systems. We report some experiments on question classification, which show an unprecedented result, e.g. 41% of error reduction of the former state-of-the-art, along with the analysis of the nice properties of the approach.

#index 1642168
#* Recommending citations with translation model
#@ Yang Lu;Jing He;Dongdong Shan;Hongfei Yan
#t 2011
#c 1
#% 280826
#% 280851
#% 397145
#% 415107
#% 760853
#% 987287
#% 1055685
#% 1074110
#% 1083684
#% 1195999
#% 1227690
#% 1399975
#% 1482292
#! Citation Recommendation is useful for an author to find out the papers or books that can support the materials she is writing about. It is a challengeable problem since the vocabulary used in the content of papers and in the citation contexts are usually quite different. To address this problem, we propose to use translation model, which can bridge the gap between two heterogeneous languages. We conduct an experiment and find the translation model can provide much better candidates of citations than the state-of-the-art methods.

#index 1642169
#* Extracting adjective facets from community Q&A corpus
#@ Takehiro Yamamoto;Satoshi Nakamura;Katsumi Tanaka
#t 2011
#c 1
#% 290830
#% 348155
#% 1181094
#% 1185582
#% 1214645
#! In this paper, we propose a method for helping users explore information via Web searches by using a question and answer (Q&A) corpus archived in a community Q&A site. When users do not have clear information needs and have little knowledge about the task domain, it is difficult for them to create queries that adequately reflect their information needs. We focused on terms like "famous temples," "historical townscapes," and "delicious sweets," which we call "adjective facets", and developed a method of extracting these facets from question and answer archives at a community Q&A site. We evaluated the effectiveness of our adjective facets by comparing them with several baselines.

#index 1642170
#* A novel framework of training hidden markov support vector machines from lightly-annotated data
#@ Deyu Zhou;Yulan He
#t 2011
#c 1
#% 939377
#% 1330506
#% 1587361
#! Natural language understanding (NLU) aims to map sentences to their semantic mean representations. Statistical approaches to NLU normally require fully-annotated training data where each sentence is paired with its word-level semantic annotations. In this paper, we propose a novel learning framework which trains the Hidden Markov Support Vector Machines (HM-SVMs) without the use of expensive fully-annotated data. In particular, our learning approach takes as input a training set of sentences labeled with abstract semantic annotations encoding underlying embedded structural relations and automatically induces derivation rules that map sentences to their semantic meaning representations. The proposed approach has been tested on the DARPA Communicator Data and achieved 93.18% in F-measure, which outperforms the previously proposed approaches of training the hidden vector state model or conditional random fields from unaligned data, with a relative error reduction rate of 43.3% and 10.6% being achieved.

#index 1642171
#* Learning to recommend questions based on public interest
#@ Jun Wang;Xia Hu;Zhoujun Li;Wenhan Chao;Biyun Hu
#t 2011
#c 1
#% 268079
#% 340948
#% 1035587
#% 1166519
#% 1272053
#% 1292541
#% 1482384
#! This paper is concerned with the problem of question recommendation in the setting of Community Question Answering (CQA). Given a question as query, our goal is to rank all of the retrieved questions according to their likelihood of being good recommendations for the query. In this paper, we propose a notion of public interest, and show how public interest can boost the performance of question recommendation. In particular, to model public interest in question recommendation, we build a language model to combine relevance score to the query and popularity score regarding question popularity. Experimental results on Yahoo!Answers dataset demonstrate the performance of question recommendation can be greatly improved with considering the public interest.

#index 1642172
#* CQC: classifying questions in CQA websites
#@ Amit Singh;Karthik Visweswariah
#t 2011
#c 1
#% 309141
#% 344447
#% 815303
#% 881477
#% 1074110
#! Community Question Answering portals like Yahoo! Answers have recently become a popular method for seeking information online. Users express their information need as questions for which other users generate potential answers. These questions are organized into pre-defined hierarchical categories to facilitate effective answering, hence Question Classification is an important aspect of these systems. In this paper we propose a novel system, CQC, for automatically classifying new questions into one of the hierarchical categories. Experiments conducted on large scale real data from Yahoo Answers! show that the proposed techniques are effective and outperform existing methods significantly.

#index 1642173
#* Automatic query reformulation with syntactic operators to alleviate search difficulty
#@ Huizhong Duan;Rui Li;ChengXiang Zhai
#t 2011
#c 1
#% 397161
#% 577224
#% 987229
#% 1019183
#% 1074078
#% 1074098
#% 1130990
#% 1387547
#% 1482204
#! Modern search engines usually provide a query language with a set of advanced syntactic operators (e.g., plus sign to require a term's appearance, or quotation marks to require a phrase's appearance) which if used appropriately, can significantly improve the effectiveness of a plain keyword query. However, they are rarely used by ordinary users due to the intrinsic difficulties and users' lack of corpora statistics. In this paper, we propose to automatically reformulate queries that do not work well by selectively adding syntactic operators. Particularly, we propose to perform syntactic operator-based query reformulation when a retrieval system detects users encounter difficulty in search as indicated by users' behaviors such as scanning over top k documents without click-through. We frame the problem of automatic reformulation with syntactic operators as a supervised learning problem, and propose a set of effective features to represent queries with syntactic operators. Experiment results verify the effectiveness of the proposed method and its applicability as a query suggestion mechanism for search engines. As a negative feedback strategy, syntactic operator-based query reformulation also shows promising results in improving search results for difficult queries as compared with existing methods.

#index 1642174
#* Question routing in community question answering: putting category in its place
#@ Baichuan Li;Irwin King;Michael R. Lyu
#t 2011
#c 1
#% 838464
#% 1130900
#% 1190249
#% 1207005
#% 1292492
#% 1482384
#% 1491834
#! This paper investigates a ground-breaking incorporation of question category to Question Routing (QR) in Community Question Answering (CQA) services. The incorporation of question category was designed to estimate answerer expertise for routing questions to potential answerers. Two category-sensitive Language Models (LMs) were developed with large-scale real world data sets being experimented. Results demonstrated that higher accuracies of routing questions with lower computational costs were achieved, relative to traditional Query Likelihood LM (QLLM), state-of-the-art Cluster-Based LM (CBLM) and the mixture of Latent Dirichlet Allocation and QLLM (LDALM).

#index 1642175
#* Fact-based question decomposition for candidate answer re-ranking
#@ Aditya Kalyanpur;Siddharth Patwardhan;Branimir Boguraev;Adam Lally;Jennifer Chu-Carroll
#t 2011
#c 1
#% 926881
#% 938724
#% 1223497
#! Factoid questions often contain one or more assertions (facts) about their answers. However, existing question-answering (QA) systems have not investigated how the multiple facts may be leveraged to enhance system performance. We argue that decomposing complex factoid questions can benefit QA, as an answer candidate is more likely to be correct if multiple independent facts support it. We categorize decomposable questions as parallel or nested, depending on processing strategy required. We present a novel decomposition framework---for parallel and nested questions---which can be overlaid on top of traditional QA systems. It contains decomposition rules for identifying fact sub-questions, a question-rewriting component and a candidate re-ranker. In a particularly challenging domain for our baseline QA system, our framework shows a statistically significant improvement in end-to-end QA performance.

#index 1642176
#* CoDet: sentence-based containment detection in news corpora
#@ Emre Varol;Fazli Can;Cevdet Aykanat;Oguz Kaya
#t 2011
#c 1
#% 201935
#% 255137
#% 262102
#% 345087
#% 347225
#% 379532
#% 616528
#% 867125
#% 874972
#% 879617
#% 956507
#% 1074121
#% 1331586
#% 1383942
#% 1450881
#% 1450913
#% 1684713
#! We study a generalized version of the near-duplicate detection problem which concerns whether a document is a subset of another document. In text-based applications, document containment can be observed in exact-duplicates, near-duplicates, or containments, where the first two are special cases of the third. We introduce a novel method, called CoDet, which focuses particularly on this problem, and compare its performance with four well-known near-duplicate detection methods (DSC, full fingerprinting, I-Match, and SimHash) that are adapted to containment detection. Our method is expandable to different domains, and especially suitable for streaming news. Experimental results show that CoDet effectively and efficiently produces remarkable results in detecting containments.

#index 1642177
#* Smoothing NDCG metrics using tied scores
#@ Andrey Kustarev;Yury Ustinovsky;Yury Logachev;Evgeny Grechnikov;Ilya Segalovich;Pavel Serdyukov
#t 2011
#c 1
#% 983820
#% 1035577
#% 1211826
#% 1415743
#% 1442575
#! One of promising directions in research on learning to rank concerns the problem of appropriate choice of the objective function to maximize by means of machine learning algorithms. We describe a novel technique of smoothing an arbitrary ranking metric and demonstrate how to utilize it to maximize the retrieval quality in terms of the $NDCG$ metric. The idea behind our listwise ranking model called TieRank is artificial probabilistic tying of predicted relevance scores at each iteration of learning process, which defines a distribution on the set of all permutations of retrieved documents. Such distribution provides a desired smoothed version of the target retrieval quality metric. This smooth function is possible to maximize using a gradient descent method. Experiments on LETOR collections show that TieRank outperforms most of the existing learning to rank algorithms.

#index 1642178
#* Learning to rank with cross entropy
#@ Yuan Lin;Hongfei Lin;Jiajin Wu;Kan Xu
#t 2011
#c 1
#% 983820
#% 1268491
#% 1482385
#! Learning to rank algorithms are usually grouped into three types: the point wise approach, the pairwise approach, and the listwise approach, according to the input spaces. Much of the prior work is based on the three approaches to learn the ranking model to predict the relevance of a document to a query. In this paper, we focus on the problem of constructing new input space based on groups of documents with the same relevance judgment. A novel approach is proposed based on cross entropy to improve the existing ranking method. The experimental results show that our approach leads to significant improvements in retrieval effectiveness.

#index 1642179
#* Predicting document effectiveness in pseudo relevance feedback
#@ Mostafa Keikha;Jangwon Seo;W. Bruce Croft;Fabio Crestani
#t 2011
#c 1
#% 340901
#% 448194
#% 766429
#% 987230
#% 1074080
#% 1450860
#% 1450901
#! Pseudo relevance feedback (PRF) is one of effective practices in Information Retrieval. In particular, PRF via the relevance model (RM) has been widely used due to the theoretical soundness and effectiveness. In a PRF scenario, an underlying relevance model is inferred by combining language models of the top retrieved documents where the contribution of each document is assumed to be proportional to its score for the initial query. However, it is not clear that selecting the top retrieved documents only by the initial retrieval scores is actually the optimal way for query expansion. We show that the initial score of a document is not a good indicator of its effectiveness in query expansion. Our experiments show that if we can estimate the true effectiveness of the top retrieved documents, we can obtain almost 50% improvement over RM. Based on this observation, we introduce various document features that can be used to estimate the effectiveness of documents. Our experiments on the TREC Robust collection show that the proposed features make good predictors, and PRF using the effectiveness predictors can achieve statistically significant improvements over RM.

#index 1642180
#* Learning to rank categories for web queries
#@ Prashant V. Ullegaddi;Vasudeva Varma
#t 2011
#c 1
#% 458379
#% 853543
#% 879581
#% 987203
#% 1130910
#% 1227616
#% 1268491
#! In web search, understanding the user intent plays an important role in improving search experience of the end users. Such an intent can be represented by the categories which the user query belongs to. In this work, we propose an information retrieval based approach to query categorization with an emphasis on learning category rankings. To carry out categorization we first represent a category by web documents (from Open Directory Project) that describe the semantics of the category. Then, we learn the category rankings for the queries using 'learning to rank' techniques. To show that the results obtained are consistent and do not vary across datasets, we evaluate our approach on two datasets including the publicly available KDD Cup dataset. We report an overall improvement of 20% on all evaluation metrics (precision, recall and F-measure) over two baselines: a text categorization baseline and an unsupervised IR baseline.

#index 1642181
#* Supervised language modeling for temporal resolution of texts
#@ Abhimanu Kumar;Matthew Lease;Jason Baldridge
#t 2011
#c 1
#% 262096
#% 319244
#% 480467
#% 730070
#% 875959
#% 881498
#% 940042
#% 989587
#% 1002316
#% 1107069
#% 1264755
#% 1264790
#% 1292475
#% 1451248
#% 1481624
#% 1481659
#% 1495112
#% 1536521
#% 1592024
#% 1682064
#% 1692327
#! We investigate temporal resolution of documents, such as determining the date of publication of a story based on its text. We describe and evaluate a model that build histograms encoding the probability of different temporal periods for a document. We construct histograms based on the Kullback-Leibler Divergence between the language model for a test document and supervised language models for each interval. Initial results indicate this language modeling approach is effective for predicting the dates of publication of short stories, which contain few explicit mentions of years.

#index 1642182
#* Context-aware query recommendation by learning high-order relation in query logs
#@ Xiaohui Yan;Jiafeng Guo;Xueqi Cheng
#t 2011
#c 1
#% 310567
#% 330617
#% 728105
#% 1083721
#% 1712595
#! Query recommendation has been widely used in modern search engines. Recently, several context-aware methods have been proposed to improve the accuracy of recommendation by mining query sequence patterns from query sessions. However, the existing methods usually do not address the ambiguity of queries explicitly and often suffer from the sparsity of the training data. In this paper, we propose a novel context-aware query recommendation approach by modeling the high-order relation between queries and clicks in query log, which captures users' latent search intents. Empirical experiment results demonstrate that our approach outperforms the baseline methods in providing high quality recommendations for ambiguous queries.

#index 1642183
#* Efficient lp-norm multiple feature metric learning for image categorization
#@ Shuhui Wang;Qingming Huang;Shuqiang Jiang;Qi Tian
#t 2011
#c 1
#% 347225
#% 763697
#% 770846
#% 961190
#% 961279
#% 983830
#% 1073923
#% 1232015
#% 1292880
#! Previous metric learning approaches are only able to learn the metric based on single concatenated multivariate feature representation. However, for many real world problems with multiple feature representation such as image categorization, the model trained by previous approaches will degrade because of sparsity brought by significant dimension growth and uncontrolled influence from each feature channel. In this paper, we propose an efficient distance metric learning model which adapts Distance Metric Learning on multiple feature representations. The aim is to learn the Mahalanobis matrices for each independent feature and their non-sparse lp-norm weight coefficients simultaneously by maximizing the margin of the overall learned distance metric among the pairs from the same class and the distance of pairs from different classes. We further extend this method to nonlinear kernel learning and category specific metric learning, which demonstrate the applicability of using many existing kernels for image data and exploring the hierarchical semantic structures for large scale image datasets. Experiments on various datasets demonstrate the promising power of our method.

#index 1642184
#* Re-ranking by local re-scoring for video indexing and retrieval
#@ Bahjat Safadi;Georges Quénot
#t 2011
#c 1
#% 990300
#% 997095
#! Video retrieval can be done by ranking the samples according to their probability scores that were predicted by classifiers. It is often possible to improve the retrieval performance by re-ranking the samples. In this paper, we proposed a re-ranking method that improves the performance of semantic video indexing and retrieval, by re-evaluating the scores of the shots by the homogeneity and the nature of the video they belong to. Compared to previous works, the proposed method provides a framework for the re-ranking via the homogeneous distribution of video shots content in a temporal sequence. The experimental results showed that the proposed re-ranking method was able to improve the system performance by about 18% in average on the TRECVID 2010 semantic indexing task, videos collection with homogeneous contents. For TRECVID 2008, in the case of collections of videos with non-homogeneous contents, the system performance was improved by about 11-13%.

#index 1642185
#* Tightly coupling visual and linguistic features for enriching audio-based web browsing experience
#@ Muhammad Asiful Islam;Faisal Ahmed;Yevgen Borodin;I. V. Ramakrishnan
#t 2011
#c 1
#% 731938
#% 956496
#% 1006709
#% 1055709
#% 1077150
#% 1400129
#! People who are blind use screen readers for browsing web pages. Since screen readers read out content serially, a naive readout tends to mix irrelevant and relevant content thereby disrupting the coherency of the material being read out and confusing the listener. To address this problem we can partition web pages into coherent segments and narrate each such piece separately. Extant methods to do segmentation use visual and structural cues without taking the semantics into account and consequently create segments containing irrelevant material. In this paper, we describe a new technique for creating coherent segments by tightly coupling visual, structural, and linguistic features present in the content. A notable aspect of the technique is that it produces segments with little irrelevant content. Preliminary experiments indicate that the technique is effective in creating highly coherent segments and the experiences of an early adopter who is blind suggest that it enriches the overall browsing experience.

#index 1642186
#* Robust video fingerprinting based on hierarchical symmetric difference feature
#@ Jungho Lee;Seungjae Lee;Yongseok Seo;Wonyoung Yoo
#t 2011
#c 1
#% 251485
#% 522743
#% 710231
#% 1858257
#% 1858890
#! The piracy of copyrighted digital content over the Internet infringes copyrights and damages the digital content industry. Accordingly, identifying and monitoring technology on the online content service like fingerprinting is getting valuable through the explosion of digital content sharing. This paper proposes a robust video fingerprinting feature to identify a modified video clip from a large scale database. Hierarchical symmetric difference feature is proposed in order to offer efficient video fingerprinting. The feature is robust and pairwise independent against various video modifications such as compression, resizing, or cropping. Moreover, videos undergoing a transformation such as flipping or mirroring can be identified by simply disordering the bit pattern of fingerprints. The performance of the proposed feature is extensively experimented on 6,482 hours of database and the experimental results show that the proposed fingerprinting is efficient and robust against various modifications.

#index 1642187
#* Image clustering fusion technique based on BFS
#@ Luca Costantini;Raffaele Nicolussi
#t 2011
#c 1
#% 437405
#% 1326507
#% 1464041
#% 1854912
#% 1855310
#! With the increasing in number and size of databases dedicated to the storage of visual content, the need for effective retrieval systems has become crucial. The proposed method makes a significant contribution to meet this need through a technique in which sets of clusters are fused together to create an unique and more significant set of clusters. The images are represented by some features and then are grouped by these features, that are considered one by one. A probability matrix is then built and explored by the breadth first search algorithm with the aim of select an unique set of clusters. Experimental results, obtained using two different datasets, show the effectiveness of the proposed technique. Furthermore, the proposed approach overcomes the drawback of tuning a set of parameters that fuse the similarity measurement obtained by each feature to get an overall similarity between two images.

#index 1642188
#* Efficient retrieval of 3D building models using embeddings of attributed subgraphs
#@ Raoul Wessel;Sebastian Ochmann;Richard Vock;Ina Blümel;Reinhard Klein
#t 2011
#c 1
#% 368615
#% 881526
#% 1196948
#% 1328111
#% 1356669
#% 1511215
#% 1674683
#! We present a novel method for retrieval and classification of 3D building models that is tailored to the specific requirements of architects. In contrast to common approaches our algorithm relies on the interior spatial arrangement of rooms instead of exterior geometric shape. We first represent the internal topological building structure by a Room Connectivity Graph (RCG). To enable fast and efficient retrieval and classification with RCGs, we transform the structured graph representation into a vector-based one by introducing a new concept of subgraph embeddings. We provide comprehensive experiments showing that the introduced subgraph embeddings yield superior performance compared to state-of-the-art graph retrieval approaches.

#index 1642189
#* Constructing seminal paper genealogy
#@ Duck-Ho Bae;Se-Mi Hwang;Sang-Wook Kim;Christos Faloutsos
#t 2011
#c 1
#% 290830
#% 805896
#% 818916
#% 967278
#% 983833
#% 1328067
#% 1400120
#! When a researcher starts with a new topic, it would be very useful if seminal papers in the topic and their relationships are provided in advance. We propose an approach to construct seminal paper genealogy and show the effectiveness and efficiency of our approach.

#index 1642190
#* Leveraging Wikipedia concept and category information to enhance contextual advertising
#@ Zongda Wu;Guandong Xu;Rong Pan;Yanchun Zhang;Zhiwen Hu;Jianfeng Lu
#t 2011
#c 1
#% 818265
#% 879633
#% 1019092
#% 1040857
#% 1051059
#% 1074073
#% 1190102
#% 1195847
#% 1212636
#% 1214660
#% 1433962
#% 1442564
#! As a prevalent type of Web advertising, contextual advertising refers to the placement of the most relevant ads into a Web page, so as to increase the number of ad-clicks. However, some problems of homonymy and polysemy, low intersection of keywords etc., can lead to the selection of irrelevant ads for a page. In this paper, we present a new contextual advertising approach to overcome the problems, which uses Wikipedia concept and category information to enrich the content representation of an ad (or a page). First, we map each ad and page into a keyword vector, a concept vector and a category vector. Next, we select the relevant ads for a given page based on a similarity metric that combines the above three feature vectors together. Last, we evaluate our approach by using real ads, pages, as well as a great number of concepts and categories of Wikipedia. Experimental results show that our approach can improve the precision of ads-selection effectively.

#index 1642191
#* Beyond relevance in marketplace search
#@ Nish Parikh;Neel Sundaresan
#t 2011
#c 1
#% 262112
#% 1130842
#% 1166473
#% 1206662
#% 1560251
#! In this paper we study diversity and its relations to search relevance in the context of an online marketplace. We conduct a large-scale log-based study using click-stream data from a leading eCommerce site. We introduce 3 main metrics -- selection (diversity), trust, and value. In our analysis we also show how these interact with relevance in different ways. We study the benefits of diversity and also show why guaranteeing diversity is important.

#index 1642192
#* Relative effect of spam and irrelevant documents on user interaction with search engines
#@ Timothy Jones;David Hawking;Paul Thomas;Ramesh Sankaranarayana
#t 2011
#c 1
#% 1125900
#% 1331572
#% 1432789
#% 1455267
#% 1565812
#% 1598454
#! Meaningful evaluation of web search must take account of spam. Here we conduct a user experiment to investigate whether satisfaction with search engine result pages as a whole is harmed more by spam or by irrelevant documents. On some measures, search result pages are differentially harmed by the insertion of spam and irrelevant documents. Additionally we find that when users are given two documents of equal utility, the one with the lower spam score will be preferred; a result page without any spam documents will be preferred to one with spam; and an irrelevant document high in a result list is surprisingly more damaging to user satisfaction than a spam document. We conclude that web ranking and evaluation should consider both utility (relevance) and "spamminess" of documents.

#index 1642193
#* Inferring query aspects from reformulations using clustering
#@ Van Dang;Xiaobing Xue;W. Bruce Croft
#t 2011
#c 1
#% 262112
#% 340901
#% 642975
#% 643069
#% 869501
#% 879618
#% 879686
#% 1166473
#% 1227604
#% 1292596
#% 1355020
#% 1400021
#% 1400099
#! When the information need is not clear from the user query, a good strategy would be to return documents that cover as many aspects of the query as possible. To do this, the possible aspects of the query need to be automatically identified. In this paper, we propose to do this by clustering reformulated queries generated from publicly available resources and using each cluster to represent an aspect of the query. Our results show that the automatically generated reformulations for the TREC Web Track queries match up quite well with actual sub-topics of these queries identified by TREC experts. Moreover, agglomerative clustering using query-to-query similarity based on co-occurrence in text passages can provide clusters of high quality that potentially can be used to identify aspects.

#index 1642194
#* Advertiser-centric approach to understand user click behavior in sponsored search
#@ Sungchul Kim;Tao Qin;Hwanjo Yu;Tie-Yan Liu
#t 2011
#c 1
#% 818265
#% 956546
#% 1214728
#% 1227651
#% 1450834
#% 1450842
#% 1456843
#% 1810385
#! Sponsored search is the major business model of commercial search engines. The number of clicks on ads is a key indicator of success for both advertisers and search engines, and increasing ad clicks is a goal of both of them. Many existing works stand on the view of search engines concerning how to help search engines to earn more revenue by accurately predicting ad clicks. Unlike the existing works, this paper aims at understanding user clicks on ads from "the view of advertisers", in order to help advertisers to improve their ad quality and therefore advertising effectiveness. To do this, a factor graph model is proposed, which considers two advertiser-controllable factors to understand user click behaviors: the relevance between a query and an ad, which has been well studied in previous literatures, and the "attractiveness" of the ad, which is a newly-proposed concept. The proposed model can be used to predict user clicks and also to mine a set of attractive words that could be leveraged to improve the quality of the ads. We have verified the effectiveness of the proposed approach using real-world datasets, through quantitative evaluations and informative case studies.

#index 1642195
#* Supervised matching of comments with news article segments
#@ Dyut Kumar Sil;Srinivasan H. Sengamedu;Chiranjib Bhattacharyya
#t 2011
#c 1
#% 1275012
#% 1560208
#% 1602946
#! Comments constitute an important part of Web 2.0. In this paper, we consider comments on news articles. To simplify the task of relating the comment content to the article content the comments are about, we propose the idea of showing comments alongside article segments and explore automatic mapping of comments to article segments. This task is challenging because of the vocabulary mismatch between the articles and the comments. We present supervised and unsupervised techniques for aligning comments to segments the of article the comments are about. More specifically, we provide a novel formulation of supervised alignment problem using the framework of structured classification. Our experimental results show that structured classification model performs better than unsupervised matching and binary classification model.

#index 1642196
#* User action interpretation for personalized content optimization in recommender systems
#@ Anlei Dong;Jiang Bian;Xiaofeng He;Srihari Reddy;Yi Chang
#t 2011
#c 1
#% 1190057
#! User interaction plays a vital role in recommender systems. Previous studies on algorithmic recommender systems have mainly focused on modeling techniques and feature development. Traditionally, implicit user feedback or explicit user ratings on the recommended items form the basis for designing and training of recommendation algorithms. But user interactions in real-world Web applications (e.g., a portal website with different recommendation modules in the interface) are unlikely to be as ideal as those assumed by previously proposed models. To address this problem, we build an online learning framework for personalized recommendation. We argue that appropriate user action interpretation is critical for a recommender system. The main contribution in this paper is an approach of interpreting users' actions for the online learning to achieve better item relevance estimation. Our experiments on the large-scale data from a commercial Web recommender system demonstrate significant improvement in terms of a precision metric over the baseline model that does not incorporate user action interpretation. The efficacy of this new algorithm is also proved by the online test results on real user traffic.

#index 1642197
#* A personalized recommendation system on scholarly publications
#@ Maria Soledad Pera;Yiu-Kai Ng
#t 2011
#c 1
#% 232703
#% 1127486
#% 1227622
#% 1271961
#% 1412119
#% 1476448
#% 1480499
#% 1536533
#% 1693254
#% 1755308
#! Researchers, as well as ordinary users who seek information in diverse academic fields, turn to the web to search for publications of interest. Even though scholarly publication recommenders have been developed to facilitate the task of discovering literature pertinent to their users, they (i) are not personalized enough to meet users' expectations, since they provide the same suggestions to users sharing similar profiles/preferences, (ii) generate recommendations pertaining to each user's general interests as opposed to the specific need of the user, and (iii) fail to take full advantages of valuable user-generated data at social websites that can enhance their performance. To address these problems, we propose PubRec, a recommender that suggests closely-related references to a particular publication P tailored to a specific user U, which minimizes the time and efforts imposed on U in browsing through general recommended publications. Empirical studies conducted using data extracted from CiteULike (i) verify the efficiency of the recommendation and ranking strategies adopted by PubRec and (ii) show that PubRec significantly outperforms other baseline recommenders.

#index 1642198
#* Collaborative exploratory search in real-world context
#@ Naoki Tani;Danushka Bollegala;Naiwala Chandrasiri;Keisuke Okamoto;Kazunari Nawa;Shuhei Iitsuka;Yutaka Matsuo
#t 2011
#c 1
#% 246877
#% 450041
#% 722904
#% 788094
#% 848003
#% 857478
#% 1047489
#% 1047490
#% 1328070
#% 1399949
#% 1399987
#% 1403461
#! We propose Collaborative Exploratory Search (CES), which is an integration of dialog analysis and web search that involves multiparty collaboration to accomplish an exploratory information retrieval goal. Given a real-time dialog between users on a single topic; we define CES as the task of automatically detecting the topic of the dialog and retrieving task-relevant web pages to support the dialog. To recognize the task of the dialog, we apply the Author--Topic model as a topic model. Then, attribute extraction is applied to the dialog to obtain the attributes of the tasks. Finally, a specific search query is generated to identify the task-relevant information. We implement and evaluate the CES system for a commercial in-vehicle conversation. We also develop an iPad application that listens to conversations among users and continuously retrieves relevant web pages. Our experimental results reveal that the proposed method outperforms existing methods, which demonstrates the potential usefulness of collaborative exploratory search with practically usable accuracy levels.

#index 1642199
#* Beyond precision@10: clustering the long tail of web search results
#@ Benno Stein;Tim Gollub;Dennis Hoppe
#t 2011
#c 1
#% 281186
#% 534271
#% 642975
#% 754124
#% 807295
#% 807363
#% 857482
#% 1130878
#% 1166473
#% 1195369
#% 1202162
#% 1280747
#% 1280751
#% 1450829
#% 1450850
#% 1467778
#% 1481547
#% 1482231
#% 1532581
#% 1560378
#% 1682423
#! The paper addresses the missing user acceptance of web search result clustering. We report on selected analyses and propose new concepts to improve existing result clustering approaches. Our findings in a nutshell are: 1. Don't compete with a search engine's top hits. In response to a query we presume search engines to return an optimal result list in the sense of the probabilistic ranking principle: documents that are expected by the majority of users are placed on top and form the result list head. We argue that, with respect to the top results, it is not beneficial to replace this established form of result presentation. 2. Improve document access in the result list tail. Documents that address the information need of "minorities" appear at some position in the result list tail. Especially for ambiguous and multi-faceted queries we expect this tail to be long, with many users appreciating different documents. In this situation web search result clustering can improve user satisfaction by reorganizing the long tail into topic-specific clusters. 3. Avoid shadowing when constructing cluster labels. We show that most of the cluster labels that are generated by current clustering technology occur within the snippets of the result list head--an effect which we call shadowing. The value of such labels for topic organization and navigating within a clustering of the entire result list is limited. We propose and analyze a filtering approach to significantly alleviate the label shadowing effect.

#index 1642200
#* Spectral analysis of a blogosphere
#@ Sang-Wook Kim;Ki-Nam Kim;Christos Faloutsos;Joon-Ho Lee
#t 2011
#c 1
#% 227924
#% 937549
#% 1206918
#% 1292639
#% 1296930
#% 1300087
#% 1366208
#% 1366209
#! A blogosphere is a representative example of online social networks. In this paper, we address spectral analysis of a blogosphere. We model a real-world blogosphere as a matrix and a tensor, and then analyze it by using the SVD and PARAFAC decomposition. According to the results, the SVD successfully identified communities, each of which focuses on a specific topic, and also found hub blogs and authoritative posts within each community. The PARAFAC decomposition also succeeded in extracting more communities of finer granules than the SVD. Also, the PARAFAC decomposition could identify the dominant keywords in addition to the hub blogs and authoritative posts honored in each community.

#index 1642201
#* Citation chain aggregation: an interaction model to support citation cycling
#@ Timothy F. Cribbin
#t 2011
#c 1
#% 56829
#% 60635
#% 137475
#% 201993
#% 247507
#% 1408402
#% 1472922
#! Citation chaining is a powerful means of exploring the academic literature. Starting from just one or two known relevant items, a naïve researcher can cycle backwards and forwards through the citation graph to generate a rich overview of key works, authors and journals relating to their topic. Whilst online citation indexes greatly facilitate this process, the size and complexity of the search space can rapidly escalate. In this paper, we propose a novel interaction model called citation chain aggregation (CCA). CCA employs a simple three-list view which highlights the overlaps that occur between the first-generation relations of known relevant items. As more relevant articles are identified, differences in the frequencies of citations made by or to unseen articles provide strong relevance feedback cues. The benefits of this technique are illustrated using a simple case study.

#index 1642202
#* Collaborative blacklist generation via searches-and-clicks
#@ Lung-Hao Lee;Hsin-Hsi Chen
#t 2011
#c 1
#% 306468
#% 743632
#% 766447
#% 845226
#% 954849
#% 961629
#% 968332
#% 1055885
#% 1076648
#% 1279843
#% 1400127
#% 1598477
#% 1775427
#! This paper presents an intent conformity model to collaboratively generate blacklists for cyberporn filtering. A novel porn detection framework via searches-and-clicks is proposed to explore collective intelligence embedded in query logs. Firstly, the clicked pages are represented in terms of the weighted queries to reflect the degrees related to pornography. Consequently, these weighted queries are regarded as discriminative features to calculate the pornography indicator by an inverse chi-square method for candidate determination. Finally, a candidate whose URL contains at least one pornographic keyword is included in our collaborative blacklists. The experiments on a MSN porn data set indicate that the generated blacklist achieves a high precision, while maintaining a favorably low false positive rate. In addition, real-life filtering simulations reveal that our blacklist is more effective than some publicly released blacklists.

#index 1642203
#* Attention prediction on social media brand pages
#@ Himabindu Lakkaraju;Jitendra Ajmera
#t 2011
#c 1
#% 722904
#% 963349
#% 1270702
#% 1399995
#% 1512437
#% 1517940
#! In this paper, we deal with the problem of predicting how much attention a newly submitted post would receive from fellow community members of closed communities in social networking sites. Though the concept of attention is subjective, the number of comments received by a post serves as a very good indicator of the same. Unlike previous work which primarily made use of either content features or the network features (friendship links on the network), we exploit both the content features and community level features (for instance, what time of the day is the community more active) for tackling this problem. Further, we focus on dedicated pages of corporate brands on social media websites and accordingly extract important features from the content and community activity of such brand pages. The attention prediction task finds direct application in the listening, monitoring and engaging activities of the businesses that have such brand-pages. In this paper, we formulate the problem of attention prediction on social media brand pages. We further propose Attention Prediction (AP) framework which integrates the various features that influence the attention received by a post using classification and regression based approaches. Experimental results on real world data extracted from some highly active brand pages on Facebook demonstrate the efficacy of the proposed framework.

#index 1642204
#* Do they belong to the same class: active learning by querying pairwise label homogeneity
#@ Yifan Fu;Bin Li;Xingquan Zhu;Chengqi Zhang
#t 2011
#c 1
#% 770851
#% 1074125
#% 1083692
#% 1211801
#% 1264829
#% 1272282
#% 1424132
#! Traditional active learning methods request experts to provide ground truths to the queried instances, which can be expensive in practice. An alternative solution is to ask nonexpert labelers to do such labeling work, which can not tell the definite class labels. In this paper, we propose a new active learning paradigm, in which a nonexpert labeler is only asked "whether a pair of instances belong to the same class". To instantiate the proposed paradigm, we adopt the MinCut algorithm as the base classifier. We first construct a graph based on the pairwise distance of all the labeled and unlabeled instances and then repeatedly update the unlabeled edge weights on the max-flow paths in the graph. Finally, we select an unlabeled subset of nodes with the highest prediction confidence as the labeled data, which are included into the labeled data set to learn a new classifier for the next round of active learning. The experimental results and comparisons, with state-of-the-art methods, demonstrate that our active learning paradigm can result in good performance with nonexpert labelers.

#index 1642205
#* Structured data classification by means of matrix factorization
#@ Paolo Garza
#t 2011
#c 1
#% 92533
#% 136350
#% 246832
#% 1034722
#% 1169065
#% 1260273
#% 1301004
#% 1558464
#! Singular Value Decomposition (SVD) has been extensively used in the classification context as a preprocessing step aiming to reduce the number of features of the input space. Traditional classification algorithms are then applied on the new space to generate accurate models. In this paper, we propose a different use of SVD. In our approach SVD is the building block of a new classification algorithm, called CMF, and not that of a feature reduction algorithm. In particular, we propose a new classification algorithm where the classification model corresponds to the k largest right singular vectors of the factorization of the training dataset obtained by applying SVD. The selected singular vectors allows representing the main "characteristics" of the training data and can be used to provide accurate predictions. The experiments performed on 15 structured UCI datasets show that CMF is efficient and, despite its simplicity, it is more accurate than many state of the art classification algorithms.

#index 1642206
#* Transfer active learning
#@ Zhenfeng Zhu;Xingquan Zhu;Yangdong Ye;Yue-Fei Guo;Xiangyang Xue
#t 2011
#c 1
#% 236497
#% 342598
#% 983828
#% 1464068
#% 1867998
#! Active learning traditionally assumes that labeled and unlabeled samples are subject to the same distributions and the goal of an active learner is to label the most informative unlabeled samples. In reality, situations may exist that we may not have unlabeled samples from the same domain as the labeled samples (i.e. target domain), whereas samples from auxiliary domains might be available. Under such situations, an interesting question is whether an active learner can actively label samples from auxiliary domains to benefit the target domain. In this paper, we propose a transfer active learning method, namely Transfer Active SVM (TrAcSVM), which uses a limited number of target instances to iteratively discover and label informative auxiliary instances. TrAcSVM employs an extended sigmoid function as instance weight updating approach to adjust the models for prediction of (newly arrived) target data. Experimental results on real-world data sets demonstrate that TrAcSVM obtains better efficiency and prediction accuracy than its peers.

#index 1642207
#* A probabilistic approach to nearest-neighbor classification: naive hubness bayesian kNN
#@ Nenad Tomasev;Miloa Radovanović;Dunja Mladenić;Mirjana Ivanović
#t 2011
#c 1
#% 982755
#% 988163
#% 1169642
#% 1215932
#% 1384065
#% 1551185
#% 1617051
#! Most machine-learning tasks, including classification, involve dealing with high-dimensional data. It was recently shown that the phenomenon of hubness, inherent to high-dimensional data, can be exploited to improve methods based on nearest neighbors (NNs). Hubness refers to the emergence of points (hubs) that appear among the k NNs of many other points in the data, and constitute influential points for kNN classification. In this paper, we present a new probabilistic approach to kNN classification, naive hubness Bayesian k-nearest neighbor (NHBNN), which employs hubness for computing class likelihood estimates. Experiments show that NHBNN compares favorably to different variants of the kNN classifier, including probabilistic kNN (PNN) which is often used as an underlying probabilistic framework for NN classification, signifying that NHBNN is a promising alternative framework for developing probabilistic NN algorithms.

#index 1642208
#* Representing document as dependency graph for document clustering
#@ Yujing Wang;Xiaochuan Ni;Jian-Tao Sun;Yunhai Tong;Zheng Chen
#t 2011
#c 1
#% 143306
#% 262045
#% 289053
#% 321635
#% 329531
#% 406493
#% 643008
#% 729911
#% 730050
#% 757329
#% 766432
#% 766434
#% 772845
#% 817472
#% 881552
#% 956505
#% 989584
#% 995140
#% 1083702
#% 1214660
#% 1451216
#% 1835483
#! In traditional clustering methods, a document is often represented as "bag of words" (in BOW model) or n-grams (in suffix tree document model) without considering the natural language relationships between the words. In this paper, we propose a novel approach DGDC (Dependency Graph-based Document Clustering algorithm) to address this issue. In our algorithm, each document is represented as a dependency graph where the nodes correspond to words which can be seen as meta-descriptions of the document; whereas the edges stand for the relations between pairs of words. A new similarity measure is proposed to compute the pairwise similarity of documents based on their corresponding dependency graphs. By applying the new similarity measure in the Group-average Agglomerative Hierarchial Clustering (GAHC) algorithm, the final clusters of documents can be obtained. The experiments were carried out on five public document datasets. The empirical results have indicated that the DGDC algorithm can achieve better performance in document clustering tasks compared with other approaches based on the BOW model and suffix tree document model.

#index 1642209
#* Finding redundant and complementary communities in multidimensional networks
#@ Michele Berlingerio;Michele Coscia;Fosca Giannotti
#t 2011
#c 1
#% 1675858
#! Community Discovery in networks is the problem of detecting, for each node, its membership to one of more groups of nodes, the communities, that are densely connected, or highly interactive. We define the community discovery problem in multidimensional networks, where more than one connection may reside between any two nodes. We also introduce two measures able to characterize the communities found. Our experiments on real world multidimensional networks support the methodology proposed in this paper, and open the way for a new class of algorithms, aimed at capturing the multifaceted complexity of connections among nodes in a network.

#index 1642210
#* Promotional subspace mining with EProbe framework
#@ Yan Zhang;Yiyu Jia;Wei Jin
#t 2011
#c 1
#% 1292475
#% 1328118
#% 1663626
#! In multidimensional data, Promotional Subspace Mining (PSM) aims to find out outstanding subspaces for a given object, and to discover meaningful rules from them. In PSM, one major research issue is to produce top subspaces efficiently given a predefined subspace ranking measure. A common approach is to achieve an exact solution, which searches through the entire subspace search space and evaluate the target object's rank in every subspace, assisted with possible pruning strategies. In this paper, we propose EProbe, an Efficient Subspace Probing framework. This novel framework strives to initialize the idea of "early stop" of the top subspace search process. The essential goal is to provide a scalable, cost-effective, and flexible solution where its accuracy can be traded with the efficiency using adjustable parameters. This framework is especially useful when the computation resources are insufficient and only a limited number of candidate subspaces can be evaluated. As a first attempt to seek solutions under EProbe framework, we propose two novel algorithms SRatio and SlidingCluster. In our experiments, we illustrate that these two algorithms could produce a more effective subspace traversal order. Being effective, the top-k subspaces included in the final results are shown to be evaluated in the early stage of the subspace traversal process.

#index 1642211
#* A partitioning method for symbolic interval data based on kernelized metric
#@ Bruno Pimentel;Anderson Costa;Renata Souza
#t 2011
#c 1
#% 296738
#% 729437
#% 769935
#% 772216
#% 906637
#% 992320
#% 1046215
#! To solve the problem of situations with nonlinearly separable clusters, kernel clustering methods have been proposed. Symbolic Data Analysis (SDA) has emerged to deal with variables that can have intervals, histograms, and even functions as values, in order to consider the variability and/or uncertainty innate to the data. In this paper, we present a K-means clustering method based in kernelized squared L2 distance for symbolic interval-type data. Experiments with real and syntectic symbolic interval-type data sets are considered.

#index 1642212
#* Hierarchy evolution for improved classification
#@ Xiaoguang Qi;Brian D. Davison
#t 2011
#c 1
#% 309141
#% 829975
#% 881494
#% 996818
#% 1021186
#% 1227578
#% 1558464
#! Hierarchical classification has been shown to have superior performance than flat classification. It is typically performed on hierarchies created by and for humans rather than for classification performance. As a result, classification based on such hierarchies often yields suboptimal results. In this paper, we propose a novel genetic algorithm-based method on hierarchy adaptation for improved classification. Our approach customizes the typical GA to optimize classification hierarchies. In several text classification tasks, our approach produced hierarchies that significantly improved upon the accuracy of the original hierarchy as well as hierarchies generated by state-of-the-art methods.

#index 1642213
#* Using random walks for multi-label classification
#@ Chaokun Wang;Wei Zheng;Zhang Liu;Yiyuan Bai;Jianmin Wang
#t 2011
#c 1
#% 950571
#% 1095861
#% 1100077
#% 1117042
#% 1176915
#! The Multi-Label Classification (MLC) problem has aroused wide concern in these years since the multi-labeled data appears in many applications, such as page categorization, tag recommendation, mining of semantic web data, social network analysis, and so forth. In this paper, we propose a novel MLC solution based on the random walk model, called MLRW. MLRW maps the multi-labeled instances to graphs, on which the random walk is applied. When an unlabeled data is fed, MLRW transforms the original multi-label problem to some single-label subproblems. Experimental results on several real-world data sets demonstrate that MLRW is a better solution to the MLC problems than many other existing multi-label classification methods.

#index 1642214
#* Latent feature encoding using dyadic and relational data
#@ Shin Ando
#t 2011
#c 1
#% 643008
#% 722904
#% 1117093
#% 1130899
#% 1211703
#% 1211773
#% 1214627
#! Learning from dyadic and relational data is a fundamental problem for IR and KDD applications in web and social media domain. Basic behaviors and characteristics of users and documents are typically described by a collection of dyads, i.e., pairs of entities. Discriminative features extracted from such data are essential in exploratory and discriminatory analyses. Relational properties of the entities reflect pair-wise similarities and their collective community structure which are also valuable for discriminative learning. A challenging aspect of learning from the relational data in many domains, is that the generative process of relational links appears noisy and is not well described by a stochastic model. In this paper, we present a principled approach for learning discriminative features from heterogeneous sources of dyadic and relational data. We propose an information-theoretic framework called Latent Feature Encoding (LFE) which projects the entities and the links to a latent feature space in the analogy of -encoding. Projection is formalized as a maximization of the mutual information preserved in the latent features, regularized by the compression rate of encoding. The regularization is emphasized over more probable links to account for the noisiness of the observation. An empirical evaluation of the proposed method using text and social media datasets is presented. Performances in supervised and unsupervised learning tasks are compared with those of conventional latent feature extraction methods.

#index 1642215
#* Learning kernels with upper bounds of leave-one-out error
#@ Yong Liu;Shizhong Liao;Yuexian Hou
#t 2011
#c 1
#% 197394
#% 425040
#% 763697
#% 1545594
#! We propose a new leaning method for Multiple Kernel Learning (MKL) based on the upper bounds of the leave-one-out error that is an almost unbiased estimate of the expected generalization error. Specifically, we first present two new formulations for MKL by minimizing the upper bounds of the leave-one-out error. Then, we compute the derivatives of these bounds and design an efficient iterative algorithm for solving these formulations. Experimental results show that the proposed method gives better accuracy results than that of both SVM with the uniform combination of basis kernels and other state-of-art kernel learning approaches.

#index 1642216
#* KLEAP: an efficient cleaning method to remove cross-reads in RFID streams
#@ Guoqiong Liao;Jing Li;Lei Chen;Changxuan Wan
#t 2011
#c 1
#% 893102
#% 1016228
#% 1206747
#% 1426506
#! Recently, the RFID technology has been widely used in many kinds of applications. However, because of the interference from environmental factors and limitations of the radio frequency technology, the data streams collected by the RFID readers are usually contain a lot of cross-reads. To address this issue, we propose a KerneL dEnsity-bAsed Probability cleaning method (KLEAP) to remove cross-reads within a sliding window. The method estimates the density of each tag using a kernel-based function. The reader corresponding to the micro-cluster with the largest density will be regarded as the position that the tagged object should locate in current window, and the readings derived from other readers will be treated as the cross-reads. Experiments verify the effectiveness and efficiency of the proposed method.

#index 1642217
#* A diversity measure leveraging domain specific auxiliary information
#@ Narayan Bhamidipati;Nagaraj Kota
#t 2011
#c 1
#% 115608
#% 284557
#% 413618
#% 770826
#% 1055738
#! This article deals with the notion of reduction in uncertainty when the probability mass is distributed over similar values than dissimilar values. Shannon's entropy is a frequently used information theoretic measure of the uncertainty associated with random variables, but it depends solely on the set of values the probability mass function assumes, and does not take into consideration whether the mass is distributed among extreme values or not. A similarity structure, possibly obtained through domain knowledge, on the values assumed by the random variable may reduce the associated uncertainty. More the similarity, less the uncertainty. A novel measure named Similarity Adjusted Entropy (or Sim-adjusted Entropy for short), that generalizes Shannon's entropy, is then proposed to capture the effects of this similarity structure. Sim-adjusted entropy provides a mechanism for incorporating the domain expertise into an entropy based framework for solving various data mining tasks. Applications highlighted in this manuscript include clustering of categorical data and measuring audience diversity. Experiments performed on Yahoo! Answers data set demonstrate the ability of the proposed method to obtain more cohesive clusters. Another set of experiments confirm the utility of the proposed measure for measuring audience diversity.

#index 1642218
#* Mining query structure from click data: a case study of product queries
#@ Julia Kiseleva;Eugene Agichtein;Daniel Billsus
#t 2011
#c 1
#% 729913
#% 1074093
#% 1227648
#% 1265094
#% 1400079
#% 1450888
#! Most of the information on the Web is inherently structured, product pages of large online shopping sites such as Amazon.com being a typical example. Yet, unstructured keyword queries are still the most common way to search for such structured information, producing an ambiguities and poor ranking, and by that degrading user experience. This problem can be resolved by query segmentation, that is, transformation of unstructured keyword queries into structured queries. The resulting queries can be used to search product databases more accurately, and improve result presentation and query suggestion. The main contribution of our work is a novel approach to query segmentation based on unsupervised machine learning. Its highlight is that query and click-through logs are used for training. Extensive experiments over a large query and click log from a leading shopping engine demonstrate that our approach significantly outperforms baseline.

#index 1642219
#* Towards expert finding by leveraging relevant categories in authority ranking
#@ Hengshu Zhu;Huanhuan Cao;Hui Xiong;Enhong Chen;Jilei Tian
#t 2011
#c 1
#% 722904
#% 879576
#% 956516
#% 1074111
#% 1535370
#! How to improve authority ranking is a crucial research problem for expert finding. In this paper, we propose a novel framework for expert finding based on the authority information in the target category as well as the relevant categories. First, we develop a scalable method for measuring the relevancy between categories through topic models. Then, we provide a link analysis approach for ranking user authority by considering the information in both the target category and the relevant categories. Finally, the extensive experiments on two large-scale real-world Q&A data sets clearly show that the proposed method outperforms the baseline methods with a significant margin.

#index 1642220
#* Joint inference for cross-document information extraction
#@ Qi Li;Sam Anzaroot;Wen-Pin Lin;Xiang Li;Heng Ji
#t 2011
#c 1
#% 881561
#% 939856
#% 1251693
#% 1264790
#% 1310477
#% 1470685
#% 1471259
#% 1592041
#% 1624766
#! Previous information extraction (IE) systems are typically organized as a pipeline architecture of separated stages which make independent local decisions. When the data grows beyond some certain size, the extracted facts become inter-dependent and thus we can take advantage of information redundancy to conduct reasoning across documents and improve the performance of IE. We describe a joint inference approach based on information network structure to conduct cross-fact reasoning with an integer linear programming framework. Without using any additional labeled data this new method obtained 13.7%-24.4% user browsing cost reduction over a state-of-the-art IE system which extracts various types of facts independently.

#index 1642221
#* Building a generic debugger for information extraction pipelines
#@ Anish Das Sarma;Alpa Jain;Philip Bohannon
#t 2011
#c 1
#% 301241
#% 893167
#% 939515
#% 1063547
#% 1127409
#% 1174746
#% 1206799
#% 1206986
#% 1231247
#% 1250378
#% 1292670
#% 1366498
#% 1426568
#! Complex information extraction (IE) pipelines are becoming an integral component of most text processing frameworks. We introduce a first system to help IE users analyze extraction pipeline semantics and operator transformations interactively while debugging. This allows the effort to be proportional to the need, and to focus on the portions of the pipeline under the greatest suspicion. We present a generic debugger for running post-execution analysis of any IE pipeline consisting of arbitrary types of operators. For this, we propose an effective provenance model for IE pipelines which captures a variety of operator types, ranging from those for which full to no specifications are available. We have evaluated our proposed algorithms and provenance model on large-scale real-world extraction pipelines.

#index 1642222
#* Fast supervised feature extraction by term discrimination information pooling
#@ Amara Tariq;Asim Karim
#t 2011
#c 1
#% 803769
#% 1136450
#% 1176922
#% 1249577
#% 1275285
#% 1473931
#! Dimensionality reduction (DR) through feature extraction (FE) is desirable for efficient and effective processing of text documents. Many of the techniques for text FE produce features that are not readily interpretable and require super-linear computation time. In this paper, we present a fast supervised DR/FE technique, named FEDIP, that is motivated by the notion of relatedness of terms to topics or contexts. This relatedness is quantified by using the discrimination information provided by a term for a topic in a labeled document collection. Features are constructed by pooling the discrimination information of highly related terms for each topic. FEDIP's time complexity is linear in the size of the vocabulary and document collection. FEDIP is evaluated for document classification with SVM and naive Bayes classifiers on six text data sets. The results show that FEDIP produces low-dimension feature spaces that yield higher classification accuracy when compared with LDA and LSI. FEDIP is also found to be significantly faster than the other techniques on our evaluation data sets.

#index 1642223
#* Constructing efficient information extraction pipelines
#@ Henning Wachsmuth;Benno Stein;Gregor Engels
#t 2011
#c 1
#% 815821
#% 939924
#% 1130931
#% 1249541
#% 1330545
#% 1482179
#% 1484251
#% 1484367
#% 1527301
#% 1543775
#! Information Extraction (IE) pipelines analyze text through several stages. The pipeline's algorithms determine both its effectiveness and its run-time efficiency. In real-world tasks, however, IE pipelines often fail acceptable run-times because they analyze too much task-irrelevant text. This raises two interesting questions: 1) How much "efficiency potential" depends on the scheduling of a pipeline's algorithms? 2) Is it possible to devise a reliable method to construct efficient IE pipelines? Both questions are addressed in this paper. In particular, we show how to optimize the run-time efficiency of IE pipelines under a given set of algorithms. We evaluate pipelines for three algorithm sets on an industrially relevant task: the extraction of market forecasts from news articles. Using a system-independent measure, we demonstrate that efficiency gains of up to one order of magnitude are possible without compromising a pipeline's original effectiveness.

#index 1642224
#* CoRankBayes: bayesian learning to rank under the co-training framework and its application in keyphrase extraction
#@ Chen Wang;Sujian Li
#t 2011
#c 1
#% 252011
#% 281480
#% 577224
#% 1144073
#% 1227720
#! Recently, learning to rank algorithms have become a popular and effective tool for ordering objects (e.g. terms) according to their degrees of importance. The contribution of this paper is that we propose a simple and fast learning to rank model RankBayes and embed it in the co-training framework. The detailed proof is given that Naïve Bayes algorithm can be used to implement a learning to rank model. To solve the problem of two-model inconsistency, an ingenious approach is put forward to rank all the phrases by making use of the labeled results of two RankBayes models. Experimental results show that the proposed approach is promising in solving ranking problems.

#index 1642225
#* Discovering trending phrases on information streams
#@ Krishna Y. Kamath;James Caverlee
#t 2011
#c 1
#% 333931
#% 492912
#% 649540
#% 783740
#% 801696
#% 874906
#% 1366462
#% 1700144
#! We study the problem of efficient discovery of trending phrases from high-volume text streams -- be they sequences of Twitter messages, email messages, news articles, or other time-stamped text documents. Most existing approaches return top-k trending phrases. But, this approach neither guarantees that the top-k phrases returned are all trending, nor that all trending phrases are returned. In addition, the value of k is difficult to set and is indifferent to stream dynamics. Hence, we propose an approach that identifies all the trending phrases in a stream and is flexible to the changing stream properties.

#index 1642226
#* Review recommendation: personalized prediction of the quality of online reviews
#@ Samaneh Moghaddam;Mohsen Jamali;Martin Ester
#t 2011
#c 1
#% 907490
#% 1261574
#% 1269378
#% 1287270
#% 1331577
#% 1400002
#% 1476453
#% 1476461
#% 1482445
#% 1598400
#! The problem of identifying high quality and helpful reviews automatically has attracted many attention recently. Current methods assume that the helpfulness of a review is independent from the readers of that review. However, we argue that the quality of a review may not be the same for different users. In this paper, we employ latent factor models to address this problem. We evaluate the proposed models using a real life database from Epinions.com. The experiments demonstrate that the latent factor models outperform the state-of-the-art approaches and confirms that the helpfulness of a review is indeed not the same for all users.

#index 1642227
#* Improving k-nearest neighbors algorithms: practical application of dataset analysis
#@ Fidel Cacheda;Victor Carneiro;Diego Fernández;Vreixo Formoso
#t 2011
#c 1
#% 420539
#% 1357698
#% 1476448
#% 1524234
#! In the last years, recommender systems have achieved a great popularity. Many different techniques have been developed and applied to this field. However, in many cases the algorithms do not obtain the expected results. In particular, when the applied model does not fit the real data the results are especially bad. This happens because many times models are directly applied to a domain without a previous analysis of the data. In this work we study the most popular datasets in the movie recommendation domain, in order to understand how the users behave in this particular context. We have found some remarkable facts that question the utility of the similarity measures traditionally used in k-Nearest Neighbors (kNN) algorithms. These findings can be useful in order to develop new algorithms. In particular, we modify traditional kNN algorithms by introducing a new similarity measure specially suited for sparse contexts, where users have rated very few items. Our experiments show slight improvements in prediction accuracy, which proves the importance of a thorough dataset analysis as a previous step to any algorithm development.

#index 1642228
#* Structured collaborative filtering
#@ Alejandro Bellogin;Jun Wang;Pablo Castells
#t 2011
#c 1
#% 319273
#% 648320
#% 818216
#% 1083539
#% 1287255
#% 1581393
#% 1587363
#! In a general collaborative filtering (CF) setting, a user profile contains a set of previously rated items and is used to represent the user's interest. Unfortunately, most CF approaches ignore the underlying structure of user profiles. In this paper, we argue that a certain class of interest is best represented jointly by several items, drawing an analogy to "phrases" in text retrieval, which are not equivalent to the separate meaning of their words. At an alternative stance, we also consider the situation where, analogously to word synonyms, two items might be substitutable when representing a class of interest. We propose an approach integrating these two notions as opposing poles on a continuum spectrum. Upon this, we model the underlying structure in user profiles, drawing an analogy with text retrieval. The approach gives rise to a novel structured Vector Space Model for CF. We show that item-based CF approaches are a special case of the proposed method.

#index 1642229
#* User oriented tweet ranking: a filtering approach to microblogs
#@ Ibrahim Uysal;W. Bruce Croft
#t 2011
#c 1
#% 262096
#% 1298864
#% 1355042
#% 1379671
#% 1450992
#% 1476470
#% 1482397
#% 1512437
#% 1560174
#% 1560422
#% 1583594
#! The increasing volume of streaming data on microblogs has re-introduced the necessity of effective filtering mechanisms for such media. Microblog users are overwhelmed with mostly uninteresting pieces of text in order to access information of value. In this paper, we propose a personalized tweet ranking method, leveraging the use of retweet behavior, to bring more important tweets forward. In addition, we also investigate how to determine the audience of tweets more effectively, by ranking the users based on their likelihood of retweeting the tweets. Finally, conducting a pilot user study, we analyze how retweet likelihood correlates with the interestingness of the tweets.

#index 1642230
#* A semi-supervised hybrid system to enhance the recommendation of channels in terms of campaign roi
#@ Julie Séguéla;Gilbert Saporta
#t 2011
#c 1
#% 241033
#% 304425
#% 813966
#! In domains such as Marketing, Advertising or even Human Resources (sourcing), decision-makers have to choose the most suitable channels according to their objectives when starting a campaign. In this paper, three recommender systems providing channel ("user") ranking for a given campaign ("item") are introduced. This work refers exclusively to the new item problem, which is still a challenging topic in the literature. The first two systems are standard content-based recommendation approaches, with different rating estimation techniques (model-based vs heuristic-based). To overcome the lacks of previous approaches, we introduce a new hybrid system using a supervised similarity based on PLS components. Algorithms are compared in a case study: purpose is to predict the ranking of job boards (job search web sites) in terms of ROI (return on investment) per job posting. In this application, the semi-supervised hybrid system outperforms standard approaches.

#index 1642231
#* YANA: an efficient privacy-preserving recommender system for online social communities
#@ Dongsheng Li;Qin Lv;Li Shang;Ning Gu
#t 2011
#c 1
#% 616944
#% 734592
#% 813966
#% 832368
#% 868479
#% 956521
#% 1001278
#% 1632533
#! In online social communities, many recommender systems use collaborative filtering, a method that makes recommendations based on what are liked by other users with similar interests. Serious privacy issues may arise in this process, as sensitive personal information (e.g., content interests) may be collected and disclosed to other parties, especially the recommender server. In this paper, we propose YANA (short for "you are not alone"), an efficient group-based privacy-preserving collaborative filtering system for content recommendation in online social communities. We have developed a prototype system on desktop and mobile devices, and evaluated it using real world data. The results demonstrate that YANA can effectively protect users' privacy, while achieving high recommendation quality and energy efficiency.

#index 1642232
#* More influence means less work: fast latent dirichlet allocation by influence scheduling
#@ Mirwaes Wahabzada;Kristian Kersting;Anja Pilz;Christian Bauckhage
#t 2011
#c 1
#% 643056
#% 722904
#% 789800
#% 879661
#% 1211835
#% 1617371
#! There have recently been considerable advances in fast inference for (online) latent Dirichlet allocation (LDA). While it is widely recognized that the scheduling of documents in stochastic optimization and in turn in LDA may have significant consequences, this issue remains largely unexplored. Instead, practitioners schedule documents essentially uniformly at random, due perhaps to ease of implementation, and to the lack of clear guidelines on scheduling the documents. In this work, we address this issue and propose to schedule documents for an update that exert a disproportionately large influence on the topics of the corpus before less influential ones. More precisely, we justify to sample documents randomly biased towards those ones with higher norms to form mini-batches. On several real-world datasets, including 3M articles from Wikipedia and 8M from PubMed, we demonstrate that the resulting influence scheduled LDA can handily analyze massive document collections and find topic models as good or better than those found with online LDA, often at a fraction of time.

#index 1642233
#* Utility-driven anonymization in data publishing
#@ Mingqiang Xue;Panagiotis Karras;Chedy Raïssi;Hung Keng Pung
#t 2011
#c 1
#% 300184
#% 443463
#% 576111
#% 864406
#% 864412
#% 883232
#% 1200329
#% 1206582
#% 1446819
#% 1523886
#% 1538423
#! Privacy-preserving data publication has been studied intensely in the past years. Still, all existing approaches transform data values by random perturbation or generalization. In this paper, we introduce a radically different data anonymization methodology. Our proposal aims to maintain a certain amount of patterns, defined in terms of a set of properties of interest that hold for the original data. Such properties are represented as linear relationships among data points. We present an algorithm that generates a set of anonymized data that strictly preserves these properties, thus maintaining specified patterns in the data. Extensive experiments with real and synthetic data show that our algorithm is efficient, and produces anonymized data that affords high utility in several data analysis tasks while safeguarding privacy.

#index 1642234
#* Privacy preserving feature selection for distributed data using virtual dimension
#@ Madhushri Banerjee;Sumit Chakravarty
#t 2011
#c 1
#% 512307
#% 577289
#% 760529
#% 819234
#% 823389
#% 1066745
#% 1068712
#! Data Mining often suffers from the curse of dimensionality. Huge numbers of dimensions or attributes in the data pose serious problems to the data mining tasks. Traditionally data dimensionality reduction techniques like Principal Component Analysis have been used to address this problem.However, the need might be to remain in the original attribute space and identify the key predictive attributes instead of moving to a transformed space. As a result feature subset selection has become an important area of research over the last few years. With the advent of network technologies data is sometimes distributed in multiple locations and often with multiple parties. The biggest concern while sharing data is data privacy. Here, in this paper a secure distributed protocol is proposed that will allow feature selection for multiple parties without revealing their own data. The proposed distributed feature selection method has evolved from a method called virtual dimension reduction used in the field of hyperspectral image processing for selection of subset of hyperspectral bands for further analysis. The experimental results with real life datasets presented in this paper will demonstrate the effectiveness of the proposed method.

#index 1642235
#* Switch detector: an activity spotting system for desktop
#@ Hamid Turab Mirza;Ling Chen;Gencai Chen;Ibrar Hussain;Xufeng He
#t 2011
#c 1
#% 281480
#% 848653
#% 1169576
#% 1275214
#! An average white-collar worker deals with enormous amount of digital information on daily basis. Recently, there has been a growing interest to support their work. However, in order to be really supportive there is a need to know the current activity of the user at all times. In this paper we present a new technique that takes advantage of temporal aspects of user activity behavior to infer when it is most likely that an activity switch is occurring. We then describe "Activity Switch Detector" an interactive switch notification system embodying these ideas, and an extensive user study by ten participants to test the validity of the approach and present its results.

#index 1642236
#* LSH based outlier detection and its application in distributed setting
#@ Madhuchand Rushi Pillutla;Nisarg Raval;Piyush Bansal;Kannan Srinathan;C. V. Jawahar
#t 2011
#c 1
#% 249321
#% 300183
#% 479791
#% 1594653
#! In this paper, we give an approximate algorithm for distance based outlier detection using Locality Sensitive Hashing (LSH) technique. We propose an algorithm for the centralized case wherein the entire dataset is locally available for processing. However, in case of very large datasets collected from various input sources, often the data is distributed across the network. Accordingly, we show that our algorithm can be effectively extended to a constant round protocol with low communication costs, in a distributed setting with horizontal partitioning.

#index 1642237
#* Authormagic: an approach to author disambiguation in large-scale digital libraries
#@ Henning Weiler;Klaus Meyer-Wegener;Salvatore Mele
#t 2011
#c 1
#% 387427
#% 760866
#% 874458
#% 1127749
#% 1133176
#% 1211086
#% 1434125
#% 1467901
#! A collaboration of leading research centers in the field of High Energy Physics (HEP) has built INSPIRE, a novel information infrastructure, which comprises the entire corpus of about one million documents produced within the discipline, including a rich set of metadata, citation information and half a million full-text documents, and offers a unique opportunity for author disambiguation strategies. The presented approach features extended metadata comparison metrics and a three-step unsupervised graph clustering technique. The algorithm aided in identifying 200'000 individuals from 6'500'000 author signatures. Preliminary tests based on knowledge of external experts and a pilot of a crowd-sourcing system show a success rate of more than 96% within the selected test cases. The obtained author clusters serve as a recommendation for INSPIRE users to further clean the publication list in a crowd-sourced approach.

#index 1642238
#* DIGRank: using global degree to facilitate ranking in an incomplete graph
#@ Xiang Niu;Lusong Li;Ke Xu
#t 2011
#c 1
#% 783528
#% 1130835
#! PageRank has been broadly applied to get credible rank sequences of nodes in many networks such as the web, citation networks, or online social networks. However, in the real world, it is usually hard to ascertain a complete structure of a network, particularly a large-scale one. Some researchers have begun to explore how to get a relatively accurate rank more efficiently. They have proposed some local approximation methods, which are especially designed for quickly estimating the PageRank value of a new node, after it is just added to the network. Yet, these local approximation methods rely on the link server too much, and it is difficult to use them to estimate rank sequences of nodes in a group. So we propose a new method called DIGRank, which uses global Degree to facilitate Ranking in an Incomplete Graph and which takes into account the frequent need for applications to rank users in a community, retrieve pages in a particular area, or mine nodes in a fractional or limited network. Based on experiments in small-world and scale-free networks generated by models, the DIGRank method performs better than other local estimation methods on ranking nodes in a given subgraph. In the models, it tends to perform best in graphs that have low average shortest path length, high average degree, or weak community structure. Besides, compared with an local PageRank and an advanced local approximation method, it significantly reduces the computational cost and error rate.

#index 1642239
#* On selection of objective functions in multi-objective community detection
#@ Chuan Shi;Philip S. Yu;Yanan Cai;Zhenyu Yan;Bin Wu
#t 2011
#c 1
#% 1108429
#% 1321500
#% 1399996
#% 1617355
#% 1777209
#% 1777426
#! There is a surge of community detection of complex networks in recent years. Different from conventional single-objective community detection, this paper formulates community detection as a multi-objective optimization problem and proposes a general algorithm NSGA-Net based on evolutionary multi-objective optimization. Interested in the effect of optimization objectives on the performance of the multi-objective community detection, we further study the correlations (i.e., positively correlated, independent, or negatively correlated) of 11 objective functions that have been used or can potentially be used for community detection. Our experiments show that NSGA-Net optimizing over a pair of negatively correlated objectives usually performs better than the single-objective algorithm optimizing over either of the original objectives, and even better than other well-established community detection approaches.

#index 1642240
#* Suggesting ghost edges for a smaller world
#@ Manos Papagelis;Francesco Bonchi;Aristides Gionis
#t 2011
#c 1
#% 26749
#% 300078
#% 1266981
#% 1291626
#% 1482237
#% 1720136
#! Small changes in the network topology can have dramatic effects on its capacity to disseminate information. In this paper, we consider the problem of adding a small number of ghost edges in the network in order to minimize the average shortest-path distance between nodes, towards a smaller-world network. We formalize the problem of suggesting ghost edges and we propose a novel method for quickly evaluating the importance of ghost edges in sparse graphs. Through experiments on real and synthetic data sets, we demonstrate that our approach performs very well, for a varying range of conditions, and it outperforms sensible baselines.

#index 1642241
#* Examining the "leftness" property of Wikipedia categories
#@ Karl Gyllstrom;Marie-Francine Moens
#t 2011
#c 1
#% 956564
#% 1269899
#% 1409954
#% 1587344
#! Wikipedia's rich category structure has helped make it one of the largest semantic taxonomies in existence, a property that has been central to much recent research. However, Wikipedia's category representation is simplistic: an article contains a single list of categories, with no data about their relative importance. We investigate the ordering of category lists to determine how a category's position in the list correlates with its relevance to the article and overall significance. We identify a number of interesting connections between a category's position and its persistence within the article, age, popularity, size, and descriptiveness.

#index 1642242
#* Detection of text quality flaws as a one-class classification problem
#@ Maik Anderka;Benno Stein;Nedim Lipka
#t 2011
#c 1
#% 400847
#% 770870
#% 781774
#% 1035587
#% 1108867
#% 1190277
#% 1202160
#% 1213445
#% 1288484
#% 1400087
#% 1558411
#% 1560148
#! For Web applications that are based on user generated content the detection of text quality flaws is a key concern. Our research contributes to automatic quality flaw detection. In particular, we propose to cast the detection of text quality flaws as a one-class classification problem: we are given only positive examples (= texts containing a particular quality flaw) and decide whether or not an unseen text suffers from this flaw. We argue that common binary or multiclass classification approaches are ineffective in here, and we underpin our approach by a real-world application: we employ a dedicated one-class learning approach to determine whether a given Wikipedia article suffers from certain quality flaws. Since in the Wikipedia setting the acquisition of sensible test data is quite intricate, we analyze the effects of a biased sample selection. In addition, we illustrate the classifier effectiveness as a function of the flaw distribution in order to cope with the unknown (real-world) flaw-specific class imbalances. Altogether, provided test data with little noise, four from ten important quality flaws in Wikipedia can be detected with a precision close to 1.

#index 1642243
#* Two birds with one stone: learning semantic models for text categorization and word sense disambiguation
#@ Roberto Navigli;Stefano Faralli;Aitor Soroa;Oier de Lacalle;Eneko Agirre
#t 2011
#c 1
#% 402289
#% 939906
#% 1022765
#% 1083703
#% 1131827
#% 1212636
#% 1260669
#% 1272267
#% 1305530
#% 1471332
#% 1471333
#! In this paper we present a novel approach to learning semantic models for multiple domains, which we use to categorize Wikipedia pages and to perform domain Word Sense Disambiguation (WSD). In order to learn a semantic model for each domain we first extract relevant terms from the texts in the domain and then use these terms to initialize a random walk over the WordNet graph. Given an input text, we check the semantic models, choose the appropriate domain for that text and use the best-matching model to perform WSD. Our results show considerable improvements on text categorization and domain WSD tasks.

#index 1642244
#* More or better: on trade-offs in compacting textual problem solution repositories
#@ Deepak P.;Sutanu Chakraborti;Deepak Khemani
#t 2011
#c 1
#% 92533
#% 490445
#% 490605
#% 492197
#% 1014782
#% 1273764
#% 1275276
#! In this paper, we look into the problem of filtering problem solution repositories (from sources such as community-driven question answering systems) to render them more suitable for usage in knowledge reuse systems. We explore harnessing the fuzzy nature of usability of a solution to a problem, for such compaction. Fuzzy usabilities lead to several challenges; notably, the trade-off between choosing generic or better solutions. We develop an approach that can heed to a user specification of the trade-off between these criteria and introduce several quality measures based on fuzzy usability estimates to ascertain the quality of a problem-solution repository for usage in a Case Based Reasoning system. We establish, through a detailed empirical analysis, that our approach outperforms state-of-the-art approaches on virtually all quality measures.

#index 1642245
#* Mining frequent patterns across multiple data streams
#@ Jing Guo;Peng Zhang;Jianlong Tan;Li Guo
#t 2011
#c 1
#% 300120
#% 751684
#% 993960
#% 1013629
#% 1074831
#% 1231939
#% 1482456
#% 1505314
#% 1605932
#! Mining frequent patterns from data streams has drawn increasing attention in recent years. However, previous mining algorithms were all focused on a single data stream. In many emerging applications, it is of critical importance to combine multiple data streams for analysis. For example, in real-time news topic analysis, it is necessary to combine multiple news report streams from dierent media sources to discover collaborative frequent patterns which are reported frequently in all media, and comparative frequent patterns which are reported more frequently in a media than others. To address this problem, we propose a novel frequent pattern mining algorithm Hybrid-Streaming, H-Stream for short. H-Stream builds a new Hybrid-Frequent tree to maintain historical frequent and potential frequent itemsets from all data streams, and incrementally updates these itemsets for efficient collaborative and comparative pattern mining. Theoretical and empirical studies demonstrate the utility of the proposed method.

#index 1642246
#* SILA: a spatial instance learning approach for deep webpages
#@ Ermelinda Oro;Massimo Ruffolo
#t 2011
#c 1
#% 729978
#% 805845
#% 837605
#% 902460
#% 1364949
#% 1523976
#! Deep Web pages convey very relevant information for different application domains like e-government, e-commerce, social networking. For this reason there is a constant high interest in efficiently, effectively and automatically extracting data from Deep Web data sources. In this paper we present SILA, a novel Spatial Instance Learning Approach, that allows for extracting data records from Deep Web pages by exploiting both the spatial arrangement and the presentation features of data items/fields produced by layout engines of Web browsers in visualizing Deep Web pages on the screen. SILA is independent from the internal HTML encodings of Web pages, and allows for recognizing data records in pages having multiple data regions in which data items are arranged by many different presentation layouts. Experimental results show that SILA has very high precision and recall and that it works much better than MDR and ViNTs approaches.

#index 1642247
#* A geographic study of tie strength in social media
#@ Jeffrey McGee;James A. Caverlee;Zhiyuan Cheng
#t 2011
#c 1
#% 1047484
#% 1399939
#% 1399992
#% 1482254
#% 1573621
#! In this paper, we investigate the interplay of distance and tie strength through an examination of 20 million geo-encoded tweets collected from Twitter and 6 million user profiles. Concretely, we investigate the relationship between the strength of the tie between a pair of users, and the distance between the pair. We identify several factors -- including following, mentioning, and actively engaging in conversations with another user -- that can strongly reveal the distance between a pair of users. We find a bimodal distribution in Twitter, with one peak around 10 miles from people who live nearby, and another peak around 2500 miles, further validating Twitter's use as both a social network (with geographically nearby friends) and as a news distribution network (with very distant relationships).

#index 1642248
#* Named entity recognition using a modified Pegasos algorithm
#@ Changki Lee;Pum-Mo Ryu;HyunKi Kim
#t 2011
#c 1
#% 742424
#% 770763
#% 816155
#% 855119
#% 983905
#% 1264133
#! In this paper, we describe a named entity recognition using a modified Pegasos algorithm for structural SVMs. We show the modified Pegasos algorithm significantly outperformed CRFs and the training time for the modified Pegasos algorithm is reduced 17-26 times compared to CRFs.

#index 1642249
#* WikiLabel: an encyclopedic approach to labeling documents en masse
#@ Tadashi Nomoto
#t 2011
#c 1
#% 575568
#% 989620
#% 1227594
#! This paper presents a particular approach to collective labeling of multiple documents, which works by associating the documents with Wikipedia pages and labeling them with headings the pages carry. The approach has an obvious advantage over past approaches in that it is able to produce fluent labels, as they are hand-written by human editors. We carried out some experiments on the TDT5 dataset, which found that the approach works rather robustly for an arbitrary set of documents in the news domain. Comparisons were made with some baselines, including the state of the art, with results strongly in favor of our approach.

#index 1642250
#* Towards noise-resilient document modeling
#@ Tao Yang;Dongwon Lee
#t 2011
#c 1
#% 722904
#% 1292515
#% 1481559
#! We introduce a generative probabilistic document model based on latent Dirichlet allocation (LDA), to deal with textual errors in the document collection. Our model is inspired by the fact that most large-scale text data are machine-generated and thus inevitably contain many types of noise. The new model, termed as TE-LDA, is developed from the traditional LDA by adding a switch variable into the term generation process in order to tackle the issue of noisy text data. Through extensive experiments, the efficacy of our proposed model is validated using both real and synthetic data sets.

#index 1642251
#* Probabilistic model for discovering topic based communities in social networks
#@ Mrinmaya Sachan;Danish Contractor;Tanveer Faruquie;Venkata Subramaniam
#t 2011
#c 1
#% 869480
#% 1211773
#% 1269775
#! Social graphs have received renewed interest as a research topic with the advent of social networking websites. These online networks provide a rich source of data to study user relationships and interaction patterns on a large scale. In this paper, we propose a generative Bayesian model for extracting latent communities from a social graph. We assume that community memberships depend on topics of interest between users and the link relationships between them in the social graph topology. In addition, we make use of the nature of interaction to gauge user interests. Our model allows communities to be related to multiple topics and each user in the graph can be a member of multiple communities. This gives an insight into user interests and topical distribution in communities. We show the effectiveness of our model using a real world data set and also compare our model with existing community discovery methods.

#index 1642252
#* Scalable entity matching computation with materialization
#@ Sanghoon Lee;Jongwuk Lee;Seung-won Hwang
#t 2011
#c 1
#% 333854
#% 350103
#% 397378
#% 810014
#% 927033
#% 1201863
#% 1314445
#% 1523813
#% 1523838
#% 1523912
#! Entity matching (EM) is the task of identifying records that refer to the same real-world entity from different data sources. While EM is widely used in data integration and data cleaning applications, the naive method for EM incurs quadratic cost with respect to the size of the datasets. To address this problem, this paper proposes a scalable EM algorithm that employs a pre-materialized structure. Specifically, once the structure is built, our proposed algorithm can identify the EM results with sub-linear cost. In addition, as the rules evolve, our algorithm can efficiently adapt to new rules by selectively accessing records using the materialized structure. Our evaluation results show that our proposed EM algorithm is significantly faster than the state-of-the-art method for extensive real-life datasets.

#index 1642253
#* Predicting the optimal ad-hoc index for reachability queries on graph databases
#@ Jintian Deng;Fei Liu;Yun Peng;Byron Choi;Jianliang Xu
#t 2011
#c 1
#% 58365
#% 379482
#% 745479
#% 1194484
#% 1194592
#% 1380974
#% 1594600
#! Due to the recent advances in graph databases, a large number of ad-hoc indexes for a fundamental query, in particular, reachability query, have been proposed. The performances of these indexes on different graphs have known to be very different. Worst still, deriving an accurate cost model for selecting the optimal index of a graph database appears to be a daunting task. In this paper, we propose a hierarchical prediction framework, based on neural networks and a set of graph features and a knowledge base on past predictions, to determine the optimal index for a graph database. For ease of presentation, we propose our framework with three structurally distinguishable indexes. Our experiments show that our framework is accurate.

#index 1642254
#* Collection-based compression using discovered long matching strings
#@ Andrew Peel;Anthony Wirth;Justin Zobel
#t 2011
#c 1
#% 251442
#% 346149
#% 398751
#% 521993
#% 768815
#% 872086
#! Many collections of data contain items that are inherently similar. For example, archives contain files with incremental changes between releases. Long-range inter-file similarities are not exploited by standard approaches to compression. We investigate compression using similarity from all parts of a collection, collection-based compression (CBC). Input files are delta-encoded by reference to long string matches in a source collection. The expected space requirement of our encoding algorithm is sublinear with the collection size, and the compression time complexity is linear with the input file size. We show that our scheme achieves better compression for large input files than existing differential compression systems, and scales better. Also, we achieve significant compression improvement compared to compressing each file individually using standard utilities: our scheme achieves several times the compression of gzip or 7-zip. The overall result is a dramatic improvement on compression available with existing approaches.

#index 1642255
#* A robust index for regular expression queries
#@ Dominic Tsang;Sanjay Chawla
#t 2011
#c 1
#% 390964
#% 660009
#% 783499
#! The like regular expression predicate has been part of the SQL standard since at least 1989. However, despite its popularity and wide usage, database vendors provide only limited indexing support for regular expression queries which almost always require a full table scan. In this paper we propose a rigorous and robust approach for providing indexing support for regular expression queries. Our approach consists of formulating the indexing problem as a combinatorial optimization problem. We begin with a database, abstracted as a collection of strings. From this data set we generate a query workload. The input to the optimization problem is the database and the workload. The output is a set of multigrams (substrings) which can be used as keys to records which satisfy the query workload. The multigrams can then be integrated with the data structure (like B+ trees) to provide indexing support for the queries. We provide a deterministic and a randomized approximation algorithm (with provable guarantees) to solve the optimization problem. Extensive experiments on synthetic data sets demonstrate that our approach is accurate and efficient. We also present a case study on PROSITE patterns - which are complex regular expression signatures for classes of proteins. Again, we are able to demonstrate the utility of our indexing approach in terms of accuracy and efficiency. Thus, perhaps for the first time, there is a robust and practical indexing mechanism for an important class of database queries.

#index 1642256
#* Integrating and querying web databases and documents
#@ Carlos Garcia-Alvarado;Carlos Ordonez
#t 2011
#c 1
#% 278445
#% 1016622
#% 1063537
#% 1063538
#% 1131152
#% 1206808
#% 1215253
#% 1217198
#% 1479591
#! There exist many interrelated information sources on the Internet that can be categorized into structured (database) and semistructured (documents). A key challenge is to integrate, query and analyze such heterogeneous collections of information. In this paper, we defend the idea of building web metadata repositories using relational databases as the main source and central data management technology of structured data, enriched by the semistructured data surrounding it. Our proposal rests on the assumption that heterogeneous relational databases can be integrated (i.e. entity resolution is assumed to work well) and thus can serve as references for external data. That is, we tackle the problem of integrating information in the deep web, departing from databases. We discuss a prototype system that can integrate and query metadata and related documents, based on relational database technology. Metadata includes database ER model elements like database name, table, and column (entity, attribute). Web document data include files, documents and web pages. Links between metadata and external documents are built with SQL queries. Once databases and documents are linked, they are managed and queried with SQL. We discuss an interesting scientific application of our solution with a water pollution database.

#index 1642257
#* Processing the signature quadratic form distance on many-core GPU architectures
#@ Martin Kruliš;Jakub Lokoč;Christian Beecks;Tomáš Skopal;Thomas Seidl
#t 2011
#c 1
#% 325683
#% 778279
#% 857113
#% 1434103
#% 1583894
#% 1586177
#! The Signature Quadratic Form Distance on feature signatures represents a flexible distance-based similarity model for effective content-based multimedia retrieval. Although metric indexing approaches are able to speed up query processing by two orders of magnitude, their applicability to large-scale multimedia databases containing billions of images is still a challenging issue. In this paper, we propose the utilization of GPUs for efficient query processing with the Signature Quadratic Form Distance. We show how to process multiple distance computations in parallel and demonstrate efficient query processing by comparing many-core GPU with multi-core CPU implementations.

#index 1642258
#* Top-k most influential locations selection
#@ Jin Huang;Zeyi Wen;Jianzhong Qi;Rui Zhang;Jian Chen;Zhen He
#t 2011
#c 1
#% 300163
#% 427199
#% 479819
#% 480661
#% 527189
#% 580978
#% 824730
#% 1080128
#% 1181288
#% 1206682
#% 1328203
#% 1523830
#% 1594683
#% 1720751
#! We propose and study a new type of facility location selection query, the top-k most influential location selection query. Given a set M of customers and a set F of existing facilities, this query finds k locations from a set C of candidate locations with the largest influence values, where the influence of a candidate location c (c in C) is defined as the number of customers in M who are the reverse nearest neighbors of c. We first present a naive algorithm to process the query. However, the algorithm is computationally expensive and not scalable to large datasets. This motivates us to explore more efficient solutions. We propose two branch and bound algorithms, the Estimation Expanding Pruning (EEP) algorithm and the Bounding Influence Pruning (BIP) algorithm. These algorithms exploit various geometric properties to prune the search space, and thus achieve much better performance than that of the naive algorithm. Specifically, the EEP algorithm estimates the distances to the nearest existing facilities for the customers and the numbers of influenced customers for the candidate locations, and then gradually refines the estimation until the answer set is found, during which distance metric based pruning techniques are used to improve the refinement efficiency. BIP only estimates the numbers of influenced customers for the candidate locations. But it uses the existing facilities to limit the space for searching the influenced customers and achieve a better estimation, which results in an even more efficient algorithm. Extensive experiments conducted on both real and synthetic datasets validate the efficiency of the algorithms.

#index 1642259
#* Defining isochrones in multimodal spatial networks
#@ Johann Gamper;Michael Böhlen;Willi Cometti;Markus Innerebner
#t 2011
#c 1
#% 443208
#% 824723
#% 864397
#% 874062
#% 976718
#% 1015321
#% 1016199
#% 1044452
#% 1063472
#% 1135210
#% 1211643
#% 1292553
#% 1372715
#% 1482228
#! An isochrone in a spatial network is the minimal, possibly disconnected subgraph that covers all locations from where a query point is reachable within a given time span and by a given arrival time. In this paper we formally define isochrones for multimodal spatial networks with different transportation modes that can be discrete or continuous in, respectively, space and time. For the computation of isochrones we propose the multimodal incremental network expansion (MINE) algorithm, which is independent of the actual network size and depends only on the size of the isochrone. An empirical study using real-world data confirms the analytical results.

#index 1642260
#* On the elasticity of NoSQL databases over cloud management platforms
#@ Ioannis Konstantinou;Evangelos Angelou;Christina Boumpouka;Dimitrios Tsoumakos;Nectarios Koziris
#t 2011
#c 1
#% 978404
#% 1400975
#% 1426489
#% 1426550
#% 1459252
#% 1594596
#! NoSQL databases focus on analytical processing of large scale datasets, offering increased scalability over commodity hardware. One of their strongest features is elasticity, which allows for fairly portioned premiums and high-quality performance and directly applies to the philosophy of a cloud-based platform. Yet, the process of adaptive expansion and contraction of resources usually involves a lot of manual effort during cluster configuration. To date, there exists no comparative study to quantify this cost and measure the efficacy of NoSQL engines that offer this feature over a cloud provider. In this work, we present a cloud-enabled framework for adaptive monitoring of NoSQL systems. We perform a study of the elasticity feature on some of the most popular NoSQL databases over an open-source cloud platform. Based on these measurements, we finally present a prototype implementation of a decision making system that enables automatic elastic operations of any NoSQL engine based on administrator or application-specified constraints.

#index 1642261
#* Continuous data stream query in the cloud
#@ Jun Li;Peng Zhang;Jianlong Tan;Ping Liu;Li Guo
#t 2011
#c 1
#% 300167
#% 300179
#% 427199
#% 654488
#% 765500
#% 963669
#% 1127559
#% 1328186
#% 1426551
#% 1505314
#% 1605932
#! Cloud computing represents one of the most important research directions for modern computing systems. Existing research efforts on Cloud computing were all focused on designing advanced storage and query techniques for static data. None of them consider the problem that data in a Cloud may appear as continuous and rapid data streams. To address this problem, in this paper we propose a new LCN-Index framework to handle continuous data stream queries in the Cloud. LCN-Index uses the Map-Reduce computing paradigm to process all the queries. In the Mapping stage, it divides all the queries into a batch of predicate sets which are then deployed onto mapping nodes using interval predicate index. In the reducing stage, it merges results from the mapping nodes using multi attribute hash index. In so doing, a data stream can be efficiently evaluated by traversing through the LCN-Index framework. Experiments demonstrate the utility of the proposed method.

#index 1642262
#* A cluster based mobile peer to peer architecture in wireless ad hoc networks
#@ He Li;KyoungSoo Bok;JaeSoo Yoo
#t 2011
#c 1
#% 332941
#% 421124
#% 622728
#% 889658
#% 975411
#% 1432434
#% 1445689
#% 1684786
#! With the rapid development of wireless communication technologies and mobile devices, the mobile peer to peer (MP2P) network has been emerged. Since the existing MP2P architectures have high management cost, in this paper, we propose a hierarchical MP2P architecture using clustering mobile peers. The proposed method clusters the mobile peers by considering three aspects like the maximum connection time, the minimum hop count and the number of the connected peers. The connection times between the connected peers can be determined by the location, velocity vector and communication range of the mobile peers. Since the maximum connection time of the connected peers are considered, the network topology is relatively stable. Therefore, the management cost of the network is decreased and the success rate of contents search is increased. Experiments have shown that our proposed method outperforms the existing schemes.

#index 1642263
#* Block-based load balancing for entity resolution with MapReduce
#@ Lars Kolb;Andreas Thor;Erhard Rahm
#t 2011
#c 1
#% 963669
#% 1215321
#% 1314445
#% 1426543
#% 1426602
#% 1523838
#% 1694380
#! The effectiveness and scalability of MapReduce-based implementations of complex data-intensive tasks depend on an even redistribution of data between map and reduce tasks. In the presence of skewed data, sophisticated redistribution approaches thus become necessary to achieve load balancing among all reduce tasks to be executed in parallel. For the complex problem of entity resolution with blocking, we propose BlockSplit, a load balancing approach that supports blocking techniques to reduce the search space of entity resolution. The evaluation on a real cloud infrastructure shows the value and effectiveness of the proposed approach.

#index 1642264
#* PCMLogging: reducing transaction logging overhead with PCM
#@ Shen Gao;Jianliang Xu;Bingsheng He;Byron Choi;Haibo Hu
#t 2011
#c 1
#% 116078
#% 1278382
#% 1523856
#! Phase Changing Memory (PCM), as one of the most promising next-generation memory technologies, offers various attractive properties such as non-volatility, bit-alterability, and low idle energy consumption. In this paper, we present PCMLogging, a novel logging scheme that exploits PCM devices for both data buffering and transaction logging in disk-based databases. Different from the traditional approach where buffered updates and transaction logs are completely separated, they are integrated in the new logging scheme. Our preliminary experiments show an up to 40% improvement of PCMLogging in disk I/O performance in comparison with a basic buffering and logging scheme.

#index 1642265
#* A continuous query evaluation scheme for a detection-only query over data streams
#@ Hong Kyu Park;Won Suk Lee
#t 2011
#c 1
#% 397353
#% 726621
#% 765435
#% 1060252
#% 1061935
#% 1206771
#% 1426524
#! In a data stream environment, a multi-way join continuous query is employed to monitor a considerable number of source data streams from various remote sites in real-time. One key role of a continuous query is detecting only the invocation of a particular event corresponding to the specifications of the query. The evaluation of such a detection-only query does not require to produce either an intermediate tuple or a final result tuple, which not only shortens the processing time of a query but also reduces the usage of memory space. However, there has been no special effort to deal with a query of this type. This paper proposes a new evaluation framework which efficiently processes a multi-way detection-only query without generating any intermediate result tuple explicitly.

#index 1642266
#* Subject-oriented top-k hot region queries in spatial dataset
#@ Junling Liu;Ge Yu;Huanliang Sun
#t 2011
#c 1
#% 287466
#% 824730
#% 1035440
#% 1127435
#% 1206696
#% 1211442
#% 1246166
#% 1720757
#! This paper proposes and solves a novel type of spatial queries named Subject-oriented Top-k hot Region (STR) queries. Given a subject S defined by a feature set R and features importance denoted by weights, an STR query retrieves k non-overlapping regions that have the highest scores computed by the number of feature objects and their weights. As an example, the culture subject is defined by exhibition halls, libraries and museums. On the subject, an STR query finds cultural centers intensively distributed feature objects. In this paper, we propose two efficient algorithms, single-partition (SP) algorithm and dual-partition (DP) algorithm, to process STR queries. Extensive experiments evaluate the proposed solutions under a wide range of parameter settings.

#index 1642267
#* k-Nearest neighbor query processing method based on distance relation pattern
#@ Yonghun Park;Dongmin Seo;Kyoungsoo Bok;Jaesoo Yoo
#t 2011
#c 1
#% 800571
#% 810061
#% 975051
#% 1408835
#! The k-nearest neighbor (k-NN) query is one of the most important query types for location based services (LBS). Various methods have been proposed to efficiently process the k-NN query. However, most of the existing methods suffer from high computation time and larger memory requirement because they unnecessarily access cells to find the nearest cells on a grid index. In this paper, we propose a new efficient method, called Pattern Based k-NN (PB-kNN) to process the k-NN query. The proposed method uses the patterns of the distance relationships among the cells in a grid index. The basic idea is to normalize the distance relationships as certain patterns. Using this approach, PB-kNN significantly improves the overall performance of the query processing. It is shown through various experiments that our proposed method outperforms the existing methods in terms of query processing time and storage overhead.

#index 1642268
#* Efficient query rewrite for structured web queries
#@ Sreenivas Gollapudi;Samuel Ieong;Alexandros Ntoulas;Stelios Paparizos
#t 2011
#c 1
#% 201876
#% 893105
#% 1127393
#% 1127404
#% 1426566
#% 1536531
#! Web search engines incorporate results from structured data sources to answer semantically rich user queries, i.e. Samsung 50 inch led tv can be answered from a table of television data. However, users are not domain experts and quite often enter values that do not match precisely the underlying data, so a literal execution will return zero results. A search engine would prefer to return at least a minimum number of results as close to the original query as possible while providing a time-bound execution guarantee. In this paper, we formalize these requirements, show the problem is NP-Hard and present approximation algorithms that produce rewrites that work in practice. We empirically validate our algorithms on large-scale data from a major search engine.

#index 1642269
#* Rule-based construction of matching processes
#@ Eric Peukert;Julian Eberius;Erhard Rahm
#t 2011
#c 1
#% 59697
#% 572314
#% 735938
#% 903009
#% 993982
#% 1092531
#% 1111257
#% 1218637
#% 1246170
#% 1292779
#% 1383548
#% 1456651
#! Semi-automatic schema matching systems have been developed to compute mapping suggestions that can be corrected by a user. However, constructing and tuning match strategies still requires a high manual effort. We therefore propose a self-configuring schema matching system that is able to automatically adapt to the given mapping problem at hand. Our approach is based on analyzing the input schemas as well as intermediate match results. A variety of matching rules use the analysis results to automatically construct and adapt an underlying matching process for a given match task. The evaluation shows that our system is able to robustly return good quality mappings across different mapping problems and domains.

#index 1642270
#* A taxonomy of local search: semi-supervised query classification driven by information needs
#@ Jiang Bian;Yi Chang
#t 2011
#c 1
#% 590523
#% 642982
#% 730051
#% 766447
#% 844287
#% 987221
#% 1301004
#! Local search service (e.g. Yelp, Yahoo! Local) has emerged as a popular and effective paradigm for a wide range of information needs for local businesses; it now provides a viable and even more effective alternative to general purpose web search for queries on local businesses. However, due to the diversity of information needs behind local search, it is necessary to use different information retrieval strategies for different query types in local search. In this paper, we explore a taxonomy of local search driven by users' information needs, which categorizes local search queries into three types: business category, chain business, and non-chain business. To decide which search strategy to use for each category in this taxonomy without placing the burden on the web users, it is indispensable to build an automatic local query classifier. However, since local search queries yield few online features and it is expensive to obtain editorial labels, it is insufficient to use only a supervised learning approach. In this paper, we address these problems by developing a semi-supervised approach for mining information needs from a vast amount of unlabeled data from local query logs to boost local query classification. Results of a large scale evaluation over queries from a commercial local search site illustrate that the proposed semi-supervised method allow us to accurately classify a substantially larger proportion of local queries than the supervised learning approach.

#index 1642271
#* ONTOCUBE: efficient ontology extraction using OLAP cubes
#@ Carlos Garcia-Alvarado;Zhibo Chen;Carlos Ordonez
#t 2011
#c 1
#% 745439
#% 1131093
#% 1186739
#% 1482326
#% 1512993
#! Ontologies are knowledge conceptualizations of a particular domain and are commonly represented with hierarchies. While final ontologies appear deceivingly simple on paper, building ontologies represents a time-consuming task that is normally performed by natural language processing techniques or schema matching. On the other hand, OLAP cubes are most commonly used during decision-making processes via the analysis of data summarizations. In this paper, we present a novel approach based on using OLAP cubes for ontology extraction. The resulting ontology is obtained through an analytical process of the summarized frequencies of keywords within a corpus. The solution was implemented within a relational database system (DBMS). In our experiments, we show how all the proposed discrimination measures (frequency, correlation, lift) affect the resulting classes. We also show a sample ontology result and the accuracy of finding true classes. Finally, we show the performance breakdown of our algorithm.

#index 1642272
#* An algorithm for axiom pinpointing in EL+ and its incremental variant
#@ Xiaojun Cheng;Guilin Qi
#t 2011
#c 1
#% 1099873
#% 1279264
#% 1289408
#% 1374393
#% 1409922
#% 1655399
#! Axiom pinpointing plays an important role in the development and maintenance of ontologies. It helps the user to comprehend an unwanted entailment of an ontology by presenting all minimal subsets of the ontology which are responsible for the entailment (called MinAs). In this paper, we consider the problem of axiom pinpointing in description logic EL+, which underpins OWL 2 EL, a profile of the latest version of Web Ontology Language (OWL). We propose a novel method to compute all MinAs that utilizes the hierarchy information obtained from the classification of an EL+ ontology. The advantage of our method over an existing labeled classification based method is that we do not attach labels to entailed subsumptions, which can be memory exhaustion for large scale ontologies. We further consider axiom pinpointing in EL+ when ontologies change. An incremental algorithm is given to compute all MinAs by reusing MinAs previously computed.

#index 1642273
#* Folksonomy-based term extraction for word cloud generation
#@ David Carmel;Erel Uziel;Ido Guy;Yosi Mass;Haggai Roitman
#t 2011
#c 1
#% 280851
#% 340948
#% 860021
#% 955229
#% 1035588
#% 1074117
#% 1077150
#% 1215458
#% 1227594
#% 1242496
#% 1292505
#% 1305619
#% 1399992
#% 1667787
#! In this work we study the task of term extraction for word cloud generation. We present a folksonomy-based term extraction method, called tag-boost, which boosts terms that are frequently used by the public to tag content. Our experiments with tag-boost-based term extraction over different domains demonstrate tremendous improvement in word cloud quality, as reflected by the agreement between extracted terms and manually assigned tags of the testing items. Additionally, we show that tag-boost can be effectively applied even in non-tagged domains, by using an external rich folksonomy borrowed from a well-tagged domain.

#index 1642274
#* Efficient association discovery with keyword-based constraints on large graph data
#@ Mo Zhou;Yifan Pan;Yuqing Wu
#t 2011
#c 1
#% 577372
#% 805884
#% 824693
#% 956574
#% 960259
#% 1063537
#% 1098424
#% 1127445
#% 1190676
#% 1292635
#% 1426512
#% 1642302
#! In many domains, such as social networks and chem-informatics, data can be represented naturally in graph model, with nodes being data entries and edges the relationships between them. We study the application requirements in these domains and find that discovering Constrained Acyclic Paths (CAP) is highly in demand. In this paper, we define the CAP search problem and introduce a set of quantitative metrics for describing keyword-based constraints. We propose a series of algorithms to efficiently evaluate CAP queries on large-scale graph data. Extensive experiments illustrate that our algorithms are both efficient and scalable.

#index 1642275
#* AWETO: efficient incremental update and querying in rdf storage system
#@ Xu Pu;Jianyong Wang;Ping Luo;Min Wang
#t 2011
#c 1
#% 322884
#% 824697
#% 1022236
#% 1127402
#% 1190676
#% 1217194
#% 1366460
#% 1374374
#% 1523817
#! With the fast growth of the knowledge bases built over the Internet, storing and querying millions or billions of RDF triples in a knowledge base have attracted increasing research interests. Although the latest RDF storage systems achieve good querying performance, few of them pay much attention to the characteristic of dynamic growth of the knowledge base. In this paper, to consider the efficiency of both querying and incremental update in RDF data, we propose a hAsh-based tWo-tiEr rdf sTOrage system (abbr. to AWETO) with new index architecture and query execution engine. The performance of our system is systematically measured over two large-scale datesets. Compared with the other three state-of-the-art RDF storage systems, our system achieves the best incremental update efficiency, meanwhile, the query efficiency is competitive.

#index 1642276
#* Insert-friendly XML containment labeling scheme
#@ Canwei Zhuang;Ziyu Lin;Shaorong Feng
#t 2011
#c 1
#% 333981
#% 378412
#% 397366
#% 838506
#% 1046512
#! The labeling scheme is designed to label the XML nodes so that both ordered and un-ordered queries can be processed without accessing the original XML file. When XML data become dynamic, it is important to design a labeling scheme that can facilitate updates and support query processing efficiently. In this paper, we propose a novel containment labeling scheme called DXCL (Dynamic XML Containment Labeling) to effectively process updating in dynamic XML data. Compared with the existing dynamic labeling schemes, a distinguishing feature of DXCL is that DXCL is compact and efficient regardless of whether the documents are updated or not. DXCL uses fixed length integer numbers to label initial XML documents and hence yields compact label size and high query performance. When updates take place, DXCL also has high performance on both label updates and query processing especially in the case of skewed insertions. Experimental results conform the benefits of our approach over the previous dynamic schemes.

#index 1642277
#* A pretopological framework for the automatic construction of lexical-semantic structures from texts
#@ Guillaume Cleuziou;Davide Buscaldi;Vincent Levorato;Gaël Dias
#t 2011
#c 1
#% 384416
#% 459483
#% 939376
#% 1249494
#% 1272078
#! We present in this paper a new approach for the automatic generation of lexical structures from texts. This tedious task is based on the strong hypothesis that simple statistical observations on textual usages can provide pieces of semantics about the lexicon. Using such "naive" observations only, we propose a (pre)-topological framework to formalize and combine various hypothesis on textual data usages and then to derive a structure similar to usual lexical knowledge basis such as WordNet. In addition we also consider the evaluation problem for obtained lexical structures ; a multi-level evaluation strategy is proposed that measures the fitting between a given reference structure and automatically generated structures on different point of views : intrinsic/structural and application-based points of view. The evaluation strategy is then used to quantify the contribution of the new structuring approach with respect to the corresponding solution proposed by (Sanderson et al. 2000) on two case studies that differs on the domain and the size of the lexicon.

#index 1642278
#* Leveraging web 2.0 data for scalable semi-supervised learning of domain-specific sentiment lexicons
#@ Raymond Yiu Keung Lau;Chun Lam Lai;Peter B. Bruza;Kam F. Wong
#t 2011
#c 1
#% 262096
#% 722308
#% 766423
#% 838521
#% 854646
#% 943811
#% 989576
#% 1037633
#% 1130915
#% 1190865
#% 1209657
#% 1260740
#% 1261566
#% 1268503
#% 1305481
#% 1415716
#% 1471238
#% 1566286
#! Since manually constructing domain-specific sentiment lexicons is extremely time consuming and it may not even be feasible for domains where linguistic expertise is not available, research on automatic construction of domain-specific sentiment lexicons has become a hot topic in recent years. The main contribution of this paper is the illustration of a novel semi-supervised learning method which exploits both term-to-term and document-to-term relations hidden in a corpus for the construction of domain-specific sentiment lexicons. More specifically, the proposed two-pass pseudo labeling method combines shallow linguistic parsing and corpus-base statistical learning to make domain-specific sentiment extraction scalable with respect to the sheer volume of opinionated documents archived on the Internet these days. Our experiments show that the proposed method can generate high quality domain-specific sentiment lexicons according to users' evaluation.

#index 1642279
#* Classifying trending topics: a typology of conversation triggers on Twitter
#@ Arkaitz Zubiaga;Damiano Spina;Víctor Fresno;Raquel Martínez
#t 2011
#c 1
#% 413199
#% 458379
#% 763699
#% 1040837
#% 1297059
#% 1399992
#% 1400018
#% 1432574
#% 1450992
#% 1535333
#% 1560425
#! Twitter summarizes the great deal of messages posted by users in the form of trending topics that reflect the top conversations being discussed at a given moment. These trending topics tend to be connected to current affairs. Different happenings can give rise to the emergence of these trending topics. For instance, a sports event broadcasted on TV, or a viral meme introduced by a community of users. Detecting the type of origin can facilitate information filtering, enhance real-time data processing, and improve user experience. In this paper, we introduce a typology to categorize the triggers that leverage trending topics: news, current events, memes, and commemoratives. We define a set of straightforward language-independent features that rely on the social spread of the trends to discriminate among those types of trending topics. Our method provides an efficient way to immediately and accurately categorize trending topics without need of external data, outperforming a content-based approach.

#index 1642280
#* Enhancing accessibility of microblogging messages using semantic knowledge
#@ Xia Hu;Lei Tang;Huan Liu
#t 2011
#c 1
#% 727861
#% 878454
#% 987328
#% 1227600
#% 1289518
#% 1292559
#! The volume of microblogging messages is increasing exponentially with the popularity of microblogging services. With a large number of messages appearing in user interfaces, it hinders user accessibility to useful information buried in disorganized, incomplete, and unstructured text messages. In order to enhance user accessibility, we propose to aggregate related microblogging messages into clusters and automatically assign them semantically meaningful labels. However, a distinctive feature of microblogging messages is that they are much shorter than conventional text documents. These messages provide inadequate term co occurrence information for capturing semantic associations. To address this problem, we propose a novel framework for organizing unstructured microblogging messages by transforming them to a semantically structured representation. The proposed framework first captures informative tree fragments by analyzing a parse tree of the message, and then exploits external knowledge bases (Wikipedia and WordNet) to enhance their semantic information. Empirical evaluation on a Twitter dataset shows that our framework significantly outperforms existing state-of-the-art methods.

#index 1642281
#* Imbalanced sentiment classification
#@ Shoushan Li;Guodong Zhou;Zhongqing Wang;Sophia Yat Mei Lee;Rangyang Wang
#t 2011
#c 1
#% 815915
#% 843876
#% 854646
#% 1261576
#% 1271973
#% 1338678
#% 1470682
#% 1481569
#% 1484312
#! Sentiment classification has undergone significant development in recent years. However, most existing studies assume the balance between negative and positive samples, which may not be true in reality. In this paper, we investigate imbalanced sentiment classification instead. In particular, a novel clustering-based stratified under-sampling framework and a centroid-directed smoothing strategy are proposed to address the imbalanced class and feature distribution problems respectively. Evaluation across different datasets shows the effectiveness of both the under-sampling framework and the smoothing strategy in handling the imbalanced problems in real sentiment classification applications.

#index 1642282
#* The where in the tweet
#@ Wen Li;Pavel Serdyukov;Arjen P. de Vries;Carsten Eickhoff;Martha Larson
#t 2011
#c 1
#% 869500
#% 907558
#% 1040837
#% 1399992
#% 1482254
#% 1573369
#% 1858640
#! Twitter is a widely-used social networking service which enables its users to post text-based messages, so-called tweets. POI tags on tweets can show more human-readable high-level information about a place rather than just a pair of coordinates. In this paper, we attempt to predict the POI tag of a tweet based on its textual content and time of posting. Potential applications include accurate positioning when GPS devices fail and disambiguating places located near each other. We consider this task as a ranking problem, i.e., we try to rank a set of candidate POIs according to a tweet by using language and time models. To tackle the sparsity of tweets tagged with POIs, we use web pages retrieved by search engines as an additional source of evidence. From our experiments, we find that users indeed leak some information about their accurate locations in their tweets.

#index 1642283
#* Question identification on twitter
#@ Baichuan Li;Xiance Si;Michael R. Lyu;Irwin King;Edward Y. Chang
#t 2011
#c 1
#% 660658
#% 1074109
#% 1384287
#% 1484370
#% 1523411
#% 1523928
#! In this paper, we investigate the novel problem of automatic question identification in the microblog environment. It contains two steps: detecting tweets that contain questions (we call them "interrogative tweets") and extracting the tweets which really seek information or ask for help (so called "qweets") from interrogative tweets. To detect interrogative tweets, both traditional rule-based approach and state-of-the-art learning-based method are employed. To extract qweets, context features like short urls and Tweet-specific features like Retweets are elaborately selected for classification. We conduct an empirical study with sampled one hour's English tweets and report our experimental results for question identification on Twitter.

#index 1642284
#* OpinioNetIt: understanding the opinions-people network for politically controversial topics
#@ Rawia Awadallah;Maya Ramanath;Gerhard Weikum
#t 2011
#c 1
#% 301241
#% 956564
#% 1127964
#% 1166537
#% 1450945
#! The wikileaks documents or the economic crises in Ireland and Portugal are some of the controversial topics being played on the news everyday. Each of these topics has many different aspects, and there is no absolute, simple truth in answering questions such as: should the EU guarantee the financial stability of each member country, or should the countries themselves be solely responsible? To understand the landscape of opinions, it would be helpful to know which politician or other stakeholder takes which position - support or opposition - on these aspects of controversial topics. In this paper, we describe our system, named OpinioNetIt (pronounced similar to "opinionated"), which aims to automatically derive a map of the opinions-people network from news and other Web documents. We build this network as follows. First, we make use of a small number of generic seeds to identify controversial phrases from text. These phrases are then clustered and organized into a hierarchy of topics. Second, opinion holders are identified for each topic and their opinions (either supporting or opposing the topic) are extracted. Third, the known topics and people are used to construct a lexicon phrases indicating support or opposition. Finally, the lexicon is uses to identify more opinion holders, opinions and topics. Our system currently consists of approximately 30000 person-opinion-topic triples. Our evaluation shows that OpinioNetIt has high accuracy.

#index 1642285
#* Predicting the uncertainty of sentiment adjectives in indirect answers
#@ Mitra Mohtarami;Hadi Amiri;Man Lan;Chew Lim Tan
#t 2011
#c 1
#% 741442
#% 746885
#% 815915
#% 1471196
#% 1471444
#% 1472110
#% 1472195
#! Opinion question answering (QA) requires automatic and correct interpretation of an answer relative to its question. However, the ambiguity that often exists in the question-answer pairs causes complexity in interpreting the answers. This paper aims to infer yes/no answers from indirect yes/no question-answer pairs (IQAPs) that are ambiguous due to the presence of ambiguous sentiment adjectives. We propose a method to measure the uncertainty of the answer in an IQAP relative to its question. In particular, to infer the yes or no response from an IQAP, our method employs antonyms, synonyms, word sense disambiguation as well as the semantic association between the sentiment adjectives that appear in the IQAP. Extensive experiments demonstrate the effectiveness of our method over the baseline.

#index 1642286
#* Sentiment classification via l2-norm deep belief network
#@ Tao Liu;Minghui Li;Shusen Zhou;Xiaoyong Du
#t 2011
#c 1
#% 815915
#% 854646
#% 1127964
#% 1330516
#! Automatic analysis of sentiments expressed in large scale online reviews is very important for intelligent business applications. Sentiment classification is the most popular task of sentiment analysis, which is more challenging than traditional topic-based text classification. Basic features, such as vocabulary words, are not enough to classify sentiments well. Deep Belief Network (DBN) is introduced to discover more abstract features of sentiments. To capture full information of the features, large-size network can be constructed, but at the same time, large-size network tends to over fit the training data and even noise, which will reduce the generalization ability of the network. In this paper, L2-norm Deep Belief Network (L2DBN) is proposed, which uses L2-norm regularization to optimize the network parameters of DBN. L2DBN is first initialized by an unsupervised layer-wise training algorithm, and then fine-tuned by a supervised procedure. Network parameters are optimized using both classification loss and network complexity. Experimental results show that the proposed L2DBN outperforms the state-of-the-art method and the basic DBN on golden, noisy and heterogeneous datasets.

#index 1642287
#* Domain customization for aspect-oriented opinion analysis with multi-level latent sentiment clues
#@ Honglei Guo;Huijia Zhu;Zhili Guo;Zhong Su
#t 2011
#c 1
#% 78171
#% 722822
#% 722904
#% 815915
#% 938687
#% 1227700
#% 1292576
#% 1400008
#% 1482298
#! Aspect-oriented opinion mining detects the reviewers' sentiment orientation (e.g. positive, negative or neutral) towards different product-features. Domain customization is a big challenge for opinion mining due to the accuracy loss across domains. In this paper, we show our experiences and lessons learned in the domain customization for the aspect-oriented opinion analysis system OpinionIt. We present a customization method for sentiment classification with multi-level latent sentiment clues. We first construct Latent Semantic Association model to capture latent association among product-features from the unlabeled corpus. Meanwhile, we present an unsupervised method to effectively extract various domain-specific sentiment clues from the unlabeled corpus. In the customization, we tune the sentiment classifier on the labeled source domain data by incorporating the multi-level latent sentiment clues (e.g. latent association among product-features, domain-specific and generic sentiment clues). Experimental results show that the proposed method significantly reduces the accuracy loss of sentiment classification without any labeled target domain data.

#index 1642288
#* Accurate information extraction for quantitative financial events
#@ Hassan H. Malik;Vikas S. Bhardwaj;Huascar Fiorletta
#t 2011
#c 1
#% 577298
#% 742161
#% 742371
#% 1166537
#% 1592779
#! In this paper, we present a novel financial event extraction system that achieves very high extraction quality by combining the outcome of statistical classifiers with a set of rules. Using expert-annotated press releases as training data, and novel feature generation schemes, our system learns multiple binary classifiers for each "slot" in a financial event. At runtime, common parsing and search indexing methods are used to normalize incoming press releases and to identify candidate event "slots". Rules are applied on candidates that satisfy a combination of classifiers, and the system confidence on extracted events is estimated using a unique confidence model learned from training data. We present results of experiments performed on European corporate press releases for extracting dividend events, and show that our system achieves a precision of 96% and a recall of 79%.

#index 1642289
#* A machine-learned proactive moderation system for auction fraud detection
#@ Liang Zhang;Jie Yang;Wei Chu;Belle Tseng
#t 2011
#c 1
#% 73441
#% 224755
#% 251365
#% 1073973
#! Online auction and shopping are gaining popularity with the growth of web-based eCommerce. Criminals are also taking advantage of these opportunities to conduct fraudulent activities against honest parties with the purpose of deception and illegal profit. In practice, proactive moderation systems are deployed to detect suspicious events for further inspection by human experts. Motivated by real-world applications in commercial auction sites in Asia, we develop various advanced machine learning techniques in the proactive moderation system. Our proposed system is formulated as optimizing bounded generalized linear models in multi-instance learning problems, with intrinsic bias in selective labeling and massive unlabeled samples. In both offline evaluations and online bucket tests, the proposed system significantly outperforms the rule-based system on various metrics, including area under ROC (AUC), loss rate of labeled frauds and customer complaints. We also show that the metrics of loss rates are more effective than AUC in our cases.

#index 1642290
#* Simultaneously improving CSAT and profit in a retail banking organization
#@ Sameep Mehta;Ullas Nambiar;Vishal Batra;Sumit Negi;Prasad Deshpande;Gyana Praija
#t 2011
#c 1
#% 86465
#! Customer satisfaction (CSAT) is the key driver for retention and growth in retail banking and several techniques have been applied by banks to achieve this. For instance, banks in emerging markets with high footfall in branches have gone beyond the traditional approach of segmenting customers and services to optimizing the wait time for customers visiting the bank's branch. While this approach has significantly improved service quality, it has also added a new dimension in the service quality metric : pro-actively identify and address customer needs for (i) efficient banking experience and (ii) enhancing profit by selling additional services to existing customer. In this paper we present a system that addresses the challenge involved in providing better service to retail banking customer while ensuring that a larger share of customer's wallet comes to the branch. We do this by combining predictive analytics, scheduling and process optimization techniques.

#index 1642291
#* Coarse-to-fine classification via parametric and nonparametric models for computer-aided diagnosis
#@ Le Lu;Meizhu Liu;Xiaojing Ye;Shipeng Yu;Heng Huang
#t 2011
#c 1
#% 593047
#% 770866
#% 814023
#% 883981
#% 1073973
#% 1270185
#! Classification is one of the core problems in Computer-Aided Diagnosis (CAD), targeting for early cancer detection using 3D medical imaging interpretation. High detection sensitivity with desirably low false positive (FP) rate is critical for a CAD system to be accepted as a valuable or even indispensable tool in radiologists' workflow. Given various spurious imagery noises which cause observation uncertainties, this remains a very challenging task. In this paper, we propose a novel, two-tiered coarse-to-fine (CTF) classification cascade framework to tackle this problem. We first obtain classification-critical data samples (e.g., implicit samples on the decision boundary) extracted from the holistic data distributions using a robust parametric model (e.g., [13]); then we build a graph-embedding based nonparametric classifier on sampled data, which can more accurately preserve or formulate the complex classification boundary. These two steps can also be considered as effective "sample pruning" and "feature pursuing + kNN/template matching", respectively. Our approach is validated comprehensively in colorectal polyp detection and lung nodule detection CAD systems, as the top two deadly cancers, using hospital scale, multi-site clinical datasets. The results show that our method achieves overall better classification/detection performance than existing state-of-the-art algorithms using single-layer classifiers, such as the support vector machine variants [17], boosting [15], logistic regression [11], relevance vector machine [13], k-nearest neighbor [9] or spectral projections on graph [2].

#index 1642292
#* Exploratory search over social-medical data
#@ Haggai Roitman;Sivan Yogev;Yevgenia Tsimerman;Dae Won Kim;Yossi Mesika
#t 2011
#c 1
#% 577273
#% 857478
#% 1292521
#% 1467778
#% 1491122
#% 1643133
#! In this demo we shall present the IBM Patient Empowerment System (PES), and more specifically, its social-medical discovery sub-system. Social and medical data are represented using entities and relationships and are explored using a combination of expressive, yet intuitive, query language, faceted search, and ER graph navigation. While this demonstration focuses on the healthcare domain, the underlining search technology is generic and can be utilized in many other domains. Therefore, this demo has two main contributions. First, we present a novel entity-relationship indexing and retrieval solution, and discuss its implementation challenges. Second, the demonstration depicts a practical entity-relationship discovery technology in a real domain setting within a real IBM system.

#index 1642293
#* Black swan: augmenting statistics with event data
#@ Johannes Lorey;Felix Naumann;Benedikt Forchhammer;Andrina Mascher;Peter Retzlaff;Armin ZamaniFarahani;Soeren Discher;Cindy Faehnrich;Stefan Lemme;Thorsten Papenbrock;Robert Christoph Peschel;Stephan Richter;Thomas Stening;Sven Viehmeier
#t 2011
#c 1
#% 481290
#% 800590
#% 1021641
#% 1455643
#! A large number of statistical indicators (GDP, life expectancy, income, etc.) collected over long periods of time as well as data on historical events (wars, earthquakes, elections, etc.) are published on the World Wide Web. By augmenting statistical outliers with relevant historical occurrences, we provide a means to observe (and predict) the influence and impact of events. The vast amount and size of available data sets enable the detection of recurring connections between classes of events and statistical outliers with the help of association rule mining. The results of this analysis are published at http://www.blackswanevents.org and can be explored interactively.

#index 1642294
#* A data mining system based on SQL queries and UDFs for relational databases
#@ Carlos Ordonez;Carlos Garcia-Alvarado
#t 2011
#c 1
#% 300213
#% 845220
#% 1235700
#% 1326702
#% 1376243
#% 1426636
#% 1482609
#% 1512993
#% 1535426
#! Most research on data mining has proposed algorithms and optimizations that work on flat files, outside a DBMS, mainly due to the following reasons. It is easier to develop efficient algorithms in a traditional programming language. The integration of data mining algorithms into a DBMS is difficult given its relational model foundation and system architecture. Moreover, SQL may be slow and cumbersome for numerical analysis computations. Therefore, data mining users commonly export data sets outside the DBMS for data mining processing, which creates a performance bottleneck and eliminates important data management capabilities such as query processing and security, among others (e.g. concurrency control and fault tolerance). With that motivation in mind, we developed a novel system based on SQL queries and User-Defined Functions (UDFs) that can directly analyze relational tables to compute statistical models, storing such models as relational tables as well. Most algorithms have been optimized to reduce the number of passes on the data set. Our system can analyze large and high dimensional data sets faster than external data mining tools.

#index 1642295
#* Data-thirsty business analysts need SODA: search over data warehouse
#@ Lukas Blunschi;Claudio Jossen;Donald Kossmann;Magdalini Mori;Kurt Stockinger
#t 2011
#c 1
#% 660011
#% 839172
#% 960243
#% 960259
#% 993987
#% 1581893
#! Querying large data warehouses is very hard for non-tech savvy business users. Deep technical knowledge of both SQL as well as the schema of the database is required in order to build correct queries and to come up with new business insights. In this paper we introduce a novel system called SODA (Search Over DAta Warehouse) that bridges the gap between the business world and the IT world by enabling extended keyword search in a data warehouse. SODA uses metadata information, DBpedia entries as well as base data to generate SQL to allow intuitive exploration of the data. The process of query classification, query graph generation and SQL generation is visualized to provide the analysts with information on how the query results are produced. Experiments with real data of a global financial institution comprising around 300 tables showed promising results.

#index 1642296
#* An integrated environment for semantic knowledge work
#@ Aba-Sah Dadzie;Victoria Uren;Ziqi Zhang;Philip Webster
#t 2011
#c 1
#% 1152485
#% 1401651
#! In this demonstration, we will present a semantic environment called the K-Box. The K-Box supports the lightweight integration of knowledge tools, with a focus on semantic tools, but with the flexibility to integrate natural language and conventional tools. We discuss the implementation of the framework, and two existing applications, including details of a new application for developers of semantic workflows. The demonstration will be of interest to developers and researchers of ontology-based knowledge management systems, and semantic desktops, and to analysts working with cross-media information.

#index 1642297
#* Editing knowledge resources: the wiki way
#@ Francesco Ronzano;Andrea Marchetti;Maurizio Tesconi
#t 2011
#c 1
#% 1039356
#% 1409913
#% 1547216
#% 1696336
#% 1696351
#! The creation, customization, and maintenance of knowledge resources are essential for fostering the full deployment of Language Technologies. The definition and refinement of knowledge resources are time- and resource-consuming activities. In this paper we explore how the Wiki paradigm for online collaborative content editing can be exploited to gather massive social contributions from common Web users in editing knowledge resources. We discuss the Wikyoto Knowledge Editor, also called Wikyoto. Wikyoto is a collaborative Web environment that enables users with no knowledge engineering background to edit the multilingual network of knowledge resources exploited by KYOTO, a cross-lingual text mining system developed in the context of the KYOTO European Project.

#index 1642298
#* Marco Polo: a system for brand-based shopping and exploration
#@ Nish Parikh;Neel Sundaresan
#t 2011
#c 1
#% 939794
#% 1130842
#% 1536581
#! In today's world, brand based shopping is popular especially in product lines like clothing and shoes, appliances, and electronics. Because of the importance of brands while shopping, it has become important for online shopping portals to consider brand loyalty and brand preferences of users. In this paper, we describe a system designed for brand-based shopping and exploration. The system is built by analyzing a large query set consisting of 115M queries from eBay.com -- a vibrant marketplace with more than 95M active users. The system allows brand-pivoted exploration of inventory. It allows exploration and purchase of substitute branded goods (e.g. Sony camcorder for Canon camcorder) and complementary branded merchandise (e.g. Lego castle set for Lego train station set).

#index 1642299
#* Jasmine: a real-time local-event detection system based on geolocation information propagated to microblogs
#@ Kazufumi Watanabe;Masanao Ochi;Makoto Okabe;Rikio Onai
#t 2011
#c 1
#% 766441
#% 1040837
#% 1071087
#% 1269909
#% 1298864
#% 1400018
#% 1478990
#% 1482254
#% 1592024
#! We propose a system for detecting local events in the real-world using geolocation information from microblog documents. A local event happens when people with a common purpose gather at the same time and place. To detect such an event, we identify a group of Twitter documents describing the same theme that were generated within a short time and a small geographic area. Timestamps and geotags are useful for finding such documents, but only 0.7% of documents are geotagged and not sufficient for this purpose. Therefore, we propose an automatic geotagging method that identifies the location of non-geotagged documents. Our geotagging method successfully increased the number of geographic groups by about 115 times. For each group of documents, we extract co-occurring terms to identify its theme and determine whether it is about an event. We subjectively evaluated the precision of our detected local events and found that it had 25.5% accuracy. These results demonstrate that our system can detect local events that are difficult to identify using existing event detection methods. A user can interactively specify the size of a desired event by manipulating the parameters of date, area size, and the minimum number of Twitter users associated with the location. Our system allows users to enjoy the novel experience of finding a local event happening near their current location in real time.

#index 1642300
#* Scalable similarity search of timeseries with variable dimensionality
#@ Omar U. Florez;Curtis Dyreson
#t 2011
#c 1
#% 427199
#% 795273
#% 824705
#% 977990
#% 1044456
#% 1083693
#% 1375789
#! Timeseries can be similar in shape but differ in length. For example, the sound waves produced by the same word spoken twice have roughly the same shape, but one may be shorter in duration. Stream data mining, approximate querying of image and video databases, data compression, and near duplicate detection are applications that need to be able to classify or cluster such timeseries, and to search for and rank timeseries that are similar to a chosen timeseries. We demonstrate software for clustering and performing similarity search in databases of timeseries data, where the timeseries have high and variable dimensionality. Our demonstration uses Timeseries Sensitive Hashing (TSH)[3] to index the timeseries. TSH adapts Locality Sensitive Hashing (LSH), which is an approximate algorithm to index data points in a d-dimensional space under some (e.g., Euclidean) distance function. TSH, unlike LSH, can index points that do not have the same dimensionality. As examples of the potential of TSH, the demonstration will index and classify timeseries from an image database and timeseries describing human motion extracted from a video stream and a motion capture system.

#index 1642301
#* RoSeS: a continuous query processor for large-scale RSS filtering and aggregation
#@ Jordi Creus;Bernd Amann;Nicolas Travers;Dan Vodislav
#t 2011
#c 1
#% 36117
#% 282600
#% 1034727
#% 1583596
#! We present RoSeS, a running system for large-scale content-based RSS feed filtering and aggregation. The implementation of RoSeS is based on standard database concepts like declarative query languages, views and multi-query optimization. Users create personalized feeds by defining and composing content-based filtering and aggregation queries on collections of RSS feeds. These queries are translated into continuous multi-query execution plans which are optimized using a new cost-based multi-query optimization strategy.

#index 1642302
#* Conkar: constraint keyword-based association discovery
#@ Mo Zhou;Yifan Pan;Yuqing Wu
#t 2011
#c 1
#% 577372
#% 869503
#% 956574
#% 960259
#% 1098424
#% 1426512
#% 1493778
#% 1642274
#! In many domains, such as bioinformatics, cheminformatics, health informatics and social networks, data can be represented naturally as labeled graphs. To address the increasing needs in discovering interesting associations between entities in such data graphs, especially under complicated keyword-based and structural constraints, we introduce Conkar (Constrained Keyword-based Association DiscoveRy) System. Conkar is the first system for discovering constrained acyclic paths (CAP) in graph data under keyword-based constraints, with the highlight being the set of quantitative constraint metrics that we proposed, including coverage and relevance. We will demonstrate the key features of Conkar: powerful and userfriendly query specification, efficient query evaluation, flexible and on-demand result ranking, visual result display, as well as an insight tour on our novel CAP query evaluation algorithms.

#index 1642303
#* Interactive reasoning in uncertain RDF knowledge bases
#@ Timm Meiser;Maximilian Dylla;Martin Theobald
#t 2011
#c 1
#% 94459
#% 754068
#% 810098
#% 850430
#% 893167
#% 960292
#% 1022236
#% 1092530
#% 1127378
#% 1127402
#% 1206732
#% 1206735
#% 1270258
#% 1291123
#% 1355026
#% 1426558
#% 1523866
#% 1560247
#% 1573237
#! Recent advances in Web-based information extraction have allowed for the automatic construction of large, semantic knowledge bases, which are typically captured in RDF format. The very nature of the applied extraction techniques however entails that the resulting RDF knowledge bases may face a significant amount of incorrect, incomplete, or even inconsistent (i.e., uncertain) factual knowledge, which makes query answering over this kind of data a challenge. Our reasoner, coined URDF, supports SPARQL queries along with rule-based, first-order predicate logic to infer new facts and to resolve data uncertainty over millions of RDF triplets directly at query time. We demonstrate a fully interactive reasoning engine, combining a Java-based reasoning backend and a Flash-based visualization frontend in a dynamic client-server architecture. Our visualization frontend provides interactive access to the reasoning backend, including tasks like exploring the knowledge base, rule-based and statistical reasoning, faceted browsing of large query graphs, and explaining answers through lineage.

#index 1642304
#* Fu-Finder: a game for studying querying behaviours
#@ Carly O'Neil;James Purvis;Leif Azzopardi
#t 2011
#c 1
#% 597946
#% 751818
#% 1065099
#% 1074200
#% 1227633
#% 1227715
#% 1292493
#! Usually the focus of evaluation within Information Retrieval has been placed largely upon the system. However, the individual user and their submitted queries are typically the greatest source of variation in the search process. This demonstration paper presents Fu-Finder, a fun and enjoyable game that measures the user's querying abilities (or search-fu). This game provides useful data for the study of user querying behaviour and assesses how well users can find specific web pages using different search engines.

#index 1642305
#* PDFMeat: managing publications on the semantic desktop
#@ David Aumüller;Erhard Rahm
#t 2011
#c 1
#% 333679
#% 614036
#% 893089
#% 893119
#% 1074073
#% 1126562
#% 1214660
#% 1378482
#% 1483644
#! Researchers maintain bibliographies and extensive sets of PDF files of scholarly publications on their desktop. The lack of proper metadata of downloaded PDFs makes this task a tedious one. With PDFMeat we present a solution to automatically determine publication metadata for scholarly papers within the user's desktop environment and link the metadata to the files. PDFMeat effectively matches local full texts to an online repository. In an evaluation for more than 2.000 diverse PDF files it worked highly reliable and showed excellent accuracy of up to 98 percent. We demonstrate PDFMeat for different sets of papers, highlighting the semantic integration and use of the retrieved metadata within the file browser of the desktop environment.

#index 1642306
#* MEMSCALE: in-cluster-memory databases
#@ Héctor Montaner;Federico Silla;Holger Fröning;José Duato
#t 2011
#c 1
#% 769155
#! We have developed a new memory architecture for clusters that allows automatic access from any processor to any memory module in the cluster completely by hardware. Thus, with a single assembly instruction a processor can retrieve (or update) a memory location in a remote node. The efficiency of this new paradigm makes it possible to speed-up the execution of shared-memory applications with very large memory footprints by running them across the entire cluster, thus providing them a true shared-memory environment (contrary to the emulation typically carried out by software-based distributed shared memory). This new memory architecture, referred to as MEMSCALE, opens up a new frontier for memory-hungry applications. In this paper we focus on in-memory databases and show how this target application can be boosted by our memory architecture, which can virtually provide unlimited memory resources to it. In the demo presented in this paper we show the advantages of our architecture by means of a prototype cluster. We configure two cluster sizes, 16 and 32 nodes, to analyze throughput scalability and latency worsening, to extrapolate these metrics to bigger clusters, and to show the benefits of our technology compared to other alternatives like SSD-based databases. Moreover, we also show the easiness of use of our architecture by explaining how we ported MySQL Server to our prototype cluster. Finally, the possibility of executing queries in any processor of the cluster during the live demo will show the audience how our system aggregates the advantages of the scale out and scale up approaches for database server growing.

#index 1642307
#* H-DB: a hybrid quantitative-structural sql optimizer
#@ Lucantonio Ghionna;Gianluigi Greco;Francesco Scarcello
#t 2011
#c 1
#% 320062
#% 375577
#% 756494
#% 847068
#% 942358
#% 993437
#% 1224352
#! Structural decomposition methods are query optimization methods specifically conceived in the database theory community to efficiently answer (near-)acyclic queries. We propose to demonstrate H-DB, an SQL query optimizer that combines classical quantitative optimization techniques with such structural decomposition methods, which so far have been just analyzed from the theoretical viewpoint. The system provides support to optimizing SQL queries with arbitrary output variables, aggregate operators, ORDER BY statements, and nested queries. H-DB can be put on top of any existing database management system supporting JDBC technology, by transparently interacting/replacing its standard query optimization module. However, to push at maximum its optimization capabilities, H-DB should be coupled with an ad-hoc physical semi-join operator, which (as a relevant example) we implemented and integrated within the PostgreSQL database management system.

#index 1642308
#* Health conversational system based on contextual matching of community-driven question-answer pairs
#@ Wilson Wong;John Thangarajah;Lin Padgham
#t 2011
#c 1
#% 1074109
#% 1074163
#% 1471303
#% 1558874
#% 1565144
#! More and more people are turning to the World Wide Web for learning and sharing information about their health using search engines, forums and question answering systems. In this demonstration, we look at a new way of delivering health information to the end-users via coherent conversations. The proposed conversational system allows the end-users to vaguely express and gradually refine their information needs using only natural language questions or statements as input. We provide example scenarios in this demonstration to illustrate the inadequacies of current delivery mechanisms and highlight the innovative aspects of the proposed conversational system.

#index 1642309
#* Annotating knowledge work lifelog: term extraction from sensor and operation history
#@ Masayuki Okamoto;Nayuko Watanabe;Shinichi Nagano;Kenta Cho
#t 2011
#c 1
#% 722904
#% 757280
#% 1083687
#% 1169576
#% 1286104
#% 1482483
#! We present a system that supports review of a knowledge work lifelog as an activity history. Since knowledge workers often review their own activity histories, gathering each user's activities on his/her terminal as a lifelog is a promising approach. However, readability of the stored lifelog is a large problem of lifelog-based application. We propose a term extraction method to add annotation labels to the stored lifelog for supporting knowledge workers, exploiting text data acquired from desktop activities. Our prototype system monitors a user's desktop activities after combining raw events, and then extracts possible annotation labels with LDA and C-value techniques from documents and text data in sensor events. In this paper, we introduce a lifelogging module and a lifelog annotation method based on term extraction techniques. According to an empirical evaluation for three weeks, we found that the current method is useful for one-week review.

#index 1642310
#* Entity timelines: visual analytics and named entity evolution
#@ Arturas Mazeika;Tomasz Tylenda;Gerhard Weikum
#t 2011
#c 1
#% 434614
#% 956564
#% 1108676
#% 1560258
#! The constantly evolving Web reflects the evolution of society. Knowledge about entities (people, companies, political parties, etc.) evolves over time. Facts add up (e.g., awards, lawsuits, divorces), change (e.g., spouses, CEOs, political positions), and even cease to exist (e.g., countries split into smaller or join into bigger ones). Analytics of the evolution of the entities poses many challenges including extraction, disambiguation, and canonization of entities from large text collections as well as introduction of specific analysis and interactivity methods for the evolving entity data. In this demonstration proposal, we consider a novel problem of the evolution of named entities. To this end, we have extracted, disambiguated, canonicalized, and connected named entities with the YAGO ontology. To analyze the evolution we have developed a visual analytics system. Careful preprocessing and ranking of the ontological data allowed us to propose wide range of effective interactions and data analysis techniques including advanced filtering, contrasting timeliness of entities and drill down/roll up evolving data.

#index 1642311
#* PICASSO: automated soundtrack suggestion for multi-modal data
#@ Aleksandar Stupar;Sebastian Michel
#t 2011
#c 1
#% 762054
#% 952823
#% 997224
#% 1021652
#% 1279931
#% 1328172
#% 1484454
#% 1598405
#% 1857842
#! We demonstrate PICASSO, a novel approach to soundtrack recommendation. Given text, video, or image documents, PICASSO selects the best fitting music pieces, out of a given set of files, for instance, a user's personal mp3 collection. This task, commonly referred to as soundtrack suggestion, is non-trivial as it requires a lot of human attention and a good deal of experience, with master pieces distinguished, e.g., with the Academy Award for Best Original Score. We put forward PICASSO to solve this task in a fully automated way. We address the problem by extracting the required information, in form of music/screenshot samples, from available contemporary movies, making the training set easily obtainable. The training set is further extended with information acquired from movie scripts and subtitles, giving us a richer description of the action and atmosphere expressed in a particular movie scene. Although the number of applications for this approach is very large, we focus on two selected applications. First, we consider recommendation of the soundtrack for the slide show generation based on the given set of images. Second, we consider recommending a soundtrack as the background music for given audio books.

#index 1642312
#* P2Prec: a social-based P2P recommendation system
#@ Fady Draidi;Esther Pacitti;Didier Parigot;Guillaume Verger
#t 2011
#c 1
#% 169777
#% 722904
#% 813966
#% 985402
#% 1245946
#% 1498004
#% 1612164
#! P2Prec is a social-based P2P recommendation system for large-scale content sharing that leverages content-based and social-based recommendation. The main idea is to recommend high quality documents related to query topics and contents held by useful friends (of friends) of the users, by exploiting friendship networks. We have implemented a prototype of P2Prec using the Shared-Data Overlay Network (SON), an open source development platform for P2P networks using web services, JXTA and OSGi. In this paper, we describe the demo of P2Prec's main services (installing P2Prec peers, initializing peers, gossiping topics of interest among friends, key-word querying for contents) using our prototype implemented as an application of SON.

#index 1642313
#* Computational geography
#@ Vanessa Murdock;Gary Gale
#t 2011
#c 1
#! As the industry moves to personalization and mobility, users expect their applications to be location savvy, and relevant to their lives in increasing detail. While we can pinpoint a user at a location within 700 meters with just their IP address, and within a meter with their GPS-enabled mobile phone, we fall short when it comes to understanding their geographic context. A person's geographic context includes their current and previous location, the things that surround them, their activity in a given place, as well as their thoughts and feelings in that place. Understanding this context allows us to personalize their experience and refine their interactions with an application, on a hyper-local level.

#index 1642314
#* Large-scale array analytics: taming the data tsunami
#@ Peter Baumann
#t 2011
#c 1
#! Never before in history mankind has collected data at the rates we face today. Alone in 2002, an estimated 403 Petabyte of data has been acquired, equivalent to all printed information ever created before. Earth orbiting satellites, as well as ground, airborne, and underwater sensors, space observatories scan their environment at unprecedented resolutions, giving rise to "Big Science". The same holds for the life sciences where genomic data, high-resolution scans, and other modalities are collected in steadily increasing streams. Social network analysis, OLAP, and stock exchange trading represent further examples, the latter involving real-time correlation of thousands of ticker time series resulting in Terabytes of data to be analysed per single run. Summarized under Large-Scale Analytics we are witnessing an exploding demand for flexible access to massive volumes of scientific and business data sets. Arguably a large class of these massive data is represented by multi-dimensional arrays. Consequently, large arrays pose new challenges to data modelling, querying, optimization, and maintenance -- in short: we need Large-Scale Array Analytics. This tutorial introduces to the topic from a database perspective. Aspects addressed include modelling, query languages, query optimization and parallelization, and storage management. High emphasis will be devoted to applications in "Big Science", particularly geo, space, and life sciences; real-life use cases will be presented and discussed which stem from our 15 years of experience with the open-source rasdaman array DBMS and our work on geo raster service standardization. We will highlight requirements, achievements, open research issues, and avenues for future research. Discussion will make use of real-life examples, many of which Internet connected participants can replay hands-on.

#index 1642315
#* Large-scale information retrieval experimentation with terrier
#@ Rodrygo L.T. Santos;Richard McCreadie;Vassilis Plachouras
#t 2011
#c 1
#! This tutorial aims to provide a practical introduction to conducting large-scale information retrieval (IR) experiments, using Terrier (http://terrier.org) as an experimentation platform. Written in Java, Terrier provides an open-source, feature-rich, flexible, and robust environment for large-scale IR experimentation. This tutorial will cover the experimentation process end-to-end, from configuring Terrier to a particular experimental setting, to efficiently indexing a document corpus and retrieving from it, and to evaluating the outcome. Moreover, it will describe how to use and extend the platform to one's own needs, and will be illustrated by practical research-driven examples. As a half-day tutorial, it will be split into two major sessions, with each session comprising both background information and practical demonstrations. In the first session, we will provide an overview of several aspects of large-scale IR experimentation, spanning areas such as indexing, data structures, query languages, and advanced retrieval models, and how these are implemented within Terrier. In the second session, we will discuss how to extend Terrier to conduct one's own experiments in a large-scale setting, including how to facilitate the evaluation of non-standard IR tasks through crowdsourcing. The practical demonstrations will cover recent use cases identified from Terrier's online discussion forum, so as to provide attendees with concrete examples of what can be done within Terrier.

#index 1642316
#* Statistical information retrieval modelling: from the probability ranking principle to recent advances in diversity, portfolio theory, and beyond
#@ Jun Wang;Kevyn Collins-Thompson
#t 2011
#c 1
#! Statistical modelling of Information Retrieval (IR) systems is a key driving force in the development of the IR field. The goal of this tutorial is to provide a comprehensive and up-to-date introduction to statistical IR modelling. We take a fresh and systematic perspective from the viewpoint of portfolio theory of IR and risk management. A unified treatment and new insights will be given to reflect the recent developments of considering the ranked retrieval results as a whole. Recent research progress in diversification, risk management, and portfolio theory will be covered, in addition to classic methods such as Maron and Kuhns' Probabilistic Indexing, Robertson-Sparck Jones model (and the resulting BM25 formula) and language modelling approaches. The tutorial also reviews the resulting practical algorithms of risk-aware query expansion, diverse ranking, IR metric optimization as well as their performance evaluations. Practical IR applications such as web search, multimedia retrieval, and collaborative filtering are also introduced, as well as discussion of new opportunities for future research and applications that intersect among information retrieval, knowledge management, and databases.

#index 1642317
#* Web-based open-domain information extraction
#@ Marius Pasca
#t 2011
#c 1
#! This tutorial provides an overview of extraction methods developed in the area of Web-based open-domain information extraction, whose purpose is the acquisition of open-domain classes, instances and relations from Web text. The extraction methods operate over unstructured or semi-structured text. They take advantage of weak supervision provided in the form of seed examples or small amounts of annotated data, or draw upon knowledge already encoded within resources created strictly by experts or collaboratively by users. The tutorial teaches the audience about existing resources that include instances and relations; details of methods for extracting such data from structured and semi-structured text available on the Web; and strengths and limitations of resources extracted from text as part of recent literature, with applications in knowledge discovery and information retrieval.

#index 1642318
#* Advances in data stream mining for mobile and ubiquitous environments
#@ Shonali Krishnaswamy;Joao Gama;Mohamed Medhat Gaber
#t 2011
#c 1
#% 1538039
#! The tutorial presents the state-of-the-art in mobile and ubiquitous data stream mining and discusses open research problems, issues, and challenges in this area.

#index 1642319
#* Information diffusion in social networks: observing and affecting what society cares about
#@ Divyakant Agrawal;Ceren Budak;Amr El Abbadi
#t 2011
#c 1
#% 729923
#% 1130857
#% 1214671
#% 1298864
#% 1407359
#% 1536509
#% 1560421
#% 1561562
#% 1606346
#! Information diffusion in social networks provide great opportunities for political and social change as well as societal education. Therefore understanding information diffusion in social networks is a critical research goal. This greater understanding can be achieved through data analysis, development of reliable models that can predict outcomes of social processes, and ultimately the creation of applications that can shape the outcome of these processes. In this tutorial, we aim to provide an overview of such recent research based on a wide variety of techniques such as optimization algorithms, data mining, data streams covering a large number of problems such as influence spread maximization, misinformation limitation and study of trends in online social networks.

#index 1642320
#* Information retrieval challenges in computational advertising
#@ Andrei Broder;Evgeniy Gabrilovich;Vanja Josifovski
#t 2011
#c 1

#index 1642321
#* Object ranking
#@ Roelof van Zwol;Srinivas Vadrevu
#t 2011
#c 1
#! Object ranking is an emerging discipline within information retrieval that is concerned with the ranking of objects, e.g. named entities and their attributes, in context of given a user query, or application. In this tutorial we will address the different aspects involved when building an object ranking system. We will present the state-of-the-art research in object ranking, as well as going into detail about our hands-on experiences when designing and developing the system for object ranking as it is in production at Yahoo! today. This allows for a unique mixture of research and development that will give the participants in-depth insights into the problem of object ranking. The focus of current Web search engines is to retrieve relevant documents on the Web, and more precisely documents that match with the query intent of the user. Some users are looking for specific information, while other just want to access rich media content (images, videos, etc.) or explore a topic. In the latter scenario, users do not have a fixed or pre-determined information need, but are using the search engine to discover information related to a particular object of interest. In this scenario one can say that the user is in a exploratory mode. To support users in their exploratory search the search engines are offering semantic search suggestions. In this tutorial, we will present a generic framework for ranking related objects. This framework ranks related entities according to two dimensions: a lateral dimension and a faceted dimension. In the lateral dimension, related entities are of the same nature as the entity queried (e.g. Barcelona and Madrid, or Angelina Jolie and Jessica Alba). In the faceted dimension, related entities are usually not of the same type as the queried entity, and refer to a specific aspect of the queried entity (e.g. Jennifer Aniston and the tvshow Friends). In this tutorial we will describe the process of building a Web-scale object ranking system. In particular we will address the construction of a knowledge base that forms the basis for the object ranking, and the generation of ranking features using external sources such as search engine query logs, photo annotations in Flickr, and tweets on Twitter. Next, we will discuss machine learned ranking models using an ensemble of pair-wise preference models, and address various aspects of object ranking, including multi-media extensions, vertical solutions, attribute-aware ranking, and the importance of freshness. Last but not least, we will address the evaluation methodologies involved to tune the performance of Web-scale object ranking strategies.

#index 1642322
#* Uncertain schema matching: the power of not knowing
#@ Avigdor Gal
#t 2011
#c 1

#index 1642323
#* DTMBIO 2011: international workshop on data and textmining in biomedical informatics
#@ Sophia Ananiadou;Doheon Lee;Shamkant Navathe;Min Song
#t 2011
#c 1
#% 1642894
#% 1642895
#% 1642896
#% 1642897
#% 1642898
#% 1642899
#% 1642900
#% 1642901
#% 1642902
#% 1642903
#% 1642904
#! ACM Fifth International Workshop on Data and Text Mining in Biomedical Informatics (DTMBIO 11) organizers are pleased to announce that the fifth DTMBIO will be held in conjunction with CIKM, one of the largest data and text mining conferences. While CIKM presents the state-of-the-art research in informatics with the primary focus on data and text mining, the main focus of DTMBIO is on biomedical informatics. DTMBIO delegates will bring forth interesting applications of up-to-date informatics in the context of biomedical research.

#index 1642324
#* BooksOnline'11: 4th workshop on online books, complementary social media, and crowdsourcing
#@ Gabriella Kazai;Carsten Eickhoff;Peter Brusilovsky
#t 2011
#c 1
#% 1642340
#% 1642341
#% 1642342
#% 1642343
#% 1642344
#% 1642345
#% 1642346
#% 1642347
#% 1642349
#% 1642350
#! The BooksOnline Workshop series aims to foster the discussion and exchange of research ideas towards addressing challenges and exploring opportunities around large collections of digital books and complementary media. The fourth workshop in the series, BooksOnline'11 pays special attention to the role of social media and the phenomena of crowdsourcing in the context of online books, which is expected to be key in defining new user experiences in digital libraries and on the Web. The workshop boasts a high quality program, including keynote addresses by Ville Miettinnen, CEO of Microtask and Adam Farquhar, Head of Digital Library Technology at The British Library. From the accepted papers two main themes became salient: 1) Information retrieval and information extraction methods focused on enhancing digital libraries, and 2) Studies and analyses of reading experience and behaviour. This paper provides an overview of the workshop and the accepted contributions.

#index 1642325
#* Detect'11: international workshop on DETecting and Exploiting Cultural diversiTy on the social web
#@ Sergej Sizov;Stefan Siersdorfer;Thomas Gottron;Philipp Sorg
#t 2011
#c 1
#% 1642685
#% 1642686
#% 1642688
#% 1642690
#% 1642691
#% 1642692
#% 1642694
#% 1642695

#index 1642326
#* 4th international workshop on patent information retrieval (PaIR'11)
#@ Mihai Lupu;Allan Hanbury;Andreas Rauber
#t 2011
#c 1
#! The 4th International Workshop on Patent Information Retrieval builds on the experiences of the first three workshops, to provide its participants an exciting, scientifically challenging and interactive event, where specific issues of patent retrieval may be put into the general context of Information Retrieval and Knowledge Management, in order to explore innovative solutions to new and old problems, but also to evaluate and adapt traditional or classic approaches to new problems. This year, we observe an increase in the use of standardized test collections in the contributions received, and, at the same time, new discussion points on how to make such standardized evaluation exercises more accessible to the larger IP community.

#index 1642327
#* Overview of the third international workshop on search and mining user-generated contents
#@ Ivan Cantador;José C. Cortizo;Francisco Carrero;Jose A. Troyano;Paolo Rosso;Markus Schedl
#t 2011
#c 1
#% 1643153
#% 1643154
#% 1643155
#% 1643156
#% 1643158
#% 1643159
#% 1643160
#% 1643161
#% 1643163
#% 1643164
#! In this paper, we provide an overview of the 3rd International Workshop on Search and Mining User-generated Contents, held in conjunction with the 20th ACM International Conference on Information and Knowledge Management. We present the motivation and goals of the workshop, and some statistics and details about accepted papers and keynotes.

#index 1642328
#* Web science and information exchange in the medical web
#@ Kerstin Denecke;Peter Dolog
#t 2011
#c 1
#! The amount of social media data dealing with medical and health issues increased significantly in the last couple of years. Medical social media data now provides a new source of information within information gaining contexts. Facts, experiences, opinions or information on behavior can be found in the Medicine 2.0 or Health 2.0 and could support a broad range of applications. This workshop is devoted to the technologies for dealing with social- and multi media for medical information gathering and exchange. This specific data and the processes of information gathering poses many challenges given the increasing content on the Web and the trade off of filtering noise at the cost of losing information which is potentially relevant.

#index 1642329
#* 3rd international workshop on collaborative information retrieval (CIR2011)
#@ Gene Golovchinsky;Juan M. Fernández-Luna;Juan F. Huete;Meredith Ringel Morris;Jeremy Pickens;Julio C. Rodríguez-Cano
#t 2011
#c 1
#% 805877
#% 848012
#% 987374
#% 998795
#% 1047489
#% 1189153
#% 1474635
#% 1474642
#% 1642352
#% 1642353
#% 1642354
#% 1642355
#% 1642356
#% 1642357
#% 1642358
#! Synchronous, explicit search has some interesting characteristics that distinguish it from other types of interaction: there is much more emphasis on interaction, as the system has to not only communicate search results to the user, but also mediate some forms of communication and data sharing among its users. There are new algorithms that need to be invented that use inputs from multiple people to produce search results, and new evaluation metrics need to be invented that reflect the collaborative and interactive nature of the task. Finally, we need to integrate the expertise of library and information science researchers and practitioners by revisiting real-world information seeking situations with an eye for explicit, synchronous collaborative search.

#index 1642330
#* DESIRE 2011: first international workshop on data infrastructures for supporting information retrieval evaluation
#@ Maristella Agosti;Nicola Ferro;Costantino Thanos
#t 2011
#c 1
#! The workshop focuses on the three areas of interest to CIKM to discuss how to envisage and design evaluation infrastructures able to store, manage, and make accessible the scientific data and knowledge of interest for advancing the evaluation of information retrieval and access tools. Main goal is to understand how to make use of the expertise of the three scientific areas in a cooperative way to avoid the duplication of efforts which may occur when addressing the problem separately in each specific area and to trigger synergies and joint actions on the issue. Main purposes of the workshop are the identification of a roadmap and the definition of initial best practices to guide the development of the necessary evaluation infrastructures.

#index 1642331
#* PIKM 2011: the 4th ACM workshop for Ph.D. students in information and knowledge management
#@ Anisoara Nica;Fabian M. Suchanek
#t 2011
#c 1
#% 1016295
#% 1077041
#% 1131183
#% 1301009
#% 1482497
#% 1542529
#% 1642331
#% 1643138
#% 1643139
#% 1643140
#% 1643141
#% 1643142
#% 1643143
#% 1643144
#% 1643145
#% 1643146
#% 1643147
#% 1643148
#% 1643149
#% 1643150
#! The PIKM workshop gives Ph.D. students an opportunity to present their dissertation proposals at a global stage. Similarly to the CIKM, the PIKM workshop covers a wide range of topics in the areas of databases, information retrieval and knowledge management. Interdisciplinary work across these tracks is particularly encouraged.

#index 1642332
#* Managing interoperability and complexity inhealth systems: MIXHS'11 workshop summary
#@ Matt-Mouley Bouamrane;Cui Tao
#t 2011
#c 1
#! Managing Interoperability and Complexity in Health Systems, MIXHS'11, aims to be a forum focussing on recent research and technical results in knowledge management and information systems in bio-medical and electronic health systems. The workshop will provide an opportunity for sharing practical experiences and best practices in e-Health information infrastructure development and management. Of particular interest to the workshop themes are technical solutions to recurring practical systems deployment issues, including harnessing the complexity of bio-medical domain knowledge and the interoperability of heterogeneous health systems. The workshop will gather experts, researchers, system developers, practitioners and policymakers designing and implementing solutions for managing clinical data and integrating existing and future electronic health systems infrastructures.

#index 1642333
#* Report on the third international workshop on cloud datamanagement (CloudDB 2011)
#@ Xiaofeng Meng;Zhiming Ding;Haibo Hu
#t 2011
#c 1

#index 1642334
#* Search and mining entity-relationship data
#@ Haggai Roitman;Ralf Schenkel;Marko Grobelnik
#t 2011
#c 1
#% 1643128
#% 1643129
#% 1643130
#% 1643132
#% 1643133
#% 1643134
#! This paper summarizes the details of the first international workshop on search and mining entity-relationship data. This workshop will bridge between IR, DB, and KM researchers to seek novel solutions for search and data mining of rich entity-relationship data and their applications in various domains. We first provide an overview about the workshop. We then briefly discuss the workshop program.

#index 1642335
#* Fourth workshop on exploiting semantic annotations in information retrieval (ESAIR)
#@ Omar Alonso;Jaap Kamps;Jussi Karlgren
#t 2011
#c 1
#% 1482633
#% 1642905
#% 1642907
#% 1642909
#% 1642910
#% 1642911
#% 1642912
#% 1642913
#% 1642917
#% 1642919
#! There is an increasing amount of structure on the Web as a result of modern Web languages, user tagging and annotation, and emerg- ing robust NLP tools. These meaningful, semantic, annotations hold the promise to significantly enhance information access, by enhancing the depth of analysis of today's systems. Currently, we have only started exploring the possibilities and only begin to un- derstand how these valuable semantic cues can be put to fruitful use. Unleashing the potential of semantic annotations requires us to think outside the box, by combining the insights of natural lan- guage processing (NLP) to go beyond bags of words, the insights of databases (DB) to use structure efficiently even when aggregating over millions of records, the insights of information retrieval (IR) in effective goal-directed search and evaluation, and the insights of knowledge management (KM) to get grips on the greater whole. This workshop aims to bring together researchers from these dif- ferent disciplines and work together on one of the greatest chal- lenges in the years to come. The desired result of the workshop will be to gain concrete insight into the potential of semantic an- notations, and in concrete steps to take this research forward; to synchronize related research happening in NLP, DB, IR, and KM, in ways that combine the strengths of each discipline; and to have a lively, interactive workshop where every participant contributes actively and which inspires attendees to think freely and creatively, working towards a common goal.

#index 1642336
#* LSDS-IR'11: the 9th workshop on large-scale and distributed systems for information retrieval
#@ Claudio Lucchese;B. Barla Cambazoglu
#t 2011
#c 1
#! The growth of the Web and user bases lead to important performance problems for large-scale Web search engines. The LSDS- IR '11 workshop focuses on research contributions related to the scalability and efficiency of distributed information retrieval (IR) systems. The workshop also encourages contributions that propose different ways of leveraging diversity and multiplicity of resources available in distributed systems. More specifically, we are interested in novel applications, models, and architectures that deal with efficiency and scalability of distributed IR systems.

#index 1642337
#* DOLAP 2011: overview of the 14th international workshop on data warehousing and olap
#@ Alfredo Cuzzocrea;Karen C. Davis;Il-Yeol Song
#t 2011
#c 1
#% 1642877
#% 1642878
#% 1642879
#% 1642880
#% 1642881
#% 1642882
#% 1642883
#% 1642884
#% 1642885
#% 1642886
#% 1642887
#% 1642888
#% 1642889
#% 1642890
#! The ACM 14th International Workshop on Data Warehousing and OLAP (DOLAP 2011), held in Glasgow, Scotland, UK on October 28, 2011, in conjunction with the ACM 20th International Conference on Information and Knowledge Management (CIKM 2011), presents research on data warehousing and On-Line Analytical Processing (OLAP). The DOLAP 2011 program has three interesting sessions on data warehouse modeling and maintenance, ETL and performance, and OLAP visualization and extensions, and a panel discussing analytics in data warehouses.

#index 1642338
#* Social and collaborative information seeking: panel
#@ Jeremy Pickens
#t 2011
#c 1
#! In recent years, information retrieval and information seeking have moved beyond their single-user roots and are becoming multi-user endeavors. However, there are multiple visions for how best to design multi-user interactions: social search versus collaborative search. The terms "social" and "collaborative" are overloaded with meaning, having been used to describe a wide variety of systems, user needs and goals, interaction styles, and algorithms. In this panel we adopt the following primary definitions: Information seeking tasks in which there are two or more people who lack the same information (share the same information need) and explicitly set out together to satisfy that need are known as collaborative. A collaborative information retrieval system provides mechanisms -- interfaces and mediation algorithms -- that allow the team to work together to find information that neither individual would have found when working alone. There is an inherent division of labor in collaborative work. On the other hand, information seeking tasks in which only a single individual lacks information, but is willing or able to let an larger group assist in the satisfaction of that need, is known as social search. The larger group may be an community of like-minded individuals, or it might be a social network of friends and associates. But either way, the assumption is that someone in that community or network already possesses the information that the initial individual seeks. The goal of the system is therefore to correctly propagate or diffuse that existing knowledge throughout the network, to amplify and repeat information that has already been discovered by at least one person. Despite these fundamental differences between collaborative (team-oriented, jointly-held information need) and social (network- and community-augmented, though ultimately solitary need), there are similarities in process. This panel will explore both these similarities and differences, and provide insight about whether one type of multi-user information seeking vision will ultimately eclipse the other, or whether each will remain separate but complementary.

#index 1642339
#* Proceedings of the 4th ACM workshop on Online books, complementary social media and crowdsourcing
#@ Gabriella Kazai;Carsten Eickhoff;Peter Brusilovsky
#t 2011
#c 1
#! It is our great pleasure to welcome you to the BooksOnline 2011 Workshop on Research Advances in Large Digital Book Repositories and Complementary Media. This year, the fourth edition of the workshop series places strong emphasis on the social dimension of reading and explores the role of social media in enhancing digital libraries and online digital book repository services. Online social networks and services have seen a tremendous increase in popularity in recent years. They enable a fast growing population of users to connect, socialize, and share data and experiences on the Web, as well as support users in working together, contributing towards common goals. Examples of such online social movements around books and book repositories include various efforts of cataloguing and sharing "home library" records and systems that elicit and share book recommendations and reviews. A related recent phenomenon is that of crowdsourcing, which aims to harness the abilities of communities or crowds of people to complete otherwise daunting tasks such as correcting OCR errors in whole libraries of digitized books. Such social interactions and resulting social media hold the potential to revolutionize online publishing and digital libraries. These themes are reflected in the topics chosen by our keynote speakers. We are honoured to have as our keynote speakers Adam Farquhar, Head of Digital Library Technology at the British Library and Ville Miettinen, founder and CEO of Microtask, a Finnish company specialised in commercializing technology for distributing digital labour. Ville Miettinen, who has been leading the digitization efforts of the Finnish National Library, will discuss the challenges of introducing human computation into heterogeneous large-scale digitization workflows. Adam Farquhar will address the transition from digital libraries as a means of supporting traditional book repositories to digital publishing. The social dimension of online publishing and reading is also reflected in the work of several authors who contributed research or position papers to the workshop. Overall, the program consists of a high-quality selection of papers that focus on: 1) Technological innovations in the fields of information retrieval and information extraction that further the usability and engagement of digital books in order to better exploit the capabilities of digital media rather than inheriting the limitations of conventional print media, and 2) Methodologies and studies of changes in reading behaviour as a consequence of societal and technological developments that can guide further innovation. We hope that you will find this program interesting and thought-provoking and that the workshop will provide you with a valuable opportunity to share ideas with other researchers and practitioners from institutions around the world.

#index 1642351
#* Proceedings of the 3rd international workshop on Collaborative information retrieval
#@ Gene Golovchinsky;Juan M. Fernández-Luna;Juan F. Huete;Meredith Ringel Morris;Jeremy Pickens;Julio C. Rodríguez-Cano
#t 2011
#c 1
#! It is our great pleasure to welcome you to the 3rd International Workshop on Collaborative Information Retrieval (CIR2011). The first workshop focused on broad topics and sought to establish a vocabulary for discussion about collaborative information seeking, to identify work practices and disciplines that might benefit from collaborative information seeking, and to establish a community of researchers with related interests. The second workshop built on the previous results, and focused on issues of communication and awareness in support of collaborative information seeking. In this third workshop we focus on algorithmic and other software issues related to collaborative information seeking. We would like to explore a variety of algorithms for mediating collaboration, look at the design of frameworks and toolkits to support collaborative information seeking, and also to examine how different user interface elements can be used to support associated activity.

#index 1642359
#* Proceedings of the third international workshop on Cloud data management
#@ Xiaofeng Meng;Zhiming Ding;Haibo Hu
#t 2011
#c 1
#! It is our great pleasure to welcome you to the Third International Workshop on Cloud Data Management (CloudDB 2011). This year's workshop continues its tradition of being the premier forum for presentation of research results and experience reports on the challenges of large-scale data management based on the cloud computing infrastructure. The mission of the workshop is to bring together researchers and practitioners from cloud computing, distributed storage, query processing, parallel algorithms, data mining, and system analysis, and identify new directions for future research and development. CloudDB gives them a unique opportunity to share their perspectives with others interested in the various aspects of cloud data management. The call for papers attracted 14 submissions from Asia, North America and Europe. The program committee accepted 6 papers that cover a variety of topics, including cloud-based storage, machine learning, query processing, distributed algorithms, security and privacy. In addition, the program includes two exciting keynote speeches by Prof. Malcolm Atkinson and Prof. Paul Watson. We hope that these proceedings will serve as a valuable reference for cloud data management researchers and developers.

#index 1642469
#* Proceedings of the 2011 workshop on Data infrastructurEs for supporting information retrieval evaluation
#@ Maristella Agosti;Nicola Ferro;Costantino Thanos
#t 2011
#c 1
#! This volume contains the reports on the two keynote addresses and the papers accepted for presentation at the Data infrastructurEs for Supporting Information Retrieval Evaluation -- DESIRE 2011 -- Workshop1 held on 28 October 2011 in conjunction with the 20th ACM International Conference on Information and Knowledge Management (CIKM), Glasgow, UK. The theme of the workshop has been envisaged in the context of the area of Evaluation of Information Retrieval. In fact, Information Retrieval has a strong and long tradition dating back to the 1960s in producing and processing scientific data resulting from the experimental evaluation of search algorithms and search systems. This attitude towards evaluation has led to fast and continuous progress in the evolution of information retrieval systems and search engines. However, in order to make the data test collections, that are used in the context of the evaluation activities, understandable and usable they must be endowed with some auxiliary information, i.e., provenance, quality, context. Therefore, there is a need for metadata models able to describe the main characteristics of evaluation data. In addition, in order to make distributed data collections accessible, sharable, and interoperable, there is a need for advanced data infrastructures. In contrast, the information retrieval area has barely explored and exploited the possibilities for managing, storing, and effectively accessing the scientific data produced during the evaluation studies by making use of the methods typical of the database and knowledge management areas. Over the years, the information retrieval area has produced a vast set of large test collections which have become the main benchmark tools of the area and contribute to reproducible and comparable experiments. However, these same collections have not been organised into coherent and integrated infrastructures which make them accessible, searchable, citable, exploitable, and re-usable to all possibly interested researchers, developers, and user communities. It is thus time for these three communities -- information retrieval, databases, and knowledge management -- to join efforts, meet, and cooperate to address the problem of envisaging and designing useful infrastructures able to coherently manage pertinent data collections and sources of information, and so take concrete steps towards developing them. Indeed, the information retrieval experts need to recognise this need, while the database and knowledge management experts need to understand the problem and work together to solve it by using the methods and techniques specific to information management. Taking into consideration all these issues, the main objective of the workshop is to gather together experts from these three areas, to encourage them to recognise the urgency of addressing the problem in an integrated and coherent way, and to coordinate efforts towards drawing a roadmap and suggesting best practices for an effective solution of the problem. The topics that have been addressed by DESIRE 2011workshop include: Conceptual and logical data models for representing IR evaluation scientific data Metadata formats for describing scientific data produced during IR evaluation Knowledge management for IR experimental evaluation Data quality, provenance, adaptability and reusability in the IR evaluation Data pre- and post-processing, metrics, and analyses in the IR evaluation Data exchange, integration, evolution and migration for IR evaluation infrastructures Workflow, Web services and Web Service Composition for IR evaluation infrastructures Metadata formats for describing scientific data produced during IR evaluation Information Extraction and Text Mining for linking scientific literature and experimental data Data citation Evaluation, Test collections, Crowdsourcing for IR evaluation Visualization of scientific data coming from experimental evaluation The two keynote addresses have been on aspects related to the design of evaluation infrastructures supporting interactive information retrieval and on the management of data that can support the conceptual representation of the domain of interest of an information management application. To give some introductory information on the contents of the two keynote addresses: The keynote address of Professor Norbert Fuhr of the University of Duisburg-Essen in Germany, entitled An Infrastructure for Supporting the Evaluation of Interactive Information Retrieval, addressed the presentation of a testbed for the evaluation of interactive information access. Starting with the INEX2 interactive track in 2004, the group lead by professor Fuhr developed the Daffodil (now ezDL) framework, providing an experimental framework for interactive retrieval, that allows for easy exchange or extension of the system components. Moreover, this framework also contains tools for organizing laboratory experiments. Besides extensive logging (including the possibility to exploit eye tracking data), the system allows for presenting questionnaires at all stages of a search session (pre- /post- task/session), as well as the scheduling of search tasks and monitoring task time. The keynote address of Professor Maurizio Lenzerini of the Sapienza University of Rome, Italy, entitled Ontology-based data management, addressed how the ontology-based data management aims at accessing and using data by means of a conceptual representation of the domain of interest in the underlying information system. The talk provided an introduction to ontology-based data management, by illustrating the main ideas and techniques for using an ontology to access the data layer of an information system. Then, it described an architecture for ontology-based data access and discussed the issue of choosing the appropriate language for expressing the various components of the architecture, by illustrating the main advantages one gains in managing the information system through the ontology. Finally, the issue of developing methodologies and tools for the design and usage of ontology-based data management solutions have been explained. Two types of contribution have been presented: communications and position papers. The papers have been peer-reviewed by members of the program committee and the selection procedure has taken into account aspects related to originality, clarity, and technical quality. The six accepted and presented papers are included in this volume.

#index 1642672
#* Proceedings of the 2011 international workshop on DETecting and Exploiting Cultural diversiTy on the social web
#@ Sergej Sizov;Stefan Siersdorfer;Philipp Sorg;Thomas Gottron
#t 2011
#c 1
#! It is our great pleasure to welcome you to the 2011 International Workshop on DETecting and Exploiting Cultural diversiTy on the Social Web -- DETECT'11. The rapid growth of Social Media originates in the ease of collaborative content generation and content sharing for non-expert mass users. In the recent years, substantial research progress was achieved regarding information retrieval, text mining, information filtering and recommendation in social media. However, some fundamental questions remain still open. One of the key challenges for Social Web research is the deeper understanding of the intercultural and multi-lingual nature of online communities like Wikipedia, Facebook, or Twitter. In particular, this is the case in settings where differences in culture and language coincide, e.g. when dealing with resources in African, Arabian, Asian and Western languages. Understanding content and user relations across languages and countries, intercultural opinion differences, and country/language-specific topics of interest may allow for deeper insights into such Social Web communities. Promising research directions include multi-lingual retrieval, mining of inter-cultural societal trends, as well as detecting controversial opinions across countries, cultures, and languages. Our workshop DETECT (an acronym for DETecting and Exploiting Cultural diversiTy on the social web) aims to facilitate inter-disciplinary research on complex dependencies between culture, language, and content on the social web. DETECT topics of interest are in the key scope of recent IR research on social media. However, they also offer the possibility of intensive inter-disciplinary cooperation of experts from computer science, linguistics, psychology, sociology, and other domains. Consequently, DETECT aims to establish and to facilitate inter-disciplinary collaboration of researchers from various fields in the context of Social Web communities. The call for papers attracted high-quality submissions from Europe, United States, and Japan. The program committee accepted six full papers that cover a variety of relevant topics, including content diversification, understanding of user behavior, automatic content organization using cultural, lingual and behavioral aspects, as well as content semantics. The program is supplemented by keynotes closely related to cultural diversity of Social Media: diversification / personalization of recommendations and detection of cultural aspects in user behavior. With our workshop we aim at bringing together researchers and practitioners dealing with intercultural, multi-lingual and multi-national information environments in distinct contexts, and discover synergies between their research fields. Consequently, the last DETECT session will be organized as a plenary break-up discussion on lessons learned, open issues, and potential follow-up events. In particular, we aim to establish an inter-disciplinary special interest group on inter-cultural/crosslingual analysis of Social Media.

#index 1642876
#* Proceedings of the ACM 14th international workshop on Data Warehousing and OLAP
#@ Il-Yeol Song;Alfredo Cuzzocrea;Karen Davis
#t 2011
#c 1
#! The ACM International Workshop on Data Warehousing and Online Analytical Processing (DOLAP) is an annual event that provides an international forum where both researchers and practitioners can share their findings in theoretical foundations, current methodologies, practical experiences, and new research directions in the areas of data warehousing and online analytical processing. The diverse topics of the 14th DOLAP workshop include data warehouse modeling and integration, cloud data warehouses, spatial modeling, designing and parallelizing ETL, query optimization, and visualization and manipulation of OLAP cubes, among others. These proceedings contain the papers selected for presentation at the workshop. We received 27 submissions from countries in North and South America, Europe, Asia, and Africa. After careful review, the program committee selected 14 papers for presentation at the workshop. The accepted papers were presented in 3 sessions: data warehouse modeling and maintenance, ETL and performance, and OLAP visualization and extensions. A panel discussed analytics in data warehouses. We hope that these proceedings will serve as a valuable reference for data warehousing and OLAP researchers and practitioners.

#index 1642892
#* Proceedings of the ACM fifth international workshop on Data and text mining in biomedical informatics
#@ Doheon Kim;Sophia Ananiadou;Min Song;Shamkant Navathe
#t 2011
#c 1
#! It is our great pleasure to welcome you to the ACM Fifth International Workshop on Data and Text Mining in Biomedical Informatics (DTMBIO'11), in conjunction with the 20th ACM International Conference on Information and Knowledge Management (CIKM'11). The aim of the 2011 workshop has been to bring together researchers in the areas of data and text mining and computational biology, who are interested in integrating heterogeneous, structured and unstructured data, while using the literature in a number of bioinformatics applications. Manual techniques and conventional information retrieval techniques are unable to deliver timely, reliable, exhaustive and specific results, given the vastness of the literature and its speed of growth. Methods that utilize relevant background knowledge from text, including scientific publications, and database annotations, range in sophistication: from concept extraction, document clustering, document classification, synonymy detection, disambiguation, normalization, named entity recognition, to extraction of relations, complex events, inference, network construction, etc. Knowledge discovery methods consolidate and process heterogeneous biomedical data collected from electronic bulletin boards, medical records, abstracts, full papers in scientific publications, and other types of experiments. Developing computational tools and methods that contribute to a better understanding of not only biological systems, but also their interaction with clinical data, requires use of advanced automated means to support knowledge discovery, by flagging key biomedical entities and biomedical processes and tracking accumulation of evidence over time (hypothesis generation). This year's workshop continues the tradition of bringing together researchers who work in the fields of data mining, text mining, computational biology and translational bioinformatics, and provides a forum to present and discuss current research topics at the interface of these related fields. Biomedicine faces many challenges due to its fragmentation into many specialties and subspecialties, the increasing interdisciplinary nature of research, and the need to address issues of communication and understanding between those working at the various 'omics' levels and those working in areas such as translational medicine, computational biology, epidemiology, development of best practice clinical guidelines and assessment of new drugs and treatments: for a given problem, all have different views at different levels of abstraction. The papers accepted for presentation and publication in this volume cover a variety of topics, including semantic search, disambiguation, clinical text mining, inference, image processing, translational bioinformatics, systems bioinformatics, and database applications. We hope that these proceedings will serve as a valuable and up-to-date reference concerning the application of data- and text-mining techniques within biomedical informatics.

#index 1642905
#* Proceedings of the fourth workshop on Exploiting semantic annotations in information retrieval
#@ Omar Alonso;Jaap Kamps;Jussi Karlgren
#t 2011
#c 1
#! These proceedings contain the invited talks and contributed posters of the Fourth Workshop on Exploiting Semantic Annotations in Information Retrieval (ESAIR 2011), held at CIKM 2011 in Glasgow, Scotland, on October 28, 2011. After successful workshops at ECIR 2008 in Glasgow, WSDM 2009 in Barcelona, and CIKM 2010 in Toronto, this year's workshop will focus on how to best formulate and use semantic annotation of information objects and information streams for information access tasks such as search, retrieval, categorization and related information refinement tasks. ESAIR 2011 will be a real workshop where researchers from these different disciplines will work together to identify natural use cases, barriers to success, and work on ways of addressing them: Use Cases: Are we looking at the right applications? What are use cases that make obvious the need for semantic annotation of information? What tasks cannot be solved by document retrieval using the traditional bag-of-words? What are the prerequisites of successful application? How can the expressive power of semantic annotation best be put to use? What is keeping searchers from exploring these powerful search requests? Annotations: Are we using the right types of data and annotation? What types of annotation are available? Are there crucial differences between author-, software-, user-, and machine-generated annotations? What are novel types of annotations that are within our grasp? What semantic theories do we need to formulate further annotation schemes? Data Curation: Are we using contextual information in the right way? Annotations may live inside documents, or be stored externally (e.g., annotated by uncontrolled authors or tools) or both (e.g., annotation with linked data). How to keep data and metadata together? Does the annotation stop somewhere, or is all social or linked data of potential use for searching or navigating? How important is source of the annotations? Are there issues with credibility or trust? Result Aggregation: Are we using the right types of results? Whereas IR focuses almost exclusively at finding individual chunks of information, DB naturally focuses on results that combine information and produce aggregated results (think of OLAP queries), and KM naturally deals with the whole information space. How can we fruitfully combine these strengths? The workshop will consist of three main parts: A keynote by Arjen de Vries to help us formulate the challenges. A boaster and poster session with 13 papers selected by the program committee from 15 submissions (87%). Each paper was reviewed by at least two members of the program committee. Break out groups on different aspects of exploiting semantic annotations, with reports being discussed in the final session.

#index 1642920
#* Proceedings of the 9th workshop on Large-scale and distributed informational retrieval
#@ B. Barla Cambazoglu;Claudio Lucchese
#t 2011
#c 1
#! It is our great pleasure to welcome you to the 9th Workshop on Large-Scale and Distributed Systems for Information Retrieval -- LSDS-IR'11. As in the previous years, LSDS-IR continues to be the leading venue for presentation of cutting edge research findings on topics including largescale data processing, efficient and scalable information systems, large-scale web search, and distributed information retrieval. The primary objective of the LSDS-IR workshop is to bring together both young and experienced researchers to share their ideas and disseminate their research findings among a small yet tightly coupled researcher community working on large-scale and distributed information retrieval systems. This year's call has attracted nine papers. The author affiliations spanned four different continents (Asia, Europe, South America, and North America) and eleven different countries (Argentina, Chile, China, Czech Republic, Greece, France, Italy, Norway, Spain, U.K., and U.S.A.). The program committee, which is formed of 24 experienced researchers, accepted six papers for presentation in the workshop. The accepted papers cover a diverse set of research areas, including P2P information retrieval systems, community detection in social networks, search efficiency, and management of web graphs. In addition to these, the workshop program contains two keynote speeches by leading researchers and a group presentation session, in which selected research groups will have the opportunity to present their on-going research.

#index 1642928
#* Proceedings of the second international workshop on Web science and information exchange in the medical web
#@ Kerstin Denecke;Peter Dolog
#t 2011
#c 1
#! It is our great pleasure to welcome you to the Second International Workshop on Web Science and Information Exchange in the Medical Web (MedEx'11). This year's workshop continues the first release of MedEx in 2010 which was held in conjunction with the International World Wide Web Conference in Raleigh, 2010. MedEx is devoted to the technologies for dealing with social- and multi-media for medical information gathering and exchange. The workshop provides a forum for presentations of research results and experience reports on issues of analyzing medical Web data, personalizing medical applications, and evaluating medical web applications. The mission of the workshop is to share ideas and solutions for dealing with the immense data provided in the medical Web and to identify new directions for future research and development. MedEx'11 gives researchers and practitioners a unique opportunity to share their perspectives with others interested in the various aspects of Web science and information exchange in the medical Web. As usual, we attracted submissions for full papers. A novelty in this year are short position papers that allow researchers to present their research ideas and challenges within the workshop. These contributions will be seen as starting point for discussions on the future developments in this interesting area. In addition, the program includes two keynote speeches. Sophia Ananiadou will give a talk on latest developments in biomedical text mining and implications for medical web. Wendy Chapman's speech will discuss the shortcomings and barriers in the clinical natural language processing (NLP) domain and envisions the creation of an NLP ecosystem for the medical domain. Even though, these proceedings can contain only the full papers, we hope that also the short papers, that will be published as a volume of CEUR workshop proceedings, will serve as a valuable reference for future research and discussions.

#index 1642932
#* Proceedings of the first international workshop on Managing interoperability and complexity in health systems
#@ Matt-Mouley Bouamrane;Cui Tao
#t 2011
#c 1
#! It is our great pleasure to welcome you on October 28th, 2011 to the First International Workshop on Managing Interoperability and compleXity In Health Systems, MIXHS'11, collocated with the 20th ACM International Conference on Information and Knowledge Management, CIKM 2011, in Glasgow, Scotland, U.K. MIXHS is a multi-disciplinary forum open to experts, researchers, system developers, practitioners and policymakers designing and implementing solutions for managing clinical data and integrating existing and future eHealth systems and electronic infrastructures. The level of interest and engagement generated by MIXHS is a clear sign that the themes and topics covered in the calls for participation in the workshop are addressing important implementations, challenges of modern and future enterprise, and national electronic health systems. The workshop has attracted considerable interest nationally and internationally and will become a regular fixture of CIKM and other related international conferences in the future. In order to develop, implement and integrate large-scale health systems, one faces numerous complex technical challenges, including: information retrieval and extraction in large heterogeneous data-sets, information management in heterogeneous clinical repositories, knowledge engineering in medicine, the development and practical deployment of clinical interoperability standards in distributed health systems, the structuring of clinical documents and the deployment of the patient Electronic Health Record (EHR). The call for papers attracted 18 submissions from the U.S., the U.K., Ireland, the Netherlands, Portugal, Sweden and Brazil. Each submission was reviewed by an average of 3 members of the International Technical Program Committee (please see the workshop summary for additional information on the Program Committee). The chairs wrote detailed meta-reviews for each of the submissions, and 15 were selected for presentation on the day of the workshop. The program includes 7 full research papers and 8 work-in-progress / Challenges and Visions track papers. The submissions are grouped in 4 themes: Data Integration in Bio-Medical Information Systems (Session Chair: A. Rosemary Tate, University of Sussex, U.K.), Information Extraction in the Electronic Health Record (Session Chair: Matt-Mouley Bouamrane, University of Glasgow, Scotland, U.K.), Electronic Health Systems Interoperability and Integration (Session Chair: Adel Taweel, King's College London, U.K.), and Bio-Medical Knowledge Representation & Engineering (Session Chair: Albert Burger, Herriot Watt University, Edinburgh, Scotland, U.K.). The MIXHS'11 workshop will begin with a keynote by Dr. Paul Woolman, Enterprise Architect for the Scottish Government e-Health programme. Dr. Woolman has worked on interoperability for over twenty years and is regularly a featured speaker at international forums and conferences on eHealth topics. On this occasion, Dr. Woolman will be discussing the "Challenges of managing interoperability in regional healthcare organisations".

#index 1643117
#* Proceedings of the 4th workshop on Patent information retrieval
#@ Mihai Lupu;Andreas Rauber;Allan Hanbury
#t 2011
#c 1
#! On behalf of the PaIR workshop organizing committee, we welcome you to the 4th workshop on Patent Information Retrieval (PaIR'11), organized by the Information Retrieval Facility (IRF) and the Vienna University of Technology. Previous PaIR workshops were held in Napa Valley, California (PaIR'08), Hong Kong (PaIR'09) and Toronto (PaIR'10). This year's workshop continues our examination of many of the most challenging aspects of patent-related information retrieval. Despite the enormous recent progress in Information Retrieval techniques, advanced search tools for patent professionals are still in the early stages of development -- thus, the research in patent retrieval discussed here today may become key components in the patent search tools of tomorrow. Patents are not only crucial in protecting intellectual property but also serve as a strategic business factor in all modern economies. Patent search is a particular challenge to information retrieval and access systems for many reasons that are obvious and some reasons that are far subtler. Looking forward, successful patent search systems of the future will need to address the following aspects: a vast amount of highly-complex structured documents; a highly heterogeneous document collection (scientific papers, legal public disclosure as well as patents); multiple languages; ambiguous and conflicting technical jargon; complex technological concepts; sophisticated legal jargon; harmonization issues between patent-issuing bodies; evaluation of numerical ranges and other complex query types; tracking temporal issues like publication data and patent priority dates; tabular and graphical information embedded and referred to through placeholders in the patent text; and many others. The objective of this workshop is to provide a forum for Information Retrieval and Knowledge Management scientists, as well as Patent Retrieval experts from industry to exchange ideas, discuss the state-of-the-art and to study the next generation of patent search tools. This year the workshop received 8 submissions, from which 6 full papers were accepted. All of them were also invited to prepare posters, as the afternoon session is dedicated to discussions surrounding the works presented here, as well as the keynote talks. The accepted papers cover some of the most significant issues in Patent IR. Multilinguality is addressed by Nanba in his work on creating a bilingual terminology resource. The state of image retrieval techniques, and their potential use and utility for the patent domain is described by Hanbury and his colleagues. Then, three works this year address the core issue of text retrieval in the patent domain: Ganguly et al., Magdy and Jones, and Verma and Varma. Finally, Lupu describes his experience with and the status of IR evaluation in this domain. These papers will initiate interesting conversations and hopefully will spark future development in intellectual property search, fostering further collaboration between researchers and industry representatives. Our primary goal for this workshop is to trigger more discussions on ways to go forward in this domain. Despite the intense research carried out in the last few years, we have seen little take up from the industry. We need an introspective view on this subdomain, and guidelines for future research. This is why the second half of the workshop will be dedicated to discussions in breakout groups, on specific topics, to identify ways to go forward. In these efforts we are grateful for the support of the two invited speakers, Jane List of the Lighthouse IP Group and Stephen Adams, from Magister Ltd., both of whom have extensive domain experience.

#index 1643126
#* Proceedings of the 1st international workshop on Search and mining entity-relationship data
#@ Haggai Roitman;Ralf Schenkel;Marko Grobelnik
#t 2011
#c 1
#! It is our great pleasure to welcome you to the 1st International Workshop on Search and Mining Entity-Relationship Data -- SMER 2011, held at CIKM 2011 in Glasgow on October 28, 2011. Data complexity and its diversity have been rapidly expanding over the last years. Numerous applications in various domains such as social-media, healthcare, e-commerce, and business intelligence require new methods and tools for collecting and extracting entities and their relationships from unstructured data sources to be transformed into useful knowledge. Even though lots of useful facts are being added on a daily basis on multitude web and enterprise data sources, they are still hidden behind barriers of language constraints, data heterogeneity and ambiguity, and the lack of proper query interfaces. In addition, novel search and data mining methods are required to provide expressive and powerful discovery capabilities, yet intuitive enough, for exploring the large amounts of entity-relationship data. The workshop picks up these problems in two main themes. The first is search and discovery over rich entity-relationship data and includes among others topics such as ER data collection, indexing, query languages and optimization, ranking methods, similarity and exploratory search, privacy, etc. The second theme is entity-relationship data mining methods and includes among others topics such ER data extraction, resolution, representation, fusion, ER graph mining, etc. The workshop main objective is to serve as an open forum for discussing the new research challenges in search and mining of large scale ER data extracted from multitude of unstructured and semi-structured data sources, driven by recent industry trends and requirements in various domains and increasing academic interest. The workshop will bring together researchers from different communities working on similar problems in the context of ER and other semantic data, allowing for cross-fertilization between areas. During the workshop, we will identify common problems and their various solution approaches in DB, KM, and IR. The workshop will consist of three parts: Keynote talks by Michael Witbrock (Cycorp), Krisztian Balog (Norwegian University of Science and Technology), and David Carmel (IBM Research) with valuable insights from both research and industry, three research talks and three posters, selected by the program committee from a total of 10 papers, with topics spanning from entity extraction methods to entity search and privacy, and an open panel for discussing open issues and advances in ER search and data mining, serving as an open stage for interdisciplinary collaboration for facing the emerging research challenges.

#index 1643136
#* Proceedings of the 4th workshop on Workshop for Ph.D. students in information & knowledge management
#@ Anisoara Nica;Fabian M. Suchanek
#t 2011
#c 1
#! For the 4th time, the International Conference on Information and Knowledge Management (ACM CIKM) hosts a workshop for Ph.D. students: PIKM 2011. The goal of this workshop is two-fold: First, a Ph.D. workshop gives doctoral students an opportunity to present their work in an early stage to a global audience. This allows the students not only to crystallize their ideas into a scientific article, and to practice scientific presentation, but also to receive feedback from reviewers, from fellow students and from the general CIKM audience. Second, we believe that the research community, too, benefits from such a workshop: Ph.D. theses are the grassroots of research. They point out new research avenues and indicate current promising topics. They provide fresh viewpoints from the researchers of tomorrow. Last, we hope that the interaction with other researchers at the workshop itself, across all levels of seniority, will help propel science forward. The PIKM workshop covers topics in all core areas of the general CIKM conference: information retrieval (IR), databases (DB), and knowledge management (KM). This includes subjects as diverse as resource monitoring, semantic search, pattern recognition, data mining, and data warehousing. This diversity of topics was reflected in the submissions we received. The call for papers attracted 18 submissions from nearly all continents of the world. Out of these, 9 papers were accepted as full papers. In addition, 4 papers were accepted as poster papers. The papers cover proposals at various stages of the dissertation, from early outline of research plans, to in-depth investigations of acute questions and mid-term reports of work in progress. The dissertations touch all main areas of the PIKM including, for example, work on user interaction and ranking, as well as research on workflow management. Similar to past PIKM workshops, the best submission will receive a best paper award. This year's award will go to Minsuk Kahng, Sangkeun Lee and Sang-Goo Lee for their paper "Ranking Objects by Following Paths in Entity-Relationship Graphs". As a special highlight, this year's PIKM features a keynote talk by Prof. Dr. Felix Naumann from the Hasso-Plattner-Institute, Potsdam, Germany. Prof. Naumann will talk about the challenges of "Extreme Web Data Integration" -- a task that becomes ever more challenging with the relentless growth of the Web.

#index 1643151
#* Proceedings of the 3rd international workshop on Search and mining user-generated contents
#@ Iván Cantador;Francisco M. Carrero;José C. Cortizo;Paolo Rosso;Markus Schedl;José A. Troyano
#t 2011
#c 1
#! The International Workshop on Search and Mining User-generated Contents (SMUC) have evolved as one of the main multidisciplinary forums for researchers and practitioners who work on knowledge extraction, management and exploitation in Social Media, and belong to different, but complementary fields such as Web (content/structure/usage) mining, information retrieval, opinion mining and sentiment analysis, user modeling, personalization and recommendation, and multimedia processing and retrieval. Following the previous editions of SMUC workshop (at CAEPIA-TTIA 2009 and CIKM 2010), the 3rd edition of the workshop (SMUC 2011) was held in conjunction with the 20th ACM Conference on Information and Knowledge Management (CIKM 2011), in Glasgow (UK) on the 28th of October 2011. SMUC 2011 received a total of 17 submissions from which the program committee selected 11 for oral presentation, which represents an acceptance rate of 64.7%. In addition, it featured 3 keynote talks. They were given by David E. Losada, from Universidad de Santiago de Compostela (Spain), on "The challenge of understanding the flow of sentiments in social media documents", Joemon M. Jose, from University of Glasgow (UK), on "Information retrieval techniques for social media", and Martin Atzmüller, from University of Kassel (Germany), on "Analysis of communities in social media."

#index 1912610
#* Proceedings of the 2012 international workshop on Web-scale knowledge representation, retrieval and reasoning
#@ Spyros Kotoulas;Yi Zeng;Zhisheng Huang
#t 2012
#c 1
#! It is our great pleasure to welcome you to the 2012 International Workshop on Web-scale Knowledge Representation, Retrieval, and Reasoning (Web-KR 2012), co-located with the 21st ACM International Conference on Information and Knowledge Management (CIKM 2012) at Hawaii, USA. This workshop is the third version in the workshop series under the title of "Webscale Knowledge Representation, Retrieval, and Reasoning (Web-KR)". Web-KR 2012 continues its mission to take grand challenges for KR in the Web age (such as scalability, inconsistency, uncertainty and dynamics). It brings together researchers from the Web, Artificial Intelligence (AI), High Performance Computing, Cognitive Science, Knowledge Management, and Machine Learning to discuss all issues of Web-KR in a synergistic setting. We hope to motivate different thoughts and solutions from researchers in these different fields and they can learn from each other. The call for papers attracted submissions from Switzerland, Germany, and the United States. The Web-KR 2012 program committee accepted 3 papers that cover different interesting and important topics, including uncertainty in Web knowledge representation and retrieval, parallel Web knowledge processing, and Web mining of knowledge with sequential patterns. In addition, this year, we have co-located Web-KR with the City Data Management 2012 workshop. We are delighted to have the opportunity to discuss the problem of managing big city data in the Web age with researchers from the Urban Computing field.

#index 1912614
#* Proceedings of the first edition workshop on Politics, elections and data
#@ Ingmar Weber;Ana-Maria Popescu;Marco Pennacchiotti
#t 2012
#c 1
#! It is our pleasure to welcome you to the first edition of the Politics, Elections and Data workshop (PLEAD at CIKM 2012). The goal of this workshop, taking place a few days before the 2012 U.S. presidential elections, is to bring together researchers working at the intersection of social network analysis, computational social science and political science, to share and discuss their ideas in a common forum; and to inspire further developments in this growing, fascinating field of computational political science. The call for papers attracted submissions from researchers in different areas and from different countries, which underscores the wide appeal of computational political science. The program committee accepted 6 papers which cover a variety of topics: measuring the online popularity of political campaigns, the adoption of social networking platforms by members of the political class and the feasibility of characterizing political communities by tracking topics in the message streams of their members, analyzing patterns in candidate funding to discern the state of a political party, mining opinions on politically controversial topics and finally, analyzing the effects of voter microtargeting and proposing solutions for mitigating the more worrisome aspects. In addition, the program includes a keynote speech from Filippo Menczer on "The Diffusion of Political Memes in Social Media". Filippo presents ongoing work on the study of information diffusion in social media with a focus on polarization, cross-ideological communication and partisan asymmetries in online political activities. The workshop will also feature a presentation of existing online tools for political analysis and a panel discussion with industry, academia and media participants.

#index 1912616
#* Proceedings of the first edition workshop on Politics, elections and data
#@ Ingmar Weber;Ana-Maria Popescu;Marco Pennacchiotti
#t 2012
#c 1
#! It is our pleasure to welcome you to the first edition of the Politics, Elections and Data workshop (PLEAD at CIKM 2012). The goal of this workshop, taking place a few days before the 2012 U.S. presidential elections, is to bring together researchers working at the intersection of social network analysis, computational social science and political science, to share and discuss their ideas in a common forum; and to inspire further developments in this growing, fascinating field of computational political science. The call for papers attracted submissions from researchers in different areas and from different countries, which underscores the wide appeal of computational political science. The program committee accepted 6 papers which cover a variety of topics: measuring the online popularity of political campaigns, the adoption of social networking platforms by members of the political class and the feasibility of characterizing political communities by tracking topics in the message streams of their members, analyzing patterns in candidate funding to discern the state of a political party, mining opinions on politically controversial topics and finally, analyzing the effects of voter microtargeting and proposing solutions for mitigating the more worrisome aspects. In addition, the program includes a keynote speech from Filippo Menczer on "The Diffusion of Political Memes in Social Media". Filippo presents ongoing work on the study of information diffusion in social media with a focus on polarization, cross-ideological communication and partisan asymmetries in online political activities. The workshop will also feature a presentation of existing online tools for political analysis and a panel discussion with industry, academia and media participants.

#index 1912622
#* Proceedings of the 2nd international workshop on Managing interoperability and compleXity in health systems
#@ Cui Tao;Matt-Mouley Bouamrane
#t 2012
#c 1
#! It is our great pleasure to welcome you on October 29th, 2012 to the Second International Workshop on Managing Interoperability and compleXity In Health Systems, MIX-HS'12, collocated with the 21st ACM International Conference on Information and Knowledge Management, CIKM 2012, in Maui, HI, U.S.A. This year's workshop continues its tradition of being a multi-disciplinary forum open to experts, researchers, system developers, practitioners and policymakers designing and implementing solutions for managing clinical data and integrating existing and future eHealth systems and electronic infrastructures. The level of interest and engagement generated by MIX-HS is a clear sign that the themes and topics covered in the calls for participation in the workshop are addressing important implementations challenges of modern and future enterprise and national electronic health systems. The workshop has attracted considerable interest nationally and internationally and will become a regular fixture of CIKM and other related international conferences in future. The MIXHS 2012 program includes 6 full research papers and 4 work-in-progress papers. The submissions are grouped in 3 themes: Electronic Health Systems Interoperability and Integration, (Session Chair: Cui Tao, Mayo Clinic, USA), Ontology-based Application on Clinical Data, (Session Chair: Guoqian Jiang, Mayo Clinic, USA), and Bio-Medical Knowledge Representation & Engineering (Session Chair: Hua Min, George Mason University, USA).

#index 1912633
#* Proceedings of the 5th Ph.D. workshop on Information and knowledge
#@ Aparna S. Varde;Fabian M. Suchanek
#t 2012
#c 1
#! For the 5th time, the International Conference Information and Knowledge Management (CIKM) hosts a workshop for PhD students: The PIKM 2012. The goal of this workshop is two-fold: First, a PhD workshop gives doctoral students an opportunity to present their work in an early stage to a global audience. This allows the students not only to crystallize their ideas into a scientific article, and to practice scientific presentation, but also to receive feedback from reviewers, from fellow students and from the general CIKM audience. Second, we believe that the research community, too, benefits from such a workshop: PhD dissertations are the grassroots of research. They point out new research avenues and indicate current promising topics. They provide fresh viewpoints from the researchers of tomorrow. Also, we hope that the interaction with other researchers at the workshop itself, across all levels of seniority, will help propel science forward. The PIKM workshop covers topics in all core areas of the general CIKM conference: information retrieval (IR), databases (DB), and knowledge management (DB). This includes subjects as diverse as link prediction, semantic search, query languages, and data mining. This diversity of topics got reflected in the submissions we received. The call for papers attracted 25 submissions from nearly all continents of the world. Out of these, 10 papers got accepted as full papers. In addition, 4 papers got accepted as poster papers. The papers cover proposals at various stages of the dissertation, from early outlines of research plans, to in depth investigations of acute questions and mid-term reports of work in progress. The dissertations touch all three main areas of the PIKM, including, e.g., work on community discovery in Twitter, linked data, process modeling, advertising in social networks, and event detection. The best submission will receive a special best paper award. As a special highlight, this year's PIKM features a keynote talk by Dr. Ingmar Weber from Yahoo! Research Barcelona. Dr. Weber said he will give "Advice for Young Jedi Knights and PhD Students". May the force be with us.

#index 1912649
#* Proceedings of the 2012 international workshop on Smart health and wellbeing
#@ Christopher C. Yang;Hsinchun Chen;Howard Wactlar;Carlo Combi;Xuning Tang
#t 2012
#c 1
#! It is our great pleasure to welcome you to the 2012 ACM CIKM International Workshop on Smart Health and Wellbeing -- SHB'12. The mission of SHB workshop is to develop a platform for authors to discuss fundamental principles, algorithms or applications of intelligent data acquisition, processing and analysis of healthcare data. We are particularly interested in information and knowledge management papers, in which the approaches are accompanied by an in-depth experimental evaluation with real world data. SHB gives researchers and practitioners a unique opportunity to share their perspectives with others interested in the various aspects of healthcare. The call for papers attracted submissions from Asia, Europe, and the United States. The program committee accepted nine papers for oral presentations. These papers cover a variety of topics, including knowledge discovery from health data warehouse, data schema design, pharmaceutical informatics, medical record retrieval, mobile applications and simulations. We hope that these proceedings will serve as a valuable reference for healthcare researchers and developers.

#index 1912687
#* Proceedings of the first workshop on Information and knowledge management for developing region
#@ Rakesh Agrawal;Douglas W. Oard;Nitendra Rajput
#t 2012
#c 1
#! It is our great pleasure to welcome you to the First Workshop on Information and Knowledge Management for Developing Regions, IKM4DR'12. This workshop aims to bring the data management, information retrieval, and knowledge management communities together around questions of technology support for societal development. Our twin goals are to help to identify new applications for existing information and knowledge management technologies in developing regions and to inspire new research on problems that arise in developing region contexts. We anticipate a mix of industrial and academic participation. The workshop call was focused on three main themes: Components: Development of tools for low-resource languages that have the potential to transform information and knowledge management for those languages. Applications: Ways of employing new and existing data management, information retrieval and knowledge management technology to novel and important challenges that arise in the context of developing regions. Fostering adoption: Working together with partners from developing regions to craft solutions that embed societal expectations and values in ways that make it possible to achieve the envisioned impact. The call for papers attracted 10 submissions from India, China, Japan, and the United States. The program committee accepted 7 papers that cover a variety of topics, including voice search systems, domain specific search for low-resource languages and several applications in developing regions with respect to content management. In addition, the program includes invited talk on Text Books for Developing Regions, and a keynote speech by Ricardo Baeza-Yates. We hope that these proceedings will serve as a valuable reference for researchers and developers who are looking to bventure in the area of applying technologies for developing regions.

#index 1912764
#* Proceedings of the twelfth international workshop on Web information and data management
#@ George H.L. Fletcher;Prasenjit Mitra
#t 2012
#c 1
#! This volume contains the proceedings of the twelfth International Workshop on Web Information and Data Management, held in Maui in conjunction with CIKM 2012 on November 2, 2012. These proceedings consist of an abstract based on the keynote address by Maria Grineva and 10 contributed papers that were selected from 16 submissions by the program committee for presentation at the workshop. The objective of the workshop since its inception at CIKM 1998 is to bring together researchers and industrial practitioners to present and discuss leading research into how web data and information can be extracted, stored, analyzed, and processed to provide useful knowledge to end users for advanced database and web applications. The rich WIDM'12 program continues this tradition. We hope that the reader finds inspiration and stimulation in the contributed papers.

#index 1912831
#* Proceedings of the fourth international workshop on Cloud data management
#@ Xiaofeng Meng;Adam Silberstein;Fusheng Wang
#t 2012
#c 1
#! It is our great pleasure to welcome you to the Fourth International Workshop on Cloud Data Management (CloudDB 2012). This year we continue our tradition of serving as a premier forum for researchers and practitioners to present research results and share ideas and progress in the area of data management within cloud computing infrastructure. This broad area includes work in distributed storage, parallel algorithms, data mining, serving and analytic workloads, privacy, security, green computing, social workloads, and many others. The call for papers attracted a wide range of submissions. The program committee accepted 7 papers on a variety of topics. In addition, the program includes 4 keynote speakers from leading cloud computing researchers: Dr. Carlo Curino, Dr. Mohamed Sharaf, Prof. Geoffrey Fox and Prof. Ashwin Machanavajjhala. We hope these proceedings will serve as a valuable resource to learn about the latest and most exciting work in cloud computing.

#index 1912843
#* Proceedings of the 1st international workshop on Multimodal crowd sensing
#@ Haggai Roitman;Iván Cantador;Miriam Fernandez
#t 2012
#c 1
#! It is our great pleasure to welcome you to the 1st International Workshop on Multimodal Crowd Sensing -- CrowdSens 2012, held at CIKM 2021 in Maui, Hawaii, USA, on November 2nd, 2012. The ubiquitous availability of computing technology, in particular smartphones, tablets, laptops and other easily portable devices, and the adoption of social networking sites, makes it possible to be connected and continuously contribute to a massively distributed information publishing process. By doing so, users are (unconsciously) acting as social sensors, whose sensor readings are their manually generated data. People document their daily life experiences, report on their physical locations and social interactions with others, express opinions and provide diverse observations on both the physical world (sights, sounds, smells, feelings, etc.) and the online world (news, music, events, etc.). Such massive amounts of ubiquitous social sensors, if wisely utilized, can provide new forms of valuable information that are currently not available by any traditional data collection methods including real physical sensors, and can be used to enhance decision making processes. The main goal of the 1st International Workshop on Multimodal Crowd Sensing (CrowdSens 2012) was to provide an open forum for researchers from various fields such as Natural Language Processing, Information Extraction, Data Mining, Information Retrieval, User Modeling and Personalization, Stream Processing, and Sensor Networks, for addressing the challenges of effectively mining, analyzing, fusing, and exploiting information from multimodal physical and social sensor data sources. Many factors are included into the complexity of the problem, such as the real-time requirements of the data processing; the heterogeneity of the data sources, from physical sensors data to posts on social media; and the ubiquitous and noisy nature of the human-sensor generated information, which can be written in an informal style, and can be redundant, incomplete, or even incorrect. The workshop thus aimed to stimulate discussions about how the knowledge embedded in human sensor data can be collected, extracted, modeled, analyzed, integrated, summarized, and finally exploited. The workshop consisted of: keynote talks by Ido Guy (IBM Research - Haifa, Israel) and Manuel Cebrián (University of California, San Diego, California, USA), three research talks and two posters, selected by the program committee from a total of nine submissions, with topics spanning from crowdsourcing modelling to event detection in the Social Media, and an open panel for discussing advances and open issues on management and exploitation of knowledge embedded in human sensor data, serving as an open stage for interdisciplinary collaboration for facing the emerging research challenges.

#index 1912850
#* Proceedings of the fifteenth international workshop on Data warehousing and OLAP
#@ Il-Yeol Song;Matteo Golfarelli
#t 2012
#c 1
#! Research in data warehousing and OLAP has produced important technologies for the design, management and use of information systems for decision support. Much of the interest and success in this area can be attributed to the need for software and tools to improve data management and analysis given the large amount of information that are being accumulated in corporate as well as scientific databases. However, even though the high maturity of these technologies, new data needs or applications currently run at companies not only demand more capacity, but also new methods, models, techniques and architectures to satisfy these needs. The ACM International Workshop on Data Warehousing and OLAP -- DOLAP is an annual event that provides an international forum where both researchers and practitioners can share their findings in theoretical foundations, current methodologies, practical experiences, and new research directions in the areas of data warehousing and online analytical processing. This year the call for papers attracted 33 submissions from all the five continents. After a careful review the program committee accepted 10 full papers and 8 short ones, making an overall acceptance rate of 54% (30% for full papers). The paper topics reflect the hot issues in the data warehouse area: warehousing and OLAP on complex data, performance optimization and benchmarking, ontology-based OLAP, approximate query answering, data warehouse design in complex or distributed architectures. The presentations have been organized in four sessions: Data Warehouse Design and Maintainability, OLAP Query Processing and Trends, Performance Optimization and Benchmarking, and Warehousing of Complex Data.

#index 1912869
#* Proceedings of the ACM sixth international workshop on Data and text mining in biomedical informatics
#@ Doheon Kim;Sophia Ananiadou;Min Song;Hua Xu
#t 2012
#c 1
#! It is our great pleasure to welcome you to the ACM Sixth International Workshop on Data and Text Mining in Biomedical Informatics (DTMBIO'12), in conjunction with the 21st ACM International Conference on Information and Knowledge Management (CIKM'12). Biomedical researchers face the current challenge of making effective use of the enormous amount of electronic biomedical data in order to better understand and explain complex biological systems. The biomedical data repositories include data in a wide variety of forms, including genomic sequences, gene expression profiles, proteomics, metabolomics, epigenomics, microbiomics, electronics medical records, literature information, and so on. The ability to automatically and effectively extract, integrate, understand and make use of information embedded in such heterogeneous - structured and unstructured - data remains a challenging task. The aim of the 2012 workshop has been to bring together researchers in the areas of data and text mining and computational biology, who are interested in integrating and analyzing heterogeneous, structured and unstructured data. The papers accepted for presentation and publication in this volume cover a variety of topics, including biomedical text mining, data-driven hypothesis generation, clinical genomics, systems biology, proteomics, and clinical test design. We hope that these proceedings will serve as a valuable and up-to-date reference concerning the application of data- and textmining techniques within biomedical informatics.

#index 1912898
#* Proceedings of the fifth ACM workshop on Research advances in large digital book repositories and complementary media
#@ Gabriella Kazai;Monica Landoni;Carsten Eickhoff;Peter Brusilovsky
#t 2012
#c 1
#! It is our great pleasure to welcome you to the BooksOnline 2012 Workshop on Research Advances in Large Digital Book Repositories and Complementary Media. This year, the fifth edition of the workshop series focuses on the readers, their experiences and their specific needs and preferences in terms of content selection, presentation and adjunct comprehension support technologies. In many domains, such as Web search, content recommendation, and online social networks, personalized services have been able to greatly benefit users. The domain of digital reading is no exception in this trend. When browsing online book stores, reading group discussions or book recommendations shared among friends, the strong personal aspects that apply to book selection become obvious. This becomes especially relevant in the case of user groups with specific or special needs, such as young readers or people with disabilities. The central role of the reader is reflected in the range of accepted contributions to this year's workshop. The accepted contributions naturally showed three salient themes: (1) "Search and Discovery", encompassing work addressing serendipitous content discovery, customizable information filtering as well as faceted topic exploration; (2) "Personalization and Recommendation", introducing reader-centric means of content selection based on comprehensibility and entertainment; and (3) "Reading Experiences beyond Text", proposing novel content presentation paradigms addressing accessibility concerns. We are honoured to welcome as our keynote speakers two industry leaders and visionaries, Maribeth Back (FX Labs Palo Alto) who will share her work on the "Future of Digital Reading" and Natasa Milic-Frayling (Microsoft Research) who will talk about her research on techniques to facilitate long-term access to digital content. We hope that you will find the workshop's program interesting and thought-provoking and that the workshop will provide you with a valuable opportunity to share ideas with other researchers and practitioners from institutions around the world.

#index 1912908
#* Proceedings of the 2012 workshop on Data-driven user behavioral modelling and mining from social media
#@ Jalal Mahmud;James Caverlee;Jeffrey Nichols;John O'Donovan;Michelle Zhou
#t 2012
#c 1
#! It is our great pleasure to welcome you to the workshop on Data-driven user behavioral modeling and mining from social media (DUBMMSM), co-located with 21st ACM Conference on Information and Knowledge Management -- CIKM'12. This workshop builds upon the success of our previously held workshop User modeling from social media (UMSOCIAL), in conjunction with ACM IUI 2012 and focuses more on data-centric analysis and mining aspects of user and user behavior understanding from social media. The call for papers attracted 16 submissions. We accepted 12 papers - 7 for oral presentation and 5 for minute madness presentation. The papers represent various topics such as modeling collective emotions, understanding mood transitions, understanding temporal user preference, understanding user interest, behavioral model and targeting, characterizing user communities, understanding social media friendship and sentiment analysis. We hope that workshop proceedings will serve as a valuable reference for researchers and practitioners from diverse areas, such as user modeling, social media analysis, natural language processing, data mining and machine learning.

#index 1912921
#* Proceedings of the fifth workshop on Exploiting semantic annotations in information retrieval
#@ Jaap Kamps;Jussi Karlgren;Peter Mika;Vanessa Murdock
#t 2012
#c 1
#! These proceedings contain the contributed papers of the Fifth Workshop on Exploiting Semantic Annotations in Information Retrieval (ESAIR 2012), held at CIKM 2012 at Maui, Hawaii, on November 2, 2012. After successful workshops at ECIR'08 in Glasgow, WSDM'09 in Barcelona, CIKM'10 in Toronto, and CIKM'11 in Glasgow, this year's workshop will focus on how to leverage the rich context currently available, especially in a mobile search scenario, giving powerful new handles to exploit semantic annotations. And how can we fruitfully combine information retrieval and semantic web approaches, and for the first time work actively toward a unified view on exploiting semantic annotations. ESAIR'12 will be a real workshop where researchers from these different disciplines will work together to identify natural use cases, barriers to success, and work on ways of addressing them: Application/Use Case: What are use cases that make obvious the need for semantic annotation of information? What tasks cannot be solved by document retrieval using the traditional bag-of-words? What is keeping searchers from exploring these powerful search requests? Annotations: What types of annotation are available? Are there crucial differences between author-, software-, user-, and machine-generated annotations? How similar or different are linked data and annotated text? What is impact of the web of data with more and more information in preprocessed form? Rich Context: Besides personalization and geo-positional information, mobiles have a wide and growing range of locational, mechanical and even biometrical sensor data available to them. Can kick-start the query by inferring task and situational context in the mobile use case? (Un)certainty: How should we interpret the annotations? Can expect a messy world to be captured in a clean set of meaningful categories? Or is all information fundamentally uncertain and only partly known? How can we fruitfully combine information retrieval and semantic web approaches? These and other related questions will be discussed at this open format workshop --- the aim is to provide paths for further research to change the way we understand information access today! The workshop will consist of three main parts: A keynote to help us formulate the challenges. A boaster and poster session with 10 papers selected by the program committee from 13 submissions (77%). Each paper was reviewed by at least two members of the program committee. Break out groups on different aspects of exploiting semantic annotations, with reports being discussed in the final session.

#index 1912973
#* Proceedings of the 2012 ACM workshop on City data management workshop
#@ Veli Bicer;Thanh Tran;Fatma Ozcan;Opher Etzion
#t 2012
#c 1
#! It is our great pleasure to welcome you to the City Data Management Workshop 2012 (CDMW'12), co-located with the 21st ACM International Conference on Information and Knowledge Management (CIKM 2012) at Hawaii, USA. The workshop aims to provide a forum to discuss challenges and opportunities of city data management and to promote interaction among experts in different domains. In this regard, one of the main objectives includes the positioning of the city data management topic among the other types of data management (e.g. enterprise, Web) by identifying its unique characteristics (e.g. heterogeneity, openness, geospatially, privacy, data source variety etc.) and by listing the requirements for managing such data. Another objective is also to show how the particular challenges (e.g. urban planning, resource optimization, emergency evacuation, transportation planning) of cities can be addressed by the information contained within the vast amount of available heterogeneous city data. Thus CDMW'12 gives researchers and practitioners a unique opportunity to share their perspectives with others interested in the various aspects of city data management. The CDMW'12 program committee accepted 3 papers that cover a variety of topics, including data integration, data mapping, provenance, urban planning, ubiquitous sensing, spatial aggregation and high performance computing over city data. In addition, this year we have co-located CDMW'12 with the 2012 International Workshop on Web-scale Knowledge Representation, Retrieval, and Reasoning (Web-KR 2012). Thus we are delighted to have the opportunity to discuss the topic of city data management together with researchers from the Web, high performance computing, knowledge mmanagement and representation, and machine learning in a great depth.

#index 1918331
#* Proceedings of the 21st ACM international conference on Information and knowledge management
#@ Xuewen Chen;Guy Lebanon;Haixun Wang;Mohammed J. Zaki
#t 2012
#c 1
#! On behalf of the organizing committee, it is my genuine honor and great pleasure to welcome you to the 21st ACM International Conference on Information and Knowledge Management (CIKM 2012) in Maui, Hawaii! I hope this conference proves to be both interesting and beneficial. Since its inception, the CIKM conference has provided a unique international forum for the presentation, discussion and dissemination of research findings in data management, information retrieval and knowledge management. The purpose of the conference is to identify challenging problems facing the development of future knowledge and information systems and to shape future research directions though the publication of high quality, applied and theoretical research findings. The conference has been a leading forum in which experts from academic, industry and the public sector gather to exchange ideas, research achievements and technical developments in multidisciplinary research areas. As one of the world's most recognized conferences in the field, this year's CIKM conference has received a record high number of submissions in the history of CIKM, as can be seen from the following statistics: 1492 abstracts submitted 1088 full papers, 229 posters, and 70 demo papers submitted 146 papers accepted for presentation as full papers (13.4% acceptance rate) and 157 papers were accepted for short papers (27.8% cumulative acceptance rate) The increased number of submissions alone is a great demonstration of the lively research areas that contribute to the CIKM area. In addition, CIKM 2012 will host 15 workshops on cutting-edge areas of research and a dedicated Industry Event featuring leading industrial practitioners. We are grateful to all authors who chose to submit their work to CIKM 2012 and are very excited by the final program. CIKM values interdisciplinary research and we are proud to present three keynote speakers for the main conference (Dr. Ricardo Baeza-yates, Prof. William Cohen, and Prof. Jeffrey S. Vitter) and four keynote speakers for the Industry Event (Drs. Eric Brill, Raghu Ramakrishnan, Tom Malloy, and Xuedong Huang), all of whom will give presentations that cross discipline boundaries. I deeply appreciate their time and commitment to deliver their speeches and share their cutting-edge research experiences and insightful comments in their research topics.

#index 1918332
#* User engagement: the network effect matters!
#@ Ricardo Baeza-Yates;Mounia Lalmas
#t 2012
#c 1
#% 1747087
#% 1765173
#% 1888037
#! In the online world, user engagement refers to the quality of the user experience that emphasizes the positive aspects of the interaction with a web application and, in particular, the phenomena associated with wanting to use that application longer and frequently. This definition is motivated by the observation that successful web applications are not just used, but they are engaged with. Users invest time, attention, and emotion into them. Online providers aim not only to engage users with each service, but across all services in their network. They spend increasing effort to direct users to various services (e.g.~using hyperlinks to help users navigate to and explore other services), to increase user traffic between their services. Nothing is known for users engaging across such a network of Web sites, something we call networked user engagement. We address this problem by combining techniques from web analytics and mining, information retrieval evaluation, and existing works on user engagement coming from the domains of information science, multimodal human computer interaction and cognitive psychology. In this way, we can combine insights from big data with deep analysis of human behavior in the lab or through crowd-sourcing experiments.

#index 1918333
#* Learning similarity measures based on random walks
#@ Willia W. Cohen
#t 2012
#c 1
#! We describe a novel learnable proximity measure based on personalized PageRank (also known as "random walk with reset"). Instead of introducing one weight per edge label, as in most prior work, we introduce one weight for each edge label sequence. We show that this approach is advantageous for a number of real-world tasks, including querying graph databases, recommendation tasks, and inference in large, noisy knowledge bases.

#index 1918334
#* Compressed data structures with relevance
#@ Jeffrey Scott Vitter
#t 2012
#c 1
#% 300103
#% 379448
#% 453572
#% 497916
#% 593970
#% 754755
#% 823464
#% 835496
#% 919830
#% 936965
#% 1022545
#% 1074714
#% 1379539
#% 1404889
#% 1485982
#% 1668251
#! We describe recent breakthroughs in the field of compressed data structures, in which the data structure is stored in a compressed representation that still allows fast answers to queries. We focus in particular on compressed data structures to support the important application of pattern matching on massive document collections. Given an arbitrary query pattern in textual form, the job of the data structure is to report all the locations where the pattern appears. Another variant is to report all the documents that contain at least one instance of the pattern. We are particularly interested in reporting only the most relevant documents, using a variety of notions of relevance. We discuss recently developed techniques that support fast search in these contexts as well as under additional positional and temporal constraints.

#index 1918335
#* LogUCB: an explore-exploit algorithm for comments recommendation
#@ Dhruv Kumar Mahajan;Rajeev Rastogi;Charu Tiwari;Adway Mitra
#t 2012
#c 1
#% 425053
#% 722904
#% 722906
#% 879627
#% 891559
#% 983894
#% 992949
#% 1042718
#% 1073970
#% 1073982
#% 1190068
#% 1214623
#% 1261574
#% 1270702
#% 1318590
#% 1355025
#% 1399999
#% 1400022
#% 1560422
#% 1711777
#! The highly dynamic nature of online commenting environments makes accurate ratings prediction for new comments challenging. In such a setting, in addition to exploiting comments with high predicted ratings, it is also critical to explore comments with high uncertainty in the predictions. In this paper, we propose a novel upper confidence bound (UCB) algorithm called LOGUCB that balances exploration with exploitation when the average rating of a comment is modeled using logistic regression on its features. At the core of our LOGUCB algorithm lies a novel variance approximation technique for the Bayesian logistic regression model that is used to compute the UCB value for each comment. In experiments with a real-life comments dataset from Yahoo! News, we show that LOGUCB with bag-of-words and topic features outperforms state-of-the-art explore-exploit algorithms.

#index 1918336
#* DQR: a probabilistic approach to diversified query recommendation
#@ Ruirui Li;Ben Kao;Bin Bi;Reynold Cheng;Eric Lo
#t 2012
#c 1
#% 210173
#% 262112
#% 310567
#% 330617
#% 411762
#% 838531
#% 840846
#% 878624
#% 987193
#% 987203
#% 1074113
#% 1083721
#% 1130854
#% 1227619
#% 1292743
#% 1482240
#% 1560358
#% 1641944
#% 1712595
#! Web search queries issued by casual users are often short and with limited expressiveness. Query recommendation is a popular technique employed by search engines to help users refine their queries. Traditional similarity-based methods, however, often result in redundant and monotonic recommendations. We identify five basic requirements of a query recommendation system. In particular, we focus on the requirements of redundancy-free and diversified recommendations. We propose the DQR framework, which mines a search log to achieve two goals: (1) It clusters search log queries to extract query concepts, based on which recommended queries are selected. (2) It employs a probabilistic model and a greedy heuristic algorithm to achieve recommendation diversification. Through a comprehensive user study we compare DQR against five other recommendation methods. Our experiment shows that DQR outperforms the other methods in terms of relevancy, diversity, and ranking performance of the recommendations.

#index 1918337
#* Dynamic covering for recommendation systems
#@ Ioannis Antonellis;Anish Das Sarma;Shaddin Dughmi
#t 2012
#c 1
#% 224463
#% 262112
#% 347225
#% 580676
#% 793252
#% 813966
#% 879618
#% 929605
#% 1083629
#% 1676479
#! In this paper, we identify a fundamental algorithmic problem that we term succinct dynamic covering (SDC), arising in many modern-day web applications, including ad-serving and online recommendation systems such as in eBay, Netflix, and Amazon. Roughly speaking, SDC applies two restrictions to the well-studied Max-Coverage problem [14]: Given an integer k, X={1,2,...,n}and I={S_1,...,S_m}, S_i subseteq X, find |J| subseteq I, such that |J| We present algorithms and complexity results for coverage oracles. We present deterministic and probabilistic near-tight upper and lower bounds on the approximation ratio of SDC as a function of the amount of space available to the oracle. Our lower bound results show that to obtain constant-factor approximations we need Omega(mn) space. Fortunately, our upper bounds present an explicit tradeoff between space and approximation ratio, allowing us to determine the amount of space needed to guarantee certain accuracy.

#index 1918338
#* MEET: a generalized framework for reciprocal recommender systems
#@ Lei Li;Tao Li
#t 2012
#c 1
#% 342621
#% 729918
#% 835906
#% 961152
#% 987339
#% 989597
#% 1073905
#% 1078582
#% 1176909
#% 1292681
#% 1450837
#% 1476471
#% 1482274
#% 1489892
#% 1565432
#% 1607074
#% 1625396
#% 1826412
#! Reciprocal recommender systems refer to systems from which users can obtain recommendations of other individuals by satisfying preferences of both parties being involved. Different from the traditional user-item recommendation, reciprocal recommenders focus on the preferences of both parties simultaneously, as well as some special properties in terms of "reciprocal". In this paper, we propose MEET -- a generalized framework for reciprocal recommendation, in which we model the correlations of users as a bipartite graph that maintains both local and global "reciprocal" utilities. The local utility captures users' mutual preferences, whereas the global utility manages the overall quality of the entire reciprocal network. Extensive empirical evaluation on two real-world data sets (online dating and online recruiting) demonstrates the effectiveness of our proposed framework compared with existing recommendation algorithms. Our analysis also provides deep insights into the special aspects of reciprocal recommenders that differentiate them from user-item recommender systems.

#index 1918339
#* Social contextual recommendation
#@ Meng Jiang;Peng Cui;Rui Liu;Qiang Yang;Fei Wang;Wenwu Zhu;Shiqiang Yang
#t 2012
#c 1
#% 220709
#% 330687
#% 722904
#% 783490
#% 1001279
#% 1083671
#% 1130901
#% 1214666
#% 1260273
#% 1287261
#% 1292542
#% 1482198
#% 1536533
#% 1560408
#% 1598352
#% 1598435
#% 1641992
#% 1641995
#% 1669913
#% 1868022
#! Exponential growth of information generated by online social networks demands effective recommender systems to give useful results. Traditional techniques become unqualified because they ignore social relation data; existing social recommendation approaches consider social network structure, but social context has not been fully considered. It is significant and challenging to fuse social contextual factors which are derived from users' motivation of social behaviors into social recommendation. In this paper, we investigate social recommendation on the basis of psychology and sociology studies, which exhibit two important factors: individual preference and interpersonal influence. We first present the particular importance of these two factors in online item adoption and recommendation. Then we propose a novel probabilistic matrix factorization method to fuse them in latent spaces. We conduct experiments on both Facebook style bidirectional and Twitter style unidirectional social network datasets in China. The empirical result and analysis on these two large datasets demonstrate that our method significantly outperform the existing approaches.

#index 1918340
#* Mining high utility itemsets without candidate generation
#@ Mengchi Liu;Junfeng Qu
#t 2012
#c 1
#% 443350
#% 447910
#% 481290
#% 729418
#% 829993
#% 867053
#% 984426
#% 985041
#% 1019450
#% 1083506
#% 1327654
#% 1451164
#% 1656318
#% 1688415
#% 1688439
#% 1699270
#% 1707858
#! High utility itemsets refer to the sets of items with high utility like profit in a database, and efficient mining of high utility itemsets plays a crucial role in many real-life applications and is an important research issue in data mining area. To identify high utility itemsets, most existing algorithms first generate candidate itemsets by overestimating their utilities, and subsequently compute the exact utilities of these candidates. These algorithms incur the problem that a very large number of candidates are generated, but most of the candidates are found out to be not high utility after their exact utilities are computed. In this paper, we propose an algorithm, called HUI-Miner (High Utility Itemset Miner), for high utility itemset mining. HUI-Miner uses a novel structure, called utility-list, to store both the utility information about an itemset and the heuristic information for pruning the search space of HUI-Miner. By avoiding the costly generation and utility computation of numerous candidate itemsets, HUI-Miner can efficiently mine high utility itemsets from the utility-lists constructed from a mined database. We compared HUI-Miner with the state-of-the-art algorithms on various databases, and experimental results show that HUI-Miner outperforms these algorithms in terms of both running time and memory consumption.

#index 1918341
#* A general framework to encode heterogeneous information sources for contextual pattern mining
#@ Weishan Dong;Wei Fan;Lei Shi;Changjin Zhou;Xifeng Yan
#t 2012
#c 1
#% 92776
#% 300120
#% 420087
#% 459006
#% 463903
#% 481290
#% 481588
#% 481758
#% 527021
#% 629708
#% 731604
#% 731609
#% 785396
#% 823342
#% 864462
#% 982764
#% 1044453
#% 1567974
#% 1660989
#% 1688416
#! Traditional pattern mining methods usually work on single data sources. However, in practice, there are often multiple and heterogeneous information sources. They collectively provide contextual information not available in any single source alone describing the same set of objects, and are useful for discovering hidden contextual patterns. One important challenge is to provide a general methodology to mine contextual patterns easily and efficiently. In this paper, we propose a general framework to encode contextual information from multiple sources into a coherent representation---Contextual Information Graph (CIG). The complexity of the encoding scheme is linear in both time and space. More importantly, CIG can be handled by any single-source pattern mining algorithms that accept taxonomies without any modification. We demonstrate by three applications of the contextual association rule, sequence and graph mining, that contextual patterns providing rich and insightful knowledge can be easily discovered by the proposed framework. It enables Contextual Pattern Mining (CPM) by reusing single-source methods, and is easy to deploy and use in real-world systems.

#index 1918342
#* Incorporating occupancy into frequent pattern mining for high quality pattern recommendation
#@ Linpeng Tang;Lei Zhang;Ping Luo;Min Wang
#t 2012
#c 1
#% 152934
#% 232136
#% 300120
#% 463903
#% 464873
#% 464989
#% 465003
#% 731608
#% 769889
#% 785336
#% 800181
#% 841959
#% 985041
#% 1165482
#% 1210653
#% 1396223
#% 1565634
#! Mining interesting patterns from transaction databases has attracted a lot of research interest for more than a decade. Most of those studies use frequency, the number of times a pattern appears in a transaction database, as the key measure for pattern interestingness. In this paper, we introduce a new measure of pattern interestingness, occupancy. The measure of occupancy is motivated by some real-world pattern recommendation applications which require that any interesting pattern X should occupy a large portion of the transactions it appears in. Namely, for any supporting transaction t of pattern X, the number of items in X should be close to the total number of items in t. In these pattern recommendation applications, patterns with higher occupancy may lead to higher recall while patterns with higher frequency lead to higher precision. With the definition of occupancy we call a pattern dominant if its occupancy is above a user-specified threshold. Then, our task is to identify the qualified patterns which are both frequent and dominant. Additionally, we also formulate the problem of mining top-k qualified patterns: finding the qualified patterns with the top-k values of any function (e.g. weighted sum of both occupancy and support). The challenge to these tasks is that the monotone or anti-monotone property does not hold on occupancy. In other words, the value of occupancy does not increase or decrease monotonically when we add more items to a given itemset. Thus, we propose an algorithm called DOFIA (DOminant and Frequent Itemset mining Algorithm), which explores the upper bound properties on occupancy to reduce the search process. The tradeoff between bound tightness and computational complexity is also systematically addressed. Finally, we show the effectiveness of DOFIA in a real-world application on print-area recommendation for Web pages, and also demonstrate the efficiency of DOFIA on several large synthetic data sets.

#index 1918343
#* PARMA: a parallel randomized algorithm for approximate association rules mining in MapReduce
#@ Matteo Riondato;Justin A. DeBrabant;Rodrigo Fonseca;Eli Upfal
#t 2012
#c 1
#% 152934
#% 300120
#% 434348
#% 481290
#% 481779
#% 614619
#% 629652
#% 789002
#% 809871
#% 818434
#% 887530
#% 946709
#% 1022310
#% 1023420
#% 1127463
#% 1399956
#% 1444745
#% 1446955
#% 1456834
#% 1605950
#% 1632939
#% 1635434
#% 1712895
#% 1842389
#! Frequent Itemsets and Association Rules Mining (FIM) is a key task in knowledge discovery from data. As the dataset grows, the cost of solving this task is dominated by the component that depends on the number of transactions in the dataset. We address this issue by proposing PARMA, a parallel algorithm for the MapReduce framework, which scales well with the size of the dataset (as number of transactions) while minimizing data replication and communication cost. PARMA cuts down the dataset-size-dependent part of the cost by using a random sampling approach to FIM. Each machine mines a small random sample of the dataset, of size independent from the dataset size. The results from each machine are then filtered and aggregated to produce a single output collection. The output will be a very close approximation of the collection of Frequent Itemsets (FI's) or Association Rules (AR's) with their frequencies and confidence levels. The quality of the output is probabilistically guaranteed by our analysis to be within the user-specified accuracy and error probability parameters. The sizes of the random samples are independent from the size of the dataset, as is the number of samples. They depend on the user-chosen accuracy and error probability parameters and on the parallel computational model. We implemented PARMA in Hadoop MapReduce and show experimentally that it runs faster than previously introduced FIM algorithms for the same platform, while 1) scaling almost linearly, and 2) offering even higher accuracy and confidence than what is guaranteed by the analysis.

#index 1918344
#* Interactive pattern mining on hidden data: a sampling-based solution
#@ Mansurul Bhuiyan;Snehasis Mukhopadhyay;Mohammad Al Hasan
#t 2012
#c 1
#% 329537
#% 466644
#% 480479
#% 481290
#% 576110
#% 577214
#% 580588
#% 629708
#% 729418
#% 743280
#% 763882
#% 813989
#% 844342
#% 881549
#% 1074096
#% 1206906
#% 1311697
#% 1372686
#% 1426573
#% 1536575
#% 1536581
#% 1560191
#% 1605977
#% 1606001
#% 1618915
#% 1663621
#! Mining frequent patterns from a hidden dataset is an important task with 43 various real-life applications. In this research, we propose a solution to this problem that is based on Markov Chain Monte Carlo (MCMC) sampling of frequent patterns. Instead of returning all the frequent patterns, the proposed paradigm returns a small set of randomly selected patterns so that the clandestinity of the dataset can be maintained. Our solution also allows interactive sampling, so that the sampled patterns can fulfill the user's requirement effectively. We show experimental results from several real life datasets to validate the capability and usefulness of our solution; in particular, we show examples that by using our proposed solution, an eCommerce marketplace can allow pattern mining on user session data without disclosing the data to the public; such a mining paradigm helps the sellers of the marketplace, which eventually boost the marketplace's own revenue.

#index 1918345
#* An analysis of systematic judging errors in information retrieval
#@ Gabriella Kazai;Nick Craswell;Emine Yilmaz;S.M.M Tahaghoghi
#t 2012
#c 1
#% 262105
#% 340892
#% 720198
#% 1015625
#% 1074134
#% 1130866
#% 1151011
#% 1450896
#% 1452857
#% 1478132
#% 1482232
#% 1536510
#% 1587349
#% 1598354
#% 1598440
#! Test collections are powerful mechanisms for the evaluation and optimization of information retrieval systems. However, there is reported evidence that experiment outcomes can be affected by changes to the judging guidelines or changes in the judge population. This paper examines such effects in a web search setting, comparing the judgments of four groups of judges: NIST Web Track judges, untrained crowd workers and two groups of trained judges of a commercial search engine. Our goal is to identify systematic judging errors by comparing the labels contributed by the different groups, working under the same or different judging guidelines. In particular, we focus on detecting systematic differences in judging depending on specific characteristics of the queries and URLs. For example, we ask whether a given population of judges, working under a given set of judging guidelines, are more likely to consistently overrate Wikipedia pages than another group judging under the same instructions. Our approach is to identify judging errors with respect to a consensus set, a judged gold set and a set of user clicks. We further demonstrate how such biases can affect the training of retrieval systems.

#index 1918346
#* On caption bias in interleaving experiments
#@ Katja Hofmann;Fritz Behr;Filip Radlinski
#t 2012
#c 1
#% 262036
#% 325001
#% 577224
#% 731615
#% 766472
#% 818207
#% 857180
#% 879567
#% 946521
#% 954948
#% 954949
#% 987209
#% 1035578
#% 1074092
#% 1130811
#% 1130814
#% 1166517
#% 1173703
#% 1173704
#% 1214757
#% 1250379
#% 1292763
#% 1355048
#% 1400034
#% 1415711
#% 1450873
#% 1450912
#% 1450952
#% 1641943
#% 1667281
#% 1693903
#! Information retrieval evaluation most often involves manually assessing the relevance of particular query-document pairs. In cases where this is difficult (such as personalized search), interleaved comparison methods are becoming increasingly common. These methods compare pairs of ranking functions based on user clicks on search results, thus better reflecting true user preferences. However, by depending on clicks, there is a potential for bias. For example, users have been previously shown to be more likely to click on results with attractive titles and snippets. An interleaving evaluation where one ranker tends to generate results that attract more clicks (without being more relevant) may thus be biased. We present an approach for detecting and compensating for this type of bias in interleaving evaluations. Introducing a new model of caption bias, we propose features that model bias based on (1) per-document effects, and (2) the (pairwise) relationships between a document and surrounding documents. We show that our model can effectively capture click behavior, with best results achieved by a model that combines both per-document and pairwise features. Applying this model to re-weight observed user clicks, we find a small overall effect on real interleaving comparisons, but also identify a case where initially detected preferences vanish after caption bias re-weighting is applied. Our results indicate that our model of caption bias is effective and can successfully identify interleaving experiments affected by caption bias.

#index 1918347
#* Alternative assessor disagreement and retrieval depth
#@ William Webber;Praveen Chandar;Ben Carterette
#t 2012
#c 1
#% 232703
#% 236052
#% 262105
#% 312689
#% 340890
#% 340936
#% 818276
#% 987199
#% 987238
#% 1015625
#% 1074134
#% 1083692
#% 1314931
#% 1450896
#% 1482232
#% 1879138
#! Assessors are well known to disagree frequently on the relevance of documents to a topic, but the factors leading to assessor disagreement are still poorly understood. In this paper, we examine the relationship between the rank at which a document is returned by a set of retrieval systems and the likelihood that a second assessor will disagree with the relevance assessment of the initial assessor, and find that there is a strong and consistent correlation between the two. We adopt a metarank method of summarizing a document's rank across multiple runs, and propose a logistic regression predictive model of second assessor disagreement given metarank and initially-assessed relevance. The consistency of the model parameters across different topics, assessor pairs, and collections is considered. The model gives comparatively accurate predictions of absolute system scores, but less consistent predictions of relative scores than a simpler rank-insensitive model. We demonstrate that the logistic regression model is robust to using sampled, rather than exhaustive, dual assessment. We demonstrate the use of the sampled predictive model to incorporate assessor disagreement into tests of statistical significance.

#index 1918348
#* Incorporating variability in user behavior into systems based evaluation
#@ Ben Carterette;Evangelos Kanoulas;Emine Yilmaz
#t 2012
#c 1
#% 248074
#% 310567
#% 342961
#% 1035578
#% 1074139
#% 1095876
#% 1130811
#% 1292528
#% 1366523
#% 1400017
#% 1482378
#% 1507026
#% 1641985
#% 1666906
#% 1747052
#! Click logs present a wealth of evidence about how users interact with a search system. This evidence has been used for many things: learning rankings, personalizing, evaluating effectiveness, and more. But it is almost always distilled into point estimates of feature or parameter values, ignoring what may be the most salient feature of users---their variability. No two users interact with a system in exactly the same way, and even a single user may interact with results for the same query differently depending on information need, mood, time of day, and a host of other factors. We present a Bayesian approach to using logs to compute posterior distributions for probabilistic models of user interactions. Since they are distributions rather than point estimates, they naturally capture variability in the population. We show how to cluster posterior distributions to discover patterns of user interactions in logs, and discuss how to use the clusters to evaluate search engines according to a user model. Because the approach is Bayesian, our methods can be applied to very large logs (such as those possessed by Web search engines) as well as very small (such as those found in almost any other setting).

#index 1918349
#* Constructing test collections by inferring document relevance via extracted relevant information
#@ Shahzad Rajput;Matthew Ekstrand-Abueg;Virgil Pavlu;Javed A. Aslam
#t 2012
#c 1
#% 116165
#% 169732
#% 255137
#% 262097
#% 340890
#% 342727
#% 384911
#% 411762
#% 520224
#% 544011
#% 562942
#% 643036
#% 763705
#% 766407
#% 879598
#% 881477
#% 891559
#% 939970
#% 940039
#% 1074133
#% 1130811
#% 1450896
#% 1452857
#% 1598438
#% 1641986
#% 1693901
#% 1806006
#! The goal of a typical information retrieval system is to satisfy a user's information need---e.g., by providing an answer or information "nugget"---while the actual search space of a typical information retrieval system consists of documents---i.e., collections of nuggets. In this paper, we characterize this relationship between nuggets and documents and discuss applications to system evaluation. In particular, for the problem of test collection construction for IR system evaluation, we demonstrate a highly efficient algorithm for simultaneously obtaining both relevant documents and relevant information. Our technique exploits the mutually reinforcing relationship between relevant documents and relevant information, yielding document-based test collections whose efficiency and efficacy exceed those of typical Cranfield-style test collections, while also generating sets of highly relevant information.

#index 1918350
#* Twevent: segment-based event detection from tweets
#@ Chenliang Li;Aixin Sun;Anwitaman Datta
#t 2012
#c 1
#% 262042
#% 262043
#% 309100
#% 445316
#% 577220
#% 643016
#% 824666
#% 987218
#% 989601
#% 1013086
#% 1399966
#% 1399992
#% 1400018
#% 1426611
#% 1470583
#% 1517726
#% 1536506
#% 1560198
#% 1560425
#% 1642033
#% 1711864
#% 1846751
#% 1879064
#! Event detection from tweets is an important task to understand the current events/topics attracting a large number of common users. However, the unique characteristics of tweets (e.g. short and noisy content, diverse and fast changing topics, and large data volume) make event detection a challenging task. Most existing techniques proposed for well written documents (e.g. news articles) cannot be directly adopted. In this paper, we propose a segment-based event detection system for tweets, called Twevent. Twevent first detects bursty tweet segments as event segments and then clusters the event segments into events considering both their frequency distribution and content similarity. More specifically, each tweet is split into non-overlapping segments (i.e. phrases possibly refer to named entities or semantically meaningful information units). The bursty segments are identified within a fixed time window based on their frequency patterns, and each bursty segment is described by the set of tweets containing the segment published within that time window. The similarity between a pair of bursty segments is computed using their associated tweets. After clustering bursty segments into candidate events, Wikipedia is exploited to identify the realistic events and to derive the most newsworthy segments to describe the identified events. We evaluate Twevent and compare it with the state-of-the-art method using 4.3 million tweets published by Singapore-based users in June 2010. In our experiments, Twevent outperforms the state-of-the-art method by a large margin in terms of both precision and recall. More importantly, the events detected by Twevent can be easily interpreted with little background knowledge because of the newsworthy segments. We also show that Twevent is efficient and scalable, leading to a desirable solution for event detection from tweets.

#index 1918351
#* Making your interests follow you on twitter
#@ Marco Pennacchiotti;Fabrizio Silvestri;Hossein Vahabi;Rossano Venturini
#t 2012
#c 1
#% 124004
#% 220706
#% 321635
#% 408396
#% 414514
#% 722904
#% 729923
#% 961697
#% 987328
#% 989613
#% 1055680
#% 1384223
#% 1399966
#% 1399992
#% 1484274
#% 1530857
#% 1536509
#% 1560169
#% 1560421
#! In this paper we introduce the task of "tweet recommendation", the problem of suggesting tweets that match a user's interests and likes. We propose an Information-Retrieval-like model that leverages the content of the user's tweets and those of her friends, and that effectively retrieves a set of tweets that is personalized and varied in nature. Our approach could be easily leveraged to build, for example, a Twitter or Facebook timeline that collects messages that are of interest for the user, but that are not posted by her friends. We compare to typical approaches used in similar tasks, reporting significant gains in terms of overall precision, up to about +20%, on both a corpus-based evaluation and real world user study.

#index 1918352
#* Generating event storylines from microblogs
#@ Chen Lin;Chun Lin;Jingxuan Li;Dingding Wang;Yang Chen;Tao Li
#t 2012
#c 1
#% 313959
#% 469401
#% 575570
#% 643520
#% 730070
#% 769897
#% 771924
#% 823344
#% 960414
#% 995518
#% 1022338
#% 1065814
#% 1074081
#% 1074086
#% 1074089
#% 1166534
#% 1214669
#% 1227584
#% 1227682
#% 1275040
#% 1399992
#% 1400018
#% 1426611
#% 1450965
#% 1476470
#% 1482447
#% 1484351
#% 1536506
#% 1556479
#% 1560422
#% 1581252
#% 1581900
#% 1587351
#% 1587369
#% 1598383
#% 1598408
#% 1598524
#! Microblogging service has emerged to be a dominant web medium for billions of individuals sharing and spreading instant news and information, therefore monitoring the event evolution on microblog sphere is crucial for providing both better user experience and deeper understanding on real-time events. In this paper we explore the problem of generating storylines from microblogs for user input queries. This problem is challenging due to the sparse, dynamic and social nature of microblogs. Given a query of an ongoing event, we propose to sketch the real-time storyline of the event by a two-level solution. We first propose a language model with dynamic pseudo relevance feedback to obtain relevant tweets, and then generate storylines via graph optimization. Comprehensive experiments on Twitter data sets demonstrate the effectiveness of the proposed methods in each level and the overall framework.

#index 1918353
#* Social book search: comparing topical relevance judgements and book suggestions for evaluation
#@ Marijn Koolen;Jaap Kamps;Gabriella Kazai
#t 2012
#c 1
#% 303504
#% 561315
#% 855601
#% 999301
#% 1002317
#% 1074124
#% 1074133
#% 1243045
#% 1478132
#% 1480444
#% 1495129
#% 1505097
#% 1587349
#% 1587350
#% 1598354
#! The Web and social media give us access to a wealth of information, not only different in quantity but also in character---traditional descriptions from professionals are now supplemented with user generated content. This challenges modern search systems based on the classical model of topical relevance and ad hoc search: How does their effectiveness transfer to the changing nature of information and to the changing types of information needs and search tasks? We use the INEX 2011 Books and Social Search Track's collection of book descriptions from Amazon and social cataloguing site LibraryThing. We compare classical IR with social book search in the context of the LibraryThing discussion forums where members ask for book suggestions. Specifically, we compare book suggestions on the forum with Mechanical Turk judgements on topical relevance and recommendation, both the judgements directly and their resulting evaluation of retrieval systems. First, the book suggestions on the forum are a complete enough set of relevance judgements for system evaluation. Second, topical relevance judgements result in a different system ranking from evaluation based on the forum suggestions. Although it is an important aspect for social book search, topical relevance is not sufficient for evaluation. Third, professional metadata alone is often not enough to determine the topical relevance of a book. User reviews provide a better signal for topical relevance. Fourth, user-generated content is more effective for social book search than professional metadata. Based on our findings, we propose an experimental evaluation that better reflects the complexities of social book search.

#index 1918354
#* Content-based crowd retrieval on the real-time web
#@ Krishna Y. Kamath;James Caverlee
#t 2012
#c 1
#% 118771
#% 249321
#% 340942
#% 347225
#% 479973
#% 575570
#% 722904
#% 823395
#% 939408
#% 993961
#% 1015261
#% 1077150
#% 1206700
#% 1206845
#% 1217579
#% 1400018
#% 1451233
#% 1470583
#% 1488059
#% 1535324
#% 1536563
#% 1560425
#! In this paper, we propose and evaluate a novel content-driven crowd discovery algorithm that can efficiently identify newly-formed communities of users from the real-time web. Short-lived crowds reflect the real-time interests of their constituents and provide a foundation for user-focused web monitoring. Three of the salient features of the algorithm are its: (i) prefix-tree based locality-sensitive hashing approach for discovering crowds from high-volume rapidly-evolving social media; (ii) efficient user profile updating for incorporating new user activities and fading older ones; and (iii) key dimension identification, so that crowd detection can be focused on the most active portions of the real-time web. Through extensive experimental study, we find significantly more efficient crowd discovery as compared to both a k-means clustering-based approach and a MapReduce-based implementation, while maintaining high-quality crowds as compared to an offline approach. Additionally, we find that expert crowds tend to be "stickier" and last longer in comparison to crowds of typical users.

#index 1918355
#* Graph classification: a diversified discriminative feature selection approach
#@ Yuanyuan Zhu;Jeffrey Xu Yu;Hong Cheng;Lu Qin
#t 2012
#c 1
#% 466644
#% 629644
#% 629708
#% 727845
#% 729938
#% 769940
#% 769951
#% 793250
#% 813990
#% 814023
#% 1063502
#% 1083688
#% 1207028
#% 1292523
#% 1328170
#% 1328171
#% 1426575
#% 1587176
#! A graph models complex structural relationships among objects, and has been prevalently used in a wide range of applications. Building an automated graph classification model becomes very important for predicting unknown graphs or understanding complex structures between different classes. The graph classification framework being widely used consists of two steps, namely, feature selection and classification. The key issue is how to select important subgraph features from a graph database with a large number of graphs including positive graphs and negative graphs. Given the features selected, a generic classification approach can be used to build a classification model. In this paper, we focus on feature selection. We identify two main issues with the most widely used feature selection approach which is based on a discriminative score to select frequent subgraph features, and introduce a new diversified discriminative score to select features that have a higher diversity. We analyze the properties of the newly proposed diversified discriminative score, and conducted extensive performance studies to demonstrate that such a diversified discriminative score makes positive/negative graphs separable and leads to a higher classification accuracy.

#index 1918356
#* Multi-scale link prediction
#@ Donghyuk Shin;Si Si;Inderjit S. Dhillon
#t 2012
#c 1
#% 730089
#% 1013696
#% 1080078
#% 1083675
#% 1117026
#% 1291642
#% 1451163
#% 1524266
#% 1536568
#% 1556172
#% 1606045
#% 1688463
#% 1800699
#! The automated analysis of social networks has become an important problem due to the proliferation of social networks, such as LiveJournal, Flickr and Facebook. The scale of these social networks is massive and continues to grow rapidly. An important problem in social network analysis is proximity estimation that infers the closeness of different users. Link prediction, in turn, is an important application of proximity estimation. However, many methods for computing proximity measures have high computational complexity and are thus prohibitive for large-scale link prediction problems. One way to address this problem is to estimate proximity measures via low-rank approximation. However, a single low-rank approximation may not be sufficient to represent the behavior of the entire network. In this paper, we propose Multi-Scale Link Prediction (MSLP), a framework for link prediction, which can handle massive networks. The basic idea of MSLP is to construct low-rank approximations of the network at multiple scales in an efficient manner. To achieve this, we propose a fast tree-structured approximation algorithm. Based on this approach, MSLP combines predictions at multiple scales to make robust and accurate predictions. Experimental results on real-life datasets with more than a million nodes show the superior performance and scalability of our method.

#index 1918357
#* An analysis of how ensembles of collective classifiers improve predictions in graphs
#@ Hoda Eldardiry;Jennifer Neville
#t 2012
#c 1
#% 132583
#% 209021
#% 420054
#% 451218
#% 529493
#% 844322
#% 961268
#% 1047784
#% 1089648
#% 1176994
#% 1499573
#% 1688440
#% 1693530
#! We present a theoretical analysis framework that shows how ensembles of collective classifiers can improve predictions for graph data. We show how collective ensemble classification reduces errors due to variance in learning and more interestingly inference. We also present an empirical framework that includes various ensemble techniques for classifying relational data using collective inference. The methods span single- and multiple-graph network approaches, and are tested on both synthetic and real world classification tasks. Our experimental results, supported by our theoretical justifications, confirm that ensemble algorithms that explicitly focus on both learning and inference processes and aim at reducing errors associated with both, are the best performers.

#index 1918358
#* Density index and proximity search in large graphs
#@ Nan Li;Xifeng Yan;Zhen Wen;Arijit Khan
#t 2012
#c 1
#% 93664
#% 227894
#% 296571
#% 397378
#% 660011
#% 763882
#% 790142
#% 824693
#% 863389
#% 907289
#% 960259
#% 1055877
#% 1063537
#% 1073984
#% 1075132
#% 1075875
#% 1178476
#% 1207007
#% 1214668
#% 1449326
#% 1604093
#% 1606349
#% 1816764
#! Given a large real-world graph where vertices are associated with labels, how do we quickly find interesting vertex sets according to a given query? In this paper, we study label-based proximity search in large graphs, which finds the top-k query-covering vertex sets with the smallest diameters. Each set has to cover all the labels in a query. Existing greedy algorithms only return approximate answers, and do not scale well to large graphs. We propose a novel framework, called gDensity, which uses density index and likelihood ranking to find vertex sets in an efficient and accurate manner. Promising vertices are ordered and examined according to their likelihood to produce answers, and the likelihood calculation is greatly facilitated by density indexing. Techniques such as progressive search and partial indexing are further proposed. Experiments on real-world graphs show the efficiency and scalability of gDensity.

#index 1918359
#* Gelling, and melting, large graphs by edge manipulation
#@ Hanghang Tong;B. Aditya Prakash;Tina Eliassi-Rad;Michalis Faloutsos;Christos Faloutsos
#t 2012
#c 1
#% 150237
#% 282905
#% 324817
#% 577217
#% 577360
#% 725348
#% 729923
#% 754107
#% 868469
#% 1426255
#% 1451243
#% 1451246
#% 1496777
#% 1535389
#% 1535470
#% 1597390
#% 1642240
#% 1688538
#% 1746851
#% 1807559
#! Controlling the dissemination of an entity (e.g., meme, virus, etc) on a large graph is an interesting problem in many disciplines. Examples include epidemiology, computer security, marketing, etc. So far, previous studies have mostly focused on removing or inoculating nodes to achieve the desired outcome. We shift the problem to the level of edges and ask: which edges should we add or delete in order to speed-up or contain a dissemination? First, we propose effective and scalable algorithms to solve these dissemination problems. Second, we conduct a theoretical study of the two problems and our methods, including the hardness of the problem, the accuracy and complexity of our methods, and the equivalence between the different strategies and problems. Third and lastly, we conduct experiments on real topologies of varying sizes to demonstrate the effectiveness and scalability of our approaches.

#index 1918360
#* One seed to find them all: mining opinion features via association
#@ Zhen Hai;Kuiyu Chang;Gao Cong
#t 2012
#c 1
#% 329569
#% 722904
#% 727877
#% 740900
#% 755835
#% 769892
#% 843652
#% 854646
#% 939896
#% 939897
#% 1055682
#% 1211748
#% 1250237
#% 1292503
#% 1299639
#% 1305481
#% 1464068
#% 1481541
#% 1481636
#% 1484314
#% 1536586
#% 1543770
#% 1544171
#% 1566286
#! Feature-based opinion analysis has attracted extensive attention recently. Identifying features associated with opinions expressed in reviews is essential for fine-grained opinion mining. One approach is to exploit the dependency relations that occur naturally between features and opinion words, and among features (or opinion words) themselves. In this paper, we propose a generalized approach to opinion feature extraction by incorporating robust statistical association analysis in a bootstrapping framework. The new approach starts with a small set of feature seeds, on which it iteratively enlarges by mining feature-opinion, feature-feature, and opinion-opinion dependency relations. Two association model types, namely likelihood ratio tests (LRT) and latent semantic analysis (LSA), are proposed for computing the pair-wise associations between terms (features or opinions). We accordingly propose two robust bootstrapping approaches, LRTBOOT and LSABOOT, both of which need just a handful of initial feature seeds to bootstrap opinion feature extraction. We benchmarked LRTBOOT and LSABOOT against existing approaches on a large number of real-life reviews crawled from the cellphone and hotel domains. Experimental results using varying number of feature seeds show that the proposed association-based bootstrapping approach significantly outperforms the competitors. In fact, one seed feature is all that is needed for LRTBOOT to significantly outperform the other methods. This seed feature can simply be the domain feature, e.g., "cellphone" or "hotel". The consequence of our discovery is far reaching: starting with just one feature seed, typically just the domain concept word, LRTBOOT can automatically extract a large set of high-quality opinion features from the corpus without any supervision or labeled features. This means that the automatic creation of a set of domain features is no longer a pipe dream!

#index 1918361
#* Topic-driven reader comments summarization
#@ Zongyang Ma;Aixin Sun;Quan Yuan;Gao Cong
#t 2012
#c 1
#% 262112
#% 577355
#% 642990
#% 722904
#% 788094
#% 869500
#% 875959
#% 881054
#% 1016332
#% 1055680
#% 1055681
#% 1055682
#% 1074087
#% 1202162
#% 1281981
#% 1292503
#% 1292559
#% 1292698
#% 1318718
#% 1400022
#% 1451208
#% 1451218
#% 1484620
#% 1536586
#% 1642003
#% 1693879
#% 1693884
#% 1746840
#% 1746841
#% 1746842
#% 1826347
#! Readers of a news article often read its comments contributed by other readers. By reading comments, readers obtain not only complementary information about this news article but also the opinions from other readers. However, the existing ranking mechanisms for comments (e.g., by recency or by user rating) fail to offer an overall picture of topics discussed in comments. In this paper, we first propose to study Topic-driven Reader Comments Summarization (Torcs) problem. We observe that many news articles from a news stream are related to each other; so are their comments. Hence, news articles and their associated comments provide context information for user commenting. To implicitly capture the context information, we propose two topic models to address the Torcs problem, namely, Master-Slave Topic Model (MSTM) and Extended Master-Slave Topic Model (EXTM). Both models treat a news article as a master document and each of its comments as a slave document. MSTM model constrains that the topics discussed in comments have to be derived from the commenting news article. On the other hand, EXTM model allows generating words of comments using both the topics derived from the commenting news article, and the topics derived from all comments themselves. Both models are used to group comments into topic clusters. We then use two ranking mechanisms Maximal Marginal Relevance (MMR) and Rating & Length (RL) to select a few most representative comments from each comment cluster. To evaluate the two models, we conducted experiments on 1005 Yahoo! News articles with more than one million comments. Our experimental results show that EXTM significantly outperforms MSTM by perplexity. Through a user study, we also confirm that the comment summary generated by EXTM achieves better intra-cluster topic cohesion and inter-cluster topic diversity.

#index 1918362
#* Visualizing timelines: evolutionary summarization via iterative reinforcement between text and image streams
#@ Rui Yan;Xiaojun Wan;Mirella Lapata;Wayne Xin Zhao;Pu-Jen Cheng;Xiaoming Li
#t 2012
#c 1
#% 280835
#% 309096
#% 340883
#% 457912
#% 635689
#% 722927
#% 766460
#% 787502
#% 815920
#% 816173
#% 1074088
#% 1074089
#% 1227614
#% 1269818
#% 1275220
#% 1470687
#% 1471304
#% 1482206
#% 1598408
#% 1642168
#% 1711764
#! We present a novel graph-based framework for timeline summarization, the task of creating different summaries for different timestamps but for the same topic. Our work extends timeline summarization to a multimodal setting and creates timelines that are both textual and visual. Our approach exploits the fact that news documents are often accompanied by pictures and the two share some common content. Our model optimizes local summary creation and global timeline generation jointly following an iterative approach based on mutual reinforcement and co-ranking. In our algorithm, individual summaries are generated by taking into account the mutual dependencies between sentences and images, and are iteratively refined by considering how they contribute to the global timeline and its coherence. Experiments on real-world datasets show that the timelines produced by our model outperform several competitive baselines both in terms of ROUGE and when assessed by human evaluators.

#index 1918363
#* Fast multi-task learning for query spelling correction
#@ Xu Sun;Anshumali Shrivastava;Ping Li
#t 2012
#c 1
#% 83929
#% 131061
#% 236497
#% 274189
#% 464434
#% 465743
#% 762147
#% 770804
#% 815881
#% 829014
#% 840962
#% 916788
#% 939629
#% 939973
#% 983937
#% 1309353
#% 1338621
#% 1396688
#% 1471206
#% 1482412
#% 1484281
#% 1560366
#% 1573188
#% 1581478
#% 1591934
#% 1674792
#% 1688543
#% 1747073
#! In this paper, we explore the use of a novel online multi-task learning framework for the task of search query spelling correction. In our procedure, correction candidates are initially generated by a ranker-based system and then re-ranked by our multi-task learning algorithm. With the proposed multi-task learning method, we are able to effectively transfer information from different and highly biased training datasets, for improving spelling correction on all datasets. Our experiments are conducted on three query spelling correction datasets including the well-known TREC benchmark dataset. The experimental results demonstrate that our proposed method considerably outperforms the existing baseline systems in terms of accuracy. Importantly, the proposed method is about one order of magnitude faster than baseline systems in terms of training speed. Compared to the commonly used online learning methods which typically require more than (e.g.,) 60 training passes, our proposed method is able to closely reach the empirical optimum in about 5 passes.

#index 1918364
#* Cross-argument inference for implicit discourse relation recognition
#@ Yu Hong;Xiaopei Zhou;Tingting Che;Jianmin Yao;Qiaoming Zhu;Guodong Zhou
#t 2012
#c 1
#% 815909
#% 816183
#% 854106
#% 939668
#% 1265041
#% 1299622
#% 1330514
#% 1338563
#% 1411867
#% 1471251
#% 1544176
#% 1673558
#! Motivated by the critical importance of connectives in recognizing discourse relations, we present an unsupervised cross-argument inference mechanism to implicit discourse relation recognition. The basic idea is to infer the implicit discourse relation of an argument pair from a large number of comparable argument pairs, which are automatically retrieved from the web in an unsupervised way. In this way, the inference proceeds from explicit relations to implicit ones via connective as bridge. This kind of pair-to-pair inference is based on the assumption that two argument pairs with high content similarity (i.e. comparable argument pairs) should have similar discourse relationship. Evaluation on PDTB proves the effectiveness of our inference mechanism in implicit relation recognition to the four level-1 relations. It also shows that our mechanism significantly outperforms other alternatives.

#index 1918365
#* Interpreting keyword queries over web knowledge bases
#@ Jeffrey Pound;Alexander K. Hudek;Ihab F. Ilyas;Grant Weddell
#t 2012
#c 1
#% 464434
#% 474645
#% 660011
#% 816181
#% 824693
#% 956564
#% 993987
#% 1015325
#% 1022234
#% 1063536
#% 1120997
#% 1206910
#% 1264824
#% 1330534
#% 1399933
#% 1400010
#% 1409952
#% 1426467
#% 1426537
#% 1426566
#% 1471314
#% 1591988
#% 1641483
#% 1728984
#! Many keyword queries issued to Web search engines target information about real world entities, and interpreting these queries over Web knowledge bases can often enable the search system to provide exact answers to queries. Equally important is the problem of detecting when the reference knowledge base is not capable of answering the keyword query, due to lack of domain coverage. In this work we present an approach to computing structured representations of keyword queries over a reference knowledge base. We mine frequent query structures from a Web query log and map these structures into a reference knowledge base. Our approach exploits coarse linguistic structure in keyword queries, and combines it with rich structured query representations of information needs.

#index 1918366
#* RDF pattern matching using sortable views
#@ Zhihong Chong;He Chen;Zhenjie Zhang;Hu Shu;Guilin Qi;Aoying Zhou
#t 2012
#c 1
#% 237190
#% 248038
#% 288990
#% 464056
#% 572311
#% 599549
#% 726626
#% 798967
#% 824755
#% 871765
#% 1016146
#% 1022279
#% 1055731
#% 1063515
#% 1127380
#% 1190676
#% 1207020
#% 1292635
#% 1357868
#% 1366460
#% 1529012
#% 1603793
#% 1654043
#% 1776509
#! In the last few years, RDF is becoming the dominating data model used in semantic web for knowledge representation and inference. In this paper, we revisit the problem of pattern matching query in RDF model, which is usually expensive in efficiency due to the huge cost on join operations. To alleviate the efficiency pain, view materialization techniques are usually deployed to accelerate the query processing. However, given an arbitrary view, it remains difficult to identify how to reuse the view for a particular query, because of the NP-hardness behind the algorithm matching patterns and views. To fully exploit the benefit of the materialized views, we propose a new paradigm to enhance the effectiveness of the materialized view. Instead of choosing materialized views in arbitrary form, our paradigm aims to select the views only if they are sortable. The property of sortability raises huge gains on the pattern-view matching, bringing down the cost to linear complexity in terms of the pattern size. On the other side, the costs on identifying sortable views and searching over the views using inverted index are affordable. Moreover, sortable views generally improve the overall performance of pattern matching, by means of a cost model used to optimize the query rewriting on the most appropriate views. Finally, we demonstrate extensive experimental results to verify the superiority of our proposal on both efficiency and effectiveness.

#index 1918367
#* Efficient algorithms for generalized subgraph query processing
#@ Wenqing Lin;Xiaokui Xiao;James Cheng;Sourav S. Bhowmick
#t 2012
#c 1
#% 288990
#% 378391
#% 601159
#% 765429
#% 810072
#% 864425
#% 960305
#% 1022279
#% 1127380
#% 1181230
#% 1328111
#% 1328183
#% 1372657
#% 1426577
#% 1523835
#% 1523898
#% 1594585
#% 1615871
#% 1618133
#% 1846801
#! We study a new type of graph queries, which injectively maps its edges to paths of the graphs in a given database, where the length of each path is constrained by a given threshold specified by the weight of the corresponding matching edge. We give important applications of the new graph query and identify new challenges of processing such a query. Then, we devise the cost model of the branch-and-bound algorithm framework for processing the graph query, and propose an efficient algorithm to minimize the cost overhead. We also develop three indexing techniques to efficiently answer the queries online. Finally, we verify the efficiency of our proposed indexes with extensive experiments on large real and synthetic datasets.

#index 1918368
#* G-SPARQL: a hybrid engine for querying large attributed graphs
#@ Sherif Sakr;Sameh Elnikety;Yuxiong He
#t 2012
#c 1
#% 722530
#% 765429
#% 960362
#% 989645
#% 1016150
#% 1022236
#% 1127402
#% 1181254
#% 1194591
#% 1217208
#% 1217249
#% 1223424
#% 1265149
#% 1426510
#% 1523825
#% 1592340
#% 1694378
#% 1846755
#! We propose a SPARQL-like language, G-SPARQL, for querying attributed graphs. The language expresses types of queries which of large interest for applications which model their data as large graphs such as: pattern matching, reachability and shortest path queries. Each query can combine both of structural predicates and value-based predicates (on the attributes of the graph nodes and edges). We describe an algebraic compilation mechanism for our proposed query language which is extended from the relational algebra and based on the basic construct of building SPARQL queries, the Triple Pattern. We describe a hybrid Memory/Disk representation of large attributed graphs where only the topology of the graph is maintained in memory while the data of the graph is stored in a relational database. The execution engine of our proposed query language splits parts of the query plan to be pushed inside the relational database while the execution of other parts of the query plan are processed using memory-based algorithms, as necessary. Experimental results on real datasets demonstrate the efficiency and the scalability of our approach and show that our approach outperforms native graph databases by several factors.

#index 1918369
#* A graph-based approach for ontology population with named entities
#@ Wei Shen;Jianyong Wang;Ping Luo;Min Wang
#t 2012
#c 1
#% 815283
#% 855119
#% 956564
#% 975019
#% 1055761
#% 1083654
#% 1130858
#% 1220729
#% 1249547
#% 1251619
#% 1264778
#% 1275182
#% 1291356
#% 1409954
#% 1481370
#% 1484272
#% 1560206
#% 1746843
#% 1747064
#% 1872403
#! Automatically populating ontology with named entities extracted from the unstructured text has become a key issue for Semantic Web and knowledge management techniques. This issue naturally consists of two subtasks: (1) for the entity mention whose mapping entity does not exist in the ontology, attach it to the right category in the ontology (i.e., fine-grained named entity classification), and (2) for the entity mention whose mapping entity is contained in the ontology, link it with its mapping real world entity in the ontology (i.e., entity linking). Previous studies only focus on one of the two subtasks and cannot solve this task of populating ontology with named entities integrally. This paper proposes APOLLO, a grAph-based aPproach for pOpuLating ontoLOgy with named entities. APOLLO leverages the rich semantic knowledge embedded in the Wikipedia to resolve this task via random walks on graphs. Meanwhile, APOLLO can be directly applied to either of the two subtasks with minimal revision. We have conducted a thorough experimental study to evaluate the performance of APOLLO. The experimental results show that APOLLO achieves significant accuracy improvement for the task of ontology population with named entities, and outperforms the baseline methods for both subtasks.

#index 1918370
#* Decomposition-by-normalization (DBN): leveraging approximate functional dependencies for efficient tensor decomposition
#@ Mijung Kim;K. Selçuk Candan
#t 2012
#c 1
#% 129570
#% 237380
#% 420062
#% 458762
#% 487843
#% 765455
#% 844312
#% 1176933
#% 1218275
#% 1581212
#% 1583182
#% 1642120
#! For many multi-dimensional data applications, tensor operations as well as relational operations need to be supported throughout the data lifecycle. Although tensor decomposition is shown to be effective for multi-dimensional data analysis, the cost of tensor decomposition is often very high. We propose a novel decomposition-by-normalization scheme that first normalizes the given relation into smaller tensors based on the functional dependencies of the relation and then performs the decomposition using these smaller tensors. The decomposition and recombination steps of the decomposition-by- normalization scheme fit naturally in settings with multiple cores. This leads to a highly efficient, effective, and parallelized decomposition-by-normalization algorithm for both dense and sparse tensors. Experiments confirm the efficiency and effectiveness of the proposed decomposition-by-normalization scheme compared to the conventional nonnegative CP decomposition approach.

#index 1918371
#* A filter-based protocol for continuous queries over imprecise location data
#@ Yifan Jin;Reynold Cheng;Ben Kao;Kam-Yiu Lam;Yinuo Zhang
#t 2012
#c 1
#% 527176
#% 654488
#% 772835
#% 824654
#% 832568
#% 890594
#% 962910
#% 993955
#% 1181270
#% 1181287
#% 1206716
#% 1291122
#% 1327640
#% 1408794
#% 1431583
#% 1633061
#% 1697255
#! In typical location-based services (LBS), moving objects (e.g., GPS-enabled mobile phones) report their locations through a wireless network. An LBS server can use the location information to answer various types of continuous queries. Due to hardware limitations, location data reported by the moving objects are often uncertain. In this paper, we study efficient methods for the execution of Continuous Possible Nearest Neighbor Query (CPoNNQ) that accesses imprecise location data. A CPoNNQ is a standing query (which is active during a period of time) such that, at any time point, all moving objects that have non-zero probabilities of being the nearest neighbor of a given query point are reported. To handle the continuous nature of a CPoNNQ, a simple solution is to require moving objects to continuously report their locations to the LBS server, which evaluates the query at every time step. To save communication bandwidth and mobile devices' batteries, we develop two filter-based protocols for CPoNNQ evaluation. Our protocols install "filter bounds" on moving objects, which suppress unnecessary location reporting and communication between the server and the moving objects. Through extensive experiments, we show that our protocols can effectively reduce communication costs while maintaining a high query quality.

#index 1918372
#* Leveraging read rates of passive RFID tags for real-time indoor location tracking
#@ Da Yan;Zhou Zhao;Wilfred Ng
#t 2012
#c 1
#% 339218
#% 433320
#% 757953
#% 787175
#% 890628
#% 893102
#% 960283
#% 989604
#% 1206639
#% 1206879
#% 1214685
#% 1426506
#! RFID (radio frequency identification) technology has been widely used for object tracking in many real-life applications, such as inventory monitoring and product flow tracking. These applications usually rely on passive RFID technologies rather than active ones, since passive RFID tags are more attractive than active ones in many aspects, such as lower tag cost and simpler maintenance. RFID technology is also important for indoor location tracking systems that require high degree of accuracy. However, most existing systems estimate object locations by using active RFID tags, which usually incur localization error of more than one meter. Although recent studies begin to investigate the application of passive tags for indoor location tracking, these methods are far from deployable and research of this application is still in its infancy. In this paper, we propose a new indoor location tracking system, named PassTrack, which relies on the read rates of passive RFID tags for location estimation. PassTrack is designed to tolerate noise arising from external environmental factors, by probabilistically modeling the relationship between tag read rate and tag-reader distance, and updating the model parameters based on the current readings of reference tags. Besides tolerance of noise, PassTrack is also outstanding in terms of localization accuracy and efficiency. Several new approaches for location inference are supported by PassTrack, and the best one incurs an average error of around 30 cm, and is able to carry out over 7500 location estimations per second on an ordinary machine. Furthermore, as a result of using passive RFID tags, PassTrack also enjoys the many other benefits of passive RFID tags mentioned before. We have conducted extensive experiments on both real and synthetic datasets, which demonstrate that our PassTrack system outperforms the previous localization approaches in localization accuracy, tracking efficiency and space applicability.

#index 1918373
#* Location-aware instant search
#@ Ruicheng Zhong;Ju Fan;Guoliang Li;Kian-Lee Tan;Lizhu Zhou
#t 2012
#c 1
#% 86950
#% 288975
#% 527026
#% 838407
#% 874993
#% 879610
#% 982560
#% 1190092
#% 1206801
#% 1217199
#% 1217200
#% 1328137
#% 1581875
#% 1581876
#% 1594674
#% 1615855
#% 1618134
#% 1846749
#% 1848110
#% 1879028
#% 1918428
#% 1984480
#! Location-Based Services (LBS) have been widely accepted by mobile users recently. Existing LBS-based systems require users to type in complete keywords. However for mobile users it is rather difficult to type in complete keywords on mobile devices. To alleviate this problem, in this paper we study the location-aware instant search problem, which returns users location-aware answers as users type in queries letter by letter. The main challenge is to achieve high interactive speed. To address this challenge, in this paper we propose a novel index structure, prefix-region tree (called PR-Tree), to efficiently support location-aware instant search. PR-Tree is a tree-based index structure which seamlessly integrates the textual description and spatial information to index the spatial data. Using the PR-Tree, we develop efficient algorithms to support single prefix queries and multi-keyword queries. Experiments show that our method achieves high performance and significantly outperforms state-of-the-art methods.

#index 1918374
#* Indexing uncertain spatio-temporal data
#@ Tobias Emrich;Hans-Peter Kriegel;Nikos Mamoulis;Matthias Renz;Andreas Züfle
#t 2012
#c 1
#% 86950
#% 300174
#% 417825
#% 458849
#% 527176
#% 723186
#% 771228
#% 772835
#% 824728
#% 983259
#% 992830
#% 1016193
#% 1063523
#% 1080157
#% 1127377
#% 1181271
#% 1181287
#% 1328209
#% 1428630
#% 1445746
#% 1464054
#% 1594591
#% 1605948
#% 1846727
#! The advances in sensing and telecommunication technologies allow the collection and management of vast amounts of spatio-temporal data combining location and time information.Due to physical and resource limitations of data collection devices (e.g., RFID readers, GPS receivers and other sensors) data are typically collected only at discrete points of time. In-between these discrete time instances, the positions of tracked moving objects are uncertain. In this work, we propose novel approximation techniques in order to probabilistically bound the uncertain movement of objects; these techniques allow for efficient and effective filtering during query evaluation using an hierarchical index structure.To the best of our knowledge, this is the first approach that supports query evaluation on very large uncertain spatio-temporal databases, adhering to possible worlds semantics. We experimentally show that it accelerates the existing, scan-based approach by orders of magnitude.

#index 1918375
#* Local anomaly descriptor: a robust unsupervised algorithm for anomaly detection based on diffusion space
#@ Hao Huang;Hong Qin;Shinjae Yoo;Dantong Yu
#t 2012
#c 1
#% 196994
#% 271259
#% 279861
#% 300136
#% 874171
#% 902496
#% 905895
#% 1013691
#% 1108867
#% 1176944
#% 1196028
#% 1202160
#% 1286826
#% 1318708
#% 1366432
#% 1370044
#% 1425693
#% 1451239
#% 1520474
#% 1535390
#% 1535428
#% 1551198
#% 1688484
#% 1701530
#% 1761154
#! Current popular anomaly detection algorithms are capable of detecting global anomalies but oftentimes fail to distinguish local anomalies from normal instances. This paper aims to improve unsupervised anomaly detection via the exploration of physics-based diffusion space. Building upon the embedding manifold derived from diffusion maps, we devise Local Anomaly Descriptor (LAD) whose originality results from faithfully preserving intrinsic and informative density-relevant neighborhood information. This robust and effective algorithm is designed with a weighted umbrella Laplacian operator to bridge global and local properties. To further enhance the efficacy of our proposed algorithm, we explore the utility of anisotropic Gaussian kernel (AGK) which can offer better manifold-aware affinity information. Comprehensive experiments on both synthetic and UCI real datasets verify that our LAD outperforms existing anomaly detection algorithms.

#index 1918376
#* Fast and reliable anomaly detection in categorical data
#@ Leman Akoglu;Hanghang Tong;Jilles Vreeken;Christos Faloutsos
#t 2012
#c 1
#% 144520
#% 300136
#% 333929
#% 662750
#% 989593
#% 1051998
#% 1117701
#% 1176968
#% 1565634
#% 1710593
#% 1755297
#% 1787250
#% 1944641
#! Spotting anomalies in large multi-dimensional databases is a crucial task with many applications in finance, health care, security, etc. We introduce COMPREX, a new approach for identifying anomalies using pattern-based compression. Informally, our method finds a collection of dictionaries that describe the norm of a database succinctly, and subsequently flags those points dissimilar to the norm---with high compression cost---as anomalies. Our approach exhibits four key features: 1) it is parameter-free; it builds dictionaries directly from data, and requires no user-specified parameters such as distance functions or density and similarity thresholds, 2) it is general; we show it works for a broad range of complex databases, including graph, image and relational databases that may contain both categorical and numerical features, 3) it is scalable; its running time grows linearly with respect to both database size as well as number of dimensions, and 4) it is effective; experiments on a broad range of datasets show large improvements in both compression, as well as precision in anomaly detection, outperforming its state-of-the-art competitors.

#index 1918377
#* TALMUD: transfer learning for multiple domains
#@ Orly Moreno;Bracha Shapira;Lior Rokach;Guy Shani
#t 2012
#c 1
#% 191257
#% 853532
#% 881468
#% 989592
#% 1074129
#% 1098241
#% 1211767
#% 1273828
#% 1305617
#% 1464068
#% 1650569
#% 1659471
#% 1835187
#! Most collaborative Recommender Systems (RS) operate in a single domain (such as movies, books, etc.) and are capable of providing recommendations based on historical usage data which is collected in the specific domain only. Cross-domain recommenders address the sparsity problem by using Machine Learning (ML) techniques to transfer knowledge from a dense domain into a sparse target domain. In this paper we propose a transfer learning technique that extracts knowledge from multiple domains containing rich data (e.g., movies and music) and generates recommendations for a sparse target domain (e.g., games). Our method learns the relatedness between the different source domains and the target domain, without requiring overlapping users between domains. The model integrates the appropriate amount of knowledge from each domain in order to enrich the target domain data. Experiments with several datasets reveal that, using multiple sources and the relatedness between domains improves accuracy of results.

#index 1918378
#* Utilizing common substructures to speedup tensor factorization for mining dynamic graphs
#@ Wei Liu;Jeffrey Chan;James Bailey;Christopher Leckie;Ramamohanarao Kotagiri
#t 2012
#c 1
#% 248798
#% 316143
#% 503213
#% 881493
#% 915297
#% 961134
#% 1042588
#% 1089780
#% 1133992
#% 1176933
#% 1300087
#% 1451219
#% 1642065
#! In large and complex graphs of social, chemical/biological, or other relations, frequent substructures are commonly shared by different graphs or by graphs evolving through different time periods. Tensors are natural representations of these complex time-evolving graph data. A factorization of a tensor provides a high-quality low-rank compact basis for each dimension of the tensor, which facilitates the interpretation of frequent substructures of the original graphs. However, the high computational cost of tensor factorization makes it infeasible for conventional tensor factorization methods to handle large graphs that evolve frequently with time. To address this problem, in this paper we propose a novel iterative tensor factorization (ITF) method whose time complexity is linear in the cardinalities of all dimensions of a tensor. This low time complexity means that when using tensors to represent dynamic graphs, the computational cost of ITF is linear in the size (number of edges/vertices) of graphs and is also linear in the number of time periods over which the graph evolves. More importantly, an error estimation of ITF suggests that its factorization correctness is comparable to that of the standard factorization method. We empirically evaluate our method on publication networks and chemical compound graphs, and demonstrate that ITF is an order of magnitude faster than the conventional method and at the same time preserves factorization quality. To the best of our knowledge, this research is the first work that uses important frequent substructures to speed up tensor factorizations for mining dynamic graphs.

#index 1918379
#* Predicting emerging social conventions in online social networks
#@ Farshad Kooti;Winter A. Mason;Krishna P. Gummadi;Meeyoung Cha
#t 2012
#c 1
#% 233136
#% 428342
#% 1379671
#% 1414421
#% 1560424
#% 1605961
#% 1746850
#! The way in which social conventions emerge in communities has been of interest to social scientists for decades. Here we report on the emergence of a particular social convention on Twitter---the way to indicate a tweet is being reposted and attributing the content to its source. Despite being invented at different times and having different adoption rates, only two variations became widely adopted. In this paper we describe this process in detail, highlighting the factors that come into play in deciding which variation individuals will adopt. Our classification analysis demonstrates that the date of adoption and the number of exposures are particularly important in the adoption process, while personal features (such as the number of followers and join date) and the number of adopter friends have less discriminative power in predicting adoptions. We discuss implications of these findings in the design of future Web applications and services.

#index 1918380
#* Collective intelligence in the online social network of yahoo!answers and its implications
#@ Ze Li;Haiying Shen;Joseph Edward Grant
#t 2012
#c 1
#% 220708
#% 290830
#% 319704
#% 426885
#% 881460
#% 881523
#% 956517
#% 956544
#% 1002007
#% 1020024
#% 1047396
#% 1055738
#% 1074111
#% 1183153
#% 1190127
#% 1190236
#% 1206841
#% 1215458
#% 1246431
#% 1292492
#% 1292590
#% 1384287
#% 1399976
#% 1399992
#% 1400037
#% 1474633
#% 1560430
#% 1573677
#% 1588381
#! Question and Answer (Q&A) websites such as Yahoo!Answers provide a platform where users can post questions and receive answers. These systems take advantage of the collective intelligence of users to find information. In this paper, we analyze the online social network (OSN) in Yahoo!Answers. Based on a large amount of our collected data, we studied the OSN's structural properties, which reveals strikingly distinct properties such as low link symmetry and weak correlation between indegree and outdegree. After studying the knowledge base and behaviors of the users, we find that a small number of top contributors answer most of the questions in the system. Also, each top contributor focuses on only a few knowledge categories. In addition, the knowledge categories of the users are highly clustered. We also study the knowledge base in a user's social network, which reveals that the members in a user's social network share only a few knowledge categories. Based on the findings, we provide guidance in the design of spammer detection algorithms and distributed Q&A systems. We also propose a friendship-knowledge oriented Q&A framework that synergically combines current OSN-based Q&A and web Q&A. We believe that the results presented in this paper are crucial in understanding the collective intelligence in the web Q&A OSNs and lay a cornerstone for the evolution of next-generation Q&A systems.

#index 1918381
#* From face-to-face gathering to social structure
#@ Chunyan Wang;Mao Ye;Wang-chien Lee
#t 2012
#c 1
#% 839727
#% 881460
#% 1083699
#% 1169572
#% 1183090
#% 1214699
#% 1215485
#% 1281823
#% 1287262
#% 1355043
#% 1399963
#% 1400036
#% 1560202
#% 1560396
#% 1605971
#% 1606045
#% 1606049
#% 1606051
#% 1617365
#! The rapid development of on-line social networking sites has dramatically changed the way people live and communicate. One particularly interesting phenomena came along with this development is the prominent role of various on-line networking portals played in scheduling and organizing off-line group events and activities. In this paper, we focus on studying the face-to-face(f2f) group formed through, or facilitated by, on-line portals. We first show the distinct characteristics of such f2f groups by analyzing datasets collected from Whrrl and Meetup. Next, we propose a dynamic model for group gathering based on the process of friend invitation to interpret how a f2f group is formed on-line. The results of our model are confirmed by empirical observations. Finally, we demonstrate that using such group information can effectively improve the accuracies of social tie inference and friend recommendation.

#index 1918382
#* Delineating social network data anonymization via random edge perturbation
#@ Mingqiang Xue;Panagiotis Karras;Raissi Chedy;Panos Kalnis;Hung Keng Pung
#t 2012
#c 1
#% 408396
#% 956511
#% 1063476
#% 1127360
#% 1127417
#% 1206763
#% 1328188
#% 1426540
#% 1594593
#! Social network data analysis raises concerns about the privacy of related entities or individuals. To address this issue, organizations can publish data after simply replacing the identities of individuals with pseudonyms, leaving the overall structure of the social network unchanged. However, it has been shown that attacks based on structural identification (e.g., a walk-based attack) enable an adversary to re-identify selected individuals in an anonymized network. In this paper we explore the capacity of techniques based on random edge perturbation to thwart such attacks. We theoretically establish that any kind of structural identification attack can effectively be prevented using random edge perturbation and show that, surprisingly, important properties of the whole network, as well as of subgraphs thereof, can be accurately calculated and hence data analysis tasks performed on the perturbed data, given that the legitimate data recipient knows the perturbation probability as well. Yet we also examine ways to enhance the walk-based attack, proposing a variant we call probabilistic attack. Nevertheless, we demonstrate that such probabilistic attacks can also be prevented under sufficient perturbation. Eventually, we conduct a thorough theoretical study of the probability of success of any}structural attack as a function of the perturbation probability. Our analysis provides a powerful tool for delineating the identification risk of perturbed social network data; our extensive experiments with synthetic and real datasets confirm our expectations.

#index 1918383
#* Multiview hierarchical bayesian regression model andapplication to online advertising
#@ Tianbing Xu;Ruofei Zhang;Zhen Guo
#t 2012
#c 1
#% 33917
#% 252011
#% 956546
#% 963669
#% 1055713
#% 1385970
#! With the development of Web applications, large scale data are popular; and they are not only getting richer, but also ubiquitously interconnected with users and other objects in various ways, which brings about multi-view data with implicit structure. In this paper, we propose a novel hierarchical Bayesian mixture regression model, which discovers and then exploits the relationships among multiple views of the data to perform various machine learning tasks. A stochastic EM inference and learning algorithm is derived; and a parallel implementation in Hadoop MapReduce [9] paradigm is developed to scale up the learning. We apply the developed model and algorithm on click-through-rate (CTR) prediction and campaign targeting recommendation in online advertising to measure its effectiveness. The experiments on both synthetic data and large scale ads serving data from a real world online advertising exchange demonstrate the superior CTR prediction accuracy of our method compared to existing state-of-the-art methods. The results also show that our model can recommend high performance targeting features for online advertising campaigns.

#index 1918384
#* Visual appearance of display ads and its effect on click through rate
#@ Javad Azimi;Ruofei Zhang;Yang Zhou;Vidhya Navalpakkam;Jianchang Mao;Xiaoli Fern
#t 2012
#c 1
#% 219845
#% 268121
#% 313959
#% 540161
#% 874603
#% 883919
#% 956546
#% 1055713
#% 1148278
#% 1558464
#% 1581646
#% 1677518
#% 1698382
#% 1750393
#% 1854998
#! One of the most important categories of online advertising is display advertising which provides publishers with significant revenue. Similar to other categories, the main goal in display advertising is to maximize user response rate for advertising campaigns, such as click through rates (CTR) or conversion rates. Previous studies have tried to optimize these parameters using objectives such as behavioral targeting. However, there is no published work so far to address the effect of the visual appearance of ads (creatives) on user response rate via a systematic data-driven approach. In this paper, we quantitatively study the relationship between the visual appearance and performance of creatives using large scale data in the world's largest display ads exchange system, RightMedia. We designed a set of 43 visual features, some of which are novel and others are inspired by related work. We extracted these features from real creatives served on RightMedia. We also designed and conducted a series of experiments to evaluate the effectiveness of visual features for CTR prediction, ranking and performance classification. Based on the evaluation results, we selected a subset of features that have the highest impact on CTR. We believe that the findings presented in this paper will be very useful for the online advertising industry in designing high-performance creatives. It also provides the research community with the first ever data set, initial insights into visual appearance's effect on user response propensity, and evaluation benchmarks for further study.

#index 1918385
#* The wisdom of advertisers: mining subgoals via query clustering
#@ Takehiro Yamamoto;Tetsuya Sakai;Mayu Iwata;Chen Yu;Ji-Rong Wen;Katsumi Tanaka
#t 2012
#c 1
#% 310567
#% 330617
#% 411762
#% 590523
#% 642982
#% 838531
#% 869550
#% 949162
#% 1043040
#% 1074101
#% 1077150
#% 1083721
#% 1130868
#% 1130878
#% 1130910
#% 1214708
#% 1355052
#% 1399962
#% 1399965
#% 1400017
#% 1450834
#% 1450842
#% 1450845
#% 1598439
#% 1641919
#% 1642076
#% 1712595
#% 1746837
#! This paper tackles the problem of mining subgoals of a given search goal from data. For example, when a searcher wants to travel to London, she may need to accomplish several subtasks such as "book flights," "book a hotel," "find good restaurants" and "decide which sightseeing spots to visit." As another example, if a searcher wants to lose weight, there may exist several alternative solutions such as "do physical exercise," "take diet pills," and "control calorie intake." In this paper, we refer to such subtasks or solutions as subgoals, and propose to utilize sponsored search data for finding subgoals of a given query by means of query clustering. Advertisements (ads) reflect advertisers' tremendous efforts in trying to match a given query with implicit user needs. Moreover, ads are usually associated with a particular action or transaction. We therefore hypothesized that they are useful for subgoal mining. To our knowledge, our work is the first to use sponsored search data for this purpose. Our experimental results show that sponsored search data is a good resource for obtaining related queries and for identifying subgoals via query clustering. In particular, our method that combines ad impressions from sponsored search data and query co-occurrences from session data outperforms a state-of-the-art query clustering method that relies on document clicks rather than ad impressions in terms of purity, NMI, Rand Index, F1-measure and subgoal recall.

#index 1918386
#* Sequential selection of correlated ads by POMDPs
#@ Shuai Yuan;Jun Wang
#t 2012
#c 1
#% 280852
#% 361730
#% 425053
#% 736189
#% 818265
#% 956546
#% 983894
#% 987262
#% 1019092
#% 1055713
#% 1174380
#% 1272075
#% 1379740
#% 1482218
#% 1558523
#% 1641982
#% 1641984
#% 1642190
#% 1650569
#! Online advertising has become a key source of revenue for both web search engines and online publishers. For them, the ability of allocating right ads to right webpages is critical because any mismatched ads would not only harm web users' satisfactions but also lower the ad income. In this paper, we study how online publishers could optimally select ads to maximize their ad incomes over time. The conventional offline, content-based matching between webpages and ads is a fine start but cannot solve the problem completely because good matching does not necessarily lead to good payoff. Moreover, with the limited display impressions, we need to balance the need of selecting ads to learn true ad payoffs (exploration) with that of allocating ads to generate high immediate payoffs based on the current belief (exploitation). In this paper, we address the problem by employing Partially observable Markov decision processes (POMDPs) and discuss how to utilize the correlation of ads to improve the efficiency of the exploration and increase ad incomes in a long run. Our mathematical derivation shows that the belief states of correlated ads can be naturally updated using a formula similar to collaborative filtering. To test our model, a real world ad dataset from a major search engine is collected and categorized. Experimenting over the data, we provide an analyse of the effect of the underlying parameters, and demonstrate that our algorithms significantly outperform other strong baselines.

#index 1918387
#* Diversity in blog feed retrieval
#@ Mostafa Keikha;Fabio Crestani;W. Bruce Croft
#t 2012
#c 1
#% 262112
#% 750863
#% 818211
#% 1074094
#% 1074171
#% 1130913
#% 1130914
#% 1131220
#% 1311687
#% 1587376
#% 1621233
#% 1742093
#! Blog distillation (blog feed retrieval) is a task in blog retrieval where the goal is to rank blogs according to their recurrent relevance to a query topic. One of the main properties of blog feed retrieval is that the unit of retrieval is a collection of documents as opposed to a single document as in other IR tasks. This collection retrieval nature of blog distillation introduces new challenges and requires new investigations specific to this problem. Researchers have addressed this problem by considering a wide range of evidence and information resources. However, previous work has not studied the effect of on-topic diversity of blog posts in blog relevance. By on-topic diversity of blog posts we mean that those posts that are about the query topic need to have high diversity and cover different sub-topics of the query. In this study, we investigate three types of on-topic diversity and their effect on retrieval performance: topical diversity, temporal diversity and hybrid diversity. Our experiments over different blog collections and different baseline methods show that on-topic diversity can improve the performance of the retrieval system. Among the three types of diversity, hybrid diversity, that considers both topical and temporal diversities, achieves the best performance.

#index 1918388
#* Efficient retrieval of recommendations in a matrix factorization framework
#@ Noam Koenigstein;Parikshit Ram;Yuval Shavitt
#t 2012
#c 1
#% 2115
#% 249321
#% 347225
#% 956521
#% 1038334
#% 1260273
#% 1476454
#% 1625364
#% 1872342
#% 1893855
#! Low-rank Matrix Factorization (MF) methods provide one of the simplest and most effective approaches to collaborative filtering. This paper is the first to investigate the problem of efficient retrieval of recommendations in a MF framework. We reduce the retrieval in a MF model to an apparently simple task of finding the maximum dot-product for the user vector over the set of item vectors. However, to the best of our knowledge the problem of efficiently finding the maximum dot-product in the general case has never been studied. To this end, we propose two techniques for efficient search -- (i) We index the item vectors in a binary spatial-partitioning metric tree and use a simple branch and-bound algorithm with a novel bounding scheme to efficiently obtain exact solutions. (ii) We use spherical clustering to index the users on the basis of their preferences and pre-compute recommendations only for the representative user of each cluster to obtain extremely efficient approximate solutions. We obtain a theoretical error bound which determines the quality of any approximate result and use it to control the approximation. Both these simple techniques are fairly independent of each other and hence are easily combined to further improve recommendation retrieval efficiency. We evaluate our algorithms on real-world collaborative-filtering datasets, demonstrating more than ×7 speedup (with respect to the naive linear search) for the exact solution and over ×250 speedup for approximate solutions by combining both techniques.

#index 1918389
#* KORE: keyphrase overlap relatedness for entity disambiguation
#@ Johannes Hoffart;Stephan Seufert;Dat Ba Nguyen;Martin Theobald;Gerhard Weikum
#t 2012
#c 1
#% 249238
#% 249321
#% 342963
#% 479973
#% 896031
#% 939408
#% 956564
#% 975019
#% 1008096
#% 1130858
#% 1131827
#% 1214667
#% 1270225
#% 1272185
#% 1275012
#% 1409954
#% 1419423
#% 1450836
#% 1471332
#% 1482395
#% 1484272
#% 1560155
#% 1560388
#% 1591937
#% 1592008
#% 1693902
#% 1711796
#% 1815364
#% 1925702
#! Measuring the semantic relatedness between two entities is the basis for numerous tasks in IR, NLP, and Web-based knowledge extraction. This paper focuses on disambiguating names in a Web or text document by jointly mapping all names onto semantically related entities registered in a knowledge base. To this end, we have developed a novel notion of semantic relatedness between two entities represented as sets of weighted (multi-word) keyphrases, with consideration of partially overlapping phrases. This measure improves the quality of prior link-based models, and also eliminates the need for (usually Wikipedia-centric) explicit interlinkage between entities. Thus, our method is more versatile and can cope with long-tail and newly emerging entities that have few or no links associated with them. For efficiency, we have developed approximation techniques based on min-hash sketches and locality-sensitive hashing. Our experiments on semantic relatedness and on named entity disambiguation demonstrate the superiority of our method compared to state-of-the-art baselines.

#index 1918390
#* Shard ranking and cutoff estimation for topically partitioned collections
#@ Anagha Kulkarni;Almer S. Tigelaar;Djoerd Hiemstra;Jamie Callan
#t 2012
#c 1
#% 144034
#% 194246
#% 280856
#% 287463
#% 298181
#% 375017
#% 578337
#% 643012
#% 643013
#% 728102
#% 730066
#% 789959
#% 818229
#% 818262
#% 878657
#% 976948
#% 993964
#% 1227597
#% 1227629
#% 1292595
#% 1373774
#% 1392444
#% 1450840
#% 1482223
#% 1532603
#% 1711388
#% 1917649
#! Large document collections can be partitioned into 'topical shards' to facilitate distributed search. In a low-resource search environment only a few of the shards can be searched in parallel. Such a search environment faces two intertwined challenges. First, determining which shards to consult for a given query: shard ranking. Second, how many shards to consult from the ranking: cutoff estimation. In this paper we present a family of three algorithms that address both of these problems. As a basis we employ a commonly used data structure, the central sample index (CSI), to represent the shard contents. Running a query against the CSI yields a flat document ranking that each of our algorithms transforms into a tree structure. A bottom up traversal of the tree is used to infer a ranking of shards and also to estimate a stopping point in this ranking that yields cost-effective selective distributed search. As compared to a state-of-the-art shard ranking approach the proposed algorithms provide substantially higher search efficiency while providing comparable search effectiveness.

#index 1918391
#* Daily-deal selection for revenue maximization
#@ Theodoros Lappas;Evimaria Terzi
#t 2012
#c 1
#% 70370
#% 722904
#% 850621
#% 944197
#% 995868
#% 1560371
#% 1783941
#% 1860710
#! Daily-Deal Sites (DDS) like Groupon, LivingSocial, Amazon's Goldbox, and many more, have become particularly popular over the last three years, providing discounted offers to customers for restaurants, ticketed events, services etc. In this paper, we study the following problem: among a set of candidate deals, which are the ones that a DDS should feature as daily-deals in order to maximize its revenue? Our first contribution lies in providing two combinatorial formulations of this problem. Both formulations take into account factors like the diversification of daily deals and the limited consuming capacity of the userbase. We prove that our problems are NP-hard and devise pseudopolynomial -- time approximation algorithms for their solution. We also propose a set of heuristics, and demonstrate their efficiency in our experiments. In the context of deal selection and scheduling, we acknowledge the importance of the ability to estimate the expected revenue of a candidate deal. We explore the nature of this task in the context of real data, and propose a framework for revenue-estimation. We demonstrate the effectiveness of our entire methodology in an experimental evaluation on a large dataset of daily-deals from Groupon.

#index 1918392
#* Enabling direct interest-aware audience selection
#@ Ariel Fuxman;Anitha Kannan;Zhenhui Li;Panayiotis Tsaparas
#t 2012
#c 1
#% 186340
#% 449294
#% 577285
#% 748499
#% 823348
#% 869651
#% 1089474
#% 1107136
#% 1130868
#% 1130878
#% 1166521
#% 1190081
#% 1214642
#% 1214692
#% 1292754
#% 1400017
#% 1598334
#% 1605925
#% 1641982
#% 1642128
#% 1642129
#% 1755301
#% 1783934
#! Advertisers typically have a fairly accurate idea of the interests of their target audience. However, today's online advertising systems are unable to leverage this information. The reasons are two-fold. First, there is no agreed upon vocabulary of interests for advertisers and advertising systems to communicate. More importantly, advertising systems lack a mechanism for mapping users to the interest vocabulary. In this paper, we tackle both problems. We present a system for direct interest-aware audience selection. This system takes the query histories of search engine users as input, extracts their interests, and describes them with interpretable labels. The labels are not drawn from a predefined taxonomy, but rather dynamically generated from the query histories, and are thus easy for the advertisers to interpret and use for targeting users. In addition, the system enables seamless addition of interest labels that may be provided by the advertiser.

#index 1918393
#* Influence propagation in adversarial setting: how to defeat competition with least amount of investment
#@ Shahrzad Shirazipourazad;Brian Bogard;Harsh Vachhani;Arunabha Sen;Paul Horn
#t 2012
#c 1
#% 325580
#% 342596
#% 729923
#% 990216
#% 1214641
#% 1407359
#% 1451244
#% 1491561
#% 1605972
#% 1642030
#% 1693922
#! It has been observed that individuals' decisions to adopt a product or innovation are often influenced by the recommendations of their friends and acquaintances. Motivated by this observation, the last few years have seen a number of studies on influence maximization in social networks. The primary goal of these studies is identification of k most influential nodes in a network. A major limitation of these studies is that they focus on a non-adversarial environment, where only one player is engaged in influencing the nodes. However, in a realistic scenario multiple players attempt to influence the nodes in a competitive fashion. The proposed model considers a competitive environment where a node that has not yet adopted an innovation, can adopt only one of the several competing innovations and once it adopts an innovation, it does not switch. The paper studies the scenario where the first player has already chosen a set of k nodes and the second player, with the knowledge of the choice of the first, attempts to identify a smallest set of nodes (excluding the ones already chosen by the first) so that when the influence propagation process ends, the number of nodes influenced by the second player is larger than the number of nodes influenced by the first. The paper studies two propagation models and shows that in both the models, the identification of the smallest set of nodes to defeat the adversary is NP-Hard. It provides an approximation algorithm and proves that the performance bound is tight. It also presents the results of extensive experimentation using the collaboration network data. Experimental results show that the second player can easily defeat the first with this algorithm, if the first utilizes the node degree or closeness centrality based algorithms for the selection of influential nodes. The proposed algorithm also provides better performance if the second player utilizes it instead of the greedy algorithm to maximize its influence.

#index 1918394
#* Large-scale item categorization for e-commerce
#@ Dan Shen;Jean-David Ruvini;Badrul Sarwar
#t 2012
#c 1
#% 165110
#% 280817
#% 309141
#% 344447
#% 420466
#% 458379
#% 465747
#% 465754
#% 466078
#% 674497
#% 763699
#% 770796
#% 783478
#% 829975
#% 1074128
#% 1117691
#% 1272365
#% 1538188
#% 1554566
#% 1642144
#! This paper studies the problem of leveraging computationally intensive classification algorithms for large scale text categorization problems. We propose a hierarchical approach which decomposes the classification problem into a coarse level task and a fine level task. A simple yet scalable classifier is applied to perform the coarse level classification while a more sophisticated model is used to separate classes at the fine level. However, instead of relying on a human-defined hierarchy to decompose the problem, we we use a graph algorithm to discover automatically groups of highly similar classes. As an illustrative example, we apply our approach to real-world industrial data from eBay, a major e-commerce site where the goal is to classify live items into a large taxonomy of categories. In such industrial setting, classification is very challenging due to the number of classes, the amount of training data, the size of the feature space and the real-world requirements on the response time. We demonstrate through extensive experimental evaluation that (1) the proposed hierarchical approach is superior to flat models, and (2) the data-driven extraction of latent groups works significantly better than the existing human-defined hierarchy.

#index 1918395
#* Matching product titles using web-based enrichment
#@ Vishrawas Gopalakrishnan;Suresh Parthasarathy Iyengar;Amit Madaan;Rajeev Rastogi;Srinivasan Sengamedu
#t 2012
#c 1
#% 262084
#% 420072
#% 577238
#% 654467
#% 844289
#% 864392
#% 869500
#% 874510
#% 913783
#% 937552
#% 975019
#% 987221
#% 1077150
#% 1190070
#% 1201863
#% 1328142
#% 1523838
#% 1563057
#% 1605958
#% 1798424
#% 1890006
#! Matching product titles from different data feeds that refer to the same underlying product entity is a key problem in online shopping. This matching problem is challenging because titles across the feeds have diverse representations with some missing important keywords like brand and others containing extraneous keywords related to product specifications. In this paper, we propose a novel unsupervised matching algorithm that leverages web earch engines to (1) enrich product titles by adding important missing tokens that occur frequently in search results, and (2) compute importance scores for tokens based on their ability to retrieve other (enriched title) tokens in search results. Our matching scheme calculates the Cosine similarity between enriched title pairs with tokens weighted by their importance scores. We propose an optimization that exploits the templatized structure of product titles to reduce the number of search queries. In experiments with real-life shopping datasets, we found that our matching algorithm has superior F1 scores compared to IDF-based cosine similarity.

#index 1918396
#* Scalable clustering of signed networks using balance normalized cut
#@ Kai-Yang Chiang;Joyce Jiyoung Whang;Inderjit S. Dhillon
#t 2012
#c 1
#% 274612
#% 313959
#% 342621
#% 629666
#% 765548
#% 995140
#% 1013626
#% 1013696
#% 1399997
#% 1872290
#% 1966695
#! We consider the general $k$-way clustering problem in signed social networks where relationships between entities can be either positive or negative. Motivated by social balance theory, the clustering problem in signed networks aims to find mutually antagonistic groups such that entities within the same group are friends with each other. A recent method proposed in [13] extended the spectral clustering algorithm to the signed network setting by considering the signed graph Laplacian. This has been shown to be equivalent to finding clusters that minimize the 2-way signed ratio cut. In this paper, we show that there is a fundamental weakness when we directly extend the signed Laplacian to the k-way clustering problem. To overcome this weakness, we formulate new k-way objectives for signed networks. In particular, we propose a criterion that is analogous to the normalized cut, called balance normalized cut, which is not only theoretically sound but also experimentally effective in k-way clustering. In addition, we prove that these objectives are equivalent to weighted kernel k-means objectives by choosing an appropriate kernel matrix. Employing this equivalence, we develop a multilevel clustering framework for signed networks. In this framework, we coarsen the graph level by level and refine the clustering results at each level via a k-means based algorithm so that the signed clustering objectives are optimized. This approach gives good quality clustering results, and is also highly efficient and scalable. In experiments, we see that our multilevel approach is competitive to other state-of-the-art methods, while it is much faster and more scalable. In particular, the largest graph we have considered in our experiments contains 1 million nodes and 100 million edges --- this graph can be clustered in less than four hundred seconds using our algorithm.

#index 1918397
#* Maximum margin clustering on evolutionary data
#@ Xuhui Fan;Lin Zhu;Longbing Cao;Xia Cui;Yew-Soon Ong
#t 2012
#c 1
#% 594012
#% 769946
#% 881514
#% 989586
#% 1015261
#% 1074028
#% 1128617
#% 1179992
#% 1269502
#% 1279659
#% 1399310
#% 1451147
#% 1537112
#% 1538537
#% 1872347
#! Evolutionary data, such as topic changing blogs and evolving trading behaviors in capital market, is widely seen in business and social applications. The time factor and intrinsic change embedded in evolutionary data greatly challenge evolutionary clustering. To incorporate the time factor, existing methods mainly regard the evolutionary clustering problem as a linear combination of snapshot cost and temporal cost, and reflect the time factor through the temporal cost. It still faces accuracy and scalability challenge though promising results gotten. This paper proposes a novel evolutionary clustering approach, evolutionary maximum margin clustering (e-MMC), to cluster large-scale evolutionary data from the maximum margin perspective. e-MMC incorporates two frameworks: Data Integration from the data changing perspective and Model Integration corresponding to model adjustment to tackle the time factor and change, with an adaptive label allocation mechanism. Three e-MMC clustering algorithms are proposed based on the two frameworks. Extensive experiments are performed on synthetic data, UCI data and real-world blog data, which confirm that e-MMC outperforms the state-of-the-art clustering algorithms in terms of accuracy, computational cost and scalability. It shows that e-MMC is particularly suitable for clustering large-scale evolving data.

#index 1918398
#* Document-topic hierarchies from document graphs
#@ Tim Weninger;Yonatan Bisk;Jiawei Han
#t 2012
#c 1
#% 46809
#% 65440
#% 262096
#% 451052
#% 722904
#% 750863
#% 788094
#% 806594
#% 818254
#% 840872
#% 983883
#% 1055681
#% 1083684
#% 1083734
#% 1289476
#% 1310058
#% 1330507
#% 1531275
#% 1592262
#% 1746872
#! Topic taxonomies present a multi-level view of a document collection, where general topics live towards the top of the taxonomy and more specific topics live towards the bottom. Topic taxonomies allow users to quickly drill down into their topic of interest to find documents. We show that hierarchies of documents, where documents live at the inner nodes of the hierarchy-tree can also be inferred by combining document text with inter-document links. We present a Bayesian generative model by which an explicit hierarchy of documents is created. Experiments on three document-graph data sets shows that the generated document hierarchies are able to fit the observed data, and that the levels in the constructed document hierarchy represent practical groupings.

#index 1918399
#* Improving document clustering using automated machine translation
#@ Xiang Wang;Buyue Qian;Ian Davidson
#t 2012
#c 1
#% 3084
#% 252011
#% 313959
#% 848482
#% 983949
#% 995140
#% 1085668
#% 1250560
#% 1275645
#% 1377376
#% 1450982
#% 1451196
#% 1663626
#! With the development of statistical machine translation, we have ready-to-use tools that can translate documents from one language to many other languages. These translations provide different yet correlated views of the same set of documents. This gives rise to an intriguing question: can we use the extra information to achieve a better clustering of the documents? Some recent work on multiview clustering provided positive answers to this question. In this work, we propose an alternative approach to address this problem using the constrained clustering framework. Unlike traditional Must-Link and Cannot-Link constraints, the constraints generated from machine translation are dense yet noisy. We show how to incorporate this type of constraints by presenting two algorithms, one parametric and one non-parametric. Our algorithms are easy to implement, efficient, and can consistently improve the clustering of real data, namely the Reuters RCV1/RCV2 Multilingual Dataset. In contrast to existing multiview clustering algorithms, our technique does not need the compatibility or the conditional independence assumption, nor does it involve subtle parameter tuning.

#index 1918400
#* Right-protected data publishing with hierarchical clustering preservation
#@ Michail Vlachos;Aleksander Wieczorek;Johannes Schneider
#t 2012
#c 1
#% 198196
#% 263798
#% 727904
#% 729930
#% 784512
#% 844360
#% 863402
#% 874166
#% 972310
#% 993944
#% 1341079
#% 1456852
#% 1468226
#% 1480783
#% 1605948
#% 1669943
#% 1854543
#% 1857578
#! The emergence of cloud-based storage services is opening up new avenues in data exchange and data dissemination. This has amplified the interest in right-protection mechanisms for establishing ownership in case of data leakage. Current right-protection technologies, however, rarely provide strong guarantees on the dataset utility after the protection process. This work presents techniques that explicitly address this shortcoming and provably preserve the outcome of certain mining operations. In particular, we take special care to guarantee that the outcome of hierarchical clustering operations remains the same before and after right protection. We encode data ownership using watermarking principles. In the process, we derive fundamental bounds on the distortion incurred by the watermarking. We leverage our theoretical analysis to design fast algorithms for right protection without exhaustively searching the vast design space.

#index 1918401
#* Metaphor: a system for related search recommendations
#@ Azarias Reda;Yubin Park;Mitul Tiwari;Christian Posse;Sam Shah
#t 2012
#c 1
#% 132938
#% 169729
#% 209021
#% 218978
#% 323131
#% 328946
#% 387427
#% 551723
#% 641976
#% 728105
#% 754125
#% 813966
#% 818257
#% 869501
#% 869651
#% 963669
#% 987193
#% 998845
#% 1063553
#% 1074200
#% 1096052
#% 1125901
#% 1130854
#% 1130868
#% 1173699
#% 1273928
#% 1358747
#% 1476448
#% 1536581
#% 1598413
#% 1693897
#% 1712595
#% 1715593
#% 1765838
#! Search plays an important role in online social networks as it provides an essential mechanism for discovering members and content on the network. Related search recommendation is one of several mechanisms used for improving members' search experience in finding relevant results to their queries. This paper describes the design, implementation, and deployment of Metaphor, the related search recommendation system on LinkedIn, a professional social networking site with over 175~million members worldwide. Metaphor builds on a number of signals and filters that capture several dimensions of relatedness across member search activity. The system, which has been in live operation for over a year, has gone through multiple iterations and evaluation cycles. This paper makes three contributions. First, we provide a discussion of a large-scale related search recommendation system. Second, we describe a mechanism for effectively combining several signals in building a unified dataset for related search recommendations. Third, we introduce a query length model for capturing bias in recommendation click behavior. We also discuss some of the practical concerns in deploying related search recommendations.

#index 1918402
#* Exploring personal impact for group recommendation
#@ Xingjie Liu;Yuan Tian;Mao Ye;Wang-Chien Lee
#t 2012
#c 1
#% 124010
#% 173879
#% 260780
#% 301259
#% 330687
#% 342777
#% 722904
#% 733578
#% 734592
#% 788094
#% 813966
#% 878355
#% 955930
#% 1054718
#% 1328172
#% 1358747
#% 1396104
#% 1476459
#% 1650569
#% 1872355
#% 1879059
#! Group activities are essential ingredients of people's social life. The rapid growth of online social networking services has greatly boosted group activities by providing convenient platform for users to organize and participate in such activities. Therefore, recommender systems, as a critical component in social networking services, now face new challenges in supporting group activities. In this paper, we study the group recommendation problem, i.e., making recommendations to a group of people in social networking services. We analyze the decision making process in a group to propose a personal impact topic (PIT) model for group recommendations. The PIT model effectively identifies the group preference profile for a given group by considering the personal preferences and personal impacts of group members. Moreover, we further enhance the discovery of personal impact with social network information to obtain an extended personal impact topic (E-PIT) model. We have conducted comprehensive data analysis and evaluations on three real datasets. The results show that our proposed group recommendation techniques outperform baseline approaches.

#index 1918403
#* The efficient imputation method for neighborhood-based collaborative filtering
#@ Yongli Ren;Gang Li;Jun Zhang;Wanlei Zhou
#t 2012
#c 1
#% 173879
#% 197391
#% 330687
#% 465928
#% 564483
#% 766448
#% 813966
#% 818216
#% 879627
#% 913192
#% 987197
#% 1038334
#% 1052676
#% 1083671
#% 1155623
#% 1287235
#% 1287272
#% 1492057
#% 1650569
#% 1811496
#% 1938458
#% 1966767
#! As each user tends to rate a small proportion of available items, the resulted Data Sparsity issue brings significant challenges to the research of recommender systems. This issue becomes even more severe for neighborhood-based collaborative filtering methods, as there are even lower numbers of ratings available in the neighborhood of the query item. In this paper, we aim to address the Data Sparsity issue in the context of the neighborhood-based collaborative filtering. Given the (user, item) query, a set of key ratings are identified, and an auto-adaptive imputation method is proposed to fill the missing values in the set of key ratings. The proposed method can be used with any similarity metrics, such as the Pearson Correlation Coefficient and Cosine-based similarity, and it is theoretically guaranteed to outperform the neighborhood-based collaborative filtering approaches. Results from experiments prove that the proposed method could significantly improve the accuracy of recommendations for neighborhood-based Collaborative Filtering algorithms.

#index 1918404
#* Multi-faceted ranking of news articles using post-read actions
#@ Deepak Agarwal;Bee-Chung Chen;Xuanhui Wang
#t 2012
#c 1
#% 330687
#% 340948
#% 424806
#% 452563
#% 452641
#% 754106
#% 956521
#% 987198
#% 1073982
#% 1083671
#% 1083696
#% 1117691
#% 1214623
#% 1399999
#% 1450874
#% 1598346
#% 1598352
#% 1598363
#% 1598397
#% 1605930
#! Personalized article recommendation is important for news portals to improve user engagement. Existing work quantifies engagement primarily through click rates. We suggest that quality of recommendations may be improved by exploiting different types of "post-read" engagement signals like sharing, commenting, printing and e-mailing article links. Specifically, we propose a multi-faceted ranking problem for recommending articles, where each facet corresponds to a ranking task that seeks to maximize actions of a particular post-read type (e.g., ranking articles to maximize sharing actions). Our approach is to predict the probability that a user would take a post-read action on an article, so that articles can be ranked according to such probabilities. However, post-read actions are rare events --- enormous data sparsity makes the problem challenging. We meet the challenge by exploiting correlations across different post-read action types through a novel locally augmented tensor (LAT) model, so that the ranking performance of a particular action type can be improved by leveraging data from all other action types. Through extensive experiments, we show that our LAT model significantly outperforms a variety of state-of-the-art factor models, logistic regression and IR models.

#index 1918405
#* A decentralized recommender system for effective web credibility assessment
#@ Thanasis G. Papaioannou;Jean-Eudes Ranvier;Alexandra Olteanu;Karl Aberer
#t 2012
#c 1
#% 280852
#% 314935
#% 324926
#% 330687
#% 505869
#% 571351
#% 657509
#% 897330
#% 943777
#% 1016177
#% 1260373
#% 1281981
#% 1388581
#% 1573488
#% 1573489
#% 1914862
#! An overwhelming and growing amount of data is available online. The problem of untrustworthy online information is augmented by its high economic potential and its dynamic nature, e.g. transient domain names, dynamic content, etc. In this paper, we address the problem of assessing the credibility of web pages by a decentralized social recommender system. Specifically, we concurrently employ i) item-based collaborative filtering (CF) based on specific web page features, ii) user-based CF based on friend ratings and iii) the ranking of the page in search results. These factors are appropriately combined into a single assessment based on adaptive weights that depend on their effectiveness for different topics and different fractions of malicious ratings. Simulation experiments with real traces of web page credibility evaluations suggest that our hybrid approach outperforms both its constituent components and classical content-based classification approaches.

#index 1918406
#* Towards an effective and unbiased ranking of scientific literature through mutual reinforcement
#@ Xiaorui Jiang;Xiaoping Sun;Hai Zhuge
#t 2012
#c 1
#% 268079
#% 290830
#% 340147
#% 340932
#% 722474
#% 835342
#% 967278
#% 1077150
#% 1083734
#% 1116996
#% 1176882
#% 1300654
#% 1301029
#% 1338740
#% 1467899
#% 1523885
#% 1588383
#% 1606064
#! It is important to help researchers find valuable scientific papers from a large literature collection containing information of authors, papers and venues. Graph-based algorithms have been proposed to rank papers based on networks formed by citation and co-author relationships. This paper proposes a new graph-based ranking framework MutualRank that integrates mutual reinforcement relationships among networks of papers, researchers and venues to achieve a more synthetic, accurate and fair ranking result than previous graph-based methods. MutualRank leverages the network structure information among papers, authors, and their venues available from a literature collection dataset and sets up a unified mutual reinforcement model that involves both intra- and inter-network information for ranking papers, authors and venues simultaneously. To evaluate, we collect a set of recommended papers from websites of graduate-level computational linguistics courses of 15 top universities as the benchmark and apply different methods to estimate paper importance. The results show that MutualRank greatly outperforms the competitors including Pag-eRank, HITS and CoRank in ranking papers as well as researchers. The experimental results also demonstrate that venues ranked by MutualRank are reasonable.

#index 1918407
#* A math-aware search engine for math question answering system
#@ Tam T. Nguyen;Kuiyu Chang;Siu Cheung Hui
#t 2012
#c 1
#% 818239
#% 879588
#% 879713
#% 961152
#% 987226
#% 1073905
#% 1077150
#% 1099066
#% 1099068
#% 1105978
#% 1432321
#% 1598344
#% 1598512
#% 1655230
#% 1680510
#% 1681910
#% 1715628
#% 1716011
#% 1728015
#% 1826291
#! We propose a math-aware search engine that is capable of handling both textual keywords as well as mathematical expressions. Our math feature extraction and representation framework captures the semantics of math expressions via a Finite State Machine model. We adapt the passive aggressive online learning binary classifier as the ranking model. We benchmarked our approach against three classical information retrieval (IR) strategies on math documents crawled from Math Overflow, a well-known online math question answering system. Experimental results show that our proposed approach can perform better than other methods by more than 9%.

#index 1918408
#* Contextualization using hyperlinks and internal hierarchical structure of Wikipedia documents
#@ Muhammad Ali Norozi;Paavo Arvola;Arjen P. de Vries
#t 2012
#c 1
#% 290830
#% 577328
#% 654442
#% 838390
#% 879597
#% 879695
#% 893737
#% 909448
#% 1016176
#% 1100806
#% 1131026
#% 1166528
#% 1227721
#% 1415731
#% 1489456
#% 1598872
#% 1622375
#% 1950408
#! Context surrounding hyperlinked semi-structured documents, externally in the form of citations and internally in the form of hierarchical structure, contains a wealth of useful but implicit evidence about a document's relevance. These rich sources of information should be exploited as contextual evidence. This paper proposes various methods of accumulating evidence from the context, and measures the effect of contextual evidence on retrieval effectiveness for document and focused retrieval of hyperlinked semi-structured documents. We propose a re-weighting model to contextualize (a) evidence from citations in a query-independent and query-dependent fashion (based on Markovian random walks) and (b) evidence accumulated from the internal tree structure of documents. The in-links and out-links of a node in the citation graph are used as external context, while the internal document structure provides internal, within-document context. We hypothesize that documents in a good context (having strong contextual evidence) should be good candidates to be relevant to the posed query, and vice versa. We tested several variants of contextualization and verified notable improvements in comparison with the baseline system and gold standards in the retrieval of full documents and focused elements.

#index 1918409
#* Understanding book search behavior on the web
#@ Jin Young Kim;Henry Feild;Marc Cartright
#t 2012
#c 1
#% 262096
#% 642992
#% 823348
#% 879565
#% 1074053
#% 1213448
#% 1292597
#% 1523413
#% 1598338
#% 1622383
#% 1742093
#! With the increased availability of e-books and digitized book collections, more users are searching the web for information about books. There are many online digital libraries containing book, author and subject data, which are accessed via internal search services as well as external web sites, such as Google. Although this is a common yet complex information-seeking behavior involving multiple search systems with different characteristics, little is known about how users find information in this scenario. In this work, we analyze web-based book search behavior using three months of logs from the Open Library, a globally accessible digital library. Our study encompasses the user behavior on web search engines and the digital library, unlike previous work which focused on institution-level digital libraries. Among our findings are (1) query characteristics and session-level behaviors are drastically different between internal and external searchers; (2) the field usage is different based on the modes of interaction---keyword search, advanced search interface and faceted filtering; (3) users go through with more iterations of faceted filtering than query reformulation. To facilitate future research on book search, we also create a book search test collection based on the log data. We then perform an evaluation of several retrieval methods, finding that field-based retrieval models have advantages over document-based models.

#index 1918410
#* Temporal corpus summarization using submodular word coverage
#@ Ruben Sipos;Adith Swaminathan;Pannaga Shivaswamy;Thorsten Joachims
#t 2012
#c 1
#% 262112
#% 297675
#% 309096
#% 340883
#% 648320
#% 722904
#% 760853
#% 783535
#% 989633
#% 1074025
#% 1074123
#% 1195365
#% 1214650
#% 1260752
#% 1338740
#% 1392478
#% 1470696
#% 1473300
#% 1480442
#% 1598408
#% 1605962
#% 1641948
#% 1715653
#% 1905999
#! In many areas of life, we now have almost complete electronic archives reaching back for well over two decades. This includes, for example, the body of research papers in computer science, all news articles written in the US, and most people's personal email. However, we have only rather limited methods for analyzing and understanding these collections. While keyword-based retrieval systems allow efficient access to individual documents in archives, we still lack methods for understanding a corpus as a whole. In this paper, we explore methods that provide a temporal summary of such corpora in terms of landmark documents, authors, and topics. In particular, we explicitly model the temporal nature of influence between documents and re-interpret summarization as a coverage problem over words anchored in time. The resulting models provide monotone sub-modular objectives for computing informative and non-redundant summaries over time, which can be efficiently optimized with greedy algorithms. Our empirical study shows the effectiveness of our approach over several baselines.

#index 1918411
#* TCSST: transfer classification of short & sparse text using external data
#@ Guodong Long;Ling Chen;Xingquan Zhu;Chengqi Zhang
#t 2012
#c 1
#% 465746
#% 769886
#% 869500
#% 956570
#% 983828
#% 983899
#% 987328
#% 1055680
#% 1250629
#% 1269907
#% 1275012
#% 1292559
#% 1450992
#% 1464068
#% 1598472
#% 1642003
#! Short & sparse text is becoming more prevalent on the web, such as search snippets, micro-blogs and product reviews. Accurately classifying short & sparse text has emerged as an important while challenging task. Existing work has considered utilizing external data (e.g. Wikipedia) to alleviate data sparseness, by appending topics detected from external data as new features. However, training a classifier on features concatenated from different spaces is not easy considering the features have different physical meanings and different significance to the classification task. Moreover, it exacerbates the "curse of dimensionality" problem. In this study, we propose a transfer classification method, TCSST, to exploit the external data to tackle the data sparsity issue. The transfer classifier will be learned in the original feature space. Considering that the labels of the external data may not be readily available or sufficiently enough, TCSST further exploits the unlabeled external data to aid the transfer classification. We develop novel strategies to allow TCSST to iteratively select high quality unlabeled external data to help with the classification. We evaluate the performance of TCSST on both benchmark as well as real-world data sets. Our experimental results demonstrate that the proposed method is effective in classifying very short & sparse text, consistently outperforming existing and baseline methods.

#index 1918412
#* The generalized dirichlet distribution in enhanced topic detection
#@ Karla L. Caballero;Joel Barajas;Ram Akella
#t 2012
#c 1
#% 275111
#% 722904
#% 875981
#% 876017
#% 879587
#% 1083687
#% 1130996
#% 1211693
#% 1211725
#% 1211828
#% 1383567
#! We present a new, robust and computationally efficient Hierarchical Bayesian model for effective topic correlation modeling. We model the prior distribution of topics by a Generalized Dirichlet distribution (GD) rather than a Dirichlet distribution as in Latent Dirichlet Allocation (LDA). We define this model as GD-LDA. This framework captures correlations between topics, as in the Correlated Topic Model (CTM) and Pachinko Allocation Model (PAM), and is faster to infer than CTM and PAM. GD-LDA is effective to avoid over-fitting as the number of topics is increased. As a tree model, it accommodates the most important set of topics in the upper part of the tree based on their probability mass. Thus, GD-LDA provides the ability to choose significant topics effectively. To discover topic relationships, we perform hyper-parameter estimation based on Monte Carlo EM Estimation. We provide results using Empirical Likelihood(EL) in 4 public datasets from TREC and NIPS. Then, we present the performance of GD-LDA in ad hoc information retrieval (IR) based on MAP, P@10, and Discounted Gain. We discuss an empirical comparison of the fitting time. We demonstrate significant improvement over CTM, LDA, and PAM for EL estimation. For all the IR measures, GD-LDA shows higher performance than LDA, the dominant topic model in IR. All these improvements with a small increase in fitting time than LDA, as opposed to CTM and PAM.

#index 1918413
#* Modeling topic hierarchies with the recursive chinese restaurant process
#@ Joon Hee Kim;Dongwoo Kim;Suin Kim;Alice Oh
#t 2012
#c 1
#% 722904
#% 840872
#% 876017
#% 983883
#% 1211828
#% 1482243
#% 1482290
#% 1482392
#% 1606004
#% 1642014
#! Topic models such as latent Dirichlet allocation (LDA) and hierarchical Dirichlet processes (HDP) are simple solutions to discover topics from a set of unannotated documents. While they are simple and popular, a major shortcoming of LDA and HDP is that they do not organize the topics into a hierarchical structure which is naturally found in many datasets. We introduce the recursive Chinese restaurant process (rCRP) and a nonparametric topic model with rCRP as a prior for discovering a hierarchical topic structure with unbounded depth and width. Unlike previous models for discovering topic hierarchies, rCRP allows the documents to be generated from a mixture over the entire set of topics in the hierarchy. We apply rCRP to a corpus of New York Times articles, a dataset of MovieLens ratings, and a set of Wikipedia articles and show the discovered topic hierarchies. We compare the predictive power of rCRP with LDA, HDP, and nested Chinese restaurant process (nCRP) using heldout likelihood to show that rCRP outperforms the others. We suggest two metrics that quantify the characteristics of a topic hierarchy to compare the discovered topic hierarchies of rCRP and nCRP. The results show that rCRP discovers a hierarchy in which the topics become more specialized toward the leaves, and topics in the immediate family exhibit more affinity than topics beyond the immediate family.

#index 1918414
#* Two-part segmentation of text documents
#@ Deepak P.;Karthik Visweswariah;Nirmalie Wiratunga;Sadiq Sani
#t 2012
#c 1
#% 81669
#% 176887
#% 278106
#% 287253
#% 448786
#% 494595
#% 741060
#% 748482
#% 748583
#% 748631
#% 817421
#% 818299
#% 1019124
#% 1074110
#% 1264752
#% 1301157
#% 1642244
#% 1711750
#% 1737314
#% 1737324
#! We consider the problem of segmenting text documents that have a two-part structure such as a problem part and a solution part. Documents of this genre include incident reports that typically involve description of events relating to a problem followed by those pertaining to the solution that was tried. Segmenting such documents into the component two parts would render them usable in knowledge reuse frameworks such as Case-Based Reasoning. This segmentation problem presents a hard case for traditional text segmentation due to the lexical inter-relatedness of the segments. We develop a two-part segmentation technique that can harness a corpus of similar documents to model the behavior of the two segments and their inter-relatedness using language models and translation models respectively. In particular, we use separate language models for the problem and solution segment types, whereas the inter-relatedness between segment types is modeled using an IBM Model 1 translation model. We model documents as being generated starting from the problem part that comprises of words sampled from the problem language model, followed by the solution part whose words are sampled either from the solution language model or from a translation model conditioned on the words already chosen in the problem part. We show, through an extensive set of experiments on real-world data, that our approach outperforms the state-of-the-art text segmentation algorithms in the accuracy of segmentation, and that such improved accuracy translates well to improved usability in Case-based Reasoning systems. We also analyze the robustness of our technique to varying amounts and types of noise and empirically illustrate that our technique is quite noise tolerant, and degrades gracefully with increasing amounts of noise.

#index 1918415
#* On the design of LDA models for aspect-based opinion mining
#@ Samaneh Moghaddam;Martin Ester
#t 2012
#c 1
#% 248218
#% 722904
#% 769892
#% 805873
#% 936239
#% 1035591
#% 1055682
#% 1190068
#% 1214741
#% 1250237
#% 1292503
#% 1292576
#% 1400002
#% 1470684
#% 1471389
#% 1481541
#% 1482445
#% 1484314
#% 1536585
#% 1536586
#% 1591941
#% 1598400
#% 1603759
#% 1605982
#% 1642287
#% 1693877
#! Aspect-based opinion mining, which aims to extract aspects and their corresponding ratings from customers reviews, provides very useful information for customers to make purchase decisions. In the past few years several probabilistic graphical models have been proposed to address this problem, most of them based on Latent Dirichlet Allocation (LDA). While these models have a lot in common, there are some characteristics that distinguish them from each other. These fundamental differences correspond to major decisions that have been made in the design of the LDA models. While research papers typically claim that a new model outperforms the existing ones, there is normally no "one-size-fits-all" model. In this paper, we present a set of design guidelines for aspect-based opinion mining by discussing a series of increasingly sophisticated LDA models. We argue that these models represent the essence of the major published methods and allow us to distinguish the impact of various design decisions. We conduct extensive experiments on a very large real life dataset from Epinions.com (500K reviews) and compare the performance of different models in terms of the likelihood of the held-out test set and in terms of the accuracy of aspect identification and rating prediction.

#index 1918416
#* Predicting query performance for fusion-based retrieval
#@ Gad Markovits;Anna Shtok;Oren Kurland;David Carmel
#t 2012
#c 1
#% 194276
#% 232703
#% 340890
#% 340936
#% 340959
#% 389801
#% 397161
#% 413613
#% 420464
#% 766431
#% 766497
#% 818262
#% 818267
#% 878912
#% 879582
#% 879614
#% 907544
#% 987260
#% 987265
#% 1130851
#% 1130990
#% 1227721
#% 1392446
#% 1392447
#% 1450861
#% 1450999
#% 1467729
#% 1529948
#% 1536584
#% 1598445
#% 1622337
#% 1741000
#% 1748097
#% 1918417
#! Estimating the effectiveness of a search performed in response to a query in the absence of relevance judgments is the goal of query-performance prediction methods. Post-retrieval predictors analyze the result list of the most highly ranked documents. We address the prediction challenge for retrieval approaches wherein the final result list is produced by fusing document lists that were retrieved in response to a query. To that end, we present a novel fundamental prediction framework that accounts for this special characteristics of the fusion setting; i.e., the use of intermediate retrieved lists. The framework is based on integrating prediction performed upon the final result list with that performed upon the lists that were fused to create it; prediction integration is controlled based on inter-list similarities. We empirically demonstrate the merits of various predictors instantiated from the framework. A case in point, their prediction quality substantially transcends that of applying state-of-the-art predictors upon the final result list.

#index 1918417
#* Back to the roots: a probabilistic framework for query-performance prediction
#@ Oren Kurland;Anna Shtok;Shay Hummel;Fiana Raiber;David Carmel;Ofri Rom
#t 2012
#c 1
#% 280864
#% 324129
#% 340899
#% 340901
#% 340948
#% 397161
#% 719598
#% 804915
#% 818267
#% 879578
#% 879613
#% 879614
#% 907544
#% 987260
#% 987265
#% 1130851
#% 1130990
#% 1195854
#% 1217048
#% 1227662
#% 1263599
#% 1392447
#% 1415713
#% 1450861
#% 1467729
#% 1482276
#% 1536512
#% 1598445
#% 1622337
#% 1697426
#% 1748097
#% 1879131
#! The query-performance prediction task is estimating the effectiveness of a search performed in response to a query when no relevance judgments are available. Although there exist many effective prediction methods, these differ substantially in their basic principles, and rely on diverse hypotheses about the characteristics of effective retrieval. We present a novel fundamental probabilistic prediction framework. Using the framework, we derive and explain various previously proposed prediction methods that might seem completely different, but turn out to share the same formal basis. The derivations provide new perspectives on several predictors (e.g., Clarity). The framework is also used to devise new prediction approaches that outperform the state-of-the-art.

#index 1918418
#* Learning to rank for robust question answering
#@ Arvind Agarwal;Hema Raghavan;Karthik Subbian;Prem Melville;Richard D. Lawrence;David C. Gondek;James Fan
#t 2012
#c 1
#% 309124
#% 330769
#% 340936
#% 411762
#% 577224
#% 734915
#% 766414
#% 815272
#% 840846
#% 879619
#% 976952
#% 983820
#% 987226
#% 987240
#% 987241
#% 1301004
#% 1482224
#% 1565540
#% 1590304
#% 1987084
#! This paper aims to solve the problem of improving the ranking of answer candidates for factoid based questions in a state-of-the-art Question Answering system. We first provide an extensive comparison of 5 ranking algorithms on two datasets -- from the Jeopardy quiz show and a medical domain. We then show the effectiveness of a cascading approach, where the ranking produced by one ranker is used as input to the next stage. The cascading approach shows sizeable gains on both datasets. We finally evaluate several rank aggregation techniques to combine these algorithms, and find that Supervised Kemeny aggregation is a robust technique that always beats the baseline ranking approach used by Watson for the Jeopardy competition. We further corroborate our results on TREC Question Answering datasets.

#index 1918419
#* Learning to rank by aggregating expert preferences
#@ Maksims N. Volkovs;Hugo Larochelle;Richard S. Zemel
#t 2012
#c 1
#% 309095
#% 340936
#% 387427
#% 413613
#% 464451
#% 577224
#% 956542
#% 1073931
#% 1211737
#% 1227721
#% 1563304
#% 1605919
#! We present a general treatment of the problem of aggregating preferences from several experts into a consensus ranking, in the context where information about a target ranking is available. Specifically, we describe how such problems can be converted into a standard learning-to-rank one on which existing learning solutions can be invoked. This transformation allows us to optimize the aggregating function for any target IR metric, such as Normalized Discounted Cumulative Gain, or Expected Reciprocal Rank. When applied to crowdsourcing and meta-search benchmarks, our new algorithm improves on state-of-the-art preference aggregation methods.

#index 1918420
#* Learning to rank duplicate bug reports
#@ Jian Zhou;Hongyu Zhang
#t 2012
#c 1
#% 201935
#% 255137
#% 347225
#% 376266
#% 571725
#% 577224
#% 734915
#% 766414
#% 783474
#% 840846
#% 868135
#% 879588
#% 879600
#% 907546
#% 961406
#% 983820
#% 987240
#% 987241
#% 1056192
#% 1074021
#% 1074121
#% 1268491
#% 1426340
#% 1532937
#% 1642044
#% 1748770
#! For a large and complex software system, the project team could receive a large number of bug reports. Some bug reports could be duplicates as they essentially report the same problem. It is often tedious and costly to manually check if a newly reported bug is a duplicate of an already reported bug. In this paper, we propose BugSim, a method that can automatically retrieve duplicate bug reports given a new bug report. BugSim is based on learning to rank concepts. We identify textual and statistical features of bug reports and propose a similarity function for bug reports based on the features. We then construct a training set by assembling pairs of duplicate and non-duplicate bug reports. We train the weights of features by applying the stochastic gradient descent algorithm over the training set. For a new bug report, we retrieve candidate duplicate reports using the trained model. We evaluate BugSim using more than 45,100 real bug reports of twelve Eclipse projects. The evaluation results show that the proposed method is effective. On average, the recall rate for the top 10 retrieved reports is 76.11%. Furthermore, BugSim outperforms the previous state-of-art methods that are implemented using SVM and BM25Fext.

#index 1918421
#* A model-based approach for RFID data stream cleansing
#@ Zhou Zhao;Wilfred Ng
#t 2012
#c 1
#% 824747
#% 864527
#% 891559
#% 893102
#% 893103
#% 1016227
#% 1063523
#% 1206879
#% 1372724
#% 1426506
#% 1550753
#% 1651534
#% 1760888
#! In recent years, RFID technologies have been used in many applications, such as inventory checking and object tracking. However, raw RFID data are inherently unreliable due to physical device limitations and different kinds of environmental noise. Currently, existing work mainly focuses on RFID data cleansing in a static environment (e.g. inventory checking). It is therefore difficult to cleanse RFID data streams in a mobile environment (e.g. object tracking) using the existing solutions, which do not address the data missing issue effectively. In this paper, we study how to cleanse RFID data streams for object tracking, which is a challenging problem, since a significant percentage of readings are routinely dropped. We propose a probabilistic model for object tracking in a mobile environment. We develop a Bayesian inference based approach for cleansing RFID data using the model. In order to sample data from the movement distribution, we devise a sequential sampler that cleans RFID data with high accuracy and efficiency. We validate the effectiveness and robustness of our solution through extensive simulations and demonstrate its performance by using two real RFID applications of human tracking and conveyor belt monitoring.

#index 1918422
#* What is the IQ of your data transformation system?
#@ Giansalvatore Mecca;Paolo Papotti;Salvatore Raunich;Donatello Santoro
#t 2012
#c 1
#% 42401
#% 375017
#% 384978
#% 480134
#% 480429
#% 739899
#% 806215
#% 824676
#% 826007
#% 826032
#% 960233
#% 993981
#% 1039063
#% 1044442
#% 1081947
#% 1127370
#% 1127589
#% 1206578
#% 1217196
#% 1328193
#% 1332777
#% 1332778
#% 1351373
#% 1426593
#% 1523804
#% 1576025
#% 1661428
#! Mapping and translating data across different representations is a crucial problem in information systems. Many formalisms and tools are currently used for this purpose, to the point that developers typically face a difficult question: "what is the right tool for my translation task?" In this paper, we introduce several techniques that contribute to answer this question. Among these, a fairly general definition of a data transformation system, a new and very efficient similarity measure to evaluate the outputs produced by such a system, and a metric to estimate user efforts. Based on these techniques, we are able to compare a wide range of systems on many translation tasks, to gain interesting insights about their effectiveness, and, ultimately, about their "intelligence".

#index 1918423
#* On the foundations of probabilistic information integration
#@ Fereidoon Sadri
#t 2012
#c 1
#% 32879
#% 333988
#% 480645
#% 482108
#% 572314
#% 745436
#% 765433
#% 790843
#% 800006
#% 800497
#% 810021
#% 810073
#% 824769
#% 830529
#% 864394
#% 893089
#% 960233
#% 960272
#% 960352
#% 992830
#% 993982
#% 993985
#% 1016201
#% 1022259
#% 1127372
#% 1130983
#% 1190673
#% 1200291
#% 1206717
#% 1206987
#% 1270473
#% 1291115
#% 1328155
#% 1425119
#% 1523891
#% 1567959
#% 1661428
#% 1846699
#% 1853523
#! Information integration has been a subject of research for several decades and still remains a very active research area. Many new applications depend or benefit from large scale integration. Examples include large research projects in life sciences, need for data sharing among government agencies, reliance of corporations on business intelligence (which requires data integration from many heterogeneous sources), and integration of information on the web. The importance of information integration with uncertainty has been observed in recent years. Frequently, information from multiple sources are uncertain and possibly inconsistent. Further the process of integration often depends on approximate schema mappings, another source of uncertainty. An integration system is useful only to the extent that the information it produces can be trusted. Hence, providing a measure of certainty for integrated information is of crucial importance in many important applications. In this paper we study the problem of integration of uncertain information. We present a simple and intuitive approach to the representation and integration of uncertain information from multiple sources, and show that our integration approach coincides with a recent formalism for uncertain information integration. We extend the model to probabilistic possible-worlds, and show certain unintuitive constraints are imposed upon probabilities of possible-worlds of sources. In particular, we show the probabilities of possible worlds of a source are not independent, rather, they are dependent on probabilities of other sources. We study the problem of determining the probabilities for the result of integration. Finally, we present a practical approach to relaxing probabilistic constraints in integration.

#index 1918424
#* GPU acceleration of probabilistic frequent itemset mining from uncertain databases
#@ Yusuke Kozawa;Toshiyuki Amagasa;Hiroyuki Kitagawa
#t 2012
#c 1
#% 152934
#% 300120
#% 379325
#% 443091
#% 481290
#% 874997
#% 1022310
#% 1063508
#% 1127463
#% 1189215
#% 1214633
#% 1222050
#% 1270566
#% 1299131
#% 1393138
#% 1451166
#% 1482221
#% 1632939
#! Uncertain databases have been widely developed to deal with the vast amount of data that contain uncertainty. To extract valuable information from the uncertain databases, several methods of frequent itemset mining, one of the major data mining techniques, have been proposed. However, their performance is not satisfactory because handling uncertainty incurs high processing costs. In order to address this problem, we utilize GPGPU (General-Purpose computation on GPU). GPGPU implies using a GPU (Graphics Processing Unit), which is originally designed for processing graphics, to accelerate general purpose computation. In this paper, we propose a method of frequent itemset mining from uncertain databases using GPGPU. The main idea is to speed up probability computations by making the best use of GPU's high parallelism and low-latency memory. We also employ an algorithm to manipulate a bitstring and data-parallel primitives to improve performance in the other parts of the method. Extensive experiments show that our proposed method is up to two orders of magnitude faster than existing methods.

#index 1918425
#* Completeness of queries over SQL databases
#@ Werner Nutt;Simon Razniewski
#t 2012
#c 1
#% 663
#% 2984
#% 12515
#% 67457
#% 94459
#% 264858
#% 384978
#% 465057
#% 481786
#% 599549
#% 778322
#% 809238
#% 903332
#% 1217124
#% 1426458
#% 1661452
#! Data completeness is an important aspect of data quality. We consider a setting, where databases can be incomplete in two ways: records may be missing and records may contain null values. We (i) formalize when the answer set of a query is complete in spite of such incompleteness, and (ii) we introduce table completeness statements, by which one can express that certain parts of a database are complete. We then study how to deduce from a set of table-completeness statements that a query can be answered completely. Null values as used in SQL are ambiguous. They can indicate either that no attribute value exists or that a value exists, but is unknown. We study completeness reasoning for the different interpretations. We show that in the combined case it is necessary to syntactically distinguish between different kinds of null values and present an encoding for doing that in standard SQL databases. With this technique, any SQL DBMS evaluates complete queries correctly with respect to the different meanings that nulls can carry. We study the complexity of completeness reasoning and provide algorithms that in most cases agree with the worst-case lower bounds.

#index 1918426
#* Being picky: processing top-k queries with set-defined selections
#@ Aleksandar Stupar;Sebastian Michel
#t 2012
#c 1
#% 2833
#% 223781
#% 249238
#% 274612
#% 278831
#% 340886
#% 379448
#% 399762
#% 480330
#% 519953
#% 643566
#% 768521
#% 777931
#% 810018
#% 824704
#% 879610
#% 893127
#% 1016183
#% 1075132
#% 1124990
#% 1206752
#% 1408797
#% 1409954
#% 1480899
#% 1523799
#% 1523872
#% 1598405
#% 1598433
#% 1701405
#! Focusing on the top-K items according to a ranking criterion constitutes an important functionality in many different query answering scenarios. The idea is to read only the necessary information---mostly from secondary storage---with the ultimate goal to achieve low latency. In this work, we consider processing such top-K queries under the constraint that the result items are members of a specific set, which is provided at query time. We call this restriction a set-defined selection criterion. Set-defined selections drastically influence the pros and cons of an id-ordered index vs. a score-ordered index. We present a mathematical model that allows to decide at runtime which index to choose, leading to a combined index. To improve the latency around the break even point of the two indices, we show how to benefit from a partitioned score-ordered index and present an algorithm to create such partitions based on analyzing query logs. Further performance gains can be enjoyed using approximate top-K results, with tunable result quality. The presented approaches are evaluated using both real-world and synthetic data.

#index 1918427
#* Finding top k most influential spatial facilities over uncertain objects
#@ Liming Zhan;Ying Zhang;Wenjie Zhang;Xuemin Lin
#t 2012
#c 1
#% 300163
#% 427199
#% 464859
#% 772835
#% 824728
#% 824730
#% 1063520
#% 1206893
#% 1207234
#% 1211651
#% 1380967
#% 1565405
#% 1606348
#% 1642087
#% 1642258
#% 1648946
#% 1798418
#% 1931745
#! Uncertainty is inherent in many important applications, such as location-based services (LBS), sensor monitoring and radio-frequency identification (RFID). Recently, considerable research efforts have been put into the field of uncertainty-aware spatial query processing. In this paper, we study the problem of finding top k most influential facilities over a set of uncertain objects, which is an important spatial query in the above applications. Based on the maximal utility principle, we propose a new ranking model to identify the top k most influential facilities, which carefully captures influence of facilities on the uncertain objects. By utilizing two uncertain object indexing techniques, R-tree and U-Quadtree, effective and efficient algorithms are proposed following the filtering and verification paradigm, which significantly improves the performance of the algorithms in terms of CPU and I/O costs. Comprehensive experiments on real datasets demonstrate the effectiveness and efficiency of our techniques.

#index 1918428
#* Efficient safe-region construction for moving top-K spatial keyword queries
#@ Weihuang Huang;Guoliang Li;Kian-Lee Tan;Jianhua Feng
#t 2012
#c 1
#% 397377
#% 527187
#% 654478
#% 783533
#% 810048
#% 836096
#% 838407
#% 874993
#% 982560
#% 993955
#% 1127438
#% 1206801
#% 1206997
#% 1328137
#% 1581875
#% 1581876
#% 1594674
#% 1846749
#% 1848110
#% 1918373
#! Many real-world applications have requirements to support moving spatial keyword queries. For example a tourist looks for top-k "seafood restaurants" while walking in a city. She will continuously issue moving queries. However existing spatial keyword search methods focus on static queries and it calls for new effective techniques to support moving queries efficiently. In this paper we propose an effective method to support moving top-k spatial keyword queries. In addition to finding top-k answers of a moving query, we also calculate a safe region such that if a new query with a location falling in the safe region, we can directly use the answer set to answer the query. To this end, we propose an effective model to represent the safe region and devise efficient search algorithms to compute the safe region. We have implemented our method and experimental results on real datasets show that our method achieves high efficiency and outperforms existing methods significantly.

#index 1918429
#* Monochromatic and bichromatic reverse nearest neighbor queries on land surfaces
#@ Da Yan;Zhou Zhao;Wilfred Ng
#t 2012
#c 1
#% 28114
#% 86822
#% 201876
#% 287466
#% 300163
#% 480661
#% 652272
#% 824730
#% 863390
#% 864466
#% 1015320
#% 1016191
#% 1058620
#% 1072648
#% 1127432
#% 1127435
#% 1328202
#% 1480782
#% 1567154
#% 1581882
#% 1594683
#% 1694372
#! Finding reverse nearest neighbors (RNNs) is an important operation in spatial databases. The problem of evaluating RNN queries has already received considerable attention due to its importance in many real-world applications, such as resource allocation and disaster response. While RNN query processing has been extensively studied in Euclidean space, no work ever studies this problem on land surfaces. However, practical applications of RNN queries involve terrain surfaces that constrain object movements, which rendering the existing algorithms inapplicable. In this paper, we investigate the evaluation of two types of RNN queries on land surfaces: monochromatic RNN (MRNN) queries and bichromatic RNN (BRNN) queries. On a land surface, the distance between two points is calculated as the length of the shortest path along the surface. However, the computational cost of the state-of-the-art shortest path algorithm on a land surface is quadratic to the size of the surface model, which is usually quite huge. As a result, surface RNN query processing is a challenging problem. Leveraging some newly-discovered properties of Voronoi cell approximation structures, we make use of standard index structures such as an R-tree to design efficient algorithms that accelerate the evaluation of MRNN and BRNN queries on land surfaces. Our proposed algorithms are able to localize query evaluation by accessing just a small fraction of the surface data near the query point, which helps avoid shortest path evaluation on a large surface. Extensive experiments are conducted on large real-world datasets to demonstrate the efficiency of our algorithms.

#index 1918430
#* Pay-as-you-go maintenance of precomputed nearest neighbors in large graphs
#@ Tom Crecelius;Ralf Schenkel
#t 2012
#c 1
#% 211658
#% 347196
#% 379482
#% 510420
#% 593917
#% 731402
#% 789798
#% 800534
#% 824695
#% 833131
#% 919828
#% 960304
#% 1063514
#% 1074116
#% 1181254
#% 1181255
#% 1206702
#% 1217208
#% 1218741
#% 1265149
#% 1292590
#% 1380974
#% 1399992
#% 1400031
#% 1482228
#! An important building block of many graph applications such as searching in social networks, keyword search in graphs, and retrieval of linked documents is retrieving the transitive neighbors of a node in ascending order of their distances. Since large graphs cannot be kept in memory and graph traversals at query time would be prohibitively expensive, the list of neighbors for each node is usually precomputed and stored in a compact form. While the problem of precomputing all-pairs shortest distances has been well studied for decades, efficiently maintaining this information when the graph changes is not as well understood. This paper presents an algorithm for maintaining nearest neighbor lists in weighted graphs under node insertions and decreasing edge weights. It considers the important case where queries are a lot more frequent than updates, and presents two approaches for transparently performing necessary index updates while executing queries. Extensive experiments with large graphs, including a subset of Twitter's user graph, demonstrate that the overhead for this maintenance is small.

#index 1918431
#* Spatial influence vs. community influence: modeling the global spread of social media
#@ Krishna Y. Kamath;James Caverlee;Zhiyuan Cheng;Daniel Z. Sui
#t 2012
#c 1
#% 480467
#% 729923
#% 1055707
#% 1083672
#% 1214671
#% 1399939
#% 1399992
#% 1451242
#% 1482254
#% 1535333
#% 1544032
#% 1560424
#% 1605960
#% 1606436
#% 1676017
#% 1693865
#% 1693881
#% 1746822
#! In this paper we seek to understand and model the global spread of social media. How does social media spread from location to location across the globe? Can we model this spread and predict where social media will be popular in the future? Toward answering these questions, we develop a probabilistic model that synthesizes two conflicting hypotheses about the nature of online information spread: (i) the spatial influence model, which asserts that social media spreads to locations that are close by; and (ii) the community affinity influence model, which asserts that social media spreads between locations that are culturally connected, even if they are distant. Based on the geospatial footprint of 755 million geo-tagged hashtags spread through Twitter, we evaluate these models at predicting locations that will adopt hashtags in the future. We find that distance is the single most important explanation of future hashtag adoption since hashtags are fundamentally local. We also find that community affinities (like culture, language, and common interests) enhance the quality of purely spatial models, indicating the necessity of incorporating non-spatial features into models of global social media spread.

#index 1918432
#* TUT: a statistical model for detecting trends, topics and user interests in social media
#@ Xuning Tang;Christopher C. Yang
#t 2012
#c 1
#% 218992
#% 280819
#% 722904
#% 766433
#% 771924
#% 788094
#% 823344
#% 875959
#% 881498
#% 1176853
#% 1289476
#% 1292563
#% 1446962
#% 1536536
#% 1560381
#% 1728235
#% 1746887
#! The rapid development of online social media sites is accompanied by the generation of tremendous web contents. Web users are shifting from data consumers to data producers. As a result, topic detection and tracking without taking users' interests into account is not enough. This paper presents a statistical model that can detect interpretable trends and topics from document streams, where each trend (short for trending story) corresponds to a series of continuing events or a storyline. A topic is represented by a cluster of words frequently co-occurred. A trend can contain multiple topics and a topic can be shared by different trends. In addition, by leveraging a Recurrent Chinese Restaurant Process (RCRP), the number of trends in our model can be determined automatically without human intervention, so that our model can better generalize to unseen data. Furthermore, our proposed model incorporates user interest to fully simulate the generation process of web contents, which offers the opportunity for personalized recommendation in online social media. Experiments on three different datasets indicated that our proposed model can capture meaningful topics and trends, monitor rise and fall of detected trends, outperform baseline approach in terms of perplexity on held-out dataset, and improve the result of user participation prediction by leveraging users' interests to different trends.

#index 1918433
#* Predicting aggregate social activities using continuous-time stochastic process
#@ Shu Huang;Min Chen;Bo Luo;Dongwon Lee
#t 2012
#c 1
#% 823342
#% 881460
#% 956578
#% 989663
#% 991977
#% 1035580
#% 1083675
#% 1214722
#% 1246431
#% 1399995
#% 1451233
#% 1482199
#% 1482200
#% 1535324
#% 1560406
#% 1560408
#% 1606049
#% 1606051
#% 1642028
#% 1693927
#% 1746858
#% 1746868
#! How to accurately model and predict the future status of social networks has become an important problem in recent years. Conventional solutions to such a problem often employ topological structure of the sociogram, i.e., friendship links. However, they often disregard different levels of activeness of social actors and become insufficient to deal with complex dynamics of user behaviors. In this paper, to address this issue, we first refine the notion of social activity to better describe dynamic user behaviors in social networks. We then propose a Parameterized Social Activity Model (PSAM) using continuous-time stochastic process for predicting aggregate social activities. With social activities evolving over time, PSAM itself also evolves and therefore dynamically captures the real-time characteristics of the current active population. Our experiments using two real social networks (Facebook and CiteSeer) reveal that the proposed PSAM model is effective in simulating social activity evolution and predicting aggregate social activities accurately at different time scales.

#index 1918434
#* Acquiring temporal constraints between relations
#@ Partha Pratim Talukdar;Derry Wijaya;Tom Mitchell
#t 2012
#c 1
#% 757953
#% 811284
#% 939595
#% 1024551
#% 1261547
#% 1264790
#% 1267783
#% 1328349
#% 1372745
#% 1560247
#% 1642010
#% 1693868
#% 1913782
#! We consider the problem of automatically acquiring knowledge about the typical temporal orderings among relations (e.g., actedIn(person, film) typically occurs before wonPrize (film, award)), given only a database of known facts (relation instances) without time information, and a large document collection. Our approach is based on the conjecture that the narrative order of verb mentions within documents correlates with the temporal order of the relations they represent. We propose a family of algorithms based on this conjecture, utilizing a corpus of 890m dependency parsed sentences to obtain verbs that represent relations of interest, and utilizing Wikipedia documents to gather statistics on narrative order of verb mentions. Our proposed algorithm, GraphOrder, is a novel and scalable graph-based label propagation algorithm that takes transitivity of temporal order into account, as well as these statistics on narrative order of verb mentions. This algorithm achieves as high as 38.4% absolute improvement in F1 over a random baseline. Finally, we demonstrate the utility of this learned general knowledge about typical temporal orderings among relations, by showing that these temporal constraints can be successfully used by a joint inference framework to assign specific temporal scopes to individual facts.

#index 1919709
#* Towards optimum query segmentation: in doubt without
#@ Matthias Hagen;Martin Potthast;Anna Beyer;Benno Stein
#t 2012
#c 1
#% 869501
#% 878624
#% 987224
#% 1019133
#% 1055706
#% 1227747
#% 1264824
#% 1310431
#% 1399978
#% 1450970
#% 1482372
#% 1560191
#% 1560364
#% 1591939
#% 1598362
#% 1697473
#! Query segmentation is the problem of identifying those keywords in a query, which together form compound concepts or phrases like "new york times". Such segments can help a search engine to better interpret a user's intents and to tailor the search results more appropriately. Our contributions to this problem are threefold. (1) We conduct the first large-scale study of human segmentation behavior based on more than 500000 segmentations. (2) We show that the traditionally applied segmentation accuracy measures are not appropriate for such large-scale corpora and introduce new, more robust measures. (3) We develop a new query segmentation approach with the basic idea that, in cases of doubt, it is often better to (partially) leave queries without any segmentation. This new in-doubt-without approach chooses different segmentation strategies depending on query types. A large-scale evaluation shows substantial improvement upon the state of the art in terms of segmentation accuracy. To draw a complete picture, we also evaluate the impact of segmentation strategies on retrieval performance in a TREC setting. It turns out that more accurate segmentation not necessarily yields better retrieval performance. Based on this insight, we propose an in-doubt-without variant which achieves the best retrieval performance despite leaving many queries unsegmented. But there is still room for improvement: the optimum segmentation strategy which always chooses the segmentation that maximizes retrieval performance, significantly outperforms all other tested approaches.

#index 1919710
#* Leaving so soon?: understanding and predicting web search abandonment rationales
#@ Abdigani Diriye;Ryen White;Georg Buscher;Susan Dumais
#t 2012
#c 1
#% 754059
#% 770857
#% 805200
#% 818221
#% 879565
#% 879567
#% 954948
#% 956495
#% 1004294
#% 1048694
#% 1053505
#% 1073970
#% 1083643
#% 1130852
#% 1227582
#% 1292474
#% 1355038
#% 1450833
#% 1450834
#% 1450845
#% 1560357
#% 1573487
#% 1598367
#% 1598515
#% 1693899
#% 1697423
#! Users of search engines often abandon their searches. Despite the high frequency of Web search abandonment and its importance to Web search engines, little is known about why searchers abandon beyond that it can be for good or bad reasons. In this paper, we ex-tend previous work by studying search abandonment using both a retrospective survey and an in-situ method that captures aban-donment rationales at abandonment time. We show that although satisfaction is a common motivator for abandonment, one-in-five abandonment instances does not relate to satisfaction. We also studied the automatic prediction of the underlying reason for ob-served abandonment. We used features of the query and the results, interaction with the result page (e.g., cursor movements, scrolling, clicks), and the full search session. We show that our classifiers can learn to accurately predict the reasons for observed search abandonment. Such accurate predictions help search providers estimate user satisfaction for queries without clicks, affording a more complete understanding of search engine performance.

#index 1919711
#* Click patterns: an empirical representation of complex query intents
#@ Huizhong Duan;Emre Kiciman;ChengXiang Zhai
#t 2012
#c 1
#% 131434
#% 283425
#% 292686
#% 642994
#% 643017
#% 956632
#% 995516
#% 1035574
#% 1035578
#% 1074071
#% 1074093
#% 1074113
#% 1190102
#% 1264955
#% 1279769
#% 1450895
#% 1470617
#% 1482378
#% 1598413
#% 1712595
#! Understanding users' search intents is critical component of modern search engines. A key limitation made by most query log analyses is the assumption that each clicked web result represents one unique intent. However, there are many search tasks, such as comparison shopping or in-depth research, where a user's intent is to explore many documents. In these cases, the assumption of a one-to-one correspondence between clicked documents and user intent breaks down. To capture and understand such behaviors, we propose the use of click patterns. Click patterns capture the relationship among clicks on search results by treating the set of clicks made by a user as a single unit. We aggregate click patterns together using a hierarchical clustering algorithm to discover the common click patterns. By using click patterns as an empirical representation of user intent, we are able to create a rich representation of mixtures of multiple navigational and informational intents. We analyze real search logs and demonstrate that such complex mixtures of intents do occur in the wild and can be identified using click patterns. We further demonstrate the usefulness of click patterns by integrating them into a measure of query ambiguity and into a query recommendation task. We show that calculating query ambiguity as the entropy over the distribution of click patterns provides a measure of ambiguity with improved discriminative power, consistency and temporal stability as compared to previous measures of ambiguity. We explore the use of click pattern similarity and click pattern entropy in generating query recommendations and show promising results.

#index 1919712
#* Domain dependent query reformulation for web search
#@ Van Dang;Giridhar Kumaran;Adam Troy
#t 2012
#c 1
#% 310567
#% 330617
#% 348155
#% 411762
#% 577224
#% 579944
#% 643057
#% 740915
#% 823348
#% 838531
#% 843645
#% 869501
#% 939629
#% 939973
#% 987272
#% 1074098
#% 1130854
#% 1130855
#% 1130878
#% 1227621
#% 1355020
#% 1536531
#% 1549092
#% 1642268
#% 1712595
#! Query reformulation has been studied as a domain independent task. Existing work attempts to expand a query or substitute its terms with the same set of candidates regardless of the domain of this query. Since terms might be semantically related in one domain but not in others, it is more effective to provide candidates for queries with respect to their domain. This paper demonstrates the advantage of this domain dependent query reformulation approach, which learns its candidates, using a standard technique, for each domain from a separate sample of data derived automatically from a generic query log. Our results show that our approach statistically significantly outperforms the domain independent approach, which learns to reformulate from the same log using the same technique, on a large query set consisting of both health and commerce queries. Our results have very practical interpretation: while building different reformulation systems to handle queries from different domains does not require additional manual effort, it provides substantially better retrieval effectiveness than having a single system handling all queries. Additionally, we show that leveraging domain specific manually labelled data leads to further improvement.

#index 1919713
#* An automatic blocking mechanism for large-scale de-duplication tasks
#@ Anish Das Sarma;Ankur Jain;Ashwin Machanavajjhala;Philip Bohannon
#t 2012
#c 1
#% 201889
#% 310516
#% 577238
#% 587758
#% 875066
#% 913783
#% 915242
#% 1015256
#% 1019087
#% 1201863
#% 1217163
#% 1250576
#% 1411056
#% 1426543
#% 1581925
#% 1890006
#! De-duplication - identification of distinct records referring to the same real-world entity - is a well-known challenge in data integration. Since very large datasets prohibit the comparison of every pair of records, blocking has been identified as a technique of dividing the dataset for pairwise comparisons, thereby trading off recall of identified duplicates for efficiency. Traditional de-duplication tasks, while challenging, typically involved a fixed schema such as Census data or medical records. However, with the presence of large, diverse sets of structured data on the web and the need to organize it effectively on content portals, de-duplication systems need to scale in a new dimension to handle a large number of schemas, tasks and data sets, while handling ever larger problem sizes. In addition, when working in a map-reduce framework it is important that canopy formation be implemented as a hash function, making the canopy design problem more challenging. We present CBLOCK, a system that addresses these challenges. CBLOCK learns hash functions automatically from attribute domains and a labeled dataset consisting of duplicates. Subsequently, CBLOCK expresses blocking functions using a hierarchical tree structure composed of atomic hash functions. The application may guide the automated blocking process based on architectural constraints, such as by specifying a maximum size of each block (based on memory requirements), impose disjointness of blocks (in a grid environment), or specify a particular objective function trading off recall for efficiency. As a post-processing step to automatically generated blocks, CBLOCK rolls-up smaller blocks to increase recall. We present experimental results on two large-scale de-duplication datasets from a commercial search engine - consisting of over 140K movies and 40K restaurants respectively - and demonstrate the utility of CBLOCK.

#index 1919714
#* Processing continuous text queries featuring non-homogeneous scoring functions
#@ Nelly Vouzoukidou;Bernd Amann;Vassilis Christophides
#t 2012
#c 1
#% 300174
#% 397608
#% 736290
#% 754106
#% 772835
#% 805848
#% 1015265
#% 1077150
#% 1130912
#% 1206848
#% 1206927
#% 1355053
#% 1356185
#% 1482227
#% 1492069
#% 1631205
#% 1633088
#% 1682074
#% 1694383
#! In this work we are interested in the scalable processing of content filtering queries over text item streams. In particular, we are aiming to generalize state of the art solutions with non-homogeneous scoring functions combining query-independent item importance with query-dependent content relevance. While such complex ranking functions are widely used in web search engines this is to our knowledge the first scientific work studying their usage in a continuous query scenario. Our main contribution consists in the definition and the evaluation of new efficient in-memory data structures for indexing continuous top-k queries based on an original two-dimensional representation of text queries. We are exploring locally-optimal score bounds and heuristics that efficiently prune the search space of candidate top-k query results which have to be updated at the arrival of new stream items. Finally, we experimentally evaluate memory/matching time trade-offs of these index structures. In particular we experimentally illustrate their linear scaling behavior with respect to the number of indexed queries.

#index 1919715
#* Comprehension-based result snippets
#@ Abhijith Kashyap;Vagelis Hristidis
#t 2012
#c 1
#% 311872
#% 397130
#% 566642
#% 654442
#% 765464
#% 875003
#% 879618
#% 907550
#% 960259
#% 987208
#% 1016176
#% 1063493
#% 1074133
#% 1130808
#% 1166473
#% 1206993
#% 1263587
#% 1328135
#% 1426572
#% 1482250
#% 1594636
#! Result snippets are used by most search interfaces to preview query results. Snippets help users quickly decide the relevance of the results, thereby reducing the overall search time and effort. Most work on snippets have focused on text snippets for Web pages in Web search. However, little work has studied the problem of snippets for structured data, e.g., product catalogs. Furthermore, all works have focused on the important goal of creating informative snippets, but have ignored the amount of user effort required to comprehend, i.e., read and digest, the displayed snippets. In particular, they implicitly assume that the comprehension effort or cost only depends on the length of the snippet, which we show is incorrect for structured data. We propose novel techniques to construct snippets of structured heterogeneous results, which not only select the most informative attributes for each result, but also minimize the expected user effort (time) to comprehend these snippets. We create a comprehension model to quantify the effort incurred by users in comprehending a list of result snippets. Our model is supported by an extensive user-study. A key observation is that the user effort for comprehending an attribute across multiple snippets only depends on the number of unique positions (e.g., indentations) where this attribute is displayed and not on the number of occurrences. We analyze the complexity of the snippet construction problem and show that the problem is NP-hard, even when we only consider the comprehension cost. We present efficient approximate algorithms, and experimentally demonstrate their effectiveness and efficiency.

#index 1919716
#* An effective rule miner for instance matching in a web of data
#@ Xing Niu;Shu Rong;Haofen Wang;Yong Yu
#t 2012
#c 1
#% 300120
#% 481290
#% 913783
#% 924747
#% 1023420
#% 1246170
#% 1288161
#% 1288166
#% 1292570
#% 1333469
#% 1333474
#% 1390188
#% 1409921
#% 1416176
#% 1455643
#% 1540301
#% 1540329
#% 1560363
#% 1597482
#% 1719976
#% 1826430
#! Publishing structured data and linking them to Linking Open Data (LOD) is an ongoing effort to create a Web of data. Each newly involved data source may contain duplicated instances (entities) whose descriptions or schemata differ from those of the existing sources in LOD. To tackle this heterogeneity issue, several matching methods have been developed to link equivalent entities together. Many general-purpose matching methods which focus on similarity metrics suffer from very diverse matching results for different data source pairs. On the other hand, the dataset-specific ones leverage heuristic rules or even manual efforts to ensure the quality, which makes it impossible to apply them to other sources or domains. In this paper, we offer a third choice, a general method of automatically discovering dataset-specific matching rules. In particular, we propose a semi-supervised learning algorithm to iteratively refine matching rules and find new matches of high confidence based on these rules. This dramatically relieves the burden on users of defining rules but still gives high-quality matching results. We carry out experiments on real-world large scale data sources in LOD; the results show the effectiveness of our approach in terms of the precision of discovered matches and the number of missing matches found. Furthermore, we discuss several extensions (like similarity embedded rules, class restriction and SPARQL rewriting) to fit various applications with different requirements.

#index 1919717
#* Non-stationary bayesian networks based on perfect simulation
#@ Yi Jia;Wenrong Zeng;Jun Huan
#t 2012
#c 1
#% 722804
#% 866470
#% 906464
#% 906571
#% 976849
#% 994148
#% 1126391
#% 1556733
#! Non-stationary Dynamic Bayesian Networks (Non-stationary DBNs) are widely used to model the temporal changes of directed dependency structures from multivariate time series data. However, the existing change-points based non-stationary DBNs methods have several drawbacks including excessive computational cost, and low convergence speed. In this paper we proposed a novel non-stationary DBNs method. Our method is based on the perfect simulation model. We applied this approach for network structure inference from synthetic data and biological microarray gene expression data and compared it with other two state-of-the-art non-stationary DBNs methods. The experimental results demonstrated that our method outperformed two other state-of-the-art methods in both computational cost and structure prediction accuracy. The further sensitivity analysis showed that once converged our model is robust to large parameter ranges, which reduces the uncertainty of the model behavior.

#index 1919718
#* Active learning for relation type extension with local and global data views
#@ Ang Sun;Ralph Grishman
#t 2012
#c 1
#% 252011
#% 529191
#% 722926
#% 783552
#% 939383
#% 939384
#% 939604
#% 939944
#% 1292488
#% 1330551
#% 1330553
#% 1470649
#% 1484258
#% 1544141
#% 1591981
#! Relation extraction is the process of identifying instances of specified types of semantic relations in text; relation type extension involves extending a relation extraction system to recognize a new type of relation. We present LGCo-Testing, an active learning system for relation type extension based on local and global views of relation instances. Locally, we extract features from the sentence that contains the instance. Globally, we measure the distributional similarity between instances from a 2 billion token corpus. Evaluation on the ACE 2004 corpus shows that LGCo-Testing can reduce annotation cost by 97% while maintaining the performance level of supervised learning.

#index 1919719
#* Segmenting web-domains and hashtags using length specific models
#@ Sriram Srinivasan;Sourangshu Bhattacharya;Rudrasis Chakraborty
#t 2012
#c 1
#% 208705
#% 464434
#% 741035
#% 748738
#% 770763
#% 811358
#% 939585
#% 939729
#% 1077150
#% 1450887
#% 1560210
#% 1560390
#% 1592068
#! Segmentation of a string of English language characters into a sequence of words has many applications. Here, we study two applications in the internet domain. First application is the web domain segmentation which is crucial for monetization of broken URLs. Secondly, we propose and study a novel application of twitter hashtag segmentation for increasing recall on twitter searches. Existing methods for word segmentation use unsupervised language models. We find that when using multiple corpora, the joint probability model from multiple corpora performs significantly better than the individual corpora. Motivated by this, we propose weighted joint probability model, with weights specific to each corpus. We propose to train the weights in a supervised manner using max-margin methods. The supervised probability models improve segmentation accuracy over joint probability models. Finally, we observe that length of segments is an important parameter for word segmentation, and incorporate length-specific weights into our model. The length specific models further improve segmentation accuracy over supervised probability models. For all models proposed here, inference problem can be solved using the dynamic programming algorithm. We test our methods on five different datasets, two from web domains data, and three from news headlines data from an LDC dataset. The supervised length specific models show significant improvements over unsupervised single corpus and joint probability models. Cross-testing between the datasets confirm that supervised probability models trained on all datasets, and length specific models trained on news headlines data, generalize well. Segmentation of hashtags result in significant improvement in recall on searches for twitter trends.

#index 1919720
#* Crosslingual distant supervision for extracting relations of different complexity
#@ Andre Blessing;Hinrich Schütze
#t 2012
#c 1
#% 471758
#% 754104
#% 782759
#% 939384
#% 940028
#% 956564
#% 1063570
#% 1083647
#% 1166514
#% 1271483
#% 1328351
#% 1330550
#% 1409944
#% 1409954
#% 1471191
#% 1471208
#% 1471426
#% 1481634
#% 1482288
#% 1484250
#% 1484251
#% 1484304
#% 1496780
#% 1592192
#! We propose crosslingual distant supervision (crosslingual DS) for relation extraction, an approach that automatically extracts labels from a pivot language for labeling one or more target languages. The approach has two benefits compared to standard DS: (i) increased coverage if target language labels are not available; and (ii) higher accuracy of automatically generated labels because noisy labels are eliminated in crosslingual filtering. An evaluation for two relations of different complexity shows that crosslingual DS increases the accuracy of relation extraction. Our approach is language independent; we successfully apply it to four different languages: Chinese, English, French and German.

#index 1919721
#* Labeling by landscaping: classifying tokens in context by pruning and decorating trees
#@ Siddharth Patwardhan;Branimir Boguraev;Apoorv Agarwal;Alessandro Moschitti;Jennifer Chu-Carroll
#t 2012
#c 1
#% 190581
#% 464434
#% 551432
#% 798823
#% 815303
#% 816081
#% 854645
#% 938695
#% 938706
#% 938754
#% 939941
#% 940027
#% 1084592
#% 1261542
#% 1289510
#% 1338672
#% 1484322
#% 1665151
#% 1711819
#% 1880064
#% 1987072
#! State-of-the-art approaches to token labeling within text documents typically cast the problem either as a classification task, without using complex structural characteristics of the input, or as a sequential labeling task, carried out by a Conditional Random Field (CRF) classifier. Here we explore principled ways for structure to be brought to bear on the task. In line with recent trends in statistical learning of structured natural language input, we use a Support Vector Machine (SVM) classification framework deploying tree kernels. We then propose tree transformations and decorations, as a methodology for modeling complex linguistic phenomena in highly multi-dimensional feature spaces. We develop a general purpose tree engineering framework, which enables us to transcend the typically complex and laborious process of feature engineering. We build kernel based classifiers for two token labeling tasks: fine-grained event recognition, and lexical answer type detection in questions. For both, we show that in comparison with a corresponding linear kernel SVM, our method of using tree kernels improves recognition, thanks to appropriately engineering tree structures for use by the tree kernel. We also observe significant improvements when comparing with a CRF-based realization of structured prediction, itself performing at levels comparable to state-of-the-art.

#index 1919722
#* G-WSTD: a framework for geographic web search topic discovery
#@ Di Jiang;Jan Vosecky;Kenneth Wai-Ting Leung;Wilfred Ng
#t 2012
#c 1
#% 46803
#% 253188
#% 722904
#% 749907
#% 788094
#% 818256
#% 939973
#% 987250
#% 1016371
#% 1055914
#% 1077150
#% 1190103
#% 1227658
#% 1292473
#% 1355044
#% 1385969
#% 1399973
#% 1481659
#% 1536505
#% 1560379
#% 1641979
#! Search engine query log is an important information source that contains millions of users' interests and information needs. In this paper, we tackle the problem of discovering latent geographic search topics via mining search engine query logs. A novel framework G-WSTD that contains search session derivation, geographic information extraction and geographic search topic discovery is developed to support a variety of downstream web applications. The core components of the framework are two topic models, which discover geographic search topics from two different perspectives. The first one is the Discrete Search Topic Model (DSTM), which aims to capture the semantic commonalities across discrete geographic locations. The second one is the Regional Search Topic Model (RSTM), which focuses on a specific region on the map and discovers web search topics that demonstrate geographic locality. We evaluate our framework against several strong baselines on a real-life query log. The framework demonstrates improved data interpretability, better prediction performance and higher topic distinctiveness in the experimentation. The effectiveness of the framework is also verified by applications such as user profiling and URL annotation.

#index 1919723
#* Supporting factual statements with evidence from the web
#@ Chee Wee Leong;Silviu Cucerzan
#t 2012
#c 1
#% 268079
#% 754068
#% 763708
#% 869501
#% 876017
#% 1016177
#% 1074052
#% 1074112
#% 1173692
#% 1211636
#% 1227647
#% 1250278
#% 1264744
#% 1292473
#% 1299562
#% 1482284
#! Fact verification has become an important task due to the increased popularity of blogs, discussion groups, and social sites, as well as of encyclopedic collections that aggregate content from many contributors. We investigate the task of automatically retrieving supporting evidence from the Web for factual statements. Using Wikipedia as a starting point, we derive a large corpus of statements paired with supporting Web documents, which we employ further as training and test data under the assumption that the contributed references to Wikipedia represent some of the most relevant Web documents for supporting the corresponding statements. Given a factual statement, the proposed system first transforms it into a set of semantic terms by using machine learning techniques. It then employs a quasi-random strategy for selecting subsets of the semantic terms according to topical likelihood. These semantic terms are used to construct queries for retrieving Web documents via a Web search API. Finally, the retrieved documents are aggregated and re-ranked by employing additional measures of their suitability to support the factual statement. To gauge the quality of the retrieved evidence, we conduct a user study through Amazon Mechanical Turk, which shows that our system is capable of retrieving supporting Web documents comparable to those chosen by Wikipedia contributors.

#index 1919724
#* Role-explicit query identification and intent role annotation
#@ Haitao Yu;Fuji Ren
#t 2012
#c 1
#% 158687
#% 296646
#% 309124
#% 757306
#% 956503
#% 1019076
#% 1019130
#% 1055706
#% 1074112
#% 1074114
#% 1084595
#% 1130868
#% 1190103
#% 1214708
#% 1215246
#% 1227610
#% 1263586
#% 1264824
#% 1292591
#% 1301004
#% 1400023
#% 1400033
#% 1450954
#% 1456294
#% 1560364
#! Understanding the information need or intent encoded within a query has long been regarded as an essential factor of effective information retrieval. For better query representation and understanding, two intent roles (kernel-object and modifier) are introduced to structurally parse a class of role-explicit queries, which constitute a majority of common user queries. Furthermore, we focus on two research problems: RP-1: Given a role-explicit query, how to identify the kernel-object and modifier, namely intent role annotation; RP-2: How to determine whether an arbitrary query is role-explicit or not. To solve RP-1, we propose a simplified word n-gram role model (SWNR), which quantifies the generating probability of a role-explicit query and performs intent role annotation effectively. Using a set of discriminative features, we build classifiers to address RP-2 in a supervised manner. The experimental results show that: (1) SWNR can achieve a satisfactory performance, more than 73% in terms of different metrics; (2) The classifiers can achieve more than 90% precision in identifying role-explicit queries; (3) Compared with traditional techniques for query representation and understanding, e.g., name entity recognition in query and class-level query intent inference, intent role annotation provides a more flexible framework and a number of applications can benefit from annotating role-explicit queries, such as intent mining and diversified document ranking.

#index 1919725
#* Joint topic modeling for event summarization across news and social media streams
#@ Wei Gao;Peng Li;Kareem Darwish
#t 2012
#c 1
#% 280819
#% 722904
#% 769967
#% 816173
#% 939539
#% 1214645
#% 1270689
#% 1270753
#% 1272053
#% 1292504
#% 1338675
#% 1355042
#% 1392432
#% 1481542
#% 1587367
#% 1598359
#% 1828477
#! Social media streams such as Twitter are regarded as faster first-hand sources of information generated by massive users. The content diffused through this channel, although noisy, provides important complement and sometimes even a substitute to the traditional news media reporting. In this paper, we propose a novel unsupervised approach based on topic modeling to summarize trending subjects by jointly discovering the representative and complementary information from news and tweets. Our method captures the content that enriches the subject matter by reinforcing the identification of complementary sentence-tweet pairs. To valuate the complementarity of a pair, we leverage topic modeling formalism by combining a two-dimensional topic-aspect model and a cross-collection approach in the multi-document summarization literature. The final summaries are generated by co-ranking the news sentences and tweets in both sides simultaneously. Experiments give promising results as compared to state-of-the-art baselines.

#index 1919726
#* CGStream: continuous correlated graph query for data streams
#@ Shirui Pan;Xingquan Zhu
#t 2012
#c 1
#% 227919
#% 387427
#% 466476
#% 601159
#% 629708
#% 769909
#% 893372
#% 989610
#% 1070887
#% 1083718
#% 1206924
#% 1318643
#% 1426516
#% 1605979
#! In this paper, we propose to query correlated graph in a data stream scenario, where given a query graph q an algorithm is required to retrieve all the subgraphs whose Pearson's correlation coefficients with q are greater than a threshold Θ over some graph data flowing in a stream fashion. Due to the dynamic changing nature of the stream data and the inherent complexity of the graph query process, treating graph streams as static datasets is computationally infeasible or ineffective. In the paper, we propose a novel algorithm, CGStream, to identify correlated graphs from data stream, by using a sliding window which covers a number of consecutive batches of stream data records. Our theme is to regard stream query as the traversing along a data stream and the query is achieved at a number of outlooks over the data stream. For each outlook, we derive a lower frequency bound to mine a set of frequent subgraph candidates, where the lower bound guarantees that no pattern is missing from the current outlook to the next outlook. On top of that, we derive an upper correlation bound and a heuristic rule to prune the candidate size, which helps reduce the computation cost at each outlook. Experimental results demonstrate that the proposed algorithm is several times, or even an order of magnitude, more efficient than the straightforward algorithm. Meanwhile, our algorithm achieves good performance in terms of query precision.

#index 1919727
#* Efficient influence-based processing of market research queries
#@ Anastasios Arvanitis;Antonios Deligiannakis;Yannis Vassiliou
#t 2012
#c 1
#% 201876
#% 287414
#% 333854
#% 420082
#% 806212
#% 875025
#% 1022226
#% 1063485
#% 1181300
#% 1206698
#% 1206819
#% 1328118
#% 1328184
#% 1372687
#% 1523827
#% 1549863
#% 1594611
#% 1618269
#% 1847978
#! The rapid growth of social web has contributed vast amounts of user preference data. Analyzing this data and its relationships with products could have several practical applications, such as personalized advertising, market segmentation, product feature promotion etc. In this work we develop novel algorithms for efficiently processing two important classes of queries involving user preferences, i.e. potential customers identification and product positioning. With regards to the first problem, we formulate product attractiveness based on the notion of reverse skyline queries. We then present a new algorithm, termed as RSA, that significantly reduces the I/O cost, as well as the computation cost, when compared to the state-of-the-art reverse skyline algorithm, while at the same time being able to quickly report the first results. Several real-world applications require processing of a large number of queries, in order to identify the product characteristics that maximize the number of potential customers. Motivated by this problem, we also develop a batched extension of our RSA algorithm that significantly improves upon processing multiple queries individually, by grouping contiguous candidates, exploiting I/O commonalities and enabling shared processing. Our experimental study using both real and synthetic data sets demonstrates the superiority of our proposed algorithms for the studied classes of queries.

#index 1919728
#* Deco: declarative crowdsourcing
#@ Aditya Ganesh Parameswaran;Hyunjung Park;Hector Garcia-Molina;Neoklis Polyzotis;Jennifer Widom
#t 2012
#c 1
#% 13016
#% 152940
#% 273912
#% 300169
#% 378409
#% 481101
#% 481923
#% 810098
#% 1206636
#% 1217153
#% 1252609
#% 1526538
#% 1550748
#% 1560402
#% 1573506
#% 1581851
#% 1628171
#% 1770349
#% 1770351
#% 1895096
#! Crowdsourcing enables programmers to incorporate "human computation" as a building block in algorithms that cannot be fully automated, such as text analysis and image recognition. Similarly, humans can be used as a building block in data-intensive applications--providing, comparing, and verifying data used by applications. Building upon the decades-long success of declarative approaches to conventional data management, we use a similar approach for data-intensive applications that incorporate humans. Specifically, declarative queries are posed over stored relational data as well as data computed on-demand from the crowd, and the underlying system orchestrates the computation of query answers. We present Deco, a database system for declarative crowdsourcing. We describe Deco's data model, query language, and our prototype. Deco's data model was designed to be general (it can be instantiated to other proposed models), flexible (it allows methods for data cleansing and external access to be plugged in), and principled (it has a precisely-defined semantics). Syntactically, Deco's query language is a simple extension to SQL. Based on Deco's data model, we define a precise semantics for arbitrary queries involving both stored data and data obtained from the crowd. We then describe the Deco query processor which uses a novel push-pull hybrid execution model to respect the Deco semantics while coping with the unique combination of latency, monetary cost, and uncertainty introduced in the crowdsourcing environment. Finally, we experimentally explore the query processing alternatives provided by Deco using our current prototype.

#index 1919729
#* Predicting the effectiveness of keyword queries on databases
#@ Shiwen Cheng;Arash Termehchy;Vagelis Hristidis
#t 2012
#c 1
#% 397161
#% 660011
#% 818267
#% 907544
#% 944349
#% 960355
#% 960360
#% 1015325
#% 1077150
#% 1195848
#% 1206957
#% 1217198
#% 1426467
#% 1426566
#% 1450870
#% 1523859
#% 1567974
#% 1594556
#% 1622389
#% 1697426
#! Keyword query interfaces (KQIs) for databases provide easy access to data, but often suffer from low ranking quality, i.e. low precision and/or recall, as shown in recent benchmarks. It would be useful to be able to identify queries that are likely to have low ranking quality to improve the user satisfaction. For instance, the system may suggest to the user alternative queries for such hard queries. In this paper, we analyze the characteristics of hard queries and propose a novel framework to measure the degree of difficulty for a keyword query over a database, considering both the structure and the content of the database and the query results. We evaluate our query difficulty prediction model against two relevance judgment benchmarks for keyword search on databases, INEX and SemSearch. Our study shows that our model predicts the hard queries with high accuracy. Further, our prediction algorithms incur minimal time overhead.

#index 1919730
#* You can stop early with COLA: online processing of aggregate queries in the cloud
#@ Yingjie Shi;Xiaofeng Meng;Fusheng Wang;Yantao Gan
#t 2012
#c 1
#% 77967
#% 210353
#% 227883
#% 273910
#% 397370
#% 458615
#% 480953
#% 503535
#% 503719
#% 723279
#% 765424
#% 765425
#% 810055
#% 963669
#% 1328146
#% 1426584
#% 1426601
#! Cloud-based data management systems are emerging as scalable, fault-tolerant, and efficient solutions to manage large volumes of data with cost effective infrastructures, and more and more data analysis applications are migrated to the cloud. As an attractive solution to provide a quick sketch of massive data before a long wait of the final accurate query result, online processing of aggregate queries in the cloud is of paramount importance. This problem is challenging to solve because of the large block based data organization and distributed processing mode in the cloud. In this paper, we present COLA, a system for Cloud Online Aggregation to provide progressive approximate answers for both single tables and joined multiple tables. We develop an online query processing algorithm for MapReduce to support incremental and continuous computing of aggregations on joins which minimizes the waiting time before an acceptable estimate is achieved. We formulate a statistical foundation that supports block-level sampling for single-table online aggregations and effective estimation of approximate results and confidence intervals of statistical significance. We also develop a two-phase stratified sampling method to support multi-table aggregations to improve the approximate query answers and speed up the convergence of confidence intervals. We implement COLA in Hadoop, and our experiments demonstrate that COLA can deliver reasonable precise online estimates within a time period two orders of magnitude shorter than that used to produce exact answers.

#index 1919731
#* A novel local patch framework for fixing supervised learning models
#@ Yilei Wang;Bingzheng Wei;Jun Yan;Yang Hu;Zhi-Hong Deng;Zheng Chen
#t 2012
#c 1
#% 175736
#% 191257
#% 197394
#% 209021
#% 217168
#% 235377
#% 402289
#% 411762
#% 543600
#% 970964
#% 974238
#% 1232015
#% 1268491
#% 1560393
#% 1579181
#% 1632907
#% 1661934
#% 1663721
#% 1702637
#% 1776405
#! In the past decades, machine learning models, especially supervised learning algorithms, have been widely used in various real world applications. However, no matter how strong a learning model is, it will suffer from the prediction errors when it is applied to real world problems. Due to the black box nature of supervised learning models, it is a challenging problem to fix the supervised learning models by further learning from the failure cases it generates. In this paper, we propose a novel Local Patch Framework (LPF) to locally fix supervised learning models by learning from its predicted failure cases. Since the learning models are generally globally optimized during training process, our proposed LPF assumes that most of the learning errors are led by local errors in the model. Thus we aim to break the black boxes of learning models by identifying and fixing the local errors of various models automatically. The proposed LPF has two key steps, which are local error region subspace learning and local patch model learning. Through this way, we aim to fix the errors of learning models locally and automatically with certain generalization ability on unseen testing data. Experiments on both classification and ranking problems show that the proposed LPF is effective and outperforms the original algorithms and the incremental learning model.

#index 1919732
#* Automated feature weighting in naive bayes for high-dimensional data classification
#@ Lifei Chen;Shengrui Wang
#t 2012
#c 1
#% 243728
#% 246832
#% 458369
#% 464445
#% 480132
#% 799040
#% 814023
#% 945638
#% 948088
#% 961134
#% 1127359
#% 1269504
#% 1301004
#% 1558464
#% 1581100
#% 1650665
#% 1650901
#% 1673007
#% 1688520
#! Naive Bayes (NB for short) is one of the popular methods for supervised classification in a knowledge management system. Currently, in many real-world applications, high-dimensional data pose a major challenge to conventional NB classifiers, due to noisy or redundant features and local relevance of these features to classes. In this paper, an automated feature weighting solution is proposed to result in a NB method effective in dealing with high-dimensional data. We first propose a locally weighted probability model, for Bayesian modeling in high-dimensional spaces, to implement a soft feature selection scheme. Then we propose an optimization algorithm to find the weights in linear time complexity, based on the Logitnormal priori distribution and the Maximum a Posteriori principle. Experimental studies show the effectiveness and suitability of the proposed model for high-dimensional data classification.

#index 1919733
#* Learning to discover complex mappings from web forms to ontologies
#@ Yuan An;Xiaohua Hu;Il-Yeol Song
#t 2012
#c 1
#% 333990
#% 348187
#% 479783
#% 480134
#% 480645
#% 572314
#% 577319
#% 654459
#% 660001
#% 765409
#% 765410
#% 765433
#% 777930
#% 800497
#% 824763
#% 943035
#% 976685
#% 993981
#% 993982
#% 1110756
#% 1127405
#% 1292467
#% 1328136
#% 1472963
#% 1651538
#% 1655384
#% 1671608
#% 1725994
#% 1728698
#! In order to realize the Semantic Web, various structures on the Web including Web forms need to be annotated with and mapped to domain ontologies. We present a machine learning-based automatic approach for discovering complex mappings from Web forms to ontologies. A complex mapping associates a set of semantically related elements on a form to a set of semantically related elements in an ontology. Existing schema mapping solutions mainly rely on integrity constraints to infer complex schema mappings. However, it is difficult to extract rich integrity constraints from forms. We show how machine learning techniques can be used to automatically discover complex mappings between Web forms and ontologies. The challenge is how to capture and learn the complicated knowledge encoded in existing complex mappings. We develop an initial solution that takes a naive Bayesian approach. We evaluated the performance of the solution on various domains. Our experimental results show that the solution returns the expected mappings as the top-1 results usually among several hundreds candidate mappings for more than 80% of the test cases. Furthermore, the expected mappings are always returned as the top-k results with k

#index 1919734
#* Modeling semantic relations between visual attributes and object categories via dirichlet forest prior
#@ Xin Chen;Xiaohua Hu;Zhongna Zhou;Yuan An;Tingting He;E.K. Park
#t 2012
#c 1
#% 443894
#% 760805
#% 913845
#% 1074095
#% 1132493
#% 1211693
#% 1214660
#% 1482268
#% 1495441
#% 1750483
#! In this paper, we deal with two research issues: the automation of visual attribute identification and semantic relation learning between visual attributes and object categories. The contribution is two-fold, firstly, we provide uniform framework to reliably extract both categorical attributes and depictive attributes. Secondly, we incorporate the obtained semantic associations between visual attributes and object categories into a text-based topic model and extract descriptive latent topics from external textual knowledge sources. Specifically, we show that in mining natural language descriptions from external knowledge sources, the relation between semantic visual attributes and object categories can be encoded as Must-Links and Cannot-Links, which can be represented by Dirichlet-Forest prior. To alleviate the workload of manual supervision and labeling in image categorization process, we introduce a semi-supervised training framework using soft-margin semi-supervised SVM classifier. We also show that the large-scale image categorization results can be significantly improved by combining automatically acquired visual attributes. Experimental results show that the proposed model achieves better ability in describing object-related attributes and makes the inferred latent topics more descriptive.

#index 1919735
#* CoNet: feature generation for multi-view semi-supervised learning with partially observed views
#@ Brian Quanz;Jun Huan
#t 2012
#c 1
#% 252011
#% 316509
#% 466888
#% 577286
#% 832574
#% 891060
#% 891549
#% 947326
#% 961218
#% 983807
#% 983949
#% 1013616
#% 1073994
#% 1074018
#% 1100081
#% 1272126
#% 1274924
#% 1301004
#% 1386101
#% 1386117
#% 1527593
#% 1617306
#% 1653965
#% 1699580
#% 1750339
#! Multi-view semi-supervised learning methods try to exploit the combination of multiple views along with large amounts of unlabeled data in order to learn better predictive functions when limited labeled data is available. However, lack of complete view data limits the applicability of multi-view semi-supervised learning to real world data. Commonly, one data view is readily and cheaply available, but additionally views may be costly or only available in some cases. This work aims to make multi-view semi-supervised learning approaches more applicable to real world data specifically by addressing the issue of missing views. We introduce CoNet, a feature generation method that learns a mapping from one view to another that is specifically designed to produce features that are useful for multi-view semi-supervised learning algorithms. The mapping is then used to fill in views as pre-processing. Our comprehensive experimental study demonstrates the utility of our method as compared to the state-of-the-art multi-view semi-supervised learning methods for this scenario of partially observed views.

#index 1919736
#* Generating facets for phone-based navigation of structured data
#@ Krishna Kummamuru;Ajith Jujjuru;Mayuri Duggirala
#t 2012
#c 1
#% 318028
#% 420084
#% 838540
#% 976989
#% 1035573
#% 1130808
#% 1206827
#% 1269148
#% 1425707
#% 1482281
#! Designing interactive voice systems that have optimum cognitive load on callers has been an active research topic for quite some time. There have been many studies comparing the user preferences on navigation trees with higher depths over higher breadths. In this paper, we consider the navigation of structured data containing various types of attributes using phone-based interactions. This problem is particularly relevant to emerging economies in which innovative voice-based applications are being built to address semi-literate population. We address the problem of identifying the right sequence of facets to be presented to the user for phone-based navigation of the data in two stages. Firstly, we perform extensive user studies in the target population to understand the relation between the nature of facets (attributes) of the data and the cognitive load. Secondly, we propose an algorithm to design optimum navigation trees based on the inferences made in the first phase. We compare the proposed algorithm with the traditional facet generation algorithms with respect to various factors and discuss the optimality of the proposed algorithm.

#index 1919737
#* The effect of aggregated search coherence on search behavior
#@ Jaime Arguello;Robert Capra
#t 2012
#c 1
#% 262112
#% 1073970
#% 1074093
#% 1074113
#% 1166473
#% 1166523
#% 1227616
#% 1227617
#% 1227620
#% 1263586
#% 1292528
#% 1450915
#% 1455256
#% 1482230
#% 1536510
#% 1536576
#% 1560361
#% 1587348
#% 1641937
#! Aggregated search is the task of blending results from different specialized search services, or verticals, into the web search results. Aggregated search coherence refers to the degree to which results from different systems focus on similar senses of the query. While cross-component coherence has been cited as an important criterion for whole-page evaluation, its effect on search behavior has not been deeply investigated in prior research. In this work, we focus on the coherence between two aggregated search components: images and web results. In particular, we investigate whether the query-senses associated with the blended image results can influence user interaction with the web results. For example, if a user wants web results about "jaguar" the animal, are they more likely to examine the web results if the image results contain pictures of the animal instead of pictures of the car? Based on two large user studies, our results show that the image results can systematically affect user interaction with the web results. If the web results are largely consistent with the search task, then the effect of the image results is small. However, if the web results are only marginally consistent with the search task, such as when they are highly diversified across query-senses, the image results have a significant effect on user interaction with the web results. Our findings have implications on current research in whole-page evaluation, aggregated search, and diversity ranking.

#index 1919738
#* Improving bag-of-visual-words model with spatial-temporal correlation for video retrieval
#@ Lei Wang;Dawei Song;Eyad Elyan
#t 2012
#c 1
#% 428926
#% 724174
#% 755467
#% 760805
#% 836690
#% 853308
#% 883971
#% 883972
#% 884118
#% 884155
#% 903632
#% 990328
#% 1040539
#% 1058303
#% 1085939
#% 1131856
#% 1169843
#% 1169844
#% 1279775
#% 1298139
#% 1298146
#% 1338304
#% 1354888
#% 1598384
#% 1677683
#% 1750327
#% 1775873
#% 1828387
#! Most of the state-of-art approaches to Query-by-Example (QBE) video retrieval are based on the Bag-of-visual-Words (BovW) representation of visual content. It, however, ignores the spatial-temporal information, which is important for similarity measurement between videos. Direct incorporation of such information into the video data representation for a large scale data set is computationally expensive in terms of storage and similarity measurement. It is also static regardless of the change of discriminative power of visual words for different queries. To tackle these limitations, in this paper, we propose to discover Spatial-Temporal Correlations (STC) imposed by the query example to improve the BovW model for video retrieval. The STC, in terms of spatial proximity and relative motion coherence between different visual words, is crucial to identify the discriminative power of the visual words. We develop a novel technique to emphasize the most discriminative visual words for similarity measurement, and incorporate this STC-based approach into the standard inverted index architecture. Our approach is evaluated on the TRECVID2002 and CC\_WEB\_VIDEO datasets for two typical QBE video retrieval tasks respectively. The experimental results demonstrate that it substantially improves the BovW model as well as a state of the art method that also utilizes spatial-temporal information for QBE video retrieval.

#index 1919739
#* Exploring and predicting search task difficulty
#@ Jingjing Liu;Chang Liu;Michael Cole;Nicholas J. Belkin;Xiangmin Zhang
#t 2012
#c 1
#% 397161
#% 805200
#% 860649
#% 879613
#% 907516
#% 1126944
#% 1156208
#% 1156210
#% 1292474
#% 1384094
#% 1434128
#% 1450833
#% 1450994
#% 1523400
#% 1598334
#% 1598368
#% 1598515
#% 1697426
#! We report on an investigation of behavioral differences between users in difficult and easy search tasks. Behavioral factors that can be used in real-time to predict task difficulty are identified. User data was collected in a controlled lab experiment (n=38) where each participant completed four search tasks in the genomics domain. We looked at user behaviors that can be obtained by systems at three levels, distinguished by the time point when the measurements can be done. They are: 1) first-round level at the beginning of the search, 2) accumulated level during the search, and 3) whole-session level by the end of the search. Results show that a number of user behaviors at all three levels differed between easy and difficult tasks. Models predicting task difficulty at all three levels were developed and evaluated. A real-time model incorporating first-round and accumulated levels of behaviors (FA) had fairly good prediction performance (accuracy 83%; precision 88%), which is comparable with the model using the whole-session level behaviors which are not real-time (accuracy 75%; precision 92%). We also found that for efficiency purpose, using only a limited number of significant variables (FC_FA) can obtain a prediction accuracy of 75%, with a precision of 88%. Our findings can help search systems predict task difficulty and adapt search results to users.

#index 1919740
#* Iterative relevance feedback with adaptive exploration/exploitation trade-off
#@ Nicolae Suditu;François Fleuret
#t 2012
#c 1
#% 219847
#% 318785
#% 341355
#% 437405
#% 479788
#% 522284
#% 760805
#% 1081621
#% 1209697
#% 1457039
#% 1702722
#% 1854912
#% 1857498
#% 1885671
#! Content-based image retrieval systems have to cope with two different regimes: understanding broadly the categories of interest to the user, and refining the search in this or these categories to converge to specific images among them. Here, in contrast with other types of retrieval systems, these two regimes are of great importance since the search initialization is hardly optimal (i.e. the page-zero problem) and the relevance feedback must tolerate the semantic gap of the image's visual features. We present a new approach that encompasses these two regimes, and infers from the user actions a seamless transition between them. Starting from a query-free approach meant to solve the page-zero problem, we propose an adaptive exploration/exploitation trade-off that transforms the original framework into a versatile retrieval framework with full searching capabilities. Our approach is compared to the state-of-the-art it extends by conducting user evaluations on a collection of 60,000 images from the ImageNet database.

#index 1919741
#* A practical concurrent index for solid-state drives
#@ Risi Thonangi;Shivnath Babu;Jun Yang
#t 2012
#c 1
#% 208047
#% 479470
#% 570884
#% 659954
#% 951778
#% 1063551
#% 1092670
#% 1328139
#% 1447927
#% 1523901
#% 1581848
#% 1625031
#% 1668634
#% 1770337
#! Solid-state drives are becoming a viable alternative to magnetic disks in database systems, but their performance characteristics, particularly those caused by their erase-before-write behavior, make conventional database indexes a poor fit. There have been various proposals of indexes specialized for these devices, but to make such indexes practical, we must address the issue of concurrency control. Good concurrency control is especially critical to indexes on solid-state drives, because they typically rely on batch updates, which may take long and block concurrent index accesses. We design, implement, and evaluate an index structure called FD+tree and an associated concurrency control scheme called FD+FC. Our evaluation confirms significant performance advantages of our approach over less sophisticated ones, and brings ou insights on data structure design and OLTP performance tuning on solid-state drives.

#index 1919742
#* Robust distributed indexing for locality-skewed workloads
#@ Mu-Woong Lee;Seung-won Hwang
#t 2012
#c 1
#% 86950
#% 201876
#% 232771
#% 252304
#% 321455
#% 340175
#% 340176
#% 340298
#% 401599
#% 427199
#% 460871
#% 462059
#% 505869
#% 824706
#% 998845
#% 1047894
#% 1063489
#% 1127398
#% 1217207
#% 1426551
#% 1523902
#! Multidimensional indexing is crucial for enabling a fast search over large-scale data. Owing to the unprecedented scale of data, extending such indexing technology has recently gained attention in distributed environments. The goal of existing efforts in distributed indexing has been the localization of queries to data residing at a small number of nodes (i.e., locality-preserving indexing) to minimize communication cost. However, considering that workloads often correlate with data locality, such indexing often generates hotspots. Location-based queries are typically skewed to disaster areas during certain periods of time, e.g., during Hurricane Irene, search traffic increased by more than 2000%. To alleviate such hotspots, we propose workload-balancing as an optimization goal. A cost model analytically supporting the need for load balancing is first developed, then a distributed index that evenly distributes the workload is presented. Our empirical study suggests that hotspots degrading search performance can be effectively alleviated. Specifically, when deployed to Amazon EC2, our proposed scheme showed maximum speed-up of 127.7%. Even in hostile settings where workload is not at all correlated with the search criteria, the proposed scheme's performance is comparable to existing approaches optimized for such settings.

#index 1919743
#* Efficient provenance storage for relational queries
#@ Zhifeng Bao;Henning Köhler;Liwei Wang;Xiaofang Zhou;Shazia Sadiq
#t 2012
#c 1
#% 462072
#% 464891
#% 577523
#% 803468
#% 875015
#% 893167
#% 960267
#% 976987
#% 1016179
#% 1016204
#% 1022258
#% 1063544
#% 1206861
#% 1231247
#% 1426581
#% 1581830
#% 1818427
#! Provenance information is vital in many application areas as it helps explain data lineage and derivation. However, storing fine-grained provenance information can be expensive. In this paper, we present a framework for storing provenance information relating to data derived via database queries. In particular, we first propose a provenance tree data structure which matches the query structure and thereby presents a possibility to avoid redundant storage of information regarding the derivation process. Then we investigate two approaches for reducing storage costs. The first approach utilizes two ingenious rules to achieve reduction on provenance trees. The second one is a dynamic programming solution, which provides a way of optimizing the selection of query tree nodes where provenance information should be stored. The optimization algorithm runs in polynomial time in the query size and is linear in the size of the provenance information, thus enabling provenance tracking and optimization without incurring large overheads. Experiments show that our approaches guarantee significantly lower storage costs than existing approaches.

#index 1919744
#* Generically extending anonymization algorithms to deal with successive queries
#@ Manuel Barbosa;Alexandre Pinto;Bruno Gomes
#t 2012
#c 1
#% 576762
#% 800514
#% 800515
#% 864406
#% 874988
#% 881497
#% 881551
#% 937550
#% 960291
#% 982549
#% 1044457
#% 1080356
#% 1083653
#% 1105808
#% 1176939
#% 1181950
#% 1372735
#% 1446819
#% 1725659
#! This paper addresses the scenario of multi-release anonymization of datasets. We consider dynamic datasets where data can be inserted and deleted, and view this scenario as a case where each release is a small subset of the dataset corresponding, for example, to the results of a query. Compared to multiple releases of the full database, this has the obvious advantage of faster anonymization. We present an algorithm for post-processing anonymized queries that prevents anonymity attacks using multiple released queries. This algorithm can be used with several distinct protection principles and anonymization algorithms, which makes it generic and flexible. We give an experimental evaluation of the algorithm and compare it to $m$-invariance both in terms of efficiency and data quality. To this end, we propose two data quality metrics based on Shannon's entropy, and show that they can be seen as a refinement of existing metrics.

#index 1919745
#* Authentication of moving range queries
#@ Duncan Yung;Eric Lo;Man Lung Yiu
#t 2012
#c 1
#% 86950
#% 319994
#% 333947
#% 513367
#% 527187
#% 654478
#% 765438
#% 803119
#% 810042
#% 824701
#% 870307
#% 871761
#% 874980
#% 940830
#% 993955
#% 1015321
#% 1022213
#% 1022214
#% 1127362
#% 1127363
#% 1127438
#% 1201868
#% 1211647
#% 1217147
#% 1223423
#% 1230820
#% 1328176
#% 1372733
#% 1480809
#% 1594676
#% 1595892
#! A moving range query continuously reports the query result (e.g., restaurants) that are within radius $r$ from a moving query point (e.g., moving tourist). To minimize the communication cost with the mobile clients, a service provider that evaluates moving range queries also returns a safe region that bounds the validity of query results. However, an untrustworthy service provider may report incorrect safe regions to mobile clients. In this paper, we present efficient techniques for authenticating the safe regions of moving range queries. We theoretically proved that our methods for authenticating moving range queries can minimize the data sent between the service provider and the mobile clients. Extensive experiments are carried out using both real and synthetic datasets and results show that our methods incur small communication costs and overhead.

#index 1919746
#* Proceedings of the 21st ACM international conference on Information and knowledge management
#@ Xuewen Chen;Guy Lebanon;Haixun Wang;Mohammed J. Zaki
#t 2012
#c 1
#! On behalf of the organizing committee, it is my genuine honor and great pleasure to welcome you to the 21st ACM International Conference on Information and Knowledge Management (CIKM 2012) in Maui, Hawaii! I hope this conference proves to be both interesting and beneficial. Since its inception, the CIKM conference has provided a unique international forum for the presentation, discussion and dissemination of research findings in data management, information retrieval and knowledge management. The purpose of the conference is to identify challenging problems facing the development of future knowledge and information systems and to shape future research directions though the publication of high quality, applied and theoretical research findings. The conference has been a leading forum in which experts from academic, industry and the public sector gather to exchange ideas, research achievements and technical developments in multidisciplinary research areas. As one of the world's most recognized conferences in the field, this year's CIKM conference has received a record high number of submissions in the history of CIKM, as can be seen from the following statistics: 1492 abstracts submitted 1088 full papers, 229 posters, and 70 demo papers submitted 146 papers accepted for presentation as full papers (13.4% acceptance rate) and 157 papers were accepted for short papers (27.8% cumulative acceptance rate) The increased number of submissions alone is a great demonstration of the lively research areas that contribute to the CIKM area. In addition, CIKM 2012 will host 15 workshops on cutting-edge areas of research and a dedicated Industry Event featuring leading industrial practitioners. We are grateful to all authors who chose to submit their work to CIKM 2012 and are very excited by the final program. CIKM values interdisciplinary research and we are proud to present three keynote speakers for the main conference (Dr. Ricardo Baeza-yates, Prof. William Cohen, and Prof. Jeffrey S. Vitter) and four keynote speakers for the Industry Event (Drs. Eric Brill, Raghu Ramakrishnan, Tom Malloy, and Xuedong Huang), all of whom will give presentations that cross discipline boundaries. I deeply appreciate their time and commitment to deliver their speeches and share their cutting-edge research experiences and insightful comments in their research topics.

#index 1919747
#* Model the complex dependence structures of financial variables by using canonical vine
#@ Wei Wei;Xuhui Fan;Jinyan Li;Longbing Cao
#t 2012
#c 1
#% 1451147
#! Financial variables such as asset returns in the massive market contain various hierarchical and horizontal relationships forming complicated dependence structures. Modeling and mining of these structures is challenging due to their own high structural complexities as well as the stylized facts of the market data. This paper introduces a new canonical vine dependence model to identify the asymmetric and non-linear dependence structures of asset returns without any prior independence assumptions. To simplify the model while maintaining its merit, a partial correlation based method is proposed to optimize the canonical vine. Compared with the original canonical vine, the new model can still maintain the most important dependence but many unimportant nodes are removed to simplify the canonical vine structure. Our model is applied to construct and analyze dependence structures of European stocks as case studies. Its performance is evaluated by measuring portfolio of Value at Risk, a widely used risk management measure. In comparison to a very recent canonical vine model and the 'full' model, our experimental results demonstrate that our model has a much better quality of Value at Risk, providing insightful knowledge for investors to control and reduce the aggregation risk of the portfolio.

#index 1919748
#* A unified learning framework for auto face annotation by mining web facial images
#@ Dayong Wang;Steven Chu Hong Hoi;Ying He
#t 2012
#c 1
#% 156421
#% 235342
#% 251145
#% 434882
#% 457912
#% 729344
#% 769908
#% 780804
#% 884043
#% 884044
#% 905209
#% 913845
#% 954944
#% 961218
#% 975105
#% 997184
#% 1038781
#% 1091874
#% 1149120
#% 1164188
#% 1176935
#% 1378408
#% 1484400
#% 1495448
#% 1502510
#% 1505154
#% 1562701
#% 1598387
#% 1606361
#% 1649049
#% 1694055
#% 1775706
#% 1775920
#% 1860946
#! Auto face annotation plays an important role in many real-world multimedia information and knowledge management systems. Recently there is a surge of research interests in mining weakly-labeled facial images on the internet to tackle this long-standing research challenge in computer vision and image understanding. In this paper, we present a novel unified learning framework for face annotation by mining weakly labeled web facial images through interdisciplinary efforts of combining sparse feature representation, content-based image retrieval, transductive learning and inductive learning techniques. In particular, we first introduce a new search-based face annotation paradigm using transductive learning, and then propose an effective inductive learning scheme for training classification-based annotators from weakly labeled facial images, and finally unify both transductive and inductive learning approaches to maximize the learning efficacy. We conduct extensive experiments on a real-world web facial image database, in which encouraging results show that the proposed unified learning scheme outperforms the state-of-the-art approaches.

#index 1919749
#* Efficient jaccard-based diversity analysis of large document collections
#@ Fan Deng;Stefan Siersdorfer;Sergej Zerr
#t 2012
#c 1
#% 249321
#% 311808
#% 314829
#% 347225
#% 378388
#% 387427
#% 494667
#% 544011
#% 616528
#% 722802
#% 762054
#% 763708
#% 805841
#% 907522
#% 1077150
#% 1190093
#% 1206662
#% 1385837
#% 1400011
#% 1598392
#% 1697438
#! We propose two efficient algorithms for exploring topic diversity in large document corpora such as user generated content on the social web, bibliographic data, or other web repositories. Analyzing diversity is useful for obtaining insights into knowledge evolution, trends, periodicities, and topic heterogeneity of such collections. Calculating diversity statistics requires averaging over the similarity of all object pairs, which, for large corpora, is prohibitive from a computational point of view. Our proposed algorithms overcome the quadratic complexity of the average pair-wise similarity computation, and allow for constant time (depending on dataset properties) or linear time approximation with probabilistic guarantees. We show examples of diversity-based studies on large samples from corpora such as the social photo sharing site Flickr, the DBLP bibliography, and US Census data.

#index 1919750
#* Knowing where and how criminal organizations operate using web content
#@ Michele Coscia;Viridiana Rios
#t 2012
#c 1
#% 939842
#% 1480887
#% 1482450
#% 1584316
#% 1635162
#% 1806137
#! We develop a framework that uses Web content to obtain quantitative information about a phenomenon that would otherwise require the operation of large scale, expensive intelligence exercises. Exploiting indexed reliable sources such as online newspapers and blogs, we use unambiguous query terms to characterize a complex evolving phenomena and solve a security policy problem: identifying the areas of operation and modus operandi of criminal organizations, in particular, Mexican drug trafficking organizations over the last two decades. We validate our methodology by comparing information that is known with certainty with the one we extracted using our framework. We show that our framework is able to use information available on the web to efficiently extract implicit knowledge about criminal organizations. In the scenario of Mexican drug trafficking, our findings provide evidence that criminal organizations are more strategic and operate in more differentiated ways than current academic literature thought.

#index 1919751
#* Social recommendation across multiple relational domains
#@ Meng Jiang;Peng Cui;Fei Wang;Qiang Yang;Wenwu Zhu;Shiqiang Yang
#t 2012
#c 1
#% 330687
#% 643009
#% 734594
#% 823328
#% 915227
#% 915344
#% 1001279
#% 1083696
#% 1130901
#% 1190129
#% 1214661
#% 1214703
#% 1227601
#% 1260273
#% 1275197
#% 1292542
#% 1464068
#% 1482198
#% 1482274
#% 1536533
#% 1598352
#% 1621481
#% 1641995
#% 1669913
#% 1775905
#! Social networks enable users to create different types of personal items. In dealing with serious information overload, the major problems of social recommendation are sparsity and cold start. In existing approaches, relational and heterogeneous domains can not be effectively utilized for social recommendation, which brings a challenge to model users and multiple types of items together on social networks. In this paper, we consider how to represent social networks with multiple relational domains and alleviate the major problems in an individual domain by transferring knowledge from other domains. We propose a novel Hybrid Random Walk (HRW), which can integrate multiple heterogeneous domains including directed/undirected links, signed/unsigned links and within-domain/cross-domain links into a star-structured hybrid graph with user graph at the center. We perform random walk until convergence and use the steady state distribution for recommendation. We conduct experiments on a real social network dataset and show that our method can significantly outperform existing social recommendation approaches.

#index 1919752
#* Mining competitive relationships by learning across heterogeneous networks
#@ Yang Yang;Jie Tang;Jacklyne Keomany;Yanting Zhao;Juanzi Li;Ying Ding;Tian Li;Liangwei Wang
#t 2012
#c 1
#% 280819
#% 342605
#% 722904
#% 823382
#% 840967
#% 869509
#% 915344
#% 967552
#% 989646
#% 1055681
#% 1117697
#% 1355042
#% 1399992
#% 1399997
#% 1426611
#% 1451245
#% 1471245
#% 1558464
#% 1592092
#% 1617365
#% 1642046
#% 1650318
#% 1688485
#% 1693935
#% 1810385
#% 1872393
#! Detecting and monitoring competitors is fundamental to a company to stay ahead in the global market. Existing studies mainly focus on mining competitive relationships within a single data source, while competing information is usually distributed in multiple networks. How to discover the underlying patterns and utilize the heterogeneous knowledge to avoid biased aspects in this issue is a challenging problem. In this paper, we study the problem of mining competitive relationships by learning across heterogeneous networks. We use Twitter and patent records as our data sources and statistically study the patterns behind the competitive relationships. We find that the two networks exhibit different but complementary patterns of competitions. Our proposed model, Topical Factor Graph Model (TFGM), defines a latent topic layer to bridge the two networks and learns a semi-supervised learning model to classify the relationships between entities (e.g., companies or products). We test the proposed model on two real data sets and the experimental results validate the effectiveness of our model, with an average of +46\% improvement over alternative methods.

#index 1919753
#* Evaluating geo-social influence in location-based social networks
#@ Chao Zhang;Lidan Shou;Ke Chen;Gang Chen;Yijun Bei
#t 2012
#c 1
#% 268079
#% 290830
#% 333854
#% 577329
#% 730089
#% 1035589
#% 1055737
#% 1073984
#% 1083624
#% 1130854
#% 1214702
#% 1270186
#% 1300556
#% 1399993
#% 1536509
#% 1581924
#% 1598366
#% 1606045
#% 1606049
#! The emerging location-based social network (LBSN) services not only allow people to maintain cyber links with their friends, but also enable them to share the events happening on them at different locations. The geo-social correlations among event participants make it possible to quantify mutual user influence for various events. Such a quantification of influence could benefit a wide spectrum of real-life applications such as targeted advertising and viral marketing. In this paper, we perform an in-depth analysis of the geo-social correlations among LBSN users at event level, based on which we address two problems: user influence evaluation and influential events discovery. To capture the geo-social closeness between LBSN users, we propose a unified influence metric. This metric combines a novel social proximity measure named penalized hitting time, with a geographical weight function modeled by power law distribution. We propose two approximate algorithms, namely global iteration (GI) and dynamic neighborhood expansion (DNE), to efficiently evaluate user influence with tight theoretical error bounds. We then adopt the sampling technique and the threshold algorithm to support efficient retrieval of top-K influential events. Extensive experiments on both real-life and synthetic LBSN data sets confirm that the proposed algorithms are effective, efficient, and scalable.

#index 1919754
#* The walls have ears: optimize sharing for visibility and privacy in online social networks
#@ Thang N. Dinh;Yilin Shen;My T. Thai
#t 2012
#c 1
#% 656708
#% 729923
#% 754107
#% 1083624
#% 1153832
#% 1190127
#% 1201036
#% 1214702
#% 1246431
#% 1355040
#% 1399992
#% 1443107
#% 1449326
#% 1451243
#% 1676017
#% 1765902
#% 1846473
#! With a rapid expansion of online social networks (OSNs), millions of users are tweeting and sharing their personal status daily without being aware of where that information eventually travels to. Likewise, with a huge magnitude of data available on OSNs, it poses a substantial challenge to track how a piece of information leaks to specific targets. In this paper, we study the problem of smartly sharing information to control the propagation of sensitive information in OSNs. In particular, we formulate and investigate the Maximum Circle of Trust problem of which we seek to construct a circle of trust on the fly so that OSN users can safely share their information knowing that it will not be propagated to their unwanted targets (whom they are not willing to share with). Since most of messages in OSNs are propagated within 2 to 5 hops, we first investigate this problem under 2-hop information propagation by showing the hardness of obtaining an optimal solution, along with an algorithm with proven performance guarantee. In a general case where information can be propagated more than two hops, the problem is #P-hard i.e. the problem cannot be solved in a polynomial time. Thus we propose a novel greedy algorithm, hybridizing the handy but costly sampling method with a novel cut-based estimation. The quality of the hybrid algorithm is comparable to that of the sampling method while taking only a tiny fraction of the time. We have validated the effectiveness of our solutions in many real-world traces. Such an extensive experiment also highlights several important observations on information leakage which help to sharpen the security of OSNs in the future.

#index 1919755
#* Influence and similarity on heterogeneous networks
#@ Guan Wang;Qingbo Hu;Philip S. Yu
#t 2012
#c 1
#% 729923
#% 1083734
#% 1214641
#% 1292521
#% 1355040
#! In the social network research, the studies on social influence maximization and entity similarity are two important and orthogonal tasks. On homogeneous networks, social influence maximization research tries to identify an initial influential set that maximizes the spread of the information, while similarity studies focus on designing meaningful ways to quantify entities' similarities. When heterogeneous networks are becoming ubiquitous and entities of different types are related to each other, we observe the possibility of merging the two directions together to improve the performance for both of them. In fact, we found that influence values among one type of nodes and similarity scores among the other type of nodes reinforce each other towards better and more meaningful results. Therefore, we introduce a framework that computes social influence for one type of nodes and simultaneously measures similarity of the other type of nodes in a heterogeneous network. First, we decouple the target heterogeneous network (or we call it Influence Similarity (IS) network) into three different parts: Influence network, Similarity network and information tunnels (IT) between them. Through IT, we exchange the influence scores and the similarity scores to calculate more precise similarity and influence scores in order to improve both of their qualities. The experiment results on real world data shows that our framework enables influence maximization framework to identify more influential seeds in Influence network and similarity measures to produce more meaningful similarity scores in Similarity network simultaneously.

#index 1919756
#* GRAFT: an approximate graphlet counting algorithm for large graph analysis
#@ Mahmudur Rahman;Mansurul Bhuiyan;Mohammad Al Hasan
#t 2012
#c 1
#% 283833
#% 823342
#% 937071
#% 1214705
#% 1563212
#! Graphlet frequency distribution (GFD) is an analysis tool for understanding the variance of local structure in a graph. Many recent works use GFD for comparing, and characterizing real-life networks. However, the main bottleneck for graph analysis using GFD is the excessive computation cost for obtaining the frequency of each of the graphlets in a large network. To overcome this, we propose a simple, yet powerful algorithm, called GRAFT, that obtains the approximate graphlet frequency for all graphlets that have upto 5 vertices. Comparing to an exact counting algorithm, our algorithm achieves a speedup factor between 10 and 100 for a negligible counting error, which is, on average, less than 5%; For example, exact graphlet counting for ca-AstroPh takes approximately 3 days; but, GRAFT runs for 45 minutes to perform the same task with a counting accuracy of 95.6%.

#index 1919757
#* Hierarchical co-clustering based on entropy splitting
#@ Wei Cheng;Xiang Zhang;Feng Pan;Wei Wang
#t 2012
#c 1
#% 309128
#% 342621
#% 458652
#% 722934
#% 729918
#% 769928
#% 823396
#% 879602
#% 1131160
#% 1268070
#% 1407020
#% 1451002
#! Two dimensional contingency tables or co-occurrence matrices arise frequently in various important applications such as text analysis and web-log mining. As a fundamental research topic, co-clustering aims to generate a meaningful partition of the contingency table to reveal hidden relationships between rows and columns. Traditional co-clustering algorithms usually produce a predefined number of flat partition of both rows and columns, which do not reveal relationship among clusters. To address this limitation, hierarchical co-clustering algorithms have attracted a lot of research interests recently. Although successful in various applications, the existing hierarchial co-clustering algorithms are usually based on certain heuristics and do not have solid theoretical background. In this paper, we present a new co-clustering algorithm with solid information theoretic background. It simultaneously constructs a hierarchical structure of both row and column clusters which retains sufficient mutual information between rows and columns of the contingency table. An efficient and effective greedy algorithm is developed which grows a co-cluster hierarchy by successively performing row-wise or column-wise splits that lead to the maximal mutual information gain. Extensive experiments on real datasets demonstrate that our algorithm can reveal essential relationships of row (and column) clusters and has better clustering precision than existing algorithms.

#index 1919758
#* Mining long-lasting exploratory user interests from search history
#@ Bin Tan;Yuanhua Lv;ChengXiang Zhai
#t 2012
#c 1
#% 169781
#% 262096
#% 342707
#% 397133
#% 420529
#% 577224
#% 731615
#% 818207
#% 818259
#% 823348
#% 878624
#% 879567
#% 881540
#% 987203
#% 1269723
#% 1560360
#% 1598334
#! A user's web search history contains many valuable search patterns. In this paper, we study search patterns that represent a user's long-lasting and exploratory search interests. By focusing on long-lastingness and exploratoriness, we are able to discover search patterns that are most useful for recommending new and relevant information to the user. Our approach is based on language modeling and clustering, and specifically designed to handle web search logs. We run our algorithm on a real web search log collection, and evaluate its performance using a novel simulated study on the same search log dataset. Experiment results support our hypothesis that long-lastingness and exploratoriness are necessary for generating successful recommendation. Our algorithm is shown to effectively discover such search interest patterns, and thus directly useful for making recommendation based on personal search history.

#index 1919759
#* Feature selection based on term frequency and T-test for text categorization
#@ Deqing Wang;Hui Zhang;Rui Liu;Weifeng Lv
#t 2012
#c 1
#% 46803
#% 197394
#% 344447
#% 465747
#% 465754
#% 466266
#% 478128
#% 722929
#% 722935
#% 740900
#% 1330515
#% 1599036
#! Much work has been done on feature selection. Existing methods are based on document frequency, such as Chi-Square Statistic, Information Gain etc. However, these methods have two shortcomings: one is that they are not reliable for low-frequency terms, and the other is that they only count whether one term occurs in a document and ignore the term frequency. Actually, high-frequency terms within a specific category are often regards as discriminators. This paper focuses on how to construct the feature selection function based on term frequency, and proposes a new approach based on t-test, which is used to measure the diversity of the distributions of a term between the specific category and the entire corpus. Extensive comparative experiments on two text corpora using three classifiers show that our new approach is comparable to or or slightly better than the state-of-the-art feature selection methods (i.e., chi2, and IG) in terms of macro-F1 and micro-F1

#index 1919760
#* Adapting vector space model to ranking-based collaborative filtering
#@ Shuaiqiang Wang;Jiankai Sun;Byron J. Gao;Jun Ma
#t 2012
#c 1
#% 124004
#% 387427
#% 411762
#% 1074061
#% 1272396
#% 1292542
#% 1358747
#% 1476486
#% 1560178
#% 1567974
#% 1650569
#! Collaborative filtering (CF) is an effective technique addressing the information overload problem. Recently ranking-based CF methods have shown advantages in recommendation accuracy, being able to capture the preference similarity between users even if their rating scores differ significantly. In this study, we seek accuracy improvement of ranking-based CF through adaptation of the vector space model, where we consider each user as a document and her pairwise relative preferences as terms. We then use a novel degree-specialty weighting scheme resembling TF-IDF to weight the terms. Then we use cosine similarity to select a neighborhood of users for the target user to make recommendations. Experiments on benchmarks in comparison with the state-of-the-art methods demonstrate the promise of our approach.

#index 1919761
#* Joint relevance and answer quality learning for question routing in community QA
#@ Guangyou Zhou;Kang Liu;Jun Zhao
#t 2012
#c 1
#% 290830
#% 340948
#% 722904
#% 838398
#% 879593
#% 881498
#% 1019165
#% 1035587
#% 1074110
#% 1130900
#% 1166519
#% 1190060
#% 1207005
#% 1399976
#% 1482384
#% 1491834
#% 1591994
#% 1642174
#% 1761010
#! Community question answering (cQA) has become a popular service for users to ask and answer questions. In recent years, the efficiency of cQA service is hindered by a sharp increase of questions in the community. This paper is concerned with the problem of question routing. Question routing in cQA aims to route new questions to the eligible answerers who can give high quality answers. However, the traditional methods suffer from the following two problems: (1) word mismatch between the new questions and the users' answering history; (2) high variance in perceived answer quality. To solve the above two problems, this paper proposes a novel joint learning method by taking both word mismatch and answer quality into a unified framework for question routing. We conduct experiments on large-scale real world data set from Yahoo! Answers. Experimental results show that our proposed method significantly outperforms the traditional query likelihood language model (QLLM) as well as state-of-the-art cluster-based language model (CBLM) and category-sensitive query likelihood language model (TCSLM).

#index 1919762
#* Fast approximation of steiner trees in large graphs
#@ Andrey Gubichev;Thomas Neumann
#t 2012
#c 1
#% 338382
#% 660011
#% 824693
#% 960259
#% 1002007
#% 1063537
#% 1206817
#% 1355056
#% 1399997
#% 1482228
#! Finding the minimum connected subtree of a graph that contains a given set of nodes (i.e., the Steiner tree problem) is a fundamental operation in keyword search in graphs, yet it is known to be NP-hard. Existing approximation techniques either make use of the heavy indexing of the graph, or entirely rely on online heuristics. In this paper we bridge the gap between these two extremes and present a scalable landmark-based index structure that, combined with a few lightweight online heuristics, yields a fast and accurate approximation of the Steiner tree. Our solution handles real-world graphs with millions of nodes and provides an approximation error of less than 5% on average.

#index 1919763
#* Automatically embedding newsworthy links to articles
#@ Hakan Ceylan;Ioannis Arapakis;Pinar Donmez;Mounia Lalmas
#t 2012
#c 1
#% 1019082
#% 1074394
#% 1130858
#% 1292682
#% 1353991
#% 1450987
#% 1624266
#% 1642135
#! It is of great interest to news providers such as Yahoo! News to attain higher visitor rates by promoting greater engagement with their content. One aspect of engagement deals with keeping users on the site longer by allowing them to navigate through content with enhanced, click-through experiences. News portals have invested in ways to provide embedded links within news stories. So far these links have been manually curated by professional editors, and due to the manual effort involved, the use of such links has been limited. In this paper we propose an automated approach to detecting and linking newsworthy events to associated articles. Our analysis, conducted on Amazon's Mechanical Turk, reveals that our system's performance is comparable to that of professional editors, and that users find the automatically generated highlights interesting and the associated articles worthy of reading.

#index 1919764
#* Learning spectral embedding via iterative eigenvalue thresholding
#@ Fanhua Shang;L. C. Jiao;Yuanyuan Liu;Fei Wang
#t 2012
#c 1
#% 313959
#% 466890
#% 1156095
#% 1279294
#% 1309918
#% 1441070
#% 1442717
#% 1451196
#% 1504249
#% 1535296
#% 1581622
#% 1688549
#! Learning data representation is a fundamental problem in data mining and machine learning. Spectral embedding is one popular method for learning effective data representations. In this paper we propose a novel framework to learn enhanced spectral embedding, which not only considers the geometrical structure of the data space, but also takes advantage of the given pairwise constraints. The proposed formulation can be solved by an iterative eigenvalue thresholding (IET) algorithm. Specially, we convert the problem of learning spectral embedding with pairwise constraints into the one of completing an "ideal" kernel matrix. And we introduce the spectral embedding of graph Laplacian as the auxiliary information and cast it as a small-scale positive semidefinite (PSD) matrix optimization problem with nuclear norm regularization. Then, we develop an IET algorithm to solve it efficiently. Moreover, we also present an effective semi-supervised clustering (SSC) approach with learned spectral embedding (LSE). Finally, we validate the proposed IET algorithm and LSE approach by extensive experiments on real-world data sets.

#index 1919765
#* Measuring robustness of complex networks under MVC attack
#@ Rong-Hua Li;Jeffrey Xu Yu;Xin Huang;Hong Cheng;Zechao Shang
#t 2012
#c 1
#% 2833
#% 61455
#% 577219
#% 989613
#% 1867871
#! Measuring robustness of complex networks is a fundamental task for analyzing the structure and function of complex networks. In this paper, we study the network robustness under the maximal vertex coverage (MVC) attack, where the attacker aims to delete as many edges of the network as possible by attacking a small fraction of nodes. First, we present two robustness metrics of complex networks based on MVC attack. We then propose an efficient randomized greedy algorithm with near-optimal performance guarantee for computing the proposed metrics. Finally, we conduct extensive experiments on 20 real datasets. The results show that P2P and co-authorship networks are extremely robust under the MVC attack while both the online social networks and the Email communication networks exhibit vulnerability under the MVC attack. In addition, the results demonstrate the efficiency and effectiveness of our proposed algorithms for computing the corresponding robustness metrics.

#index 1919766
#* A simple approach to the design of site-level extractors using domain-centric principles
#@ Chong Long;Xiubo Geng;Chang Xu;Sathiya Keerthi
#t 2012
#c 1
#% 881505
#% 961152
#% 1166537
#% 1190073
#% 1406981
#% 1538764
#% 1606085
#! We consider the problem of extracting, in a domain-centric fashion, a given set of attributes from a large number of semi-structured websites. Previous approaches [7, 5] to solve this problem are based on page level inference. We propose a distinct new approach that directly chooses attribute extractors for a site using a scoring mechanism that is designed at the domain level via simple classification methods using a training set from a small number of sites. To keep the number of candidate extractors in each site manageably small we use two observations that hold in most domains: (a) imprecise annotators can be used to identify a small set of candidate extractors for a few attributes (anchors); and (b) non-anchor attributes lie in close proximity to the anchor attributes. Experiments on three domains (Events, Books and Restaurants) show that our approach is very effective in spite of its simplicity.

#index 1919767
#* Extraction of topic evolutions from references in scientific articles and its GPU acceleration
#@ Tomonari Masada;Atsuhiro Takasu
#t 2012
#c 1
#% 73441
#% 290830
#% 722904
#% 875959
#% 1073975
#% 1083684
#% 1318691
#% 1746886
#% 1913321
#! This paper provides a topic model for extracting topic evolutions as a corpus-wide transition matrix among latent topics. Recent trends in text mining point to a high demand for exploiting metadata. Especially, exploitation of reference relationships among documents induced by hyperlinking Web pages, citing scientific articles, tumblring blog posts, retweeting tweets, etc., is put in the foreground of the effort for an effective mining. We focus on scholarly activities and propose a topic model for obtaining a corpus-wide view on how research topics evolve along citation relationships. Our model, called TERESA, extends latent Dirichlet allocation (LDA) by introducing a corpus-wide topic transition probability matrix, which models reference relationships as transitions among topics. Our approximated variational inference updates LDA posteriors and topic transition posteriors alternately. The main issue is execution time amounting to O(MK2), where K is the number of topics and M is that of links in citation network. Therefore, we accelerate the inference with Nvidia CUDA compatible GPUs. We compare the effectiveness of TERESA with that of LDA by introducing a new measure called diversity plus focusedness (D+F). We also present topic evolution examples our method gives.

#index 1919768
#* Graph-based workflow recommendation: on improving business process modeling
#@ Bin Cao;Jianwei Yin;Shuiguang Deng;Dongjing Wang;Zhaohui Wu
#t 2012
#c 1
#% 245500
#% 629708
#% 772836
#% 1267634
#% 1497996
#% 1960013
#! How to improve the modeling efficiency and accuracy has become a burning problem. The popularization of recommendation technique in E-Commerce provide us new trajectories that can be used for addressing the problem. In this paper, we propose a graph-based workflow recommendation for improving business process modeling. The start point is so-called "workflow repository" including a set of already developed process models. Graph mining method is used to extract the process patterns from the repository. Based on graph edit distance (GED) [2], we calculate the distance between patterns and the partial business process, viewed as reference model, which is under modeling and select the candidate nodes with smaller distances for recommendation. The performance study show its feasibility for practical uses.

#index 1919769
#* Reconciling ontologies and the web of data
#@ Ziawasch Abedjan;Johannes Lorey;Felix Naumann
#t 2012
#c 1
#% 445448
#% 481290
#% 1055735
#% 1288161
#% 1418167
#% 1466286
#% 1545103
#% 1586117
#% 1597472
#% 1643132
#% 1707718
#! To integrate Linked Open Data, which originates from various and heterogeneous sources, the use of well-defined ontologies is essential. However, oftentimes the utilization of these ontologies by data publishers differs from the intended application envisioned by ontology engineers. This may lead to unspecified properties being used ad-hoc as predicates in RDF triples or it may result in infrequent usage of specified properties. These mismatches impede the goals and propagation of the Web of Data as data consumers face difficulties when trying to discover and integrate domain-specific information. In this work, we identify and classify common misusage patterns by employing frequency analysis and rule mining. Based on this analysis, we introduce an algorithm to propose suggestions for a data-driven ontology re-engineering workflow, which we evaluate on two large-scale RDF datasets.

#index 1919770
#* Efficient extraction of ontologies from domain specific text corpora
#@ Tianyu Li;Pirooz Chubak;Laks V.S. Lakshmanan;Rachel Pottinger
#t 2012
#c 1
#% 115608
#% 169777
#% 280849
#% 459006
#% 756964
#% 786515
#% 786523
#% 896039
#% 939515
#% 1264995
#% 1471312
#% 1500872
#% 1610990
#% 1826362
#! Extracting ontological relationships (e.g., ISA and HASA) from free-text repositories (e.g., engineering documents and instruction manuals) can improve users' queries, as well as benefit applications built for these domains. Current methods to extract ontologies from text usually miss many meaningful relationships because they either concentrate on single-word terms and short phrases or neglect syntactic relationships between concepts in sentences. We propose a novel pattern-based algorithm to find ontological relationships between complex concepts by exploiting parsing information to extract multi-word concepts and nested concepts. Our procedure is iterative: we tailor the constrained sequential pattern mining framework to discover new patterns. Our experiments on three real data sets show that our algorithm consistently and significantly outperforms previous representative ontology extraction algorithms.

#index 1919771
#* Effective and efficient?: bilingual sentiment lexicon extraction using collocation alignment
#@ Zheng Lin;Songbo Tan;Xueqi Cheng;Xueke Xu;Weisong Shi
#t 2012
#c 1
#% 529193
#% 557006
#% 579944
#% 740901
#% 740915
#% 746885
#% 756236
#% 769892
#% 1055768
#! Bilingual sentiment lexicon is fundamental resource for cross-language sentiment analysis but its compilation remains a major bottleneck in computational linguistics. Traditional word alignment algorithm faces with the status of large alignment space, which may introduce redundant computations as well as alignment errors. In this paper, we use collocation alignment to extract bilingual sentiment lexicon overcoming the drawbacks of word alignment. The idea of collocation alignment is inspired by the strong cohesion between feature words and opinion words in sentiment corpus. Experimental results show that our approach not only decreases the computing time dramatically but also improves the precision of extracted bilingual word pairs due to the smaller alignment space.

#index 1919772
#* Exploiting latent relevance for relational learning of ubiquitous things
#@ Lina Yao;Quan Z. Sheng
#t 2012
#c 1
#% 411799
#% 769942
#% 787175
#% 891559
#% 1047785
#% 1131829
#% 1214703
#% 1399939
#% 1498325
#% 1605971
#% 1692139
#% 1706747
#! With recent advances in radio-frequency identification (RFID), wireless sensor networks, and Web services, physical things are becoming an integral part of the emerging ubiquitous Web. While this integration offers many exciting opportunities such as efficient supply chains and improved environmental monitoring, it also presents many significant challenges. One such challenge lies in how to classify, discover, and manage ubiquitous things, which is critical for efficient and effective object search, recommendation, and composition. In this paper, we focus on automatically classifying ubiquitous things into manageable semantic category labels by exploiting the information hidden in interactions between users and ubiquitous things. We develop a novel approach to extract latent relevance by building a relational network of ubiquitous things (RNUbiT) where similar things are linked via virtual edges according to their latent relevance. A discriminative learning algorithm is also developed to automatically determine category labels for ubiquitous things. We conducted experiments using real-world data and the experimental results demonstrate the feasibility and validity of our proposed approach.

#index 1919773
#* Discovering personally semantic places from GPS trajectories
#@ Mingqi Lv;Ling Chen;Gencai Chen
#t 2012
#c 1
#% 723186
#% 778502
#% 815787
#% 960412
#% 1047352
#% 1052661
#% 1183285
#% 1478992
#% 1480826
#% 1483081
#% 1484671
#% 1524237
#! A place is a locale that is frequently visited by an individual user and carries important semantic meanings (e.g. home, work, etc.). Many location-aware applications will be greatly enhanced with the ability of the automatic discovery of personally semantic places. The discovery of a user's personally semantic places involves obtaining the physical locations and semantic meanings of these places. In this paper, we propose approaches to address both of the problems. For the physical place extraction problem, a hierarchical clustering algorithm is proposed to firstly extract visit points from the GPS trajectories, and then these visit points can be clustered to form physical places. For the semantic place recognition problem, Bayesian networks (encoding the temporal patterns in which the places are visited) are used in combination with a customized POI (i.e. place of interest) database (containing the spatial features of the places) to categorize the extracted physical places into pre-defined types. An extensive set of experiments have been conducted to demonstrate the effectiveness of the proposed approaches based on a dataset of real-world GPS trajectories.

#index 1919774
#* Mining coherent anomaly collections on web data
#@ Hanbo Dai;Feida Zhu;Ee-Peng Lim;HweeHwa Pang
#t 2012
#c 1
#% 769883
#% 1083642
#% 1202160
#% 1495600
#% 1746817
#! The recent boom of weblogs and social media has attached increasing importance to the identification of suspicious users with unusual behavior, such as spammers or fraudulent reviewers. A typical spamming strategy is to employ multiple dummy accounts to collectively promote a target, be it a URL or a product. Consequently, these suspicious accounts exhibit certain coherent anomalous behavior identifiable as a collection. In this paper, we propose the concept of Coherent Anomaly Collection (CAC) to capture this kind of collections, and put forward an efficient algorithm to simultaneously find the top-K disjoint CACs together with their anomalous behavior patterns. Compared with existing approaches, our new algorithm can find disjoint anomaly collections with coherent extreme behavior without having to specify either their number or sizes. Results on real Twitter data show that our approach discovers meaningful and informative hashtag spammer groups of various sizes which are hard to detect by clustering-based methods.

#index 1919775
#* Mining topic-level opinion influence in microblog
#@ Daifeng Li;Xin Shuai;Guozheng Sun;Jie Tang;Ying Ding;Zhipeng Luo
#t 2012
#c 1
#% 722904
#% 729923
#% 788094
#% 956510
#% 1083624
#% 1130939
#% 1214702
#% 1292503
#% 1338553
#% 1482198
#% 1603758
#% 1605929
#% 1606084
#! This paper proposes a Topic-Level Opinion Influence Model (TOIM) that simultaneously incorporates topic factor, user opinions and social influence in a unified probabilistic model with two stages learning processes. In the first stage, topic factor and user influence are integrated to generate users' influential relationship based on different topics; in the second stage, users' historical messages and social interaction records are leveraged by TOIM to construct their historical opinions and neighbors' opinion influence through a statistical learning process, which can be further utilized to predict users' future opinions on some specific topics. We evaluate our TOIM on a large-scaled dataset from Tencent Weibo, one of the largest microbloggings website in China. The experimental results show that TOIM can better predict users' opinion than other baseline methods.

#index 1919776
#* Meta path-based collective classification in heterogeneous information networks
#@ Xiangnan Kong;Philip S. Yu;Ying Ding;David J. Wild
#t 2012
#c 1
#% 1301029
#% 1650403
#! Collective classification approaches exploit the dependencies of a group of linked objects whose class labels are correlated and need to be predicted simultaneously. In this paper, we focus on studying the collective classification problem in heterogeneous networks, which involves multiple types of data objects interconnected by multiple types of links. Intuitively, two objects are correlated if they are linked by many paths in the network. By considering different linkage paths in the network, one can capture the subtlety of different types of dependencies among objects. We introduce the concept of meta-path based dependencies among objects, where a meta path is a path consisting a certain sequence of linke types. We show that the quality of collective classification results strongly depends upon the meta paths used. To accommodate the large network size, a novel solution, called HCC (meta-path based Heterogenous Collective Classification), is developed to effectively assign labels to a group of instances that are interconnected through different meta-paths. The proposed HCC model can capture different types of dependencies among objects with respect to different meta paths. Empirical studies on real-world networks demonstrate that effectiveness of the proposed meta path-based collective classification approach.

#index 1919777
#* Discretionary social network data revelation with a user-centric utility guarantee
#@ Yi Song;Panagiotis Karras;Sadegh Nobari;Giorgos Cheliotis;Mingqiang Xue;Stéphane Bressan
#t 2012
#c 1
#% 325683
#% 327432
#% 577219
#% 793987
#% 956511
#% 1055763
#% 1063476
#% 1080078
#% 1127360
#% 1206763
#% 1400043
#% 1426540
#% 1642233
#% 1872357
#% 1882108
#% 1918382
#! The proliferation of online social networks has created intense interest in studying their nature and revealing information of interest to the end user. At the same time, such revelation raises privacy concerns. Existing research addresses this problem following an approach popular in the database community: a model of data privacy is defined, and the data is rendered in a form that satisfies the constraints of that model while aiming to maximize some utility measure. Still, these is no consensus on a clear and quantifiable utility measure over graph data. In this paper, we take a different approach: we define a utility guarantee, in terms of certain graph properties being preserved, that should be respected when releasing data, while otherwise distorting the graph to an extend desired for the sake of confidentiality. We propose a form of data release which builds on current practice in social network platforms: A user may want to see a subgraph of the network graph, in which that user as well as connections and affiliates participate. Such a snapshot should not allow malicious users to gain private information, yet provide useful information for benevolent users. We propose a mechanism to prepare data for user view under this setting. In an experimental study with real data, we demonstrate that our method preserves several properties of interest more successfully than methods that randomly distort the graph to an equal extent, while withstanding structural attacks proposed in the literature.

#index 1919778
#* Empirical validation of the buckley-osthus model for the web host graph: degree and edge distributions
#@ Maxim Zhukovskiy;Dmitry Vinogradov;Yuri Pritykin;Liudmila Ostroumova;Evgeniy Grechnikov;Gleb Gusev;Pavel Serdyukov;Andrei Raigorodskii
#t 2012
#c 1
#% 299941
#% 309749
#% 337580
#% 763566
#% 769391
#% 824711
#% 1300556
#% 1425621
#% 1715587
#! We consider the Buckley-Osthus implementation of preferential attachment and its ability to model the web host graph in two aspects. One is the degree distribution that we observe to follow the power law, as often being the case for real-world graphs. Another one is the two-dimensional edge distribution, the number of edges between vertices of given degrees. We fit a single "initial attractiveness" parameter a of the model, first with respect to the degree distribution of the web host graph, and then, absolutely independently, with respect to the edge distribution. Surprisingly, the values of a we obtain turn out to be nearly the same. Therefore the same model with the same value of the parameter a fits very well the two independent and basic aspects of the web host graph. In addition, we demonstrate that other models completely lack the asymptotic behavior of the edge distribution of the web host graph, even when accurately capturing the degree distribution. To the best of our knowledge, this is the first study confirming the ability of preferential attachment models to reflect the distribution of edges between vertices with respect to their degrees in a real graph of Internet.

#index 1919779
#* gSCorr: modeling geo-social correlations for new check-ins on location-based social networks
#@ Huiji Gao;Jiliang Tang;Huan Liu
#t 2012
#c 1
#% 17631
#% 1049928
#% 1214685
#% 1595763
#% 1606049
#! Location-based social networks (LBSNs) have attracted an increasing number of users in recent years. The availability of geographical and social information of online LBSNs provides an unprecedented opportunity to study the human movement from their socio-spatial behavior, enabling a variety of location-based services. Previous work on LBSNs reported limited improvements from using the social network information for location prediction; as users can check-in at new places, traditional work on location prediction that relies on mining a user's historical trajectories is not designed for this "cold start" problem of predicting new check-ins. In this paper, we propose to utilize the social network information for solving the "cold start" location prediction problem, with a geo-social correlation model to capture social correlations on LBSNs considering social networks and geographical distance. The experimental results on a real-world LBSN demonstrate that our approach properly models the social correlations of a user's new check-ins by considering various correlation strengths and correlation measures.

#index 1919780
#* Swimming against the streamz: search and analytics over the enterprise activity stream
#@ Ido Guy;Tal Steier;Maya Barnea;Inbal Ronen;Tal Daniel
#t 2012
#c 1
#% 1035573
#% 1384195
#% 1450853
#% 1476488
#% 1477588
#% 1499606
#% 1512968
#% 1573470
#% 1625366
#% 1642273
#% 1765693
#% 1846784
#! Activity streams have become prevalent on the web and are starting to emerge in enterprises. In this work, we present Streamz, a novel application that uses a faceted search approach to provide employees with advanced capabilities of search, navigation, attention management, and other types of analytics on top of an enterprise activity stream. We provide a detailed description of the Streamz tool as well as usage analysis based on user interface logs and interviews of active users.

#index 1919781
#* What is happening right now ... that interests me?: online topic discovery and recommendation in twitter
#@ Ernesto Diaz-Aviles;Lucas Drumond;Zeno Gantner;Lars Schmidt-Thieme;Wolfgang Nejdl
#t 2012
#c 1
#% 1331
#% 577224
#% 894646
#% 1127481
#% 1176909
#% 1260273
#% 1417104
#% 1451238
#% 1476448
#% 1536522
#% 1625387
#% 1893816
#! Users engaged in the Social Web increasingly rely upon continuous streams of Twitter messages (tweets) for real-time access to information and fresh knowledge about current affairs. However, given the deluge of tweets, it is a challenge for individuals to find relevant and appropriately ranked information. We propose to address this knowledge management problem by going beyond the general perspective of information finding in Twitter, that asks: "What is happening right now?", towards an individual user perspective, and ask: "What is interesting to me right now?" In this paper, we consider collaborative filtering as an online ranking problem and present RMFO, a method that creates, in real-time, user-specific rankings for a set of tweets based on individual preferences that are inferred from the user's past system interactions. Experiments on the 476 million Twitter tweets dataset show that our online approach largely outperforms recommendations based on Twitter's global trend and Weighted Regularized Matrix Factorization (WRMF), a highly competitive state-of-the-art Collaborative Filtering technique, demonstrating the efficacy of our approach.

#index 1919782
#* Frequent grams based embedding for privacy preserving record linkage
#@ Luca Bonomi;Li Xiong;Rui Chen;Benjamin C.M. Fung
#t 2012
#c 1
#% 375017
#% 578407
#% 819551
#% 913783
#% 960288
#% 1068712
#% 1206749
#% 1206992
#% 1217148
#% 1372692
#% 1670071
#% 1740518
#% 1872252
#! In this paper, we study the problem of privacy preserving record linkage which aims to perform record linkage without revealing anything about the non-linked records. We propose a new secure embedding strategy based on frequent variable length grams which allows record linkage on the embedded space. The frequent grams used for constructing the embedding base are mined from the original database under the framework of differential privacy. Compared with the state-of-the-art secure matching schema [15], our approach provides formal, provable privacy guarantees and achieves better scalability while providing comparable utility.

#index 1919783
#* If you are happy and you know it... tweet
#@ Amir Asiaee T.;Mariano Tepper;Arindam Banerjee;Guillermo Sapiro
#t 2012
#c 1
#% 311027
#% 1127964
#% 1211829
#% 1299092
#% 1544009
#% 1591944
#% 1606433
#! Extracting sentiment from Twitter data is one of the fundamental problems in social media analytics. Twitter's length constraint renders determining the positive/negative sentiment of a tweet difficult, even for a human judge. In this work we present a general framework for per-tweet (in contrast with batches of tweets) sentiment analysis which consists of: (1) extracting tweets about a desired target subject, (2) separating tweets with sentiment, and (3) setting apart positive from negative tweets. For each step, we study the performance of a number of classical and new machine learning algorithms. We also show that the intrinsic sparsity of tweets allows performing classification in a low dimensional space, via random projections, without losing accuracy. In addition, we present weighted variants of all employed algorithms, exploiting the available labeling uncertainty, which further improve classification accuracy. Finally, we show that spatially aggregating our per-tweet classification results produces a very satisfactory outcome, making our approach a good candidate for batch tweet sentiment analysis.

#index 1919784
#* PRemiSE: personalized news recommendation via implicit social experts
#@ Chen Lin;Runquan Xie;Lei Li;Zhenhua Huang;Tao Li
#t 2012
#c 1
#% 173879
#% 330687
#% 734592
#% 754106
#% 879628
#% 956494
#% 1055785
#% 1083671
#% 1190124
#% 1214661
#% 1227602
#% 1227643
#% 1260273
#% 1287222
#% 1287242
#% 1287290
#% 1291600
#% 1586577
#% 1598346
#% 1598365
#% 1650470
#% 1710959
#! A variety of news recommender systems based on different strategies have been proposed to provide news personalization services for online news readers. However, little research work has been reported on utilizing the implicit "social" factors (i.e., the potential influential experts in news reading community) among news readers to facilitate news personalization. In this paper, we investigate the feasibility of integrating content-based methods, collaborative filtering and information diffusion models by employing probabilistic matrix factorization techniques. We propose PRemiSE, a novel Personalized news Recommendation framework via implicit Social Experts, in which the opinions of potential influencers on virtual social networks extracted from implicit feedbacks are treated as auxiliary resources for recommendation. Empirical results demonstrate the efficacy and effectiveness of our method, particularly, on handling the so-called cold-start problem.

#index 1919785
#* Hierarchical topic integration through semi-supervised hierarchical topic modeling
#@ Xian-Ling Mao;Jing He;Hongfei Yan;Xiaoming Li
#t 2012
#c 1
#% 722904
#% 788094
#% 876017
#% 983883
#% 1077150
#% 1131015
#% 1152447
#% 1166510
#% 1592262
#% 1605964
#% 1650298
#% 1872027
#% 1913642
#! Lots of document collections are well organized in hierarchical structure, and such structure can help users browse and understand these collections. Meanwhile, there are a large number of plain document collections loosely organized, and it is difficult for users to understand them effectively. In this paper we study how to automatically integrate latent topics in a plain collection with the topics in a hierarchical structured collection. We propose to use semi-supervised topic modeling to solve the problem in a principled way. The experiments show that the proposed method can generate both meaningful latent topics and expand high quality hierarchical topic structures.

#index 1919786
#* Exploiting enriched contextual information for mobile app classification
#@ Hengshu Zhu;Huanhuan Cao;Enhong Chen;Hui Xiong;Jilei Tian
#t 2012
#c 1
#% 321635
#% 722904
#% 869500
#% 879581
#% 1227577
#% 1482408
#% 1560178
#% 1583279
#% 1642219
#% 1688509
#% 1746821
#% 1874842
#% 1930407
#! A key step for the mobile app usage analysis is to classify apps into some predefined categories. However, it is a nontrivial task to effectively classify mobile apps due to the limited contextual information available for the analysis. To this end, in this paper, we propose an approach to first enrich the contextual information of mobile apps by exploiting the additional Web knowledge from the Web search engine. Then, inspired by the observation that different types of mobile apps may be relevant to different real-world contexts, we also extract some contextual features for mobile apps from the context-rich device logs of mobile users. Finally, we combine all the enriched contextual information into a Maximum Entropy model for training a mobile app classifier. The experimental results based on 443 mobile users' device logs clearly show that our approach outperforms two state-of-the-art benchmark methods with a significant margin.

#index 1919787
#* Incorporating word correlation into tag-topic model for semantic knowledge acquisition
#@ Fang Li;Tingting He;Xinhui Tu;Xiaohua Hu
#t 2012
#c 1
#% 280819
#% 722904
#% 783484
#% 879587
#% 956564
#% 1074403
#% 1085668
#% 1211693
#% 1250381
#% 1269899
#% 1271481
#% 1697444
#% 1905997
#! This paper presents a tag-topic model with Dirichlet Forest prior (TTM-DF) for semantic knowledge acquisition from blog. The TTM-DF model extends the tag-topic model (TTM) by replacing the Dirichlet prior with the Dirichlet Forest prior over the topic-word multinomial. The correlation between words are calculated to generate a set of Must-Links and Cannot-Links, then the structures of Dirichlet trees are obtained though encoding the constraints of Must-Links and Cannot-Links. Words under the same subtrees are expected to be more correlated than words under different subtrees. We conduct experiments on a synthetic and a blog dataset. Both of the experimental results show that the TTM-DF model performs much better than the TTM model. It can improve the coherence of the underlying topics and the tag-topic distributions, and capture semantic knowledge effectively.

#index 1919788
#* PriSM: discovering and prioritizing severe technical issues from product discussion forums
#@ Rashmi Gangadharaiah;Rose Catherine
#t 2012
#c 1
#% 248218
#% 252011
#% 375017
#% 1559807
#% 1560397
#% 1573551
#! Online forums provide a channel for users to report and discuss problems related to products and troubleshooting, for faster resolution. These could garner negative publicity if left unattended by the companies. Manually monitoring these massive amounts of discussions is laborious. This paper makes the first attempt at collecting issues that require immediate action by the product supplier by analyzing the immense information on forums. Features that are specific to forum discussions, in conjunction with linguistic cues help in capturing and better prioritizing issues. Any attempt to collect training data for learning a classifier for this task will require enormous labeling effort. Hence, this paper adopts a co-training approach, which uses minimal manual labeling, coupled with linguistic features extracted using a set-expansion algorithm to discover severe problems. Further, most distinct and recent issues are obtained by incorporating a measure of 'centrality', 'diversity' and temporal aspect of the forum threads. We show that this helps in better prioritizing longstanding issues and identify issues that need to be addressed immediately.

#index 1919789
#* Preprocessing of informal mathematical discourse in context ofcontrolled natural language
#@ Raúl Ernesto Gutiérrez de Piñerez Reyes;Juan Francisco Díaz Frías
#t 2012
#c 1
#% 708948
#% 939953
#% 983538
#% 995447
#% 1216878
#% 1499938
#% 1543237
#! Informal Mathematical Discourse (IMD) is characterized by the mixture of natural language and symbolic expressions in the context of textbooks, publications in mathematics and mathematical proof. We focused the IMD processing at the low level of discourse. In this paper, we proposed the preprocessing phase before the IMD structure analysis within the context of Controlled Natural Language (CNL). Our contribution is defined in context of the IMD processing and the use of machine learning; first, we present a CNL, a pure corpus and Matemathical Treebank for processing IMD; second, we present a preprocessing phase for IMD analysis with connectives disambiguation and verbs treatment, finally, we found a satisfactory result on input text parsing using a statistical parsing model. We will propagate these results for classification of argumentative informal practices via the low level discourse in IMD processing.

#index 1919790
#* PathRank: a novel node ranking measure on a heterogeneous graph for recommender systems
#@ Sangkeun Lee;Sungchan Park;Minsuk Kahng;Sang-goo Lee
#t 2012
#c 1
#% 282905
#% 577273
#% 641979
#% 813966
#% 1451212
#% 1457044
#% 1476448
#% 1625355
#% 1746925
#! In this paper, we present a novel random-walk based node ranking measure, PathRank, which is defined on a heterogeneous graph by extending the Personalized PageRank algorithm. Not only can our proposed measure exploit the semantics behind the different types of nodes and edges in a heterogeneous graph, but also it can emulate various recommendation semantics such as collaborative filtering, content-based filtering, and their combinations. The experimental results show that PathRank can produce more various and effective recommendation results compared to existing approaches.

#index 1919791
#* Unsupervised discovery of opposing opinion networks from forum discussions
#@ Yue Lu;Hongning Wang;ChengXiang Zhai;Dan Roth
#t 2012
#c 1
#% 280819
#% 577355
#% 577356
#% 854646
#% 938737
#% 939346
#% 939848
#% 1250356
#% 1261563
#% 1299754
#% 1328328
#% 1472908
#% 1481637
#% 1544104
#% 1591963
#% 1592079
#! With more and more people freely express opinions as well as actively interact with each other in discussion threads, online forums are becoming a gold mine with rich information about people's opinions and social behaviors. In this paper, we study an interesting new problem of automatically discovering opposing opinion networks of users from forum discussions, which are subset of users who are strongly against each other on some topic. Toward this goal, we propose to use signals from both textual content (e.g., who says what) and social interactions (e.g., who talks to whom) which are both abundant in online forums. We also design an optimization formulation to combine all the signals in an unsupervised way. We created a data set by manually annotating forum data on five controversial topics and our experimental results show that the proposed optimization method outperforms several baselines and existing approaches, demonstrating the power of combining both text analysis and social network analysis in analyzing and generating the opposing opinion networks.

#index 1919792
#* Exploring the existing category hierarchy to automatically label the newly-arising topics in cQA
#@ Guangyou Zhou;Li Cai;Kang Liu;Jun Zhao
#t 2012
#c 1
#% 413609
#% 465914
#% 757306
#% 807363
#% 878454
#% 1074073
#% 1117027
#% 1227594
#% 1269899
#% 1270281
#% 1399953
#% 1450829
#% 1482389
#% 1482407
#% 1591994
#% 1642070
#! This work investigates selecting concise labels for the newly-arising topics in community question answer. Previous methods of generating labels do not take the information of the existing category hierarchy into consideration. The main motivation of our paper is to utilize this information into the label generation process. We propose a general framework to address this problem. Firstly, we map the questions into Wikipedia concept sets, which are more meaningful than terms. Secondly, important concepts are identified to represent the main focus of the newly-arising topics. Thirdly, candidate labels are extracted from Wikipedia category graph. Finally, candidate labels are filtered and reranked by combination of structure information of existing category hierarchy and Wikipedia category graph. The experiments show that in our test collections, about 80% "correct" labels appear in the top ten labels recommended by our system.

#index 1919793
#* Query-focused multi-document summarization based on query-sensitive feature space
#@ Wenpeng Yin;Yulong Pei;Fan Zhang;Lian'en Huang
#t 2012
#c 1
#% 722904
#% 818266
#% 939968
#% 1190062
#% 1275220
#% 1482430
#% 1492056
#! Query-oriented relevance, information richness and novelty are important requirements in query-focused summarization, which, to a considerable extent, determine the summary quality. Previous work either rarely took into account all above demands simultaneously or dealt with part of them in the dynamic process of choosing sentences to generate a summary. In this paper, we propose a novel approach that integrates all these requirements skillfully by treating them as sentence features, making that the finally generated summary could fully reflect the combinational effect of these properties. Experimental results on the DUC2005 and DUC2006 datasets demonstrate the effectiveness of our approach.

#index 1919794
#* Time-aware topic recommendation based on micro-blogs
#@ Huizhi Liang;Yue Xu;Dian Tjondronegoro;Peter Christen
#t 2012
#c 1
#% 1214666
#% 1384223
#% 1429408
#% 1451212
#% 1476470
#% 1482399
#% 1536522
#% 1573367
#% 1598383
#% 1605960
#% 1605967
#% 1607052
#% 1746831
#! Topic recommendation can help users deal with the information overload issue in micro-blogging communities. This paper proposes to use the implicit information network formed by the multiple relationships among users, topics and micro-blogs, and the temporal information of micro-blogs to find semantically and temporally relevant topics of each topic, and to profile users' time-drifting topic interests. The Content based, Nearest Neighborhood based and Matrix Factorization models are used to make personalized recommendations. The effectiveness of the proposed approaches is demonstrated in the experiments conducted on a real world dataset that collected from Twitter.com.

#index 1919795
#* Topic-sensitive probabilistic model for expert finding in question answer communities
#@ Guangyou Zhou;Siwei Lai;Kang Liu;Jun Zhao
#t 2012
#c 1
#% 290830
#% 348173
#% 722904
#% 769906
#% 788094
#% 879576
#% 956516
#% 1019165
#% 1035587
#% 1083687
#% 1083720
#% 1355042
#% 1401230
#% 1481571
#% 1482364
#% 1482384
#% 1536507
#% 1591994
#% 1598376
#% 1623582
#! In this paper, we address the problem of expert finding in community question answering (CQA). Most of the existing approaches attempt to find experts in CQA by means of link analysis techniques. However, these traditional techniques only consider the link structure while ignore the topical similarity among users (askers and answerers) and user expertise and user reputation. In this study, we propose a topic-sensitive probabilistic model, which is an extension of PageRank algorithm to find experts in CQA. Compared to the traditional link analysis techniques, our proposed method is more effective because it finds the experts by taking into account both the link structure and the topical similarity among users. We conduct experiments on real world data set from Yahoo! Answers. Experimental results show that our proposed method significantly outperforms the traditional link analysis techniques and achieves the state-of-the-art performance for expert finding in CQA.

#index 1919796
#* iSampling: framework for developing sampling methods considering user's interest
#@ Jinoh Oh;Hwanjo Yu
#t 2012
#c 1
#% 310900
#% 823360
#% 960283
#% 1214647
#% 1214735
#% 1271973
#% 1401375
#% 1496772
#% 1535329
#% 1720760
#! Sampling is one of fundamental techniques for data preprocessing and mining. It helps to reduce computational costs and improve the mining quality. A sampling method is typically developed independently for a specific problem and for a specific user's interest, because it is hard to develop a method that is generalized across various user's interests. An absence of general framework for sampling makes it inefficient to develop or revise a sampling method as user's interest changes. This paper proposes a general framework, isampling, which facilitates a user developing sampling methods and easily modifying the user's sampling interest in the method. In the framework, a user explicitly describes her sampling interest into a graph model called interest model. Then, isampling automatically selects a sample set according to the model, which satisfies the user's interest. In order to demonstrate the effectiveness of our framework, we develop new trajectory sampling methods using our framework; trajectory sampling has been a challenging problem due to its high complexity of data and various user's interests. We demonstrate the flexibility of our framework by showing how easily trajectory samples of different interests can be generated within our framework.

#index 1919797
#* WiSeNet: building a wikipedia-based semantic network with ontologized relations
#@ Andrea Moro;Roberto Navigli
#t 2012
#c 1
#% 874707
#% 939600
#% 1019083
#% 1089602
#% 1108875
#% 1272263
#% 1471191
#% 1478186
#% 1482180
#% 1482288
#% 1585243
#% 1711865
#% 1919042
#% 1925702
#% 1925703
#! In this paper we present an approach for building a Wikipedia-based semantic network by integrating Open Information Extraction with Knowledge Acquisition techniques. Our algorithm extracts relation instances from Wikipedia page bodies and ontologizes them by, first, creating sets of synonymous relational phrases, called relation synsets, second, assigning semantic classes to the arguments of these relation synsets and, third, disambiguating the initial relation instances with relation synsets. As a result we obtain WiSeNet, a Wikipedia-based Semantic Network with Wikipedia pages as concepts and labeled, ontologized relations between them.

#index 1919798
#* Shaping communities out of triangles
#@ Arnau Prat-Pérez;David Dominguez-Sal;Josep M. Brunat;Josep-Lluis Larriba-Pey
#t 2012
#c 1
#% 755402
#% 955712
#% 974973
#% 1083675
#% 1399996
#% 1587733
#! Community detection has arisen as one of the most relevant topics in the field of graph data mining due to its importance in many fields such as biology, social networks or network traffic analysis. The metrics proposed to shape communities are too lax and do not consider the internal layout of the edges in the community, which lead to undesirable results. We define a new community metric called WCC. The proposed metric meets a minimum set of basic properties that guarantees communities with structure and cohesion. We experimentally show that WCC correctly quantifies the quality of communities and community partitions using real and synthetic datasets, and compare some of the most used community detection algorithms in the state of the art.

#index 1919799
#* The early-adopter graph and its application to web-page recommendation
#@ Ida Mele;Francesco Bonchi;Aristides Gionis
#t 2012
#c 1
#% 173879
#% 729923
#% 754107
#% 832271
#% 956521
#% 987212
#% 1035589
#% 1055676
#% 1083624
#% 1083641
#% 1107420
#% 1214702
#% 1355040
#% 1399993
#% 1400031
#% 1536509
#% 1560424
#! In this paper we present a novel graph-based data abstraction for modeling the browsing behavior of web users. The objective is to identify users who discover interesting pages before others. We call these users early adopters. By tracking the browsing activity of early adopters we can identify new interesting pages early, and recommend these pages to similar users. We focus on news and blog pages, which are more dynamic in nature and more appropriate for recommendation. Our proposed model is called early-adopter graph. In this graph, nodes represent users and a directed arc between users u and v expresses the fact that u and v visit similar pages and, in particular, that user u tends to visit those pages before user v. The weight of the edge is the degree to which the temporal rule "v visits a page before v" holds. Based on the early-adopter graph, we build a recommendation system for news and blog pages, which outperforms other out-of-the-shelf recommendation systems based on collaborative filtering.

#index 1919800
#* Relational co-clustering via manifold ensemble learning
#@ Ping Li;Jiajun Bu;Chun Chen;Zhanying He
#t 2012
#c 1
#% 342621
#% 823396
#% 833529
#% 837604
#% 876018
#% 881468
#% 915329
#% 961218
#% 1090641
#% 1176865
#% 1211703
#% 1214657
#% 1305450
#% 1327693
#% 1537112
#% 1573152
#% 1595860
#% 1641946
#% 1688428
#% 1845612
#! Co-clustering targets on grouping the samples and features simultaneously. It takes advantage of the duality between the samples and features. In many real-world applications, the data points or features usually reside on a submanifold of the ambient Euclidean space, but it is nontrivial to estimate the intrinsic manifolds in a principled way. In this study, we focus on improving the co-clustering performance via manifold ensemble learning, which aims to maximally approximate the intrinsic manifolds of both the sample and feature spaces. To achieve this, we develop a novel co-clustering algorithm called Relational Multi-manifold Co-clustering (RMC) based on symmetric nonnegative matrix tri-factorization, which decomposes the relational data matrix into three matrices. This method considers the inter-type relationship revealed by the relational data matrix and the intra-type information reflected by the affinity matrices. Specifically, we assume the intrinsic manifold of the sample or feature space lies in a convex hull of a group of pre-defined candidate manifolds. We hope to learn an appropriate convex combination of them to approach the desired intrinsic manifold. To optimize the objective, the multiplicative rules are utilized to update the factorized matrices and the entropic mirror descent algorithm is exploited to automatically learn the manifold coefficients. Experimental results demonstrate the superiority of the proposed algorithm.

#index 1919801
#* SemaFor: semantic document indexing using semantic forests
#@ George Tsatsaronis;Iraklis Varlamis;Kjetil Nørvåg
#t 2012
#c 1
#% 521054
#% 779875
#% 816186
#% 1026936
#% 1104925
#% 1131827
#% 1260784
#% 1275032
#% 1348068
#% 1473931
#% 1656540
#% 1705693
#! Traditional document indexing techniques store documents using easily accessible representations, such as inverted indices, which can efficiently scale for large document sets. These structures offer scalable and efficient solutions in text document management tasks, though, they omit the cornerstone of the documents' purpose: meaning. They also neglect semantic relations that bind terms into coherent fragments of text that convey messages. When semantic representations are employed, the documents are mapped to the space of concepts and the similarity measures are adapted appropriately to better fit the retrieval tasks. However, these methods can be slow both at indexing and retrieval time. In this paper we propose SemaFor, an indexing algorithm for text documents, which uses semantic spanning forests constructed from lexical resources, like Wikipedia, and WordNet, and spectral graph theory in order to represent documents for further processing.

#index 1919802
#* Measuring website similarity using an entity-aware click graph
#@ Pablo N. Mendes;Peter Mika;Hugo Zaragoza;Roi Blanco
#t 2012
#c 1
#% 590523
#% 754059
#% 946523
#% 956506
#% 987222
#% 989578
#% 1022765
#% 1043040
#% 1074093
#% 1130854
#% 1130868
#% 1130921
#% 1190102
#% 1215434
#% 1227610
#% 1227619
#% 1227648
#% 1252651
#% 1400010
#! Query logs record the actual usage of search systems and their analysis has proven critical to improving search engine functionality. Yet, despite the deluge of information, query log analysis often suffers from the sparsity of the query space. Based on the observation that most queries pivot around a single entity that represents the main focus of the user's need, we propose a new model for query log data called the entity-aware click graph. In this representation, we decompose queries into entities and modifiers, and measure their association with clicked pages. We demonstrate the benefits of this approach on the crucial task of understanding which websites fulfill similar user needs, showing that using this representation we can achieve a higher precision than other query log-based approaches.

#index 1919803
#* Community-based classification of noun phrases in twitter
#@ Freddy Chong Tat Chua;William W. Cohen;Justin Betteridge;Ee-Peng Lim
#t 2012
#c 1
#% 876067
#% 1130819
#% 1272187
#% 1355026
#% 1355042
#% 1400018
#% 1482547
#% 1591965
#% 1592152
#% 1711864
#% 1879064
#! Many event monitoring systems rely on counting known keywords in streaming text data to detect sudden spikes in frequency. But the dynamic and conversational nature of Twitter makes it hard to select known keywords for monitoring. Here we consider a method of automatically finding noun phrases (NPs) as keywords for event monitoring in Twitter. Finding NPs has two aspects, identifying the boundaries for the subsequence of words which represent the NP, and classifying the NP to a specific broad category such as politics, sports, etc. To classify an NP, we define the feature vector for the NP using not just the words but also the author's behavior and social activities. Our results show that we can classify many NPs by using a sample of training data from a knowledge-base.

#index 1919804
#* Real-time bid optimization for group-buying ads
#@ Raju Balakrishnan;Rushi P. Bhatt
#t 2012
#c 1
#% 739687
#% 991785
#% 1270024
#% 1336448
#% 1425621
#% 1560371
#% 1606074
#% 1784612
#! Group-buying ads seeking a minimum number of customers before the deal expiry are increasingly used by the daily-deal providers. Unlike the traditional web ads, the advertiser's profits for group-buying ads depends on the time to expiry and additional customers needed to satisfy the minimum group size. Since both these quantities are time-dependent, optimal bid amounts to maximize profits change with every impression. Consequently, traditional static bidding strategies are far from optimal. Instead, bid values need to be optimized in real-time to maximize expected bidder profits. This online optimization of deal profits is made possible by the advent of ad exchanges offering real-time (spot) bidding. To this end, we propose a real-time bidding strategy for group-buying deals based on the online optimization of the bid values. We derive the expected bidder profit of deals as a function of the bid amounts, and dynamically vary bids to maximize profits. Further, to satisfy time constraints of the online bidding, we present methods of minimizing computation timings. We evaluate the proposed bidding on a multi-million click stream of 935 ads. The method shows significant profit improvement over the existing strategies.

#index 1919805
#* Degree relations of triangles in real-world networks and graph models
#@ Nurcan Durak;Ali Pinar;Tamara G. Kolda;C. Seshadhri
#t 2012
#c 1
#% 823342
#% 937071
#% 983866
#% 1176970
#% 1245882
#% 1300556
#% 1394202
#% 1400019
#! Triangles are an important building block and distinguishing feature of real-world networks, but their structure is still poorly understood. Despite numerous reports on the abundance of triangles, there is very little information on what these triangles look like. We initiate the study of degree-labeled triangles, - specifically, degree homogeneity versus heterogeneity in triangles. This yields new insight into the structure of real-world graphs. We observe that networks coming from social and collaborative situations are dominated by homogeneous triangles, i.e., degrees of vertices in a triangle are quite similar to each other. On the other hand, information networks (e.g., web graphs) are dominated by heterogeneous triangles, i.e., the degrees in triangles are quite disparate. Surprisingly, nodes within the top 1% of degrees participate in the vast majority of triangles in heterogeneous graphs. We investigate whether current graph models reproduce the types of triangles that are observed in real data and observe that most models fail to accurately capture these salient features.

#index 1919806
#* A probabilistic approach to mining geospatial knowledge from social annotations
#@ Suradej Intagorn;Kristina Lerman
#t 2012
#c 1
#% 280849
#% 416733
#% 766441
#% 891559
#% 1134137
#% 1156097
#% 1190131
#% 1227637
#% 1536560
#% 2033585
#! User-generated content, such as photos and videos, is often annotated by users with free-text labels, called tags. Increasingly, such content is also georeferenced, i.e., it is associated with geographic coordinates. The implicit relationships between tags and their locations can tell us much about how people conceptualize places and relations between them. However, extracting such knowledge from social annotations presents many challenges, since annotations are often ambiguous, noisy, uncertain and spatially inhomogeneous. We introduce a probabilistic framework for modeling georeferenced annotations and a method for learning model parameters from data. The framework is flexible and general, and can be used in a variety of applications that mine geospatial knowledge from user-generated content. Specifically, we study three problems: extracting place semantics, predicting locations of photos and learning part-of relations between places. We show our method performs well compared to state-of-the-art approaches developed for the first two problems, and offers a novel solution to the problem of learning relations between places.

#index 1919807
#* Providing grades and feedback for student summaries by ontology-based information extraction
#@ Fernando Gutierrez;Dejing Dou;Stephen Fickas;Gina Griffiths
#t 2012
#c 1
#% 272325
#% 1250361
#% 1313373
#% 1428282
#% 1482179
#% 1655433
#% 1702407
#! Automatic grading systems for summaries and essays have been studied for years. Most commercial and research implementations are based in statistical methods, such as Latent Semantic Analysis (LSA), which can provide high accuracy on similarity between the essay and the graded or standard essays, but they can offer very limited feedback. In the present work, we propose a novel method to provide both grades and meaningful feedback for student summaries by Ontology-based Information Extraction (OBIE). We use ontological concepts and relationships to create extraction rules to identify correct statements. Based on ontology constraints (e.g., disjointness between concepts), we define patterns that are logically inconsistent with the ontology to create rules to extract incorrect statements. Experiments show that the grades given to 18 student summaries on Ecosystems by OBIE are correlated to human gradings. OBIE also provide meaningful feedback on the errors those students made in their summaries.

#index 1919808
#* Joint bilingual name tagging for parallel corpora
#@ Qi Li;Haibo Li;Heng Ji;Wen Wang;Jing Zheng;Fei Huang
#t 2012
#c 1
#% 158687
#% 464434
#% 632448
#% 747914
#% 811367
#% 817596
#% 961269
#% 983575
#% 1275696
#% 1471243
#% 1544113
#% 1609146
#! Traditional isolated monolingual name taggers tend to yield inconsistent results across two languages. In this paper, we propose two novel approaches to jointly and consistently extract names from parallel corpora. The first approach uses standard linear-chain Conditional Random Fields (CRFs) as the learning framework, incorporating cross-lingual features propagated between two languages. The second approach is based on a joint CRFs model to jointly decode sentence pairs, incorporating bilingual factors based on word alignment. Experiments on Chinese-English parallel corpora demonstrated that the proposed methods significantly outperformed monolingual name taggers, were robust to automatic alignment noise and achieved state-of-the-art performance. With only 20%of the training data, our proposed methods can already achieve better performance compared to the baseline learned from the whole training set.1

#index 1919809
#* Using program synthesis for social recommendations
#@ Alvin Cheung;Armando Solar-Lezama;Samuel Madden
#t 2012
#c 1
#% 3034
#% 73372
#% 136350
#% 449508
#% 466887
#% 765519
#% 896791
#% 989580
#% 1000502
#% 1291600
#% 1292593
#% 1399997
#% 1417383
#% 1451236
#% 1528293
#% 1589901
#% 1609714
#% 1642129
#% 1648195
#! This paper presents a new approach to select events of interest to users in a social media setting where events are generated from mobile devices. We argue that the problem is best solved by inductive learning, where the goal is to first generalize from the users' expressed "likes" and "dislikes" of specific events, then to produce a program that can be used to collect only data of interest. The key contribution of this paper is a new algorithm that combines machine learning techniques with program synthesis technology to learn users' preferences. We show that when compared with the more standard approaches, our new algorithm provides up to order-of-magnitude reductions in model training time, and significantly higher prediction accuracies for our target application.1

#index 1919810
#* Web-scale multi-task feature selection for behavioral targeting
#@ Amr Ahmed;Mohamed Aly;Abhimanyu Das;Alexander J. Smola;Tasos Anastasakos
#t 2012
#c 1
#% 232771
#% 1211747
#% 1302843
#% 1302853
#% 1523858
#% 1605925
#% 1642128
#% 1693873
#% 1746906
#% 1815826
#! A typical behavioral targeting system optimizing purchase activities, called conversions, faces two main challenges: the web-scale amounts of user histories to process on a daily basis, and the relative sparsity of conversions. In this paper, we try to address these challenges through feature selection. We formulate a multi-task (or group) feature-selection problem among a set of related tasks (sharing a common set of features), namely advertising campaigns. We apply a group-sparse penalty consisting of a combination of an l1 and l2 penalty and an associated fast optimization algorithm for distributed parameter estimation. Our algorithm relies on a variant of the well known Fast Iterative Thresholding Algorithm (FISTA), a closed-form solution for mixed norm programming and a distributed subgradient oracle. To efficiently handle web-scale user histories, we present a distributed inference algorithm for the problem that scales to billions of instances and millions of attributes. We show the superiority of our algorithm in terms of both sparsity and ROC performance over baseline feature selection methods (both single-task -regularization and multi-task mutual-information gain).

#index 1919811
#* Balanced coverage of aspects for text summarization
#@ Takuya Makino;Hiroya Takamura;Manabu Okumura
#t 2012
#c 1
#% 262112
#% 939705
#% 1260752
#% 1392478
#! We propose a new model for the guided text summarization task. In this task, it is required that a generated summary covers all the aspects, which are predefined for the topic of the given document cluster; for example, aspects for the topic "Accidents and Natural Disasters" include WHAT, WHEN, WHERE, WHY, WHO AFFECTED, DAMAGES and COUNTERMEASURES. We use as a scorer for an aspect, the maximum entropy classifier that predicts whether each sentence reflects the aspect or not. We formalize the coverage of the aspects as a max-min problem, which enables a summary to cover aspects in a well-balanced manner. In the max-min problem, the minimum of the aspect scores is going to be maximized so that the summary contains all the aspects as much as possible. Furthermore, we integrate the model based on the max-min problem with the maximum coverage summarization model, which generates a summary containing as many conceptual units as possible. Through the experiments on benchmark datasets for the guided summarization, we show that our model outperforms other approaches in terms of ROUGE-2.

#index 1919812
#* Dynamic effects of ad impressions on commercial actions in display advertising
#@ Joel Barajas;Ram Akella;Marius Holtan;Jaimie Kwon;Aaron Flores;Victor Andrei
#t 2012
#c 1
#% 235061
#% 1151254
#% 1451139
#% 1560149
#% 1560370
#! In this paper, we develop a time series approach, based on Dynamic Linear Models (DLM), to estimate the impact of ad impressions on the daily number of commercial actions when no user tracking is possible. The proposed method uses aggregate data, and hence it is simple to implement without expensive infrastructure. Specifically, we model the impact of daily number of ad impressions in daily number of commercial actions. We incorporate persistence of campaign effects on actions assuming a decay factor. We relax the assumption of a linear impact of ads on actions using the log-transformation. We also account for outliers with long-tailed distributions fitted and estimated automatically without a pre-defined threshold. This is applied to observational data post-campaign and does not require an experimental set-up. We apply the method to data from one commercial ad network on 2,885 campaigns for 1,251 products during six months, to calibrate and perform model selection. We set up a randomized experiment for two campaigns where user tracking is feasible. We find that the output of the proposed method is consistent with the results of A/B testing with similar confidence intervals.

#index 1919813
#* A hybrid approach for efficient provenance storage
#@ Yulai Xie;Dan Feng;Zhipeng Tan;Lei Chen;Kiran-Kumar Muniswamy-Reddy;Yan Li;Darrell D.E. Long
#t 2012
#c 1
#% 754117
#% 900797
#% 978444
#% 1017302
#% 1063544
#% 1245541
#! Efficient provenance storage is an essential step towards the adoption of provenance. In this paper, we analyze the provenance collected from multiple workloads with a view towards efficient storage. Based on our analysis, we characterize the properties of provenance with respect to long term storage. We then propose a hybrid scheme that takes advantage of the graph structure of provenance data and the inherent duplication in provenance data. Our evaluation indicates that our hybrid scheme, a combination of web graph compression (adapted for provenance) and dictionary encoding, provides the best tradeoff in terms of compression ratio, compression time and query performance when compared to other compression schemes.

#index 1919814
#* Content-based relevance estimation on the web using inter-document similarities
#@ Fiana Raiber;Oren Kurland;Moshe Tennenholtz
#t 2012
#c 1
#% 340899
#% 340948
#% 375017
#% 818262
#% 1002316
#% 1077150
#% 1292528
#% 1465383
#% 1532609
#% 1536512
#! In adversarial and noisy search settings as the Web, the document-query surface level similarity can be a highly misleading relevance signal. Thus, devising content-based relevance estimation (ranking) approaches becomes highly challenging. We address this challenge using two methods that utilize inter-document similarities in an initially retrieved list. The first removes documents from the list that exhibit high query similarity, but for which there is insufficient additional support for relevance that is based on inter-document similarities. The method is based on a probabilistic model that decouples document-query similarities from relevance estimation. The second method re-ranks the list by "rewarding" documents that exhibit high similarity both to the query and to other documents in the list. Both methods incorporate, in addition, at the model level, query-independent document quality estimates. Extensive empirical evaluation demonstrates the merits of our methods.

#index 1919815
#* Trust prediction via aggregating heterogeneous social networks
#@ Jin Huang;Feiping Nie;Heng Huang;Yi-Cheng Tu
#t 2012
#c 1
#% 465928
#% 577273
#% 577367
#% 754098
#% 757953
#% 875974
#% 1001279
#% 1083641
#% 1275183
#% 1327693
#% 1451176
#% 1743951
#! Along with the increasing popularity of social web sites, users rely more on the trustworthiness information for many online activities among users. However, such social network data often suffers from severe data sparsity and are not able to provide users with enough information. Therefore, trust prediction has emerged as an important topic in social network research. Traditional approaches explore the topology of trust graph. Previous research in sociology and our life experience suggest that people who are in the same social circle often exhibit similar behavior and tastes. Such ancillary information, is often accessible and therefore could potentially help the trust prediction. In this paper, we address the link prediction problem by aggregating heterogeneous social networks and propose a novel joint manifold factorization (JMF) method. Our new joint learning model explores the user group level similarity between correlated graphs and simultaneously learns the individual graph structure, therefore the shared structures and patterns from multiple social networks can be utilized to enhance the prediction tasks. As a result, we not only improve the trust prediction in the target graph, but also facilitate other information retrieval tasks in the auxiliary graphs. To optimize the objective function, we break down the proposed objective function into several manageable sub-problems, then further establish the theoretical convergence with the aid of auxiliary function. Extensive experiments were conducted on real world data sets and all empirical results demonstrated the effectiveness of our method.

#index 1919816
#* Estimating interleaved comparison outcomes from historical click data
#@ Katja Hofmann;Shimon Whiteson;Maarten de Rijke
#t 2012
#c 1
#% 384911
#% 411762
#% 466751
#% 577224
#% 943049
#% 987199
#% 1130811
#% 1166517
#% 1173703
#% 1227581
#% 1292763
#% 1355034
#% 1450912
#% 1450952
#% 1495111
#% 1536534
#% 1560394
#% 1641943
#! Interleaved comparison methods, which compare rankers using click data, are a promising alternative to traditional information retrieval evaluation methods that require expensive explicit judgments. A major limitation of these methods is that they assume access to live data, meaning that new data must be collected for every pair of rankers compared. We investigate the use of previously collected click data (i.e., historical data) for interleaved comparisons. We start by analyzing to what degree existing interleaved comparison methods can be applied and find that a recent probabilistic method allows such data reuse, even though it is biased when applied to historical data. We then propose an interleaved comparison method that is based on the probabilistic approach but uses importance sampling to compensate for bias. We experimentally confirm that probabilistic methods make the use of historical data for interleaved comparisons possible and effective.

#index 1919817
#* Automatic image annotation using tag-related random search over visual neighbors
#@ Zijia Lin;Guiguang Ding;Mingqing Hu;Jianmin Wang;Jiaguang Sun
#t 2012
#c 1
#% 876068
#% 905199
#% 975105
#% 1081621
#% 1119139
#% 1126940
#% 1132018
#% 1148273
#% 1502531
#% 1648803
#! In this paper, we propose a novel image auto-annotation model using tag-related random search over range-constrained visual neighbors of the to-be-annotated image. The proposed model, termed as TagSearcher, observes that the annotating performances of many previous visual-neighbor-based models are generally sensitive to the quantity setting of visual neighbors, and the probabilities for visual neighbors to be selected is better to be tag-dependent, meaning that each candidate tag can have its own trustworthy part of visual neighbors for score prediction. And thus TagSearcher uses a constrained range rather than an identical and fixed number of visual neighbors for auto-annotation. By performing a novel tag-related random search process over the graphical model made up of range-constrained visual neighbors, TagSearcher can find the trustworthy part for each candidate tag, and further utilize both visual similarities and tag correlations for score prediction. With the range constraint for visual neighbors and the tag-related random search process, TagSearcher can not only achieve satisfactory annotating performances, but also reduce the performance sensitivity. Experiments conducted on benchmark Corel5k well demonstrate its rationality and effectiveness.

#index 1919818
#* Diversionary comments under political blog posts
#@ Jing Wang;Clement T. Yu;Philip S. Yu;Bing Liu;Weiyi Meng
#t 2012
#c 1
#% 321635
#% 387427
#% 722904
#% 912202
#% 1035590
#% 1077150
#% 1127962
#% 1263886
#% 1264748
#% 1275012
#% 1565812
#! An important issue that has been neglected so far is the identification of diversionary comments. Diversionary comments under political blog posts are defined as comments that deliberately twist the bloggers' intention and divert the topic to another one. The purpose is to distract readers from the original topic and draw attention to a new topic. Given that political blogs have significant impact on the society, we believe it is imperative to identify such comments. We then categorize diversionary comments into 5 types, and propose an effective technique to rank comments in descending order of being diversionary. To the best of our knowledge, the problem of detecting diversionary comments has not been studied so far. Our evaluation on 2,109 comments under 20 different blog posts from Digg.com shows that the proposed method achieves the high mean average precision (MAP) of 92.6%. Sensitivity analysis indicates that the effectiveness of the method is stable under different parameter settings.

#index 1919819
#* Discover breaking events with popular hashtags in twitter
#@ Anqi Cui;Min Zhang;Yiqun Liu;Shaoping Ma;Kuo Zhang
#t 2012
#c 1
#% 754107
#% 956515
#% 956589
#% 1055743
#% 1127482
#% 1399992
#% 1426611
#% 1523469
#% 1535480
#% 1536522
#% 1544421
#% 1560424
#% 1606436
#% 1642279
#% 1693881
#% 1913112
#! In this paper, we utilize tags in Twitter (the hashtags) as an indicator of events. We first study the properties of hashtags for event detection. Based on several observations, we proposed three attributes of hashtags, including (1) instability for temporal analysis, (2) Twitter meme possibility to distinguish social events from virtual topics or memes, and (3) authorship entropy for mining the most contributed authors. Based on these attributes, breaking events are discovered with hashtags, which cover a wide range of social events among different languages in the real world.

#index 1919820
#* Query likelihood with negative query generation
#@ Yuanhua Lv;ChengXiang Zhai
#t 2012
#c 1
#% 120104
#% 262096
#% 280850
#% 324129
#% 340901
#% 340948
#% 766503
#% 987232
#% 1074078
#% 1166534
#% 1598382
#% 1641914
#! The query likelihood retrieval function has proven to be empirically effective for many retrieval tasks. From theoretical perspective, however, the justification of the standard query likelihood retrieval function requires an unrealistic assumption that ignores the generation of a "negative query" from a document. This suggests that it is a potentially non-optimal retrieval function. In this paper, we attempt to improve the query likelihood function by bringing back the negative query generation. We propose an effective approach to estimate the probabilities of negative query generation based on the principle of maximum entropy, and derive a more complete query likelihood retrieval function that also contains the negative query generation component. The proposed approach not only bridges the theoretical gap in the existing query likelihood retrieval function, but also improves retrieval effectiveness significantly with no additional computational cost.

#index 1919821
#* On the connections between explicit semantic analysis and latent semantic analysis
#@ Chao Liu;Yi-Min Wang
#t 2012
#c 1
#% 228088
#% 279755
#% 393059
#% 430757
#% 1074073
#% 1214660
#% 1227677
#% 1250362
#% 1270267
#% 1272267
#% 1275012
#% 1289518
#% 1369940
#% 1415756
#% 1494759
#! Semantic analysis tries to solve problems arising from polysemy and synonymy that are abundant in natural languages. Recently, Gabrilovich and Markovitch propose the Explicit Semantic Analysis (ESA) technique, which complements the well-known Latent Semantic Analysis (LSA) technique. In this paper, we show that the two techniques are not as distinct as their names suggest; instead, we find that ESA is equivalent to a LSA variant, and this equivalence generalizes to all kernel methods using kernels arising from the canonical dot product. Effectively, this result guarantees that ESA would not outperform the peak efficacy of LSA for any applications using the above kernel methods. In short, this paper for the first time establishes the connections between ESA and LSA, quantifies their relative efficacy, and generalizes the result to a big category of kernel methods.

#index 1919822
#* Variance maximization via noise injection for active sampling in learning to rank
#@ Wenbin Cai;Ya Zhang
#t 2012
#c 1
#% 236729
#% 309095
#% 734915
#% 823360
#% 1035577
#% 1073903
#% 1227673
#% 1450862
#% 1598345
#% 1617356
#! Active learning for ranking, which is to selectively label the most informative examples, has been widely studied in recent years. In this paper, we propose a general active learning for ranking strategy called Variance Maximization (VM). The algorithm relies on noise injection to perturb the original unlabeled examples and generate the rank distribution of each example. Using a DCG-like gain function to measure each ranked list sampled from the rank distribution, Variance Maximization selects the unlabeled example with the largest variance in the gain. The VM strategy is applied at both the query level and the document level, and a two-stage active learning algorithm is further derived. Experimental results on both the LETOR 4.0 dataset and a real-world Web search ranking dataset have demonstrated the effectiveness of the proposed active learning approach.

#index 1919823
#* More than relevance: high utility query recommendation by mining users' search behaviors
#@ Xiaofei Zhu;Jiafeng Guo;Xueqi Cheng;Yanyan Lan
#t 2012
#c 1
#% 310567
#% 330617
#% 591792
#% 766472
#% 869501
#% 869651
#% 1035578
#% 1130854
#% 1130868
#% 1130879
#% 1190055
#% 1270276
#% 1355032
#% 1355034
#% 1482240
#% 1560358
#% 1598414
#% 1641944
#% 1642182
#! Query recommendation plays a critical role in helping users' search. Most existing approaches on query recommendation aim to recommend relevant queries. However, the ultimate goal of query recommendation is to assist users to reformulate queries so that they can accomplish their search task successfully and quickly. Only considering relevance in query recommendation is apparently not directly toward this goal. In this paper, we argue that it is more important to directly recommend queries with high utility, i.e., queries that can better satisfy users' information needs. For this purpose, we propose a novel generative model, referred to as Query Utility Model (QUM), to capture query utility by simultaneously modeling users' reformulation and click behaviors. The experimental results on a publicly released query log show that, our approach is more effective in helping users find relevant search results and thus satisfying their information needs.

#index 1919824
#* Finding nuggets in IP portfolios: core patent mining through textual temporal analysis
#@ Po Hu;Minlie Huang;Peng Xu;Weichang Li;Adam K. Usadi;Xiaoyan Zhu
#t 2012
#c 1
#% 155349
#% 1214739
#% 1385969
#% 1606056
#% 1688485
#! Patents are critical for a company to protect its core technologies. Effective patent mining in massive patent databases can provide companies with valuable insights to develop strategies for IP management and marketing. In this paper, we study a novel patent mining problem of automatically discovering core patents (i.e., patents with high novelty and influence in a domain). We address the unique patent vocabulary usage problem, which is not considered in traditional word-based statistical methods, and propose a topic-based temporal mining approach to quantify a patent's novelty and influence. Comprehensive experimental results on real-world patent portfolios show the effectiveness of our method.

#index 1919825
#* Interest-matching information propagation in multiple online social networks
#@ Yilin Shen;Thang N. Dinh;Huiyuan Zhang;My T. Thai
#t 2012
#c 1
#% 340147
#% 342596
#% 729923
#% 939870
#% 989613
#% 1214641
#% 1222657
#% 1399992
#% 1449326
#% 1536522
#% 1540408
#% 1663638
#! Online social networks have become an imperative channel for extremely fast information propagation and influence. Thus, the problem of finding a minimum number of seed users who can eventually influence as many users in the network as possible has become one of the central research topics recently. Unfortunately, most of related works have only focused on the network topologies and largely ignored many other important factors such as the users' engagements and the negative or positive impacts between users. More challengingly, the behavior of information propagation across multiple networks simultaneously remains an untrodden area and becomes an urgent need. Our work is the first attempt to tackle the above problem in multiple networks, considering these lacking important factors. In order to capture the users' engagement, we propose to targeting the set of interest-matching users whose interests are similar to what we try to propagate. Then, we develop our Iterative Semi-Supervising Learning based approach to identify the minimum seed users. We validate the effectiveness of our solution by using real-world Twitter-Foursquare networks and academic collaboration multiple networks.

#index 1919826
#* Customizing search results for non-native speakers
#@ Theodoros Lappas;Michail Vlachos
#t 2012
#c 1
#% 861988
#% 939765
#% 1045468
#% 1076733
#% 1264737
#% 1280217
#! Blog posts, news articles and other webpages are present on the web in multiple languages. Standard search engines evaluate the relevance of the candidate documents to the given query. However, when considering documents with overlapping content, many of them written in a foreign language other than the user's own native tongue, it is beneficial to promote documents that are easy enough for the user to read. Here, we show how to rank a collection of foreign documents based on both: a) relevance to the query, and b) the comprehension difficulty of the document. We design effective ranking operators that evaluate the difficulty of a foreign document with respect to the user's native language. We show that existing search engines can easily augment their scoring function by incorporating the proposed comprehensibility metrics. Finally, we provide extensive experimental evidence that the comprehensibility-aware ranking model significantly improves the standard relevance-based ranking paradigm.

#index 1919827
#* Quality models for microblog retrieval
#@ Jaeho Choi;W. Bruce Croft;Jin Young Kim
#t 2012
#c 1
#% 262096
#% 268079
#% 290830
#% 340901
#% 340948
#% 818262
#% 838472
#% 976952
#% 1019124
#% 1450951
#% 1484274
#% 1536506
#% 1536512
#% 1560174
#% 1560422
#% 1587369
#% 1641934
#% 1689528
#! Microblog services typically contain very short documents (e.g., tweets) containing comments about the latest news and events. Many of these documents are not informative or have very little content due to their personal and ephemeral nature. Providing effective retrieval in a microblog service will require addressing the challenge of distinguishing the high-quality, informative documents from the others. Recent work has focused on finding features that indicate the quality of microblog documents, but the impact these quality features on retrieval is not clear. In this paper, we suggest a low-cost quality model using surrogate judgments based on user behavior (i.e., retweets) that can be collected automatically. We analyze the relationship between document informativeness and relevance judgments for microblog retrieval. Then we demonstrate that our behavior-based quality metric has a high correlation with manual judgments. Also, we perform experiments to study the impact of the quality model on microblog retrieval. The results based on the TREC Microblog track show that the proposed quality model, combined with a variety of retrieval models, can improve retrieval performance and is competitive with a model trained using manual relevance judgments.

#index 1919828
#* Do ads compete or collaborate?: designing click models with full relationship incorporated
#@ Xin Xin;Irwin King;Ritesh Agrawal;Michael R. Lyu;Heyan Huang
#t 2012
#c 1
#% 577224
#% 1035578
#% 1074092
#% 1166517
#% 1190055
#% 1190056
#% 1214675
#% 1355048
#% 1450842
#% 1450873
#% 1451161
#% 1693911
#! Traditionally click models predict click-through rate (CTR) of an advertisement (ad) independent of other ads. Recent researches however indicate that the CTR of an ad is dependent on the quality of the ad itself but also of the neighboring ads. Using historical click-through data of a commercially available ad server, we identify two types (competing and collaborating) of influences among sponsored ads and further propose a novel click-model, Full Relation Model (FRM), which explicitly models dependencies between ads. On a test data, FRM shows significant improvement in CTR prediction as compared to earlier click models.

#index 1919829
#* Exploiting concept hierarchy for result diversification
#@ Wei Zheng;Hui Fang;Conglei Yao
#t 2012
#c 1
#% 340948
#% 768898
#% 1166473
#% 1292596
#% 1400021
#% 1482296
#% 1642139
#% 1697422
#% 1910510
#! The goal of result diversification is to maximize the coverage of query subtopics while minimizing the redundancy in the search results. Intuitively, it is more desirable for a diversification system to cover independent subtopics since it would retrieve sets of non-overlapped relevant documents, which leads to less redundancy in the search results. Unfortunately, existing diversification methods assume that query subtopics are independent and ignore their relations in the diversification process. To overcome this limitation, we propose to exploit concept hierarchies to extract query subtopics and infer their relations. We then apply axiomatic approaches to derive a structural diversification method that can leverage the subtopic relations in result diversification. Experimental results over an enterprise collection show that the relations among query subtopics are useful to improve the diversification performance.

#index 1919830
#* Ranking news events by influence decay and information fusion for media and users
#@ Liang Kong;Shan Jiang;Rui Yan;Shize Xu;Yan Zhang
#t 2012
#c 1
#% 262042
#% 262043
#% 378508
#% 411762
#% 577297
#% 643016
#% 805848
#% 987218
#% 987219
#% 1055717
#% 1130912
#% 1450878
#% 1631311
#% 1693401
#% 1747176
#! In many cases, people would like to read the news with great importance on the Internet. However, what users can grasp covers a very small part compared with the huge amount of news which never stops increasing. In this paper, we try to find what users are most likely to be interested in. We notice that media focus plays an essential role in distinguishing news topics and user attention is also an important factor. Therefore, we first propose five strategies which only exploit media focus to decide news influence impact. Then we provide three strategies to combine user attention with media focus. Meanwhile, we also take four types of interaction between user attention and media focus into consideration. To the best of our knowledge, this is the first work to establish different models for computing influence decay of news topics. Experiments show that better influence scores will be achieved by a decay algorithm based on Ebbinghaus forgetting curve and information fusion by considering interactions between user attention and media focus.

#index 1919831
#* Leveraging tagging for neighborhood-aware probabilistic matrix factorization
#@ Le Wu;Enhong Chen;Qi Liu;Linli Xu;Tengfei Bao;Lei Zhang
#t 2012
#c 1
#% 280852
#% 330687
#% 734594
#% 813966
#% 1038334
#% 1055739
#% 1083671
#% 1116993
#% 1127483
#% 1260273
#% 1282008
#% 1287228
#% 1476461
#! Collaborative Filtering(CF) is a popular way to build recommender systems and has been successfully employed in many applications. Generally, two kinds of approaches to CF, the local neighborhood methods and the global matrix factorization models, have been widely studied. Though some previous researches target on combining the complementary advantages of both approaches, the performance is still limited due to the extreme sparsity of the rating data. Therefore, it is necessary to consider more information for better reflecting user preference and item content. To that end, in this paper, by leveraging the extra tagging data, we propose a novel unified two-stage recommendation framework, named Neighborhood-aware Probabilistic Matrix Factorization(NHPMF). Specifically, we first use the tagging data to select neighbors of each user and each item, then add unique Gaussian distributions on each user's(item's) latent feature vector in the matrix factorization to ensure similar users(items) will have similar latent features}. Since the proposed method can effectively explores the external data source(i.e., tagging data) in a unified probabilistic model, it leads to more accurate recommendations. Extensive experimental results on two real world datasets demonstrate that our NHPMF model outperforms the state-of-the-art methods.

#index 1919832
#* Semantic context learning with large-scale weakly-labeled image set
#@ Yao Lu;Wei Zhang;Ke Zhang;Xiangyang Xue
#t 2012
#c 1
#% 961218
#% 997184
#% 1131835
#% 1148273
#% 1174086
#% 1279790
#% 1292880
#% 1484401
#% 1484402
#% 1484457
#% 1484460
#% 1502531
#% 1648755
#! There are a large number of images available on the web; meanwhile, only a subset of web images can be labeled by professionals because manual annotation is time-consuming and labor-intensive. Although we can now use the collaborative image tagging system, e.g., Flickr, to get a lot of tagged images provided by Internet users, these labels may be incorrect or incomplete. Furthermore, semantics richness requires more than one label to describe one image in real applications, and multiple labels usually interact with each other in semantic space. It is of significance to learn semantic context with large-scale weakly-labeled image set in the task of multi-label annotation. In this paper, we develop a novel method to learn semantic context and predict the labels of web images in a semi-supervised framework. To address the scalability issue, a small number of exemplar images are first obtained to cover the whole data cloud; then the label vector of each image is estimated as a local combination of the exemplar label vectors. Visual context, semantic context, and neighborhood consistency in both visual and semantic spaces are sufficiently leveraged in the proposed framework. Finally, the semantic context and the label confidence vectors for exemplar images are both learned in an iterative way. Experimental results on the real-world image dataset demonstrate the effectiveness of our method.

#index 1919833
#* Sketch-based indexing of n-words
#@ Samuel Huston;J. Shane Culpepper;W. Bruce Croft
#t 2012
#c 1
#% 290703
#% 818262
#% 894646
#% 1074112
#% 1127608
#% 1217130
#% 1366462
#% 1536517
#% 1567726
#% 1674728
#% 1870668
#! Formulating and processing phrases and other term dependencies to improve query effectiveness is an important problem in information retrieval. However, accessing these types of statistics using standard inverted indexes requires unreasonable processing time or incurs a substantial space overhead. Establishing a balance between these competing space and time trade-offs can dramatically improve system performance. In this paper, we present and analyze a new index structure designed to improve query efficiency in term dependency retrieval models, with bounded space requirements. By adapting a class of (ε,δ)-approximation algorithms originally proposed for sketch summarization in networking applications, we show how to accurately estimate various statistics important in term dependency models with low, probabilistically bounded error rates. The space requirements of the sketch index structure is largely independent of this size and the number of phrase term dependencies. Empirically, we show that the sketch index can reduce the space requirements of the vocabulary component of an index of all n-grams consisting of between 1 and 5 words extracted from the Clueweb-Part-B collection to less than 0.2% of the requirements of an equivalent full index. We show that n-gram queries of 5 words can be processed more efficiently than in current alternatives, such as next-word indexes. We show retrieval using the sketch index to be up to 400 times faster than with positional indexes, and 15 times faster than next-word indexes.

#index 1919834
#* Interactive and context-aware tag spell check and correction
#@ Francesco Bonchi;Ophir Frieder;Franco Maria Nardini;Fabrizio Silvestri;Hossein Vahabi
#t 2012
#c 1
#% 131061
#% 278102
#% 465743
#% 955712
#% 1074224
#% 1338621
#% 1484281
#% 1529931
#% 1592019
#! Collaborative content creation and annotation creates vast repositories of all sorts of media, and user-defined tags play a central role as they are a simple yet powerful tool for organizing, searching and exploring the available resources. We observe that when a user annotates a resource with a set of tags, those tags are introduced one at a time. Therefore, when the fourth tag is introduced, a knowledge represented by the previous three tags, i.e., the context in which the fourth tag is produced, is available and exploitable for generating potential correction of the current tag. This context, together with the "wisdom of the crowd" represented by the co-occurrences of tags in all the resources of the repository, can be exploited to provide interactive tag spell check and correction. We develop this idea in a framework, based on a weighted tag co-occurrence graph and on nodes relatedness measures defined on weighted neighborhoods. We test our proposal on a dataset coming from YouTube. The results show that our framework is effective as it outperforms two important baselines. We also show that it is efficient, thus enabling its use in modern tagging services.

#index 1919835
#* Federated search in the wild: the combined power of over a hundred search engines
#@ Dong Nguyen;Thomas Demeester;Dolf Trieschnigg;Djoerd Hiemstra
#t 2012
#c 1
#% 301225
#% 312689
#% 340146
#% 480479
#% 608625
#% 643012
#% 722311
#% 818211
#% 878624
#% 987254
#% 993964
#% 1227616
#% 1264113
#% 1565813
#% 1642923
#! Federated search has the potential of improving web search: the user becomes less dependent on a single search provider and parts of the deep web become available through a unified interface, leading to a wider variety in the retrieved search results. However, a publicly available dataset for federated search reflecting an actual web environment has been absent. As a result, it has been difficult to assess whether proposed systems are suitable for the web setting. We introduce a new test collection containing the results from more than a hundred actual search engines, ranging from large general web search engines such as Google and Bing to small domain-specific engines. We discuss the design and analyze the effect of several sampling methods. For a set of test queries, we collected relevance judgements for the top 10 results of each search engine. The dataset is publicly available and is useful for researchers interested in resource selection for web search collections, result merging and size estimation of uncooperative resources.

#index 1919836
#* From sBoW to dCoT marginalized encoders for text representation
#@ Zhixiang (Eddie) Xu;Minmin Chen;Kilian Q. Weinberger;Fei Sha
#t 2012
#c 1
#% 46803
#% 218982
#% 280819
#% 311027
#% 321635
#% 722904
#% 770767
#% 891559
#% 1074009
#% 1211703
#% 1211829
#% 1292484
#% 1305656
#! In text mining, information retrieval, and machine learning, text documents are commonly represented through variants of sparse Bag of Words (sBoW) vectors (e.g. TF-IDF [1]). Although simple and intuitive, sBoW style representations suffer from their inherent over-sparsity and fail to capture word-level synonymy and polysemy. Especially when labeled data is limited (e.g. in document classification), or the text documents are short (e.g. emails or abstracts), many features are rarely observed within the training corpus. This leads to overfitting and reduced generalization accuracy. In this paper we propose Dense Cohort of Terms (dCoT), an unsupervised algorithm to learn improved sBoW document features. dCoT explicitly models absent words by removing and reconstructing random sub-sets of words in the unlabeled corpus. With this approach, dCoT learns to reconstruct frequent words from co-occurring infrequent words and maps the high dimensional sparse sBoW vectors into a low-dimensional dense representation. We show that the feature removal can be marginalized out and that the reconstruction can be solved for in closed-form. We demonstrate empirically, on several benchmark datasets, that dCoT features significantly improve the classification accuracy across several document classification tasks.

#index 1919837
#* Task tours: helping users tackle complex search tasks
#@ Ahmed Hassan;Ryen W. White
#t 2012
#c 1
#% 49499
#% 51397
#% 148007
#% 151411
#% 204872
#% 268112
#% 272821
#% 309505
#% 577224
#% 728111
#% 807420
#% 823348
#% 879567
#% 955711
#% 956495
#% 956533
#% 1055676
#% 1450902
#! Complex search tasks such as planning a vacation often comprise multiple queries and may span a number of search sessions. When engaged in such tasks, users may require holistic support in determining the required task activities. Unfortunately, current search engines do not offer such support to their users. In this paper, we propose methods to automatically generate task tours comprising a starting task and a set of relevant related tasks, some or all of which may be necessary to satisfy a user's information needs. Applications of the tours include helping users understand the required steps to complete a task, finding URLs related to the active task, and alerting users to activities they may have missed. We demonstrate through experimentation with human judges and large-scale search logs that our tours are of good quality and can benefit a significant fraction of search engine users.

#index 1919838
#* Structured query reformulations in commerce search
#@ Sreenivas Gollapudi;Samuel Ieong;Anitha Kannan
#t 2012
#c 1
#% 481290
#% 956495
#% 1055676
#% 1190070
#% 1190105
#% 1227648
#% 1426566
#% 1536531
#% 1603792
#% 1642268
#! Recent work in commerce search has shown that understanding the semantics in user queries enables more effective query analysis and retrieval of relevant products. However, due to lack of sufficient domain knowledge, user queries often include terms that cannot be mapped directly to any product attribute. For example, a user looking for designer handbags might start with such a query because she is not familiar with the manufacturers, the price ranges, and/or the material that gives a handbag designer appeal. Current commerce search engines treat terms such as designer as keywords and attempt to match them to contents such as product reviews and product descriptions, often resulting in poor user experience. In this study, we propose to address this problem by reformulating queries involving terms such as designer, which we call modifiers, to queries that specify precise product attributes. We learn to rewrite the modifiers to attribute values by analyzing user behavior and leveraging structured data sources such as the product catalog that serves the queries. We first produce a probabilistic mapping between the modifiers and attribute values based on user behavioral data. These initial associations are then used to retrieve products from the catalog, over which we infer sets of attribute values that best describe the semantics of the modifiers. We evaluate the effectiveness of our approach based on a comprehensive Mechanical Turk study. We find that users agree with the attribute values selected by our approach in about 95% of the cases and they prefer the results surfaced for our reformulated queries to ones for the original queries in 87% of the time.

#index 1919839
#* Towards jointly extracting aspects and aspect-specific sentiment knowledge
#@ Xueke Xu;Songbo Tan;Yue Liu;Xueqi Cheng;Zheng Lin
#t 2012
#c 1
#% 722904
#% 769892
#% 956510
#% 1195867
#% 1292503
#% 1470684
#% 1481541
#% 1536586
#% 1560389
#! In this paper, we aim to jointly extract aspects and aspect-specific sentiment knowledge from online reviews, where the sentiment knowledge refers to the aspect-specific opinion words along with their aspect-aware sentiment polarities. To this end, we propose a Joint Aspect/Sentiment model (JAS). JAS detects aspect-specific opinion words by integrating opinion word lexicon knowledge to explicitly separate opinion words from factual words. More importantly, JAS exploits sentiment prior and aspect-contextual sentence-level co-occurrences of opinion words in reviews to further identify aspect-aware sentiment polarities for the opinion words. We apply the learned aspect-specific sentiment knowledge to practical aspect-level sentiment analysis tasks. Experimental results show the effectiveness of JAS in learning aspect-specific sentiment knowledge and the practical value of this knowledge when applied to aspect-level sentiment classification.

#index 1919840
#* Collaborative ranking: improving the relevance for tail queries
#@ Ke Zhou;Xin Li;Hongyuan Zha
#t 2012
#c 1
#% 564279
#% 577224
#% 987228
#% 987358
#% 1392432
#% 1400023
#% 1450893
#% 1450899
#% 1470633
#% 1482231
#% 1536566
#% 1536584
#% 1560359
#! It is well known that tail queries contribute to a substantial fraction of distinct queries submitted to search engines and thus become a major battle field for search engines. Unfortunately, compared with popular queries, it is much more difficult to obtain good search results for tail queries due to the lack of important relevance signals, such as user clicks, phrase matches and so on. In this paper, we propose to utilize the similarities between different queries to overcome the data sparsity problem for tail queries. Specifically, we propose to jointly learn query similarities and the ranking function from data so that the relevance signals of different but related queries can be collaboratively pooled to enhance the ranking of tail queries. We emphasize that the joint optimization is critical so that the learned query similarity function can adapt to the problem of learning ranking functions. Our proposed method is evaluated on two data sets and the results show that our method improves the relevance of tail queries over several baseline alternatives.

#index 1919841
#* BiasTrust: teaching biased users about controversial topics
#@ V. G. Vinod Vydiswaran;ChengXiang Zhai;Dan Roth;Peter Pirolli
#t 2012
#c 1
#% 1047414
#% 1183236
#% 1355029
#% 1613005
#! Deciding whether a claim is true or false often requires understanding the evidence supporting and contradicting the claim. However, when learning about a controversial claim, human biases and viewpoints may affect which evidence documents are considered "trustworthy" or credible. It is important to overcome this bias and know both viewpoints to get a balanced perspective. In this paper, we study various factors that affect learning about the truthfulness of controversial claims. We designed a user study to understand the impact of these factors. Specifically, we studied the impact of presenting evidence with contrasting viewpoints and source expertise rating on how users accessed the evidence documents. This would help us optimize how to teach users about controversial topics in the most effective way, and to design better claim verification systems. We find that users do not seek contrasting viewpoints by themselves, but explicitly presenting contrasting evidence helps them get a well-rounded understanding of the topic. Furthermore, explicit knowledge of the source credibility and the context not only affects what users read, but also how credible they perceive the document to be.

#index 1919842
#* Recommending citations: translating papers into references
#@ Wenyi Huang;Saurabh Kataria;Cornelia Caragea;Prasenjit Mitra;C. Lee Giles;Lior Rokach
#t 2012
#c 1
#% 280819
#% 280851
#% 415107
#% 722904
#% 740915
#% 766409
#% 817596
#% 939939
#% 987287
#% 1083684
#% 1130828
#% 1195999
#% 1399975
#% 1415726
#% 1642168
#% 1711869
#% 1826424
#! When we write or prepare to write a research paper, we always have appropriate references in mind. However, there are most likely references we have missed and should have been read and cited. As such a good citation recommendation system would not only improve our paper but, overall, the efficiency and quality of literature search. Usually, a citation's context contains explicit words explaining the citation. Using this, we propose a method that "translates" research papers into references. By considering the citations and their contexts from existing papers as parallel data written in two different "languages", we adopt the translation model to create a relationship between these two "vocabularies". Experiments on both CiteSeer and CiteULike dataset show that our approach outperforms other baseline methods and increase the precision, recall and f-measure by at least 5% to 10%, respectively. In addition, our approach runs much faster in the both training and recommending stage, which proves the effectiveness and the scalability of our work.

#index 1919843
#* Query-biased learning to rank for real-time twitter search
#@ Xin Zhang;Ben He;Tiejian Luo;Baobin Li
#t 2012
#c 1
#% 577224
#% 730070
#% 915281
#% 1074065
#% 1074082
#% 1074084
#% 1268491
#% 1272396
#% 1355017
#% 1399992
#% 1484274
#% 1598342
#% 1598383
#% 1598446
#% 1641991
#% 1674764
#! By incorporating diverse sources of evidence of relevance, learning to rank has been widely applied to real-time Twitter search, where users are interested in fresh relevant messages. Such approaches usually rely on a set of training queries to learn a general ranking model, which we believe that the benefits brought by learning to rank may not have been fully exploited as the characteristics and aspects unique to the given target queries are ignored. In this paper, we propose to further improve the retrieval performance of learning to rank for real-time Twitter search, by taking the difference between queries into consideration. In particular, we learn a query-biased ranking model with a semi-supervised transductive learning algorithm so that the query-specific features, e.g. the unique expansion terms, are utilized to capture the characteristics of the target query. This query-biased ranking model is combined with the general ranking model to produce the final ranked list of tweets in response to the given target query. Extensive experiments on the standard TREC Tweets11 collection show that our proposed query-biased learning to rank approach outperforms strong baseline, namely the conventional application of the state-of-the-art learning to rank algorithms.

#index 1919844
#* Discovering logical knowledge for deep question answering
#@ Zhao Liu;Xipeng Qiu;Ling Cao;Xuanjing Huang
#t 2012
#c 1
#% 198058
#% 309127
#% 340953
#% 741891
#% 756964
#% 815320
#% 815843
#% 835018
#% 850430
#% 871576
#% 1084594
#% 1264726
#% 1344869
#% 1467732
#% 1481641
#% 1494776
#% 1573237
#% 1711865
#! Most open-domain question answering systems achieve better performances with large corpora, such as Web, by taking advantage of information redundancy. However, explicit answers are not always mentioned in the corpus, many answers are implicitly contained and can only be deducted by inference. In this paper, we propose an approach to discover logical knowledge for deep question answering, which automatically extracts knowledge in an unsupervised, domain-independent manner from background texts and reasons out implicit answers for the questions. Firstly, we use semantic role labeling to transform natural language expressions to predicates in first-order logic. Then we use association analysis to uncover the implicit relations among these predicates and build propositions for inference. Since our knowledge is drawn from different sources, we use Markov logic to merge multiple knowledge bases without resolving their inconsistencies. Our experiments show that these propositions can improve the performance of question answering significantly.

#index 1919845
#* Mining noisy tagging from multi-label space
#@ Zhongang Qi;Ming Yang;Zhongfei (Mark) Zhang;Zhengyou Zhang
#t 2012
#c 1
#% 787726
#% 855904
#% 1100077
#% 1176915
#% 1261431
#% 1292880
#% 1505154
#% 1761097
#! In this paper we study the problem of mining noisy tagging. Most of the existing discriminative classification methods to this problem only consider one tag at a time as the classification target, and completely ignore the rest of the given tags at the same time. In this paper we argue that all the given multiple tags can be utilized simultaneously as an additional feature and the information contained in the multi-label space can be taken advantage of to improve the performance of the classification. We first propose a novel distance measure to compute the distance between instances in the multi-label space. Then we propose several novel methods to incorporate the information of the multi-label space into the discriminative classification methods in one view learning or in two views learning to solve a general multi-label classification problem and to mitigate the influence of the noise in the classification. We apply the proposed solutions to the problem with a more specific context - noisy image annotation, and evaluate the proposed methods on a standard dataset from the related literature. Experiments show that they are superior to the peer methods in the existing literature on solving the problem of mining noisy tagging.

#index 1919846
#* Learning from mistakes: towards a correctable learning algorithm
#@ Karthik Raman;Krysta M. Svore;Ran Gilad-Bachrach;Chris J. C. Burges
#t 2012
#c 1
#% 246243
#% 829043
#% 871302
#% 881477
#% 919460
#% 991230
#% 1211872
#% 1232036
#% 1273928
#! Many learning algorithms generate complex models that are difficult for a human to interpret, debug, and extend. In this paper, we address this challenge by proposing a new learning paradigm called correctable learning, where the learning algorithm receives external feedback about which data examples are incorrectly learned. We define a set of metrics which measure the correctability of a learning algorithm. We then propose a simple and efficient correctable learning algorithm which learns local models for different regions of the data space. Given an incorrect example, our method samples data in the neighborhood of that example and learns a new, more correct local model over that region. Experiments over multiple classification and ranking datasets show that our correctable learning algorithm offers significant improvements over the state-of-the-art techniques.

#index 1919847
#* CONSENTO: a new framework for opinion based entity search and summarization
#@ Jaehoon Choi;Donghyeon Kim;Seongsoon Kim;Junkyu Lee;Sangrak Lim;Sunwon Lee;Jaewoo Kang
#t 2012
#c 1
#% 939896
#% 1297087
#% 1457110
#% 1505090
#% 1544008
#% 1566286
#% 1711839
#% 1747007
#% 1763376
#! Search engines have become an important decision making tool today. Decision making queries are often subjective, such as "a good birthday present for my girlfriend", "best action movies in 2010", to name a few. Unfortunately, such queries may not be answered properly by conventional search systems. In order to address this problem, we introduce Consento, a consensus search engine designed to answer subjective queries. Consento performs segment indexing, as opposed to document indexing, to capture semantics from user opinions more precisely. In particular, we define a new indexing unit, Maximal Coherent Semantic Unit (MCSU). An MCSU represents a segment of a document, which captures a single coherent semantic. We also introduce a new ranking method, called ConsensusRank that counts online comments referring to an entity as a weighted vote. In order to validate the efficacy of the proposed framework, we compare Consento with standard retrieval models and their recent extensions for opinion based entity ranking. Experiments using movie and hotel data show the effectiveness of our framework.

#index 1919848
#* Search result presentation based on faceted clustering
#@ Benno Stein;Tim Gollub;Dennis Hoppe
#t 2012
#c 1
#% 281186
#% 342739
#% 642975
#% 754124
#% 807295
#% 807363
#% 1202162
#% 1280751
#% 1467778
#% 1532581
#% 1560378
#% 1642199
#% 1682423
#% 1879104
#! We propose a competence partitioning strategy for Web search result presentation: the unmodified head of a ranked result list is combined with a clustering of documents from the result list tail. We identify two principles to which such a clustering must adhere to improve the user's search experience: (1) Avoid the unwanted effect of query aspect repetition, which is called shadowing here. (2) Avoid extreme clusterings, i.e., neither the number of cluster labels nor the number of documents per cluster should exceed the size of the result list head. We present measures to quantify the shadowing effect, and with Faceted Clustering we introduce an algorithm that optimizes the identified principles. The key idea of Faceted Clustering is a dynamic, user-controlled reorganization of a clustering, similar to a faceted navigation system. We report on evaluations using the AMBIENT corpus and demonstrate the potential of our approach by a comparison with two well-known clustering search engines.

#index 1919849
#* PolariCQ: polarity classification of political quotations
#@ Rawia Awadallah;Maya Ramanath;Gerhard Weikum
#t 2012
#c 1
#% 868089
#% 1127964
#% 1227758
#% 1471332
#% 1484323
#% 1561554
#% 1592079
#% 1693867
#% 1693914
#! We consider the problem of automatically classifying quotations about political debates into both topic and polarity. These quotations typically appear in news media and online forums. Our approach maps quotations onto one or more topics in a category system of political debates, containing more than a thousand fine-grained topics. To overcome the difficulty that pro/con classification faces due to the brevity of quotations and sparseness of features, we have devised a model of quotation expansion that harnesses antonyms from thesauri like WordNet. We developed a suite of statistical language models, judiciously customized to our settings, and use these to define similarity measures for unsupervised or supervised classifications. Experiments show the effectiveness of our method.

#index 1919850
#* A comprehensive analysis of parameter settings for novelty-biased cumulative gain
#@ Teerapong Leelanupab;Guido Zuccon;Joemon M. Jose
#t 2012
#c 1
#% 642975
#% 879630
#% 1074124
#% 1074133
#% 1263586
#% 1292528
#% 1312812
#% 1598424
#% 1598438
#% 1622365
#! In the TREC Web Diversity track, novelty-biased cumulative gain (α-NDCG) is one of the official measures to assess retrieval performance of IR systems. The measure is characterised by a parameter, α, the effect of which has not been thoroughly investigated. We find that common settings of α, i.e. α=0.5, may prevent the measure from behaving as desired when evaluating result diversification. This is because it excessively penalises systems that cover many intents while it rewards those that redundantly cover only few intents. This issue is crucial since it highly influences systems at top ranks. We revisit our previously proposed threshold, suggesting α be set on a query-basis. The intuitiveness of the measure is then studied by examining actual rankings from TREC 09-10 Web track submissions. By varying α according to our query-based threshold, the discriminative power of α-NDCG is not harmed and in fact, our approach improves α-NDCG's robustness. Experimental results show that the threshold for α can turn the measure to be more intuitive than using its common settings.

#index 1919851
#* Entity centric query expansion for enterprise search
#@ Xitong Liu;Hui Fang;Fei Chen;Min Wang
#t 2012
#c 1
#% 262096
#% 340901
#% 340948
#% 342707
#% 464434
#% 722926
#% 768898
#% 879570
#% 879678
#% 987375
#% 1130922
#% 1166537
#% 1489451
#% 1641918
#! Enterprise search is important, and the search quality has a direct impact on the productivity of an enterprise. Many information needs of enterprise search center around entities. Intuitively, information related to the entities mentioned in the query, such as related entities, would be useful to reformulate the query and improve the retrieval performance. However, most existing studies on query expansion are term-centric. In this paper, we propose a novel entity-centric query expansion framework for enterprise search. Specifically, given a query containing entities, we first utilize both unstructured and structured information to find entities that are related to the ones in the query. We then discuss how to adapt existing feedback methods to use the related entities to improve search quality. Experiment results show that the proposed entity-centric query expansion strategy is more effective to improve the search performance than the state-of-the-art pseudo feedback methods on longer, natural language-like queries with entities.

#index 1919852
#* Location-sensitive resources recommendation in social tagging systems
#@ Chang Wan;Ben Kao;David W. Cheung
#t 2012
#c 1
#% 411762
#% 956544
#% 1035588
#% 1214694
#% 1355024
#% 1417104
#% 1581875
#% 1581877
#% 1594584
#! In social tagging systems, resources such as images and videos are annotated with descriptive words called tags. It has been shown that tag-based resource searching and retrieval is much more effective than content-based retrieval. With the advances in mobile technology, many resources are also geo-tagged with location information. We observe that a traditional tag (word) can carry different semantics at different locations. We study how location information can be used to help distinguish the different semantics of a resource's tags and thus to improve retrieval accuracy. Given a search query, we propose a location-partitioning method that partitions all locations into regions such that the user query carries distinguishing semantics in each region. Based on the identified regions, we utilize location information in estimating the ranking scores of resources for the given query. These ranking scores are learned using the Bayesian Personalized Ranking (BPR) framework. Two algorithms, namely, LTD and LPITF, which apply Tucker Decomposition and Pairwise Interaction Tensor Factorization, respectively for modeling the ranking score tensor are proposed. Through experiments on real datasets, we show that LTD and LPITF outperform other tag-based resource retrieval methods.

#index 1919853
#* Differences in effectiveness across sub-collections
#@ Mark Sanderson;Andrew Turpin;Ying Zhang;Falk Scholer
#t 2012
#c 1
#% 133892
#% 218982
#% 262105
#% 397163
#% 857180
#% 879599
#% 1019124
#% 1074132
#% 1598440
#% 1810980
#! The relative performance of retrieval systems when evaluated on one part of a test collection may bear little or no similarity to the relative performance measured on a different part of the collection. In this paper we report the results of a detailed study of the impact that different sub-collections have on retrieval effectiveness, analyzing the effect over many collections, and with different approaches to sub-dividing the collections. The effect is shown to be substantial, impacting on comparisons between retrieval runs that are statistically significant. Some possible causes for the effect are investigated, and the implications of this work are examined for test collection design and for the strength of conclusions one can draw from experimental results.

#index 1919854
#* Map to humans and reduce error: crowdsourcing for deduplication applied to digital libraries
#@ Mihai Georgescu;Dang Duc Pham;Claudiu S. Firan;Wolfgang Nejdl;Julien Gaugaz
#t 2012
#c 1
#% 170649
#% 420072
#% 577238
#% 810014
#% 913783
#% 1083692
#% 1103296
#% 1201863
#% 1281979
#% 1451182
#% 1452857
#% 1472273
#% 1591365
#% 1746845
#! Detecting duplicate entities, usually by examining metadata, has been the focus of much recent work. Several methods try to identify duplicate entities, while focusing either on accuracy or on efficiency and speed - with still no perfect solution. We propose a combined layered approach for duplicate detection with the main advantage of using Crowdsourcing as a training and feedback mechanism. By using Active Learning techniques on human provided examples, we fine tune our algorithm toward better duplicate detection accuracy. We keep the training cost low by gathering training data on demand for borderline cases or for inconclusive assessments. We apply our simple and powerful methods to an online publication search system: First, we perform a coarse duplicate detection relying on publication signatures in real time. Then, a second automatic step compares duplicate candidates and increases accuracy while adjusting based on both feedback from our online users and from Crowdsourcing platforms. Our approach shows an improvement of 14% over the untrained setting and is at only 4% difference to the human assessors in accuracy.

#index 1919855
#* Full-text citation analysis: enhancing bibliometric and scientific publication ranking
#@ Xiaozhong Liu;Jinsong Zhang;Chun Guo
#t 2012
#c 1
#% 345120
#% 411762
#% 722904
#% 729936
#% 1338553
#! The goal of this paper is to use innovative text and graph mining algorithms along with full-text citation analysis and topic modeling to enhance classical bibliometric analysis and publication ranking. By utilizing citation contexts extracted from a large number of full-text publications, each citation or publication is represented by a probability distribution over a set of predefined topics, where each topic is labeled by an author contributed keyword. We then used publication/citation topic distribution to generate a citation graph with vertex prior and edge transitioning probability distributions. The publication importance score for each given topic is calculated by PageRank with edge and vertex prior distributions. Based on 104 topics (labeled with keywords) and their review papers, the cited publications of each review paper are assumed as "important publications" for ranking evaluation. The result shows that full text citation and publication content prior topic distribution along with the PageRank algorithm can significantly enhance bibliometric analysis and scientific publication ranking performance for academic IR system.

#index 1919856
#* Detecting offensive tweets via topical feature discovery over a large scale twitter corpus
#@ Guang Xiang;Bin Fan;Ling Wang;Jason Hong;Carolyn Rose
#t 2012
#c 1
#% 722904
#% 1478939
#% 1544032
#% 1707548
#% 1710219
#! In this paper, we propose a novel semi-supervised approach for detecting profanity-related offensive content in Twitter. Our approach exploits linguistic regularities in profane language via statistical topic modeling on a huge Twitter corpus, and detects offensive tweets using automatically these generated features. Our approach performs competitively with a variety of machine learning (ML) algorithms. For instance, our approach achieves a true positive rate (TP) of 75.1% over 4029 testing tweets using Logistic Regression, significantly outperforming the popular keyword matching baseline, which has a TP of 69.7%, while keeping the false positive rate (FP) at the same level as the baseline at about 3.77%. Our approach provides an alternative to large scale hand annotation efforts required by fully supervised learning approaches.

#index 1919857
#* Automatic query expansion based on tag recommendation
#@ Vitor Oliveira;Guilherme Gomes;Fabiano Belém;Wladmir Brandão;Jussara Almeida;Nivio Ziviani;Marcos Gonçalves
#t 2012
#c 1
#% 340901
#% 342707
#% 641976
#% 1055739
#% 1227584
#% 1598374
#% 1598437
#! We here propose a new method for expanding entity related queries that automatically filters, weights and ranks candidate expasion terms extracted from Wikipedia articles related to the original query. Our method is based on state-of-the-art tag recommendation methods that exploit heuristic metrics to estimate the descriptive capacity of a given term. Originally proposed for the context of tags, we here apply these recommendation methods to weight and rank terms extracted from multiple fields of Wikipedia articles according to their relevance for the article. We evaluate our method comparing it against three state-of-the-art baselines in three collections. Our results indicate that our method outperforms all baselines in all collections, with relative gains in MAP of up to 14% against the best ones.

#index 1919858
#* The downside of markup: examining the harmful effects of CSS and javascript on indexing today's web
#@ Karl Gyllstrom;Carsten Eickhoff;Arjen P. de Vries;Marie-Francine Moens
#t 2012
#c 1
#% 268079
#% 1450887
#% 1598355
#% 1598358
#! The continued development and maturation of advanced HTML features such as Cascading style sheets (CSS), Javascript, and AJAX, as well as their widespread adoption by browsers, has enabled web pages to flourish with sophistication and interactivity. Unfortunately, this presents challenges to the web search community, as a web page's representation in the browser (i.e., what users see) can diverge dramatically from its raw HTML content (i.e., what search engines index and retrieve). For example, interactive pages may contain content in regions that are not visible before a user action, such as focusing a tab, but which are nonetheless still contained within the raw HTML. We study this divergence by comparing raw HTML to its fully rendered form across a number of metrics spanning presentation, geometry, and content, using a large, representative sample of popular web pages. We find that a large divergence currently exists, and we show via a historical analysis that this divergence has grown more pronounced over the last decade. The general finding of our study is that continuing to index the web via simple HTML parsing will diminish the effectiveness of retrieval on the modern web, and that the IR community should work toward more sophisticated web page processing in indexing technology.

#index 1919859
#* You should read this! let me explain you why: explaining news recommendations to users
#@ Roi Blanco;Diego Ceccarelli;Claudio Lucchese;Raffaele Perego;Fabrizio Silvestri
#t 2012
#c 1
#% 319705
#% 813966
#% 850430
#% 1169569
#% 1450871
#! Recommender systems have become ubiquitous in content-based web applications, from news to shopping sites. Nonetheless, an aspect that has been largely overlooked so far in the recommender system literature is that of automatically building explanations for a particular recommendation. This paper focuses on the news domain, and proposes to enhance effectiveness of news recommender systems by adding, to each recommendation, an explanatory statement to help the user to better understand if, and why, the item can be her interest. We consider the news recommender system as a black-box, and generate different types of explanations employing pieces of information associated with the news. In particular, we engineer text-based, entity-based, and usage-based explanations, and make use of a Markov Logic Networks to rank the explanations on the basis of their effectiveness. The assessment of the model is conducted via a user study on a dataset of news read consecutively by actual users. Experiments show that news recommender systems can greatly benefit from our explanation module as it allows users to discriminate between interesting and not interesting news in the majority of the cases.

#index 1919860
#* Characterizing web search queries that match very few or no results
#@ Ismail Sengor Altingovde;Roi Blanco;Berkant Barla Cambazoglu;Rifat Ozcan;Erdem Sarigil;Özgür Ulusoy
#t 2012
#c 1
#% 878624
#% 879613
#% 956646
#% 1292474
#% 1450900
#% 1482231
#% 1598519
#! Despite the continuous efforts to improve the web search quality, a non-negligible fraction of user queries end up with very few or even no matching results in leading web search engines. In this work, we provide a detailed characterization of such queries based on an analysis of a real-life query log. Our experimental setup allows us to characterize the queries with few/no results and compare the mechanisms employed by the major search engines in handling them.

#index 1919861
#* A unified optimization framework for auction and guaranteed delivery in online advertising
#@ Konstantin Salomatin;Tie-Yan Liu;Yiming Yang
#t 2012
#c 1
#% 88364
#% 959539
#% 963359
#% 1222625
#% 1426653
#! This paper proposes a new unified optimization framework combining pay-per-click auctions and guaranteed delivery in sponsored search. Advertisers usually have different (and sometimes mixed) marketing goals: brand awareness and direct response. Different mechanisms are good at addressing different goals, e.g., guaranteed delivery was often used to build brand awareness and pay-per-click auctions was widely used for direct marketing. Our new method accommodates both in a unified framework, with the search engine revenue as an optimization objective. In this way, we can target a guaranteed number of ad clicks (or impressions) per campaign for advertisers willing to pay a premium and enable keyword auctions for all others. Specifically, we formulate this joint optimization problem using linear programming and a column generation strategy for efficiency. To select the best column (a ranked list of ads) given a query, we propose a novel dynamic programming algorithm that takes the special structure of the ad allocation and pricing mechanisms into account. We have tested the proposed framework and the algorithms on real ad data obtained from a commercial search engine. The results demonstrate that our proposed approach can outperform several baselines in guaranteeing the number of clicks for the given advertisers, and in increasing the total revenue for the search engine.

#index 1919862
#* Query recommendation for children
#@ Sergio Duarte Torres;Djoerd Hiemstra;Ingmar Weber;Pavel Serdyukov
#t 2012
#c 1
#% 575733
#% 987222
#% 989578
#% 1130854
#% 1130855
#% 1173699
#% 1190090
#% 1210687
#% 1373775
#% 1450874
#% 1450995
#% 1482194
#% 1482344
#% 1560359
#% 1641981
#% 1642156
#! One of the biggest problems that children experience while searching the web occurs during the query formulation process. Children have been found to struggle formulating queries based on keywords given their limited vocabulary and their difficulty to choose the right keywords. In this work we propose a method that utilizes tags from social media to suggest queries related to children topics. Concretely we propose a simple yet effective approach to bias a random walk defined on a bipartite graph of web resources and tags through keywords that are more commonly used to describe resources for children. We evaluate our method using a large query log sample of queries aimed at retrieving information for children. We show that our method outperforms query suggestions of state-of-the-art search engines and state-of-the art query suggestions based on random walks.

#index 1919863
#* Modeling browsing behavior for click analysis in sponsored search
#@ Azin Ashkan;Charles L. A. Clarke
#t 2012
#c 1
#% 818221
#% 906824
#% 949162
#% 956546
#% 1035578
#% 1055687
#% 1074092
#% 1190055
#% 1190056
#% 1292472
#% 1355048
#% 1560356
#% 1606083
#! Clickthrough rate provides a fundamental measure of advertising quality, which is widely used in ad selection strategies. However, ads placed in contexts where they are rarely viewed, or where users are unlikely to be interested in commercial results, may receive few clicks regardless of their quality. In this paper, we gain insight into user browsing and click behavior for the purpose of click analysis in sponsored search domain. The list of ads displayed on a page, the user's initial motivation to browse this list, and the persistence of the user are among the contextual factors considered in this paper. We propose a probabilistic model for user's browsing and click behavior using these contextual factors. To evaluate the performance of the model, we compare it with state-of-the-art methods. The experimental results confirm that these contextual factors can better reflect user browsing and click behavior in sponsored search.

#index 1919864
#* Sentiment-focused web crawling
#@ A. Gural Vural;B. Barla Cambazoglu;Pinar Senkul
#t 2012
#c 1
#% 281251
#% 723399
#% 728415
#% 783694
#% 788942
#% 1051058
#% 1127964
#% 1292510
#% 1297083
#% 1497569
#% 1558464
#% 1642034
#% 1693925
#! The sentiments and opinions that are expressed in web pages towards objects, entities, and products constitute an important portion of the textual content available in the Web. Despite the vast interest in sentiment analysis and opinion mining, somewhat surprisingly, the discovery of the sentimental or opinionated web content is mostly ignored. This work aims to fill this gap and address the problem of quickly discovering and fetching the sentimental content present in the Web. To this end, we design a sentiment-focused web crawling framework for faster discovery and retrieval of such content. In particular, we propose different sentiment-focused web crawling strategies that prioritize discovered URLs based on their predicted sentiment scores. Through simulations, these strategies are shown to achieve considerable performance improvement over general-purpose web crawling strategies in discovering sentimental content.

#index 1919865
#* User guided entity similarity search using meta-path selection in heterogeneous information networks
#@ Xiao Yu;Yizhou Sun;Brandon Norick;Tiancheng Mao;Jiawei Han
#t 2012
#c 1
#% 577273
#% 956551
#% 987243
#% 1457044
#% 1635098
#% 1635140
#% 1688473
#! With the emergence of web-based social and information applications, entity similarity search in information networks, aiming to find entities with high similarity to a given query entity, has gained wide attention. However, due to the diverse semantic meanings in heterogeneous information networks, which contain multi-typed entities and relationships, similarity measurement can be ambiguous without context. In this paper, we investigate entity similarity search and the resulting ambiguity problems in heterogeneous information networks. We propose to use a meta-path-based ranking model ensemble to represent semantic meanings for similarity queries, exploit the possibility of using using user-guidance to understand users query. Experiments on real-world datasets show that our framework significantly outperforms competitor methods.

#index 1919866
#* User activity profiling with multi-layer analysis
#@ Hongxia Jin
#t 2012
#c 1
#% 438137
#% 722904
#% 788094
#% 869480
#% 1272187
#% 1650298
#! In this paper, we are interested in discovering semantically meaningful communities from a single user's perspective. We define a multi-layer analysis problem to derive a user's activity profile. Such an activity profile would include what activity areas a user is involved with, how important each activity is to the user, and who else is involved with the user on each activity as well as each participant's participation level. We believe a semantically meaningful community (corresponding to an activity area) must also consider the topics of the social messages rather than only the social links. While it is possible to use a hybrid approach based on traditional topic modeling, in this paper we propose a unified user modeling approach based on direct clustering over the social messages taking into considerations of both social connections and topics of social messages. Our clustering algorithm can be performed in a unified way in a unsupervised fashion as well as semi-supervised fashion when the user wants to give our algorithm some seeding inputs on his viewpoints. Moreover, when the new data comes, our algorithm can perform incremental updates on the new data without re-clustering the old data. Our experiments on social media datasets available from both within an enterprise and public social network demonstrate the effectiveness of our approach.

#index 1919867
#* GTE: a distributional second-order co-occurrence approach to improve the identification of top relevant dates in web snippets
#@ Ricardo Campos;Gaël Dias;Alípio Jorge;Célia Nunes
#t 2012
#c 1
#% 983604
#% 1227692
#% 1253682
#% 1269882
#% 1344853
#% 1631304
#% 1679827
#% 1697416
#% 1730996
#% 1730997
#! In this paper, we present an approach to identify top relevant dates in Web snippets with respect to a given implicit temporal query. Our approach is two-fold. First, we propose a generic temporal similarity measure called GTE, which evaluates the temporal similarity between a query and a date. Second, we propose a classification model to accurately relate relevant dates to their corresponding query terms and withdraw irrelevant ones. We suggest two different solutions: a threshold-based classification strategy and a supervised classifier based on a combination of multiple similarity measures. We evaluate both strategies over a set of real-world text queries and compare the performance of our Web snippet approach with a query log approach over the same set of queries. Experiments show that determining the most relevant dates of any given implicit temporal query can be improved with GTE combined with the second order similarity measure InfoSimba, the Dice coefficient and the threshold-based strategy compared to (1) first-order similarity measures and (2) the query log based approach.

#index 1919868
#* Stochastic simulation of time-biased gain
#@ Mark D. Smucker;Charles L. A. Clarke
#t 2012
#c 1
#% 616528
#% 759078
#% 851306
#% 1227640
#% 1292528
#% 1450876
#% 1450903
#% 1482378
#% 1536510
#% 1631302
#% 1879002
#! Time-biased gain provides a unifying framework for information retrieval evaluation, generalizing many traditional effectiveness measures while accommodating aspects of user behavior not captured by these measures. By using time as a basis for calibration against actual user data, time-biased gain can reflect aspects of the search process that directly impact user experience, including document length, near-duplicate documents, and summaries. Unlike traditional measures, which must be arbitrarily normalized for averaging purposes, time-biased gain is reported in meaningful units, such as the total number of relevant documents seen by the user. In prior work, we proposed and validated a closed-form equation for estimating time-biased gain, explored its properties, and compared it to standard approaches. In this paper, we use stochastic simulation to numerically approximate time-biased gain. Stochastic simulation provides greater flexibility that will allow us, in future work, to easily accommodate different types of user behavior and increase the realism of the effectiveness measure.

#index 1919869
#* SonetRank: leveraging social networks to personalize search
#@ Abhijith Kashyap;Reza Amini;Vagelis Hristidis
#t 2012
#c 1
#% 348173
#% 413615
#% 577273
#% 577329
#% 754126
#% 818259
#% 869536
#% 879686
#% 987222
#% 1016176
#% 1074070
#% 1166492
#% 1292590
#% 1357833
#% 1536505
#! Earlier works on personalized Web search focused on the click-through graphs, while recent works leverage social annotations, which are often unavailable. On the other hand, many users are members of the social networks and subscribe to social groups. Intuitively, users in the same group may have similar relevance judgments for queries related to these groups. SonetRank utilizes this observation to personalize the Web search results based on the aggregate relevance feedback of the users in similar groups. SonetRank builds and maintains a rich graph-based model, termed Social Aware Search Graph, consisting of groups, users, queries and results click-through information. SonetRank's personalization scheme learns in a principled way to leverage the following three signals, of decreasing strength: the personal document preferences of the user, of the users of her social groups relevant to the query, and of the other users in the network. SonetRank also uses a novel approach to measure the amount of personalization with respect to a user and a query, based on the query-specific richness of the user's social profile. We evaluate SonetRank with users on Amazon Mechanical Turk and show a significant improvement in ranking compared to state-of-the-art techniques.

#index 1919870
#* Predicting web search success with fine-grained interaction data
#@ Qi Guo;Dmitry Lagun;Eugene Agichtein
#t 2012
#c 1
#% 805200
#% 1048694
#% 1166521
#% 1355038
#% 1384641
#% 1450833
#% 1450902
#% 1573487
#% 1598367
#% 1598368
#% 1598370
#% 1746855
#% 1879012
#% 1879187
#! Detecting and predicting searcher success is essential for automatically evaluating and improving Web search engine performance. In the past, Web searcher behavior data, such as result clickthrough, dwell time, and query reformulation sequences, have been successfully used for a variety of tasks, including prediction of success in a search session. However, the effectiveness of the previous approaches has been limited, as they tend to ignore how searchers actually view and interact with the visited pages. We show that fine-grained interactions, such as mouse cursor movements and scrolling, provide additional clues for better predicting success of a search session as a whole. To this end, we identify patterns of examination and interaction behavior that correspond to search success, and design a new Fine-grained Session Behavior (FSB) model to capture these patterns. Our experimental results show that FSB is significantly more effective than the state-of-the-art approaches that do not use these additional interaction data.

#index 1919871
#* Multi-session re-search: in pursuit of repetition and diversification
#@ Sarah K. Tyler;Yi Zhang
#t 2012
#c 1
#% 186340
#% 187999
#% 342961
#% 581916
#% 765412
#% 766447
#% 778332
#% 803556
#% 805898
#% 869501
#% 869651
#% 987211
#% 998797
#% 1074071
#% 1130878
#% 1130999
#% 1270766
#% 1292473
#% 1314946
#% 1355035
#% 1392483
#% 1403461
#% 1450832
#% 1482355
#% 1712595
#% 1715615
#! Search engine users regularly re-issue queries that are the same or similar to ones they have previously issued. In this paper we study this act of query re-issuing, called re-search, focusing on multi session re-searching from an information seeking perspective. By focusing on the series of repeat or similar queries where the user shows a continued interest, new patterns of behavior not previously seen arise. We find that the well-studied re-finding behavior is only a piece of the re-search puzzle, and that even amidst repeated re-findings users exhibit diversification and novelty seeking behaviours for many re-search queries. This suggests diversity and re-finding behaviors should be jointly modelled and captured in evaluation measures, instead of being studied as two separate problems as is seen in many previous approaches.

#index 1919872
#* Mining sentiment terminology through time
#@ Hadi Amiri;Tat-Seng Chua
#t 2012
#c 1
#% 722308
#% 939897
#% 1035590
#% 1063632
#% 1133663
#% 1471219
#% 1693880
#! The correspondence between sentiment terminology and the active language used for expressing opinions is a crucial prerequisite for effective sentiment analysis. Mining sentiment terminology includes the detection of new opinion words as well as inferring their polarities. In this paper, we first propose a novel approach based on the interchangeability characteristic of words to detect new opinion words through time. We then show that the current non-time-based polarity inference approaches may assign opposite polarity to the same opinion word at different times. To tackle this issue, we consider the polarity scores computed at different times as polarity evidences (with the possibility of flawed evidences) and combine them to compute a globally correct polarity score for each opinion word. The experiments show that our approach is effective both in terms of the quality of the discovered new opinion words as well as its ability in inferring their polarities through time. Furthermore, we show the application of mining sentiment terminology through time in the sentiment classification (SC) task. The experiments show that mining more recent new opinion words leads to greater improvement in the performance of SC. To the best of our knowledge, this is the first work that investigates "time" as an important factor in mining sentiment terminology.

#index 1919873
#* Theme chronicle model: chronicle consists of timestamp and topical words over each theme
#@ Noriaki Kawamae
#t 2012
#c 1
#% 876017
#% 1292563
#% 1536536
#% 1536565
#% 1560388
#% 1605967
#! This paper presents a topic model that discovers the correlation patterns in a given time-stamped document collection and how these patterns evolve over time. Our proposal, the theme chronicle model (TCM) divides traditional topics into temporal and stable topics to detect the change of each theme over time; previous topic models ignore these differences and characterize trends as merely bursts of topics. TCM introduces a theme topic (stable topic), a trend topic (temporal topic), timestamps, and a latent switch variable in each token to realize these differences. Its topic layers allow TCM to capture not only word co-occurrence patterns in each theme, but also word co-occurrence patterns at any given time in each theme as trends. Experiments on various data sets show that the proposed model is useful as a generative model to discover fine-grained tightly coherent topics, takes advantage of previous models, and then assigns values for new documents.

#index 1919874
#* Fast top-k similarity queries via matrix compression
#@ Yucheng Low;Alice X. Zheng
#t 2012
#c 1
#% 198335
#% 213786
#% 228097
#% 321455
#% 730065
#% 891559
#% 1023422
#% 1181253
#% 1385997
#% 1472270
#! In this paper, we propose a novel method to efficiently compute the top-K most similar items given a query item, where similarity is defined by the set of items that have the highest vector inner products with the query. The task is related to the classical k-Nearest-Neighbor problem, and is widely applicable in a number of domains such as information retrieval, online advertising and collaborative filtering. Our method assumes an in-memory representation of the dataset and is designed to scale to query lengths of 100,000s of terms. Our algorithm uses a generalized Holder's inequality to upper bound the inner product with the norms of the constituent vectors. We also propose a novel compression scheme that computes bounds for groups of candidate items, thereby speeding up computation and minimizing memory requirements per query. We conduct extensive experiments on the publicly available Wikipedia dataset, and demonstrate that, with a memory overhead of 21%, our method can provide 1-3 orders of magnitude improvement in query run-time compared to naive methods and state of the art competing methods. Our median top-10 word query time is 25 us on 7.5 million words and 2.3 million documents.

#index 1919875
#* Top-k retrieval using conditional preference networks
#@ Hongbing Wang;Xuan Zhou;Wujin Chen;Peisheng Ma
#t 2012
#c 1
#% 333854
#% 465167
#% 479816
#% 875002
#% 993957
#% 1272026
#% 1279242
#% 1289373
#% 1650274
#! This paper considers top-k retrieval using Conditional Preference Network (CP-Net). As a model for expressing user preferences on multiple mutually correlated attributes, CP-Net is of great interest for decision support systems. However, little work has addressed how to conduct efficient data retrieval using CP-Nets. This paper presents an approach to efficiently retrieve the most preferred data items based on a user's CP-Net. The proposed approach consists of a top-k algorithm and an indexing scheme. We conducted extensive experiments to compare our approach against a baseline top-k method - sequential scan. The results show that our approach outperform sequential scan in several circumstances.

#index 1919876
#* Sort-based query-adaptive loading of R-trees
#@ Daniar Achakeev;Bernhard Seeger;Peter Widmayer
#t 2012
#c 1
#% 137887
#% 153260
#% 213975
#% 273887
#% 286237
#% 415957
#% 462059
#% 479473
#% 479648
#% 482092
#% 1024519
#% 1217207
#% 1520209
#! Bulk-loading of R-trees has been an important problem in academia and industry for more than twenty years. Current algorithms create R-trees without any information about the expected query profile. However, query profiles are extremely useful for the design of efficient indexes. In this paper, we address this deficiency and present query-adaptive algorithms for building R-trees optimally designed for a given query profile. Since optimal R-tree loading is NP-hard (even without tuning the structure to a query profile), we provide efficient, easy to implement heuristics. Our sort-based algorithms for query-adaptive loading consist of two steps: First, sorting orders are identified resulting in better R-trees than those obtained from standard space-filling curves. Second, for a given sorting order, we propose a dynamic programming algorithm for generating R-trees in linear runtime. Our experimental results confirm that our algorithms generally create significantly better R-trees than the ones obtained from standard sort-based loading algorithms, even when the query profile is unknown.

#index 1919877
#* Efficient logging for enterprise workloads on column-oriented in-memory databases
#@ Johannes Wust;Joos-Hendrick Boese;Frank Renkes;Sebastian Blessing;Jens Krueger;Hasso Plattner
#t 2012
#c 1
#% 117
#% 12638
#% 442834
#% 462497
#% 465019
#% 644136
#% 893146
#% 1022298
#% 1545215
#% 1581868
#% 1594617
#% 1697286
#! The introduction of a 64 bit address space in commodity operating systems and the constant drop in hardware prices made large capacities of main memory in the order of terabytes technically feasible and economically viable. Especially column-oriented in-memory databases are a promising platform to improve data management for enterprise applications. As in-memory databases hold the primary persistence in volatile memory, some form of recovery mechanism is required to prevent potential data loss in case of failures. Two desirable characteristics of any recovery mechanism are (1) that it has a minimal impact on the running system, and (2) that the system recovers quickly and without any data loss after a failure. This paper introduces an efficient logging mechanism for dictionary-compressed column structures that addresses these two characteristics by (1) reducing the overall log size by writing dictionary-compressed values and (2) allowing for parallel writing and reading of log files. We demonstrate the efficiency of our logging approach by comparing the resulting log-file size with traditional logical logging on a workload produced by a productive enterprise system.

#index 1919878
#* Schema-free structured querying of DBpedia data
#@ Lushan Han;Tim Finin;Anupam Joshi
#t 2012
#c 1
#% 748499
#% 939944
#% 1250667
#% 1409954
#! We need better ways to query large linked data collections such as DBpedia. Using the SPARQL query language requires not only mastering its syntax but also understanding the RDF data model, large ontology vocabularies and URIs for denoting entities. Natural language interface systems address the problem, but are still subjects of research. We describe a compromise in which non-experts specify a graphical query "skeleton" and annotate it with freely chosen words, phrases and entity names. The combination reduces ambiguity and allows the generation of an interpretation that can be translated into SPARQL. Key research contributions are the robust methods that combine statistical association and semantic similarity to map user terms to the most appropriate classes and properties in the underlying ontology.

#index 1919879
#* Discovering conditional inclusion dependencies
#@ Jana Bauckmann;Ziawasch Abedjan;Ulf Leser;Heiko Müller;Felix Naumann
#t 2012
#c 1
#% 481290
#% 768943
#% 1022222
#% 1054480
#% 1063725
#% 1127381
#% 1127443
#% 1166724
#% 1267409
#% 1288161
#% 1540311
#% 1573139
#! Data dependencies are used to improve the quality of a database schema, to optimize queries, and to ensure consistency in a database. Conditional dependencies have been introduced to analyze and improve data quality. A conditional dependency is a dependency with a limited scope defined by conditions over one or more attributes. Only the matching part of the instance must adhere to the dependency. In this paper we focus on conditional inclusion dependencies (CINDs).We generalize the definition of CINDs, distinguishing covering and completeness conditions. We present a new use case for such CINDs showing their value for solving complex data quality tasks. Further, we propose efficient algorithms that identify covering and completeness conditions conforming to given quality thresholds. Our algorithms choose not only the condition values but also the condition attributes automatically. Finally, we show that our approach efficiently provides meaningful and helpful results for our use case.

#index 1919880
#* Diversifying query results on semi-structured data
#@ Mahbub Hasan;Abdullah Mueen;Vassilis Tsotras;Eamonn Keogh
#t 2012
#c 1
#% 66654
#% 1022207
#% 1328135
#% 1450870
#% 1594636
#! Queries on the web can easily result in a large number of results. Result Diversification, a process by which the query provides the k most diverse set of matches, enables the user to better understand/explore such large results. Computing the diverse subset from a large set of results needs a massive number of pair-wise distance computations as well as finding the subset that maximizes the total pair-wise distance, which is NP-hard and requires efficient approximate algorithm. The problem becomes more difficult when querying semi-structured data, since diversity can occur not only in the document content but also (and more importantly) in the document structure; thus one needs to efficiently measure the structural differences between results. The tree edit distance is the standard choice but, is too expensive for large result sets. Moreover, the generalized tree edit distance ignores the context of the query and also the content of the documents resulting in poor diversification. We present a novel algorithm for meaningful diversification that considers both the structural context of the query and the content of the matched results while computing pair-wise distances. Our algorithm is an order of magnitude faster than the tree edit distance with an elegant worst case guarantee. We also present a novel algorithm that finds the top-k diverse subset of matches in time linear on the size of the result-set. We experimentally demonstrate the utility of our algorithms as a plugin for standard query processors without introducing large error and latency to the output.

#index 1919881
#* LINDA: distributed web-of-data-scale entity matching
#@ Christoph Böhm;Gerard de Melo;Felix Naumann;Gerhard Weikum
#t 2012
#c 1
#% 913783
#% 915340
#% 924747
#% 1292570
#% 1314445
#% 1333469
#% 1538763
#% 1560363
#% 1594619
#% 1642263
#% 1654048
#% 1693866
#% 1708912
#% 1846712
#% 1903467
#! Linked Data has emerged as a powerful way of interconnecting structured data on the Web. However, the cross-linkage between Linked Data sources is not as extensive as one would hope for. In this paper, we formalize the task of automatically creating "sameAs" links across data sources in a globally consistent manner. Our algorithm, presented in a multi-core as well as a distributed version, achieves this link generation by accounting for joint evidence of a match. Experiments confirm that our system scales beyond 100 million entities and delivers highly accurate results despite the vast heterogeneity and daunting scale.

#index 1919882
#* SliceSort: efficient sorting of hierarchical data
#@ Quoc Trung Tran;Chee-Yong Chan
#t 2012
#c 1
#% 210212
#% 252608
#% 659923
#% 742561
#% 745445
#% 867058
#% 1127446
#! Sorting is a fundamental operation in data processing. While the problem of sorting flat data records has been extensively studied, there is very little work on sorting hierarchical data such as XML documents. Existing hierarchy-aware sorting approaches for hierarchical data are based on creating sorted subtrees as initial sorted runs and merging sorted subtrees to create the sorted output using either explicit pointers or absolute node key comparisons for merging subtrees. In this paper, we propose SliceSort, a novel, level-wise sorting technique for hierarchical data that avoids the drawbacks of subtree-based sorting techniques. Our experimental performance evaluation shows that SliceSort outperforms the state-of-art approach, HErMeS, by up to a factor of 27%.

#index 1919883
#* Efficient buffer management for piecewise linear representation of multiple data streams
#@ Qing Xie;Jia Zhu;Mohamed A. Sharaf;xiaofang zhou;Chaoyi Pang
#t 2012
#c 1
#% 68724
#% 466506
#% 765403
#! Piecewise Linear Representation (PLR) has been a widely used method for approximating data streams in the form of compact line segments. The buffer-based approach to PLR enables a semi-global approximation which relies on the aggregated processing of batches of streamed data so that to adjust and improve the approximation results. However, one challenge towards applying the buffer-based approach is allocating the necessary memory resources for stream buffering. This challenge is further complicated in a multi-stream environment where multiple data streams are competing for the available memory resources, especially in resource-constrained systems such as sensors and mobile devices. In this paper, we address precisely those challenges mentioned above and propose efficient buffer management techniques for the PLR of multiple data streams. In particular, we propose a new dynamic approach called Dynamic Buffer Management with Error Monitoring (DBMEM), which leverages the relationship between the buffer demands of each data stream and its exhibited pattern of data values towards estimating its sufficient buffer size. This enables DBMEM to provide a global buffer allocation strategy that maximizes the overall PLR approximation quality for multiple data streams as shown by our experimental results.

#index 1919884
#* On skyline groups
#@ Chengkai Li;Nan Zhang;Naeemul Hassan;Sundaresan Rajasekaran;Gautam Das
#t 2012
#c 1
#% 288976
#% 480671
#% 481290
#% 806212
#% 824670
#% 864451
#% 993954
#% 1206642
#% 1214668
#% 1482238
#% 1594608
#! We formulate and investigate the novel problem of finding the skyline k-tuple groups from an n-tuple dataset - i.e., groups of k tuples which are not dominated by any other group of equal size, based on aggregate-based group dominance relationship. The major technical challenge is to identify effective anti-monotonic properties for pruning the search space of skyline groups. To this end, we show that the anti-monotonic property in the well-known Apriori algorithm does not hold for skyline group pruning. We then identify order-specific property which applies to SUM, MIN, and MAX and weak candidate-generation property which applies to MIN and MAX only. Experimental results on both real and synthetic datasets verify that the proposed algorithms achieve orders of magnitude performance gain over a baseline method.

#index 1919885
#* Finding the optimal path over multi-cost graphs
#@ Yajun Yang;Jeffrey Xu Yu;Hong Gao;Jianzhong Li
#t 2012
#c 1
#% 421285
#% 479816
#% 1181255
#% 1504829
#% 1770357
#% 1846748
#! Shortest path query is an important problem in graphs and has been well-studied. However, most approaches for shortest path query are based on single-cost (weight) graphs. In this paper, we introduce the definition of multi-cost graph and study a novel query: the optimal path query over multi-cost graphs. We propose a best-first branch and bound search algorithm with two optimizing strategies. Furthermore, we propose a novel index named k-cluster index to make our method more space and time efficient for large graphs. We discuss how to construct and utilize k-cluster index. We confirm the effectiveness and efficiency of our algorithms using real-life datasets in experiments.

#index 1919886
#* An efficient index for massive IOT data in cloud environment
#@ Youzhong Ma;Jia Rao;Weisong Hu;Xiaofeng Meng;Xu Han;Yu Zhang;Yunpeng Chai;Chunqiu Liu
#t 2012
#c 1
#% 340176
#% 1054227
#% 1296919
#% 1426551
#% 1490248
#% 1618531
#% 1643332
#! The Internet of Things (IOT) has been widely applied in many fields, while the IOT data are always large volume, update frequently and inherently multi-dimensional, these characteristics bring big challenges to the traditional DBMSs. The traditional DBMSs have rich functionality and can deal with multi-attributes access efficiently, they can not scale good enough to deal with large volume data and can not support high insert throughput. The cloud-based database systems have good scalability, but they don't support multi-dimensional access natively.In order to deal with the large volume of IOT data, we propose an update and query efficient index framework (UQE-Index) based on key-value store that can support high insert throughput and provide efficient multi-dimensional query simultaneously. We implemented a prototype based on HBase and did comprehensive experiments to test our solution's scalability and efficiency.

#index 1919887
#* Clustering Wikipedia infoboxes to discover their types
#@ Thanh Hoang Nguyen;Huong Dieu Nguyen;Viviane Moreira;Juliana Freire
#t 2012
#c 1
#% 313959
#% 387427
#% 783472
#% 864509
#% 956564
#% 991230
#% 1055735
#% 1183373
#% 1288161
#% 1471589
#! Wikipedia has emerged as an important source of structured information on the Web. But while the success of Wikipedia can be attributed in part to the simplicity of adding and modifying content, this has also created challenges when it comes to using, querying, and integrating the information. Even though authors are encouraged to select appropriate categories and provide infoboxes that follow pre-defined templates, many do not follow the guidelines or follow them loosely. This leads to undesirable effects, such as template duplication, heterogeneity, and schema drift. As a step towards addressing this problem, we propose a new unsupervised approach for clustering Wikipedia infoboxes. Instead of relying on manually assigned categories and template labels, we use the structured information available in infoboxes to group them and infer their entity types. Experiments using over 48,000 infoboxes indicate that our clustering approach is effective and produces high quality clusters.

#index 1919888
#* CloST: a hadoop-based storage system for big spatio-temporal data analytics
#@ Haoyu Tan;Wuman Luo;Lionel M. Ni
#t 2012
#c 1
#% 318051
#% 321455
#% 427199
#% 480473
#% 555050
#% 723279
#% 963669
#% 1046418
#% 1518201
#% 1594639
#! During the past decade, various GPS-equipped devices have generated a tremendous amount of data with time and location information, which we refer to as big spatio-temporal data. In this paper, we present the design and implementation of CloST, a scalable big spatio-temporal data storage system to support data analytics using Hadoop. The main objective of CloST is to avoid scan the whole dataset when a spatio-temporal range is given. To this end, we propose a novel data model which has special treatments on three core attributes including an object id, a location and a time. Based on this data model, CloST hierarchically partitions data using all core attributes which enables efficient parallel processing of spatio-temporal range scans. According to the data characteristics, we devise a compact storage structure which reduces the storage size by an order of magnitude. In addition, we proposes scalable bulk loading algorithms capable of incrementally adding new data into the system. We conduct our experiments using a very large GPS log dataset and the results show that CloST has fast data loading speed, desirable scalability in query processing, as well as high data compression ratio.

#index 1919889
#* Keyword-based k-nearest neighbor search in spatial databases
#@ Guoliang Li;Jing Xu;Jianhua Feng
#t 2012
#c 1
#% 838407
#% 982560
#% 1206801
#% 1206997
#% 1328137
#% 1486233
#% 1846749
#% 1848110
#! With the ever-increasing number of spatio-textual objects, many applications require to find objects close to a given query point in spatial databases. In this paper, we study the problem of keyword-based k-nearest neighbor search in spatial databases, which, given a query point and a set of keywords, finds k-nearest neighbors of the query point that contain all query keywords. To efficiently answer such queries, we propose a new indexing framework by integrating a spatial component and a textual component, which can efficiently prune search space in terms of both spatial information and textual descriptions. We develop effective index structures and pruning techniques to improve query performance. Experimental results show that our approach significantly outperforms state-of-the-art methods.

#index 1919890
#* Credibility-based product ranking for C2C transactions
#@ Rong Zhang;Chao Feng Sha;Min Qi Zhou;Ao Ying Zhou
#t 2012
#c 1
#% 211044
#% 577246
#% 769892
#% 1176947
#% 1214645
#% 1261574
#% 1400002
#% 1560411
#% 1633079
#! A fundamental issue for C2C transactions is how to rank the products based on the reviews written by the previous customers. In this paper, we present an approach to improve products ranking by tackling the noisy ratings that exist in the practical systems. The first problem is the credibility of the customers. We design an iterative algorithm to measure the customer credibility. In the algorithm, we use a feedback strategy to increase or decrease the customer credibility. We increase the credibility for a customer if the customer gives a high (low) score to a good (bad) product and decrease the value if the customer gives a low (high) score to a good (bad) product. The second problem is the inconsistency between the review comments and scores. To deal with it, we train a classifier on a training data that is constructed automatically. The trained classifier is used to predict the scores of the comments. Finally, we calculate the scores of products by considering the customer credibility and the predicted scores. The experimental results show that our proposed approach provides better products ranking than the baseline systems.

#index 1919891
#* Location selection for utility maximization with capacity constraints
#@ Yu Sun;Jin Huang;Yueguo Chen;Rui Zhang;Xiaoyong Du
#t 2012
#c 1
#% 300163
#% 480661
#% 824730
#% 893141
#% 1022250
#% 1063470
#% 1127435
#% 1328203
#% 1370255
#% 1594580
#% 1642258
#% 1648946
#% 1846728
#% 1910904
#! Given a set of client locations, a set of facility locations where each facility has a service capacity, and the assumptions that: (i) a client seeks service from its nearest facility; (ii) a facility provides service to clients in the order of their proximity, we study the problem of selecting all possible locations such that setting up a new facility with a given capacity at these locations will maximize the number of served clients. This problem has wide applications in practice, such as setting up new distribution centers for online sales business and building additional base stations for mobile subscribers. We formulate the problem as location selection query for utility maximization. After applying three pruning rules to a baseline solution,we obtain an efficient algorithm to answer the query. Extensive experiments confirm the efficiency of our proposed algorithm.

#index 1919892
#* Efficient estimation of dynamic density functions with an application to outlier detection
#@ Abdulhakim Ali Qahtan;Xiangliang Zhang;Suojin Wang
#t 2012
#c 1
#% 300136
#% 479791
#% 587754
#% 946461
#% 981948
#% 1068964
#% 1490117
#! In this paper, we propose a new method to estimate the dynamic density over data streams, named KDE-Track as it is based on a conventional and widely used Kernel Density Estimation (KDE) method. KDE-Track can efficiently estimate the density with linear complexity by using interpolation on a kernel model, which is incrementally updated upon the arrival of streaming data. Both theoretical analysis and experimental validation show that KDE-Track outperforms traditional KDE and a baseline method Cluster-Kernels on estimation accuracy of the complex density structures in data streams, computing time and memory usage. KDE-Track is also demonstrated on timely catching the dynamic density of synthetic and real-world data. In addition, KDE-Track is used to accurately detect outliers in sensor data and compared with two existing methods developed for detecting outliers and cleaning sensor data.

#index 1919893
#* A positional access method for relational databases
#@ Dongzhe Ma;Jianhua Feng;Guoliang Li
#t 2012
#c 1
#% 872
#% 208047
#% 210174
#% 252608
#% 264263
#% 286258
#% 317933
#% 318455
#% 403195
#% 481304
#% 824697
#% 1063542
#% 1217169
#% 1369339
#% 1426547
#! Most commercial database management systems sort tuples of a relation by their primary keys for the purpose of supporting efficient insertions, deletions, and updates. However, primary keys are usually auto-generated integers, which bear little useful information about user data. Secondary indexes have to be created sometimes to help retrieve tuples by columns other than the primary key. Evidently, a better solution is to sort the data by columns that appear frequently in retrieval conditions. Unfortunately, this method does not work, at least not immediately, when the relation is vertically partitioned, which is a popular technique to reduce I/O overhead, since it is difficult to keep tuples of two partitions in exactly the same order unless the sorting columns are replicated, which again wastes storage space and disk bandwidth unnecessarily. In this paper, we introduce a positional access method that allows a partition to be sorted by another one but incurs little storage overhead and provide details about how to improve its performance.

#index 1919894
#* Real-time aggregate monitoring with differential privacy
#@ Liyue Fan;Li Xiong
#t 2012
#c 1
#% 1061644
#% 1217148
#% 1426323
#% 1426563
#% 1489408
#% 1496267
#% 1523886
#% 1581865
#% 1595893
#% 1740518
#% 1760888
#% 1846817
#! Sharing real-time aggregate statistics of private data has given much benefit to the public to perform data mining for understanding important phenomena, such as Influenza outbreaks and traffic congestion. However, releasing time-series data with standard differential privacy mechanism has limited utility due to high correlation between data values. We propose FAST, an adaptive system to release real-time aggregate statistics under differential privacy with improved utility. To minimize overall privacy cost, FAST adaptively samples long time-series according to detected data dynamics. To improve the accuracy of data release per time stamp, filtering is used to predict data values at non-sampling points and to estimate true values from noisy observations at sampling points. Our experiments with three real data sets confirm that FAST improves the accuracy of time-series release and has excellent performance even under very small privacy cost.

#index 1919895
#* Efficient distributed locality sensitive hashing
#@ Bahman Bahmani;Ashish Goel;Rajendra Shinde
#t 2012
#c 1
#% 249321
#% 249322
#% 347225
#% 479649
#% 479973
#% 749529
#% 762054
#% 847166
#% 875957
#% 956521
#% 963669
#% 1022281
#% 1181276
#% 1581928
#% 1707459
#! Distributed frameworks are gaining increasingly widespread use in applications that process large amounts of data. One important example application is large scale similarity search, for which Locality Sensitive Hashing (LSH) has emerged as the method of choice, specially when the data is high-dimensional. To guarantee high search quality, the LSH scheme needs a rather large number of hash tables. This entails a large space requirement, and in the distributed setting, with each query requiring a network call per hash bucket look up, also a big network load. Panigrahy's Entropy LSH scheme significantly reduces the space requirement but does not help with (and in fact worsens) the search network efficiency. In this paper, focusing on the Euclidian space under ι2 norm and building up on Entropy LSH, we propose the distributed Layered LSH scheme, and prove that it exponentially decreases the network cost, while maintaining a good load balance between different machines. Our experiments also verify that our theoretical results.

#index 1919896
#* Author-conference topic-connection model for academic network search
#@ Jianwen Wang;Xiaohua Hu;Xinhui Tu;Tingting He
#t 2012
#c 1
#% 722904
#% 788094
#% 1083734
#% 1117023
#% 1481049
#% 1482268
#% 1544149
#% 1650298
#% 1650387
#! This paper proposes a novel topic model, Author-Conference Topic-Connection (ACTC) Model for academic network search. The ACTC Model extends the author-conference-topic (ACT) model by adding subject of the conference and the latent mapping information between subjects and topics. It simultaneously models topical aspects of papers, authors and conferences with two latent topic layers: a subject layer corresponding to conference topic, and a topic layer corresponding to the word topic. Each author would be associated with a multinomial distribution over subjects of conference (eg., KM, DB, IR for CIKM 2012), the conference(CIKM 2012), and the topics are respectively generated from a sampled subject. Then the words are generated from the sampled topics. We conduct experiments on a data set with 8,523 authors, 22,487 papers and 1,243 conferences from the well-known Arnetminer website, and train the model with different number of subjects and topics. For a qualitative evaluation, we compare ACTC with three others models LDA, Author-Topic (AT) and ACT in academic search services. Experiments show that ACTC can effectively capture the semantic connection between different types of information in academic network and perform well in expert searching and conference searching.

#index 1919897
#* Impact neighborhood indexing (INI) in diffusion graphs
#@ Jung Hyun Kim;K. Selçuk Candan;Maria Luisa Sapino
#t 2012
#c 1
#% 249989
#% 274612
#% 479973
#% 501654
#% 722530
#% 832271
#% 956551
#% 975021
#% 989646
#% 1016176
#% 1073984
#% 1083672
#% 1130854
#% 1270186
#% 1291642
#% 1426510
#% 1560417
#% 1676017
#! A graph neighborhood consists of a set of nodes that are nearby or otherwise related to each other. While existing definitions consider the structure (or topology) of the graph, we note that they fail to take into account the information propagation and diffusion characteristics, such as decay and reinforcement, common in many networks. In this paper, we first define the propagation efficiency of nodes and edges. We use this to introduce the novel concept of zero-erasure (or impact) neighborhood (ZEN) of a given node, n, consisting of the set of nodes that receive information from (or are impacted by) n without any decay. Based on this, we present an impact neighborhood indexing (INI) algorithm that creates data structures to help quickly identify impact neighborhood of any given node. Experiment results confirm the efficiency and effectiveness of the proposed INI algorithms.

#index 1919898
#* Loyalty-based selection: retrieving objects that persistently satisfy criteria
#@ Zhitao Shen;Muhammad Aamir Cheema;Xuemin Lin
#t 2012
#c 1
#% 70020
#% 282141
#% 800572
#% 810032
#% 810033
#% 863399
#% 893139
#% 1012401
#% 1594683
#! A traditional query returns a set of objects that satisfy user defined criteria at the time query was issued. The results are based on the values of objects at query time and may be affected by outliers. Intuitively, an object better meets the user's needs if it persistently satisfies the criteria, i.e., it satisfies the criteria for majority of the time in the past T time units. In this paper, we propose a measure named loyalty that reflects how persistently an object satisfies the criteria. Formally, the loyalty of an object is the total time (in past T time units) it satisfies the query criteria. In this paper, we study top-k loyalty queries over sliding windows that continuously report k objects with the highest loyalties. Each object issues an update when it starts satisfying the criteria or when it stops satisfying the criteria. We show that the lower bound cost of updating the results of a top-k loyalty query is O(logN), for each object update, where N is the number of updates issued in last T time units. We conduct a detailed complexity analysis and show that our proposed algorithm is optimal. Moreover, effective pruning techniques are proposed to improve the efficiency. We experimentally verify the effectiveness of the proposed approach by comparing it with a classic sweep line algorithm.

#index 1919899
#* Star-Join: spatio-textual similarity join
#@ Sitong Liu;Guoliang Li;Jianhua Feng
#t 2012
#c 1
#% 152937
#% 210186
#% 210187
#% 227932
#% 427199
#% 864392
#% 893164
#% 927035
#% 956506
#% 1127425
#% 1206801
#% 1654056
#% 1846749
#% 1848110
#! Location-based services have attracted significant attention due to modern mobile phones equipped with GPS devices. These services generate large amounts of spatio-textual data which contain both spatial location and textual descriptions. Since a spatio-textual object may have different representations, possibly because of deviations of GPS or different user descriptions, it calls for efficient methods to integrate spatio-textual data from different sources. In this paper we study a new research problem called spatio-textual similarity join: given two sets of spatio-textual objects, we find the similar object pairs. To the best of our knowledge, we are the first to study this problem. We make the following contributions: (1) We develop a filter-and-refine framework and devise several efficient algorithms. We first generate spatial and textual signatures for the objects and build inverted index on top of these signatures. Then we generate candidate pairs using the inverted lists of signatures. Finally we refine the candidates and generate the final result. (2) We study how to generate high-quality signatures for spatial information. We develop an MBR-prefix based signature to prune large numbers of dissimilar object pairs. (3) Experimental results on real and synthetic datasets show that our algorithms achieve high performance and scale well.

#index 1919900
#* Adapt: adaptive database schema design for multi-tenant applications
#@ Jiacai Ni;Guoliang Li;Jun Zhang;Lei Li;Jianhua Feng
#t 2012
#c 1
#% 1063561
#% 1207027
#% 1217217
#% 1581871
#% 1594597
#% 1626816
#% 1846790
#! Multi-tenant data management is a major application of software as a Service (SaaS). Many companies outsource their data to a third party which hosts a multi-tenant database system to provide data management service. The system should have high performance, low space and excellent scalability. One big challenge is to devise a high-quality database schema. Independent Tables Shared Instances and Shared Tables Shared Instances are two state-of-the-art methods. However, the former has poor scalability, while the latter achieves good scalability at the expense of poor performance and high space overhead. In this paper, we trade-off between the two methods and propose an adaptive database schema design approach to achieve good scalability and high performance with low space. To this end, we identify the important attributes and use them to generate a base table. For other attributes, we construct supplementary tables. We propose a cost-based model to adaptively generate the tables above. Our method has the following advantages. First, our method achieves high scalability. Second, our method can trade-off performance and space requirement. Third, our method can be easily applied to existing databases (e.g., MySQL) with minor revisions. Fourth, our method can adapt to any schemas and query workloads. Experimental results show our method achieves high performance and good scalability with low space and outperforms state-of-the-art method.

#index 1919901
#* Optimizing data migration for cloud-based key-value stores
#@ Xiulei Qin;Wenbo Zhang;Wei Wang;Jun Wei;Xin Zhao;Tao Huang
#t 2012
#c 1
#% 978442
#% 998845
#% 1426489
#% 1428142
#% 1459339
#% 1523961
#% 1557845
#% 1984776
#! As one database offloading strategy, elastic key-value stores are often introduced to speed up the application performance with dynamic scalability. Since the workload is varied, efficient data migration with minimal impact in service is critical for the issue of elasticity and scalability. However, due to the new virtualization technology, real-time and low-latency requirements, data migration within cloud-based key-value stores has to face new challenges: effects of VM interference, and the need to trade off between the two ingredients of migration cost, namely migration time and performance impact. To fulfill these challenges, in this paper we explore a new approach to optimize the data migration. Explicitly, we build two interference-aware models to predict the migration time and performance impact for each migration action using statistical machine learning, and then create a cost model to strike a balance between the two ingredients. Using the load rebalancing scenario as a case study, we have designed one cost-aware migration algorithm that utilizes the cost model to guide the choice of possible migration actions. Finally, we demonstrate the effectiveness of the approach using Yahoo! Cloud Serving Benchmark (YCSB).

#index 1919902
#* Applying weighted queries on probabilistic databases
#@ Sebastian Lehrack
#t 2012
#c 1
#% 303893
#% 1111110
#% 1312979
#% 1565405
#% 1599873
#% 1610454
#% 1615075
#% 1697247
#% 1933425
#! Relational queries applied on probabilistic databases have been established as a powerful tool for accessing huge data sets of uncertain data. Often various parts of such queries have different significances for a specific user. Thus, a query language should allow us to give subqueries different weights to quantify the individual user preferences. In this work we introduce a theoretical foundation for weighted algebra operators on probabilistic databases within a SQL-like query language.

#index 1919903
#* A new tool for multi-level partitioning in teradata
#@ Young-Kyoon Suh;Ahmad Ghazal;Alain Crolotte;Pekka Kostamaa
#t 2012
#c 1
#% 248815
#% 765431
#% 1016226
#% 1581943
#! This paper introduces a new tool that recommends an optimized partitioning solution called Multi-Level Partitioned Primary Index (MLPPI) for a fact table based on the queries in the workload. The tool implements a new technique using a greedy algorithm for search space enumeration. The space is driven by predicates in the queries. This technique fits very well the Teradata MLPPI scheme, as it is based on a general framework using general expressions, ranges and case expressions for partition definitions. The cost model implemented in the tool is based on the Teradata optimizer, and it is used to prune the search space for reaching a final solution. The tool resides completely on the client, and interfaces the database through APIs as opposed to previous work that requires optimizer code extension. The APIs are used to simplify the workload queries, and to capture fact table predicates and costs necessary to make the recommendation. The predicate-driven method implemented by the tool is general, and it can be applied to any clustering or partitioning scheme based on simple field expressions or complex SQL predicates. Experimental results given a particular workload will show that the recommendation from the tool outperforms a human expert. The experiments also show that the solution is scalable both with the workload complexity and the size of the fact table.

#index 1919904
#* Fast PCA computation in a DBMS with aggregate UDFs and LAPACK
#@ Carlos Ordonez;Naveen Mohanam;Carlos Garcia-Alvarado;Predrag T. Tosic;Edgar Martinez
#t 2012
#c 1
#% 238376
#% 810092
#% 1063552
#% 1082233
#% 1512993
#! Efficient and scalable execution of numerical methods inside a DBMS is difficult as its architecture is not suited for intense numerical computations. We study computing Principal Component Analysis (PCA) on large data sets via Singular Value Decomposition (SVD). Given the difficulty to program and optimize numerical methods on an existing DBMS, we explore an alternative reusability approach: calling the well-known numerical library LAPACK. Thus we study several alternatives to summarize the data set with aggregate User-Defined Functions (UDFs) and how to efficiently call SVD numerical methods available in LAPACK via Stored Procedures (SPs). We propose algorithmic and system optimizations to enhance scalability and to push processing into RAM. We show it is feasible to efficiently solve PCA by first summarizing the data set with arrays incrementally updated with aggregate UDFs and then pushing heavy matrix processing in SVD to RAM calling LAPACK via SPs. We benchmark our solution on a modern DBMS. Our solution requires only one pass on the data set and it exhibits linear scalability.

#index 1919905
#* Scaling multiple-source entity resolution using statistically efficient transfer learning
#@ Sahand N. Negahban;Benjamin I.P. Rubinstein;Jim Gemmell Gemmell
#t 2012
#c 1
#% 799701
#% 913783
#% 916788
#% 1217162
#% 1523838
#% 1815826
#% 1815965
#% 1865472
#% 1919905
#! We consider a serious, previously-unexplored challenge facing almost all approaches to scaling up entity resolution (ER) to multiple data sources: the prohibitive cost of labeling training data for supervised learning of similarity scores for each pair of sources. While there exists a rich literature describing almost all aspects of pairwise ER, this new challenge is arising now due to the unprecedented ability to acquire and store data from online sources, interest in features driven by ER such as enriched search verticals, and the uniqueness of noisy and missing data characteristics for each source. We show on real-world and synthetic data that for state-of-the-art techniques, the reality of heterogeneous sources means that the number of labeled training data must scale quadratically in the number of sources, just to maintain constant precision/recall. We address this challenge with a brand new transfer learning algorithm which requires far less training data (or equivalently, achieves superior accuracy with the same data) and is trained using fast convex optimization. The intuition behind our approach is to adaptively share structure learned about one scoring problem with all other scoring problems sharing a data source in common. We demonstrate that our theoretically-motivated approach improves upon existing techniques for multi-source ER.

#index 1919906
#* A probabilistic approach to correlation queries in uncertain time series data
#@ Mahsa Orang;Nematollaah Shiri
#t 2012
#c 1
#% 654487
#% 765537
#% 772835
#% 885364
#% 1118895
#% 1181271
#% 1206690
#% 1218743
#% 1267585
#% 1408794
#% 1451177
#% 1451534
#% 1482191
#% 1643113
#% 1686486
#% 1846727
#! Numerous real-life applications, such as wireless sensor networks and location-based services, generate large amount of uncertain time series, where the exact value at each timestamp is unavailable or unknown. In this paper, we formalize the notion of correlation for uncertain time series data and consider a family of probabilistic, threshold-based correlation queries over such data. The proposed formulation extends the notion of correlation developed for standard, certain time series. We show that uncertain correlation is a random variable approaching normal distribution. We also formalize the notion of uncertain time series normalization which is at the core of our correlation query processing approach, while it proves to be an important pre-processing technique in particular for pattern discovery tasks. The results of our numerous experiments indicate that, unlike in the standard time series, there is a trade-off between false alarms and hit ratios, which can be controlled by the probability threshold provided by users. Our results also offer users a guideline for choosing proper threshold values.

#index 1919907
#* On bundle configuration for viral marketing in social networks
#@ De-Nian Yang;Wang-Chien Lee;Nai-Hui Chia;Mao Ye;Hui-Ju Hung
#t 2012
#c 1
#% 729923
#% 808277
#% 1451243
#% 1535389
#% 1598366
#% 1628176
#! Prior research on viral marketing mostly focuses on promoting one single product item. In this work, we explore the idea of bundling multiple items for viral marketing and formulate a new research problem, called Bundle Configuration for SpreAd Maximization (BCSAM). Efficiently obtaining an optimal product bundle under the setting of BCSAM is very challenging. Aiming to strike a balance between the quality of solution and the computational overhead, we systematically explore various heuristics to develop a suite of algorithms, including κ-Bundle Configuration and Aggregated Bundle Configuration. Moreover, we integrate all the proposed ideas into one efficient algorithm, called Aggregated Bundle Configuration (ABC). Finally, we conduct an extensive performance evaluation on our proposals. Experimental results show that ABC significantly outperforms its counterpart and two baseline approaches in terms of both computational overhead and bundle quality.

#index 1919908
#* Learning to rank for hybrid recommendation
#@ Jiankai Sun;Shuaiqiang Wang;Byron J. Gao;Jun Ma
#t 2012
#c 1
#% 301259
#% 411762
#% 577224
#% 578684
#% 729626
#% 1074061
#% 1268491
#% 1358747
#% 1386007
#% 1450868
#% 1476486
#! Most existing recommender systems can be classified into two categories: collaborative filtering and content-based filtering. Hybrid recommender systems combine the advantages of the two for improved recommendation performance. Traditional recommender systems are rating-based. However, predicting ratings is an intermediate step towards their ultimate goal of generating rankings or recommendation lists. Learning to rank is an established means of predicting rankings and has recently demonstrated high promise in improving quality of recommendations. In this paper, we propose LRHR, the first attempt that adapts learning to rank to hybrid recommender systems. LRHR first defines novel representations for both users and items so that they can be content-comparable. Then, LRHR identifies a set of novel meta-level features for learning purposes. Finally, LRHR adopts RankSVM, a pairwise learning to rank algorithm, to generate recommendation lists of items for users. Extensive experiments on benchmarks in comparison with the state-of-the-art algorithms demonstrate the performance gain of our approach.

#index 1919909
#* Importance weighted passive learning
#@ Shuaiqiang Wang;Xiaoming Xi;Yilong Yin
#t 2012
#c 1
#% 340936
#% 411762
#% 940137
#% 1074038
#% 1074061
#% 1211696
#! Importance weighted active learning (IWAL) introduces a weighting scheme to measure the importance of each instance for correcting the sampling bias of the probability distributions between training and test datasets. However, the weighting scheme of IWAL involves the distribution of the test data, which can be straightforwardly estimated in active learning by interactively querying users for labels of selected test instances, but difficult for conventional learning where there are no interactions with users, referred as passive learning. In this paper, we investigate the insufficient sampling bias problem, i.e., bias occurs only because of insufficient samples, but the sampling process is unbiased. In doing this, we present two assumptions on the sampling bias, based on which we propose a practical weighting scheme for the empirical loss function in conventional passive learning, and present IWPL, an importance weighted passive learning framework. Furthermore, we provide IWSVM, an importance weighted SVM for validation. Extensive experiments demonstrate significant advantages of IWSVM on benchmarks and synthetic datasets.

#index 1919910
#* A tag-centric discriminative model for web objects classification
#@ Lina Yao;Quan Z. Sheng
#t 2012
#c 1
#% 830281
#% 869504
#% 956544
#% 961193
#% 1214717
#% 1250573
#% 1264133
#% 1498325
#% 1598339
#! This paper studies web object classification problem with the novel exploration of social tags. More and more web objects are increasingly annotated with human interpretable labels (i.e., tags), which can be considered as an auxiliary attribute to assist the object classification. Automatically classifying web objects into manageable semantic categories has long been a fundamental pre-process for indexing, browsing, searching, and mining heterogeneous web objects. However, such heterogeneous web objects often suffer from a lack of easy-extractable and uniform descriptive features. In this paper, we propose a discriminative tag-centric model for web object classification by jointly modeling the objects category labels and their corresponding social tags and un-coding the relevance among social tags. Our approach is based on recent techniques for learning large-scale discriminative models. We conduct experiments to validate our approach using real-life data. The results show the feasibility and good performance of our approach.

#index 1919911
#* Outlier detection using centrality and center-proximity
#@ Duck-Ho Bae;Seo Jeong;Sang-Wook Kim;Minsoo Lee
#t 2012
#c 1
#% 300136
#% 300183
#% 438137
#% 570886
#% 1214636
#! An outlier is an object that is considerably dissimilar with the remainder of the dataset. In this paper, we first propose the notion of centrality and center-proximity as novel outlierness measures which can be considered to represent the characteristics of all of the objects in the dataset. We then propose a graph-based outlier detection method which can solve the problems of local density, micro-cluster, and fringe objects. Finally, through extensive experiments, we show the effectiveness of the proposed method.

#index 1919912
#* An effective category classification method based on a language model for question category recommendation on a cQA service
#@ Kyoungman Bae;Youngjoong Ko
#t 2012
#c 1
#% 1022357
#% 1074110
#% 1145733
#% 1155187
#! Classiying user's question into several topics helps respondents answering the question in a cQA service. The word weighting method must estimate the appropriate weight of a word to improve the category (or topic) classification. In this paper, we propose a novel effective word weighting method based on a language model for automatic category classification in the cQA service. We first calculate the occurrence probability of a word in each category by using a language model and then the final weight of each word is estimated by ratio of the occurrence probability of the word on a category to the occurrence probability of the word on the other categories. As a result, the proposed method significantly improves the performance of the category classification.

#index 1919913
#* Clustering short text using Ncut-weighted non-negative matrix factorization
#@ Xiaohui Yan;Jiafeng Guo;Shenghua Liu;Xue-qi Cheng;Yanfeng Wang
#t 2012
#c 1
#% 313959
#% 643008
#% 724227
#% 1077150
#% 1598402
#! Non-negative matrix factorization (NMF) has been successfully applied in document clustering. However, experiments on short texts, such as microblogs, Q&A documents and news titles, suggest unsatisfactory performance of NMF. An major reason is that the traditional term weighting schemes, like binary weight and tfidf, cannot well capture the terms' discriminative power and importance in short texts, due to the sparsity of data. To tackle this problem, we proposed a novel term weighting scheme for NMF, derived from the Normalized Cut (Ncut) problem on the term affinity graph. Different from idf, which emphasizes discriminability on document level, the Ncut weighting measures terms' discriminability on term level. Experiments on two data sets show our weighting scheme significantly boosts NMF's performance on short text clustering.

#index 1919914
#* Polygene-based evolution: a novel framework for evolutionary algorithms
#@ Shuaiqiang Wang;Byron J. Gao;Shuangling Wang;Guibao Cao;Yilong Yin
#t 2012
#c 1
#% 152934
#% 466521
#% 647162
#! In this paper, we introduce polygene-based evolution, a novel framework for evolutionary algorithms (EAs) that features distinctive operations in the evolution process. In traditional EAs, the primitive evolution unit is gene, where genes are independent components during evolution. In polygene-based evolutionary algorithms (PGEAs), the evolution unit is polygene, i.e., a set of co-regulated genes. Discovering and maintaining quality polygenes can play an effective role in evolving quality individuals. Polygenes generalize genes, and PGEAs generalize EAs. Implementing the PGEA framework involves three phases: polygene discovery, polygene planting, and polygene-compatible evolution. Extensive experiments on function optimization benchmarks in comparison with the conventional and state-of-the-art EAs demonstrate the potential of the approach in accuracy and efficiency improvement.

#index 1919915
#* A tensor encoding model for semantic processing
#@ Michael Symonds;Peter D. Bruza;Laurianne Sitbon;Ian Turner
#t 2012
#c 1
#% 218978
#% 565900
#% 815160
#% 817435
#% 885468
#% 1473935
#! This paper develops and evaluates an enhanced corpus based approach for semantic processing. Corpus based models that build representations of words directly from text do not require pre-existing linguistic knowledge, and have demonstrated psychologically relevant performance on a number of cognitive tasks. However, they have been criticised in the past for not incorporating sufficient structural information. Using ideas underpinning recent attempts to overcome this weakness, we develop an enhanced tensor encoding model to build representations of word meaning for semantic processing. Our enhanced model demonstrates superior performance when compared to a robust baseline model on a number of semantic processing tasks.

#index 1919916
#* Accelerating locality preserving nonnegative matrix factorization
#@ Guanhong Yao;Cai Deng
#t 2012
#c 1
#% 837604
#% 1305450
#% 1595860
#% 1866320
#% 1867227
#! Matrix factorization techniques have been frequently applied in information retrieval, computer vision and pattern recognition. Among them, Non-negative Matrix Factorization (NMF) has received considerable attention due to its psychological and physiological interpretation of naturally occurring data whose representation may be parts-based in the human brain. Locality Preserving Non-negative Matrix Factorization (LPNMF) is a recently proposed graph-based NMF extension which tries to preserves the intrinsic geometric structure of the data. Compared with the original NMF, LPNMF has more discriminating power on data representa- tion thanks to its geometrical interpretation and outstanding ability to discover the hidden topics. However, the computa- tional complexity of LPNMF is O(n3), where n is the number of samples. In this paper, we propose a novel approach called Accelerated LPNMF (A-LPNMF) to solve the com- putational issue of LPNMF. Specifically, A-LPNMF selects p (p j n) landmark points from the data and represents all the samples as the sparse linear combination of these landmarks. The non-negative factors which incorporates the geometric structure can then be efficiently computed. Experimental results on the real data sets demonstrate the effectiveness and efficiency of our proposed method.

#index 1919917
#* The twitaholic next door.: scalable friend recommender system using a concept-sensitive hash function
#@ Patrick Bamba;Julien Subercaze;Christophe Gravier;Nabil Benmira;Jimi Fontaine
#t 2012
#c 1
#% 198058
#% 322619
#% 464434
#% 479973
#% 511151
#% 722904
#% 1287290
#% 1333447
#% 1476470
#% 1482547
#! In this paper we present a Friend Recommender System for micro-blogging. Traditional batch processing of massive amounts of data makes it difficult to provide a near-real time friend recommender system or even a system that can properly scale to millions of users. In order to overcome these issues, we have designed a solution that represents user-generated micro posts as a set of pseudo-cliques. These graphs are assigned a hash value using an original Concept-Sensitive Hash function, a new sub-kind of Locally-Sensitive Hash functions. Finally, since the user profiles are represented as a binary footprint, the pairwise comparison of footprints using the Hamming distance provides scalability to the recommender system. The paper goes with an online application relying on a large Twitter dataset, so that the reader can freely experiment the system.

#index 1919918
#* Information propagation in social rating networks
#@ Priyanka Garg;Irwin King;Michael R. Lyu
#t 2012
#c 1
#% 729923
#% 1407359
#% 1540249
#! The polarity of opinion is a crucial part of information and ignoring the asymmetry between them, can potentially result in an inaccurate estimation of the number of product adoptions and incorrect recommendations. We analyze the propagation patterns of the negative and positive opinions on two real world datasets, Flixster and Epinions, and observe that the presence of negative opinions significantly reduces the number of expressed opinions. To account for the asymmetry between the two kind of opinions, we propose extensions of the two most popular information propagation models, Independent Cascade and Linear Threshold models. The proposed extensions give a tractable influence problem and improves the prediction accuracy of future opinions, by more than 3% on Flixster and 5% on Epinions datasets.

#index 1919919
#* Maximizing revenue from strategic recommendations under decaying trust
#@ Paul Dütting;Monika Henzinger;Ingmar Weber
#t 2012
#c 1
#% 202011
#% 342596
#% 729923
#% 734590
#% 740191
#% 754160
#% 856751
#% 868450
#% 868468
#% 868469
#% 963332
#% 963359
#% 1083977
#% 1166530
#% 1336439
#! Suppose your sole interest in recommending a product to me is to maximize the amount paid to you by the seller for a sequence of recommendations. How should you recommend optimally if I become more inclined to ignore you with each irrelevant recommendation you make? Finding an answer to this question is a key challenge in all forms of marketing that rely on and explore social ties; ranging from personal recommendations to viral marketing. We prove that even if the recommendee regains her initial trust on each successful recommendation, the expected revenue the recommender can make over an infinite period due to payments by the seller is bounded. This can only be overcome when the recommendee also incrementally regains trust during periods without any recommendation. Here, we see a connection to "banner blindness," suggesting that showing fewer ads can lead to a higher long-term revenue.

#index 1919920
#* Weighted linear kernel with tree transformed features for malware detection
#@ Prakash Mandayam Comar;Lei Liu;Sabyasachi Saha;Antonio Nucci;Pang-Ning Tan
#t 2012
#c 1
#% 400847
#% 1190610
#! Malware detection from network traffic flows is a challenging problem due to data irregularity issues such as imbalanced class distribution, noise, missing values, and heterogeneous types of features. To address these challenges, this paper presents a two-stage classification approach for malware detection. The framework initially employs random forest as a macro-level classifier to separate the malicious from non-malicious network flows, followed by a collection of one-class support vector machine classifiers to identify the specific type of malware. A novel tree-based feature construction approach is proposed to deal with data imperfection issues. As the performance of the support vector machine classifier often depends on the kernel function used to compute the similarity between every pair of data points, designing an appropriate kernel is essential for accurate identification of malware classes. We present a simple algorithm to construct a weighted linear kernel on the tree transformed features and demonstrate its effectiveness in detecting malware from real network traffic data.

#index 1919921
#* Learning to predict the cost-per-click for your ad words
#@ Chieh-Jen Wang;Hsin-Hsi Chen
#t 2012
#c 1
#% 823366
#% 942409
#% 963358
#% 1089742
#% 1272022
#% 1280759
#% 1330901
#% 1381027
#% 1405974
#! In Internet ad campaign, ranking of an ad on search result pages depends on a cost-per-click (CPC) of ad words offered by an advertiser and a quality score estimated by a search engine. Bidding for ad words with a higher CPC is more competitive than bidding for the same ad words with a lower CPC in the ad ranking competition. However, offering a higher CPC will increase a burden on advertisers. In contrast, offering a lower CPC may decrease the exposure rate of their ads. Thus, how to select an appropriate CPC for ad words is indispensable for advertisers. In this paper, we extract different semantic levels of features, such as named entities, topic terminologies, and individual words from a large-scale real-world ad words corpus, and explore various learning based prediction algorithms. The thorough experimental results show that the CPC prediction models considering more ad words semantics achieve better prediction performance, and the prediction model using the support vector regression (SVR) and features from all semantic levels performs the best.

#index 1919922
#* Dual word and document seed selection for semi-supervised sentiment classification
#@ Shengfeng Ju;Shoushan Li;Yan Su;Guodong Zhou;Yu Hong;Xiaojun Li
#t 2012
#c 1
#% 722797
#% 769892
#% 815915
#% 854646
#% 1074102
#% 1127964
#% 1176920
#% 1214749
#% 1250356
#% 1270809
#% 1328330
#% 1330516
#% 1471221
#% 1583795
#% 1826355
#! Semi-supervised sentiment classification aims to train a classifier with a small number of labeled data (called seed data) and a large amount of unlabeled data. a big advantage of this approach is its saving of annotation effort by using the unlabeled data which is usually freely available. In this paper, we propose an approach to further minimize the annotation effort of semi-supervised sentiment classification by actively selecting the seed data. Specifically, a novel selection strategy is proposed to simultaneously select good words and documents for manual annotation by considering both of their annotation costs and informativeness. Experimental results demonstrate the effectiveness of our approach.

#index 1919923
#* On empirical tradeoffs in large scale hierarchical classification
#@ Rohit Babbar;Ioannis Partalas;Eric Gaussier;Cecile Amblard
#t 2012
#c 1
#% 829975
#% 1074128
#% 1117691
#% 1227578
#! While multi-class categorization of documents has been of research interest for over a decade, relatively fewer approaches have been proposed for large scale taxonomies in which the number of classes range from hundreds of thousand as in Directory Mozilla to over a million in Wikipedia. As a result of ever increasing number of text documents and images from various sources, there is an immense need for automatic classification of documents in such large hierarchies. In this paper, we analyze the tradeoffs between the important characteristics of different classifiers employed in the top down fashion. The properties for relative comparison of these classifiers include, (i) accuracy on test instance, (ii) training time (iii) size of the model and (iv) test time required for prediction. Our analysis is motivated by the well known error bounds from learning theory, which is also further reinforced by the empirical observations on the publicly available data from the Large Scale Hierarchical Text Classification Challenge. We show that by exploiting the data heterogenity across the large scale hierarchies, one can build an overall classification system which is approximately 4 times faster for prediction, 3 times faster to train, while sacrificing only 1% point in accuracy.

#index 1919924
#* An interaction framework of service-oriented ontology learning
#@ Jingsong Zhang;Yinglin Wang;Hao Wei
#t 2012
#c 1
#% 261694
#% 859521
#% 994152
#% 1525357
#% 1544417
#% 1549514
#% 1570205
#% 1803365
#! Ontology plays a very important role in supporting knowledge-based applications. In cloud computing, ontology learning technology is facing new challenges in dealing with heterogeneous data sources from different domains and researchers, which may contain various particular concepts and relations. Traditional ontology learning frameworks usually focus only on the extraction of concepts and taxonomic relations from the multi-structured corpus. However, former researches rarely studied the interactions during ontology learning process among different researchers. Lack of interactions among people who build ontology in different domains may cause inconsistent ontology. Besides, lack of incentive during the ontology building process will also result in low efficiency. To address these challenges, this paper specifies a novel solution to perform ontology learning. The solution includes a service-oriented ontology interaction framework, a service-oriented ontology learning strategy. It shows that it advances ontology learning to a higher level of performance and portability with a number of experiments in demo system.

#index 1919925
#* Infobox suggestion for Wikipedia entities
#@ Afroza Sultana;Quazi Mainul Hasan;Ashis Kumer Biswas;Soumyava Das;Habibur Rahman;Chris Ding;Chengkai Li
#t 2012
#c 1
#% 1019061
#% 1055735
#% 1063570
#% 1263263
#% 1301004
#% 1481369
#% 1601240
#! Given the sheer amount of work and expertise required in authoring Wikipedia articles, automatic tools that help Wikipedia contributors in generating and improving content are valuable. This paper presents our initial step towards building a full-fledged author assistant, particularly for suggesting infobox templates for articles. We build SVM classifiers to suggest infobox template types, among a large number of possible types, to Wikipedia articles without infoboxes. Different from prior works on Wikipedia article classification which deal with only a few label classes for named entity recognition, the much larger 337-class setup in our study is geared towards realistic deployment of infobox suggestion tool. We also emphasize testing on articles without infoboxes, due to that labeled and unlabeled data exhibit different distributions of features, which departs from the typical assumption that they are drawn from the same underlying population.

#index 1919926
#* Time feature selection for identifying active household members
#@ Pedro G. Campos;Alejandro Bellogin;Fernando Díez;Iván Cantador
#t 2012
#c 1
#% 1127465
#% 1214666
#% 1583602
#% 1625410
#% 1669056
#% 1669058
#% 1669060
#! Popular online rental services such as Netflix and MoviePilot often manage household accounts. A household account is usually shared by various users who live in the same house, but in general does not provide a mechanism by which current active users are identified, and thus leads to considerable difficulties for making effective personalized recommendations. The identification of the active household members, defined as the discrimination of the users from a given household who are interacting with a system (e.g. an on-demand video service), is thus an interesting challenge for the recommender systems research community. In this paper, we formulate the above task as a classification problem, and address it by means of global and local feature selection methods and classifiers that only exploit time features from past item consumption records. The results obtained from a series of experiments on a real dataset show that some of the proposed methods are able to select relevant time features, which allow simple classifiers to accurately identify active members of household accounts.

#index 1919927
#* Text classification with relatively small positive documents and unlabeled data
#@ Fumiyo Fukumoto;Takeshi Yamamoto;Suguru Matsuyoshi;Yoshimi Suzuki
#t 2012
#c 1
#% 311034
#% 464268
#% 577235
#% 1450868
#! This paper addresses the problem of dealing with a collection of negative training documents which is suitable for relatively small number of positive documents, and presents a method for eliminating the need for manually collecting negative training documents based on supervised machine learning techniques. We applied an error correction technique to the results of negative training data obtained by the Positive Example Based Learning (PEBL). Moreover, we used a boosting technique to learn a set of negative data to train classifiers. The results using Japanese newspaper documents showed that the method contributes for reducing the cost of manual collection of negative training documents.

#index 1919928
#* On compressing weighted time-evolving graphs
#@ Wei Liu;Andrey Kan;Jeffrey Chan;James Bailey;Christopher Leckie;Jian Pei;Ramamohanarao Kotagiri
#t 2012
#c 1
#% 193743
#% 1063501
#% 1085182
#% 1451193
#% 1535343
#% 1606036
#! Existing graph compression techniquesmostly focus on static graphs. However for many practical graphs such as social networks the edge weights frequently change over time. This phenomenon raises the question of how to compress dynamic graphs while maintaining most of their intrinsic structural patterns at each time snapshot. In this paper we show that the encoding cost of a dynamic graph is proportional to the heterogeneity of a three dimensional tensor that represents the dynamic graph. We propose an effective algorithm that compresses a dynamic graph by reducing the heterogeneity of its tensor representation, and at the same time also maintains a maximum lossy compression error at any time stamp of the dynamic graph. The bounded compression error benefits compressed graphs in that they retain good approximations of the original edge weights, and hence properties of the original graph (such as shortest paths) are well preserved. To the best of our knowledge, this is the first work that compresses weighted dynamic graphs with bounded lossy compression error at any time snapshot of the graph.

#index 1919929
#* Graph-based collective classification for tweets
#@ Yajuan Duan;Furu Wei;Ming Zhou;Heung-Yeung Shum
#t 2012
#c 1
#% 1055680
#% 1158333
#% 1450992
#% 1806022
#! In this paper, we address the problem of classifying tweets into topical categories. Because of the short, noisy and ambiguous nature of tweets, we propose to collectively conduct the classification by exploiting the context information (i.e. related tweets) other than individually as in conventional text classification methods. In particular, we augment the content-based representation of text with tweets sharing same #hashtag or URL, which results in a tweet graph. We then formulate the tweet classification task under a graph optimization framework. We investigate three popular approaches, namely, Loopy Belief Propagation (LBP), Relaxation Labeling (RL), and Iterative Classification Algorithm (ICA). Extensive experiment results show that the graph-based tweet classification approach remarkably improves the performance, while the ICA model with relationship of sharing the same #hashtag gives the best result on separate tweet graph.

#index 1919930
#* A word-order based graph representation for relevance identification
#@ Lakshmi Ramachandran;Edward F. Gehringer
#t 2012
#c 1
#% 342630
#% 742218
#% 816186
#% 938692
#% 938786
#% 939902
#% 1270754
#% 1484251
#% 1659434
#! In this paper we propose a new word-order based graph representation for text. In our graph representation vertices represent words or phrases and edges represent relations between contiguous words or phrases. The graph representation also includes dependency information. Our text representation is suitable for applications involving the identification of relevance or paraphrases across texts, where word-order information would be useful. We show that this word-order based graph representation performs better than a dependency tree representation while identifying the relevance of one piece of text to another.

#index 1919931
#* Tracing clusters in evolving graphs with node attributes
#@ Brigitte Boden;Stephan Günnemann;Thomas Seidl
#t 2012
#c 1
#% 738913
#% 853535
#% 989586
#% 989638
#% 989640
#% 1055740
#% 1535395
#% 1617292
#! Data sources representing social networks with additional attribute information about the nodes are widely available in today's applications. Recently, combined clustering methods were introduced that consider graph information and attribute information simultaneously to detect meaningful clusters in such networks. In many cases, such attributed graphs also evolve over time. Therefore, there is a need for clustering methods that are able to trace clusters over different time steps and analyze their evolution over time. In this paper, we extend our combined clustering method DB-CSC to the analysis of evolving combined clusters.

#index 1919932
#* Prediction of retweet cascade size over time
#@ Andrey Kupavskii;Liudmila Ostroumova;Alexey Umnov;Svyatoslav Usachev;Pavel Serdyukov;Gleb Gusev;Andrey Kustarev
#t 2012
#c 1
#% 956512
#% 1355040
#% 1355042
#% 1399992
#% 1560174
#% 1560424
#% 1560425
#! Retweet cascades play an essential role in information diffusion in Twitter. Popular tweets reflect the current trends in Twitter, while Twitter itself is one of the most important online media. Thus, understanding the reasons why a tweet becomes popular is of great interest for sociologists, marketers and social media researches. What is even more important is the possibility to make a prognosis of a tweet's future popularity. Besides the scientific significance of such possibility, this sort of prediction has lots of practical applications such as breaking news detection, viral marketing etc. In this paper we try to forecast how many retweets a given tweet will gain during a fixed time period. We train an algorithm that predicts the number of retweets during time T since the initial moment. In addition to a standard set of features we utilize several new ones. One of the most important features is the flow of the cascade. Another one is PageRank on the retweet graph, which can be considered as the measure of influence of users.

#index 1919933
#* An efficient and simple under-sampling technique for imbalanced time series classification
#@ Guohua Liang;Chengqi Zhang
#t 2012
#c 1
#% 565108
#% 765521
#% 876074
#% 926881
#% 961134
#% 983921
#% 1127609
#% 1271973
#% 1301405
#% 1510746
#% 1617301
#% 1708211
#% 1747635
#% 1747798
#! Imbalanced time series classification (TSC) involving many real-world applications has increasingly captured attention of researchers. Previous work has proposed an intelligent-structure preserving over-sampling method (SPO), which the authors claimed achieved better performance than other existing over-sampling and state-of-the-art methods in TSC. The main disadvantage of over-sampling methods is that they significantly increase the computational cost of training a classification model due to the addition of new minority class instances to balance data-sets with high dimensional features. These challenging issues have motivated us to find a simple and efficient solution for imbalanced TSC. Statistical tests are applied to validate our conclusions. The experimental results demonstrate that this proposed simple random under-sampling technique with SVM is efficient and can achieve results that compare favorably with the existing complicated SPO method for imbalanced TSC.

#index 1919934
#* Top-N recommendation through belief propagation
#@ Jiwoon Ha;Soon-Hyoung Kwon;Sang-Wook Kim;Christos Faloutsos;Sunju Park
#t 2012
#c 1
#% 330687
#% 813966
#% 956513
#% 975021
#% 1074061
#% 1127487
#% 1214748
#% 1227601
#% 1357698
#% 1386007
#% 1617313
#% 1663625
#! The top-n recommendation focuses on finding the top-n items that the target user is likely to purchase rather than predicting his/her ratings on individual items. In this paper, we propose a novel method that provides top-n recommendation by probabilistically determining the target user's preference on items. This method models the purchasing relationships between users and items as a bipartite graph and employs Belief Propagation to compute the preference of the target user on items. We analyze the proposed method in detail by examining the changes in recommendation accuracy under different parameter settings. We also show that the proposed method is up to 40% more accurate than an existing method by comparing it with an RWR-based method via extensive experiments.

#index 1919935
#* Mining advices from weblogs
#@ Alfan Farizki Wicaksono;Sung-Hyon Myaeng
#t 2012
#c 1
#% 1127964
#% 1471326
#! Weblog, one of the fastest growing user generated contents, often contains key learnings gleaned from people's past experiences which are really worthy to be well presented to other people. One of the key learnings contained in weblogs is often vented in the form of advice. In this paper, we aim to provide a methodology to extract sentences that reveal advices on weblogs. We observed our data to discover the characteristics of advices contained in weblogs. Based on this observation, we define our task as a classification problem using various linguistic features. We show that our proposed method significantly outperforms the baseline. The presence or absence of imperative mood expression appears to be the most important feature in this task. It is also worth noting that the work presented in this paper is the first attempt on mining advices from English data.

#index 1919936
#* Parallel proximal support vector machine for high-dimensional pattern classification
#@ Zhenfeng Zhu;Xingquan Zhu;Yangdong Ye;Yue-Fei Guo;Xiangyang Xue
#t 2012
#c 1
#% 190581
#% 342598
#% 397139
#% 1482416
#! Proximal support vector machine (PSVM) is a simple but effective classifier, especially for solving large-scale data classification problems. An inherent deficiency of PSVM lies on its inefficiency for dealing with high-dimensional data. In this paper, we propose a parallel version of PSVM (PPSVM). Based on random dimensionality partitioning, PPSVM can obtain partitioned local model parameters in parallel, with combined parameters to form the final global solution. In fact, PPSVM enjoys two properties: 1) It can calculate model parameters in parallel and is therefore a fast learning method with theoretically proved convergence; and 2) It can avoid the inversion of large matrix, which makes it suitable for high-dimensional data. In the paper, we also propose a random PPSVM with randomly partitioned data in each iteration to improve the performance of PSVM. Experimental results on real-world data demonstrate that the proposed methods can obtain similar or even better prediction accuracy than PSVM with much better runtime efficiency.

#index 1919937
#* On using category experts for improving the performance and accuracy in recommender systems
#@ Won-Seok Hwang;Ho-Jong Lee;Sang-Wook Kim;Minsoo Lee
#t 2012
#c 1
#% 173879
#% 330687
#% 428272
#% 734592
#% 813966
#% 1358747
#% 1393622
#! A variety of recommendation methods have been proposed to satisfy the performance and accuracy; however, it is fairly difficult to satisfy both of them because there is a trade-off between them. In this paper, we introduce the notion of category experts and propose the recommendation method by exploiting the ratings of category experts instead of those of the users similar to a target user. We also extend the method that uses both the category preference of a target user and his/her similarity to category experts. We show that our method significantly outperforms the existing methods in terms of performance and accuracy through extensive experiments with real-world data.

#index 1919938
#* Finding influential products on social domination game
#@ Jinyoung Yeo;Jin-woo Park;Seung-won Hwang
#t 2012
#c 1
#% 465167
#% 729923
#% 1133032
#% 1535380
#% 1693922
#! In this paper, we propose a new type of market model called the social domination game model. Given a set C of customers and a set P of products, this model simulates market competition among P and estimates market shares, considering both the dominance relation between C and P and the influence relation among the members of C. With this model, we propose a greedy product positioning algorithm for designing a new product that approximately maximizes market share. Our experimental results show that the proposed algorithm creates a new product gaining up to 97.5% market share of the best product's market share obtained by the exact method, while significantly outperforming the exact method in terms of running time, i.e., by up to two orders of magnitude.

#index 1919939
#* Entity resolution using search engine results
#@ Madian Khabsa;Pucktada Treeratpituk;C. Lee Giles
#t 2012
#c 1
#% 742425
#% 809459
#% 855094
#% 874510
#% 1268491
#% 1271267
#% 1787060
#! Given a set of automatically extracted entities E of size n, we would like to cluster all the various names referring to the same canonical entity together. The variations of each entity include acronyms, full name, and informal naming conventions. We propose using search engine results to cluster variations of each entity based on the URLs appearing in those results. We create a cluster C for each top search result returned by querying for the entity e ∈ E assigning e to the cluster C. Our experiments on a manually created dataset shows that our approach achieves higher precision and recall than string matching algorithm and hierarchical clustering based disambiguation methods.

#index 1919940
#* Tweet classification based on their lifetime duration
#@ Hikaru Takemura;Keishi Tajima
#t 2012
#c 1
#% 1040837
#% 1399992
#% 1399997
#% 1400018
#% 1426611
#% 1450992
#% 1484274
#% 1642046
#! Many microblog messages remain useful only within a short time, and users often find such a message after its informational value has vanished. Users also sometimes miss old but still useful messages buried among outdated ones. To solve these problems, we develop a method of classifying messages into the following three categories: (1) messages that users should read now because their value will diminish soon, (2) messages that users may read later because their value will not largely change soon, and (3) messages that are not useful anymore because their value has vanished. Our method uses an error correcting output code consisting of binary classifiers each of which determines whether a given message has value at specific time point. Our experiments on Twitter data confirmed that it outperforms naive methods.

#index 1919941
#* Scalable collaborative filtering using incremental update and local link prediction
#@ Xiao Yang;Zhaoxin Zhang;Ke Wang
#t 2012
#c 1
#% 734590
#% 734594
#% 813966
#% 955712
#% 989626
#% 1232028
#% 1291600
#% 1476492
#% 1476500
#% 1707764
#! The traditional collaborative filtering approaches have been shown to suffer from two fundamental problems: data sparsity and difficulty in scalability. To address these problems, we present a novel scalable item-based collaborative filtering method by using incremental update and local link prediction. By subdividing the computations and analyzing the factors in different cases of item-to-item similarity, we design the incremental update strategies in item-based CF, which can make the recommender system more efficient and scalable. Based on the transitive structure of item similarity graph, we use the local link prediction method to find implicit candidates to alleviate the lack of neighbors in predictions and recommendations caused by the sparsity of data. The experiment results validate that our algorithm can improve the performance of traditional CF, and can increase the efficiency in recommendations.

#index 1919942
#* Composing activity groups in social networks
#@ Cheng-Te Li;Man-Kwan Shan
#t 2012
#c 1
#% 881460
#% 1214668
#% 1384246
#% 1451234
#% 1482238
#% 1512399
#% 1573239
#% 1642027
#% 1693928
#! One important function of current social networking services is allowing users to initialize different kinds of activity groups (e.g. study group, cocktail party, and group buying) and invite friends to attend in either manual or collaborative manners. However, such process of group formation is tedious, and could either include inappropriate group members or miss relevant ones. This work proposes to automatically compose the activity groups in a social network according to user-specified activity information. Given the activity host, a set of labels representing the activity's subjects, the desired group size, and a set of must-inclusive persons, we aim to find a set of individuals as the activity group, in which members are required to not only be familiar with the host but also have great communications with each other. We devise an approximation algorithm to greedily solve the group composing problem. Experiments on a real social network show the promising effectiveness of the proposed approach as well as the satisfactory human subjective study.

#index 1919943
#* A co-training based method for chinese patent semantic annotation
#@ Xu Chen;Zhiyong Peng;Cheng Zeng
#t 2012
#c 1
#% 252011
#% 301241
#% 504443
#% 855219
#% 1292765
#% 1296985
#% 1660904
#! Patents are public and scientific literatures protected by the law, and their abstracts highly contained valuable information. Patent's semantic annotation can effectively protect intellectual property rights and promote corporations' scientific research innovation. Currently, automatic patent annotation mainly used supervised machine learning algorithms, which required abundant expensive labeled patent data. Due to lack of enough labeled Chinese patent data, this paper adopted a semi-supervised machine learning method named co-training, which started from a little labeled data. This method combined keyword extraction with list extraction, and incrementally annotated functional clauses in patent abstract. Experiment results indicated this method can gradually improve the recall without sacrificing the precision.

#index 1919944
#* Automatic labeling hierarchical topics
#@ Xian-Ling Mao;Zhao-Yan Ming;Zheng-Jun Zha;Tat-Seng Chua;Hongfei Yan;Xiaoming Li
#t 2012
#c 1
#% 722904
#% 823344
#% 869516
#% 878454
#% 881498
#% 915215
#% 989620
#% 1055683
#% 1077150
#% 1166510
#% 1323439
#% 1544073
#% 1592082
#% 1592262
#! Recently, statistical topic modeling has been widely applied in text mining and knowledge management due to its powerful ability. A topic, as a probability distribution over words, is usually difficult to be understood. A common, major challenge in applying such topic models to other knowledge management problem is to accurately interpret the meaning of each topic. Topic labeling, as a major interpreting method, has attracted significant attention recently. However, previous works simply treat topics individually without considering the hierarchical relation among topics, and less attention has been paid to creating a good hierarchical topic descriptors for a hierarchy of topics. In this paper, we propose two effective algorithms that automatically assign concise labels to each topic in a hierarchy by exploiting sibling and parent-child relations among topics. The experimental results show that the inter-topic relation is effective in boosting topic labeling accuracy and the proposed algorithms can generate meaningful topic labels that are useful for interpreting the hierarchical topics.

#index 1919945
#* An unsupervised method for author extraction from web pages containing user-generated content
#@ Jing Liu;Xinying Song;Jingtian Jiang;Chin-Yew Lin
#t 2012
#c 1
#% 754107
#% 805846
#% 881505
#% 1190073
#% 1214745
#% 1468142
#% 1482182
#% 1558464
#% 1560390
#% 1598376
#% 1715211
#! In this paper, we address the problem of author extraction (AE) from user generated content (UGC) pages. Most existing solutions for web information extraction, including AE, adopt supervised approaches, which require expensive manual annotation. We propose a novel unsupervised approach for automatically collecting and labeling training data based on two key observations of author names: (1) people tend to use a single name across sites if their preferred names are available; (2) people tend to create unique usernames to easily distinguish themselves from others, e.g. travelbug61. Our AE solution only requires features extracted from a single UGC page instead of relying on clues from multiple UGC pages. We conducted extensive experiments. (1) The evaluation of automatically labeled author field data shows 95.0% precision. (2) Our method achieves an F1 score of 96.1%, which significantly outperforms a state-of-the-art supervised approach with single page features (F1 score: 68.4%) and has a comparable performance to its multiple page solution (F1 score: 95.4%). (3) We also examine the robustness of our approach on various UGC pages from forums and review sites, and achieve promising results as well.

#index 1919946
#* Hierarchical target type identification for entity-oriented queries
#@ Krisztian Balog;Robert Neumayer
#t 2012
#c 1
#% 642982
#% 730051
#% 844287
#% 853542
#% 1019130
#% 1055706
#% 1074093
#% 1074094
#% 1074223
#% 1130914
#% 1133171
#% 1227610
#% 1227616
#% 1330534
#% 1384755
#% 1400010
#% 1482185
#% 1482286
#% 1482372
#% 1489451
#% 1620194
#% 1642172
#% 1806040
#! A significant portion of information needs in web search target entities. These may come in different forms or flavours, ranging from short keyword queries to more verbose requests, expressed in natural language. We address the task of automatically annotating queries with target types from an ontology. The identified types can subsequently be used, e.g., for creating semantically more informed query and retrieval models, filtering results, or directing the requests to specific verticals. Our study makes the following contributions. First, we formalise the task of hierarchical target type identification, argue that it is best viewed as a ranking problem, and propose multiple evaluation metrics. Second, we develop a purpose-built test collection by hand-annotating over 300 queries, from various recent entity search benchmarking campaigns, with target types from the DBpedia ontology. Finally, we introduce and examine two baseline models, inspired by federated search techniques. We show that these methods perform surprisingly well when target types are limited to a flat list of top level categories; finding the right level of granularity in the hierarchy, however, is particularly challenging and requires further investigation.

#index 1919947
#* Dictionary based sparse representation for domain adaptation
#@ Rishabh Mehrotra;Rushabh Agrawal;Syed Aqueel Haider
#t 2012
#c 1
#% 416518
#% 837622
#% 1261539
#% 1272110
#% 1299092
#% 1679056
#% 1760857
#% 1815246
#! Machine Learning algorithms are often as good as the data they can learn from. Enormous amount of unlabeled data is readily available and the ability to efficiently use such amount of unlabeled data holds a significant promise in terms of increasing the performance of various learning tasks. We consider the task of supervised Domain Adaptation and present a Self-Taught learning based framework which makes use of the K-SVD algorithm for learning sparse representation of data in an unsupervised manner. To the best of our knowledge this is the first work that integrates K-SVD algorithm into the self-taught learning framework. The K-SVD algorithm iteratively alternates between sparse coding of the instances based on the current dictionary and a process of updating/adapting the dictionary to better fit the data so as to achieve a sparse representation under strict sparsity constraints. Using the learnt dictionary, a rich feature representation of the few labeled instances is obtained which is fed to a classifier along with class labels to build the model. We evaluate our framework on the task of domain adaptation for sentiment classification. Both self-domain (requiring very few domain-specific training instances) and cross-domain classification (requiring 0 labeled instances of target domain and very few labeled instances of source domain) are performed. Empirical comparisons of self-domain and cross-domain results establish the efficacy of the proposed framework.

#index 1919948
#* Selecting expansion terms as a set via integer linear programming
#@ Qi Zhang;Yan Wu;Xuanjing Huang
#t 2012
#c 1
#% 92696
#% 326522
#% 433821
#% 577301
#% 641976
#% 879612
#% 948366
#% 987230
#% 1074080
#% 1074081
#% 1181094
#% 1227584
#% 1263579
#% 1292550
#% 1292562
#! Pseudo-relevance feedback via query expansion has been widely studied from various perspectives in the past decades. Its effectiveness in improving retrieval effectiveness has been shown in many tasks. A variety of criteria were proposed to select additional terms for the original queries. However, most of the existing methods weight and select terms individually and do not consider the impact of term-to-term relationship. In this paper, we first examine the influence of combinations of terms through data analysis, which demonstrate the significant effect of term-to-term relationship on retrieval effectiveness. Then, to address this problem, we formalize the query expansion task as an integer linear programming (ILP) problem. The model combines the weights learned from a supervised method for individual terms, and integrates constraints to capture relations between terms. Finally, three standard TREC collections are used to evaluate the proposed method. Experimental results demonstrate that the proposed method can significantly improve the effectiveness of retrieval.

#index 1919949
#* An evaluation and enhancement of densitometric fragmentation for content slicing reuse
#@ Killian Levacher;Seamus Lawless;Vincent Wade
#t 2012
#c 1
#% 722902
#% 741058
#% 1130926
#% 1586580
#% 1846458
#! Content slicing addresses the need of adaptive systems to reuse open corpus material by converting it into re-composable information objects. However this conversion is highly dependent upon the ability to correctly fragment pages into structurally sound atomic pieces. A recently suggested approach to fragmentation, which relies on densitometric page representation, claims to achieve high accuracy and time performance. Although it has been well received within the research community, a full evaluation of this approach and identification of strengths and weaknesses across a range of characteristics hasn't been performed. This paper proposes an independent evaluation of the approach with respect to granularity control, accuracy, time performance, content diversity and linguistic dependency. Moreover, this paper also provides a significant contribution to address important weaknesses discovered during the analysis, in order to improve the suitability and impact of the original algorithm within the context of content slicing.

#index 1919950
#* Mathematical equation retrieval using plain words as a query
#@ Shinil Kim;Seon Yang;Youngjoong Ko
#t 2012
#c 1
#% 1065265
#% 1099068
#% 1655230
#! This paper proposes how to effectively retrieve the mathematical equations when the plain words are given as a query. The proposed system requires no complicated mathematical symbols, no particular input tool and no constraint of query. Users can enter a query with plain words like the traditional Information Retrieval. For this, we extract features from the plain texts that are converted from the real math equations. Experimental results show an outstanding performance, a MRR of 0.6585.

#index 1919951
#* Serial position effects of clicking behavior on result pages returned by search engines
#@ Mingda Wu;Shan Jiang;Yan Zhang
#t 2012
#c 1
#% 348137
#% 577302
#% 754060
#% 818221
#% 860724
#% 946521
#% 1004294
#% 1083899
#% 1581860
#! Under the joint influence of the presentation of search results and users' browsing and clicking habits, the click probability distribution does not merely obey a monotonic decreasing Zipf function. In this paper, we present evidence that the click behavior on the entries of search engines' result pages is influenced by Serial Position Effect, which is independent of how these entries are ordered, and introduce a new function to characterize the click probability distribution.

#index 1919952
#* Towards measuring the visualness of a concept
#@ Jin-Woo Jeong;Xin-Jing Wang;Dong-Ho Lee
#t 2012
#c 1
#% 839917
#% 1116393
#% 1327719
#% 1484449
#% 1493662
#% 1544078
#% 1949285
#! In this paper, we propose a new method to measure the visualness of a concept. The visualness of a concept is generally defined as what extent a concept has visual characteristics. Even though the visualness of a concept is important and useful for various image search tasks, it has not received much spotlight yet. In this work, we especially focus on how to measure the visualness of a complex concept such as "round table", "dry bed" rather than a simple concept like "ball", "apple". To measure the visualness, we first collect sample images of a complex concept using web image search engines, and then group the images based on the visual features. Finally, we compute visual purity and weighted entropy of the clusters, which will act as a visualness score for the concept. Through various experiments, we show and discuss interesting results about the visualness of a concept.

#index 1919953
#* Fast candidate generation for two-phase document ranking: postings list intersection with bloom filters
#@ Nima Asadi;Jimmy Lin
#t 2012
#c 1
#% 57989
#% 322884
#% 411762
#% 510483
#% 730065
#% 864446
#% 878624
#% 1019084
#% 1092517
#% 1328179
#% 1598342
#% 1598430
#% 1604467
#% 1667821
#% 1846784
#! Most modern web search engines employ a two-phase ranking strategy: a candidate list of documents is generated using a "cheap" but low-quality scoring function, which is then reranked by an "expensive" but high-quality method (usually machine-learned). This paper focuses on the problem of candidate generation for conjunctive query processing in this context. We describe and evaluate a fast, approximate postings list intersection algorithms based on Bloom filters. Due to the power of modern learning-to-rank techniques and emphasis on early precision, significant speedups can be achieved without loss of end-to-end retrieval effectiveness. Explorations reveal a rich design space where effectiveness and efficiency can be balanced in response to specific hardware configurations and application scenarios.

#index 1919954
#* Semantically coherent image annotation with a learning-based keyword propagation strategy
#@ Chaoran Cui;Jun Ma;Shuaiqiang Wang;Shuai Gao;Tao Lian
#t 2012
#c 1
#% 465914
#% 829043
#% 1069003
#% 1148273
#% 1264133
#! Automatic image annotation plays an important role in modern keyword-based image retrieval systems. Recently, many neighbor-based methods have been proposed and achieved good performance for image annotation. However, existing work mainly focused on exploring a distance metric learning algorithm to determine the neighbors of an image, and neglected the subsequent keyword propagation process. They usually used some simple heuristic propagation rules, and propagated each keyword independently without considering the inherent semantic coherence among keywords. In this paper, we propose a novel learning-based keyword propagation strategy and incorporate it into the neighbor-based method framework. In particular, we employ the structural SVM to learn a scoring function which can evaluate different candidate keyword sets for a test image. Moreover, we explicitly enforce the semantic coherence constraint for the propagated keywords in our approach. The annotation of the test image is propagated as a whole rather than separate keywords. Experiments on two benchmark data sets demonstrate the effectiveness of our approach for image annotation and ranked retrieval.

#index 1919955
#* Language processing for arabic microblog retrieval
#@ Kareem Darwish;Walid Magdy;Ahmed Mourad
#t 2012
#c 1
#% 397158
#% 1186431
#% 1271448
#% 1536506
#% 1587377
#% 1591966
#% 1641934
#% 1806052
#! The use of social media has profoundly affected social and political dynamics in the Arab world. In this paper, we explore the Arabic microblogs retrieval. We illustrate some of the challenges associated with Arabic microblog retrieval, which mainly stem from the use of different Arabic dialects that vary in lexical selection, morphology, and phonetics and lack orthographic and spelling conventions. We present some of the required processing for effective retrieval such as improved letter normalization, elongated word handling, stopword removal, and stemming

#index 1919956
#* Hierarchical image annotation using semantic hierarchies
#@ Hichem Bannour;Céline Hudelot
#t 2012
#c 1
#% 635689
#% 722927
#% 1432960
#% 1747808
#% 1856603
#! Semantic hierarchies have been introduced recently to improve image annotation. They was used as a framework for hierarchical image classification, and thus to improve classifiers accuracy and reduce the complexity of managing large scale data. In this paper, we investigate the contribution of semantic hierarchies for hierarchical image classification. We propose first a new method based on the hierarchy structure to train efficiently hierarchical classifiers. Our method, named One-Versus-Opposite-Nodes, allows decomposing the problem in several independent tasks and therefore scales well with large database. We also propose two methods for computing a hierarchical decision function that serves to annotate new image samples. The former is performed by a top-down classifiers voting, while the second is based on a bottom-up score fusion. The experiments on Pascal VOC'2010 dataset showed that our methods improve well the image annotation results.

#index 1919957
#* On the inference of average precision from score distributions
#@ Ronan Cummins
#t 2012
#c 1
#% 340934
#% 750863
#% 818205
#% 818263
#% 1227642
#% 1392434
#% 1450859
#% 1450904
#% 1558080
#% 1748071
#% 1748097
#% 1806004
#% 1806015
#% 1806029
#% 1879160
#! Modelling the document scores returned from an IR system for a given query using parameterised score distributions is an area of research that has become more popular in recent years. Score distribution (SD) models are useful for a number of IR tasks. These include data fusion, query performance prediction, determining thresholds in filtering applications, and tasks in the area of distributed retrieval. The inference of performance metrics, such as average precision, from these SD models is an important consideration. In this paper, we study the accuracy of a number of methods of inferring average precision from an SD model.

#index 1919958
#* An evaluation of corpus-driven measures of medical concept similarity for information retrieval
#@ Bevan Koopman;Guido Zuccon;Peter Bruza;Laurianne Sitbon;Michael Lawley
#t 2012
#c 1
#% 764597
#% 855104
#% 958367
#% 1074206
#% 1182835
#% 1334820
#% 1598394
#! Measures of semantic similarity between medical concepts are central to a number of techniques in medical informatics, including query expansion in medical information retrieval. Previous work has mainly considered thesaurus-based path measures of semantic similarity and has not compared different corpus-driven approaches in depth. We evaluate the effectiveness of eight common corpus-driven measures in capturing semantic relatedness and compare these against human judged concept pairs assessed by medical professionals. Our results show that certain corpus-driven measures correlate strongly (approx 0.8) with human judgements. An important finding is that performance was significantly affected by the choice of corpus used in priming the measure, i.e., used as evidence from which corpus-driven similarities are drawn. This paper provides guidelines for the implementation of semantic similarity measures for medical informatics and concludes with implications for medical information retrieval.

#index 1919959
#* A constraint to automatically regulate document-length normalisation
#@ Ronan Cummins;Colm O'Riordan
#t 2012
#c 1
#% 218982
#% 324129
#% 397183
#% 411760
#% 730008
#% 766412
#% 818263
#% 916805
#% 1154026
#% 1227703
#% 1539232
#% 1558081
#% 1598478
#% 1641914
#% 1715606
#! Retrieval functions in information retrieval (IR) are fundamental to the effectiveness of search systems. However, considerable parameter tuning is often needed to increase the effectiveness of the retrieval. Document length normalisation is one such aspect that requires tuning on a per-query and per-collection basis for many retrieval functions. In this paper, we develop an approach that regularises the level of normalisation to apply on a per-query basis. We formally describe the interaction between query-terms and document length normalisation using a constraint. We then develop a general pre-retrieval approach to adapt a number of state-of-the-art ranking functions so that they adhere to the constraint. Finally, we empirically demonstrate that the adapted retrieval functions outperform default versions of the original retrieval functions, and perform at least comparably to tuned versions of the original functions, on a number of datasets. Essentially this regulates the normalisation parameter in a number of retrieval functions on a per-query basis in a principled manner.

#index 1919960
#* Bridging offline and online social graph dynamics
#@ Manuel Gomez Rodriguez;Monica Rogati
#t 2012
#c 1
#% 881460
#% 1083675
#% 1536568
#! The online and offline worlds are converging. Location-based services, ubiquitous mobile devices and on-the-go social network accessibility are blurring the distinction between in-person activities and their virtual counterpart. An important effect of this convergence is the rapid and powerful impact of offline events (meetings, conferences) on the evolution and temporal dynamics of the online connectivity between members of social and professional networks. However, these effects have been largely unexplored. We study these effects by using data from LinkedIn, a popular professional social networking site. We find that offline events may induce connectivity changes in the online network -- there is a dramatic increase in the number of connections between event attendees shortly after the date of the event. Building on these insights, we describe a non-supervised method that exploits connectivity changes temporally correlated to real world events to successfully infer more than 40% of specific event attendees. Finally, we revisit the link prediction problem by including user contributed information about off-line events to achieve higher link prediction performance.

#index 1919961
#* Predicting the performance of passage retrieval for question answering
#@ Eyal Krikon;David Carmel;Oren Kurland
#t 2012
#c 1
#% 169809
#% 309124
#% 397161
#% 642979
#% 742086
#% 893735
#% 987260
#% 1263599
#% 1467729
#% 1587395
#! We present a novel approach to predicting the performance of passage retrieval for question answering. That is, estimating the effectiveness, for answer extraction, of a list of passages retrieved in response to a question when relevance judgments are not available. Our prediction model integrates two types of estimates. The first estimates the probability that the information need expressed by the question is satisfied by the passages. This estimate is devised by adapting query-performance predictors developed for the document retrieval task. The second type estimates the probability that the passages contain the answers. This estimate relies on the occurrences of named entities that are likely to answer the question. Empirical evaluation demonstrates the merits of our prediction approach. For example, the prediction quality is much better than that of the only previous prediction method devised for the task at hand.

#index 1919962
#* Coarse-to-fine sentence-level emotion classification based on the intra-sentence features and sentential context
#@ Jun Xu;Ruifeng Xu;Qin Lu;Xiaolong Wang
#t 2012
#c 1
#% 238395
#% 893916
#% 950571
#% 1338679
#% 1484344
#% 1706814
#! This paper proposes a novel approach using a coarse-to-fine analysis strategy for sentence-level emotion classification which takes into consideration of similarities to sentences in training set as well as adjacent sentences in the context. First, we use intra-sentence based features to determine the emotion label set of a target sentence coarsely through the statistical information gained from the label sets of the k most similar sentences in the training data. Then, we use the emotion transfer probabilities between neighboring sentences to refine the emotion labels of the target sentences. Such iterative refinements terminate when the emotion classification converges. The proposed algorithm is evaluated on Ren-CECps, a Chinese blog emotion corpus. Experimental results show that the coarse-to-fine emotion classification algorithm improves the sentence-level emotion classification by 19.11% on the average precision metric, which outperforms the baseline methods.

#index 1919963
#* Query-performance prediction and cluster ranking: two sides of the same coin
#@ Oren Kurland;Fiana Raiber;Anna Shtok
#t 2012
#c 1
#% 218992
#% 340899
#% 340901
#% 340948
#% 342660
#% 375017
#% 397161
#% 427921
#% 766430
#% 766431
#% 818267
#% 879575
#% 879613
#% 879614
#% 879676
#% 907544
#% 987260
#% 987265
#% 1074072
#% 1074119
#% 1130851
#% 1195854
#% 1263599
#% 1392447
#% 1415748
#% 1450860
#% 1450861
#% 1467729
#% 1529948
#% 1598445
#% 1741000
#% 1748097
#% 1763374
#! We show that two tasks which were independently addressed in the information retrieval literature actually amount to the exact same task. The first is query performance prediction; i.e., estimating the effectiveness of a search performed in response to a query in the absence of relevance judgments. The second task is cluster ranking, that is, ranking clusters of similar documents by their presumed effectiveness (i.e., relevance) with respect to the query. Furthermore, we show that several state-of-the-art methods that were independently devised for each of the two tasks are based on the same principles. Finally, we empirically demonstrate that using insights gained in work on query-performance prediction can help, in many cases, to improve the performance of a previously proposed cluster ranking method.

#index 1919964
#* Learning to rank search results for time-sensitive queries
#@ Nattiya Kanhabua;Kjetil Nørvåg
#t 2012
#c 1
#% 577224
#% 730070
#% 766408
#% 770754
#% 807756
#% 961152
#% 983905
#% 987226
#% 1130999
#% 1227692
#% 1268491
#% 1292475
#% 1355016
#% 1399966
#% 1415764
#% 1482367
#% 1495112
#% 1536521
#% 1598343
#% 1598409
#% 1598486
#% 1683873
#% 1697416
#! Retrieval effectiveness of temporal queries can be improved by taking into account the time dimension. Existing temporal ranking models follow one of two main approaches: 1) a mixture model linearly combining textual similarity and temporal similarity, and 2) a probabilistic model generating a query from the textual and temporal part of document independently. In this paper, we propose a novel time-aware ranking model based on learning-to-rank techniques. We employ two classes of features for learning a ranking model, entity-based and temporal features, which are derived from annotation data. Entity-based features are aimed at capturing the semantic similarity between a query and a document, whereas temporal features measure the temporal similarity. Through extensive experiments we show that our ranking model significantly improves the retrieval effectiveness over existing time-aware ranking models.

#index 1919965
#* On active learning in hierarchical classification
#@ Yu Cheng;Kunpeng Zhang;Yusheng Xie;Ankit Agrawal;Alok Choudhary
#t 2012
#c 1
#% 132697
#% 309141
#% 724192
#% 763708
#% 783478
#% 1260420
#! Most of the existing active learning algorithms assume all the category labels as independent or consider them in a "flat" structure. However, in reality, there are many applications in which the set of possible labels are often organized in a hierarchical structure. In this paper, we consider the problem of active learning when the categories are represented as a tree. Our goal is to exploit the structure information of the label tree in active learning to select the most informative samples to be labeled. We propose an algorithm that estimates the semantic space, embedding the category hierarchy. In this space, each category label is represented as a prototype and the uncertainty is measured using a variance-based fashion. We also demonstrate notable performance improvement with the proposed approach on synthetic and real datasets.

#index 1919966
#* Question-answer topic model for question retrieval in community question answering
#@ Zongcheng Ji;Fei Xu;Bin Wang;Ben He
#t 2012
#c 1
#% 280819
#% 838398
#% 1074110
#% 1227600
#% 1264760
#% 1292492
#% 1399953
#% 1472297
#% 1481560
#% 1482389
#% 1591994
#% 1598401
#! The major challenge for Question Retrieval (QR) in Community Question Answering (CQA) is the lexical gap between the queried question and the historical questions. This paper proposes a novel Question-Answer Topic Model (QATM) to learn the latent topics aligned across the question-answer pairs to alleviate the lexical gap problem, with the assumption that a question and its paired answer share the same topic distribution. Experiments conducted on a real world CQA dataset from Yahoo! Answers show that combining both parts properly can get more knowledge than each part or both parts in a simple mixing way and combining our QATM with the state-of-the-art translation-based language model, where the topic and translation information is learned from the question-answer pairs at two different grained semantic levels respectively, can significantly improve the QR performance.

#index 1919967
#* How do humans distinguish different people with identical names on the web?
#@ Harumi Murakami;Yuki Miyake
#t 2012
#c 1
#% 838408
#% 1257156
#% 1271267
#% 1338584
#! This research investigates how humans distinguish different people with identical names on the web to improve web people search. We asked subjects to classify 20 pages of web people-search results for each of 20 person names and analyzed their decision processes through questionnaire, protocol analysis, and interview. We found that keywords, vocations, works (for a real person, works are those made by the individual and, for a fictional person, works are those in which the individual appears), facial images, and the names of related people are important for distinguishing individuals. We proposed a model for distinguishing individuals and a knowledge-structure model based on the experiment's results.

#index 1919968
#* Enhancing product search by best-selling prediction in e-commerce
#@ Bo Long;Jiang Bian;Anlei Dong;Yi Chang
#t 2012
#c 1
#% 411762
#% 452641
#% 769472
#% 1183374
#% 1560387
#% 1598447
#! With the rapid growth of E-Commerce on the Internet, online product search service has emerged as a popular and effective paradigm for customers to find desired products and select transactions. Most product search engines today are based on adaptations of relevance models devised for information retrieval. However, there is still a big gap between the mechanism of finding products that customers really desire to purchase and that of retrieving products of high relevance to customers' query. In this paper, we address this problem by proposing a new ranking framework for enhancing product search based on dynamic best-selling prediction in E-Commerce. Specifically, we first develop an effective algorithm to predict the dynamic best-selling, i.e. the volume of sales, for each product item based on its transaction history. By incorporating such best-selling prediction with relevance, we propose a new ranking model for product search, in which we rank higher the product items that are not only relevant to the customer's need but with higher probability to be purchased by the customer. Results of a large scale evaluation, conducted over the dataset from a commercial product search engine, demonstrate that our new ranking method is more effective for locating those product items that customers really desire to buy at higher rank positions without hurting the search relevance.

#index 1919969
#* Survival analysis for freshness in microblogging search
#@ Gianni Amati;Giuseppe Amodeo;Carlo Gaibisso
#t 2012
#c 1
#% 340899
#% 730070
#% 960414
#% 1024551
#% 1130999
#% 1166534
#% 1270766
#% 1355016
#% 1355017
#% 1399966
#% 1484274
#% 1536506
#% 1598343
#% 1598383
#! Freshness of information in real-time search is central in social networks, news, blogs and micro-blogs. Nevertheless, there is not a clear experimental evidence that shows what principled approach effectively combines time and content. We introduce a novel approach to model freshness using a survival analysis of relevance over time. In such models, freshness is measured by the tail probability of relevance over time. We also assume that the probability distributions for freshness are heavy-tailed. The heavy-tailed models of freshness are shown to be highly effective on the micro-blogging test collection of TREC 2011. The improvements over the state-of-the-art time-based models are statistically significant or moderately significant.

#index 1919970
#* Information preservation in static index pruning
#@ Ruey-Cheng Chen;Chia-Jung Lee;Chiung-Min Tsai;Jieh Hsiang
#t 2012
#c 1
#% 248214
#% 340887
#% 907504
#% 1195898
#% 1302865
#% 1392436
#! We develop a new static index pruning criterion based on the notion of information preservation. This idea is motivated by the fact that model degeneration, as does static index pruning, inevitably reduces the predictive power of the resulting model. We model this loss in predictive power using conditional entropy and show that the decision in static index pruning can therefore be optimized to preserve information as much as possible. We evaluated the proposed approach on three different test corpora, and the result shows that our approach is comparable in retrieval performance to state-of-the-art methods. When efficiency is of concern, our method has some advantages over the reference methods and is therefore suggested in Web retrieval settings.

#index 1919971
#* Temporal models for microblogs
#@ Jaeho Choi;W. Bruce Croft
#t 2012
#c 1
#% 340901
#% 730070
#% 766408
#% 818262
#% 960414
#% 1019124
#% 1130999
#% 1399992
#% 1536506
#% 1560174
#% 1587369
#% 1598383
#% 1598444
#% 1641934
#% 1689528
#% 1806030
#% 1906984
#! Time information impacts relevance in retrieval for the queries that are sensitive to trends and events. Microblog services particularly focused on recent news and events so dealing with the temporal aspects of microblogs is essential for providing effective retrieval. Recent work on time-based retrieval has shown that selecting the relevant time period for query expansion is promising. In this paper, we suggest a method for selecting the time period for query expansion based on a user behavior (i.e., retweets) that can be collected easily. We then use these time periods for query expansion in a pseudo-relevance feedback setting. More specifically, we use the difference in the temporal distribution between the top retrieved documents and retweets. The experimental results based on the TREC Microblog collection show that our method for selecting periods for query expansion improves retrieval performance compared to another approach.

#index 1919972
#* I want what i need!: analyzing subjectivity of online forum threads
#@ Prakhar Biyani;Cornelia Caragea;Amit Singh;Prasenjit Mitra
#t 2012
#c 1
#% 769892
#% 855279
#% 855282
#% 1127964
#% 1292733
#% 1303712
#% 1587368
#% 1588392
#! Online forums have become a popular source of information due to the unique nature of information they contain. Internet users use these forums to get opinions of other people on issues and to find factual answers to specific questions. Topics discussed in online forum threads can be subjective seeking personal opinions or non-subjective seeking factual information. Hence, knowing subjectivity orientation of threads would help forum search engines to satisfy user's information needs more effectively by matching the subjectivities of user's query and topics discussed in the threads in addition to lexical match between the two. We study methods to analyze the subjectivity of online forum threads. Experimental results on a popular online forum demonstrate the effectiveness of our methods.

#index 1919973
#* Improving the performance of the reinforcement learning model for answering complex questions
#@ Yllias Chali;Sadid A. Hasan;Kaisar Imam
#t 2012
#c 1
#% 288614
#% 384911
#% 576214
#% 879592
#% 992304
#% 1074153
#% 1215306
#% 1470603
#% 1471273
#% 1482231
#% 1543061
#% 1711847
#! This paper addresses the task of answering complex questions using a multi-document summarization approach within a reinforcement learning setting. Given a set of complex questions, a list of relevant documents per question, and the corresponding human-generated summaries (i.e. answers to the questions) as training data, the reinforcement learning module iteratively learns a number of feature weights in order to facilitate the automatic generation of summaries i.e. answers to unseen complex questions. Previous works on this task have utilized a fully automatic reinforcement learning framework that selects the document sentences as the potential candidate (i.e. machine-generated) summary sentences by exploiting a relatedness measure with the available human-written summaries. In this paper, we propose an extension to this model that incorporates user interaction into the reinforcement learner to guide the candidate summary sentence selection process. Experimental results reveal the effectiveness of the user interaction component in the reinforcement learning framework.

#index 1919974
#* Relation regularized subspace recommending for related scientific articles
#@ Qing Zhang;Jianwu Li;Zhiping Zhang;Li Wang
#t 2012
#c 1
#% 438103
#% 722904
#% 922860
#% 955712
#% 1305469
#% 1861425
#! Recommending related scientific articles for a researcher is very important and useful in practice but also is full of challenges due to the latent complex semantic relations among scientific literatures. To deal with these challenges, this paper proposes a novel framework with link-missing data adaption, which casts the recommendation task to subspace embedding and similarity ranking problems. The relation regularized subspace in this framework is constructed via Relation Regularized Matrix Factorization (RRMF) for well modeling both content and link structure simultaneously. However, the link structure for an article is not always available in practical recommending. To solve this problem, we further propose two alternative approaches based on Latent Dirichlet Allocation (LDA) for link-missing articles recommendation as an extension of RRMF. Experiments on CiteSeer dataset demonstrate our method is more effective in comparison with some state-of-the-art approaches and is able to handle the link-missing case which the link-based methods never can fit.

#index 1919975
#* Exploring the cluster hypothesis, and cluster-based retrieval, over the web
#@ Fiana Raiber;Oren Kurland
#t 2012
#c 1
#% 32813
#% 218992
#% 228105
#% 340901
#% 340948
#% 375017
#% 427921
#% 766430
#% 818262
#% 879575
#% 907557
#% 1074072
#% 1174736
#% 1213624
#% 1263596
#% 1415748
#% 1415782
#% 1450860
#! We present a study of the cluster hypothesis, and of the performance of cluster-based retrieval methods, performed over large scale Web collections. Among the findings we present are (i) the cluster hypothesis can hold, as determined by a specific test, for large scale Web corpora to the same extent it does for newswire corpora; (ii) while spam documents do not affect the extent to which the cluster hypothesis holds, they considerably affect the performance of cluster based, as well as that of document-based, retrieval methods; and, (iii) as is the case for newswire corpora, cluster-based methods can yield better performance than document-based methods for Web corpora.

#index 1919976
#* A picture paints a thousand words: a method of generating image-text timelines
#@ Shize Xu;Liang Kong;Yan Zhang
#t 2012
#c 1
#% 635689
#% 748738
#% 1227614
#% 1451241
#% 1470687
#% 1471304
#% 1600643
#% 1642133
#% 1692174
#% 1711764
#! Manual timelines have greatly helped us to keep pace with the big world. In this paper, we introduce a novel solution which generates image-text timelines for news events based on Evolutionary Image-Text Summarization, which is an important and challenging problem. We first extract image's semantic information under translation model, and then fuse the high quality images with text timeline under an image assignment algorithm which can optimize the global coordination of the final timeline. The experimental results show that news readers can receive more satisfaction from the image-text timelines we generate.

#index 1919977
#* Short-text domain specific key terms/phrases extraction using an n-gram model with wikipedia
#@ M. Atif Qureshi;Colm O'Riordan;Gabriella Pasi
#t 2012
#c 1
#% 46803
#% 281480
#% 420487
#% 1019082
#% 1120999
#% 1130858
#% 1190102
#% 1190121
#% 1338554
#% 1471327
#% 1484298
#% 1544046
#% 1693902
#! Finding domain specific key terms/phrases from a given set of documents is a challenging task. A domain may be defined as an area of interest over a collection of documents which may not be explicitly defined but implicitly observable in those documents. When considering a collection of documents related to academic research, examples of key terms/phrases may be Information Retrieval", "Marine Biology", etc. In this paper a technique for extracting important key terms/phrases in a considered topical domain is proposed using external evidence from the titles of Wikipedia articles and the Wikipedia category graph. We performed some experiments over the document collection of Web sites of different post-graduate schools. Our preliminary evaluations show promising results for the detection of domain specific key terms/phrases from the given set of domain focused Web pages.

#index 1919978
#* A new probabilistic model for top-k ranking problem
#@ Shuzi Niu;Yanyan Lan;Jiafeng Guo;Xueqi Cheng
#t 2012
#c 1
#% 464451
#% 1074021
#% 1074044
#% 1674801
#% 1879067
#! This paper is concerned with top-k ranking problem, which reflects the fact that people pay more attention to the top ranked objects in real ranking application like information retrieval. A popular approach to top-k ranking problem is based on probabilistic models, such as Luce model and Mallows model. However, whether the sequential generative process described in these models is a suitable way for top-k ranking remains a question. According to the riffled independence factorization proposed in recent literature, which is a natural structural assumption on top-k ranking, we propose a new generative process of top-k ranking data. Our approach decomposes distributions over the top-k ranking into two layers: the first layer describes the relative ordering between the top k objects and the rest n-k objects, and the second layer describes the full ordering on the top k objects. On this basis, we propose a new probabilistic model for top-k ranking problem, called hierarchical ordering model. Specifically, we use three different probabilistic models to describe different generative processes of the first layer, and Luce model to describe the sequential generative process of the second layer, thus we obtain three different specific hierarchical ordering models. We also conduct extensive experiments on benchmark datasets to show that our proposed models can outperform previous models significantly.

#index 1919979
#* Large scale analysis of changes in english vocabulary over recent time
#@ Adam Jatowt;Katsumi Tanaka
#t 2012
#c 1
#% 1260690
#% 1267807
#% 1631322
#% 1787089
#! Recently many historical texts have become digitized and made accessible for search and browsing. As human language is subject to constant evolution, these texts pose varying challenges to current users. In this paper we report the results of large-scale studies on the usage of words and the evolution of English language vocabulary over the last two centuries to help with understanding its impact on readability and retrieval of historical documents. We perform analysis of several lexical factors which may influence accessibility and readability of historical texts based on two large scale lexical corpora: the Corpus of Historical American English and Google Books 1-gram.

#index 1919980
#* Climbing the app wall: enabling mobile app discovery through context-aware recommendations
#@ Alexandros Karatzoglou;Linas Baltrunas;Karen Church;Matthias Böhmer
#t 2012
#c 1
#% 1169589
#% 1176909
#% 1207050
#% 1214642
#% 1214688
#% 1476448
#% 1476453
#% 1535460
#% 1589885
#% 1619982
#! The explosive growth of the mobile application (app) market has made it difficult for users to find the most interesting and relevant apps from the hundreds of thousands that exist today. Context is key in the mobile space and so too are proactive services that ease user input and facilitate effective interaction. We believe that to enable truly novel mobile app recommendation and discovery, we need to support real context-aware recommendation that utilizes the diverse range of implicit mobile data available in a fast and scalable manner. In this paper we introduce the Djinn model, a novel context-aware collaborative filtering algorithm for implicit feedback data that is based on tensor factorization. We evaluate our approach using a dataset from an Android mobile app recommendation service called appazaar. Our results show that our approach compares favorably with state-of-the-art collaborative filtering methods.

#index 1919981
#* TwiSent: a multistage system for analyzing sentiment in twitter
#@ Subhabrata Mukherjee;Akshat Malu;Balamurali A.R.;Pushpak Bhattacharyya
#t 2012
#c 1
#% 464641
#% 1035590
#% 1277969
#% 1482447
#% 1544009
#% 1591918
#% 1592246
#% 1791445
#! In this paper, we present TwiSent, a sentiment analysis system for Twitter. Based on the topic searched, TwiSent collects tweets pertaining to it and categorizes them into the different polarity classes positive, negative and objective. However, analyzing micro-blog posts have many inherent challenges compared to the other text genres. Through TwiSent, we address the problems of 1) Spams pertaining to sentiment analysis in Twitter, 2) Structural anomalies in the text in the form of incorrect spellings, nonstandard abbreviations, slangs etc., 3) Entity specificity in the context of the topic searched and 4) Pragmatics embedded in text. The system performance is evaluated on manually annotated gold standard data and on an automatically annotated tweet set based on hashtags. It is a common practise to show the efficacy of a supervised system on an automatically annotated dataset. However, we show that such a system achieves lesser classification accurcy when tested on generic twitter dataset. We also show that our system performs much better than an existing system.

#index 1919982
#* Twitter hyperlink recommendation with user-tweet-hyperlink three-way clustering
#@ Dehong Gao;Renxian Zhang;Wenjie Li;Yuexian Hou
#t 2012
#c 1
#% 536048
#% 766521
#% 769928
#% 1300087
#% 1358747
#% 1711595
#! Twitter, the most famous micro-blogging service and online social network, collects millions of tweets every day. Due to the length limitation, users usually need to explore other ways to enrich the content of their tweets. Some studies have provided findings to suggest that users can benefit from added hyperlinks in tweets. In this paper, we focus on the hyperlinks in Twitter and propose a new application, called hyperlink recommendation in Twitter. We expect that the recommended hyperlinks can be used to enrich the information of user tweets. A three-way tensor is used to model the user-tweet-hyperlink collaborative relations. Two tensor-based clustering approaches, tensor decomposition-based clustering (TDC) and tensor approximation-based clustering (TAC) are developed to group the users, tweets and hyperlinks with similar interests, or similar contexts. Recommendation is then made based on the reconstructed tensor using cluster information. The evaluation results in terms of Mean Absolute Error (MAE) shows the advantages of both the TDC and TAC approaches over a baseline recommendation approach, i.e., memory-based collaborative filtering. Comparatively, the TAC approach achieves better performance than the TDC approach.

#index 1919983
#* Concavity in IR models
#@ Stéphane Clinchant
#t 2012
#c 1
#% 169781
#% 411760
#% 766412
#% 818235
#% 1450858
#! We study the impact of concavity in IR models and propose to use a generalized logarithm function, the n-logarithm to weight words in documents. We extend the family of information based Information Retrieval (IR) models with this function. We show that that concavity is indeed an important property of IR models. Experiments conducted for IR tasks, Latent Semantic Indexing and Text Categorization show improvements.

#index 1919984
#* Extracting interesting association rules from toolbar data
#@ Ilaria Bordino;Debora Donato;Barbara Poblete
#t 2012
#c 1
#% 728105
#% 987222
#% 1055676
#% 1130878
#% 1173699
#% 1399965
#% 1450893
#% 1598334
#! Toolbar navigation logs provide rich data for enhancing information discovery on the Web. The value of this data resides in its scope, which goes beyond that of traditional query-mining data sources, such as search-engine logs. In this paper we present a methodology for extracting relevant association rules for queries, based on historic user navigational data. In addition, we propose a graph-based approach for extracting related queries and URLs for a given query.

#index 1919985
#* Predicting CTR of new ads via click prediction
#@ Alexander Kolesnikov;Yury Logachev;Valeriy Topinskiy
#t 2012
#c 1
#% 956546
#! Predicting CTR of ads on the search result page is an urgent topic. The reason for this is that choosing the right advertisement greatly affects revenue of the search engine and advertisers and user's satisfaction. For ads with the large click history it is quite clear how to predict CTR by utilizing statistical data. But for new ads with a poor click history such approach is not robust and reliable. We suggest a model for predicting CTR of such new ads. Contrary to the previous models of predicting CTR of new ads, our model uses events - clicks and skips1 instead of the observed CTR. In addition we have implemented several novel features, that resulted into the increase of the performance of our model. Offline and online experiments on the real search engine system demonstrated that our model outperforms the baseline and the approaches suggested in previous papers.

#index 1919986
#* An examination of content farms in web search using crowdsourcing
#@ Richard McCreadie;Craig Macdonald;Iadh Ounis;Jim Giles;Ferris Jabr
#t 2012
#c 1
#% 1150163
#% 1194317
#% 1598354
#% 1969487
#! On the Web, content farms produce articles engineered such that search engines rank them highly, in order to turn a profit from online advertising. Recently, content farms have increasingly been the target of demotion strategies by Web search engines, since content farm articles are often considered to be of suspect quality. In this paper, we study the prevalence of content farms in the results returned by three major Web search engines over time. In particular, we develop a crowdsourced approach to identify content farm articles from the results returned by these search engines. Our results show that between the period of March and August 2011, the number of content farm articles observed on a number of indicative queries was reduced by up to 55% in the top ranks.

#index 1919987
#* Demographic context in web search re-ranking
#@ Eugene Kharitonov;Pavel Serdyukov
#t 2012
#c 1
#% 1117691
#% 1166492
#% 1190055
#% 1355051
#% 1450894
#% 1536504
#% 1598347
#% 1641961
#! In this paper we study usefulness of user's demographical context for improving ranking of ambiguous queries. Context-aware relevance model is learnt from implicit user behaviour by using a simple yet general modification of a state-of-art click model which is capable to catch dependences from the search context. After that the machine learned click model is used in an offline re-ranking experiment and it is demonstrated that the demographical context ranking features provide improvements in ranking quality. Further, we perform a study to investigate the impact of different facets of demographical features (gender, age, and income) on search ranking performance and manually analyse queries which exhibit strong context dependences to get an additional understanding of the model behaviour.

#index 1919988
#* On the usefulness of query features for learning to rank
#@ Craig Macdonald;Rodrygo L.T. Santos;Iadh Ounis
#t 2012
#c 1
#% 397161
#% 818262
#% 987243
#% 987356
#% 1074065
#% 1074113
#% 1227709
#% 1268491
#% 1467729
#% 1470617
#% 1482296
#% 1536512
#% 1560393
#% 1598342
#% 1599314
#% 1621236
#% 1674994
#! Learning to rank studies have mostly focused on query-dependent and query-independent document features, which enable the learning of ranking models of increased effectiveness. Modern learning to rank techniques based on regression trees can support query features, which are document-independent, and hence have the same values for all documents being ranked for a query. In doing so, such techniques are able to learn sub-trees that are specific to certain types of query. However, it is unclear which classes of features are useful for learning to rank, as previous studies leveraged anonymised features. In this work, we examine the usefulness of four classes of query features, based on topic classification, the history of the query in a query log, the predicted performance of the query, and the presence of concepts such as persons and organisations in the query. Through experiments on the ClueWeb09 collection, our results using a state-of-the-art learning to rank technique based on regression trees show that all four classes of query features can significantly improve upon an effective learned model that does not use any query feature.

#index 1919989
#* Session-based query performance prediction
#@ Andrey Kustarev;Yury Ustinovskiy;Anna Mazur;Pavel Serdyukov
#t 2012
#c 1
#% 1130868
#% 1292474
#% 1355038
#% 1450964
#% 1537504
#% 1598367
#% 1598368
#! Search sessions are known to be a rich source of diverse valuable information for individual query analysis. In this paper, we address the problem of query performance prediction by utilizing the entire logical search sessions containing the given query. Guided by the intuitions based on the observations made after the analysis of the search sessions' properties and performance of the queries they contain, we propose a number of features that significantly advance the existing query performance prediction models. Some of them specifically allow to focus on tail queries with sparse click-through statistics.

#index 1919990
#* A latent pairwise preference learning approach for recommendation from implicit feedback
#@ Yi Fang;Luo Si
#t 2012
#c 1
#% 577224
#% 643007
#% 1176909
#% 1268491
#% 1272396
#% 1292542
#% 1621481
#! Most of the current recommender systems heavily rely on explicit user feedback such as ratings on items to model users' interests. However, in many applications, it is very hard to collect the explicit feedback, while implicit feedback such as user clicks may be more available. Furthermore, it is often more suitable for many recommender systems to address a ranking problem than a rating predicting problem. This paper proposes a latent pairwise preference learning (LPPL) approach for recommendation with implicit feedback. LPPL directly models user preferences with respect to a set of items rather than the rating scores on individual items, which are modeled with a set of features by analyzing clickthrough data available in many real-world recommender systems. The LPPL approach models both the latent variables of group structure of users and the pairwise preferences simultaneously. We conduct experiments on the testbed from a real-world recommender system and demonstrate that the proposed approach can effectively improve the recommendation performance against several baseline algorithms.

#index 1919991
#* Topic based pose relevance learning in dance archives
#@ Reede Ren;John Collomosse;Joemon Jose
#t 2012
#c 1
#% 166097
#% 836690
#% 836851
#% 843891
#% 869236
#% 883972
#% 884118
#% 951455
#% 990309
#% 1041739
#% 1077613
#% 1169844
#% 1172327
#% 1279775
#% 1292838
#% 1350621
#% 1502487
#% 1593737
#% 1730629
#! This paper improves spatial pyramid kernel (SPK) and proposes a relevance learning approach to compare performer's poses in a large dance archive, the NRCD collection1. Domain knowledge of Choreutics is exploited to define pose topics and a selection operator is developed for pose topic matching. The visual structure descriptor of self similarity (SSF) is extended to hierarchical self similarity (HSSF) to keep shape context. The framework of Bag-of-Visual Words (BOVW) is applied to encode as well as to speed up the matching on pose topics/topic combinations. This alleviates the complexity in limb allocation which is infeasible in our data. Extensive experiments show that the new approach outperforms the original SPK in both precision and robustness.

#index 1919992
#* PhotoFall: discovering weblog stories through photographs
#@ Christopher Wienberg;Andrew S. Gordon
#t 2012
#c 1
#% 142615
#% 1043040
#% 1434155
#% 1441133
#% 1729386
#! An effective means of retrieving relevant photographs from the web is to search for terms that would likely appear in the surrounding text in multimedia documents. In this paper, we investigate the complementary search strategy, where relevant multimedia documents are retrieved using the photographs they contain. We concentrate our efforts on the retrieval of large numbers of personal stories posted to Internet weblogs that are relevant to a particular search topic. Photographs are often included in posts of this sort, typically taken by the author during the course of the narrated events of the story. We describe a new story search tool, PhotoFall, which allows users to quickly find stories related to their topic of interest by judging the relevance of the photographs extracted from top search results. We evaluate the accuracy of relevance judgments made using this interface, and discuss the implications of the results for improving topic-based searches of multimedia content.

#index 1919993
#* RESQ: rank-energy selective query forwarding for distributed search systems
#@ Amin Teymorian;Xiao Qin;Ophir Frieder
#t 2012
#c 1
#% 878624
#% 1292473
#% 1292623
#% 1450840
#% 1482451
#% 1598432
#% 1642163
#! Selective query forwarding is a promising technique to help scale high-quality and cost-efficient query evaluation in distributed search systems. The basic idea is simple. After a local site receives a query, it determines non-local sites to forward the query to and returns an aggregation of local and non-local results. We introduce "RESQ", a hybrid rank-energy selective query forwarding model. The novel contribution of RESQ is to simultaneously consider both ranking quality and energy costs when making forwarding decisions. Using a large-scale query log and publicly-available energy price time series, we demonstrate the ability of RESQ forwarding to achieve favorable tradeoffs between the possibility of returning high ranking query results and savings in temporally- and spatially-varying energy prices.

#index 1919994
#* The face of quality in crowdsourcing relevance labels: demographics, personality and labeling accuracy
#@ Gabriella Kazai;Jaap Kamps;Natasa Milic-Frayling
#t 2012
#c 1
#% 1047347
#% 1074134
#% 1083692
#% 1150163
#% 1252624
#% 1264744
#% 1384364
#% 1384503
#% 1452857
#% 1478132
#% 1480225
#% 1556468
#% 1598354
#! Information retrieval systems require human contributed relevance labels for their training and evaluation. Increasingly such labels are collected under the anonymous, uncontrolled conditions of crowdsourcing, leading to varied output quality. While a range of quality assurance and control techniques have now been developed to reduce noise during or after task completion, little is known about the workers themselves and possible relationships between workers' characteristics and the quality of their work. In this paper, we ask how do the relatively well or poorly-performing crowds, working under specific task conditions, actually look like in terms of worker characteristics, such as demographics or personality traits. Our findings show that the face of a crowd is in fact indicative of the quality of their work.

#index 1919995
#* Data filtering in humor generation: comparative analysis of hit rate and co-occurrence rankings as a method to choose usable pun candidates
#@ Pawel Dybala;Rafal Rzepka;Kenji Araki;Kohichi Sayama
#t 2012
#c 1
#% 1134812
#% 1370239
#! In this paper we propose a method of filtering excessive amount of textual data acquired from the Internet. In our research on pun generation in Japanese we experienced problems with extensively long data processing time, caused by the amount of phonetic candidates generated (i.e. phrases that can be used to generate actual puns) by our system. Simple, naive approach in which we take into considerations only phrases with the highest occurrence in the Internet, can effect in deletion of those candidates that are actually usable. Thus, we propose a data filtering method in which we compare two Internet-based rankings: a co-occurrence ranking and a hit rate ranking, and select only candidates which occupy the same or similar positions in these rankings. In this work we analyze the effects of such data reduction, considering 1 cases: when the candidates are on exactly the same positions in both rankings, and when their positions differ by 1, 2, 3 and 4. The analysis is conducted on data acquired by comparing pun candidates generated by the system (and filtered with our method) with phrases that were actually used in puns created by humans. The results show that the proposed method can be used to filter excessive amounts of textual data acquired from the Internet.

#index 1919996
#* Predicting primary categories of business listings for local search
#@ Changsung Kang;Jeehaeng Lee;Yi Chang
#t 2012
#c 1
#% 275837
#% 344447
#% 458379
#% 465754
#% 465895
#% 1550750
#% 1693907
#! We consider the problem of identifying primary categories of a business listing among the categories provided by the owner of the business. The category information submitted by business owners cannot be trusted with absolute certainty since they may purposefully add some secondary or irrelevant categories to increase recall in local search results, which makes category search very challenging for local search engines. Thus, identifying primary categories of a business is a crucial problem in local search. This problem can be cast as a multi-label classification problem with a large number of categories. However, the large scale of the problem makes it infeasible to use conventional supervised-learning-based text categorization approaches. We propose a large-scale classification framework that leverages multiple types of classification labels to produce a highly accurate classifier with fast training time. We effectively combine the complementary label sources to refine prediction. The experimental results indicate that our framework achieves very high precision and recall and outperforms a Centroid-based method.

#index 1919997
#* Where do the query terms come from?: an analysis of query reformulation in collaborative web search
#@ Zhen Yue;Jiepu Jiang;Shuguang Han;Daqing He
#t 2012
#c 1
#% 1047490
#% 1185582
#% 1224723
#% 1480247
#% 1629582
#% 1706000
#! This paper presents a user study aiming to investigate the query reformulation in collaborative Web search. 7 pairs of participants were recruited and each pair worked as a team on two collaborative exploratory Web search tasks. Through the log analysis, we compared possible sources for participants to draw query terms from. The results show that both search and collaborative actions are possible resources for new query terms. Traditional resources for query expansion such as previous search histories and relevant documents are still important resources for new query terms. The content in chat and workspace generated by participants themselves seems more likely to be the resource for new query terms than that of their partners. Task types also affect the influences on query reformulations. For the academic task, previously saved relevance documents are the most important resources for new query terms while chat histories are the most important resources for the leisure task.

#index 1919998
#* Learning to recommend with social relation ensemble
#@ Lei Guo;Jun Ma;Zhumin Chen;Haoran Jiang
#t 2012
#c 1
#% 1001279
#% 1130901
#% 1227602
#% 1275183
#% 1536533
#! Recommender systems with social networks (RSSN) have been well studied in recent works. However, these methods ignore the relationships among items, which may affect the quality of recommendations. Motivated by the observation that related items often have similar ratings, we propose a framework integrating items' relations, users' social graph and user-item rating matrix for recommendation. Experimental results show that our approach performs better than the state-of-art algorithm and the method with only users' social graph ensemble in terms of MAP and RMSE.

#index 1919999
#* A scalable approach for performing proximal search for verbose patent search queries
#@ Sumit Bhatia;Bin He;Qi He;Scott Spangler
#t 2012
#c 1
#% 342963
#% 768898
#% 879651
#% 987229
#% 987289
#% 995920
#% 1074110
#% 1074112
#% 1450865
#% 1450905
#! Even though queries received by traditional information retrieval systems are quite short, there are many application scenarios where long natural language queries are more effective. Further, incorporating term position information can help improve results of long queries. However, the techniques for incorporating term position information have been developed for terse queries and hence, can not be directly applied to long queries. Though there exist some methods for performing proximal search for long queries, they are not scalable due to long query response times. We describe an intuitive and simple, yet effective technique that implicitly incorporates term position information for long queries in a scalable manner. Our proposed approach achieves more than 700% faster query response times while maintaining the quality of retrieved results when compared with a state-of-the-art method for performing proximal search for very long queries.

#index 1920000
#* Is wikipedia too difficult?: comparative analysis of readability of wikipedia, simple wikipedia and britannica
#@ Adam Jatowt;Katsumi Tanaka
#t 2012
#c 1
#% 1260690
#! Readability is one of key factors determining document quality and reader's satisfaction. In this paper we analyze readability of Wikipedia, which is a popular source of information for searchers about unknown topics. Although Wikipedia articles are frequently listed by search engines on top ranks, they are often too difficult for average readers searching information about difficult queries. We examine the average readability of content in Wikipedia and compare it to the one in Simple Wikipedia and Britannica. Next, we investigate readability of selected categories in Wikipedia. Apart from standard readability measures we use some new metrics based on words' popularity and their distributions across different document genres and topics.

#index 1920001
#* Finding food entity relationships using user-generated data in recipe service
#@ Young-joo Chung
#t 2012
#c 1
#% 869484
#% 1074164
#! Rakuten recipe is a recipe site where users can submit their recipes and share with the others. Since recipe contents are generated by users, they usually contain many misspellings, abbreviations, synonyms, hypernyms and hyponyms. Identifying and normalizing these words is essential to retrieve relevant recipes to user's request. In this paper, we introduce a new approach to finding related words in a recipe domain using the data structure. Based on the observation that people usually write the main ingredient in the first position of ingredient lists of each recipe and such a ingredient is strongly related to the categories where recipes belong, we calculate relation scores of word pairs using real service data, which contains 790 categories and 405,519 recipes. The experimental result showed that we successfully found semantically related word pairs with f-score of 0.93.

#index 1920002
#* SRGSIS: a novel framework based on social relationship graph for social image search
#@ Bo Lu;Ye Yuan;Guoren Wang
#t 2012
#c 1
#% 349208
#% 1131835
#% 1279769
#% 1648838
#% 1649025
#% 1775905
#! Tag-based social image search predominately focus on using user-annotated tags to find out the results of user query. However, the performance of tag-based social image search is usually unable to satisfy the needs of users. In this paper, we propose a novel framework based on Social Relationship Graph for Social Image Search (SRGSIS), which involves two stages. In the first stage, we use heterogeneous data from multiple modalities to build a social relationship graph. Then, for the given query keywords, we execute an efficient keyword search algorithm over the social relationship graph and obtain top-k candidate results based on relevance score. We model these results as the answer trees connecting keyword nodes that match keywords in the query. In the second stage, for refining the candidate results, each image in social relationship graph is represented as a region adjacency graph by using the visual content of image. We further model these region adjacency graphs as a closure tree and compute approximate graph similarity between the candidate results and the closure tree to obtain more desirable results. Extensive experimental results demonstrate the effectiveness of the proposed approach.

#index 1920003
#* Exploring simultaneous keyword and key sentence extraction: improve graph-based ranking using wikipedia
#@ Xun Wang;Lei Wang;Jiwei Li;Sujian Li
#t 2012
#c 1
#% 268079
#% 290830
#% 397137
#% 1019082
#% 1130858
#! Summarization and Keyword Selection are two important tasks in NLP community. Although both aim to summarize the source articles, they are usually treated separately by using sentences or words. In this paper, we propose a two-level graph based ranking algorithm to generate summarization and extract keywords at the same time. Previous works have reached a consensus that important sentence is composed by important keywords. In this paper, we further study the mutual impact between them through context analysis. We use Wikipedia to build a two-level concept-based graph, instead of traditional term-based graph, to express their homogenous relationship and heterogeneous relationship. We run PageRank and HITS rank on the graph to adjust both homogenous and heterogeneous relationships. A more reasonable relatedness value will be got for key sentence selection and keyword selection. We evaluate our algorithm on TAC 2011 data set. Traditional term-based approach achieves a score of 0.255 in ROUGE-1 and a score of 0.037 and ROUGE-2 and our approach can improve them to 0.323 and 0.048 separately.

#index 1920004
#* Estimating query difficulty for news prediction retrieval
#@ Nattiya Kanhabua;Kjetil Nørvåg
#t 2012
#c 1
#% 397161
#% 926881
#% 953062
#% 1133171
#% 1195854
#% 1213423
#% 1263244
#% 1263248
#% 1415713
#% 1415785
#% 1467729
#% 1598409
#! News prediction retrieval has recently emerged as the task of retrieving predictions related to a given news story (or a query). Predictions are defined as sentences containing time references to future events. Such future-related information is crucially important for understanding the temporal development of news stories, as well as strategies planning and risk management. The aforementioned work has been shown to retrieve a significant number of relevant predictions. However, only a certain news topics achieve good retrieval effectiveness. In this paper, we study how to determine the difficulty in retrieving predictions for a given news story. More precisely, we address the query difficulty estimation problem for news prediction retrieval. We propose different entity-based predictors used for classifying queries into two classes, namely, Easy and Difficult. Our prediction model is based on a machine learning approach. Through experiments on real-world data, we show that our proposed approach can predict query difficulty with high accuracy.

#index 1920005
#* Recency-sensitive model of web page authority
#@ Maxim Zhukovskiy;Dmitry Vinogradov;Gleb Gusev;Pavel Serdyukov;Andrei Raigorodskii
#t 2012
#c 1
#% 754088
#% 807658
#% 869600
#% 907542
#% 967260
#% 1019188
#% 1450843
#! Traditional link-based web ranking algorithms run on a single web snapshot without concern of the dynamics of web pages and links. In particular, the correlation of web pages freshness and their classic PageRank is negative (see [11]). For this reason, in recent years a number of authors introduce some algorithms of PageRank actualization. We introduce our new algorithm called Actual PageRank, which generalizes some previous approaches and therefore provides better capability for capturing the dynamics of the Web. To the best of our knowledge we are the first to conduct ranking evaluations of a fresh-aware variation of PageRank on a large data set. The results demonstrate that our method achieves more relevant and fresh results than both classic PageRank and its "fresh" modifications.

#index 1920006
#* Evaluating reward and risk for vertical selection
#@ Ke Zhou;Ronan Cummins;Mounia Lalmas;Joemon M. Jose
#t 2012
#c 1
#% 194246
#% 397161
#% 643012
#% 1130914
#% 1227616
#% 1392444
#% 1536576
#% 1587348
#% 1641937
#% 1641985
#% 1642923
#% 1806040
#% 1879004
#! The aggregation of search results from heterogeneous verticals (news, videos, blogs, etc) has become an important consideration in search. When aiming to select suitable verticals, from which items are selected to be shown along with the standard "ten blue links", there exists the potential to both help (selecting relevant verticals) and harm (selecting irrelevant verticals) the existing result set. In this paper, we present an approach that considers both reward and risk within the task of vertical selection (VS). We propose a novel risk-aware VS evaluation metric that incorporates users' risk-levels and users' individual preference of verticals. Using the proposed metric, we present a detailed analysis of both reward and risk of current resource selection approaches within a multi-label classification framework. The results bring insights into the effectiveness and robustness of current vertical selection approaches.

#index 1920007
#* Contextual evaluation of query reformulations in a search session by user simulation
#@ Jiepu Jiang;Daqing He;Shuguang Han;Zhen Yue;Chaoqun Ni
#t 2012
#c 1
#% 818222
#% 1074133
#% 1095876
#% 1227623
#% 1355020
#% 1415709
#% 1598439
#% 1641981
#% 1712595
#% 1919997
#! We propose a method to dynamically estimate the utility of documents in a search session by modeling the users' browsing behaviors and novelty. The method can be applied to evaluate query reformulations in a search session.

#index 1920008
#* Information-complete and redundancy-free keyword search over large data graphs
#@ Byron J. Gao;Zhumin Chen;Qi Kang
#t 2012
#c 1
#% 330678
#% 660011
#% 824693
#% 956599
#% 960259
#% 993987
#% 1019060
#% 1063537
#% 1127445
#% 1207007
#! Keyword search over graphs has a wide array of applications in querying structured, semi-structured and unstructured data. Existing models typically use minimal trees or bounded subgraphs as query answers. While such models emphasize relevancy, they would suffer from incompleteness of information and redundancy among answers, making it difficult for users to effectively explore query answers. To overcome these drawbacks, we propose a novel cluster-based model, where query answers are relevancy-connected clusters. A cluster is a subgraph induced from a maximal set of relevancy-connected nodes. Such clusters are coherent and relevant, yet complete and redundancy free. They can be of arbitrary shape in contrast to the sphere-shaped bounded subgraphs in existing models. We also propose an efficient search algorithm and a corresponding graph index for large, disk-resident data graphs.

#index 1920009
#* Spatial-aware interest group queries in location-based social networks
#@ Yafei Li;Dingming Wu;Jianliang Xu;Byron Choi;Weifeng Su
#t 2012
#c 1
#% 427199
#% 867054
#% 1206997
#% 1328137
#% 1573239
#% 1581877
#! Location-based social networks, such as Foursquare and Facebook Places, are bridging the gap between the physical world and online social networking services through acquired user locations. Some social networks released check-in services that allow users to share their visiting locations with their friends. In this paper, users' interests are modeled by check-in actions. We propose a new spatial-aware interest group (SIG) query that retrieves a user group of size k where every user is highly interested in the query keyword and also spatially close to each other. An efficient algorithm AIR based on the IR-tree is proposed for the processing of SIG queries. Furthermore, an optimization is developed and achieves a much better performance than the baseline algorithm.

#index 1920010
#* Probabilistic ranking in fuzzy object databases
#@ Thomas Bernecker;Tobias Emrich;Hans-Peter Kriegel;Matthias Renz;Andreas Züfle
#t 2012
#c 1
#% 333977
#% 481947
#% 527158
#% 527167
#% 654487
#% 879211
#% 893167
#% 915262
#% 993955
#% 1044463
#% 1328151
#% 1426560
#% 1464053
#% 1523852
#% 1872204
#! Ranking queries have been investigated extensively in the past due to their broad range of applications. In this paper, we study this problem in the context of fuzzy objects that have indeterministic boundaries. Fuzzy objects play an important role in many areas, such as biomedical image databases and GIS. To the best of our knowledge, we present the first efficient approach for similarity ranking in fuzzy object databases. The main challenge of ranking fuzzy objects is that these objects consist of multiple instances, each associated with a probability. We propose a framework to transform fuzzy objects into probabilistic objects which can then be ranked using existing algorithms for probabilistic objects.

#index 1920011
#* Enabling ontology based semantic queries in biomedical database systems
#@ Shuai Zheng;Fusheng Wang;James Lu;Joel Saltz
#t 2012
#c 1
#! While current biomedical ontology repositories offer primitive query capabilities, it is difficult or cumbersome to support ontology based semantic queries directly in semantically annotated biomedical databases. The problem may be largely attributed to the mismatch between the models of the ontologies and the databases, and the mismatch between the query interfaces of the two systems. To fully realize semantic query capabilities based on ontologies, we develop a system DBOntoLink to provide unified semantic query interfaces by extending database query languages. With DBOntoLink, semantic queries can be directly and naturally specified as extended functions of the database query languages without any programming needed. DBOntoLink is adaptable to different ontologies through customizations and supports major biomedical ontologies hosted at the NCBO BioPortal. We demonstrate the use of DBOntoLink in a real world biomedical database with semantically annotated medical image annotations.

#index 1920012
#* Similarity search in 3D object-based video data
#@ Jakub Lokoč;Jürgen Wünschmann;Tomáš Skopal;Albrecht Rothermel
#t 2012
#c 1
#% 342827
#% 576978
#% 857113
#% 865834
#% 974353
#% 1165734
#% 1583894
#% 1586191
#% 1642257
#% 1831268
#! In this paper, we present the vision of the usage of an object-based video data storage format for similarity search. The efficient (fast) and effective (accurate) search in video streams is an ongoing and still unsolved problem. Using an object-based format of multimedia data, all the information that is needed to answer queries is already available in a machine accessible format. This way, the process of creating (video) descriptors as well as the similarity search becomes easier, because the data is already organized in a manner that allows fast access to specific information. To demonstrate the concept of similarity search process using the object-based 3D video format, we present experiments conducted on generated clouds of points (an abstraction of 3D video data).

#index 1920013
#* Continuous top-k query for graph streams
#@ Shirui Pan;Xingquan Zhu
#t 2012
#c 1
#% 601159
#% 989610
#% 1070887
#% 1083718
#! In this paper, we propose to query correlated graphs in a data stream scenario, where an algorithm is required to retrieve the top k graphs which are mostly correlated to a query graph q. Due to the dynamic changing nature of the stream data and the inherent complexity of the graph query process, treating graph streams as static datasets is computationally infeasible or ineffective. In the paper, we propose a novel algorithm, Hoe-PGPL, to identify top-k correlated graphs from data stream, by using a sliding window which covers a number of consecutive batches of stream data records. Our theme is to employ Hoeffding bound to discover some potential candidates and use two level candidate checking (one corresponding to the whole sliding window level and one corresponding to the local data batch level) to accurately estimate the correlation of the emerging candidate patterns, without rechecking the historical stream data. Experimental results demonstrate that the proposed algorithm not only achieves good performance in terms of query precision and recall, but also is several times, or even an order of magnitude, more efficient than the straightforward algorithm with respect to the time and the memory consumption. Our method represents the first research endeavor for data stream based top-k correlated graph query.

#index 1920014
#* Latent topics in graph-structured data
#@ Christoph Böhm;Gjergji Kasneci;Felix Naumann
#t 2012
#c 1
#% 1063546
#% 1399996
#! Large amounts of graph-structured data are emerging from various avenues, ranging from natural and life sciences to social and semantic web communities. We address the problem of discovering subgraphs of entities that reflect latent topics in graph-structured data. These topics are structured meta-information providing further insights into the data. The presented approach effectively detects such topics by exploiting only the structure of the underlying graph, thus avoiding the dependency on textual labels, which are a scarce asset in prevalent graph datasets. The viability of our approach is demonstrated in experiments on real-world datasets.

#index 1920015
#* Fast and accurate incremental entity resolution relative to an entity knowledge base
#@ Michael J. Welch;Aamod Sane;Chris Drome
#t 2012
#c 1
#% 310516
#% 913783
#% 1201863
#% 1206834
#% 1538763
#% 1544125
#% 1606044
#% 1643402
#% 1959952
#! User facing topical web applications such as events or shopping sites rely on large collections of data records about real world entities that are updated at varying latencies ranging from days to seconds. For example, event venue details are changed relatively infrequently whereas ticket pricing and availability for an event is often updated in near-realtime. Users regard these sites as high quality if they seldom show duplicates, the URLs are stable, and their content is fresh, so it is important to resolve duplicate entity records with high quality and low latencies. High quality entity resolution typically evaluates the entire record corpus for similar record clusters at the cost of latency, while low latency resolution examines the least possible entities to keep time to a minimum, even at the cost of quality. In this paper we show how to keep low latency while achieving high quality, combining the best of both approaches: given an entity to be resolved, our incremental Fastpath system, in a matter of milliseconds, makes approximately the same decisions that the underlying batch system would have made. Our experiments show that the Fastpath system makes matching decisions for previously unseen entities with 90% precision and 98% recall relative to batch decisions, with latencies under 20ms on commodity hardware.

#index 1920016
#* LUKe and MIKe: learning from user knowledge and managing interactive knowledge extraction
#@ Steffen Metzger;Michael Stoll;Katja Hose;Ralf Schenkel
#t 2012
#c 1
#% 782759
#% 1089602
#% 1190118
#% 1523947
#% 1543087
#! Semantic recognition and annotation of unqiue enities and their relations is a key in understanding the essence contained in large text corpora. It typically requires a combination of efficient automatic methods and manual verification. Usually, both parts are seen as consecutive steps. In this demo we present MIKE, a user interface enabling the integration of user feedback into an iterative extraction process. We show how an extraction system can directly learn from such integrated user supervision. In general, this setup allows for stepwise training of the extraction system to a particular domain, while using user feedback early in the iterative extraction process improves extraction quality and reduces the overall human effort needed.

#index 1920017
#* PRAVDA-live: interactive knowledge harvesting
#@ Yafang Wang;Maximilian Dylla;Zhaochun Ren;Marc Spaniol;Gerhard Weikum
#t 2012
#c 1
#% 956564
#% 1063570
#% 1089602
#% 1267783
#% 1355026
#% 1409954
#% 1536527
#% 1642010
#% 1711865
#% 1913401
#! Acquiring high-quality (temporal) facts for knowledge bases is a labor-intensive process. Although there has been recent progress in the area of semi-supervised fact extraction, these approaches still have limitations, including a restricted corpus, a fixed set of relations to be extracted or a lack of assessment capabilities. In this paper we introduce PRAVDA-live, a framework that overcomes these limitations and supports the entire pipeline of interactive knowledge harvesting. To this end, our demo exhibits fact extraction from ad-hoc corpus creation, via relation specification, labeling and assessment all the way to ready-to-use RDF exports.

#index 1920018
#* 4Is of social bully filtering: identity, inference, influence, and intervention
#@ Yunfei Chen;Lanbo Zhang;Aaron Michelony;Yi Zhang
#t 2012
#c 1
#! As the increasing of popularity of social web, cyber bullying has become a more and more serious issue among children. Bullying causes huge negative effects on children, even suicide. SocialFilter is a realtime system that helps parents and educators track children's messages on Twitter, especially in order to detect whether they have been bullied or bullying others. The aim of the system is 4 I's, identity of bullies, inference of bullying message, influence of bully behavior, and intervention. We solve this problem by using machine learning technique. The current system is tracking tens of thousands of active children users on Twitter and automatically detect bullying messages at real time.

#index 1920019
#* Lonomics Atlas: a tool to explore interconnected ionomic, genomic and environmental data
#@ Eduard C. Dragut;Mourad Ouzzani;Amgad Madkour;Nabeel Mohamed;Peter Baker;David E. Salt
#t 2012
#c 1

#index 1920020
#* CarbonDB: a semantic life cycle inventory database
#@ Benjamin Bertin;Vasile-Marian Scuturici;Jean-Marie Pinon;Emmanuel Risler
#t 2012
#c 1
#% 156337
#% 1855858
#% 1922736
#! We demonstrate CarbonDB, a web application for Life Cycle Inventory data management. Life Cycle Assessment provides a well-accepted methodology for modelling environmental impacts of human activities. This methodology relies on the decomposition of a studied system into interdependent processes in a phase called Life Cycle Inventory. Several organisations provide processes databases containing thousands of processes with their interdependency links. The usual workflow to manage those databases is based on the manipulation of individual processes, which turns out to be a very harnessing work even if there are strong semantic similarities between the involved processes. In previous publications, we proposed a new workflow for LCA inventory databases maintenance based on the addition of semantic information to the processes they contained. This method considerably eases the modeling process and offers a synthetic view of the dependencies links. We created a web application based on this approach composed of a back-end for data management and a front-end for searching processes and visualize the dependencies links in a graph.

#index 1920021
#* Supporting temporal analytics for health-related events in microblogs
#@ Nattiya Kanhabua;Sara Romano;Avaré Stewart;Wolfgang Nejdl
#t 2012
#c 1
#% 1400018
#% 1432574
#% 1472167
#% 1561563
#% 1711868
#% 1870560
#! Microblogging services, such as Twitter, are gaining interests as a means of sharing information in social networks. Numerous works have shown the potential of using Twitter posts (or tweets) in order to infer the existence and magnitude of real-world events. In the medical domain, there has been a surge in detecting public health related tweets for early warning so that a rapid response from health authorities can take place. In this paper, we present a temporal analytics tool for supporting a comparative, temporal analysis of disease outbreaks between Twitter and official sources, such as, World Health Organization (WHO) and ProMED-mail. We automatically extract and aggregate outbreak events from official outbreak reports, producing time series data. Our tool can support a correlation analysis and an understanding of the temporal developments of outbreak mentions in Twitter, based on comparisons with official sources.

#index 1920022
#* InCaToMi: integrative causal topic miner between textual and non-textual time series data
#@ Hyun Duk Kim;ChengXiang Zhai;Thomas A. Rietz;Daniel Diermeier;Meichun Hsu;Malu Castellanos;Carlos A. Ceja Limon
#t 2012
#c 1
#% 280819
#% 581406
#% 722904
#% 875959
#% 881498
#% 956510
#% 1451154
#! Topic modeling is popular for text mining tasks. Recently, topic modeling has been combined with time lines when textual data is related to external non-textual time series data such as stock prices. However, no previous work has used the external non-textual time series data in the process of topic modeling. In this paper, we describe a novel text mining system, Integrative Causal Topic Miner (InCaToMi) that integrates textual and non-textual time series data. InCaToMi automatically finds causal relationships and topics using text data and external non-textual time series data using Granger Testing. Moreover, InCaToMi considers the non-textual time series data in the topic modeling process, using the time series data to iteratively improve modeling results through interactions between it and the textual data at both topic and word levels.

#index 1920023
#* A tool for automated evaluation of algorithms
#@ Philipp Kranen;Stephan Wels;Tim Rohlfs;Sebastian Raubach;Thomas Seidl
#t 2012
#c 1
#% 1301004
#! Testing algorithms and systems involves trying different sets of parameter values on different domains or data sets. Even for a moderate number of parameters and domains the number of possible experiments can get very large due to the combinatorial explosion. Evaluating the outcome of these experiments requires comparing the results, which is often done by writing a script or inspecting the result files manually. For a new algorithm or version, the work has to be done over again. With hundreds, thousands, or even more possible experiments, both the preparation and the evaluation can become complex and tedious. In this demonstrator we present a software tool, called ET, for evaluating the parameters of an algorithm or system, either automatically or controlled by the user. It allows to launch large numbers of experiments in just a few clicks, visually explore the results and analyze the performance of the algorithm.

#index 1920024
#* A summarization tool for time-sensitive social media
#@ Walid Magdy;Ahmed Ali;Kareem Darwish
#t 2012
#c 1
#% 1536506
#% 1587367
#% 1587377
#% 1591966
#% 1641934
#% 1919955
#! Searching social content in general and microblogs (aka tweets) in particular has been basic and limited, especially for time-sensitive topics. The currently implemented microblog search on sites such as Twitter is based on simple word matching and retrieves the most recent microblogs that match a given query. Furthermore, a user may obtain hundreds or perhaps thousands of microblogs in response to a given query, leading to information overload. We present a new multidimensional microblog search tool that generates a comprehensive report from microblogs instead of a flat list of recent/relevant microblogs for a given query. Reports may include tag-clouds, topic time series, and most popular and funny microblogs, etc. The tool can be configured for monitoring time-sensitive topics using a set of predefined queries. We demonstrate our system on Arabic and English microblog collections. Additionally, we show a special configuration of the system for monitoring the 2012 Egyptian presidential elections.

#index 1920025
#* CrowdTiles: presenting crowd-based information for event-driven information needs
#@ Stewart Whiting;Ke Zhou;Joemon Jose;Omar Alonso;Teerapong Leelanupab
#t 2012
#c 1
#% 262043
#% 1482473
#% 1536521
#! Time plays a central role in many web search information needs relating to recent events. For recency queries where fresh information is most desirable, there is likely to be a great deal of highly-relevant information created very recently by crowds of people across the world, particularly on platforms such as Wikipedia and Twitter. With so many users, mainstream events are often very quickly reflected in these sources. The English Wikipedia encyclopedia consists of a vast collection of user-edited articles covering a range of topics. During events, users collaboratively create and edit existing articles in near real-time. Simultaneously, users on Twitter disseminate and discuss event details, with a small number of users becoming influential for the topic. In this demo, we propose a novel approach to presenting a summary of new information and users related to recent or ongoing events associated with the user's search topic, therefore aiding most recent information discovery. We outline methods to detect search topics which are driven by events, identify and extract changing Wikipedia article passages and find influential Twitter users. Using these, we provide a system which displays familiar tiles in search results to present recent changes in the event-related Wikipedia articles, as well as Twitter users who have tweeted recent relevant information about the event topics.

#index 1920026
#* ESA: emergency situation awareness via microbloggers
#@ Jie Yin;Sarvnaz Karimi;Bella Robinson;Mark Cameron
#t 2012
#c 1
#% 824666
#% 1384210
#% 1400018
#% 1632474
#% 1642282
#% 1746956
#% 1747102
#% 1938308
#! During a disastrous event, such as an earthquake or river flooding, information on what happened, who was affected and how, where help is needed, and how to aid people who were affected, is crucial. While communication is important in such times of crisis, damage to infrastructure such as telephone lines makes it difficult for authorities and victims to communicate. Microblogging has played a critical role as an important communication platform during crises when other media has failed. We demonstrate our ESA (Emergency Situation Awareness) system that mines microblogs in real-time to extract and visualise useful information about incidents and their impact on the community in order to equip the right authorities and the general public with situational awareness.

#index 1920027
#* Cager: a framework for cross-page search
#@ Zhumin Chen;Byron J. Gao;Qi Kang
#t 2012
#c 1
#% 330678
#% 660011
#% 824693
#% 956599
#% 993987
#% 1063537
#% 1207007
#! Existing search engines have page as the unit of information of retrieval. They typically return a ranked list of pages, each being a search result containing the query keywords. This within-one-page constraint disallows utilization of relationship information that is often available and greatly beneficial. To utilize relationship information and improve search precision, we explore cross-page search, where each answer is a logical page consisting of multiple closely related pages that collectively contain the query keywords. We have implemented a prototype Cager, providing cross-page search and visualization over real dataset.

#index 1920028
#* Mixed-initiative conversational system using question-answer pairs mined from the web
#@ Wilson Wong;Lawrence Cavedon;John Thangarajah;Lin Padgham
#t 2012
#c 1
#% 529463
#% 1183207
#% 1543962
#% 1642308
#! One of the biggest bottlenecks for conversational systems is large-scale provision of suitable content. Our approach readily provides this without the need for custom-crafting. In this demonstration, we present the use of question-answer (QA) pairs mined from online question-and-answer websites to construct system utterances for a conversational agent. Our system uses QA pairs to formulate utterances that drive a conversation in addition to the answering of user questions as has been done in previous work. We use a collection of strategies that specify how and when the different parts of our question-answer pairs can be used and augmented with a small number of generic hand-crafted text snippets to generate natural and coherent system utterances.

#index 1920029
#* PicAlert!: a system for privacy-aware image classification and retrieval
#@ Sergej Zerr;Stefan Siersdorfer;Jonathon Hare
#t 2012
#c 1
#% 269217
#% 760805
#% 1878996
#! Photo publishing in Social Networks and other Web2.0 applications has become very popular due to the pervasive availability of cheap digital cameras, powerful batch upload tools and a huge amount of storage space. A portion of uploaded images are of a highly sensitive nature, disclosing many details of the users' private life. We have developed a web service which can detect private images within a user's photo stream and provide support in making privacy decisions in the sharing context. In addition, we present a privacy-oriented image search application which automatically identifies potentially sensitive images in the result set and separates them from the remaining pictures.

#index 1920030
#* TASE: a time-aware search engine
#@ Sheng Lin;Peiquan Jin;Xujian Zhao;Lihua Yue
#t 2012
#c 1
#% 1227692
#% 1251781
#% 1415764
#% 1544175
#% 1598529
#% 1692327
#% 1697416
#! Most Web pages contain temporal information, which can be utilized by search engines to improve searching performance for users. However, traditional search engines have little support in processing temporal-textual Web queries. Aiming at solving this problem, in this paper we present and implement a prototype system for time-sensitive queries, which is called TASE (Time-Aware Search Engine). TASE extracts both the explicit and implicit temporal expressions for each Web page, and calculates the relevant score between the Web page and each temporal expression, and then re-rank search results based on the temporal-textual relevance between Web pages and the queries. It is demonstrated that TASE can improve the effectiveness of temporal-textual Web queries.

#index 1920031
#* Gumshoe quality toolkit: administering programmable search
#@ Zhuowei Bao;Benny Kimelfeld;Yunyao Li;Sriram Raghavan;Huahai Yang
#t 2012
#c 1
#% 577339
#% 768898
#% 869548
#% 879634
#% 956543
#% 1399933
#% 1560359
#% 1581844
#% 1616237
#% 1879051
#! Enterprise search is challenging due to various reasons, notably the dynamic terminology and domain structure that are specific to the enterprise, combined with the fact that search deployments are typically managed by domain experts who are not necessarily search experts. To address that, it has been proposed to design search architectures that feature two principles: comprehensibility of the ranking mechanism and customizability of the search engine by means of intuitive runtime rules. The proposed demonstration operates on top of an engine implementation based on this search philosophy, and provides an administrator toolkit to realize the two principles. In particular, the toolkit provides a complete visualization of the provenance (hence ranking) of search results, embeds an editor for programming runtime rules, facilitates the investigation of (the cause of) missing or low-ranked desired results, and provides suggestions of rewrite rules to handle such results.

#index 1920032
#* Simultaneous realization of page-centric communication and search
#@ Yuhki Shiraishi;Jianwei Zhang;Yukiko Kawai;Toyokazu Akiyama
#t 2012
#c 1
#% 879567
#% 987212
#% 1523928
#! We present a novel system that combines the advantages of social communication and Web search by simultaneously discovering important pages and users. First, the system provides a communication interface attached to pages, which allows users to talk with each other in real time while browsing the same page, i.e., page-centric communication. Then, the system can efficiently provide two ranking lists of pages and users by analyzing a hybrid structure of hyperlinks (page-page relationship) and social links (page-user relationship and user-user relationship). Thus, users can efficiently search for important pages as well as important users related to their queries through the ranking function, and immediately obtain useful information or knowledge from not only pages themselves but also from other users.

#index 1920033
#* MOUNA: mining opinions to unveil neglected arguments
#@ Mouna Kacimi;Johann Gamper
#t 2012
#c 1
#% 290482
#% 1035591
#% 1074133
#% 1166473
#% 1190093
#% 1482296
#% 1587424
#% 1641923
#! A query topic can be subjective involving a variety of opinions, judgments, arguments, and many other debatable aspects. Typically, search engines process queries independently from the nature of their topics using a relevance-based retrieval strategy. Hence, search results about subjective topics are often biased towards a specific view point or version. In this demo, we shall present MOUNA, a novel approach for opinion diversification. Given a query on a subjective topic, MOUNA ranks search results based on three scores: (1) relevance of documents, (2) semantic diversity to avoid redundancy and capture the different arguments used to discuss the query topic, and (3) sentiment diversity to cover a balanced set of documents having positive, negative, and neutral sentiments about the query topic. Moreover, MOUNA enhances the representation of search results with a summary of the different arguments and sentiments related to the query topic. Thus, the user can navigate through the results and explore the links between them. We provide an example scenario in this demonstration to illustrate the inadequacy of relevance-based techniques for searching subjective topics and highlight the innovative aspects of MOUNA. A video showing the demo can be found in http://www.youtube.com/user/mounakacimi/videos .

#index 1920034
#* MAGIK: managing completeness of data
#@ Ognjen Savković;Mirza Paramita;Sergey Paramonov;Werner Nutt
#t 2012
#c 1
#% 67457
#% 481786
#% 880394
#% 1217124
#% 1426458
#! MAGIK demonstrates how to use meta-information about the completeness of a database to assess the quality of the answers returned by a query. The system holds so-called table-completeness (TC) statements, by which one can express that a table is partially complete, that is, it contains all facts about some aspect of the domain. Given a query, MAGIK determines from such meta-information whether the database contains sufficient data for the query answer to be complete. If, according to the TC statements, the database content is not sufficient for a complete answer, MAGIK explains which further TC statements are needed to guarantee completeness. MAGIK extends and complements theoretical work on modeling and reasoning about data completeness by providing the first implementation of a reasoner. The reasoner operates by translating completeness reasoning tasks into logic programs, which are executed by an answer set engine.

#index 1920035
#* Exploration of monte-carlo based probabilistic query processing in uncertain graphs
#@ Tobias Emrich;Hans-Peter Kriegel;Johannes Niedermayer;Matthias Renz;André Suhartha;Andreas Züfle
#t 2012
#c 1
#% 863390
#% 1015321
#% 1189215
#% 1523884
#! This demo presents a framework for running probabilistic graph queries on uncertain graphs and visualizing their results. The framework supports the most common uncertainty model for uncertain graphs, i.e. existential uncertainty for the edges of the graph. A large variety of meaningful graph queries are supported, such as shortest path, range, kN, reverse kN, reachability and various aggregation queries. Since the problem of exact probability computation according to possible world semantics is in #P-Time for many combinations of model and query, and since ignoring uncertainty (e.g. by using expectations only) will yield counterintuitive and hard to interpret results, our framework uses an optimized version of Monte-Carlo sampling to estimate the results which allows us not only to perform queries that conform to possible world semantics but also to sample only parts of a graph relevant for a given query. The main strength of this framework is the visualization combined with statistic hypothesis tests, which gives the user not only the estimated result of a query, but also an indication of how significant and reliable these results are. The aim of this demonstration is to give an intuition that a sampling based approach to probabilistic graphs is viable, and that the estimated results quickly converge even for very large graphs. A video demonstrating our framework can be downloaded at http://www.dbs.ifi.lmu.de/Publikationen/videos/PGraph.html

#index 1920036
#* The nautilus analyzer: understanding and debugging data transformations
#@ Melanie Herschel;Hanno Eichelberger
#t 2012
#c 1
#% 44876
#% 318704
#% 1127409
#% 1217186
#% 1231247
#% 1328076
#% 1426503
#% 1523811
#! When developing data transformations - a task omnipresent in applications like data integration, data migration, data cleaning, or scientific data processing -developers quickly face the need to verify the semantic correctness of the transformation. Declarative specifications of data transformations, e.g., SQL or ETL tools, increase developer productivity but usually provide limited or no means for inspection or debugging. In this situation, developers today have no choice but to manually analyze the transformation and, in case of an error, to (repeatedly) fix and test the transformation. The goal of the Nautilus project is to semi-automatically support this analysis-fix-test cycle. This demonstration focuses on one main component of Nautilus, namely the Nautilus Analyzer that helps developers in understanding and debugging their data transformations. The demonstration will show the capabilities of this component for data transformations specified in SQL on scenarios from different domains that are based on real-world data. We provide an overview the Nautilus Analyzer, discuss components and implementation techniques, and outline our demonstration plan. The Nautilus website (http://nautilus-system.org) features a video, screenshots, and further details.

#index 1920037
#* Demonstrating ProApproX 2.0: a predictive query engine for probabilistic XML
#@ Asma Souihli;Pierre Senellart
#t 2012
#c 1
#% 58608
#% 1206759
#% 1291113
#% 1291120
#% 1581975
#! ProApproX 2.0 allows users to query uncertain tree-structured data in the form of probabilistic XML documents. The demonstrated version integrates a fully redesigned query engine that, first, produces a propositional formula that represents the probabilistic lineage of a given answer over the probabilistic XML document, and, second, searches for an optimal strategy to approximate the probability of the lineage. This latter part relies on a query-optimizer-like approach: exploring different evaluation plans for different parts of the formula and predicting the cost of each plan, using a cost model for the various evaluation algorithms. The demonstration presents the graphical user interface of ProApproX 2.0, that allows a user to input an XPath query and approximation parameters, and lists query results with their probabilities; the interface also gives insight into the way the computation is performed, by displaying the compilation of the query lineage as a tree annotated with evaluation operators.

#index 1920038
#* HadoopXML: a suite for parallel processing of massive XML data with multiple twig pattern queries
#@ Hyebong Choi;Kyong-Ha Lee;Soo-Hyong Kim;Yoon-Joon Lee;Bongki Moon
#t 2012
#c 1
#% 397375
#% 480489
#% 731408
#% 1023420
#% 1523839
#% 1667309
#! The volume of XML data is tremendous in many areas, but especially in data logging and scientific areas. XML data in the areas are accumulated over time as new data are continuously collected. It is a challenge to process massive XML data with multiple twig pattern queries given by multiple users in a timely manner. We showcase HadoopXML, a system that simultaneously processes many twig pattern queries for a massive volume of XML data with Hadoop. Specifically, HadoopXML provides an efficient way to process a single large XML file in parallel. It processes multiple twig pattern queries simultaneously with a shared input scan. Users do not need to iterate M/R jobs for each query. HadoopXML also reduces many I/Os by enabling twig pattern queries to share their path solutions each other. Moreover, HadoopXML provides a sophisticated runtime load balancing scheme for fairly assigning multiple twig pattern joins across nodes. With synthetic and real world XML dataset, we demonstrate how efficiently HadoopXML processes many twig pattern queries in a shared and balanced way.

#index 1920039
#* MADden: query-driven statistical text analytics
#@ Christan Earl Grant;Joir-dan Gumbs;Kun Li;Daisy Zhe Wang;George Chitouras
#t 2012
#c 1
#% 1127378
#% 1183371
#% 1581889
#% 1895052
#! In many domains, structured data and unstructured text are both important natural resources to fuel data analysis. Statistical text analysis needs to be performed over text data to extract structured information for further query processing. Typically, developers will need to connect multiple tools to build off-line batch processes to perform text analytic tasks. MADden is an integrated system developed for relational database systems such as PostgreSQL and Greenplum for real-time ad hoc query processing over structured and unstructured data. MADden implements four important text analytic functions that we have contributed to the MADlib open source library for textual analytics. In this demonstration, we will show the capability of the MADden text analytic library using computational journalism as the driving application. We show real-time declarative query processing over multiple data sources with both structured and text information.

#index 1920040
#* STFMap: query- and feature-driven visualization of large time series data sets
#@ K. Selçuk Candan;Rosaria Rossini;Maria Luisa Sapino;Xiaolan Wang
#t 2012
#c 1
#% 460919
#% 760805
#% 841783
#% 873056
#% 993965
#% 1880466
#! Since many applications rely on time-based data, visualizing temporal data and helping experts explore large time series data sets are critical in many application domains. In this interactive system preview, we argue that time series often carry structural features that can, if efficiently identified and effectively visualized, help reduce visual overload and help the user quickly focus on the relevant portions of the data sets. Relying on this observation, we introduce a novel STFMap system, which includes four innovative query- and feature-driven time series data set visualization techniques: (a) segment-maps, (b) warp-maps, (c) stretch-maps, and (d) feature-maps. These rely on the salient temporal features of the time series and their alignments with respect to the given user query to help users explore the data set in a query-driven fashion.

#index 1920041
#* Primates: a privacy management system for social networks
#@ Imen Ben Dhia;Talel Abdessalem;Mauro Sozio
#t 2012
#c 1
#% 754098
#% 1071523
#% 1261446
#% 1399968
#% 1400103
#% 1560413
#% 1586961
#! While online social networks (OSN) present unprecedented opportunities for sharing information and multimedia content among users, they raise major privacy issues as users could often access personal or confidential data of other users. Most social networks provide some basic access control policies, which however seem to be very limited given the diversity of user relationships in the current social networks (e.g. friend, acquaintance, son) as well as the needs of social network users who might want to express sophisticated access control policies (e.g. invite all children of my colleagues to my child's birthday party). In this demonstration proposal, we present Primates a privacy management system for social networks. Primates allows users to specify access control rules for their resources and enforces access control over all shared resources. The set of users who are allowed to access a given resource is defined by a set of constraints on the paths connecting the owner of a resource to its requester in the social graph. We demonstrate the accuracy of our access control model and the scalability of our system.

#index 1920042
#* AMADA: web data repositories in the amazon cloud
#@ Andrés Aranda-Andújar;Francesca Bugiotti;Jesús Camacho-Rodríguez;Dario Colazzo;François Goasdoué;Zoi Kaoudi;Ioana Manolescu
#t 2012
#c 1
#% 963669
#% 1063488
#% 1366460
#% 1426486
#% 1426550
#% 1581873
#% 1602034
#% 1855853
#% 1901452
#! We present AMADA, a platform for storing Web data (in particular, XML documents and RDF graphs) based on the Amazon Web Services (AWS) cloud infrastructure. AMADA operates in a Software as a Service (SaaS) approach, allowing users to upload, index, store, and query large volumes of Web data. The demonstration shows (i) the step-by-step procedure for building and exploiting the warehouse (storing, indexing, querying) and (ii) the monitoring tools enabling one to control the expenses (monetary costs) charged by AWS for the operations involved while running AMADA.

#index 1920043
#* DUBMMSM'12: international workshop on data-driven user behavioral modeling and mining from social media
#@ Jalal Mahmud;James Caverlee;Jeffrey Nichols;John O' Donovan;Michelle Zhou
#t 2012
#c 1
#% 1287290
#% 1400018
#% 1482657
#% 1573369
#% 1711658
#! Massive amounts of data are being generated on social media sites, such as Twitter and Facebook. This data can be used to better understand people, such as their personality traits, perceptions, and preferences, and predict their behavior. This deeper understanding of users and their behaviors can benefit a wide range of intelligent applications, such as advertising, social recommender systems, and personalized knowledge management. These applications will also benefit individual users themselves by optimizing their experiences across a wide variety of domains, such as retail, healthcare, and education. Since mining and understanding user behavior from social media often requires interdisciplinary effort, including machine learning, text mining, human-computer interaction, and social science, our workshop aims to bring together researchers and practitioners from multiple fields to discuss the creation of deeper models of individual users by mining the content that they publish and the social networking behavior that they exhibit.

#index 1920044
#* CloudDB 2012: fourth international workshop on cloud data management
#@ Xiaofeng Meng;Adam Silberstein;Fusheng Wang
#t 2012
#c 1
#! The fourth ACM international workshop on cloud data management is held in Maui, Hawaii, USA on October 29, 2012 and co-located with the ACM 21th Conference on Information and Knowledge Management (CIKM). The main objective of the workshop is to address the challenges of large scale data management based on the cloud computing infrastructure. The workshop brings together researchers and practitioners from cloud computing, distributed storage, query processing, parallel algorithms, data mining, and system analysis, all attendees share common research interests in maximizing performance, reducing cost of cloud data management and enlarging the scale of their endeavors. We have constructed an exciting program of seven refereed papers and four invited keynote talks that will give participants a full dose of emerging research.

#index 1920045
#* CDMW 2012 - city data management workshop: workshop summary
#@ Veli Bicer;Thanh Tran;Fatma Ozcan;Opher Etzion
#t 2012
#c 1
#% 1288163
#% 1577254
#% 1581945
#% 1597467
#% 1641529
#% 1642112
#% 1868738
#! Cities today have become highly dense, dynamic living areas for the majority of planet's population and also focal points of innovation, commerce, and growth in a highly modernized world. Due to its intensifying importance, cities need to transform into sustainable, smarter and credible places to enable a tenantable and comfortable life for their citizens. City data, which is the source of our digitized knowledge about the cities, is a highly important element to achive this goal as it is the main input to build complex city ecosystems and to solve particular problems that are encountered in the cities today. In this respect, this workshop will provide a major forum to identify the challenges and opportunities in terms of better managing city data and to reveal its discriminating importance in various applications in a city ecosystem. As city data becomes more widespread and prevailing, it poses novel research problems which importantly are open to the investigation of a broad community of researchers in various fields.

#index 1920046
#* Managing interoperability and compleXity in health systems - MIXHS'12
#@ Cui Tao;Matt-Mouley Bouamrane
#t 2012
#c 1
#% 1642332
#! Data management and knowledge engineering have long been important research fields in computer science, and rapid progress in recent years have increasingly seen these technologies successfully applied to solve complex biomedical challenges and support health services professionals in the course of their intellectually-demanding clinical duties, such as through the use of decision-support or expert systems. Yet, as the biomedical knowledge available in the modern digital world grows exponentially, there is a pressing need for a focused forum to promote technology and knowledge transfer from basic research to biomedical applications as well as allowing for implementers of healthcare systems to share their experiences with the research community. The Managing Interoperability and Complexity in Health Systems, MIXHS workshops are designed for such a purpose with the view that multi-disciplinary approaches within a holistic forum is essential to rise to the ever new challenges of biomedical knowledge complexity and interoperability of health systems and services.

#index 1920047
#* The 2012 international workshop on web-scale knowledge representation, retrieval, and reasoning
#@ Spyros Kotoulas;Yi Zeng;Zhisheng Huang
#t 2012
#c 1
#% 1517757
#% 1517758
#% 1517759
#% 1632380
#% 1632381
#% 1632382
#% 1912611
#% 1912612
#% 1912613
#! The rapid and perpetual growth of knowledge on the Web has given rise to many grand challenges (such as scalability, inconsistency, uncertainty, distribution and dynamics) for traditional knowledge processing methods and systems. Knowledge representation, retrieval and reasoning methods need to evolve and adapt to the Web to face these challenges and make this vast, heterogenous knowledge useful and accessible. In this light, the International Workshop on Web-scale Knowledge Representation, Retrieval, and Reasoning (Web-KR) is initiated. This workshop serves as the third one in this workshop series. This summary discusses the scope of Web-KR and introduces the advances in this field through the accepted papers in the Web-KR 2012 workshop, co-located with CIKM 2012.

#index 1920048
#* SHB 2012: international workshop on smart health and wellbeing
#@ Christopher C. Yang;Hsinchun Chen;Howard Wactlar;Carlo K. Combi;Xuning Tang
#t 2012
#c 1
#% 1912650
#% 1912651
#% 1912652
#% 1912653
#% 1912654
#% 1912655
#% 1912656
#% 1912657
#! The Smart Health and Wellbeing workshop is organized to develop a platform for authors to discuss fundamental principles, algorithms or applications of intelligent data acquisition, processing and analysis of healthcare data. We are particularly interested in information and knowledge management papers, in which the approaches are accompanied by an in-depth experimental evaluation with real world data. This paper provides an overview of the workshop and the accepted contributions.

#index 1920049
#* Booksonline'12: 5th workshop on online books, complementary social media and their impact
#@ Gabriella Kazai;Monica Landoni;Carsten Eickhoff;Peter Brusilovsky
#t 2012
#c 1
#% 1912899
#% 1912901
#% 1912902
#% 1912903
#% 1912904
#% 1912905
#% 1912906
#% 1912907
#! BooksOnline'12, the fifth workshop in the series, aims to offer a forum for bringing together expertise from academia, industry and libraries to facilitate the exchange of research results and technology in the field of digital libraries with specific focus on online books and complementary social media. The focus of this year's workshop is "engaging reading experiences", starting from the act of deciding what to read, through the exploration and interpretation of a book's content, to sharing the overall experience. Within this overall umbrella theme, the accepted papers naturally showed three salient themes: (1) Search and Discovery, (2) Personalization and Recommendation, and Reading Experiences beyond Text. The contributions demonstrate a range of technologies, including a collaborative tabletop visual approach to support the searching and discovery of books, co-citation methods to enhance document retrieval; exploring open issues in audio-book production to support non-text based reading and improving e-book accessibility; new approaches to recommendation that take into account writing style as well as looking specifically to young readers and their needs in order to develop recommendation tools that consider both content and reading level and match these against the readers' specific interests and reading ability. Following in the theme of the reader playing a central role in the future of our digital era, we are honored to welcome Maribeth Back from FX Palo Alto and Natasa Milic-Frayling from Microsoft Research as our keynote speakers.

#index 1920050
#* DTMBIO 2012: international workshop on data and text mining in biomedical informatics
#@ Min Song;Doheon Lee;Hua Xu;Sophia Ananiadou
#t 2012
#c 1
#% 1912870
#% 1912871
#% 1912872
#% 1912873
#% 1912874
#% 1912875
#% 1912876
#% 1912877
#% 1912878
#% 1912879
#% 1912880
#% 1912881
#! The organizers of ACM Sixth International Workshop on Data and Text Mining in Biomedical Informatics (DTMBIO 12) are happy announce that the sixth DTMBIO will be held in conjunction with CIKM, one of the largest data management conferences. The major interests of DTMBIO are on the state-of-the-art applications of data and text mining on biomedical research problems. DTMBIO 12 will be a forum of discussing and exchanging informatics related techniques and problems in the context of biomedical research.

#index 1920051
#* PLEAD 2012: politics, elections and data
#@ Ingmar Weber;Ana-Maria Popescu;Marco Pennacchiotti
#t 2012
#c 1
#% 1187015
#% 1912615
#% 1912617
#% 1912618
#% 1912619
#% 1912620
#% 1912621
#! What is the role of the internet in politics general and during campaigns in particular? And what is the role of large amounts of user data in all of this? In the 2008 U.S. presidential campaign the Democrats were far more successful than the Republicans in utilizing online media for mobilization, co-ordination and fundraising. For the first time, social media and the Internet played a fundamental role in political campaigns. However, technical research in this area has been surprisingly limited and fragmented. The goal of this workshop is to bring together, for the first time, researchers working at the intersection of social network analysis, computational social science and political science, to share and discuss their ideas in a common forum; and to inspire further developments in this growing, fascinating field. The workshop has Filippo Menczer as keynote speaker, it includes technical presentations of accepted papers and concludes with a panel discussion where scientists and media experts from different fields can interact and share views.

#index 1920052
#* Workshop on multimodal crowd sensing (CrowdSens 2012)
#@ Haggai Roitman;Iván Cantador;Miriam Fernández
#t 2012
#c 1
#% 1912845
#% 1912846
#% 1912848
#% 1912849
#! This paper provides an overview of the 1st International Workshop on Multimodal Crowd Sensing (CrowdSens 2012), held at the 21st ACM International Conference on Information and Knowledge Management (CIKM 2012). This workshop aimed to provide an open forum for researchers from various fields such as fields such as Natural Language Processing, Information Extraction, Data Mining, Information Retrieval, User Modeling and Personalization, Stream Processing, and Sensor Networks, for addressing the challenges of effectively mining, analyzing, fusing, and exploiting information sourced from multimodal physical and social sensor data sources.

#index 1920053
#* Fifth workshop on exploiting semantic annotations in information retrieval: ESAIR''12)
#@ Jaap Kamps;Jussi Karlgren;Peter Mika;Vanessa Murdock
#t 2012
#c 1
#% 1912921
#% 1912922
#% 1912923
#% 1912924
#% 1912925
#% 1912926
#% 1912927
#% 1912928
#% 1912929
#% 1912930
#% 1912931
#! There is an increasing amount of structure on the Web as a result of modern Web languages, user tagging and annotation, emerging robust NLP tools, and an ever growing volume of linked data. These meaningful, semantic, annotations hold the promise to significantly enhance information access, by enhancing the depth of analysis of today's systems. Currently, we have only started exploring the possibilities and only begin to understand how these valuable semantic cues can be put to fruitful use. To complicate matters, standard text search excels at shallow information needs expressed by short keyword queries, and here semantic annotation contributes very little, if anything. The main questions for the workshop are how to leverage the rich context currently available, especially in a mobile search scenario, giving powerful new handles to exploit semantic annotations. And how can we fruitfully combine information retrieval and semantic web approaches, and for the first time work actively toward a unified view on exploiting semantic annotations.

#index 1920054
#* First international workshop on information and knowledge management for developing region
#@ Rakesh Agrawal;Douglas W. Oard;Nitendra Rajput
#t 2012
#c 1
#! Several issues arise with management of content that is generated in developing regions. Some result from linguistic diversity (as in India and Africa), some result from content being available only in forms that are more difficult to computationally manipulate (e.g., handwriting, speech, and legacy digital text in nonstandard encodings), some result from underinvestment in language resources for the languages of these regions, and some result from increased contact between cultures that have different views regarding the proper use of information and information artifacts. Such issues warrant focused attention if we are to optimally leverage information and knowledge management to the advantage of populations in developing regions. That is the purpose of this workshop.

#index 1920055
#* PIKM 2012: 5th ACM workshop for PhD students in information and knowledge management
#@ Aparna Varde;Fabian M. Suchanek
#t 2012
#c 1
#% 1016295
#% 1077041
#% 1301009
#% 1482585
#% 1542529
#% 1642331
#% 1912634
#% 1912635
#% 1912637
#! The PIKM 2012 workshop is the 5th of its kind after 4 successful PhD workshops at ACM CIKM. This PhD workshop invites papers that describe the Ph.D. dissertation proposals of doctoral students in any of the CIKM areas: databases, information retrieval, data mining and knowledge management. Interdisciplinary work across these tracks is particularly encouraged. This year PIKM has received around 25 submissions from over 12 countries across the globe, among which 10 have been accepted as full papers for oral presentation while 4 have been accepted as short ones for poster presentation. The selection has been conducted based on reviews submitted by an expert team comprising 21 PC members spanning 12 countries and 6 continents with a good balance of industry and academia.

#index 1920056
#* WIDM 2012: the 12th international workshop on web information and data management
#@ George H. L. Fletcher;Prasenjit Mitra
#t 2012
#c 1
#! We give an overview of WIDM 2012, held in conjunction with CIKM 2012 in Maui, Hawaii. WIDM 2012 is the twelfth in a series of international workshops on Web Information and Data Management held in conjunction with CIKM since 1998. The objective of the workshop is to bring together researchers and industrial practitioners to present and discuss leading research into how web data and information can be extracted, stored, analyzed, and processed to provide useful knowledge to end users for advanced database and web applications.

#index 1920057
#* DOLAP 2012 workshop summary
#@ Matteo Golfarelli;Il-Yeol Song
#t 2012
#c 1
#% 1912851
#% 1912852
#% 1912853
#% 1912854
#% 1912855
#% 1912856
#% 1912857
#% 1912858
#% 1912859
#% 1912860
#% 1912861
#% 1912862
#% 1912863
#% 1912864
#% 1912865
#% 1912866
#% 1912867
#% 1912868
#! The ACM DOLAP workshop presents research on data warehousing and On-Line Analytical Processing (OLAP). The DOLAP 2012 program is organized in four interesting sessions on data warehouse design and maintainability, OLAP querying and trends, warehousing of complex data, performance optimization and benchmarking.

#index 2006018
#* Proceedings of the 22nd ACM international conference on Conference on information & knowledge management
#@ Qi He;Arun Iyengar;Wolfgang Nejdl;Jian Pei;Rajeev Rastogi
#t 2013
#c 1
#! On behalf of the organizing committee, it is our great pleasure to welcome you to the 22nd ACM International Conference on Information and Knowledge Management (CIKM 2013) in San Francisco! CIKM is a premier ACM conference in the areas of information retrieval, knowledge management and databases. Since 1992, it has successfully brought together leading researchers and developers from the three communities. The purpose of the conference is to identify challenging problems facing the development of future knowledge and information systems, and to shape future research directions through the publication of high quality applied and theoretical research findings. In CIKM 2013, we continue the tradition of promoting collaboration among multiple areas and providing a leading forum in which experts from academia, industry, and government gather to exchange ideas, research results, and technical developments in multidisciplinary research areas. As one of the world's most recognized conferences in the field, this year CIKM received 848 valid full paper submissions, 233 poster submissions, and 57 demonstration submissions. Among them, we accepted 143 full papers (16.86% acceptance rate), 107 short papers, 81 posters and 21 demos. In addition to regular research tracks, CIKM 2013 features 4 keynote speakers, a panel on Big Data, dedicated Industry events featuring 10 leading industrial practitioners, 10 tutorials from nprestigious researchers and 14 workshops on cutting-edge areas of research. This is a great demonstration of the lively research areas that contribute to the CIKM area. We are proud of our final program and gratefully thank all authors, invited speakers and organizers who chose to contribute their time and research to CIKM 2013. We are honored to present four distinguished keynote speakers to attendees: Ronald Fagin, Lee Giles, Carlos Guestrin, and Alon Halevy. Their valuable, insightful and interdisciplinary talks will guide us to a better understanding of the field.

#index 2008403
#* Proceedings of the 2nd workshop on Politics, elections and data
#@ Ingmar Weber;Ana-Maria Popescu;Marco Pennacchiotti
#t 2013
#c 1
#! It is our pleasure to welcome you to the second edition of the Politics, Elections and Data workshop (PLEAD at CIKM 2013). The goal of this workshop is to bring together researchers working at the intersection of social network analysis, computational social science and political science, to share and discuss their ideas in a common forum; and to inspire further developments in this growing, fascinating field of computational political science. The call for papers attracted submissions from researchers in different areas and from different countries, which underscores the wide appeal of computational political science. The program committee accepted 3 papers addressing the relationship between tweets and votes in 2009 Federal Election in Germany, political polarization online in the context of the French and US presidential elections and finally, early efforts for multi-cycle forecasting of congressional elections with social media. In addition, the program includes three keynotes from speakers with experience in modern campaigning, academic political science and media issues: Rayid Ghani (former Chief Scientist at Obama for America 2012), Justin Grimmer (Assistant Professor, Political Science, Stanford University) and Tarun Wadhwa (writer, researcher, entrepreneur, Forbes contributor).

#index 2008451
#* Proceedings of the 2013 workshop on Computational scientometrics: theory & applications
#@ Cornelia Caragea;C. Lee Giles;Lior Rokach;Xiaozhong Liu
#t 2013
#c 1
#! It is our great pleasure to welcome you to the 2013 ACM International Workshop on Computational Scientometrics: Theory and Applications -- CSTA'13. This is the first time when the CSTA workshop has been organized in the ACM International Conference on Information and Knowledge Management. The primary goals and objectives of the workshop are to promote both theoretical results and practical applications within digital libraries to better answer questions such as how do research ideas emerge, evolve, or disappear as a topic, what is a good measure of quality of published works, what are the most promising areas of research, how authors connect and influence each other, who are the experts in a field, what works are similar, and who funds a particular research topic. The workshop aims at bringing together researchers with diverse interdisciplinary backgrounds interested in mining the web, large digital libraries and other relevant databases for research related publications and data. The combination of classical bibliometrics and novel text mining provides a synergy unavailable within each approach taken independently. In this proposed workshop, we also focus on bibliometrics analysis by using sophisticated text mining, or natural language processing methods, which will enable researchers to generate innovative research topics, e.g. full-text citation analysis. We expect people interested in the practical applications within digital libraries such as citation analysis and recommendation, scientific and research trends, expert finding, and collaborator recommendation, to attend the CSTA'13 workshop. The call for papers attracted submissions from Europe, Australia, and the United States. The program committee accepted 6 papers that cover topics, including program committees recommendation for academic conferences, collaborators recommendation, and citations classification and labeling. In addition, the program includes a keynote speech by Xiaozhong Liu on "Full-text Citation Analysis and its Application".

#index 2009182
#* Proceedings of the 2013 workshop on Automated knowledge base construction
#@ Fabian M. Suchanek;Sebastian Riedel;Sameer Singh;Partha Pratim Talukdar
#t 2013
#c 1
#! Extracting knowledge from Web pages, and integrating it into a coherent knowledge base (KB), is a task that spans the areas of natural language processing, information extraction, information integration, databases, search and machine learning. Recent years have seen significant advances on the creation of large-scale KBs. Examples include Wikipedia-based KBs (e.g., YAGO, DBpedia, and Freebase), KBs generated from Web documents (e.g., NELL, PROSPERA), and open information extraction approaches (e.g., TextRunner, PRISMATIC, Rexa). Most prominently, all major search engine providers (Yahoo!, Microsoft Bing, and Google) nowadays experiment with semantic KBs (e.g., the Google Knowledge Graph). The workshop on automated knowledge base construction (AKBC) is the venue for sharing the latest research in the area of knowledge extraction. Unlike many other workshops, its focus is on keynotes by high profile researchers in the field. This year, we are proud to welcome talks by Bonnie Dorr, DARPA, USA Evgeniy Gabrilovich, Google Research, USA Alon Halevy, Google Research, USA Chris Manning, Stanford University, USA James Mayfield, Johns Hopkins University, USA Andrew McCallum, University of Massachusetts Amherst, USA Tom Mitchell, Carnegie Mellon University, USA Dan Weld, University of Washington, USA Haixun Wang, Microsoft Research Asia Our invited speakers will share their visions on knowledge extraction with the audience. In addition, the workshop invites regular paper submissions. We focus exclusively on short, visionary papers, even if the experimentation is still rudimentary. This way, we aim to attract the latest cutting-edge research that has not yet been presented at conferences. The main means of presentation will be through posters. By focusing on interactive presentations rather than talks, we hope to stimulate discussion, improve understanding of the work, and sow the ideas for future research. This year, the AKBC workshop accepted 19 papers. All of them will be presented as posters. In addition, 9 of these papers were selected for short oral presentations. We will have presentations that encompass many interesting topics, including confidence estimation in KBs, using computers to pass an elementary science test, and mining history from newspaper archives. A best paper award will be also given to the submission with the best reviews. We hope that, like in 2010 and 2012, our workshop will again prove to be an inspiring venue for researchers in the area of knowledge management. We are looking forward to welcoming you at the AKBC 2013!

#index 2010982
#* Proceedings of the 7th international workshop on Data and text mining in biomedical informatics
#@ Atul Butte;Doheon Kim;Min Songm;Hua Xu
#t 2013
#c 1
#! It is our great pleasure to welcome you to the ACM Seventh International Workshop on Data and Text Mining in Biomedical Informatics (DTMBIO'13), in conjunction with the 22nd ACM International Conference on Information and Knowledge Management (CIKM'13). Biomedical researchers face the current challenge of making effective use of the enormous amount of electronic biomedical data in order to better understand and explain complex biological systems. The biomedical data repositories include data in a wide variety of forms, including genomic sequences, gene expression profiles, proteomics, metabolomics, epigenomics, microbiomics, electronics medical records, literature information, and so on. The ability to automatically and effectively extract, integrate, understand and make use of information embedded in such heterogeneous - structured and unstructured - data remains a challenging task. The aim of the 2013 workshop has been to bring together researchers in the areas of data and text mining and computational biology, who are interested in integrating and analyzing heterogeneous, structured and unstructured data. The papers accepted for presentation and publication in this volume cover a variety of topics, including biomedical text mining, molecular bioinformatics, neuroinformatics, and systems biology. We hope that these proceedings will serve as a valuable and up-to-date reference concerning the application of data- and text-mining techniques within biomedical informatics.

#index 2011239
#* Proceedings of the 4th international workshop on Web-scale knowledge representation retrieval and reasoning
#@ Yi Zeng;Spyros Kotoulas;Zhisheng Huang
#t 2013
#c 1
#! It is our great pleasure to welcome you to the 2013 International Workshop on Web-scale Knowledge Representation, Retrieval, and Reasoning (Web-KR 2013), co-located with the 22nd ACM International Conference on Information and Knowledge Management (CIKM 2013) at San Francisco, USA. This workshop is the fourth version in the workshop series under the title of "Web-scale Knowledge Representation, Retrieval, and Reasoning (Web-KR)". Web-KR 2013 continues its mission to take grand challenges for knowledge processing in the Web age (such as dynamics, uncertainty, inconsistency and scalability). It brings together researchers from the Web, Artificial Intelligence, High Performance Computing, Knowledge Management, Databases, and Machine Learning to discuss all issues of Web-KR in a synergistic setting. We hope to motivate different thoughts on Web-KR related issues and solutions from researchers in these different fields. The Web-KR 2013 program committee accepted 4 papers that cover different interesting and important topics, including faceted analysis of Web knowledge, Web knowledge base acceleration, stream reasoning, deep Web knowledge crawling, etc.

#index 2011244
#* Proceedings of the 2013 international workshop on Data management & analytics for healthcare
#@ Ullas Nambiar;Niranjan Thirumale
#t 2013
#c 1
#! Economic growth is an important factor in reducing poverty and generating the resources necessary for human development and environmental protection. However, along with economic growth we also need well-functioning civil institutions, secure individual and property rights, and most importantly broad-based health services for raising overall living standards. Therefore in 2000, under the guidance of United Nations, governments across the world set out the Millennium Development Goals (MDGs), a series of commitments to lift around 500 million people out of poverty by 2015. Despite these efforts, in 2013, OxFam International reports that in developing countries every day 1,400 women die needlessly in pregnancy and childbirth. The problem is not of intent but an "inability to scale" the solutions of today to provide cost effective access to medical care. The DARE 2013 workshop was organized to bring together computer scientists working to address issues in healthcare together to understand the state-of-the-art and to highlight challenges and discuss potential ways for solving them. The call for papers attracted several quality submissions from leading Computer Science researchers working to bring innovative solutions for providing data management and manalytics challenges in healthcare. Most papers focused on the challenges in integrating data from the diverse data collected in large hospitals. After review we chose five papers for presentation at the workshop. Solutions presented include building taxonomy for indexing medical records, integrating multiple datasets to get a unified view and using background knowledge to understand clinical notes.We would like to thank our Program Committee for selecting this high quality program for DARE 2013. We also invited three leading researchers working at the confluence of CS and Healthcare to present invited talks. Special thanks to Charles Boicey (Informatics Solutions Architect, University of California, Irvine), Prof. Amit Sheth (Kno.e.sis @ Wright State University) and Dr Hulya Farinas (Sr. Principal Data Scientist, Healthcare & Lifesciences, Pivotal Inc) for giving invited talks derived from their experiences in implementing healthcare data analytics solutions. Details about DARE 2013 including list of accepted papers are available at https://sites.google.com/site/ubnambiar/dare2013/

#index 2011600
#* Proceedings of the 1st workshop on User engagement optimization
#@ Liangjie Hong;Shuang-Hong Yang
#t 2013
#c 1
#! It is our great pleasure to welcome you to The First Workshop on Online User Engagement Optimization -- UEO'13. The aim of UEO is to being the premier forum for presentation of research results and experience reports on leading edge issues of user engagement optimization, including machine learning, information retrieval, recommender systems, online experimental platforms and social network analysis. The mission of the workshop is to share latest research results and bring industrial practitioners and academic researchers together. We have accepted six high quality papers from both industrial and academic institutions, covering topics of query analysis, recommender systems, online experiments and user engagement analysis. In addition, the program includes several invited speakers which would boost the discussion further. We also encourage attendees to attend the keynote and invited talk presentations. These valuable and insightful talks can and will guide us to a better understanding of the future: Recommending Items to Users: An Explore/Exploit Perspective, Deepak Agarwal (LinkedIn) Online Controlled Experiments: Introduction, Insights, Scaling, and Humbling Statistics, Ron Kohavi (Microsoft)

#index 2011793
#* Proceedings of the 2013 workshop on Living labs for information retrieval evaluation
#@ Krisztian Balog;David Elsweiler;Evangelos Kanoulas;Liadh Kelly;Mark Smucker
#t 2013
#c 1
#! It is our great pleasure to welcome you to the Workshop on Living Labs for Information Retrieval Evaluation -- LL'13, held at CIKM 2013 in San Francisco, on November 1, 2013. In the past few years the information retrieval (IR) community has been exploring ways to move further away from the Cranfield style evaluation paradigm, and make evaluations more "realistic" (more centered on real users, their needs and behaviours). As part of this drive, living labs, which involve and integrate users in the research process, have been proposed. Living labs would offer huge benefits to the community, such as: availability of, potentially larger, cohorts of real users and their behaviours; cross-comparability across research centres; and greater knowledge transfer between industry and academia, when industry partners are involved. The need for this methodology is further amplified by the increased reliance of IR approaches on proprietary data; living labs are a way to bridge the data divide between academia and industry. Progress towards realising actual living labs has nevertheless been limited. There are many challenges to be overcome before the benefits associated with living labs for IR can be realised, including challenges associated with living labs architecture and design, hosting, maintenance, security, privacy, participant recruiting, and scenarios and tasks for use development. This workshop brings together, for the first time, people interested in progressing the living labs for IR evaluation methodology. Our aim is to work together to identify natural use cases, barriers to success, and share opinions on ways and means of addressing them. The call for papers attracted 7 submissions, all of which were found acceptable by the program committee. These include 2 short papers, 2 position papers, and 3 demonstrators. In addition, the workshop programme features an invited talk by Jan Pedersen (Microsoft Bing). The workshop is intended to be highly interactive to encourage group discussion and active collaboration among attendees; multiple breakout sessions are scheduled throughout the day. A final discussion session wraps up the event with the objective to identify and formulate specific action items for future research and development.

#index 2011802
#* Proceedings of the sixth workshop on Ph.D. students in information and knowledge management
#@ Fabian M. Suchanek;Anisoara Nica
#t 2013
#c 1
#! For the 6th time, the ACM International Conference Information and Knowledge Management (CIKM) hosts a workshop for PhD students: PIKM 2013: The 6th ACM Workshop for Ph.D. Students in Information and Knowledge Management. The goal of this workshop is two-fold: First, a PhD workshop gives doctoral students an opportunity to present their work in an early stage to a global audience. This allows the students not only to crystallize their ideas into a scientific article, and to practice scientific presentation, but also to receive feedback from reviewers, from fellow students and from the general CIKM audience. Second, we believe that the research community, too, benefits from such a workshop: PhD dissertations are the grassroots of research. They point out new research avenues and indicate current promising topics. They provide fresh viewpoints from the researchers of tomorrow. Also, we hope that the interaction with other researchers at the workshop itself, across all levels of seniority, will help propel science forward. The PIKM workshop covers topics in all core areas of the general CIKM conference: information retrieval (IR), databases (DB), and knowledge management (DB). This diversity of topics got reflected in the submissions we received. The call for papers attracted 13 submissions from all populated continents of the world. Out of these, 6 papers got accepted. The papers cover proposals at various stages of the dissertation, from early outlines of research plans, to in depth investigations of acute questions and mid-term reports of work in progress. The dissertations touch all three main areas of the PIKM, including work on graph clustering, information extraction, and spam detection. This year best submission by Avirup Sil Exploring Re-ranking Approaches for Joint Named-Entity Recognition and Linking receives a special best paper award. As a special highlight, this year's PIKM features a keynote talk by Dr. Pierre Senellart. Dr. Senellart is an associate professor at Télécom ParisTech in Paris. He has published over 30 papers in the area of databases and knowledge management, and will share his advice and experience with the students.

#index 2011810
#* Proceedings of the sixteenth international workshop on Data warehousing and OLAP
#@ Il-Yeol Song;Ladjel Bellatreche;Alfredo Cuzzocrea
#t 2013
#c 1
#! It is our great pleasure to welcome you to the 16th ACM International Workshop on Data Warehousing and OLAP (DOLAP 2013). The DOLAP workshop continues its tradition of being the premier forum where both researchers and practitioners in data warehousing and On-Line Analytical Processing (OLAP) share their findings in theoretical foundations, current methodologies, new trends and practical experiences. The mission of the DOLAP workshop is to identify and explore new directions for future research and development, as well as emerging application domains in the areas of data warehousing and OLAP. In recent years, research in these areas have addressed many topics, ranging from conceptual-level and methodological issues, which help designers to build effective decision-support applications, to deploy them in new hardware devices and query processing issues, aiming at increasing the performance of these applications in order to deal with vast amounts of data. However, the successful usages of data warehousing and OLAP technologies within organizations bring up new requirements and research opportunities, in particular to cope with non-traditional application domains, such as social networks, big data, and stream data. The call for papers attracted 26 submissions, from 16 different countries and 4 continents. After careful review and discussion, the program committee accepted 8 full papers and 5 short papers, for a competitive acceptance rate of 30% for full papers and about 50% overall. The accepted papers cover a wide variety of topics, including multidimensional design, query processing, new hardware, recommendation, big data, text cube, data clustering, among others. Papers are grouped into three sessions covering design and exploitation of social data warehouses, ETL & schema evolution and new trends. This year, the program also includes an invited keynote talk given by Prof. Vassilis J. Tsotras, from the Department of Computer Science and Engineering of the University of California - Riverside, who focuses on advanced aggregation techniques for Big Data, and a panel which moves the attention on current challenges and future research directions of the emerging topic "Data Warehousing and OLAP over Big Data".

#index 2011824
#* Proceedings of the sixth international workshop on Exploiting semantic annotations in information retrieval
#@ Paul N. Bennett;Evgeniy Gabrilovich;Jaap Kamps;Jussi Karlgren
#t 2013
#c 1
#! These proceedings contain the contributed papers of the Sixth Workshop on Exploiting Semantic Annotations in Information Retrieval (ESAIR 2013), held at CIKM 2013 in San Francisco, on October 28, 2013. After successful workshops at ECIR'08 in Glasgow, WSDM'09 in Barcelona, CIKM'10 in Toronto, CIKM'11 in Glasgow, and CIKM'12 at Maui, this year's workshop will focus on the need to include large-scale knowledge resources and on annotations beyond the topical dimension. There is a need to include the currently emerging knowledge resources (such as DBpedia, Freebase) as underlying semantic model giving access to an unprecedented scope and detail of factual information. There is also a need to include annotations beyond the topical dimension (think of sentiment, reading level, prerequisite level, etc) that contain vital cues for matching the specific needs and profile of the searcher at hand. ESAIR'13 will be a real workshop where researchers from these different disciplines will work together to identify natural use cases, barriers to success, and work on ways of addressing them: Application/Use Case: What are use cases that make obvious the need for semantic annotation of information? What tasks cannot be solved by document retrieval using the traditional bagof- words? What is keeping searchers from exploring these powerful search requests? What impact has the web of data with more and more information in preprocessed form? Annotations: What types of annotation are available? Are there crucial differences between author-, software-, user-, and machine-generated annotations? Do we annotate types/classes/categories ("person") or instances ("Albert Einstein")? How similar or different are linked data and annotated text? What are the limitations of the current annotations schemes, and how to overcome them? Rich Context: Do we annotate text? Or also search requests and interactions, and their broader context? Besides personalization and geo-positional information, mobiles have a wide and growing range of locational, mechanical and even biometrical sensor data available to them. Can kick-start the query by inferring task and situational context? (Un)certainty: How should we interpret the annotations? Can we reliably link textual annotations to known entity catalogs? Can expect a messy world to be captured in a clean set of meaningful categories? Or is all information fundamentally uncertain and only partly known? How can we fruitfully combine information retrieval and semantic web approaches? These and other related questions will be discussed at this open format workshop -- the aim is to provide paths for further research to change the way we understand information access today! The workshop will consist of three main parts: Three keynotes to help us frame the problem, and create a common understanding of the challenges: Kevyn Collins-Thompson (University of Michigan); Marti A. Hearst (University of California, Berkeley); and Dan Roth (University of Illinois at Urbana-Champaign). A boaster and poster session with 14 papers selected by the program committee from 21 submissions (a 67% acceptance rate). Each paper was reviewed by at least two members of the program committee. Breakout groups on different aspects of exploiting semantic annotations, with reports being discussed in the final session.

#index 2012144
#* Proceedings of the 2013 international workshop on Mining unstructured big data using natural language processing
#@ Xiaozhong Liu;Miao Chen;Ying Ding;Min Song
#t 2013
#c 1
#! It is our great pleasure to welcome you to the 2013 ACM International Workshop on Mining Unstructured Big Data using Natural Language Processing, which will be held at ACM International Conference on Information and Knowledge Management, CIKM 2013. Unstructured text data is heterogeneous and available in different formats, such as text document, scientific publication, web page, and customer comment. The availability of many big unstructured text datasets enables, while also challenges researchers to discover and explore valuable information/knowledge via different techniques. Mining semantics by using Natural Language Processing (NLP) methodologies is an important approach to uncover the "latent knowledge/semantic" of the unstructured text data. In the past decade, while a number of NLP based features already successfully used to enhance the performance of the text mining or information retrieval systems, we are also facing some challenges. For instance, most NLP algorithms' computational cost is high, and we can hardly employ them directly to large-scale text data for online systems. In this workshop, we aggregate different but highly related research communities, i.e., "NLP", "Text Mining" and "IR" researchers, to investigate the possible opportunities and challenges in semantic mining problem. Nine very interesting papers, covering semantic analysis, social media mining, real-time information extraction, and etc., will be presented in this workshop. For this workshop, an opportunity is offered to both NLP and text mining research communities to better clarify the opportunities and challenges in NLP based semantic mining for big unstructured text data with their research experience. We also encourage attendees to attend the keynote presentation - "HathiTrust Data, Opportunities and Challenges for Text Mining and NLP" by Dr. Beth A. Plale, Director of Data to Insight Center, and Professor at School of Informatics and Computing, Indiana University. HathiTrust is a partnership of academic & research institutions, offering a collection of millions of digitized from libraries around the world plus effective API access. We hope that you will find this program interesting and thought-provoking and that the workshop will provide you with a valuable opportunity to share ideas with other researchers and practitioners from institutions around the world.

#index 2012167
#* Proceedings of the 2013 workshop on Data-driven user behavioral modelling and mining from social media
#@ Jalal Mahmud;James Caverlee;Jeffrey Nichols;John O'Donovan;Michelle Zhou
#t 2013
#c 1
#! It is our great pleasure to welcome you to the workshop on Data-driven user behavioral modeling and mining from social media (DUBMOD), co-located with 22nd ACM Conference on Information and Knowledge Management -- CIKM'13. This workshop builds upon the success of our previously held workshops User modeling from social media (UMSOCIAL), in conjunction with ACM IUI 2012 and Data-driven user behavioral modeling and mining from social media (DUBMMSM), in conjunction with ACM CIKM 2012. The call for papers attracted 12 submissions. We accepted 8 papers. The papers represent various topics such as reaction time for user behavior model for social network, user profile for answer quality, predicting requests for Wikipedia articles, linking Pinterest pins to online webshops, scalable mining of social data, predicting online buying behavior, flickr group recommendation. We hope that these workshop proceedings will serve as a valuable reference for researchers and practitioners from diverse areas, such as user modeling, social media analysis, natural language processing, data mining and machine learning.

#index 2014775
#* Proceedings of the fifth international workshop on Cloud data management
#@ Xiaofeng Meng;Fusheng Wang;Feifei Li;Cong Yu
#t 2013
#c 1
#! It is our great pleasure to welcome you to the Fifth International Workshop on Cloud Data Management (CloudDB 2013). This year we continue our tradition of serving as a premier forum for researchers and practitioners to present research results and share ideas and progress in the area of data management within cloud computing infrastructure. This broad area includes work in distributed storage, parallel algorithms, data mining, serving and analytic workloads, privacy, security, green computing, social workloads, and many others. The call for papers attracted a wide range of submissions. The program committee accepted 4 papers on a variety of topics. In addition, the program includes a keynote speaker, Krishna Gade, from Twitter, speaking on Realtime Analytics at Twitter, a highly interesting topic for Cloud DB research. We hope these proceedings will serve as a valuable resource to learn about the latest and most exciting work in cloud computing. We hope that you will find this program interesting and thought provoking and that the symposium will provide you with a valuable opportunity to share ideas with other researchers and practitioners from institutions around the world.

#index 2026086
#* Towards Concept-Based Translation Models Using Search Logs for Query Expansion
#@ Jianfeng Gao;Jian-Yun Nie
#t 2012
#c 1
#% 218978
#% 280819
#% 280851
#% 309095
#% 340901
#% 340948
#% 342707
#% 342961
#% 348155
#% 397128
#% 641976
#% 722904
#% 740915
#% 816170
#% 818239
#% 818262
#% 838530
#% 879567
#% 879587
#% 987231
#% 1055706
#% 1074081
#% 1074110
#% 1227621
#% 1227636
#% 1251678
#% 1264756
#% 1355019
#% 1399978
#% 1418196
#% 1472297
#% 1482292
#% 1549092
#% 1598362
#% 1598401
#! Query logs have been successfully used to improve Web search. One of the directions exploits user clickthrough data to extract related terms to a query to perform query expansion (QE). How-ever, term relations have been created between isolated terms without considering their context, giving rise to the problem of term ambiguity. To solve this problem, we propose several ways to place terms in their contexts. On the one hand, contiguous terms can form a phrase; and on the other hand, terms at proximi-ty can provide less strict but useful contextual constraints mutual-ly. Relations extracted between such more constrained groups of terms are expected to be less noisy than those between single terms. In this paper, the constrained groups of terms are called concepts. We exploit user query logs to build statistical translation models between concepts, which are then used for QE. We perform experiments on the Web search task using a real world data set. Results show that the concept-based statistical translation model trained on clickthrough data outperforms signif-icantly other state-of-the-art QE systems.

