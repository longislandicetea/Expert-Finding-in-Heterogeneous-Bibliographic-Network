#index 454032
#* Proceedings of the 9th European Conference on Machine Learning
#@ Maarten van Someren;Gerhard Widmer
#t 1997
#c 20

#index 454033
#* Proceedings of the 10th European Conference on Machine Learning
#@ Claire Nedellec;Céline Rouveirol
#t 1998
#c 20

#index 454034
#* Proceedings of the 11th European Conference on Machine Learning
#@ Ramon López de Mántaras;Enric Plaza
#t 2000
#c 20

#index 454036
#* Proceedings of the 13th European Conference on Machine Learning
#@ Tapio Elomaa;Heikki Mannila;Hannu Toivonen
#t 2002
#c 20

#index 458185
#* Improving Knowledge Discovery Using Domain Knowledge in Unsupervised Learning
#@ Javier Béjar
#t 2000
#c 20
#% 36672
#% 161241
#% 418028
#% 451052
#! Using domain knowledge in unsupervised learning has shown to be a useful strategy when the set of examples of a given domain has not an evident structure or presents some level of noise. This background knowledge can be expressed as a set of classification rules and introduced as a semantic bias during the learning process. In this work we present some experiments on the use of partial domain knowledge in conceptual clustering. The domain knowledge (or domain theory) is used to select a set of examples that will be used to start the learning process, this knowledge has not to be complete neither consistent. This bias will increase the quality of the final groups and reduce the effect of the order of the examples. Some measures of stability of classification are used as evaluation method. The improvement of the acquired concepts can be used to improve and correct the domain knowledge. A set of heuristics to revise the original domain theory has been experimented, yielding to some interesting results.

#index 458186
#* Wrapper Generation via Grammar Induction
#@ Boris Chidlovskii;Jon Ragetli;Maarten de Rijke
#t 2000
#c 20
#% 101925
#% 238555
#% 244103
#% 261741
#% 275915
#% 462065
#! To facilitate effective search on the World Wide Web, meta search engines have been developed which do not search the Web themselves, but use available search engines to find the required information. By means of wrappers, meta search engines retrieve information from the pages returned by search engines. We present an approach to automatically create such wrappers by means of an incremental grammar induction algorithm. The algorithm uses an adaptation of the string edit distance. Our method performs well; it is quick, can be used for several types of result pages and requires a minimal amount of user interaction.

#index 458187
#* Handling Continuous-Valued Attributes in Decision Tree with Neural Network Modelling
#@ DaeEun Kim;Jaeho Lee
#t 2000
#c 20
#% 61477
#% 89881
#% 91872
#% 160857
#% 169767
#% 204434
#% 356892
#% 376683
#% 404742
#% 478116
#% 1272280
#! Induction tree is useful to obtain a proper set of rules for a large amount of examples. However, it has difficulty in obtaining the relation between continuous-valued data points. Many data sets show significant correlations between input variables, and a large amount of useful information is hidden in the data as nonlinearities. It has been shown that neural network is better than direct application of induction tree in modeling nonlinear characteristics of sample data. It is proposed in this paper that we derive a compact set of rules to support data with input variable relations. Those relations as a set of linear classifiers can be obtained from neural network modeling based on back-propagation. This will also solve overgeneralization amd overspecialization problems often seen in induction tree. We have tested this scheme over several data sets to compare with decision tree results.

#index 458188
#* Mining TCP/IP Traffic for Network Intrusion Detection by Using a Distributed Genetic Algorithm
#@ Filippo Neri
#t 2000
#c 20
#% 18528
#% 136350
#% 221424
#% 280429
#% 369236
#% 466543
#% 536877
#% 664547
#% 1001828
#% 1022829
#! The detection of intrusions over computer networks (i.e., network access by non-authorized users) can be cast to the task of detecting anomalous patterns of network traffic. In this case, models of normal traffic have to be determined and compared against the current network traffic. Data mining systems based on Genetic Algorithms can contribute powerful search techniques for the acquisition of patterns of the network traffic from the large amount of data made available by audit tools. We compare models of network traffic acquired by a system based on a distributed genetic algorithm with the ones acquired by a system based on greedy heuristics. Also we discuss representation change of the network data and its impact over the performances of the traffic models. Network data made available from the Information Exploration Shootout project and the 1998 DARPA Intrusion Detection Evaluation have been chosen as experimental testbed.

#index 458189
#* Using a Symbolic Machine Learning Tool to Refine Lexico-syntactic Patterns
#@ Emmanuel Morin;Emmanuelle Martienne
#t 2000
#c 20
#% 92776
#% 747888
#% 756964
#% 757318
#% 1290067
#% 1476276
#! Acquisition of patterns for information extraction systems is a common task in Natural Language Processing, mostly based on manual analysis of text corpora. We have developed a system called PROMÉTHÉE, which incrementally extracts lexico-syntactic patterns for a specific conceptual relation from a technical corpus. However, these patterns are often too general and need to be manually validated. In this paper, we demonstrate how PROMÉTHÉE has been interfaced with the machine learning system EAGLE in order to automatically refine the patterns it produces. The empirical results obtained with this technique show that the refined patterns allows to decrease the need for the human validation.

#index 458190
#* Problem Decomposition for Behavioural Cloning
#@ Dorian Suc;Ivan Bratko
#t 2000
#c 20
#% 22348
#% 90041
#% 92533
#% 126926
#% 229940
#% 540908
#% 1271847
#% 1272286
#! In behavioural cloning of the human operator's skill, a controller is usually induced directly as a classifier from system's states into actions. Experience shows that this often results in brittle controllers. In this paper we explore a decomposition of the cloning problem into two learning problems: the learning of operator's control trajectories and the learning of the system's dynamics separately. We analyse advantages of such indirect controllers. We give characterization of the learner's error that is plausible explanation of why this decomposition approach has empirically proved to be usually superior to direct cloning.

#index 458191
#* An Empirical Study of MetaCost Using Boosting Algorithms
#@ Kai Ming Ting
#t 2000
#c 20
#% 136350
#% 191910
#% 280437
#% 424997
#% 465746
#% 546014
#% 1499573
#! MetaCost is a recently proposed procedure that converts an error-based learning algorithm into a cost-sensitive algorithm. This paper investigates two important issues centered on the procedure which were ignored in the paper proposing MetaCost. First, no comparison was made between MetaCost's final model and the internal cost-sensitive classifier on which MetaCost depends. It is credible that the internal cost-sensitive classifier may outperform the final model without the additional computation required to derive the final model. Second, MetaCost assumes its internal cost-sensitive classifier is obtained by applying a minimum expected cost criterion. It is unclear whether violation of the assumption has an impact on MetaCost's performance. We study these issues using two boosting procedures, and compare with the performance of the original form of MetaCost which employs bagging.

#index 458192
#* Partially Supervised Text Classification: Combining Labeled and Unlabeled Documents Using an EM-like Scheme
#@ Carsten Lanquillon
#t 2000
#c 20
#% 46803
#% 67565
#% 169654
#% 252011
#% 260001
#% 280817
#% 311027
#% 465754
#% 466263
#! Supervised learning algorithms usually require large amounts of training data to learn reasonably accurate classifiers. Yet, in many text classification tasks, labeled training documents are expensive to obtain, while unlabeled documents are readily available in large quantities. This paper describes a general framework for extending any text learning algorithm to utilize unlabeled documents in addition to labeled document using an Expectation-Maximization-like scheme. Our instantiation of this partially supervised classification framework with a similarity-based single prototype classifier achieves encouraging results on two real-world text datasets. Classification accuracy is reduced by up to 38% when using unlabeled documents in addition to labeled documents.

#index 458193
#* K-SVCR. A Multi-class Support Vector Machine
#@ Cecilio Angulo;Andreu Català
#t 2000
#c 20
#% 190581
#% 197394
#% 269221
#% 269225
#% 420077
#% 458352
#! Support Vector Machines for pattern recognition are addressed to binary classification problems. The problem of multi-class classification is typically solved by the combination of 2-class decision functions using voting scheme methods or decison trees. We present a new multi-class classification SVM for the separable case, called K-SVCR. Learning machines operating in a kernel-induced feature space are constructed assigning output +1 or -1 if training patterns belongs to the classes to be separated, and assigning output 0 if patterns have a different label to the formers. This formulation of multi-class classification problem ever assigns a meaningful answer to every input and its architecture is more fault-tolerant than standard methods one.

#index 458194
#* Minimax TD-Learning with Neural Nets in a Markov Game
#@ Fredrik A. Dahl;Ole Martin Halck
#t 2000
#c 20
#% 36160
#% 124689
#% 233137
#% 304312
#% 449561
#% 534115
#% 1291498
#! A minimax version of temporal difference learning (minimax TD-learning) is given, similar to minimax Q-learning. The algorithm is used to train a neural net to play Campaign, a two-player zero-sum game with imperfect information of the Markov game class. Two different evaluation criteria for evaluating game-playing agents are used, and their relation to game theory is shown. Also practical aspects of linear programming and fictitious play used for solving matrix games are discussed.

#index 458195
#* Investigation and Reduction of Discretization Variance in Decision Tree Induction
#@ Pierre Geurts;Louis Wehenkel
#t 2000
#c 20
#% 92537
#% 136350
#% 157825
#% 357521
#% 520224
#% 1011200
#! This paper focuses on the variance introduced by the discretization techniques used to handle continuous attributes in decision tree induction. Different discretization procedures are first studied empirically, then means to reduce the discretization variance are proposed. The experiment shows that discretization variance is large and that it is possible to reduce it significantly without notable computational costs. The resulting variance reduction mainly improves interpretability and stability of decision trees, and marginally their accuracy.

#index 458196
#* On the Boosting Pruning Problem
#@ Christino Tamon;Jie Xiang
#t 2000
#c 20
#% 136350
#% 217812
#% 235377
#% 252009
#% 408396
#% 451186
#% 565528
#% 1499573
#! Boosting is a powerful method for improving the predictive accuracy of classifiers. The ADABOOST algorithm of Freund and Schapire has been successfully applied to many domains [2, 10, 12] and the combination of ADABOOST with the C4.5 decision tree algorithm has been called the best off-the-shelf learning algorithm in practice. Unfortunately, in some applications, the number of decision trees required by ADABOOST to achieve a reasonable accuracy is enormously large and hence is very space consuming. This problem was first studied by Margineantu and Dietterich [7], where they proposed an empirical method called Kappa pruning to prune the boosting ensemble of decision trees. The Kappa method did this without sacrificing too much accuracy. In this work-in-progress we propose a potential improvement to the Kappa pruning method and also study the boosting pruning problem from a theoretical perspective. We point out that the boosting pruning problem is intractable even to approximate. Finally, we suggest a margin-based theoretical heuristic for this problem.

#index 458197
#* Boosting Applied toe Word Sense Disambiguation
#@ Gerard Escudero;Lluís Màrquez;German Rigau
#t 2000
#c 20
#% 92537
#% 235377
#% 266293
#% 283174
#% 302391
#% 311034
#% 312727
#% 424997
#% 515213
#% 741080
#% 741084
#% 741085
#% 741113
#% 748594
#% 748703
#! In this paper Schapire and Singer's AdaBoost. MH boosting algorithm is applied to the Word Sense Disambiguation (WSD) problem. Initial experiments on a set of 15 selected polysemous words show that the boosting approach surpasses Naive Bayes and Exemplar-based approaches, which represent state-of-the-art accuracy on supervised WSD. In order to make boosting practical for a real learning domain of thousands of words, several ways of accelerating the algorithm by reducing the feature space are studied. The best variant, which we call LazyBoosting, is tested on the largest sense-tagged corpus available containing 192, 800 examples of the 191 most frequent and ambiguous English words. Again, boosting compares favourably to the other benchmark algorithms.

#index 458198
#* Beyond Occam's Razor: Process-Oriented Evaluation
#@ Pedro Domingos
#t 2000
#c 20
#% 420102
#% 465927
#% 1273832
#! Overfitting is often considered the central problem in machine learning and data mining. When good performance on training data is not enough to reliably predict good generalization, researchers and practitioners often invoke "Occam's razor" to select among hypotheses: prefer the simplest hypothesis consistent with the data. Occam's razor has a long history in science, but a mass of recent evidence suggests that in most cases it is outperformed by methods that deliberately produce more complex models. The poor performance of Occam's razor can be largely traced to its failure to account for the search process by which hypotheses are obtained: by effectively assuming that the hypothesis space is exhaustively searched, complexity-based methods tend to over-penalize large spaces. This talk describes how information about the search process can be taken into account when evaluating hypotheses. The expected generalization error of a hypothesis is computed as a function of the search steps leading to it. Two variations of this "process-oriented" approach have yielded significant improvements in the accuracy of a rule learner. Process-oriented evaluation leads to the seemingly paradoxical conclusion that the same hypothesis will have different expected generalization errors depending on how it was generated. I believe that this is as it should be, and that a corresponding shift in our way of thinking about inductive learning is required.

#index 458199
#* Clustered Partial Linear Regression
#@ Luís Torgo;Joaquim Pinto da Costa
#t 2000
#c 20
#% 73372
#% 209021
#% 229931
#% 232117
#% 246243
#! This paper presents a new method that deals with a supervised learning task usually known as multivariate regression. The main distinguishing feature of this new technique is the use of a clustering method to obtain sub-sets of the training data before the learning phase. After this "resampling" process a different regression model is fitted to each found cluster. We call the resulting method clustered partial linear regression. Predictions using this technique are preceded by a cluster membership query for each test case. The cluster membership probability of a test case is used as a weight in an averaging process that calculates the final prediction. This averaging process involves the predictions of the regression models associated to the clusters for which the test case may belong. We have tested this general multi-strategy approach using several regression techniques and we have observed significant accuracy gains in several data sets. We have also compared our method to bagging that also uses an averaging process to obtain predictions. This experiment showed that the two methods are significantly different. Finally, we present a comparison of our method with several state-of-the-art regression methods.

#index 458200
#* Exploiting Classifier Combination for Early Melanoma Diagnosis Support
#@ Enrico Blanzieri;Claudio Eccher;Stefano Forti;Andrea Sboner
#t 2000
#c 20
#% 260147
#% 424994
#% 466229
#! Melanoma is the most dangerous skin cancer and early diagnosis is the main factor for its successful treatment. Experienced dermatologists with specific training make the diagnosis by clinical inspection and they reach 80% level of both sensitivity and specificity. In this paper, we present a multiclassifiers system for supporting the early diagnosis of melanoma. The system acquires a digital image of the skin lesion and extracts a set of geometric and colorimetric features. The diagnosis is performed on the vector of features by integrating with a voting schema the diagnostic outputs of three different classifiers: discriminant analysis, k-nearest neighbor and decision tree. The system is build and validated on a set of 152 skin images acquired via D-ELM. The results are comparable or better of the diagnostic response of a group of expert dermatologists.

#index 458201
#* Comparing Complete and Partial Classification for Identifying Latently Dissatisfied Customers
#@ Tom Brijs;Gilbert Swinnen;Koen Vanhoof;Geert Wets
#t 2000
#c 20
#% 136350
#% 152934
#% 232136
#! This paper evaluates complete versus partial classification for the problem of identifying latently dissatisfied customers. Briefly, latently dissatisfied customers are defined as customers reporting overall satisfaction but who possess typical characteristics of dissatisfied customers. Unfortunately, identifying latenty dissatisfied customers, based on patterns of dissatisfaction, is difficult since in customer satisfaction surveys, typically only a small minority of customers reports to be overall dissatisfied and this is exactly the group we want to focus learning on. Therefore, it has been claimed that since traditional (complete) classification techniques have difficulties dealing with highly skewed class distributions, the adoption of partial classification techniques could be more appropriate. We evaluate three different complete and partial classification techniques and compare their performance on a ROC convex hull graph. Results on real world data show that, under the circumstances described abobe, partial classification is indeed a serious competitor for complete classification. Moreover, external validation on holdout data shows that partial classification is able to identify latently dissatisfied customers correctly.

#index 458202
#* Layered Learning
#@ Peter Stone;Manuela M. Veloso
#t 2000
#c 20
#% 73372
#% 124074
#% 132938
#% 136350
#% 151769
#% 271069
#% 280042
#% 455258
#% 455259
#% 455260
#% 466066
#% 476730
#% 506280
#% 708571
#% 1272286
#% 1273677
#! This paper presents layered learning, a hierarchical machine learning paradigm. Layered learning applies to tasks for which learning a direct mapping from inputs to outputs is intractable with existing learning algorithms. Given a hierarchical task decomposition into subtasks, layered learning seamlessly integrates separate learning at each subtask layer. The learning of each subtask directly facilitates the learning of the next higher subtask layer by determining at least one of three of its components: (i) the set of training examples; (ii) the input representation; and/or (iii) the output representation. We introduce layered learning in its domain-independent general form. We then present a full implementation in a complex domain, namely simulated robotic soccer.

#index 458203
#* Value Miner: A Data Mining Environment for the Calculation of the Customer Lifetime Value with Application to the Automotive Industry
#@ Katja Gelbrich;Reza Nakhaeizadeh
#t 2000
#c 20
#% 215664
#! The acquisition of new accounts is a major task of marketers. It is often carried out rather unsystematically, though. However, by now, one has come to terms that customer acquisition is a matter of quality. An instrument to evaluate prospective accounts is the Customer Lifetime Value (CLV). This paper introduces a Data Mining environment for its calculation and demonstrates its applicability to marketing in the automotive industry. The Car Miner refers to the evaluation of prospects rather than of current customers. This and other restrictions will be discussed along with guidelines for future research.

#index 458204
#* Some Improvements on Event-Sequence Temporal Region Methods
#@ Wei Zhang
#t 2000
#c 20
#% 232136
#% 459006
#% 463903
#% 466251
#% 549255
#! Finding hidden temporal structures from event sequences is a difficult task, particularly when events occur irregularly over time and temporal dependencies may exist in a long time horizon. The tasks involved are not only to find event patterns represented in the form of temporal orders, but more importantly to find patterns that are described with precise time conditions and rules that can be applied to predict when a future event will occur. Recent study has shown that a new approach based on learning temporal regions is a good solution for this problem. This paper investigates this approach in a greater depth and makes several improvements. It introduces multiple rule selection methods to better uncover hidden relations. It also introduces heuristic rule pruning methods to speed up search to solve large-scale problems. Experimental results are presented which show the effectiveness of the new methods.

#index 458205
#* An Efficient and Effective Procedure for Updating a Competence Model for Case-Based Reasoners
#@ Barry Smyth;Elizabeth McKenna
#t 2000
#c 20
#% 68243
#% 168280
#% 257885
#% 359837
#% 458282
#% 490445
#% 490605
#% 494448
#% 496417
#% 514712
#% 566460
#% 1275276
#! Case-based reasoning systems solve new problems by reusing previous problem solving experience stored as cases in a case-base. In recent years the maintenance problem has become an increasingly important research issue for the case-based reasoning community. In short, the goal is to develop strategies for effectively maintaining the efficiency and competence of case-based reasoning systems as they evolve. Our research has focused on the development of a model of competence for case-based reasoning systems, a model that measures the contributions of individual cases to overall system competence, and which forms the computational basis for a variety of maintenance strategies. However, while this model offers many potential advantages its upkeep adds an additional cost to the CBR cycle. In this paper we evaluate a new method for more efficiently updating the model at run-time.

#index 458206
#* Measuring Performance when Positives Are Rare: Relative Advantage versus Predictive Accuracy - A Biological Case Study
#@ Stephen Muggleton;Christopher H. Bryant;Ashwin Srinivasan
#t 2000
#c 20
#% 136350
#% 341682
#% 424988
#! This paper presents a new method of measuring performance when positives are rare and investigates whether Chomsky-like grammar representations are useful for learning accurate comprehensible predictors of members of biological sequence families. The positive-only learning framework of the Inductive Logic Programming (ILP) system CProgol is used to generate a grammar for recognising a class of proteins known as human neuropeptide precursors (NPPs). Performance is measured using both predictive accuracy and a new cost function, Relative Advantage (RA). The RA results show that searching for NPPs by using our best NPP predictor as a filter is more than 100 times more efficient than randomly selecting proteins for synthesis and testing them for biological activity. Predictive accuracy is not a good measure of performance for this domain because it does not discriminate well between NPP recognition models: despite covering varying numbers of (the rare) positives, all the models are awarded a similar (high) score by predictive accuracy because they all exclude most of the abundant negatives.

#index 458207
#* A Machine Learning Approach to Workflow Management
#@ Joachim Herbst
#t 2000
#c 20
#% 116700
#% 136350
#% 203029
#% 420063
#% 459006
#% 459021
#% 466716
#% 589166
#% 596257
#! There has recently been some interest in applying machine learning techniques to support the acquisition and adaptation of workflow models. The different learning algorithms, that have been proposed, share some restrictions, which may prevent them from being used in practice. Approaches applying techniques from grammatical inference are restricted to sequential workflows. Other algorithms allowing concurrency require unique activity nodes. This contribution shows how the basic principle of our previous approach to sequential workflow induction can be generalized, so that it is able to deal with concurrency. It does not require unique activity nodes. The presented approach uses a log-likelihood guided search in the space of workflow models, that starts with a most general workflow model containing unique activity nodes. Two split operators are available for specialization.

#index 458208
#* Relative Unsupervised Discretization for Regresseion Problems
#@ Marcus-Christopher Ludl;Gerhard Widmer
#t 2000
#c 20
#% 136350
#% 246117
#% 458283
#% 465904
#% 1271851
#! The paper describes a new, context-sensitive discretization algorithm that combines aspects of unsupervised (class-blind) and supervised methods. The algorithm is applicable to a wide range of machine learning and data mining problems where continuous attributes need to be discretized. In this paper, we evaluate its utility in a regression-by-classification setting. Preliminary experimental results indicate that the decision trees induced using this discretization strategy are significantly smaller and thus more comprehensible than those learned with standard discretization methods, while losing only minimally in numerical prediction accuracy. This may be a considerable advantage in machine learning and data mining applications where comprehensibility is an issue.

#index 458209
#* Learning Context-Free Grammars with a Simplicity Bias
#@ Pat Langley;Sean Stromsten
#t 2000
#c 20
#% 200195
#% 450973
#% 496866
#% 1348757
#! We examine the role of simplicity in directing the induction of context-free grammars from sample sentences. We present a rational reconstruction of Wolff's SNPR - the GRIDS system - which incorporates a bias toward grammars that minimize description length. The algorithm alternates between merging existing nonterminal symbols and creating new symbols, using a beam search to move from complex to simpler grammars. Experiments suggest that this approach can induce accurate grammars and that it scales reasonably to more difficult domains.

#index 458210
#* Nonparametric Regularization of Decision Trees
#@ Tobias Scheffer
#t 2000
#c 20
#% 136350
#% 229806
#% 266717
#% 449568
#% 466074
#% 466246
#% 495767
#% 565535
#! We discuss the problem of choosing the complexity of a decision tree (measured in the number of leaf nodes) that gives us highest generalization performance. We first discuss an analysis of the generalization error of decision trees that gives us a new perspective on the regularization parameter that is inherent to any regularization (e.g., pruning) algorithm. There is an optimal setting of this parameter for every learning problem; a setting that does well for one problem will inevitably do poorly for others. We will see that the optimal setting can in fact be estimated from the sample, without "trying out" various settings on holdout data. This leads us to a nonparametric decision tree regularization algorithm that can, in principle, work well for all learning problems.

#index 458211
#* Hidden Markov Models with Patterns and Their Application to Integrated Circuit Testing
#@ Laurent Bréhélin;Olivier Gascuel;Gilles Caraux
#t 2000
#c 20
#% 68730
#% 131687
#% 318041
#% 466716
#% 466850
#% 621911
#% 682435
#% 1010917
#! We present a new model, derived from classical Hidden Markov Models (HMMs), to learn sequences of large Boolean vectors. Our model - Hidden Markov Model with Patterns, or HMMP - differs from HMM by the fact that it uses patterns to define the emission probability distributions attached to the states. We also present an efficient state merging algorithm to learn this model from training vector sequences. This model and our algorithm are applied to learn Boolean vector sequences used to test integrated circuits. The learned HMMPs are used as test sequence generators. They achieve very high fault coverage, despite their reduced size, which demonstrates the effectiveness of our approach.

#index 458212
#* A Study on the Performance of Large Bayes Classifier
#@ Dimitris Meretakis;Hongjun Lu;Beat Wüthrich
#t 2000
#c 20
#% 136350
#% 246832
#% 280439
#% 481290
#! Large Bayes (LB) is a recently introduced classifier built from frequent and interesting itemsets. LB uses itemsets to create context-specific probabilistic models of the data and estimate the conditional probability P(ci|A) of each class i given a case A. In this paper we use chi-square tests to address several drawbacks of the originally proposed interestingness metric, namely: (i) the inability to capture certain really interesting patterns, (ii) the need for a user-defined and data dependent interestingness threshold, and (iii) the need to set a minimum support threshold. We also introduce some pruning criteria which allow for a trade-off between complexity and speed on one side and classification accuracy on the other. Our experimental results show that the modified LB outperforms the original LB, Naïve Bayes, C4.5 and TAN.

#index 458213
#* Diversity versus Quality in Classification Ensembles Based on Feature Selection
#@ Padraig Cunningham;John Carney
#t 2000
#c 20
#% 229972
#% 256615
#% 481349
#% 494577
#! Feature subset-selection has emerged as a useful technique for creating diversity in ensembles - particularly in classification ensembles. In this paper we argue that this diversity needs to be monitored in the creation of the ensemble. We propose an entropy measure of the outputs of the ensemble members as a useful measure of the ensemble diversity. Further, we show that using the associated conditional entropy as a loss function (error measure) works well and the entropy in the ensemble predicts well the reduction in error due to the ensemble. These measures are evaluated on a medical prediction problem and are shown to predict the performance of the ensemble well. We also show that the entropy measure of diversity has the added advantage that it seems to model the change in diversity with the size of the ensemble.

#index 458214
#* Metric-Based Inductive Learning Using Semantic Height Functions
#@ Zdravko Markov;Ivo Marinchev
#t 2000
#c 20
#% 458317
#! In the present paper we propose a consistent way to integrate syntactical least general generalizations (lgg's) with semantic evaluation of the hypotheses. For this purpose we use two different relations on the hypothesis space - a constructive one, used to generate lgg's and a semantic one giving the coverage-based evaluation of the lgg. These two relations jointly implement a semantic distance measure. The formal background for this is a height-based definition of a semi-distance in a join semi-lattice. We use some basic results from lattice theory and introduce a family of language independent coverage-based height functions. The theoretical results are illustrated by examples of solving some basic inductive learning tasks.

#index 458215
#* The Representation Race - Preprocessing for Handling Time Phenomena
#@ Katharina Morik
#t 2000
#c 20
#% 399
#% 152934
#% 185232
#% 190581
#% 191910
#% 232136
#% 269217
#% 269634
#% 280408
#% 363670
#% 380466
#% 385563
#% 388112
#% 396021
#% 420087
#% 463903
#% 466229
#% 466251
#% 545856
#% 550551
#! Designing the representation languages for the input, LE, and output, LH, of a learning algorithm is the hardest task within machine learning applications. This paper emphasizes the importance of constructing an appropriate representation LE for knowledge discovery applications using the example of time related phenomena. Given the same raw data - most frequently a database with time-stamped data - rather different representations have to be produced for the learning methods that handle time. In this paper, a set of learning tasks dealing with time is given together with the input required by learning methods which solve the tasks. Transformations from raw data to the desired representation are illustrated by three case studies.

#index 458216
#* Toward an Ecplanatory Similarity Measure for Nearest-Neighbor Classification
#@ Mathieu Latourrette
#t 2000
#c 20
#% 5182
#% 92533
#% 140588
#% 169651
#% 229972
#% 243727
#% 368984
#% 449566
#% 458364
#! In this paper, a new similarity measure for nearest-neighbor classification is introduced. This measure is an approximation of a theoretical similarity that has some interesting properties. In particular, this latter is a step toward a theory of concepts formation. It renders identical some examples that have distinct representations. Moreover, these examples share some properties relevant for the concept undertaken. Hence, a rule-based representation of the concept can be inferred from the theoretical similarity. Moreover, in this paper, the approximation is validated by some preliminary experiments on non-noisy datasets.

#index 458217
#* Dynamic Discretization of Continuous Values from Time Series
#@ Llanos Mora López;Inmaculada Fortes Ruiz;Rafael Morales Bueno;Francisco Triguero Ruiz
#t 2000
#c 20
#% 1115
#% 1116
#% 1117
#% 162940
#% 191983
#% 262902
#% 1272304
#! Two methods to assign discrete values to continuous values from time series, using dynamic information about the series, are proposed. The first method is based on a particular statistic which allows us to select a discrete value for a new continuous value from the series. The second one is based on a concept of significant distance between consecutive values from time series which is defined. This definition is based on qualitative changes in the time series values. In both methods, the conversion process of continuous values into discrete values is dynamic in opposition to static classical methods used in machine learning. Finally, we use the proposed methods in a practical case. We transform the daily clearness index time series into discrete values. The results display that the series with discrete values obtained from the dynamic process captures better the sequential properties of the original continuous series.

#index 458218
#* A Multiple Model Cost-Sensitive Approach for Intrusion Detection
#@ Wei Fan;Wenke Lee;Salvatore J. Stolfo;Matthew Miller
#t 2000
#c 20
#% 280437
#% 466268
#% 543262
#% 709657
#% 978633
#! Intrusion detection systems (IDSs) need to maximize security while minimizing costs. In this paper, we study the problem of building cost-sensitive intrusion detection models to be used for real-time detection. We briefly discuss the major cost factors in IDS, including consequential and operational costs. We propose a multiple model cost-sensitive machine learning technique to produce models that are optimized for user-defined cost metrics. Empirical experiments in off-line analysis show a reduction of approximately 97% in operational cost over a single model approach, and a reduction of approximately 30% in consequential cost over a pure accuracy-based approach.

#index 458219
#* Dynamic Feature Selection in Incremental Hierarchical Clustering
#@ Luis Talavera
#t 2000
#c 20
#% 191680
#% 243727
#% 243728
#% 444930
#% 451051
#% 451052
#% 458387
#% 465741
#% 466269
#% 998789
#% 1272283
#! Feature selection has received a lot of attention in the machine learning community, but mainly under the supervised paradigm. In this work we study the potential benefits of feature selection in hierarchical clustering tasks. Particularly we address this problem in the context of incremental clustering, following the basic ideas of Gennari [8]. By using a simple implementation, we show that a feature selection scheme running in parallel with the learning process can improve the clustering task under the dimensions of accuracy, efficiency in learning, efficiency in prediction and comprehensibility.

#index 458220
#* A Comparison of Ranking Methods for Classification Algorithm Selection
#@ Pavel Brazdil;Carlos Soares
#t 2000
#c 20
#% 126842
#% 169653
#% 191910
#% 232106
#% 232108
#% 272995
#% 459716
#% 465740
#% 478107
#! We investigate the problem of using past performance information to select an algorithm for a given classification problem. We present three ranking methods for that purpose: average ranks, success rate ratios and significant wins. We also analyze the problem of evaluating and comparing these methods. The evaluation technique used is based on a leave-one-out procedure. On each iteration, the method generates a ranking using the results obtained by the algorithms on the training datasets. This ranking is then evaluated by calculating its distance from the ideal ranking built using the performance information on the test dataset. The distance measure adopted here, average correlation, is based on Spearman's rank correlation coefficient. To compare ranking methods, a combination of Friedman's test and Dunn's multiple comparison procedure is adopted. When applied to the methods presented here, these tests indicate that the success rate ratios and average ranks methods perform better than significant wins.

#index 458221
#* The Utilization of Context Signals in the Analysis of ABR Potentials by Application of Neural Networks
#@ Andrzej Izworski;Ryszard Tadeusiewicz;Andrzej Paslawski
#t 2000
#c 20
#! The elaboration of head-surface registration techniques for auditory potentials evoked from the brainstem (ABR) enabled the construction of objective research and diagnostic methods, which can utilized in the examinations of auditory organs. The aim of the present work was the construction of a method, making use of the neural network techniques, enabling an automated detection of wave V in the ABR signals. The basic problem encountered in any attempts of automated analysis of the auditory potentials is connected with impossibility of a reliable evaluation of a single response evoked by a weak acoustic signal. It has been assumed that considerably better detection results should be obtained, when additional context information will be provided to the network's input. This assumption has been verified using complex, hybrid neural networks. As a result about 90% of correct recognitions has been achieved.

#index 458222
#* Knowledge Discovery from Very Large Databases Using Frequent Concept Lattices
#@ Kitsana Waiyamai;Lotfi Lakhal
#t 2000
#c 20
#% 169705
#% 209020
#% 279120
#% 465493
#! Data clustering and association rules discovery are two related problems in data mining. In this paper, we propose to integrate these two techniques using the frequent concept lattice data structure - a formal conceptual model that can be used to identify similarities among a set of objects based on their frequent attributes (frequent items). Experimental results show that clusterings and association rules are generated efficiently fromthe frequent concept lattice, since response time after lattice construction is measured almost in seconds.

#index 458223
#* Short-Term Profiling for a Case-Based Reasoning
#@ Esma Aïmeur;Mathieu Vézeau
#t 2000
#c 20
#% 176887
#% 494599
#% 530374
#! In this paper, we aim to address a frequent shortcoming of electronic commerce: the lack of customer service. We present an approach to product recommendation using a modified cycle for case-based reasoning in which a new refinement step is introduced. We then use this cycle combined with a heuristic we devised to create a short-term profile of the client. This profile is not stored or reused after the transaction, reducing maintenance. In fact, it allows the client and the system to find an appropriate product to satisfy the client on the basis of available products in a potentially efficient way.

#index 458224
#* Complexity Approximation Principle and Rissanen's Approach to Real-Valued Parameters
#@ Yuri Kalnishkan
#t 2000
#c 20
#% 70370
#% 234979
#% 235374
#% 251997
#% 466081
#! In this paper an application of the Complexity Approximation Principle to the non-linear regression is suggested. We combine this principle with the approximation of the complexity of a real-valued vector parameter proposed by Rissanen and thus derive a method for the choice of parameters in the non-linear regression.

#index 458315
#* Learning Different Types of New Attributes by Combining the Neural Network and Iterative Attribute Construction
#@ Yuh-Jyh Hu
#t 1997
#c 20

#index 458316
#* Uncertain Learning Agents (Abstract)
#@ Stuart J. Russell
#t 1997
#c 20

#index 458317
#* Metrics on Terms and Clauses
#@ Alan Hutchinson
#t 1997
#c 20

#index 458318
#* Parallel and Distributed Search for Structure in Multivariate Time Series
#@ Tim Oates;Matthew D. Schmill;Paul R. Cohen
#t 1997
#c 20

#index 458319
#* Induction of Feature Terms With INDIE
#@ Eva Armengol;Enric Plaza
#t 1997
#c 20

#index 458320
#* Learning When Negative Examples Abound
#@ Miroslav Kubat;Robert Holte;Stan Matwin
#t 1997
#c 20

#index 458321
#* A Model for Generalization Based on Confirmatory Induction
#@ Nicolas Lachiche;Pierre Marquis
#t 1997
#c 20

#index 458322
#* Conditions for Occam's Razor Applicability and Noise Elimination
#@ Dragan Gamberger;Nada Lavrac
#t 1997
#c 20

#index 458323
#* Constructing and Sharing Perceptual Distiinctions
#@ Luc Stells
#t 1997
#c 20

#index 458324
#* Search-Based Class Discretization
#@ Luís Torgo;Joao Gama
#t 1997
#c 20

#index 458325
#* Natural Ideal Operators in Inductive Logic Programming
#@ Fabien Torre;Céline Rouveirol
#t 1997
#c 20

#index 458326
#* Empirical Learning of Natural Language Processing Task
#@ Walter Daelemans;Antal van den Bosch;Ton Weijters
#t 1997
#c 20

#index 458327
#* Compression-Based Pruning of Decision Lists
#@ Bernhard Pfahringer
#t 1997
#c 20

#index 458328
#* Inducing and Using Decision Rules in the GRG Knowledge Discovery System
#@ Ning Shan;Howard J. Hamilton;Nick Cercone
#t 1997
#c 20

#index 458329
#* Integrated Learning and Planning Based on Truncating Temporal Differences
#@ Pawel Cichosz
#t 1997
#c 20

#index 458330
#* On Prediction by Data Compression
#@ Paul M. B. Vitányi;Ming Li
#t 1997
#c 20

#index 458331
#* Model Combination in the Multiple-Data-Batches Scenario
#@ Kai Ming Ting;Boon Toh Low
#t 1997
#c 20

#index 458332
#* Theta-Subsumption for Structural Matching
#@ Luc De Raedt;Peter Idestam-Almquist;Gunther Sablon
#t 1997
#c 20

#index 458333
#* Probabilistic Incremental Program Evolution: Stochastic Search Through Program Space
#@ Rafal Salustowicz;Jürgen Schmidhuber
#t 1997
#c 20

#index 458334
#* Ibots Learn Genuine Team Solutions
#@ Cristina Versino;Luca Maria Gambardella
#t 1997
#c 20

#index 458335
#* Constructing Intermediate Concepts by Decomposition of Real Functions
#@ Janez Demsar;Blaz Zupan;Marko Bohanec;Ivan Bratko
#t 1997
#c 20

#index 458336
#* Classification by Voting Feature Intervals
#@ Gülsen Demiröz;H. Altay Güvenir
#t 1997
#c 20

#index 458337
#* Exploiting Qualitative Knoledge to Enhance Skill Acquisition
#@ Cristina Baroglio
#t 1997
#c 20

#index 458338
#* A Case Study in Loyality and Satisfaction Research
#@ K. Vanhoof;Josee Bloemer;K. Pauwels
#t 1997
#c 20

#index 458339
#* NeuroLinear: A System for Extracting Oblique Decision Rules from Neural Networks
#@ Rudy Setiono;Huan Liu
#t 1997
#c 20

#index 458340
#* Finite-Element Methods with Local Triangulation Refinement for Continuous Reimforcement Learning Problems
#@ Rémi Munos
#t 1997
#c 20

#index 458341
#* Learning and Exploitation Do Not Conflict Under Minimax Optimality
#@ Csaba Szepesvári
#t 1997
#c 20

#index 458342
#* Learning in Dynamically Changing Domains: Theory Revision and Context Dependence Issues
#@ Charles Taylor;Gholamreza Nakhaeizadeh
#t 1997
#c 20

#index 458343
#* Learning Linear Constraints in Inductive Logic Programming
#@ Lionel Martin;Christel Vrain
#t 1997
#c 20

#index 458344
#* Inductive Genetic Programming with Decision Trees
#@ Nikolay I. Nikolaev;Vanio Slavov
#t 1997
#c 20

#index 458345
#* Case-Based Learning: Beyond Classification of Feature Vectors
#@ David W. Aha;Dietrich Wettschereck
#t 1997
#c 20

#index 458346
#* A Buffering Strategy to Avoid Ordering Effects in Clustering
#@ Luis Talavera;Josep Roure
#t 1998
#c 20

#index 458347
#* Combining Classifiers by Constructive Induction
#@ Joao Gama
#t 1998
#c 20

#index 458348
#* Induction of Recursive Program Schemes
#@ Ute Schmid;Fritz Wysotzki
#t 1998
#c 20

#index 458349
#* Part-of-Speech Tagging Using Decision Trees
#@ Lluís Màrquez;Horacio Rodríguez
#t 1998
#c 20

#index 458350
#* Recursive Lazy Learning for Modeling and Control
#@ Gianluca Bontempi;Mauro Birattari;Hugues Bersini
#t 1998
#c 20

#index 458351
#* ILP Experiments in Detecting Traffic Problems
#@ Saso Dzeroski;Nico Jacobs;Martín Molina;Carlos Moure
#t 1998
#c 20

#index 458352
#* Improved Pairwise Coupling Classification with Correcting Classifiers
#@ Miguel Moreira;Eddy Mayoraz
#t 1998
#c 20

#index 458353
#* An Inductive Logic Programming Framework to Learn a Concept from Ambiguous Examples
#@ Dominique Bouthinon;Henry Soldano
#t 1998
#c 20

#index 458354
#* Explanation-Based Generalization in Game Playing: Quantitative Results
#@ Stefan Schrödl
#t 1998
#c 20

#index 458355
#* Simulating Children Learning and Explaining Elementary Heat Transfer Phenomena: A Multistrategy System at Work
#@ Filippo Neri
#t 1998
#c 20

#index 458356
#* Error-Correcting Output Codes for Local Learners
#@ Francesco Ricci;David W. Aha
#t 1998
#c 20

#index 458357
#* Predicate Invention and Learning from Positive Examples Only
#@ Henrik Boström
#t 1998
#c 20

#index 458358
#* Inference of Finite Automata: Reducing the Search Space with an Ordering of Pairs of States
#@ François Coste;Jacques Nicolas
#t 1998
#c 20

#index 458359
#* Q-Learning and Redundancy Reduction in Classifier Systems with Internal State
#@ Antonella Giani;Andrea Sticca;Fabrizio Baiardi;Antonina Starita
#t 1998
#c 20

#index 458360
#* Inducing Models of human Control Skills
#@ Rui Camacho
#t 1998
#c 20

#index 458361
#* Pruning Decision Trees with Misclassification Costs
#@ Jeffrey P. Bradford;Clayton Kunz;Ron Kohavi;Clifford Brunk;Carla E. Brodley
#t 1998
#c 20

#index 458362
#* Speeding up Q(lambda)-Learning
#@ Marco Wiering;Jürgen Schmidhuber
#t 1998
#c 20

#index 458363
#* Continuous Mimetic Evolution
#@ Antoine Ducoulombier;Michèle Sebag
#t 1998
#c 20

#index 458364
#* Scope Classification: An Instance-Based Learning Algorithm with a Rule-Based Characterisation
#@ Nicolas Lachiche;Pierre Marquis
#t 1998
#c 20

#index 458365
#* Using Lattice-Based Framework as a Tool for Feature Extraction
#@ Engelbert Mephu Nguifo;Patrick Njiwoua
#t 1998
#c 20

#index 458366
#* God Doesn't Always Shave with Occam's Razor - Learning When and How to Prune
#@ Hilan Bensusan
#t 1998
#c 20

#index 458367
#* Interpretable Neural Networks with BP-SOM
#@ Ton Weijters;Antal van den Bosch;H. Jaap van den Herik
#t 1998
#c 20

#index 458368
#* Experiments on Solving Multiclass Learning Problems by n2-classifier
#@ Jacek Jelonek;Jerzy Stefanowski
#t 1998
#c 20

#index 458369
#* Naive (Bayes) at Forty: The Independence Assumption in Information Retrieval
#@ David D. Lewis
#t 1998
#c 20

#index 458370
#* Coevolutionary, Distributed Search for Inducing Concept Description
#@ Cosimo Anglano;Attilio Giordana;Giuseppe Lo Bello;Lorenza Saitta
#t 1998
#c 20

#index 458371
#* A Monotonic Measure for Optimal Feature Selection
#@ Huan Liu;Hiroshi Motoda;Manoranjan Dash
#t 1998
#c 20

#index 458372
#* Naive Bayesian Classifier Committees
#@ Zijian Zheng
#t 1998
#c 20

#index 458373
#* First-Order Learning for Web Mining
#@ Mark Craven;Seán Slattery;Kamal Nigam
#t 1998
#c 20

#index 458374
#* Learning to Classify X-Ray Images Using Relational Learning
#@ Claude Sammut;Tatjana Zrimec
#t 1998
#c 20

#index 458375
#* Composing Functions to Speed up Reinforcement Learning in a Changing World
#@ Chris Drummond
#t 1998
#c 20

#index 458376
#* A Short Note About the Application of Polynomial Kernels with Fractional Degree in Support Vector Learning
#@ Rolf Rossius;Gérard Zenker;Andreas Ittner;Werner Dilger
#t 1998
#c 20

#index 458377
#* Theoretical Results on Reinforcement Learning with Temporally Abstract Options
#@ Doina Precup;Richard S. Sutton;Satinder P. Singh
#t 1998
#c 20

#index 458378
#* Automatic Acquisition of Lexical Knowledge from Sparse and Noisy Data
#@ René Schneider
#t 1998
#c 20

#index 458379
#* Text Categorization with Suport Vector Machines: Learning with Many Relevant Features
#@ Thorsten Joachims
#t 1998
#c 20

#index 458380
#* Learning in Agent-Oriented Worlds (Abstract)
#@ Kenneth DeJong
#t 1998
#c 20

#index 458381
#* Convergence Rate of Minimization Learning for Neural Networks
#@ Marghny H. Mohamed;Teruya Minamoto;Koichi Niijima
#t 1998
#c 20

#index 458382
#* Error Estimators for Pruning Regression Trees
#@ Luís Torgo
#t 1998
#c 20

#index 458383
#* Classification Learning Using All Rules
#@ Murlikrishna Viswanathan;Geoffrey I. Webb
#t 1998
#c 20

#index 458384
#* Bayes Optimal Instance-Based Learning
#@ Petri Kontkanen;Petri Myllymäki;Tomi Silander;Henry Tirri
#t 1998
#c 20

#index 458385
#* A General Convergence Method for Reinforcement Learning in the Continuous Case
#@ Rémi Munos
#t 1998
#c 20

#index 458386
#* A Normalization Method for Contextual Data: Experience from a Large-Scale Application
#@ Sylvain Létourneau;Stan Matwin;Fazel Famili
#t 1998
#c 20

#index 458387
#* Determining Property Relevance in Concept Formation by Computing Correlation Between Properties
#@ João José Furtado Vasco
#t 1998
#c 20

#index 458388
#* A Host-Parasite Genetic Algorithm for Asymmetric Tasks
#@ Björn Olsson
#t 1998
#c 20

#index 458389
#* Feature Subset Selection in Text-Learning
#@ Dunja Mladenic
#t 1998
#c 20

#index 458390
#* Learning Verbal Transitivity Using LogLinear Models
#@ Nuno Miguel Marques;José Gabriel Pereira Lopes;Carlos Agra Coelho
#t 1998
#c 20

#index 458391
#* Batch Classification with Discrete Finite Mixtures
#@ Petri Kontkanen;Petri Myllymäki;Tomi Silander;Henry Tirri
#t 1998
#c 20

#index 458392
#* Bayesian and Information-Theories Priors for Bayesian Network Parameters
#@ Petri Kontkanen;Petri Myllymäki;Tomi Silander;Henry Tirri;Peter Grünwald
#t 1998
#c 20

#index 458393
#* Learning Trading Rules with Inductive Logic Programming
#@ Liviu Badea
#t 2000
#c 20
#% 224755
#% 550254
#! We apply Inductive Logic Programming (ILP) for inducing trading rules formed out of combinations of technical indicators from historical market data. To do this, we first identify ideal trading opportunities in the historical data, and then feed these as examples to an ILP learner, which will try to induce a description of them in terms of a given set of indicators. The main contributions of this paper are twofold. Conceptually, we are learning strategies in a chaotic domain in which learning a predictive model is impossible. Technically, we show a way of dealing with disjunctive positive examples, which create significant problems for most inductive learners.

#index 458394
#* Learning Patterns of Behavior by Observing System Events
#@ Marlon Núñez
#t 2000
#c 20
#% 92554
#% 459006
#! The proposed algorithm (BPL) induces behavior patterns from events taking into account characteristics of observed systems and their environment. The main strategy of this method consists on building summaries of the behaviour of a system as events arrive, and take these summaries as training examples. BPL constructs summaries with new features from events, like duration of current event values, repetitions of an event in a period of time, amongst others. This algorithm has been tested in learning faulty behavior of networks with the purpose of continuously predicting alarms.

#index 458395
#* Asymmetric Co-evolution for Imperfect-Information Zero-Sum Games
#@ Ole Martin Halck;Fredrik A. Dahl
#t 2000
#c 20
#% 124073
#% 207252
#% 233137
#% 268952
#% 372197
#% 466207
#% 1022849
#! We present an asymmetric co-evolutionary learning algorithm for imperfect-information zero-sum games. This algorithm is designed so that the fitness of the individual agents is calculated in a way that is compatible with the goal of game-theoretic optimality. This compatibility has been somewhat lacking in previous co-evolutionary approaches, as these have often depended on unwarranted assumptions about the absolute and relative strength of players. Our algorithm design is tested on a game for which the optimal strategy is known, and is seen to work well.

#index 458665
#* Scaling Boosting by Margin-Based Inclusionof Features and Relations
#@ Susanne Hoche;Stefan Wrobel
#t 2002
#c 20
#% 217072
#% 266255
#% 283138
#% 345862
#% 458257
#% 464289
#% 477950
#% 497608
#% 543595
#% 550732
#% 550736
#% 550740
#% 564956
#% 700962
#% 1499573
#! Boosting is well known to increase the accuracy of propositional and multi-relational classification learners. However, the base learner's efficiency vitally determines boosting's efficiency since the complexity of the underlying learner is amplified by iterated calls of the learner in the boosting framework. The idea of restricting the learner to smaller feature subsets in order to increase efficiency is widely used. Surprisingly, little attention has been paid so far to exploiting characteristics of boosting itself to include features based on the current learning progress. In this paper, we show that the dynamics inherent to boosting offer ideal means to maximize the efficiency of the learning process. We describe how to utilize the training examples' margins - which are known to be maximized by boosting - to reduce learning times without a deterioration of the learning quality. We suggest to stepwise include features in the learning process in response to a slowdown in the improvement of the margins. Experimental results show that this approach significantly reduces the learning time while maintaining or even improving the predictive accuracy of the underlying fully equipped learner.

#index 458666
#* Variance Optimized Bagging
#@ Philip Derbeko;Ran El-Yaniv;Ron Meir
#t 2002
#c 20
#% 190581
#% 209021
#% 312727
#% 424997
#% 465755
#% 551723
#% 1042724
#% 1478814
#% 1499573
#! We propose and study a new technique for aggregating an ensemble of bootstrapped classifiers. In this method we seek a linear combination of the base-classifiers such that the weights are optimized to reduce variance. Minimum variance combinations are computed using quadratic programming. This optimization technique is borrowed from Mathematical Finance where it is called Markowitz Mean-Variance Portfolio Optimization. We test the new method on a number of binary classification problems from the UCI repository using a Support Vector Machine (SVM) as the base-classifier learning algorithm. Our results indicate that the proposed technique can consistently outperform Bagging and can dramatically improve the SVM performance even in cases where the Bagging fails to improve the base-classifier.

#index 458667
#* A Kernel Approach for Learning from almost Orthogonal Patterns
#@ Bernhard Schölkopf;Jason Weston;Eleazar Eskin;Christina Leslie;William Stafford Noble
#t 2002
#c 20
#% 116149
#% 190581
#% 197394
#% 269222
#% 304917
#% 397654
#% 425048
#% 451933
#% 722803
#% 837668
#! In kernel methods, all the information about the training data is contained in the Gram matrix. If this matrix has large diagonal values, which arises for many types of kernels, then kernel methods do not perform well. We propose and test several methods for dealing with this problem by reducing the dynamic range of the matrix while preserving the positive definiteness of the Hessian of the quadratic programming problem that one has to solve when training a Support Vector Machine.

#index 458668
#* Revising Engineering Models: Combining Computational Discovery with Knowledge
#@ Stephen D. Bay;Daniel G. Shapiro;Pat Langley
#t 2002
#c 20
#% 24538
#% 348816
#% 465896
#% 546515
#% 546533
#% 1271832
#! Developing mathematical models that represent physical devices is a difficult and time consuming task. In this paper, we present a hybrid approach to modeling that combines machine learning methods with knowledge from a human domain expert. Specifically, we propose a system for automatically revising an initial model provided by an expert with an equation discovery program that is tightly constrained by domain knowledge. We apply our system to learning an improved model of a battery on the International Space Station from telemetry data. Our results suggest that this hybrid approach can reduce model development time and improve model quality.

#index 458669
#* Transductive Confidence Machines for Pattern Recognition
#@ Kostas Proedrou;Ilia Nouretdinov;Volodya Vovk;Alexander Gammerman
#t 2002
#c 20
#% 190581
#% 234979
#% 309208
#% 458642
#% 458679
#% 464286
#% 563093
#! We propose a new algorithm for pattern recognition that outputs some measures of "reliability" for every prediction made, in contrast to the current algorithms that output "bare" predictions only. Our method uses a rule similar to that of nearest neighbours to infer predictions; thus its predictive performance is close to that of nearest neighbours, while the measures of confidence it outputs provide practically useful information for individual predictions.

#index 458670
#* Finding Hidden Factors Using Independent Component Analysis
#@ Erkki Oja
#t 2002
#c 20
#! Independent Component Analysis (ICA) is a computational technique for revealing hidden factors that underlie sets of measurements or signals. ICA assumes a statistical model whereby the observed multivariate data, typically given as a large database of samples, are assumed to be linear or nonlinear mixtures of some unknown latent variables. The mixing coefficients are also unknown. The latent variables are non-gaussian and mutually independent, and they are called the independent components of the observed data. By ICA, these independent components, also called sources or factors, can be found. Thus ICA can be seen as an extension to Principal Component Analysis and Factor Analysis. ICA is a much richer technique, however, capable of finding the sources when these classical methods fail completely.In many cases, the measurements are given as a set of parallel signals or time series. Typical examples are mixtures of simultaneous sounds or human voices that have been picked up by several microphones, brain signal measurements from multiple EEG sensors, several radio signals arriving at a portable phone, or multiple parallel time series obtained from some industrial process. The term blind source separation is used to characterize this problem.The lecture will first cover the basic idea of demixing in the case of a linear mixing model and then take a look at the recent nonlinear demixing approaches.Although ICA was originally developed for digital signal processing applications, it has recently been found that it may be a powerful tool for analyzing text document data as well, if the documents are presented in a suitable numerical form. A case study on analyzing dynamically evolving text is covered in the talk.

#index 458671
#* Improved Smoothing for Probabilistic Suffix Trees Seen as Variable Order Markov Chains
#@ Christopher Kermorvant;Pierre Dupont
#t 2002
#c 20
#% 272995
#% 469424
#! In this paper, we compare Probabilistic Suffix Trees (PST), recently proposed, to a specific smoothing of Markov chains and show that they both induce the same model, namely a variable order Markov chain. We show a weakness of PST in terms of smoothing and propose to use an enhanced smoothing. We show that the model based on enhanced smoothing outperform the PST while needing less parameters on a protein domain detection task on public databases.

#index 458672
#* Phase Transitions and Stochastic Local Search in k-Term DNF Learning
#@ Ulrich Rückert;Stefan Kramer;Luc De Raedt
#t 2002
#c 20
#% 140949
#% 160270
#% 180945
#% 182688
#% 216995
#% 420102
#% 425004
#% 466422
#% 1272290
#% 1273577
#% 1273900
#% 1279714
#% 1478779
#! In the past decade, there has been a lot of interest in phase transitions within artificial intelligence, and more recently, in machine learning and inductive logic programming. We investigate phase transitions in learning k-term DNF boolean formulae, a practically relevant class of concepts. We do not only show that there exist phase transitions, but also characterize and locate these phase transitions using the parameters k, the number of positive and negative examples, and the number of boolean variables. Subsequently, we investigate stochastic local search (SLS) for k-term DNF learning. We compare several variants that first reduce k-term DNF to SAT and then apply well-known SLS algorithms, such as GSAT and WalkSAT. Our experiments indicate that WalkSAT is able to solve the largest fraction of hard problem instances.

#index 458673
#* Variational Extensions to EM and Multinomial PCA
#@ Wray L. Buntine
#t 2002
#c 20
#% 272536
#% 278040
#% 280819
#% 293364
#% 303620
#% 466900
#% 722904
#! Several authors in recent years have proposed discrete analogues to principle component analysis intended to handle discrete or positive only data, for instance suited to analyzing sets of documents. Methods include non-negative matrix factorization, probabilistic latent semantic analysis, and latent Dirichlet allocation. This paper begins with a review of the basic theory of the variational extension to the expectation-maximization algorithm, and then presents discrete component finding algorithms in that light. Experiments are conducted on both bigram word data and document bag-of-word to expose some of the subtleties of this new class of algorithms.

#index 458674
#* Possibilistic Induction in Decision-Tree Learning
#@ Eyke Hüllermeier
#t 2002
#c 20
#% 136350
#% 246243
#% 420102
#% 449531
#% 449568
#% 564273
#% 1499572
#% 1788248
#! We propose a generalization of Ockham's razor, a widely applied principle of inductive inference. This generalization intends to capture the aspect of uncertainty involved in inductive reasoning. To this end, Ockham's razor is formalized within the framework of possibility theory: It is not simply used for identifying a single, apparently optimal model, but rather for concluding on the possibility of various candidate models. The possibilistic version of Ockham's razor is applied to (lazy) decision tree learning.

#index 458675
#* Learning to Play a Highly Complex Game from Human Expert Games
#@ Tony Kråkenes;Ole Martin Halck
#t 2002
#c 20
#% 176887
#% 376266
#% 453691
#% 453693
#% 466906
#% 494266
#% 1022889
#% 1280043
#! When the number of possible moves in each state of a game becomes very high, standard methods for computer game playing are no longer feasible. We present an approach for learning to play such a game from human expert games. The high complexity of the action space is dealt with by collapsing the very large set of allowable actions into a small set of categories according to their semantic intent, while the complexity of the state space is handled by representing the states of collections of pieces by a few relevant features in a location-independent way. The state-action mappings implicit in the expert games are then learnt using neural networks. Experiments compare this approach to methods that have previously been applied to this domain.

#index 458676
#* On-Line Support Vector Machine Regression
#@ Mario Martin
#t 2002
#c 20
#% 190581
#% 260001
#% 309208
#% 384911
#% 466510
#% 592108
#! This paper describes an on-line method for building 驴-insensitive support vector machines for regression as described in [12]. The method is an extension of the method developed by [1] for building incremental support vector machines for classification. Machines obtained by using this approach are equivalent to the ones obtained by applying exact methods like quadratic programming, but they are obtained more quickly and allow the incremental addition of new points, removal of existing points and update of target values for existing data. This development opens the application of SVM regression to areas such as on-line prediction of temporal series or generalization of value functions in reinforcement learning.

#index 458677
#* Evidence that Incremental Delta-Bar-Delta Is an Attribute-Efficient Linear Learner
#@ Harlan D. Harris
#t 2002
#c 20
#% 67056
#% 165663
#% 243730
#% 266368
#% 266789
#% 303617
#% 425020
#% 451055
#% 722814
#! The Winnow class of on-line linear learning algorithms [10, 11] was designed to be attribute-efficient. When learning with many irrelevant attributes, Winnow makes a number of errors that is only logarithmic in the number of total attributes, compared to the Perceptron algorithm, which makes a nearly linear number of errors. This paper presents data that argues that the Incremental Delta-Bar-Delta (IDBD) second-order gradient-descent algorithm [14] is attribute-efficient, performs similarly to Winnow on tasks with many irrelevant attributes, and also does better than Winnow on a task where Winnow does poorly. Preliminary analysis supports this empirical claim by showing that IDBD, like Winnow and other attribute-efficient algorithms, and unlike the Perceptron algorithm, has weights that can grow exponentially quickly. By virtue of its more flexible approach to weight updates, however, IDBD may be a more practically useful learning algorithm than Winnow.

#index 458678
#* A Multistrategy Approach to the Classification of Phases in Business Cycles
#@ Katharina Morik;Stefan Rüping
#t 2002
#c 20
#% 399
#% 232122
#% 290482
#% 458215
#% 466506
#% 481290
#! The classification of business cycles is a hard and important problem. Government as well as business decisions rely on the assessment of the current business cycle. In this paper, we investigate how economists can be better supported by a combination of machine learning techniques. We have successfully applied Inductive Logic Programming (ILP). For establishing time and value intervals different discretization procedures are discussed. The rule sets learned from different experiments were analyzed with respect to correlations in order to find a concept drift or shift.

#index 458679
#* Inductive Confidence Machines for Regression
#@ Harris Papadopoulos;Kostas Proedrou;Volodya Vovk;Alexander Gammerman
#t 2002
#c 20
#% 190581
#% 234979
#% 309208
#% 458642
#% 458669
#% 464286
#% 466258
#% 543745
#% 1273833
#% 1650581
#! The existing methods of predicting with confidence give good accuracy and confidence values, but quite often are computationally inefficient. Some partial solutions have been suggested in the past. Both the original method and these solutions were based on transductive inference. In this paper we make a radical step of replacing transductive inference with inductive inference and define what we call the Inductive Confidence Machine (ICM); our main concern in this paper is the use of ICM in regression problems. The algorithm proposed in this paper is based on the Ridge Regression procedure (which is usually used for outputting bare predictions) and is much faster than the existing transductive techniques. The inductive approach described in this paper may be the only option available when dealing with large data sets.

#index 458680
#* Reasoning with Classifiers
#@ Dan Roth
#t 2002
#c 20
#% 97615
#% 239245
#% 266368
#% 271294
#% 464434
#% 722754
#% 815304
#% 1290050
#% 1650318
#! Research in machine learning concentrates on the study of learning single concepts from examples. In this framework the learner attempts to learn a single hidden function from a collection of examples, assumed to be drawn independently from some unknown probability distribution. However, in many cases - as in most natural language and visual processing situations - decisions depend on the outcomes of several different but mutually dependent classifiers. The classifiers' outcomes need to respect some constraints that could arise from the sequential nature of the data or other domain specific conditions, thus requiring a level of inference on top the predictions.We will describe research and present challenges related to Inference with Classifiers - a paradigm in which we address the problem of using the outcomes of several different classifiers in making coherent inferences - those that respect constraints on the outcome of the classifiers. Examples will be given from the natural language domain.

#index 458681
#* Class Probability Estimation and Cost-Sensitive Classification Decisions
#@ Dragos D. Margineantu
#t 2002
#c 20
#% 136350
#% 236656
#% 280437
#% 312727
#% 400847
#% 458361
#% 464280
#% 466086
#% 466268
#% 466568
#% 466760
#% 729437
#% 1289273
#% 1289281
#% 1378224
#! For a variety of applications, machine learning algorithms are required to construct models that minimize the total loss associated with the decisions, rather than the number of errors. One of the most efficient approaches to building models that are sensitive to non-uniform costs of errors is to first estimate the class probabilities of the unseen instances and then to make the decision based on both the computed probabilities and the loss function. Although all classification algorithms can be converted into algorithms for learning models that compute class probabilities, in many cases the computed estimates have proven to be inaccurate. As a result, there is a large research effort to improve the accuracy of the estimates computed by different algorithms. This paper presents a novel approach to cost-sensitive learning that addresses the problem of minimizing the actual cost of the decisions rather than improving the overall quality of the probability estimates. The decision making step for our methods is based on the distribution of the individual scores computed by classifiers that are built by different types of ensembles of decision trees. The new approach relies on statistics that measure the probability that the computed estimates are on one side or the other of the decision boundary, rather than trying to improve the quality of the estimates. The experimental analysis of the new algorithms that were developed based on our approach gives new insight into cost-sensitive decision making and shows that for some tasks, the new algorithms outperform some of the best probability-based algorithms for cost-sensitive learning.

#index 458682
#* Propagation of Q-values in Tabular TD(lambda)
#@ Philippe Preux
#t 2002
#c 20
#% 90041
#% 157736
#% 160859
#% 183499
#% 201257
#% 252330
#% 384911
#% 393786
#% 449561
#% 1291498
#! In this paper, we propose a new idea for tabular TD(驴) algorithm. In TD learning, rewards are propagated along the sequence of state/action pairs that have been visited recently. In complement to this, we propose to propagate rewards towards neighboring state/action pairs along this sequence, though unvisited. This leads to a great decrease in the number of iterations required for TD(驴) to be able to generalize since it is no longer necessary that a state/action pair is visited for its Q-value to be updated. The use of this propagation process makes tabular TD(驴) coming closer to neural net based TD(驴) with regards to its ability to generalize, while keeping unchanged other properties of tabular TD(驴).

#index 458683
#* Macro-Operators in Multirelational Learning: A Search-Space Reduction Technique
#@ Lourdes Peña Castillo;Stefan Wrobel
#t 2002
#c 20
#% 1474
#% 217072
#% 382569
#% 497608
#% 550417
#% 550549
#% 1280034
#! Refinement operators are frequently used in the area of multirelational learning (Inductive Logic Programming, ILP) in order to search systematically through a generality order on clauses for a correct theory. Only the clauses reachable by a finite number of applications of a refinement operator are considered by a learning system using this refinement operator; ie. the refinement operator determines the search space of the system. For efficiency reasons, we would like a refinement operator to compute the smallest set of clauses necessary to find a correct theory. In this paper we present a formal method based on macro-operators to reduce the search space defined by a downward refinement operator (驴) while finding the same theory as the original operator. Basically we define a refinement operator which adds to a clause not only single-literals but also automatically created sequences of literals (macro-operators). This in turn allows us to discard clauses which do not belong to a correct theory. Experimental results show that this technique significantly reduces the search-space and thus accelerates the learning process.

#index 458684
#* An Empirical Study of Encoding Schemes and Search Strategies in Discovering Causal Networks
#@ Honghua Dai;Gang Li;Yiqing Tu
#t 2002
#c 20
#% 44876
#% 212700
#% 268056
#% 277396
#% 501979
#% 1271903
#! Efficiently inducing precise causal models accurately reflecting given data sets is the ultimate goal of causal discovery. The algorithm proposed by Wallace et al. 10. has demonstrated its ability in discovering Linear Causal Models from data. To explore the ways to improve efficiency, this research examines three different encoding schemes and four searching strategies. The experimental results reveal that (1) specifying parents encoding method is the best among three encoding methods we examined; (2) In the discovery of linear causal models, local Hill climbing works very well compared to other more sophisticated methods, like Markov Chain Monte Carto (MCMC), Genetic Algorithm (GA) and Parallel MCMC searching.

#index 458685
#* Sparse Online Greedy Support Vector Regression
#@ Yaakov Engel;Shie Mannor;Ron Meir
#t 2002
#c 20
#% 190581
#% 269218
#% 280481
#% 309208
#% 351094
#% 425040
#% 494573
#% 722757
#% 722817
#! We present a novel algorithm for sparse online greedy kernel-based nonlinear regression. This algorithm improves current approaches to kernel-based regression in two aspects. First, it operates online - at each time step it observes a single new input sample, performs an update and discards it. Second, the solution maintained is extremely sparse. This is achieved by an explicit greedy sparsification process that admits into the kernel representation a new input sample only if its feature space image is linearly independent of the images of previously admitted samples. We show that the algorithm implements a form of gradient ascent and demonstrate its scaling and noise tolerance properties on three benchmark regression problems.

#index 458686
#* Q-Cut - Dynamic Discovery of Sub-goals in Reinforcement Learning
#@ Ishai Menache;Shie Mannor;Nahum Shimkin
#t 2002
#c 20
#% 53085
#% 122671
#% 124691
#% 124692
#% 252330
#% 270031
#% 286423
#% 393786
#% 464303
#% 466723
#% 476730
#% 565545
#% 586198
#% 1271827
#! We present the Q-Cut algorithm, a graph theoretic approach for automatic detection of sub-goals in a dynamic environment, which is used for acceleration of the Q-Learning algorithm. The learning agent creates an on-line map of the process history, and uses an efficient Max-Flow/Min-Cut algorithm for identifying bottlenecks. The policies for reaching bottlenecks are separately learned and added to the model in a form of options (macro-actions). We then extend the basic Q-Cut algorithm to the Segmented Q-Cut algorithm, which uses previously identified bottlenecks for state space partitioning, necessary for finding additional bottlenecks in complex environments. Experiments showsign ificant performance improvements, particulary in the initial learning phase.

#index 458687
#* Ranking with Predictive Clustering Trees
#@ Ljupco Todorovski;Hendrik Blockeel;Saso Dzeroski
#t 2002
#c 20
#% 191919
#% 458656
#% 466073
#% 466722
#% 478107
#% 478112
#% 478269
#% 1271968
#! A novel class of applications of predictive clustering trees is addressed, namely ranking. Predictive clustering trees, as implemented in CLUS, allow for predicting multiple target variables. This approach makes sense especially if the target variables are not independent of each other. This is typically the case in ranking, where the (relative) performance of several approaches on the same task has to be predicted from a given description of the task. We propose to use predictive clustering trees for ranking. As compared to existing ranking approaches which are instance-based, our approach also allows for an explanation of the predicted rankings. We illustrate our approach on the task of ranking machine learning algorithms, where the (relative) performance of the learning algorithms on a dataset has to be predicted from a given dataset description.

#index 458688
#* Characterizing Markov Decision Processes
#@ Bohdana Ratitch;Doina Precup
#t 2002
#c 20
#% 25998
#% 30037
#% 160859
#% 188076
#% 194647
#% 210191
#% 216112
#% 284108
#% 363744
#% 384911
#% 393786
#% 420726
#% 466075
#% 466598
#% 1650283
#! Problem characteristics often have a significant influence on the difficulty of solving optimization problems. In this paper, we propose attributes for characterizing Markov Decision Processes (MDPs), and discuss how they affect the performance of reinforcement learning algorithms that use function approximation. The attributes measure mainly the amount of randomness in the environment. Their values can be calculated from the MDP model or estimated on-line. We show empirically that two of the proposed attributes have a statistically significant effect on the quality of learning. We discuss how measurements of the proposed MDP attributes can be used to facilitate the design of reinforcement learning systems.

#index 458689
#* An Information Geometric Perspective on Active Learning
#@ Chen-Hsiang Yeang
#t 2002
#c 20
#% 132697
#% 213009
#% 1289266
#% 1393556
#% 1810527
#! The Fisher information matrix plays a very important role in both active learning and information geometry. In a special case of active learning (nonlinear regression with Gaussian noise), the inverse of the Fisher information matrix - the dispersion matrix of parameters - induces a variety of criteria for optimal experiment design. In information geometry, the Fisher information matrix defines the metric tensor on model manifolds. In this paper, I explore the intrinsic relations of these two fields. The conditional distributions which belong to exponential families are known to be dually flat. Moreover, the author proves for a certain type of conditional models, the embedding curvature in terms of true parameters also vanishes. The expected Riemannian distance between current parameters and the next update is proposed to be the loss function for active learning. Examples of nonlinear and logistic regressions are given in order to elucidate this active learning scheme.

#index 458690
#* Learning and Inference for Clause Identification
#@ Xavier Carreras;Lluís Màrquez;Vasin Punyakanok;Dan Roth
#t 2002
#c 20
#% 302391
#% 404263
#% 748721
#% 853697
#% 853887
#% 1128737
#! This paper presents an approach to partial parsing of natural language sentences that makes global inference on top of the outcome of hierarchically learned local classifiers. The best decomposition of a sentence into clauses is chosen using a dynamic programming based scheme that takes into account previously identified partial solutions. This inference scheme applies learning at several levels--when identifying potential clauses and when scoring partial solutions. The classifiers are trained in a hierarchical fashion, building on previous classifications. The method presented significantly outperforms the best methods known so far for clause identification.

#index 458691
#* Robustness Analyses of Instance-Based Collaborative Recommendation
#@ Nicholas Kushmerick
#t 2002
#c 20
#% 54204
#% 124010
#% 202011
#% 338443
#% 347192
#% 563963
#% 593891
#% 783438
#% 1650569
#! Collaborative recommendation has emerged as an effective technique for a personalized information access. However, there has been relatively little theoretical analysis of the conditions under which the technique is effective. We analyze the robustness of collaborative recommendation: the ability to make recommendations despite (possibly intentional) noisy product ratings. We formalize robustness in machine learning terms, develop two theoretically justified models of robustness, and evaluate the models on real-world data. Our investigation is both practically relevant for enterprises wondering whether collaborative recommendation leaves their marketing operations open to attack, and theoretically interesting for the light it sheds on a comprehensive theory of collaborative recommendation.

#index 458692
#* Support Vector Machines for Polycategorical Classification
#@ Ioannis Tsochantaridis;Thomas Hofmann
#t 2002
#c 20
#% 124010
#% 173879
#% 190581
#% 202011
#% 252011
#% 266281
#% 304917
#% 311027
#% 329569
#% 393059
#% 406493
#% 458638
#% 466263
#% 528182
#% 1273828
#% 1650569
#! Polycategorical classification deals with the task of solving multiple interdependent classification problems. The key challenge is to systematically exploit possible dependencies among the labels to improve on the standard approach of solving each classification problem independently. Our method operates in two stages: the first stage uses the observed set of labels to learn a joint label model that can be used to predict unobserved pattern labels purely based on inter-label dependencies. The second stage uses the observed labels as well as inferred label predictions as input to a generalized transductive support vector machine. The resulting mixed integer program is heuristically solved with a continuation method. We report experimental results on a collaborative filtering task that provide empirical support for our approach.

#index 458693
#* Stacking with an Extended Set of Meta-level Attributes and MLR
#@ Bernard Zenko;Saso Dzeroski
#t 2002
#c 20
#% 92533
#% 132938
#% 136350
#% 272995
#% 290482
#% 424994
#% 431101
#% 464452
#% 466480
#% 478435
#% 549438
#% 551723
#% 1272397
#% 1650665
#! We propose a new set of meta-level features to be used for learning how to combine classifier predictions with stacking. This set includes the probability distributions predicted by the base-level classifiers and a combination of these with the certainty of the predictions. We use these features in conjunction with multi-response linear regression (MLR) at the meta-level. We empirically evaluate the proposed approach in comparison to several state-of-the-art methods for constructing ensembles of heterogeneous classifiers with stacking. Our approach performs better than existing stacking approaches and also better than selecting the best classifier from the ensemble by cross validation (unlike existing stacking approaches, which at best perform comparably to it).

#index 458694
#* How to Make AdaBoost.M1 Work for Weak Base Classifiers by Changing Only One Line of the Code
#@ Günther Eibl;Karl Peter Pfeiffer
#t 2002
#c 20
#% 235377
#% 276516
#% 302391
#% 465751
#% 722756
#% 1272365
#! If one has a multiclass classification problem and wants to boost a multiclass base classifier AdaBoost.M1 is a well known and widely applicated boosting algorithm. However AdaBoost.M1 does not work, if the base classifier is too weak. We show, that with a modification of only one line of AdaBoost.M1 one can make it usable for weak base classifiers, too. The resulting classifier AdaBoost.M1W is guaranteed to minimize an upper bound for a performance measure, called the guessing error, as long as the base classifier is better than random guessing. The usability of AdaBoost.M1W could be clearly demonstrated experimentally.

#index 458695
#* iBoost: Boosting Using an i nstance-Based Exponential Weighting Scheme
#@ Stephen Kwek;Chau Nguyen
#t 2002
#c 20
#% 697
#% 73372
#% 132938
#% 157162
#% 165663
#% 198701
#% 209021
#% 266257
#% 345754
#% 431101
#% 465746
#% 520368
#! Recently, Freund, Mansour and Schapire established that using exponential weighting scheme in combining classifiers reduces the problem of overfitting. Also, Helmbold, Kwek and Pitt that showed in the prediction using a pool of experts framework an instance based weighting scheme improves performance. Motivated by these results, we propose here an instance-based exponential weighting scheme in which the weights of the base classifiers are adjusted according to the test instance x. Here, a competency classifier ci is constructed for each base classifier hi to predict whether the base classifier's guess of x's label can be trusted and adjust the weight of hi accordingly. We show that this instance-based exponential weighting scheme enhances the performance of AdaBoost.

#index 458696
#* Multiclass Alternating Decision Trees
#@ Geoffrey Holmes;Bernhard Pfahringer;Richard Kirkby;Eibe Frank;Mark Hall
#t 2002
#c 20
#% 252009
#% 272995
#% 465751
#% 466240
#% 564273
#% 722756
#% 722807
#% 1272365
#! The alternating decision tree (ADTree) is a successful classification technique that combines decision trees with the predictive accuracy of boosting into a set of interpretable classification rules. The original formulation of the tree induction algorithm restricted attention to binary classification problems. This paper empirically evaluates several wrapper methods for extending the algorithm to the multiclass case by splitting the problem into several two-class problems. Seeking a more natural solution we then adapt the multiclass LogitBoost and AdaBoost.MH procedures to induce alternating decision trees directly. Experimental results confirm that these procedures are comparable with wrapper methods that are based on the original ADTree formulation in accuracy, while inducing much smaller trees.

#index 458697
#* Pairwise Classification as an Ensemble Technique
#@ Johannes Fürnkranz
#t 2002
#c 20
#% 99396
#% 136350
#% 209021
#% 235377
#% 269634
#% 272518
#% 277919
#% 283138
#% 290482
#% 302391
#% 312727
#% 458623
#% 464307
#% 465751
#% 465900
#% 565543
#% 630994
#% 722756
#% 722807
#% 1272365
#% 1499573
#% 1860941
#! In this paper we investigate the performance of pairwise (or round robin) classification, originally a technique for turning multi-class problems into two-class problems, as a general ensemble technique. In particular, we show that the use of round robin ensembles will also increase the classification performance of decision tree learners, even though they can directly handle multi-class problems. The performance gain is not as large as for bagging and boosting, but on the other hand round robin ensembles have a clearly defined semantics. Furthermore, we show that the advantage of pairwise classification over direct multi-class classification and one-against-all binarization increases with the number of classes, and that round robin ensembles form an interesting alternative for problems with ordered class values.

#index 458698
#* Learning with Mixture Models: Concepts and Applications
#@ Padhraic Smyth
#t 2002
#c 20
#! Probabilistic mixture models have been used in statistics for well over a century as flexible data models. More recently these techniques have been adopted by the machine learning and data mining communities in a variety of application settings. We begin this talk with a review of the basic concepts of finite mixture models: what can they represent? how can we learn them from data? and so on. We will then discuss how the traditional mixture model (defined in a fixed dimensional vector space) can be usefully generalized to model non-vector data, such as sets of sequences and sets of curves. A number of real-world applications will be used to illustrate how these techniques can be applied to large-scale real-world data exploration and prediction problems, including clustering of visitors to a Web site based on their sequences of page requests, modeling of sparse high-dimensional "market basket" data for retail forecasting, and clustering of storm trajectories in atmospheric science.

#index 458699
#* Learning Classification with Both Labeled and Unlabeled Data
#@ Jean-Noël Vittaut;Massih-Reza Amini;Patrick Gallinari
#t 2002
#c 20
#% 131258
#% 194251
#% 252011
#% 260001
#% 266370
#% 304876
#% 309116
#% 311027
#% 311034
#% 318412
#% 397136
#% 458369
#% 458379
#% 466263
#% 466580
#% 478595
#% 492337
#% 1306081
#! A key difficulty for applying machine learning classification algorithms for many applications is that they require a lot of hand-labeled examples. Labeling large amount of data is a costly process which in many cases is prohibitive. In this paper we show how the use of a small number of labeled data together with a large number of unlabeled data can create high-accuracy classifiers. Our approach does not rely on any parametric assumptions about the data as it is usually the case with generative methods widely used in semi-supervised learning. We propose new discriminant algorithms handling both labeled and unlabeled data for training classification models and we analyze their performances on different information access problems ranging from text span classification for text summarization to e-mail spam detection and text classification.

#index 458700
#* Collaborative Learning of Term-Based Concepts for Automatic Query Expansion
#@ Stefan Klink;Armin Hust;Markus Junker;Andreas Dengel
#t 2002
#c 20
#% 2066
#% 46803
#% 108979
#% 144074
#% 169806
#% 253188
#% 279755
#% 280817
#% 387427
#% 546512
#! Information Retrieval Systems have been studied in Computer Science for decades. The traditional ad-hoc task is to find all documents relevant for an ad-hoc given query but the accuracy of ad-hoc document retrieval systems has plateaued in recent years. At DFKI, we are working on so-called collaborative information retrieval (CIR) systems which unintrusively learn from their users search processes. In this paper, a new approach is presented called term-based concept learning (TCL) which learns conceptual description terms occurring in known queries. A new query is expanded term by term using the previously learned concepts. Experiments have shown that TCL and the combination with pseudo relevance feedback result in notable improvements in the retrieval effectiveness if measured the recall/precision in comparison to the standard vector space model and to the pseudo relevance feedback. This approach can be used to improve the retrieval of documents in Digital Libraries, in Document Management Systems, in the WWW etc.

#index 458701
#* Case Exchange Strategies in Multiagent Learning
#@ Santiago Ontañón;Enric Plaza
#t 2002
#c 20
#% 378963
#% 443616
#% 458616
#% 458644
#% 490605
#% 492375
#% 1275276
#! Multiagent systems offer a new paradigm to organize AI applications. We focus on the application of Case-Based Reasoning to Multiagent systems. CBR offers the individual agents the capability of autonomously learn from experience. In this paper we present a framework for collaboration among agents that use CBR. We present explicit strategies for case retain where the agents take in consideration that they are not learning in isolation but in a multiagent system. We also present case bartering as an effective strategy when the agents have a biased view of the data. The outcome of both case retain and bartering is an improvement of individual agent performance and overall multiagent system performance. We also present empirical results comparing all the strategies proposed.

#index 458702
#* Towards a Simple Clustering Criterion Based on Minimum Length Encoding
#@ Marcus-Christopher Ludl;Gerhard Widmer
#t 2002
#c 20
#% 280407
#% 280463
#% 466083
#% 478303
#% 635713
#% 640534
#! We propose a simple and intuitive clustering evaluation criterion based on the minimum description length principle which yields a particularly simple way of describing and encoding a set of examples. The basic idea is to view a clustering as a restriction of the attribute domains, given an example's cluster membership. As a special operational case we develop the so-called rectangular uniform message length measure that can be used to evaluate clusterings described as sets of hyper-rectangles. We theoretically prove that this measure punishes cluster boundaries in regions of uniform instance distribution (i.e., unintuitive clusterings), and we experimentally compare a simple clustering algorithm using this measure with the well-known algorithms KMeans and AutoClass.

#index 458703
#* Reliable Classifications with Machine Learning
#@ Matjaz Kukar;Igor Kononenko
#t 2002
#c 20
#% 92135
#% 99400
#% 190581
#% 234979
#% 252011
#% 311027
#% 379342
#% 418022
#% 466258
#% 466398
#% 549438
#% 557130
#% 1273833
#% 1650581
#! In the past decades Machine Learning algorithms have been successfully used in numerous classification problems. While they usually significantly outperform domain experts (in terms of classification accuracy or otherwise), they are mostly not being used in practice. A plausible reason for this is that it is difficult to obtain an unbiased estimation of a single classification's reliability. In the paper we propose a general transductive method for estimation of classification's reliability on single examples that is independent of the applied Machine Learning algorithm. We compare our method with existing approaches and discuss its advantages. We perform extensive testing on 14 domains and 6 Machine Learning algorithms and show that our approach can frequently yield more than 100% improvement in reliability estimation performance.

#index 458704
#* Convergent Gradient Ascent in General-Sum Games
#@ Bikramjit Banerjee;Jing Peng
#t 2002
#c 20
#% 266286
#% 348821
#% 465913
#% 528018
#% 1289261
#! In this work we look at the recent results in policy gradient learning in a general-sum game scenario, in the form of two algorithms, IGA and WoLF-IGA. We address the drawbacks in convergence properties of these algorithms, and propose a more accurate version of WoLF-IGA that is guaranteed to converge to Nash Equilibrium policies in self-play (or against an IGA learner). We also present a control theoretic interpretation of variable learning rate which not only justifies WoLF-IGA, but also shows it to achieve fastest convergence under some constraints. Finally we derive optimal learning rates for fastest convergence in practical simulations.

#index 458705
#* Discriminative Clustering: Optimal Contingency Tables by Learning Metrics
#@ Janne Sinkkonen;Samuel Kaski;Janne Nikkilä
#t 2002
#c 20
#% 304908
#% 311413
#% 329569
#% 450876
#% 1860835
#! The learning metrics principle describes a way to derive metrics to the data space from paired data. Variation of the primary data is assumed relevant only to the extent it causes changes in the auxiliary data. Discriminative clustering finds clusters of primary data that are homogeneous in the auxiliary data. In this paper, discriminative clustering using a mutual information criterion is shown to be asymptotically equivalent to vector quantization in learning metrics. We also present a new, finite-data variant of discriminative clustering and show that it builds contingency tables that detect optimally statistical dependency between the clusters and the auxiliary data. A finite-data algorithm is demonstrated to outperform the older mutual information maximizing variant.

#index 458706
#* Using Hard Classifiers to Estimate Conditional Class Probabilities
#@ Ole Martin Halck
#t 2002
#c 20
#% 269212
#% 280437
#% 458623
#% 722760
#% 1042718
#! In many classification problems, it is desirable to have estimates of conditional class probabilities rather than just "hard" class predictions. Many algorithms specifically designed for this purpose exist; here, we present a way in which hard classification algorithms may be applied to this problem without modification. The main idea is that by stochastically changing the class labels in the training data in a simple way, a classification algorithm may be used for estimating any contour of the conditional class probability function. The method has been tested on a toy problem and a problem with real-world data; both experiments yielded encouraging results.

#index 458707
#* A Robust Boosting Algorithm
#@ Richard Nock;Patrice Lefaucheur
#t 2002
#c 20
#% 697
#% 54204
#% 63820
#% 73372
#% 116179
#% 136350
#% 178511
#% 180945
#% 190581
#% 198701
#% 209021
#% 214236
#% 235377
#% 252009
#% 424997
#% 466094
#% 1499573
#! We describe a new Boosting algorithm which combines the base hypotheses with symmetric functions. Among its properties of practical relevance, the algorithm has significant resistance against noise, and is efficient even in an agnostic learning setting. This last property is ruled out for voting-based Boosting algorithms like ADABOOST. Experiments carried out on thirty domains, most of which readily available, tend to display the reliability of the classifiers built.

#index 458708
#* RIONA: A Classifier Combining Rule Induction and k-NN Method with Automated Selection of Optimal Neighbourhood
#@ Grzegorz Góra;Arkadiusz Wojna
#t 2002
#c 20
#% 140588
#% 209023
#% 376266
#% 500527
#% 501816
#% 700962
#% 735356
#! The article describes a method combining two widely-used empirical approaches: rule induction and instance-based learning. In our algorithm (RIONA) decision is predicted not on the basis of the whole support set of all rules matching a test case, but the support set restricted to a neighbourhood of a test case. The size of the optimal neighbourhood is automatically induced during the learning phase. The empirical study shows the interesting fact that it is enough to consider a small neighbourhood to preserve classification accuracy. The combination of k-NN and a rule-based algorithm results in a significant acceleration of the algorithm using all minimal rules. We study the significance of different components of the presented method and compare its accuracy to well-known methods.

#index 564193
#* Dimensionality Reduction through Sub-space Mapping for Nearest Neighbor Algorithms
#@ Terry R. Payne;Peter Edwards
#t 2000
#c 20
#% 129212
#% 132779
#% 136350
#% 196822
#% 406493
#% 1273659
#! Many learning algorithms make an implicit assumption that all the attributes present in the data are relevant to a learning task. However, several studies have demonstrated that this assumption rarely holds; for many supervised learning algorithms, the inclusion of irrelevant or redundant attributes can result in a degradation in classification accuracy. While a variety of different methods for dimensionality reduction exist, many of these are only appropriate for datasets which contain a small number of attributes (e.g.

#index 565243
#* Human-Agent Interaction and Machine Learning
#@ Michael Kaiser;Volker Klingspor;Holger Friedrich
#t 1997
#c 20

#index 565244
#* Global Data Analysis and the Fragmentation Problem in Decision Tree Induction
#@ Ricardo Vilata;Gunnar Blix;Larry A. Rendell
#t 1997
#c 20

#index 565245
#* Boosting Trees for Cost-Sensitive Classifications
#@ Kai Ming Ting;Zijian Zheng
#t 1998
#c 20

#index 565246
#* Error Analysis of Automatic Speech Recognition Using Principal Direction Divisive Partitioning
#@ David McKoskey;Daniel Boley
#t 2000
#c 20
#% 420083
#% 786510
#! This paper describes an experiment performed using the Principal Direction Divisive Partitioning algorithm (Boley, 1998) in order to extract linguistic word error regularities from several sets of medical dictation data. For each of six physicians, two hundred finished medical dictations aligned with their corresponding automatic speech recognition output were clustered and the results analyzed for linguistic regularities between and within clusters. Sparsity measures indicated a good fit between the algorithm and the input data. Linguistic analysis of the output clusters showed evidence of systematic word recognition error for short words, function words, words with destressed vowels, and phonological confusion errors due to telephony (recording) bandwidth interference. No qualitatively significant distinctions between clusters could be made by examining word errors alone, but the results confirmed several informally held hypotheses and suggested several avenues of further investigation, such as the examination of word error contexts.

#index 565250
#* Boosting Density Function Estimators
#@ Franck Thollard;Marc Sebban;Philippe Ezequel
#t 2002
#c 20
#% 176024
#% 203295
#% 235377
#% 252009
#% 252472
#% 383413
#% 389155
#% 424997
#% 464440
#% 465746
#% 466593
#% 466850
#% 466920
#% 477950
#% 817808
#% 853704
#% 1478814
#! In this paper, we focus on the adaptation of boosting to density function estimation, useful in a number of fields including Natural Language Processing and Computational Biology. Previously, boosting has been used to optimize classification algorithms, improving generalization accuracy by combining many classifiers. The core of the boosting strategy, in the well-known ADABOOST algorithm [4], consists in updating the learning instance distribution, increasing (resp. decreasing) the weight of misclassified (resp. correctly classified) examples by the current classifier. Except in [17, 18], few works have attempted to exploit interesting theoretical properties of boosting (such as margin maximization) independently of a classification task. In this paper, we do not take into account classification errors to optimize a classifier, but rather density estimation errors to optimize an estimator (here a probabilistic automaton) of a given target density. Experimental results are presented showing the interest of our approach.

#index 903373
#* Machine Learning: ECML 2006: 17th European Conference on Machine Learning, Berlin, Germany, September 18-22, 2006, Proceedings (Lecture Notes in Computer Science)
#@ Johannes Fürnkranz;Tobias Scheffer;Myra Spiliopoulou
#t 2006
#c 20

#index 925825
#* Machine Learning: ECML 2005: 16th European Conference on Machine Learning, Porto, Portugal, October 3-7, 2005, Proceedings (Lecture Notes in Computer Science ... / Lecture Notes in Artificial Intelligence)
#@ Joäo Gama;Rui Camacho;Pavel Brazdil;Alípio Jorge;Luís Torgo
#t 2005
#c 20

#index 1100039
#* Proceedings of the 18th European conference on Machine Learning
#@ Joost N. Kok;Jacek Koronacki;Raomon Lopez Mantaras;Stan Matwin;Dunja Mladenič;Andrzej Skowron
#t 2007
#c 20

#index 1100040
#* Learning, Information Extraction and the Web
#@ Tom M. Mitchell
#t 2007
#c 20
#! Significant progress has been made recently in semi-supervised learning algorithms that require less labeled training data by utilizing unlabeled data. Much of this progress has been made in the context of natural language analysis (e.g., semi-supervised learning for named entity recognition and for relation extraction). This talk will overview progress in this area, present some of our own recent research, and explore the possibility that now is the right time to mount a community-wide effort to develop a never-ending natural language learning system.

#index 1100041
#* Putting Things in Order: On the Fundamental Role of Ranking in Classification and Probability Estimation
#@ Peter A. Flach
#t 2007
#c 20
#% 464606
#% 799748
#% 840913
#% 976824
#% 1100094
#% 1705505
#! While a binary classifier aims to distinguish positives from negatives, a ranker orders instances from high to low expectation that the instance is positive. Most classification models in machine learning output some score of `positiveness', and hence can be used as rankers. Conversely, any ranker can be turned into a classifier if we have some instance-independent means of splitting the ranking into positive and negative segments. This could be a fixed score threshold; a point obtained from fixing the slope on the ROC curve; the break-even point between true positive and true negative rates; to mention just a few possibilities.These connections between ranking and classification notwithstanding, there are considerable differences as well. Classification performance on nexamples is measured by accuracy, an O(n) operation; ranking performance, on the other hand, is measured by the area under the ROC curve (AUC), an O(nlogn) operation. The model with the highest AUC does not necessarily dominate all other models, and thus it is possible that another model would achieve a higher accuracy for certain operating conditions, even if its AUC is lower.However, within certain model classes good ranking performance and good classification performance are more closely related than suggested by the previous remarks. For instance, there is evidence that certain classification models, while designed to optimise accuracy, in effect optimise an AUC-based loss function [1]. It has also been known for some time that decision tree yield convex training set ROC curves by construction [2], and thus optimising training set accuracy is likely to lead to good training set AUC. In this talk I will investigate the relation between ranking and classification more closely.I will also consider the connection between ranking and probability estimation. The quality of probability estimates can be measured by, e.g., mean squared error in the probability estimates (the Brier score). However, like accuracy, this is an O(n) operation that doesn't fully take ranking performance into account. I will show how a novel decomposition of the Brier score into calibration loss and refinement loss [3] sheds light on both ranking and probability estimation performance. While previous decompositions are approximate [4], our decomposition is an exact one based on the ROC convex hull. (The connection between the ROC convex hull and calibration was independently noted by [5]). In the case of decision trees, the analysis explains the empirical evidence that probability estimation trees produce well-calibrated probabilities [6].

#index 1100042
#* Mining Queries
#@ Ricardo Baeza-Yates
#t 2007
#c 20
#! User queries in search engines and Websites give valuable information on the interests of people. In addition, clicks after queries relate those interests to actual content. Even queries without clicks or answers imply important missing synonyms or content. In this talk we show several examples on how to use this information to improve the performance of search engines, to recommend better queries, to improve the information scent of the content of a Website and ultimately to capture knowledge, as Web queries are the largest wisdom of crowds in Internet.

#index 1100043
#* Adventures in Personalized Information Access
#@ Barry Smyth
#t 2007
#c 20
#! Access to information plays an increasingly important role in our everyday lives and we have come to rely more and more on a variety of information access services to bring us the right information at the right time. Recently the traditional one-size-fits-all approach, which has informed the development of the majority of today's information access services, from search engines to portals, has been brought in to question as researchers consider the advantages of more personalized services. Such services can respond to the learned needs and preferences of individuals and groups of like-minded users. They provide for a more proactive model of information supply in place of today's reactive models of information search. In this talk we will consider the key challenges that motivate the need for a new generation of personalized information services, as well as the pitfalls that lie in wait. We will focus on a number of different information access scenarios, from e-commerce recommender systems and personalized mobile portals to community-based web search. In each case we will describe how different machine learning and data mining ideas have been harnessed to take advantage of key domain constraints in order to deliver information access interfaces that are capable of adapting to the changing needs and preferences of their users. In addition, we will describe the results of a number of user studies that highlight the potential for such technologies to significantly enhance the user experience and the ability of users to locate relevant information quickly and reliably.

#index 1100044
#* Statistical Debugging Using Latent Topic Models
#@ David Andrzejewski;Anne Mulhern;Ben Liblit;Xiaojin Zhu
#t 2007
#c 20
#% 339045
#% 411099
#% 654447
#% 722904
#% 809099
#% 812535
#% 823217
#% 830070
#% 840516
#% 876083
#% 938687
#% 983781
#% 1000380
#% 1650298
#% 1741972
#! Statistical debugging uses machine learning to model program failures and help identify root causes of bugs. We approach this task using a novel Delta-Latent-Dirichlet-Allocation model. We model execution traces attributed to failed runs of a program as being generated by two types of latent topics: normal usage topics and bug topics. Execution traces attributed to successful runs of the same program, however, are modeled by usage topics only. Joint modeling of both kinds of traces allows us to identify weak bug topics that would otherwise remain undetected. We perform model inference with collapsed Gibbs sampling. In quantitative evaluations on four real programs, our model produces bug topics highly correlated to the true bugs, as measured by the Rand index. Qualitative evaluation by domain experts suggests that our model outperforms existing statistical methods for bug cause identification, and may help support other software tasks not addressed by earlier models.

#index 1100045
#* Learning Balls of Strings with Correction Queries
#@ Leonor Becerra Bonache;Colin Higuera;Jean-Christophe Janodet;Frédéric Tantini
#t 2007
#c 20
#% 31215
#% 145156
#% 288885
#% 290391
#% 425028
#% 450951
#% 451056
#% 940343
#% 1728740
#! During the 80's, Angluin introduced an active learning paradigm, using an Oracle, capable of answering both membership and equivalence queries. However, practical evidence tends to show that if the former are often available, this is usually not the case of the latter. We propose new queries, called correction queries, which we study in the framework of Grammatical Inference. When a string is submitted to the Oracle, either she validates it if it belongs to the target language, or she proposes a correction, i.e., a string of the language close to the query with respect to the edit distance. We also introduce a non-standard class of languages: The topological balls of strings. We show that this class is not learnable in Angluin's Matmodel, but is with a linear number of correction queries. We conduct several experiments with an Oracle simulating a human Expert, and show that our algorithm is resistant to approximate answers.

#index 1100046
#* Neighborhood-Based Local Sensitivity
#@ Paul N. Bennett
#t 2007
#c 20
#% 132938
#% 318412
#% 465912
#% 466725
#% 642988
#% 722805
#% 758558
#% 763708
#% 770757
#% 786633
#% 788411
#% 876011
#% 881511
#% 916867
#! We introduce a nonparametric model for sensitivity estimation which relies on generating points similar to the prediction point using its knearest neighbors. Unlike most previous work, the sampled points differ simultaneously in multiple dimensions from the prediction point in a manner dependent on the local density. Our approach is based on an intuitive idea of locality which uses the Voronoi cell around the prediction point, i.e.all points whose nearest neighbor is the prediction point. We demonstrate how an implicit density over this neighborhood can be used in order to compute relative estimates of the local sensitivity. The resulting estimates demonstrate improved performance when used in classifier combination and classifier recalibration as well as being potentially useful in active learning and a variety of other problems.

#index 1100047
#* Approximating Gaussian Processes with ${\cal H}^2$-Matrices
#@ Steffen Börm;Jochen Garcke
#t 2007
#c 20
#% 238372
#% 448145
#% 731257
#% 808299
#% 833200
#% 891549
#% 916792
#! To compute the exact solution of Gaussian process regression one needs $\mathcal{O}(N^3)$ computations for direct and $\mathcal{O}(N^2)$ for iterative methods since it involves a densely populated kernel matrix of size N×N, here Ndenotes the number of data. This makes large scale learning problems intractable by standard techniques.We propose to use an alternative approach: the kernel matrix is replaced by a data-sparse approximation, called an ${\mathcal H}^2$-matrix. This matrix can be represented by only ${\cal O}(N m)$ units of storage, where mis a parameter controlling the accuracy of the approximation, while the computation of the ${\mathcal H}^2$-matrix scales with ${\cal O}(N m \log N)$.Practical experiments demonstrate that our scheme leads to significant reductions in storage requirements and computing times for large data sets in lower dimensional spaces.

#index 1100048
#* Learning Metrics Between Tree Structured Data: Application to Image Recognition
#@ Laurent Boyer;Amaury Habrard;Marc Sebban
#t 2007
#c 20
#% 66654
#% 251405
#% 288885
#% 345848
#% 428926
#% 547947
#% 770782
#% 826007
#% 844387
#% 940343
#% 1274861
#% 1387601
#% 1665128
#! The problem of learning metrics between structured data (strings, trees or graphs) has been the subject of various recent papers. With regard to the specific case of trees, some approaches focused on the learning of edit probabilities required to compute a so-called stochastic tree edit distance. However, to reduce the algorithmic and learning constraints, the deletion and insertion operations are achieved on entire subtrees rather than on single nodes. We aim in this article at filling the gap with the learning of a more general stochastic tree edit distance where node deletions and insertions are allowed. Our approach is based on an adaptation of the EM optimization algorithm to learn parameters of a tree model. We propose an original experimental approach aiming at representing images by a tree-structured representation and then at using our learned metric in an image recognition task. Comparisons with a non learned tree edit distance confirm the effectiveness of our approach.

#index 1100049
#* Shrinkage Estimator for Bayesian Network Parameters
#@ John Burge;Terran Lane
#t 2007
#c 20
#% 6199
#% 197387
#% 212021
#% 246832
#% 466078
#% 577225
#% 770761
#% 840847
#% 892526
#% 1045392
#! Maximum likelihood estimates (MLEs) are commonly used to parameterize Bayesian networks. Unfortunately, these estimates frequently have unacceptably high variance and often overfit the training data. Laplacian correction can be used to smooth the MLEs towards a uniform distribution. However, the uniform distribution may represent an unrealistic relationships in the domain being modeled and can add an unreasonable bias. We present a shrinkage estimator for domains with hierarchically related random variables that smoothes MLEs towards other distributions found in the training data. Our methods are quick enough to be performed during Bayesian network structure searches. On both a simulated and a real-world neuroimaging domain, we empirically demonstrate that our estimator yields superior parameters in the presence of noise and greater likelihoods on left-out data.

#index 1100050
#* Level Learning Set: A Novel Classifier Based on Active Contour Models
#@ Xiongcai Cai;Arcot Sowmya
#t 2007
#c 20
#% 96083
#% 116149
#% 127154
#% 136350
#% 181247
#% 196761
#% 243772
#% 244435
#% 592169
#% 625199
#% 865320
#% 926881
#% 1663159
#% 1685131
#% 1855147
#! This paper presents a novel machine learning algorithm for pattern classification based on image segmentation and optimisation techniques employed in active contour models and level set methods. The proposed classifier, named level learning set(LLS), has the ability to classify general datasets including sparse and non sparse data. It moves developments in vision segmentation into general machine learning by utilising and extending level set-based active contour models from the field of computer vision to construct decision boundaries in any feature space. This model has advantages over traditional classifiers in its ability to directly construct complex decision boundaries, and in better knowledge representation. Various experimental results including comparisons to existing machine learning algorithms are presented, and the advantages of the proposed approach are discussed.

#index 1100051
#* Learning Partially Observable Markov Models from First Passage Times
#@ Jérôme Callut;Pierre Dupont
#t 2007
#c 20
#% 137711
#% 200195
#% 531459
#% 743668
#% 1664635
#% 1699618
#! We propose a novel approach to learn the structure of Partially Observable Markov Models (POMMs) and to estimate jointly their parameters. POMMs are graphical models equivalent to Hidden Markov Models (HMMs). The model structure is built to support the First Passage Times (FPT) dynamics observed in the training sample. We argue that the FPT in POMMs are closely related to the model structure. Starting from a standard Markov chain, states are iteratively added to the model. A novel algorithm POMMPHit is proposed to estimate the POMM transition probabilities to fit the sample FPT dynamics. The transitions with the lowest expected passage times are trimmed off from the model. Practical evaluations on artificially generated data and on DNA sequence modeling show the benefits over Bayesian model induction or EM estimation of ergodic models with transition trimming.

#index 1100052
#* Context Sensitive Paraphrasing with a Global Unsupervised Classifier
#@ Michael Connor;Dan Roth
#t 2007
#c 20
#% 278102
#% 741080
#% 741891
#% 747738
#% 748457
#% 815799
#% 816156
#% 939514
#% 939557
#% 940048
#% 1289663
#! Lexical paraphrasing is an inherently context sensitive problem because a word's meaning depends on context. Most paraphrasing work finds patterns and templates that can replace other patterns or templates in somecontext, but we are attempting to make decisions for a specificcontext. In this paper we develop a global classifier that takes a word vand its context, along with a candidate word u, and determines whether ucan replace vin the given context while maintaining the original meaning.We develop an unsupervised, bootstrapped, learning approach to this problem. Key to our approach is the use of a very large amount of unlabeled data to derive a reliable supervision signal that is then used to train a supervised learning algorithm. We demonstrate that our approach performs significantly better than state-of-the-art paraphrasing approaches, and generalizes well to unseen pairs of words.

#index 1100053
#* Dual Strategy Active Learning
#@ Pinar Donmez;Jaime G. Carbonell;Paul N. Bennett
#t 2007
#c 20
#% 116165
#% 169717
#% 236729
#% 252694
#% 331916
#% 416988
#% 464268
#% 466419
#% 466887
#% 565531
#% 770771
#% 770807
#% 1272282
#% 1274885
#% 1279286
#% 1289273
#% 1387560
#! Active Learning methods rely on static strategies for sampling unlabeled point(s). These strategies range from uncertainty sampling and density estimation to multi-factor methods with learn-once-use-always model parameters. This paper proposes a dynamic approach, called DUAL, where the strategy selection parameters are adaptively updated based on estimated future residual error reduction after each actively sampled point. The objective of dual is to outperform static strategies over a large operating range: from very few to very many labeled points. Empirical results over six datasets demonstrate that DUAL outperforms several state-of-the-art methods on most datasets.

#index 1100054
#* Decision Tree Instability and Active Learning
#@ Kenneth Dwyer;Robert Holte
#t 2007
#c 20
#% 170649
#% 197057
#% 209021
#% 464268
#% 465755
#% 466095
#% 466760
#% 735358
#% 787639
#% 961134
#% 1272280
#% 1656599
#! Decision tree learning algorithms produce accurate models that can be interpreted by domain experts. However, these algorithms are known to be unstable --- they can produce drastically different hypotheses from training sets that differ just slightly. This instability undermines the objective of extracting knowledge from the trees. In this paper, we study the instability of the C4.5 decision tree learner in the context of active learning. We introduce a new measure of decision tree stability, and define three aspects of active learning stability. Several existing active learning methods that use C4.5 as a component are compared empirically; it is determined that query-by-bagging yields trees that are more stable and accurate than those produced by competing methods. Also, an alternative splitting criterion, DKM, is found to improve the stability and accuracy of C4.5 in the active learning setting.

#index 1100055
#* Constraint Selection by Committee: An Ensemble Approach to Identifying Informative Constraints for Semi-supervised Clustering
#@ Derek Greene;Pádraig Cunningham
#t 2007
#c 20
#% 32926
#% 116165
#% 551737
#% 722902
#% 769881
#% 770807
#% 833913
#! A number of clustering algorithms have been proposed for use in tasks where a limited degree of supervision is available. This prior knowledge is frequently provided in the form of pairwise must-link and cannot-link constraints. While the incorporation of pairwise supervision has the potential to improve clustering accuracy, the composition and cardinality of the constraint sets can significantly impact upon the level of improvement. We demonstrate that it is often possible to correctly "guess" a large number of constraints without supervision from the co-associations between pairs of objects in an ensemble of clusterings. Along the same lines, we establish that constraints based on pairs with uncertain co-associations are particularly informative, if known. An evaluation on text data shows that this provides an effective criterion for identifying constraints, leading to a reduction in the level of supervision required to direct a clustering algorithm to an accurate solution.

#index 1100056
#* The Cost of Learning Directed Cuts
#@ Thomas Gärtner;Gemma C. Garriga
#t 2007
#c 20
#% 266787
#% 458245
#% 593975
#% 770851
#% 885621
#% 1013923
#% 1221417
#! In this paper we investigate the problem of classifying vertices of a directed graph according to an unknown directed cut. We first consider the usual setting in which the directed cut is fixed. However, even in this setting learning is not possible without in the worst case needing the labels for the whole vertex set. By considering the size of the minimum path cover as a fixed parameter, we derive positive learnability results with tight performance guarantees for active, online, as well as PAC learning. The advantage of this parameter over possible alternatives is that it allows for an a priori estimation of the total cost of labelling all vertices. The main result of this paper is the analysis of learning directed cuts that depend on a hidden and changing context.

#index 1100057
#* Spectral Clustering and Embedding with Hidden Markov Models
#@ Tony Jebara;Yingbo Song;Kapil Thadani
#t 2007
#c 20
#% 313959
#% 771841
#% 844345
#% 871628
#% 1562549
#! Clustering has recently enjoyed progress via spectral methods which group data using only pairwise affinities and avoid parametric assumptions. While spectral clustering of vector inputs is straightforward, extensions to structured data or time-series data remain less explored. This paper proposes a clustering method for time-series data that couples non-parametric spectral clustering with parametric hidden Markov models (HMMs). HMMs add some beneficial structural and parametric assumptions such as Markov properties and hidden state variables which are useful for clustering. This article shows that using probabilistic pairwise kernel estimates between parametric models provides improved experimental results for unsupervised clustering and visualization of real and synthetic datasets. Results are compared with a fully parametric baseline method (a mixture of hidden Markov models) and a non-parametric baseline method (spectral clustering with non-parametric time-series kernels).

#index 1100058
#* Probabilistic Explanation Based Learning
#@ Angelika Kimmig;Luc Raedt;Hannu Toivonen
#t 2007
#c 20
#% 46273
#% 90104
#% 129972
#% 147677
#% 449587
#% 576214
#% 731606
#% 1026554
#% 1272388
#% 1274543
#% 1275150
#% 1665133
#% 1692830
#! Explanation based learning produces generalized explanations from examples. These explanations are typically built in a deductive manner and they aim to capture the essential characteristics of the examples.Probabilistic explanation based learning extends this idea to probabilistic logic representations, which have recently become popular within the field of statistical relational learning. The task is now to find the most likely explanation why one (or more) example(s) satisfy a given concept. These probabilistic and generalized explanations can then be used to discover similarexamples and to reason by analogy. So, whereas traditional explanation based learning is typically used for speed-up learning, probabilistic explanation based learning is used for discovering new knowledge.Probabilistic explanation based learning has been implemented in a recently proposed probabilistic logic called ProbLog, and it has been applied to a challenging application in discovering relationships of interest in large biological networks.

#index 1100059
#* Graph-Based Domain Mapping for Transfer Learning in General Games
#@ Gregory Kuhlmann;Peter Stone
#t 2007
#c 20
#% 116297
#% 217007
#% 333786
#% 348578
#% 384911
#% 550393
#% 875951
#% 878074
#% 1024713
#% 1250387
#% 1274860
#! A general game player is an agent capable of taking as input a description of a game's rules in a formal language and proceeding to play without any subsequent human input. To do well, an agent should learn from experience with past games and transfer the learned knowledge to new problems. We introduce a graph-based method for identifying previously encountered games and prove its robustness formally. We then describe how the same basic approach can be used to identify similar but non-identical games. We apply this technique to automate domain mapping for value function transfer and speed up reinforcement learning on variants of previously played games. Our approach is fully implemented with empirical results in the general game playing system.

#index 1100060
#* Learning to Classify Documents with Only a Small Positive Training Set
#@ Xiao-Li Li;Bing Liu;See-Kiong Ng
#t 2007
#c 20
#% 197922
#% 311027
#% 406493
#% 464641
#% 550254
#% 564957
#% 577235
#% 727829
#% 729939
#% 843873
#% 855602
#% 1279298
#! Many real-world classification applications fall into the class of positive and unlabeled (PU) learning problems. In many such applications, not only could the negative training examples be missing, the number of positive examples available for learning may also be fairly limited due to the impracticality of hand-labeling a large number of training examples. Current PU learning techniques have focused mostly on identifying reliable negative instances from the unlabeled set U. In this paper, we address the oft-overlooked PU learning problem when the number of training examples in the positive set Pis small. We propose a novel technique LPLP (Learning from Probabilistically Labeled Positive examples) and apply the approach to classify product pages from commercial websites. The experimental results demonstrate that our approach outperforms existing methods significantly, even in the challenging cases where the positive examples in Pand the hidden positive examples in Uwere not drawn from the same distribution.

#index 1100061
#* Structure Learning of Probabilistic Relational Models from Incomplete Relational Data
#@ Xiao-Lin Li;Zhi-Hua Zhou
#t 2007
#c 20
#% 44876
#% 246831
#% 392781
#% 400980
#% 577225
#% 676365
#% 722914
#% 771945
#% 1273915
#% 1275150
#% 1289267
#% 1650579
#! Existing relational learning approaches usually work on complete relational data, but real-world data are often incomplete. This paper proposes the MGDA approach to learn structures of probabilistic relational model (PRM) from incomplete relational data. The missing values are filled in randomly at first, and a maximum likelihood tree (MLT) is generated from the complete data sample. Then, Gibbs sampling is combined with MLT to modify the data and regulate MLT iteratively for obtaining a well-completed data set. Finally, probabilistic structure is learned through dependency analysis from the completed data set. Experiments show that the MGDA approach can learn good structures from incomplete relational data.

#index 1100062
#* Stability Based Sparse LSI/PCA: Incorporating Feature Selection in LSI and PCA
#@ Dimitrios Mavroeidis;Michalis Vazirgiannis
#t 2007
#c 20
#% 248027
#% 304931
#% 430757
#% 579655
#% 722805
#% 763861
#% 856734
#% 1674762
#! The stability of sample based algorithms is a concept commonly used for parameter tuning and validity assessment. In this paper we focus on two well studied algorithms, LSI and PCA, and propose a feature selection process that provably guarantees the stability of their outputs. The feature selection process is performed such that the level of (statistical) accuracy of the LSI/PCA input matrices is adequate for computing meaningful (stable) eigenvectors. The feature selection process "sparsifies" LSI/PCA, resulting in the projection of the instances on the eigenvectors of a principal submatrix of the original input matrix, thus producing sparse factor loadings that are linear combinations solely of the selected features. We utilize bootstrapping confidence intervals for assessing the statistical accuracy of the input sample matrices, and matrix perturbation theory in order to relate the statistical accuracy to the stability of eigenvectors. Experiments on several UCI-datasets verify empirically our approach.

#index 1100063
#* Bayesian Substructure Learning - Approximate Learning of Very Large Network Structures
#@ Andreas Nägele;Mathäus Dejori;Martin Stetter
#t 2007
#c 20
#% 197387
#% 246835
#% 297490
#% 527830
#% 722754
#% 729990
#% 770828
#% 893460
#% 1269475
#% 1650289
#% 1650638
#! In recent years, Bayesian networks became a popular framework to estimate the dependency structure of a set of variables. However, due to the NP-hardness of structure learning, this is a challenging task and typical state-of-the art algorithms fail to learn in domains with several thousands of variables. In this paper we introduce a novel algorithm, called substructure learning, that reduces the complexity of learning large networks by splitting this task into several small subtasks. Instead of learning one complete network, we estimate the network structure iteratively by learning small subnetworks. Results from several benchmark cases show that substructure learning efficiently reconstructs the network structure in large domains with high accuracy.

#index 1100064
#* Efficient Continuous-Time Reinforcement Learning with Adaptive State Graphs
#@ Gerhard Neumann;Michael Pfeiffer;Wolfgang Maass
#t 2007
#c 20
#% 160859
#% 384911
#% 425072
#% 528299
#% 876049
#% 891549
#! We present a new reinforcement learning approach for deterministic continuous control problems in environments with unknown, arbitrary reward functions. The difficulty of finding solution trajectories for such problems can be reduced by incorporating limited prior knowledge of the approximative local system dynamics. The presented algorithm builds an adaptive state graph of sample points within the continuous state space. The nodes of the graph are generated by an efficient principled exploration scheme that directs the agent towards promising regions, while maintaining good online performance. Global solution trajectories are formed as combinations of local controllers that connect nodes of the graph, thereby naturally allowing continuous actions and continuous time steps. We demonstrate our approach on various movement planning tasks in continuous domains.

#index 1100065
#* Source Separation with Gaussian Process Models
#@ Sunho Park;Seungjin Choi
#t 2007
#c 20
#% 190861
#% 236648
#% 266876
#% 450245
#% 645407
#% 891549
#% 916792
#% 1757421
#% 1762953
#! In this paper we address a method of source separation in the case where sources have certain temporal structures. The key contribution in this paper is to incorporate Gaussian process (GP) model into source separation, representing the latent function which characterizes the temporal structure of a source by a random process with Gaussian prior. Marginalizing out the latent function leads to the Gaussian marginal likelihood of source that is plugged in the mutual information-based loss function for source separation. In addition, we also consider the leave-one-out predictive distribution of source, instead of the marginal likelihood, in the same framework. Gradient-based optimization is applied to estimate the demixing matrix through the mutual information minimization, where the marginal distribution of source is replaced by the marginal likelihood of the source or its leave-one-out predictive distribution. Numerical experiments confirm the useful behavior of our method, compared to existing source separation methods.

#index 1100066
#* Discriminative Sequence Labeling by Z-Score Optimization
#@ Elisa Ricci;Tijl Bie;Nello Cristianini
#t 2007
#c 20
#% 464434
#% 466892
#% 770763
#% 854636
#! We consider a new discriminative learning approach to sequence labeling based on the statistical concept of the Z-score. Given a training set of pairs of hidden-observed sequences, the task is to determine some parameter values such that the hidden labels can be correctly reconstructed from observations. Maximizing the Z-score appears to be a very good criterion to solve this problem both theoretically and empirically. We show that the Z-score is a convex function of the parameters and it can be efficiently computed with dynamic programming methods. In addition to that, the maximization step turns out to be solvable by a simple linear system of equations. Experiments on artificial and real data demonstrate that our approach is very competitive both in terms of speed and accuracy with respect to previous algorithms.

#index 1100067
#* Fast Optimization Methods for L1 Regularization: A Comparative Study and Two New Approaches
#@ Mark Schmidt;Glenn Fung;Rómer Rosales
#t 2007
#c 20
#% 206117
#% 274586
#% 378173
#% 721164
#% 722929
#% 722937
#% 722943
#% 724344
#% 770857
#% 961223
#% 1250570
#! L1 regularization is effective for feature selection, but the resulting optimization is challenging due to the non-differentiability of the 1-norm. In this paper we compare state-of-the-art optimization techniques to solve this problem across several loss functions. Furthermore, we propose two new techniques. The first is based on a smooth (differentiable) convex approximation for the L1 regularizer that does not depend on any assumptions about the loss function used. The other technique is a new strategy that addresses the non-differentiability of the L1-regularizer by casting the problem as a constrained optimization problem that is then solved using a specialized gradient projection method. Extensive comparisons show that our newly proposed approaches consistently rank among the best in terms of convergence speed and efficiency by measuring the number of function evaluations required.

#index 1100068
#* Bayesian Inference for Sparse Generalized Linear Models
#@ Matthias Seeger;Sebastian Gerwinn;Matthias Bethge
#t 2007
#c 20
#% 286584
#% 528330
#% 722760
#% 770840
#% 857429
#! We present a framework for efficient, accurate approximate Bayesian inference in generalized linear models (GLMs), based on the expectation propagation (EP) technique. The parameters can be endowed with a factorizing prior distribution, encoding properties such as sparsity or non-negativity. The central role of posterior log-concavity in Bayesian GLMs is emphasized and related to stability issues in EP. In particular, we use our technique to infer the parameters of a point process model for neuronal spiking data from multiple electrodes, demonstrating significantly superior predictive performance when a sparsity assumption is enforced via a Laplace prior distribution.

#index 1100069
#* Classifier Loss Under Metric Uncertainty
#@ David B. Skalak;Alexandru Niculescu-Mizil;Rich Caruana
#t 2007
#c 20
#% 376266
#% 565245
#% 723244
#% 769882
#% 770822
#% 840913
#% 875965
#% 939921
#! Classifiers that are deployed in the field can be used and evaluated in ways that were not anticipated when the model was trained. The final evaluation metric may not have been known at training time, additional performance criteria may have been added, the evaluation metric may have changed over time, or the real-world evaluation procedure may have been impossible to simulate. Unforeseen ways of measuring model utility can degrade performance. Our objective is to provide experimental support for modelers who face potential "cross-metric" performance deterioration. First, to identify model-selection metrics that lead to stronger cross-metric performance, we characterize the expected loss where the selection metric is held fixed and the evaluation metric is varied. Second, we show that the number of data points evaluated by a selection metric has substantial impact on the optimal evaluation. While addressing these issues, we consider the effect of calibrating the classifiers to output probabilities influences. Our experiments show that if models are well calibrated, cross-entropy is the highest-performing selection metric if little data is available for model selection. With these experiments, modelers may be in a better position to choose selection metrics that are robust where it is uncertain what evaluation metric will be applied.

#index 1100070
#* Additive Groves of Regression Trees
#@ Daria Sorokina;Rich Caruana;Mirek Riedewald
#t 2007
#c 20
#% 209021
#% 425066
#% 448194
#% 458360
#% 769939
#! We present a new regression algorithm called Groves of trees and show empirically that it is superior in performance to a number of other established regression methods. A Grove is an additive model usually containing a small number of large trees. Trees added to the Grove are trained on the residual error of other trees already in the Grove. We begin the training process with a single small tree in the Grove and gradually increase both the number of trees in the Grove and their size. This procedure ensures that the resulting model captures the additive structure of the response. A single Grove may still overfit to the training set, so we further decrease the variance of the final predictions with bagging. We show that in addition to exhibiting superior performance on a suite of regression test problems, bagged Groves of trees are very resistant to overfitting.

#index 1100071
#* Efficient Computation of Recursive Principal Component Analysis for Structured Input
#@ Alessandro Sperduti
#t 2007
#c 20
#% 90391
#% 354202
#% 723248
#% 731607
#% 1404431
#% 1694720
#% 1860191
#% 1860363
#! Recently, a successful extension of Principal Component Analysis for structured input, such as sequences, trees, and graphs, has been proposed. This allows the embedding of discrete structures into vectorial spaces, where all the classical pattern recognition and machine learning methods can be applied. The proposed approach is based on eigenanalysis of extended vectorial representations of the input structures and substructures. One problem with the approach is that eigenanalysis can be computationally quite demanding when considering large datasets of structured objects. In this paper we propose a general approach for reducing the computational burden. Experimental results show a significant speed-up of the computation.

#index 1100072
#* Hinge Rank Loss and the Area Under the ROC Curve
#@ Harald Steck
#t 2007
#c 20
#% 197394
#% 266426
#% 349550
#% 464606
#% 734915
#% 769882
#% 770788
#% 829008
#% 840882
#% 1705503
#% 1705505
#! In ranking as well as in classification problems, the Area under the ROC Curve (AUC), or the equivalent Wilcoxon-Mann-Whitney statistic, has recently attracted a lot of attention. We show that the AUC can be lower bounded based on the hinge-rank-loss, which simply is the rank-version of the standard (parametric) hinge loss. This bound is asymptotically tight. Our experiments indicate that optimizing the (standard) hinge loss typically is an accurate approximation to optimizing the hinge rank loss, especially when using affine transformations of the data, like e.g. in ellipsoidal machines. This explains for the first time why standard training of support vector machines approximately maximizes the AUC, which has indeed been observed in many experiments in the literature.

#index 1100073
#* Clustering Trees with Instance Level Constraints
#@ Jan Struyf;Sašo Džeroski
#t 2007
#c 20
#% 136350
#% 296738
#% 464291
#% 464608
#% 466073
#% 466890
#% 741335
#% 769881
#% 770782
#% 1403611
#% 1663626
#% 1673558
#% 1742012
#! Constrained clustering investigates how to incorporate domain knowledge in the clustering process. The domain knowledge takes the form of constraints that must hold on the set of clusters. We consider instance level constraints, such as must-link and cannot-link. This type of constraints has been successfully used in popular clustering algorithms, such as k-means and hierarchical agglomerative clustering. This paper shows how clustering trees can support instance level constraints. Clustering trees are decision trees that partition the instances into homogeneous clusters. Clustering trees provide a symbolic description for each cluster. To handle non-trivial constraint sets, we extend clustering trees to support disjunctive descriptions. The paper's main contribution is ClusILC, an efficient algorithm for building such trees. We present experiments comparing ClusILC to COP-k-means.

#index 1100074
#* On Pairwise Naive Bayes Classifiers
#@ Jan-Nikolas Sulzmann;Johannes Fürnkranz;Eyke Hüllermeier
#t 2007
#c 20
#% 99396
#% 197394
#% 246831
#% 272518
#% 577298
#% 722756
#% 722807
#% 763699
#% 771846
#% 998581
#% 1272365
#% 1862526
#! Class binarizations are effective methods for improving weak learners by decomposing multi-class problems into several two-class problems. This paper analyzes how these methods can be applied to a Naive Bayes learner. The key result is that the pairwise variant of Naive Bayes is equivalent to a regular Naive Bayes. This result holds for several aggregation techniques for combining the predictions of the individual classifiers, including the commonly used voting and weighted voting techniques. On the other hand, Naive Bayes with one-against-all binarization is not equivalent to a regular Naive Bayes. Apart from the theoretical results themselves, the paper offers a discussion of their implications.

#index 1100075
#* Separating Precision and Mean in Dirichlet-Enhanced High-Order Markov Models
#@ Rikiya Takahashi
#t 2007
#c 20
#% 939624
#! Robustly estimating the state-transition probabilities of high-order Markov processes is an essential task in many applications such as natural language modeling or protein sequence modeling. We propose a novel estimation algorithm called Hierarchical Separated Dirichlet Smoothing (HSDS), where Dirichlet distributions are hierarchically assumed to be the prior distributions of the state-transition probabilities. The key idea in HSDS is to separatethe parameters of a Dirichlet distribution into the precision and mean, so that the precision depends on the context while the mean is given by the lower-order distribution. HSDS is designed to outperform Kneser-Ney smoothing especially when the number of states is small, where Kneser-Ney smoothing is currently known as the state-of-the-art technique for N-gram natural language models. Our experiments in protein sequence modeling showed the superiority of HSDS both in perplexity evaluation and classification tasks.

#index 1100076
#* Safe Q-Learning on Complete History Spaces
#@ Stephan Timmer;Martin Riedmiller
#t 2007
#c 20
#% 158924
#% 646971
#% 875996
#! In this article, we present an idea for solving deterministic partially observable markov decision processes (POMDPs) based on a history space containing sequences of past observations and actions. A novel and sound technique for learning a Q-function on history spaces is developed and discussed. We analyze certain conditions under which a history based approach is able to learn policies comparable to the optimal solution on belief states. The algorithm presented is model-free and can be combined with any method learning history spaces. We also present a procedure able to learn history spaces especially suited for our Q-learning algorithm.

#index 1100077
#* Random k-Labelsets: An Ensemble Method for Multilabel Classification
#@ Grigorios Tsoumakas;Ioannis Vlahavas
#t 2007
#c 20
#% 309208
#% 311034
#% 318412
#% 413637
#% 478470
#% 722924
#% 818236
#% 838412
#% 884074
#% 889101
#% 926881
#% 953979
#% 1223287
#% 1671162
#! This paper proposes an ensemble method for multilabel classification. The RAndom k-labELsets (RAKEL) algorithm constructs each member of the ensemble by considering a small random subset of labels and learning a single-label classifier for the prediction of each element in the powerset of this subset. In this way, the proposed algorithm aims to take into account label correlations using single-label classifiers that are applied on subtasks with manageable number of labels and adequate number of examples per label. Experimental results on common multilabel domains involving protein, document and scene classification show that better performance can be achieved compared to popular multilabel classification approaches.

#index 1100078
#* Seeing the Forest Through the Trees: Learning a Comprehensible Model from an Ensemble
#@ Anneleen Assche;Hendrik Blockeel
#t 2007
#c 20
#% 197057
#% 209021
#% 400847
#% 424997
#% 449588
#% 543945
#% 550732
#% 551723
#% 564956
#% 642073
#% 703975
#% 883329
#% 926881
#% 1393855
#% 1415878
#! Ensemble methods are popular learning methods that usually increase the predictive accuracy of a classifier though at the cost of interpretability and insight in the decision process. In this paper we aim to overcome this issue of comprehensibility by learning a single decision tree that approximates an ensemble of decision trees. The new model is obtained by exploiting the class distributions predicted by the ensemble. These are employed to compute heuristics for deciding which tests are to be used in the new tree. As such we acquire a model that is able to give insight in the decision process, while being more accurate than the single model directly learned on the data. The proposed method is experimentally evaluated on a large number of UCI data sets, and compared to an existing approach that makes use of artificially generated data.

#index 1100079
#* Avoiding Boosting Overfitting by Removing Confusing Samples
#@ Alexander Vezhnevets;Olga Barinova
#t 2007
#c 20
#% 174242
#% 209021
#% 229513
#% 266255
#% 302391
#% 312727
#% 425019
#% 451966
#% 465746
#% 529493
#% 562952
#% 715988
#% 722495
#% 770766
#% 812366
#% 823349
#% 855520
#% 876039
#! Boosting methods are known to exhibit noticeable overfitting on some datasets, while being immune to overfitting on other ones. In this paper we show that standard boosting algorithms are not appropriate in case of overlapping classes. This inadequateness is likely to be the major source of boosting overfitting while working with real world data. To verify our conclusion we use the fact that any overlapping classes' task can be reduced to a deterministic task with the same Bayesian separating surface. This can be done by removing "confusing samples" --- samples that are misclassified by a "perfect" Bayesian classifier. We propose an algorithm for removing confusing samples and experimentally study behavior of AdaBoost trained on the resulting data sets. Experiments confirm that removing confusing samples helps boosting to reduce the generalization error and to avoid overfitting on both synthetic and real world. Process of removing confusing samples also provides an accurate error prediction based on the work with the training sets.

#index 1100080
#* Planning and Learning in Environments with Delayed Feedback
#@ Thomas J. Walsh;Ali Nouri;Lihong Li;Michael L. Littman
#t 2007
#c 20
#% 30037
#% 118668
#% 151769
#% 178906
#% 229940
#% 351419
#% 363744
#% 384911
#% 466069
#% 466402
#% 466428
#% 703709
#% 722895
#! This work considers the problems of planning and learning in environments with constant observation and reward delays. We provide a hardness result for the general planning problem and positive results for several special cases with deterministic or otherwise constrained dynamics. We present an algorithm, Model Based Simulation, for planning in such environments and use model-based reinforcement learning to extend this approach to the learning setting in both finite and continuous environments. Empirical comparisons show this algorithm holds significant advantages over others for decision making in delayed environments.

#index 1100081
#* Analyzing Co-training Style Algorithms
#@ Wei Wang;Zhi-Hua Zhou
#t 2007
#c 20
#% 252011
#% 271060
#% 311027
#% 466263
#% 466888
#% 546823
#% 727818
#% 765552
#% 815908
#% 926881
#% 1289496
#! Co-training is a semi-supervised learning paradigm which trains two learners respectively from two different views and lets the learners label some unlabeled examples for each other. In this paper, we present a new PAC analysis on co-training style algorithms. We show that the co-training process can succeed even without two views, given that the two learners have large difference, which explains the success of some co-training style algorithms that do not require two views. Moreover, we theoretically explain that why the co-training process could not improve the performance further after a number of rounds, and present a rough estimation on the appropriate round to terminate co-training to avoid some wasteful learning rounds.

#index 1100082
#* Policy Gradient Critics
#@ Daan Wierstra;Jürgen Schmidhuber
#t 2007
#c 20
#% 384911
#% 466069
#% 811571
#% 959478
#% 1042867
#% 1650314
#! We present Policy Gradient Actor-Critic (PGAC), a new model-free Reinforcement Learning (RL) method for creating limited-memory stochastic policiesfor Partially Observable Markov Decision Processes (POMDPs) that require long-term memories of past observations and actions. The approach involves estimating a policy gradient for an Actor through a Policy Gradient Critic which evaluates probability distributions on actions. Gradient-based updates of history-conditional action probability distributions enable the algorithm to learn a mapping from memory states (or event histories) to probability distributions on actions, solving POMDPs through a combination of memory and stochasticity. This goes beyond previous approaches to learning purely reactive POMDP policies, without giving up their advantages. Preliminary results on important benchmark tasks show that our approach can in principle be used as a general purpose POMDP algorithm that solves RL problems in both continuous and discrete action domains.

#index 1100083
#* An Improved Model Selection Heuristic for AUC
#@ Shaomin Wu;Peter Flach;Cèsar Ferri
#t 2007
#c 20
#% 331909
#% 466639
#% 580510
#% 796197
#% 889273
#% 1673599
#! The area under the ROC curve (AUC) has been widely used to measure ranking performance for binary classification tasks. AUC only employs the classifier's scores to rank the test instances; thus, it ignores other valuable information conveyed by the scores, such as sensitivity to small differences in the score values However, as such differences are inevitable across samples, ignoring them may lead to overfitting the validation set when selecting models with high AUC. This problem is tackled in this paper. On the basis of ranks as well as scores, we introduce a new metric called scored AUC(sAUC), which is the area under the sROC curve. The latter measures how quickly AUC deteriorates if positive scores are decreased. We study the interpretation and statistical properties of sAUC. Experimental results on UCI data sets convincingly demonstrate the effectiveness of the new metric for classifier evaluation and selection in the case of limited validation data.

#index 1100084
#* Finding the Right Family: Parent and Child Selection for Averaged One-Dependence Estimators
#@ Fei Zheng;Geoffrey I. Webb
#t 2007
#c 20
#% 246832
#% 312728
#% 321059
#% 458168
#% 458259
#% 486328
#% 799040
#% 866537
#% 876084
#% 926881
#% 1665169
#% 1699581
#! Averaged One-Dependence Estimators (AODE) classifies by uniformly aggregating all qualified one-dependence estimators (ODEs). Its capacity to significantly improve naive Bayes' accuracy without undue time complexity has attracted substantial interest. Forward Sequential Selection and Backwards Sequential Elimination are effective wrapper techniques to identify and repair harmful interdependencies which have been profitably applied to naive Bayes. However, their straightforward application to AODE has previously proved ineffective. We investigate novel variants of these strategies. Our extensive experiments show that elimination of child attributes from within the constituent ODEs results in a significant improvement in probability estimate and reductions in bias and error relative to unmodified AODE. In contrast, elimination of complete constituent ODEs and the four types of attribute addition are found to be less effective and do not demonstrate any strong advantage over AODE. These surprising results lead to effective techniques for improving AODE's prediction accuracy.

#index 1100085
#* Stepwise Induction of Multi-target Model Trees
#@ Annalisa Appice;Saso Džeroski
#t 2007
#c 20
#% 465897
#% 466073
#% 744795
#% 894015
#% 1742012
#! Multi-target model trees are trees which predict the values of several target continuous variables simultaneously. Each leaf of such a tree contains several linear models, each predicting the value of a different target variable. We propose an algorithm for inducing such trees in a stepwise fashion. Experiments show that multi-target model trees are much smaller than the corresponding sets of single-target model trees and are induced much faster, while achieving comparable accuracies.

#index 1100086
#* Comparing Rule Measures for Predictive Association Rules
#@ Paulo J. Azevedo;Alípio M. Jorge
#t 2007
#c 20
#% 227917
#% 227919
#% 310505
#% 466483
#% 751575
#% 1718523
#! We study the predictive ability of some association rule measures typically used to assess descriptive interest. Such measures, namely conviction, lift and 茂戮驴2are compared with confidence, Laplace, mutual information, cosine, Jaccard and 茂戮驴-coefficient. As prediction models, we use sets of association rules. Classification is done by selecting the best rule, or by weighted voting. We performed an evaluation on 17 datasets with different characteristics and conclude that conviction is on average the best predictive measure to use in this setting. We also provide some meta-analysis insights for explaining the results.

#index 1100087
#* User Oriented Hierarchical Information Organization and Retrieval
#@ Korinna Bade;Marcel Hermkes;Andreas Nürnberger
#t 2007
#c 20
#% 346444
#% 413609
#% 464291
#% 878454
#% 961603
#% 1390149
#! In order to organize huge document collections, labeled hierarchical structures are used frequently. Users are most efficient in navigating such hierarchies, if they reflect their personal interests. Thus, we propose in this article an approach that is able to derive a personalized hierarchical structure from a document collection. The approach is based on a semi-supervised hierarchical clustering approach, which is combined with a biased cluster extraction process. Furthermore, we label the clusters for efficient navigation. Besides the algorithms itself, we describe an evaluation of our approach using benchmark datasets.

#index 1100088
#* Learning a Classifier with Very Few Examples: Analogy Based and Knowledge Based Generation of New Examples for Character Recognition
#@ S. Bayoudh;H. Mouchère;L. Miclet;E. Anquetil
#t 2007
#c 20
#% 25470
#% 190433
#% 269922
#% 479722
#% 493160
#% 718569
#% 718774
#% 747998
#% 812348
#% 1274861
#! This paper is basically concerned with a practical problem: the on-the-fly quick learning of handwritten character recognition systems. More generally, it explores the problem of generating new learning examples, especially from very scarce (2 to 5 per class) original learning data. It presents two different methods. The first one is based on applying distortions on original characters using knowledgeon handwriting properties like speed, curvature etc. The second one consists in generation based on the notion of analogical dissimilaritywhich quantifies the analogical relation "Ais to Balmost as Cis to D". We give an algorithm to compute the k-least dissimilar objects D, hence generating knew objects from three examples A, Band C. Finally, we experimentally prove the efficiency of both methods, especially when used in conjunction.

#index 1100089
#* Weighted Kernel Regression for Predicting Changing Dependencies
#@ Steven Busuttil;Yuri Kalnishkan
#t 2007
#c 20
#% 393059
#% 466081
#% 788056
#% 871302
#% 999688
#! Consider the online regression problem where the dependence of the outcome yton the signal xtchanges with time. Standard regression techniques, like Ridge Regression, do not perform well in tasks of this type. We propose two methods to handle this problem: WeCKAAR, a simple modification of an existing regression technique, and KAARCh, an application of the Aggregating Algorithm. Empirical results on artificial data show that in this setting, KAARCh is superior to WeCKAAR and standard regression techniques. On options implied volatility data, the performance of both KAARCh and WeCKAAR is comparable to that of the proprietary technique currently being used at the Russian Trading System Stock Exchange (RTSSE).

#index 1100090
#* Counter-Example Generation-Based One-Class Classification
#@ András Bánhalmi;András Kocsor;Róbert Busa-Fekete
#t 2007
#c 20
#% 302406
#% 732387
#% 803575
#% 818170
#% 855602
#% 926881
#! For One-Class Classification problems several methods have been proposed in the literature. These methods all have the common feature that the decision boundary is learnt by just using a set of the positive examples. Here we propose a method that extends the training set with a counter-example set, which is generated directly using the set of positive examples. Using the extended training set, a binary classifier (here 茂戮驴-SVM) is applied to separate the positive and the negative points. The results of this novel technique are compared with those of One-Class SVM and the Gaussian Mixture Model on several One-Class Classification tasks.

#index 1100091
#* Test-Cost Sensitive Classification Based on Conditioned Loss Functions
#@ Mumin Cebe;Cigdem Gunduz-Demir
#t 2007
#c 20
#% 92554
#% 160852
#% 447606
#% 464639
#% 729437
#% 863397
#% 875224
#% 942925
#% 1272369
#% 1273393
#% 1665179
#% 1781693
#! We report a novel approach for designing test-cost sensitive classifiers that consider the misclassification cost together with the cost of feature extraction utilizing the consistency behavior for the first time. In this approach, we propose to use a new Bayesian decision theoretical framework in which the loss is conditioned with the current decision and the expected decisions after additional features are extracted as well as the consistency among the current and expected decisions. This approach allows us to force the feature extraction for samples for which the current and expected decisions are inconsistent. On the other hand, it forces not to extract any features in the case of consistency, leading to less costly but equally accurate decisions. In this work, we apply this approach to a medical diagnosis problem and demonstrate that it reduces the overall feature extraction cost up to 47.61 percent without decreasing the accuracy.

#index 1100092
#* Probabilistic Models for Action-Based Chinese Dependency Parsing
#@ Xiangyu Duan;Jun Zhao;Bo Xu
#t 2007
#c 20
#% 708948
#% 808936
#% 827511
#% 854811
#% 939343
#% 939554
#% 939658
#% 1299529
#! Action-based dependency parsing, also known as deterministic dependency parsing, has often been regarded as a time efficient parsing algorithm while its parsing accuracy is a little lower than the best results reported by more complex parsing models. In this paper, we compare action-based dependency parsers with complex parsing methods such as all-pairs parsers on Penn Chinese Treebank. For Chinese dependency parsing, action-based parsers outperform all-pairs parsers. But action-based parsers do not compute the probability of the whole dependency tree. They only determine parsing actions stepwisely by a trained classifier. To globally model parsing actions of all steps that are taken on the input sentence, we propose two kinds of probabilistic parsing action models that can compute the probability of the whole dependency tree. Results show that our probabilistic parsing action models perform better than the original action-based parsers, and our best result improves much over them.

#index 1100093
#* Learning Directed Probabilistic Logical Models: Ordering-Search Versus Structure-Search
#@ Daan Fierens;Jan Ramon;Maurice Bruynooghe;Hendrik Blockeel
#t 2007
#c 20
#% 197387
#% 550745
#% 850430
#% 1718455
#! We discuss how to learn non-recursive directed probabilistic logical models from relational data. This problem has been tackled before by upgrading the structure-search algorithm initially proposed for Bayesian networks. In this paper we propose to upgrade another algorithm, namely ordering-search, since for Bayesian networks this was found to work better than structure-search. We experimentally compare the two upgraded algorithms on two relational domains. We conclude that there is no significant difference between the two algorithms in terms of quality of the learnt models while ordering-search is significantly faster.

#index 1100094
#* A Simple Lexicographic Ranker and Probability Estimator
#@ Peter Flach;Edson Takashi Matsubara
#t 2007
#c 20
#% 331909
#% 464280
#% 464606
#% 580510
#% 799748
#% 961134
#% 976824
#! Given a binary classification task, a ranker sorts a set of instances from highest to lowest expectation that the instance is positive. We propose a lexicographic ranker, LexRank, whose rankings are derived not from scores, but from a simple ranking of attribute values obtained from the training data. When using the odds ratio to rank the attribute values we obtain a restricted version of the naive Bayes ranker. We systematically develop the relationships and differences between classification, ranking, and probability estimation, which leads to a novel connection between the Brier score and ROC curves. Combining LexRankwith isotonic regression, which derives probability estimates from the ROC convex hull, results in the lexicographic probability estimator LexProb. Both LexRankand LexProbare empirically evaluated on a range of data sets, and shown to be highly effective.

#index 1100095
#* On Minimizing the Position Error in Label Ranking
#@ Eyke Hüllermeier;Johannes Fürnkranz
#t 2007
#c 20
#% 543918
#% 722807
#% 771846
#% 1708354
#! Conventional classification learning allows a classifier to make a one shot decision in order to identify the correct label. However, in many practical applications, the problem is not to give a single estimation, but to make repeated suggestions until the correct target label has been identified. Thus, the learner has to deliver a label ranking, that is, a ranking of all possible alternatives. In this paper, we discuss a loss function, called the position error, which is suitable for evaluating the performance of a label ranking algorithm in this setting. Moreover, we introduce "ranking through iterated choice", a general strategy for extending any multi-class classifier to this scenario, and propose an efficient implementation of this method by means of pairwise decomposition techniques.

#index 1100096
#* On Phase Transitions in Learning Sparse Networks
#@ Goele Hollanders;Geert Jan Bex;Marc Gyssens;Ronald L. Westra;Karl Tuyls
#t 2007
#c 20
#% 376266
#% 1390052
#% 1815156
#! In this paper we study the identification of sparse interaction networks as a machine learning problem. Sparsity mean that we are provided with a small data set and a high number of unknown components of the system, most of which are zero. Under these circumstances, a model needs to be learned that fits the underlying system, capable of generalization. This corresponds to the student-teacher setting in machine learning. In the first part of this paper we introduce a learning algorithm, based on L1-minimization, to identify interaction networks from poor data and analyze its dynamics with respect to phase transitions. The efficiency of the algorithm is measured by the generalization error, which represents the probability that the student is a good fit to the teacher. In the second part of this paper we show that from a system with a specific system size value the generalization error of other system sizes can be estimated. A comparison with a set of simulation experiments show a very good fit.

#index 1100097
#* Semi-supervised Collaborative Text Classification
#@ Rong Jin;Ming Wu;Rahul Sukthankar
#t 2007
#c 20
#% 165110
#% 458379
#% 466263
#% 577224
#% 642981
#% 780690
#% 879625
#% 879626
#! Most text categorization methods require text content of documents that is often difficult to obtain. We consider "Collaborative Text Categorization", where each document is represented by the feedback from a large number of users. Our study focuses on the semi-supervised case in which one key challenge is that a significant number of users have not rated any labeled document. To address this problem, we examine several semi-supervised learning methods and our empirical study shows that collaborative text categorization is more effective than content-based text categorization and the manifold regularization is more effective than other state-of-the-art semi-supervised learning methods.

#index 1100098
#* Learning from Relevant Tasks Only
#@ Samuel Kaski;Jaakko Peltonen
#t 2007
#c 20
#% 236497
#% 578684
#% 723239
#% 770816
#% 770858
#% 829014
#% 840898
#% 840962
#% 961246
#! We introduce a problem called relevant subtask learning, a variant of multi-task learning. The goal is to build a classifier for a task-of-interest having too little data. We also have data for other tasks but only some are relevant, meaning they contain samples classified in the same way as in the task-of-interest. The problem is how to utilize this "background data" to improve the classifier in the task-of-interest. We show how to solve the problem for logistic regression classifiers, and show that the solution works better than a comparable multi-task learning model. The key is to assume that data of all tasks are mixtures of relevant and irrelevant samples, and model the irrelevant part with a sufficiently flexible model such that it does not distort the model of relevant data.

#index 1100099
#* An Unsupervised Learning Algorithm for Rank Aggregation
#@ Alexandre Klementiev;Dan Roth;Kevin Small
#t 2007
#c 20
#% 203129
#% 232319
#% 235377
#% 387427
#% 420464
#% 466736
#% 879582
#% 939603
#% 952560
#% 1272396
#% 1280207
#% 1705501
#% 1705502
#% 1705505
#! Many applications in information retrieval, natural language processing, data mining, and related fields require a ranking of instances with respect to a specified criteria as opposed to a classification. Furthermore, for many such problems, multiple established ranking models have been well studied and it is desirable to combine their results into a joint ranking, a formalism denoted as rank aggregation. This work presents a novel unsupervisedlearning algorithm for rank aggregation (ULARA) which returns a linear combination of the individual ranking functions based on the principle of rewarding ordering agreement between the rankers. In addition to presenting ULARA, we demonstrate its effectiveness on a data fusion task across ad hoc retrieval systems.

#index 1100100
#* Ensembles of Multi-Objective Decision Trees
#@ Dragi Kocev;Celine Vens;Jan Struyf;Sašo Džeroski
#t 2007
#c 20
#% 156421
#% 209021
#% 226438
#% 236497
#% 251145
#% 400847
#% 418132
#% 424997
#% 443616
#% 466073
#% 477980
#% 478467
#% 478470
#% 551723
#% 913833
#% 1403611
#% 1663618
#% 1742012
#! Ensemble methods are able to improve the predictive performance of many base classifiers. Up till now, they have been applied to classifiers that predict a single target attribute. Given the non-trivial interactions that may occur among the different targets in multi-objective prediction tasks, it is unclear whether ensemble methods also improve the performance in this setting. In this paper, we consider two ensemble learning techniques, bagging and random forests, and apply them to multi-objective decision trees (MODTs), which are decision trees that predict multiple target attributes at once. We empirically investigate the performance of ensembles of MODTs. Our most important conclusions are: (1) ensembles of MODTs yield better predictive performance than MODTs, and (2) ensembles of MODTs are equally good, or better than ensembles of single-objective decision trees, i.e., a set of ensembles for each target. Moreover, ensembles of MODTs have smaller model size and are faster to learn than ensembles of single-objective decision trees.

#index 1100101
#* Kernel-Based Grouping of Histogram Data
#@ Tilman Lange;Joachim M. Buhmann
#t 2007
#c 20
#% 115608
#% 160226
#% 272512
#% 344980
#% 443948
#% 732531
#% 732552
#% 748465
#% 1810152
#% 1814838
#! Organizing objects into groups based on their co-occurrence with a second, relevance variable has been widely studied with the Information Bottleneck (IB) as one of the most prominent representatives. We present a kernel-based approach to pairwise clustering of discrete histograms using the Jensen-Shannon (JS) divergence, which can be seen as a two-sampletest. This yields a cost criterion with a solid information-theoretic justification, which can be approximated in polynomial time with arbitrary precision. In addition to that, a relation to optimal hard clustering IB solutions can be established. To our knowledge, we are the first to devise algorithms for the IB with provable approximation guaranties. In practice, one obtains convincing results in the context of image segmentation using fast optimization heuristics.

#index 1100102
#* Active Class Selection
#@ R. Lomasky;C. E. Brodley;M. Aernecke;D. Walt;M. Friedl
#t 2007
#c 20
#% 197922
#% 240783
#% 272518
#% 290482
#% 341269
#% 577286
#% 722805
#% 722921
#% 763705
#% 765523
#% 998622
#! This paper presents Active Class Selection(ACS), a new class of problems for multi-class supervised learning. If one can control the classes from which training data is generated, utilizing feedback during learning to guide the generation of new training data will yield better performance than learning from any a priorifixed class distribution. ACS is the process of iteratively selecting class proportions for data generation. In this paper we present several methods for ACS. In an empirical evaluation, we show that for a fixed number of training instances, methods based on increasing class stability outperform methods that seek to maximize class accuracy or that use random sampling. Finally we present results of a deployed system for our motivating application: training an artificial nose to discriminate vapors.

#index 1100103
#* Sequence Labeling with Reinforcement Learning and Ranking Algorithms
#@ Francis Maes;Ludovic Denoyer;Patrick Gallinari
#t 2007
#c 20
#% 216079
#% 384911
#% 464434
#% 770417
#% 770763
#% 840856
#! Many problems in areas such as Natural Language Processing, Information Retrieval, or Bioinformatic involve the generic task of sequence labeling. In many cases, the aim is to assign a label to each element in a sequence. Until now, this problem has mainly been addressed with Markov models and Dynamic Programming.We propose a new approach where the sequence labeling task is seen as a sequential decision process. This method is shown to be very fast with good generalization accuracy. Instead of searching for a globally optimal label sequence, we learn to construct this optimal sequence directly in a greedy fashion. First, we show that sequence labeling can be modelled using Markov Decision Processes, so that several Reinforcement Learning (RL) algorithms can be used for this task. Second, we introduce a new RL algorithm which is based on the ranking of local labeling decisions.

#index 1100104
#* Efficient Pairwise Classification
#@ Sang-Hyeun Park;Johannes Fürnkranz
#t 2007
#c 20
#% 272518
#% 290482
#% 722807
#% 771846
#% 998581
#% 1393012
#% 1395261
#% 1860941
#! Pairwise classification is a class binarization procedure that converts a multi-class problem into a series of two-class problems, one problem for each pair of classes. While it can be shown that for training, this procedure is more efficient than the more commonly used one-against-all approach, it still has to evaluate a quadratic number of classifiers when computing the predicted class for a given example. In this paper, we propose a method that allows a faster computation of the predicted class when weighted or unweighted voting are used for combining the predictions of the individual classifiers. While its worst-case complexity is still quadratic in the number of classes, we show that even in the case of completely random base classifiers, our method still outperforms the conventional pairwise classifier. For the more practical case of well-trained base classifiers, its asymptotic computational complexity seems to be almost linear.

#index 1100105
#* Scale-Space Based Weak Regressors for Boosting
#@ Jin-Hyeong Park;Chandan K. Reddy
#t 2007
#c 20
#% 58636
#% 71149
#% 209021
#% 302391
#% 318790
#% 382525
#! Boosting is a simple yet powerful modeling technique that is used in many machine learning and data mining related applications. In this paper, we propose a novel scale-space based boosting framework which applies scale-space theory for choosing the optimal regressors during the various iterations of the boosting algorithm. In other words, the data is considered at different resolutions for each iteration in the boosting algorithm. Our framework chooses the weak regressors for the boosting algorithm that can best fit the current resolution and as the iterations progress, the resolution of the data is increased. The amount of increase in the resolution follows from the wavelet decomposition methods. For regression modeling, we use logitboost update equations based on first derivative of the loss function. We clearly manifest the advantages of using this scale-space based framework for regression problems and show results on different real-world regression datasets.

#index 1100106
#* K-Means with Large and Noisy Constraint Sets
#@ Dan Pelleg;Dorit Baras
#t 2007
#c 20
#% 464291
#% 506338
#% 769881
#% 770782
#% 983829
#% 1250560
#% 1663626
#! We focus on the problem of clustering with soft instance-level constraints. Recently, the CVQE algorithm was proposed in this context. It modifies the objective function of traditional K-means to include penalties for violated constraints. CVQE was shown to efficiently produce high-quality clustering of UCI data. In this work, we examine the properties of CVQE and propose a modification that results in a more intuitive objective function, with lower computational complexity. We present our extensive experimentation, which provides insight into CVQE and shows that our new variant can dramatically improve clustering quality while reducing run time. We show its superiority in a large-scale surveillance scenario with noisy constraints.

#index 1100107
#* Towards `Interactive' Active Learning in Multi-view Feature Sets for Information Extraction
#@ Katharina Probst;Rayid Ghani
#t 2007
#c 20
#% 196896
#% 252011
#% 316509
#% 464466
#% 1275209
#! Research in multi-view active learning has typically focused on algorithms for selecting the next example to label. This is often at the cost of lengthy wait-times for the user between each query iteration. We deal with a real-world information extraction task, extracting attribute-value pairs from product descriptions, where the learning system needs to be interactive and the user's time needs to be used efficiently. The first step uses coEM with naive Bayes as the semi-supervised algorithm. This paper focuses on the second step which is an interactive active learning phase. We present an approximation to coEM with naive Bayes that can incorporate user feedback almost instantly and can use any sample-selection strategy for active learning. Our experimental results show high levels of accuracy while being orders of magnitude faster than using the standard coEM with naive Bayes, making our IE system practical by optimizing user time.

#index 1100108
#* Principal Component Analysis for Large Scale Problems with Lots of Missing Values
#@ Tapani Raiko;Alexander Ilin;Juha Karhunen
#t 2007
#c 20
#% 211942
#% 645407
#% 891559
#% 961250
#% 983903
#! Principal component analysis (PCA) is a well-known classical data analysis technique. There are a number of algorithms for solving the problem, some scaling better than others to problems with high dimensionality. They also differ in their ability to handle missing values in the data. We study a case where the data are high-dimensional and a majority of the values are missing. In case of very sparse data, overfitting becomes a severe problem even in simple linear models such as PCA. We propose an algorithm based on speeding up a simple principal subspace rule, and extend it to use regularization and variational Bayesian (VB) learning. The experiments with Netflix data confirm that the proposed algorithm is much faster than any of the compared methods, and that VB-PCA method provides more accurate predictions for new data than traditional PCA or regularized PCA.

#index 1100109
#* Transfer Learning in Reinforcement Learning Problems Through Partial Policy Recycling
#@ Jan Ramon;Kurt Driessens;Tom Croonenborghs
#t 2007
#c 20
#% 204531
#% 246747
#% 322913
#% 333786
#% 458634
#% 769576
#% 890312
#% 1024713
#% 1274860
#% 1274871
#% 1280031
#% 1665160
#! We investigate the relation between transfer learning in reinforcement learning with function approximation and supervised learning with concept drift. We present a new incremental relational regression tree algorithm that is capable of dealing with concept drift through tree restructuring and show that it enables a Q-learner to transfer knowledge from one task to another by recycling those parts of the generalized Q-function that still hold interesting information for the new task. We illustrate the performance of the algorithm in several experiments.

#index 1100110
#* Class Noise Mitigation Through Instance Weighting
#@ Umaa Rebbapragada;Carla E. Brodley
#t 2007
#c 20
#% 464287
#% 466236
#% 466249
#% 466425
#% 543272
#% 566793
#% 722495
#% 926881
#% 998643
#% 1393032
#% 1499584
#! We describe a novel framework for class noise mitigation that assigns a vector of class membership probabilities to each training instance, and uses the confidence on the current label as a weight during training. The probability vector should be calculated such that clean instances have a high confidence on its current label, while mislabeled instances have a low confidence on its current label and a high confidence on its correct label. Past research focuses on techniques that either discard or correct instances. This paper proposes that discarding and correcting are special cases of instance weighting, and thus, part of this framework. We propose a method that uses clustering to calculate a probability distribution over the class labels for each instance. We demonstrate that our method improves classifier accuracy over the original training set. We also demonstrate that instance weighting can outperform discarding.

#index 1100111
#* Optimizing Feature Sets for Structured Data
#@ Ulrich Rückert;Stefan Kramer
#t 2007
#c 20
#% 793239
#% 813990
#% 840863
#% 876043
#% 1250568
#% 1663621
#! Choosing a suitable feature representation for structured data is a non-trivial task due to the vast number of potential candidates. Ideally, one would like to pick a small, but informative set of structural features, each providing complementary information about the instances. We frame the search for a suitable feature set as a combinatorial optimization problem. For this purpose, we define a scoring function that favors features that are as dissimilar as possible to all other features. The score is used in a stochastic local search (SLS) procedure to maximize the diversity of a feature set. In experiments on small molecule data, we investigate the effectiveness of a forward selection approach with two different linear classification schemes.

#index 1100112
#* Roulette Sampling for Cost-Sensitive Learning
#@ Victor S. Sheng;Charles X. Ling
#t 2007
#c 20
#% 136350
#% 209021
#% 280437
#% 290482
#% 369236
#% 466760
#% 477640
#% 727925
#% 770791
#% 1272000
#% 1272369
#% 1289281
#% 1673023
#! In this paper, we propose a new and general preprocessor algorithm, called CSRoulette, which converts any cost-insensitive classification algorithms into cost-sensitive ones. CSRouletteis based on cost proportional roulette sampling technique (called CPRSin short). CSRouletteis closely related to Costing, another cost-sensitive meta-learning algorithm, which is based on rejection sampling. Unlike rejection sampling which produces smaller samples, CPRScan generate different size samples. To further improve its performance, we apply ensemble (bagging) on CPRS; the resulting algorithm is called CSRoulette. Our experiments show that CSRouletteoutperforms Costing and other meta-learning methods in most datasets tested. In addition, we investigate the effect of various sample sizes and conclude that reduced sample sizes (as in rejection sampling) cannot be compensated by increasing the number of bagging iterations.

#index 1100113
#* Modeling Highway Traffic Volumes
#@ Tomáš Šingliar;Miloš Hauskrecht
#t 2007
#c 20
#% 61079
#% 380725
#% 709446
#! Most traffic management and optimization tasks, such as accident detection or optimal vehicle routing, require an ability to adequately model, reason about and predict irregular and stochastic behavior. Our goal is to create a probabilistic model of traffic flows on highway networks that is realistic from the point of applications and at the same time supports efficient learning and inference. We study several multivariate probabilistic models and analyze their respective strengths. To balance accuracy and efficiency, we propose a novel learning model, mixture of Gaussian trees, and show its advantages in learning and inference. All models are evaluated on real-world traffic flow data from highways of the Pittsburgh area.

#index 1100114
#* Undercomplete Blind Subspace Deconvolution Via Linear Prediction
#@ Zoltán Szabó;Barnabás Póczos;András Lőrincz
#t 2007
#c 20
#% 277759
#% 339558
#% 645407
#% 762420
#% 1014641
#% 1688235
#% 1855464
#! We present a novel solution technique for the blind subspace deconvolution (BSSD) problem, where temporal convolution of multidimensional hidden independent components is observed and the task is to uncover the hidden components using the observation only. We carry out this task for the undercomplete case (uBSSD): we reduce the original uBSSD task via linear prediction to independent subspace analysis (ISA), which we can solve. As it has been shown recently, applying temporal concatenation can also reduce uBSSD to ISA, but the associated ISA problem can easily become `high dimensional' [1]. The new reduction method circumvents this dimensionality problem. We perform detailed studies on the efficiency of the proposed technique by means of numerical simulations. We have found several advantages: our method can achieve high quality estimations for smaller number of samples and it can cope with deeper temporal convolutions.

#index 1100115
#* Learning an Outlier-Robust Kalman Filter
#@ Jo-Anne Ting;Evangelos Theodorou;Stefan Schaal
#t 2007
#c 20
#% 277483
#% 876062
#! We introduce a modified Kalman filter that performs robust, real-time outlier detection, without the need for manual parameter tuning by the user. Systems that rely on high quality sensory data (for instance, robotic systems) can be sensitive to data containing outliers. The standard Kalman filter is not robust to outliers, and other variations of the Kalman filter have been proposed to overcome this issue. However, these methods may require manual parameter tuning, use of heuristics or complicated parameter estimation procedures. Our Kalman filter uses a weighted least squares-like approach by introducing weights for each data sample. A data sample with a smaller weight has a weaker contribution when estimating the current time step's state. Using an incremental variational Expectation-Maximization framework, we learn the weights and system dynamics. We evaluate our Kalman filter algorithm on data from a robotic dog.

#index 1100116
#* Imitation Learning Using Graphical Models
#@ Deepak Verma;Rajesh P. Rao
#t 2007
#c 20
#% 384911
#% 465902
#% 570003
#% 770852
#% 876036
#% 1272005
#% 1279315
#% 1650283
#! Imitation-based learning is a general mechanism for rapid acquisition of new behaviors in autonomous agents and robots. In this paper, we propose a new approach to learning by imitation based on parameter learning in probabilistic graphical models. Graphical models are used not only to model an agent's own dynamics but also the dynamics of an observed teacher. Parameter tying between the agent-teacher models ensures consistency and facilitates learning. Given only observations of the teacher's states, we use the expectation-maximization (EM) algorithm to learn both dynamics and policies within graphical models. We present results demonstrating that EM-based imitation learning outperforms pure exploration-based learning on a benchmark problem (the FlagWorld domain). We additionally show that the graphical model representation can be leveraged to incorporate domain knowledge (e.g., state space factoring) to achieve significant speed-up in learning.

#index 1100117
#* Nondeterministic Discretization of Weights Improves Accuracy of Neural Networks
#@ Marcin Wojnarski
#t 2007
#c 20
#% 92611
#% 174161
#% 190433
#% 191910
#% 380342
#% 577981
#% 646336
#% 798763
#% 926881
#! The paper investigates modification of backpropagation algorithm, consisting of discretization of neural network weights after each training cycle. This modification, aimed at overfitting reduction, restricts the set of possible values of weights to a discrete subset of real numbers, leading to much better generalization abilities of the network. This, in turn, leads to higher accuracy and a decrease in error rate by over 50% in extreme cases (when overfitting is high).Discretization is performed nondeterministically, so as to keep expected value of discretized weight equal to original value. In this way, global behavior of original algorithm is preserved. The presented method of discretization is general and may be applied to other machine-learning algorithms. It is also an example of how an algorithm for continuous optimization can be successfully applied to optimization over discrete spaces. The method was evaluated experimentally in WEKA environment using two real-world data sets from UCI repository.

#index 1100118
#* Semi-definite Manifold Alignment
#@ Liang Xiong;Fei Wang;Changshui Zhang
#t 2007
#c 20
#% 266426
#% 593047
#% 593593
#% 757953
#% 770767
#% 871628
#% 881492
#% 883838
#% 940273
#% 1502433
#! We study the problem of manifold alignment, which aims at "aligning" different data sets that share a similar intrinsic manifold provided some supervision. Unlike traditional methods that rely on pairwise correspondences between the two data sets, our method only needs some relative comparison information like "A is more similar to B than A is to C". This method provides a more flexible way to acquire the prior knowledge for alignment, thus is able to handle situations where corresponding pairs are hard or impossible to identify. We optimize our objective based on the graphs that give discrete approximations of the manifold. Further, the problem is formulated as a semi-definite programming(SDP) problem which can readily be solved. Finally, experimental results are presented to show the effectiveness of our method.

#index 1100119
#* General Solution for Supervised Graph Embedding
#@ Qubo You;Nanning Zheng;Shaoyi Du;Yang Wu
#t 2007
#c 20
#% 235342
#% 315986
#% 812580
#% 883894
#% 889099
#% 899897
#% 913838
#% 1274866
#% 1828409
#% 1828410
#! Recently, Graph Embedding Framework has been proposed for feature extraction. However, it is an open issue that how to compute the robust discriminant transformation. In this paper, we first show that supervised graph embedding algorithms share a general criterion (Generalized Rayleigh Quotient). Through novel perspective to Generalized Rayleigh Quotient, we propose a general solution, called General Solution for Supervised Graph Embedding (GSSGE), for extracting the robust discriminant transformation of Supervised Graph Embedding. Finally, extensive experiments on real-world data are performed to demonstrate the effectiveness and robustness of our proposed GSSGE.

#index 1100120
#* Multi-objective Genetic Programming for Multiple Instance Learning
#@ Amelia Zafra;Sebastián Ventura
#t 2007
#c 20
#% 224755
#% 272527
#% 420089
#% 458636
#% 465916
#% 534303
#% 565537
#% 632010
#% 770827
#% 799384
#% 850166
#% 872704
#% 940228
#% 964659
#% 1015508
#! This paper introduces the use of multi-objective evolutionary algorithms in multiple instance learning. In order to achieve this purpose, a multi-objective grammar-guided genetic programming algorithm (MOG3P-MI) has been designed. This algorithm has been evaluated and compared to other existing multiple instance learning algorithms. Research on the performance of our algorithm is carried out on two well-known drug activity prediction problems, Musk and Mutagenesis, both problems being considered typical benchmarks in multiple instance problems. Computational experiments indicate that the application of the MOG3P-MI algorithm improves accuracy and decreases computational cost with respect to other techniques.

#index 1100121
#* Exploiting Term, Predicate, and Feature Taxonomies in Propositionalization and Propositional Rule Learning
#@ Monika Žáková;Filip Železný
#t 2007
#c 20
#% 550585
#% 550723
#% 550729
#% 850431
#% 1096521
#% 1393859
#% 1499551
#% 1682484
#% 1721030
#! Knowledge representations using semantic web technologies often provide information which translates to explicit term and predicate taxonomies in relational learning. We show how to speed up the propositionalization by orders of magnitude, by exploiting such taxonomies through a novel refinement operator used in the construction of conjunctive relational features. Moreover, we accelerate the subsequent propositional search using feature generality taxonomy, determined from the initial term and predicate taxonomies and 茂戮驴-subsumption between features. This enables the propositional rule learner to prevent the exploration of conjunctions containing a feature together with any of its subsumees and to specialize a rule by replacing a feature by its subsumee. We investigate our approach with a deterministic top-down propositional rule learner, and propositional rule learner based on stochastic local search.

#index 1665120
#* Proceedings of the 17th European conference on Machine Learning
#@ Johannes Fürnkranz;Tobias Scheffer;Myra Spiliopoulou
#t 2006
#c 20

#index 1665121
#* On temporal evolution in data streams
#@ Charu C. Aggarwal
#t 2006
#c 20
#! In recent years, the progress in hardware technology has made it possible for organizations to store and record large streams of transactional data. This results in databases which grow without limit at a rapid rate. This data can often show important changes in trends over time. In such cases, it is useful to understand, visualize, and diagnose the evolution of these trends. In this talk, we discuss a method to diagnose the changes in the underlying data stream and other related methods for change detection in streams. We also discuss the problem of data stream evolution in the context of mining algorithms such as clustering and classification. In many cases, mining algorithms may not function as effectively because of the change in the underlying data. We discuss the effects of evolution on mining and synopsis construction algorithms and a number of opportunities which may be available for further research on the topic.

#index 1665122
#* The future of citeseer: citeseerx
#@ C. Lee Giles
#t 2006
#c 20
#! CiteSeer, a public online computer and information science search engine and digital library, was introduced in 1997 and was a radical departure from the traditional methods of academic and scientific document access and analysis. Computer and information scientists quickly became used to and expected immediate access to their literature and CiteSeer provided a popular partial solution. CiteSeer was based on these features: actively acquiring new documents, automatic citation indexing, and automatic linking of citations and documents. CiteSeer, now hosted at the Pennsylvania State University with several mirrors, has over 750,000 documents. The current CiteSeer model is to a limited extent portable and was recently extended to academic business documents (SMEALSearch). Why has CiteSeer been so popular and how should it progress? What is its role with regards to other similar systems such as the Google Scholar and DBLP? What role should CiteSeer play in the open access movement? We discuss this and the Next Generation CiteSeer project, CiteSeerx, which will emphasize CiteSeer as a research tool, research web service, and researcher facilitator and testbed. In contrast to the current tightly integrated CiteSeer architecture, CiteSeerx will be modular, scalable and self managed. We will discuss how new intelligent data mining and information extraction algorithms will provide improved and new indexes, enhanced document access, expanded and automatic document gathering, collaboratories, new data and metadata resources, active mirroring, and web services. As an example of new features, we point out our new API based acknowledgement index and search. This new feature not only provides insight into the impact of acknowledged individuals, funding agencies and others, but also presents an architectural model for integration and expansion of our legacy system.

#index 1665123
#* Learning to have fun
#@ Jonathan Schaeffer
#t 2006
#c 20
#! Games have played a major role in the history of artificial intelligence research. The goal of this research largely has been to build programs capable of defeating strong human players. Most of the literature has been devoted to two-player, perfect information games—games where the research results have little wide-spread applicability. However, over the past few years the need for improved AI techniques have become apparent in commercial computer games, a $25 billion industry. Laird and van Lent call the new generation of commercial games “AI's killer application”. The buying public wants to see realistic artificial intelligence in these products. Here the the metric is a “fun” experience, not winning. Hence, the outcomes from research using these applications will be of much wider applicability. This talk will discuss the challenges of using machine learning in commercial computer games to create “fun”.

#index 1665124
#* Challenges of urban sensing
#@ Henry Tirri
#t 2006
#c 20
#! Wireless sensor networks are emerging as a critical information technology, and they are continuing the trend originating in mainframe computing currently at the stage of mobile computing. This trend shows several aspects consistent in the evolution of computing including the increasing hardware miniaturization of the computing units and an increasing emphasis of the role of communication between the computing units – “networking”. In addition from the software side there is an increasing need to software solutions that are robust, exhibit distributed control, collaborative interfaces resulting in adaptive capabilities also at the system level. Like the present Internet, wireless sensor networks are large-scale distributed systems, but composed of smart sensors and actuators. They will eventually infuse the physical world and provide “grounding” for the Internet thus creating the Internet of Things. Research on wireless sensor networks has been taking place at several levels, from the lowest physical level to the highest information level – the latter is much less developed than the research at the physical levels. In addition, much of the research in wireless sensor networks has been focusing on military or science applications. However, wireless sensor networks can also play an important role in the realization of ubiquitous computing for everyday life – creating what we call “Urban sensing environment”. In urban sensing many natural gateways exist to collect and process the sensor information – static ones such as media devices, or mobile devices such as smart phones that can collect sensor information when entering the communication range of an active sensor. Some of the applications of wireless sensor network technology at home include, in addition to the surveillance functions, adding “intelligence” to utility consumption, electronic tagging, contamination control and disaster monitoring. Similarly at the community level “traffic monitoring” including people allows a development of totally unseen services from micro weather forecasts to new ways for “sensing the environment” for entertainment. In this talk we will outline some of the research challenges for urban sensing, and the role of learning and data analysis techniques for solving those challenges.

#index 1665125
#* Learning in one-shot strategic form games
#@ Alon Altman;Avivit Bercovici-Boden;Moshe Tennenholtz
#t 2006
#c 20
#% 431504
#% 465913
#% 773295
#% 1250154
#% 1272286
#% 1289278
#! We propose a machine learning approach to action prediction in one-shot games. In contrast to the huge literature on learning in games where an agent's model is deduced from its previous actions in a multi-stage game, we propose the idea of inferring correlations between agents' actions in different one-shot games in order to predict an agent's action in a game which she did not play yet. We define the approach and show, using real data obtained in experiments with human subjects, the feasibility of this approach. Furthermore, we demonstrate that this method can be used to increase payoffs of an adequately informed agent. This is, to the best of our knowledge, the first proposed and tested approach for learning in one-shot games, which is the most basic form of multi-agent interaction.

#index 1665126
#* A selective sampling strategy for label ranking
#@ Massih Amini;Nicolas Usunier;François Laviolette;Alexandre Lacasse;Patrick Gallinari
#t 2006
#c 20
#% 201259
#% 280838
#% 722797
#% 722924
#% 770753
#% 1699592
#% 1705509
#! We propose a novel active learning strategy based on the compression framework of [9] for label ranking functions which, given an input instance, predict a total order over a predefined set of alternatives. Our approach is theoretically motivated by an extension to ranking and active learning of Kääriäinen's generalization bounds using unlabeled data [7], initially developed in the context of classification. The bounds we obtain suggest a selective sampling strategy provided that a sufficiently, yet reasonably large initial labeled dataset is provided. Experiments on Information Retrieval corpora from automatic text summarization and question/answering show that the proposed approach allows to substantially reduce the labeling effort in comparison to random and heuristic-based sampling strategies.

#index 1665127
#* Combinatorial markov random fields
#@ Ron Bekkerman;Mehran Sahami;Erik Learned-Miller
#t 2006
#c 20
#% 191603
#% 199557
#% 466890
#% 528174
#% 722904
#% 722930
#% 729918
#% 785334
#% 840840
#% 1289476
#! A combinatorial random variable is a discrete random variable defined over a combinatorial set (e.g., a power set of a given set). In this paper we introduce combinatorial Markov random fields (Comrafs), which are Markov random fields where some of the nodes are combinatorial random variables. We argue that Comrafs are powerful models for unsupervised and semi-supervised learning. We put Comrafs in perspective by showing their relationship with several existing models. Since it can be problematic to apply existing inference techniques for graphical models to Comrafs, we design two simple and efficient inference algorithms specific for Comrafs, which are based on combinatorial optimization. We show that even such simple algorithms consistently and significantly outperform Latent Dirichlet Allocation (LDA) on a document clustering task. We then present Comraf models for semi-supervised clustering and transfer learning that demonstrate superior results in comparison to an existing semi-supervised scheme (constrained optimization).

#index 1665128
#* Learning stochastic tree edit distance
#@ Marc Bernard;Amaury Habrard;Marc Sebban
#t 2006
#c 20
#% 66654
#% 251405
#% 547947
#% 775353
#% 826007
#% 940343
#! Trees provide a suited structural representation to deal with complex tasks such as web information extraction, RNA secondary structure prediction, or conversion of tree structured documents. In this context, many applications require the calculation of similarities between tree pairs. The most studied distance is likely the tree edit distance (ED) for which improvements in terms of complexity have been achieved during the last decade. However, this classic ED usually uses a priori fixed edit costs which are often difficult to tune, that leaves little room for tackling complex problems. In this paper, we focus on the learning of a stochastic tree ED. We use an adaptation of the Expectation-Maximization algorithm for learning the primitive edit costs. We carried out series of experiments that confirm the interest to learn a tree ED rather than a priori imposing edit costs.

#index 1665129
#* Pertinent background knowledge for learning protein grammars
#@ Christopher H. Bryant;Daniel C. Fredouille;Alex Wilson;Channa K. Jayawickreme;Steven Jupe;Simon Topp
#t 2006
#c 20
#% 217072
#% 471430
#% 753026
#% 761271
#% 905716
#! We are interested in using Inductive Logic Programming (ILP) to infer grammars representing sets of protein sequences. ILP takes as input both examples and background knowledge predicates. This work is a first step in optimising the choice of background knowledge predicates for predicting the function of proteins. We propose methods to obtain different sets of background knowledge. We then study the impact of these sets on inference results through a hard protein function inference task: the prediction of the coupling preference of GPCR proteins. All but one of the proposed sets of background knowledge are statistically shown to have positive impacts on the predictive power of inferred rules, either directly or through interactions with other sets. In addition, this work provides further confirmation, after the work of Muggleton et al., 2001 that ILP can help to predict protein functions.

#index 1665130
#* Improving bayesian network structure search with random variable aggregation hierarchies
#@ John Burge;Terran Lane
#t 2006
#c 20
#% 6199
#% 197387
#% 286423
#% 292235
#% 466078
#% 577225
#% 840847
#% 892526
#! Bayesian network structure identification is known to be NP-Hard in the general case. We demonstrate a heuristic search for structure identification based on aggregationhierarchies. The basic idea is to perform initial exhaustive searches on composite “high-level” random variables (RVs) that are created via aggregations of atomic RVs. The results of the high-level searches then constrain a refined search on the atomic RVs. We demonstrate our methods on a challenging real-world neuroimaging domain and show that they consistently yield higher scoring networks when compared to traditional searches, provided sufficient topological complexity is permitted. On simulated data, where ground truth is known and controllable, our methods yield improved classification accuracy and structural precision, but can also result in reduced structural recall on particularly noisy datasets.

#index 1665131
#* Sequence discrimination using phase-type distributions
#@ Jérôme Callut;Pierre Dupont
#t 2006
#c 20
#% 725108
#% 743284
#% 1699618
#! We propose in this paper a novel approach to the classification of discrete sequences. This approach builds a model fitting some dynamical features deduced from the learning sample. These features are discrete phase-type (PH) distributions. They model the first passage times (FPT) between occurrences of pairs of substrings. The PHit algorithm, an adapted version of the Expectation-Maximization algorithm, is proposed to estimate PH distributions. The most informative pairs of substrings are selected according to the Jensen-Shannon divergence between their class conditional empirical FPT distributions. The selected features are then used in two classification schemes: a maximum a posteriori (MAP) classifier and support vector machines (SVM) with marginalized kernels. Experiments on DNA splicing region detection and on protein sublocalization illustrate that the proposed techniques offer competitive results with smoothed Markov chains or SVM with a spectrum string kernel.

#index 1665132
#* Languages as hyperplanes: grammatical inference with string kernels
#@ Alexander Clark;Christophe Costa Florêncio;Chris Watkins
#t 2006
#c 20
#% 63820
#% 266426
#% 288469
#% 722803
#% 740347
#% 743284
#% 748184
#% 1728750
#! Using string kernels, languages can be represented as hyperplanes in a high dimensional feature space. We present a new family of grammatical inference algorithms based on this idea. We demonstrate that some mildly context sensitive languages can be represented in this way and it is possible to efficiently learn these using kernel PCA. We present some experiments demonstrating the effectiveness of this approach on some standard examples of context sensitive languages using small synthetic data sets.

#index 1665133
#* Toward robust real-world inference: a new perspective on explanation-based learning
#@ Gerald DeJong
#t 2006
#c 20
#% 26722
#% 143185
#% 161241
#% 170409
#% 175368
#% 379345
#% 396021
#% 449587
#% 451031
#% 458668
#% 466229
#% 576214
#% 731606
#% 1269484
#% 1273622
#% 1290242
#% 1469428
#% 1657185
#! Over the last twenty years AI has undergone a sea change. The once-dominant paradigm of logical inference over symbolic knowledge representations has largely been supplanted by statistical methods. The statistical paradigm affords a robustness in the real-world that has eluded symbolic logic. But statistics sacrifices much in expressiveness and inferential richness, which is achieved by first-order logic through the nonlinear interaction and combinatorial interplay among quantified component sentences. We present a new form of Explanation Based Learning in which inference results from two forms of evidence: analytic (support via sound derivation from first-order representations of an expert's conceptualization of a domain) and empirical (corroboration derived from consistency with real-world observations). A simple algorithm provides a first illustration of the approach. Some important properties are proven including tractability and robustness with respect to the real world.

#index 1665134
#* Fisher kernels for relational data
#@ Uwe Dick;Kristian Kersting
#t 2006
#c 20
#% 304917
#% 312861
#% 345862
#% 550743
#% 722807
#% 722914
#% 829043
#% 840947
#% 926881
#% 961144
#% 1250568
#% 1269484
#% 1269496
#% 1650403
#! Combining statistical and relational learning receives currently a lot of attention. The majority of statistical relational learning approaches focus on density estimation. For classification, however, it is well-known that the performance of such generative models is often lower than that of discriminative classifiers. One approach to improve the performance of generative models is to combine them with discriminative algorithms. Fisher kernels were developed to combine them with kernel methods, and have shown promising results for the combinations of support vector machines with (logical) hidden Markov models and Bayesian networks. So far, however, Fisher kernels have not been considered for relational data, i.e., data consisting of a collection of objects and relational among these objects. In this paper, we develop Fisher kernels for relational data and empirically show that they can significantly improve over the results achieved without Fisher kernels.

#index 1665135
#* Evaluating misclassifications in imbalanced data
#@ William Elazmeh;Nathalie Japkowicz;Stan Matwin
#t 2006
#c 20
#% 272995
#% 310519
#% 840902
#% 926881
#% 1272396
#% 1389694
#% 1699621
#! Evaluating classifier performance with ROC curves is popular in the machine learning community. To date, the only method to assess confidence of ROC curves is to construct ROC bands. In the case of severe class imbalance with few instances of the minority class, ROC bands become unreliable. We propose a generic framework for classifier evaluation to identify a segment of an ROC curve in which misclassifications are balanced. Confidence is measured by Tango's 95%-confidence interval for the difference in misclassification in both classes. We test our method with severe class imbalance in a two-class problem. Our evaluation favors classifiers with low numbers of misclassifications in both classes. Our results show that the proposed evaluation method is more confident than ROC bands.

#index 1665136
#* Improving control-knowledge acquisition for planning by active learning
#@ Raquel Fuentetaja;Daniel Borrajo
#t 2006
#c 20
#% 229976
#% 243695
#% 370528
#% 732423
#% 943069
#% 1272282
#% 1272340
#% 1478821
#! Automatically acquiring control-knowledge for planning, as it is the case for Machine Learning in general, strongly depends on the training examples. In the case of planning, examples are usually extracted from the search tree generated when solving problems. Therefore, examples depend on the problems used for training. Traditionally, these problems are randomly generated by selecting some difficulty parameters. In this paper, we discuss several active learning schemes that improve the relationship between the number of problems generated and planning results in another test set of problems. Results show that these schemes are quite useful for increasing the number of solved problems.

#index 1665137
#* PAC-Learning of markov models with hidden state
#@ Ricard Gavaldà;Philipp W. Keller;Joelle Pineau;Doina Precup
#t 2006
#c 20
#% 697
#% 176024
#% 190330
#% 203295
#% 466593
#% 466716
#% 476708
#% 763712
#% 770863
#% 875996
#% 1378365
#! The standard approach for learning Markov Models with Hidden State uses the Expectation-Maximization framework. While this approach had a significant impact on several practical applications (e.g. speech recognition, biological sequence alignment) it has two major limitations: it requires a known model topology, and learning is only locally optimal. We propose a new PAC framework for learning both the topology and the parameters in partially observable Markov models. Our algorithm learns a Probabilistic Deterministic Finite Automata (PDFA) which approximates a Hidden Markov Model (HMM) up to some desired degree of accuracy. We discuss theoretical conditions under which the algorithm produces an optimal solution (in the PAC-sense) and demonstrate promising performance on simple dynamical systems.

#index 1665138
#* A discriminative approach for the retrieval of images from text queries
#@ David Grangier;Florent Monay;Samy Bengio
#t 2006
#c 20
#% 329569
#% 387427
#% 457912
#% 577224
#% 642989
#% 722927
#% 724232
#% 730230
#% 733364
#% 760805
#% 780756
#! This work proposes a new approach to the retrieval of images from text queries. Contrasting with previous work, this method relies on a discriminative model: the parameters are selected in order to minimize a loss related to the ranking performance of the model, i.e. its ability to rank the relevant pictures above the non-relevant ones when given a text query. In order to minimize this loss, we introduce an adaptation of the recently proposed Passive-Aggressive algorithm. The generalization performance of this approach is then compared with alternative models over the Corel dataset. These experiments show that our method outperforms the current state-of-the-art approaches, e.g. the average precision over Corel test data is 21.6% for our model versus 16.7% for the best alternative, Probabilistic Latent Semantic Analysis.

#index 1665139
#* TildeCRF: conditional random fields for logical sequences
#@ Bernd Gutmann;Kristian Kersting
#t 2006
#c 20
#% 33376
#% 464434
#% 577225
#% 722807
#% 770844
#% 770850
#% 850430
#% 1272104
#% 1279354
#% 1290272
#% 1650403
#% 1673026
#! Conditional Random Fields (CRFs) provide a powerful instrument for labeling sequences. So far, however, CRFs have only been considered for labeling sequences over flat alphabets. In this paper, we describe TildeCRF, the first method for training CRFs on logical sequences, i.e., sequences over an alphabet of logical atoms. TildeCRF's key idea is to use relational regression trees in Dietterich et al.'s gradient tree boosting approach. Thus, the CRF potential functions are represented as weighted sums of relational regression trees. Experiments show a significant improvement over established results achieved with hidden Markov models and Fisher kernels for logical sequences.

#index 1665140
#* Unsupervised multiple-instance learning for functional profiling of genomic data
#@ Corneliu Henegar;Karine Clément;Jean-Daniel Zucker
#t 2006
#c 20
#% 224755
#% 272527
#% 464621
#% 534303
#% 565537
#% 576696
#% 641689
#% 722913
#% 770827
#% 799384
#% 840922
#! Multiple-instance learning (MIL) is a popular concept among the AI community to support supervised learning applications in situations where only incomplete knowledge is available. We propose an original reformulation of the MIL concept for the unsupervised context (UMIL), which can serve as a broader framework for clustering data objects adequately described by the multiple-instance representation. Three algorithmic solutions are suggested by derivation from available conventional methods: agglomerative or partition clustering and MIL's citation-kNN approach. Based on standard clustering quality measures, we evaluated these algorithms within a bioinformatic framework to perform a functional profiling of two genomic data sets, after relating expression data to biological annotations into an UMIL representation. Our analysis spotlighted meaningful interaction patterns relating biological processes and regulatory pathways into coherent functional modules, uncovering profound features of the biological model. These results indicate UMIL's usefulness in exploring hidden behavioral patterns from complex data.

#index 1665141
#* Bayesian learning of markov network structure
#@ Aleks Jakulin;Irina Rish
#t 2006
#c 20
#% 44876
#% 246832
#% 464434
#% 528179
#% 722753
#% 770761
#% 810947
#% 810949
#% 840881
#% 840917
#% 846595
#% 1699603
#% 1815596
#! We propose a simple and efficient approach to building undirected probabilistic classification models (Markov networks) that extend naïve Bayes classifiers and outperform existing directed probabilistic classifiers (Bayesian networks) of similar complexity. Our Markov network model is represented as a set of consistent probability distributions on subsets of variables. Inference with such a model can be done efficiently in closed form for problems like class probability estimation. We also propose a highly efficient Bayesian structure learning algorithm for conditional prediction problems, based on integrating along a hill-climb in the structure space. Our prior based on the degrees of freedom effectively prevents overfitting.

#index 1665142
#* Approximate policy iteration for closed-loop learning of visual tasks
#@ Sébastien Jodogne;Cyril Briquet;Justus H. Piater
#t 2006
#c 20
#% 313140
#% 393786
#% 734920
#% 760805
#% 812305
#% 829011
#% 838742
#% 840884
#% 866298
#! Approximate Policy Iteration (API) is a reinforcement learning paradigm that is able to solve high-dimensional, continuous control problems. We propose to exploit API for the closed-loop learning of mappings from images to actions. This approach requires a family of function approximators that maps visual percepts to a real-valued function. For this purpose, we use Regression Extra-Trees, a fast, yet accurate and versatile machine learning algorithm. The inputs of the Extra-Trees consist of a set of visual features that digest the informative patterns in the visual signal. We also show how to parallelize the Extra-Tree learning process to further reduce the computational expense, which is often essential in visual tasks. Experimental results on real-world images are given that indicate that the combination of API with Extra-Trees is a promising framework for the interactive learning of visual tasks.

#index 1665143
#* Task-Driven discretization of the joint space of visual percepts and continuous actions
#@ Sébastien Jodogne;Justus H. Piater
#t 2006
#c 20
#% 111440
#% 252329
#% 313140
#% 384911
#% 393786
#% 425080
#% 486621
#% 760805
#% 829011
#% 840884
#! We target the problem of closed-loop learning of control policies that map visual percepts to continuous actions. Our algorithm, called Reinforcement Learning of Joint Classes (RLJC), adaptively discretizes the joint space of visual percepts and continuous actions. In a sequence of attempts to remove perceptual aliasing, it incrementally builds a decision tree that applies tests either in the input perceptual space or in the output action space. The leaves of such a decision tree induce a piecewise constant, optimal state-action value function, which is computed through a reinforcement learning algorithm that uses the tree as a function approximator. The optimal policy is then derived by selecting the action that, given a percept, leads to the leaf that maximizes the value function. Our approach is quite general and applies also to learning mappings from continuous percepts to continuous actions. A simulated visual navigation problem illustrates the applicability of RLJC.

#index 1665144
#* EM algorithm for symmetric causal independence models
#@ Rasa Jurgelenaite;Tom Heskes
#t 2006
#c 20
#% 29980
#% 44876
#% 851933
#% 926881
#% 1272302
#% 1291431
#% 1650738
#% 1712332
#! Causal independence modelling is a well-known method both for reducing the size of probability tables and for explaining the underlying mechanisms in Bayesian networks. In this paper, we present the EM algorithm to learn the parameters in causal independence models based on the symmetric Boolean function. The developed algorithm enables us to assess the practical usefulness of the symmetric causal independence models, which has not been done previously. We evaluate the classification performance of the symmetric causal independence models learned with the presented EM algorithm. The results show the competitive performance of these models in comparison to noisy OR and noisy AND models as well as other state-of-the-art classifiers.

#index 1665145
#* Deconvolutive clustering of markov states
#@ Ata Kabán;Xin Wang
#t 2006
#c 20
#% 190861
#% 266876
#% 292004
#% 380342
#% 453337
#% 466574
#% 549441
#% 784184
#! In this paper we formulate the problem of grouping the states of a discrete Markov chain of arbitrary order simultaneously with deconvolving its transition probabilities. As the name indicates, this problem is related to deconvolutive blind signal separation. However, whilst the latter has been studied in the context of continuous signal processing, e.g. as a model of a real-room mixing of sound signals, our technique tries to model computer-mediated group-discussion participation from a discrete event-log sequence. In this context, convolution occurs due to various time-delay factors, such as the network transmission bandwidth or simply the typing speed of the participants. We derive a computationally efficient maximum likelihood estimation algorithm associated with our model, which exploits the sparsity of state transitions and scales linearly with the number of observed higher order transition patterns. Results obtained on a full day worth dynamic real-world Internet Relay Chat participation sequence demonstrate the advantages of our approach over state grouping alone, both in terms of penalised data likelihood and cluster clarity. Other potential applications of our model, viewed as a novel compact approximation of large Markov chains, are also discussed.

#index 1665146
#* Patching approximate solutions in reinforcement learning
#@ Min Sub Kim;William Uther
#t 2006
#c 20
#% 68238
#% 160859
#% 384911
#% 770832
#% 823852
#% 1271827
#% 1291498
#! This paper introduces an approach to improving an approximate solution in reinforcement learning by augmenting it with a small overriding patch. Many approximate solutions are smaller and easier to produce than a flat solution, but the best solution within the constraints of the approximation may fall well short of global optimality. We present a technique for efficiently learning a small patch to reduce this gap. Empirical evaluation demonstrates the effectiveness of patching, producing combined solutions that are much closer to global optimality.

#index 1665147
#* Fast variational inference for gaussian process models through KL-Correction
#@ Nathaniel J. King;Neil D. Lawrence
#t 2006
#c 20
#% 331916
#% 715096
#% 857429
#% 889295
#% 891549
#! Variational inference is a flexible approach to solving problems of intractability in Bayesian models. Unfortunately the convergence of variational methods is often slow. We review a recently suggested variational approach for approximate inference in Gaussian process (GP) models and show how convergence may be dramatically improved through the use of a positive correction term to the standard variational bound. We refer to the modified bound as a KL-corrected bound. The KL-corrected bound is a lower bound on the true likelihood, but an upper bound on the original variational bound. Timing comparisons between optimisation of the two bounds show that optimisation of the new bound consistently improves the speed of convergence.

#index 1665148
#* Bandit based monte-carlo planning
#@ Levente Kocsis;Csaba Szepesvári
#t 2006
#c 20
#% 180122
#% 348584
#% 348585
#% 416988
#% 425053
#% 794719
#% 1273918
#! For large state-space Markovian Decision Problems Monte-Carlo planning is one of the few viable approaches to find near-optimal solutions. In this paper we introduce a new algorithm, UCT, that applies bandit ideas to guide Monte-Carlo planning. In finite-horizon or discounted MDPs the algorithm is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling. Experimental results show that in several domains, UCT is significantly more efficient than its alternatives.

#index 1665149
#* Bayesian learning with mixtures of trees
#@ Jussi Kollin;Mikko Koivisto
#t 2006
#c 20
#% 197387
#% 528151
#% 722753
#% 763715
#% 806945
#% 810948
#! We present a Bayesian method for learning mixtures of graphical models. In particular, we focus on data clustering with a tree-structured model for each cluster. We use a Markov chain Monte Carlo method to draw a sample of clusterings, while the likelihood of a clustering is computed by exact averaging over the model class, including the dependency structure on the variables. Experiments on synthetic data show that this method usually outperforms the expectation–maximization algorithm by Meilă and Jordan [1] when the number of observations is small (hundreds) and the number of variables is large (dozens). We apply the method to study how much single nucleotide polymorphisms carry information about the structure of human populations.

#index 1665150
#* Transductive gaussian process regression with automatic model selection
#@ Quoc V. Le;Alex J. Smola;Thomas G&#228/rtner;Yasemin Altun
#t 2006
#c 20
#% 277467
#% 277516
#% 840896
#% 857421
#% 875962
#% 940273
#% 961134
#% 961178
#! In contrast to the standard inductive inference setting of predictive machine learning, in real world learning problems often the test instances are already available at training time. Transductive inference tries to improve the predictive accuracy of learning algorithms by making use of the information contained in these test instances. Although this description of transductive inference applies to predictive learning problems in general, most transductive approaches consider the case of classification only. In this paper we introduce a transductive variant of Gaussian process regression with automatic model selection, based on approximate moment matching between training and test data. Empirical results show the feasibility and competitiveness of this approach.

#index 1665151
#* Efficient convolution kernels for dependency and constituent syntactic trees
#@ Alessandro Moschitti
#t 2006
#c 20
#% 269217
#% 452991
#% 722926
#% 740916
#% 743284
#% 746865
#% 815896
#% 817422
#% 823311
#% 905498
#% 938668
#% 938695
#% 938706
#% 939355
#% 939871
#% 1344869
#! In this paper, we provide a study on the use of tree kernels to encode syntactic parsing information in natural language learning. In particular, we propose a new convolution kernel, namely the Partial Tree (PT) kernel, to fully exploit dependency trees. We also propose an efficient algorithm for its computation which is futhermore sped-up by applying the selection of tree nodes with non-null kernel. The experiments with Support Vector Machines on the task of semantic role labeling and question classification show that (a) the kernel running time is linear on the average case and (b) the PT kernel improves on the other tree kernels when applied to the appropriate parsing paradigm.

#index 1665152
#* Why is rule learning optimistic and how to correct it
#@ Martin Možina;Janez Demšar;Jure Žabkar;Ivan Bratko
#t 2006
#c 20
#% 307109
#% 449566
#% 458178
#% 478279
#% 550575
#% 740900
#% 799042
#! In their search through a huge space of possible hypotheses, rule induction algorithms compare estimations of qualities of a large number of rules to find the one that appears to be best. This mechanism can easily find random patterns in the data which will – even though the estimating method itself may be unbiased (such as relative frequency) – have optimistically high quality estimates. It is generally believed that the problem, which eventually leads to overfitting, can be alleviated by using m-estimate of probability. We show that this can only partially mend the problem, and propose a novel solution to making the common rule evaluation functions account for multiple comparisons in the search. Experiments on artificial data sets and data sets from the UCI repository show a large improvement in accuracy of probability predictions and also a decent gain in AUC of the constructed models.

#index 1665153
#* Automatically evolving rule induction algorithms
#@ Gisele L. Pappa;Alex A. Freitas
#t 2006
#c 20
#% 124073
#% 126949
#% 136350
#% 277919
#% 290482
#% 290713
#% 314784
#% 376266
#% 389460
#% 411390
#% 458178
#% 520750
#% 644331
#% 1290056
#! Research in the rule induction algorithm field produced many algorithms in the last 30 years. However, these algorithms are usually obtained from a few basic rule induction algorithms that have been often changed to produce better ones. Having these basic algorithms and their components in mind, this work proposes the use of Grammar-based Genetic Programming (GGP) to automatically evolve rule induction algorithms. The proposed GGP is evaluated in extensive computational experiments involving 11 data sets. Overall, the results show that effective rule induction algorithms can be automatically generated using GGP. The automatically evolved rule induction algorithms were shown to be competitive with well-known manually designed ones. The proposed approach of automatically evolving rule induction algorithms can be considered a pioneering one, opening a new kind of research area.

#index 1665154
#* Bayesian active learning for sensitivity analysis
#@ Tobias Pfingsten
#t 2006
#c 20
#% 116388
#% 132697
#% 133673
#% 891549
#! Designs of micro electro-mechanical devices need to be robust against fluctuations in mass production. Computer experiments with tens of parameters are used to explore the behavior of the system, and to compute sensitivity measures as expectations over the input distribution. Monte Carlo methods are a simple approach to estimate these integrals, but they are infeasible when the models are computationally expensive. Using a Gaussian processes prior, expensive simulation runs can be saved. This Bayesian quadrature allows for an active selection of inputs where the simulation promises to be most valuable, and the number of simulation runs can be reduced further. We present an active learning scheme for sensitivity analysis which is rigorously derived from the corresponding Bayesian expected loss. On three fully featured, high dimensional physical models of electro-mechanical sensors, we show that the learning rate in the active learning scheme is significantly better than for passive learning.

#index 1665155
#* Mixtures of kikuchi approximations
#@ Roberto Santana;Pedro Larrañaga;Jose A. Lozano
#t 2006
#c 20
#% 272505
#% 392766
#% 479151
#% 601159
#% 722753
#% 846594
#% 846595
#! Mixtures of distributions concern modeling a probability distribution by a weighted sum of other distributions. Kikuchi approximations of probability distributions follow an approach to approximate the free energy of statistical systems. In this paper, we introduce the mixture of Kikuchi approximations as a probability model. We present an algorithm for learning Kikuchi approximations from data based on the expectation-maximization (EM) paradigm. The proposal is tested in the approximation of probability distributions that arise in evolutionary computation.

#index 1665156
#* Boosting in PN spaces
#@ Martin Scholz
#t 2006
#c 20
#% 235377
#% 290482
#% 302391
#% 466240
#% 770822
#% 799042
#% 823351
#% 881575
#% 1705505
#! This paper analyzes boosting in unscaled versions of ROC spaces, also referred to as PN spaces. A minor revision to AdaBoost 's reweighting strategy is analyzed, which allows to reformulate it in terms of stratification, and to visualize the boosting process in nested PN spaces as known from divide-and-conquer rule learning. The analyzed confidence-rated algorithm is proven to take more advantage of its base models in each iteration, although also searching a space of linear discrete base classifier combinations. The algorithm reduces the training error quicker without lacking any of the advantages of original AdaBoost. The PN space interpretation allows to derive a lower-bound for the area under the ROC curve metric (AUC) of resulting ensembles based on the AUC after reweighting. The theoretical findings of this paper are complemented by an empirical evaluation on benchmark datasets.

#index 1665157
#* Prioritizing point-based POMDP solvers
#@ Guy Shani;Ronen I. Brafman;Solomon E. Shimony
#t 2006
#c 20
#% 92301
#% 788098
#% 829022
#% 1272075
#% 1279358
#% 1478842
#% 1650702
#% 1713177
#% 1732748
#! Recent scaling up of POMDP solvers towards realistic applications is largely due to point-based methods such as PBVI, Perseus, and HSVI, which quickly converge to an approximate solution for medium-sized problems. These algorithms improve a value function by using backup operations over a single belief point. In the simpler domain of MDP solvers, prioritizing the order of equivalent backup operations on states is well known to speed up convergence. We generalize the notion of prioritized backups to the POMDP framework, and show that the ordering of backup operations on belief points is important. We also present a new algorithm, Prioritized Value Iteration (PVI), and show empirically that it outperforms current point-based algorithms. Finally, a new empirical evaluation measure, based on the number of backups and the number of belief points, is proposed, in order to provide more accurate benchmark comparisons.

#index 1665158
#* Graph based semi-supervised learning with sharper edges
#@ Hyunjung (Helen) Shin;N. Jeremy Hill;Gunnar Rätsch
#t 2006
#c 20
#% 464615
#% 466263
#% 565549
#% 757953
#% 765261
#% 770790
#% 829027
#% 840938
#% 961134
#% 961190
#% 1455666
#! In many graph-based semi-supervised learning algorithms, edge weights are assumed to be fixed and determined by the data points' (often symmetric) relationships in input space, without considering directionality. However, relationships may be more informative in one direction (e.g. from labelled to unlabelled) than in the reverse direction, and some relationships (e.g. strong weights between oppositely labelled points) are unhelpful in either direction. Undesirable edges may reduce the amount of influence an informative point can propagate to its neighbours – the point and its outgoing edges have been “blunted.” We present an approach to “sharpening” in which weights are adjusted to meet an optimization criterion wherever they are directed towards labelled points. This principle can be applied to a wide variety of algorithms. In the current paper, we present one ad hoc solution satisfying the principle, in order to show that it can improve performance on a number of publicly available benchmark data sets.

#index 1665159
#* Margin-Based active learning for structured output spaces
#@ Dan Roth;Kevin Small
#t 2006
#c 20
#% 466231
#% 722797
#% 724192
#% 770763
#% 840836
#% 840856
#% 853848
#% 854636
#% 855092
#% 939845
#% 1269476
#% 1289530
#% 1344869
#! In many complex machine learning applications there is a need to learn multiple interdependent output variables, where knowledge of these interdependencies can be exploited to improve the global performance. Typically, these structured output scenarios are also characterized by a high cost associated with obtaining supervised training data, motivating the study of active learning for these situations. Starting with active learning approaches for multiclass classification, we first design querying functions for selecting entire structured instances, exploring the tradeoff between selecting instances based on a global margin or a combination of the margin of local classifiers. We then look at the setting where subcomponents of the structured instance can be queried independently and examine the benefit of incorporating structural information in such scenarios. Empirical results on both synthetic data and the semantic role labeling task demonstrate a significant reduction in the need for supervised training data when using the proposed methods.

#index 1665160
#* Skill acquisition via transfer learning and advice taking
#@ Lisa Torrey;Jude Shavlik;Trevor Walker;Richard Maclin
#t 2006
#c 20
#% 124694
#% 203611
#% 344803
#% 384911
#% 449561
#% 464296
#% 464773
#% 823327
#% 823852
#% 1269488
#% 1699609
#! We describe a reinforcement learning system that transfers skills from a previously learned source task to a related target task. The system uses inductive logic programming to analyze experience in the source task, and transfers rules for when to take actions. The target task learner accepts these rules through an advice-taking algorithm, which allows learners to benefit from outside guidance that may be imperfect. Our system accepts a human-provided mapping, which specifies the similarities between the source and target tasks and may also include advice about the differences between them. Using three tasks in the RoboCup simulated soccer domain, we demonstrate that this system can speed up reinforcement learning substantially.

#index 1665161
#* Constant rate approximate maximum margin algorithms
#@ Petroula Tsampouka;John Shawe-Taylor
#t 2006
#c 20
#% 190581
#% 276557
#% 302390
#% 309208
#% 722814
#% 1699646
#! We present a new class of Perceptron-like algorithms with margin in which the “effective” learning rate ηeff, defined as the ratio of the learning rate to the length of the weight vector, remains constant. We prove that for ηeff sufficiently small the new algorithms converge in a finite number of steps and show that there exists a limit of the parameters involved in which convergence leads to classification with maximum margin. A soft margin extension for Perceptron-like large margin classifiers is also discussed.

#index 1665162
#* Batch classification with applications in computer aided diagnosis
#@ Volkan Vural;Glenn Fung;Balaji Krishnapuram;Jennifer Dy;Bharat Rao
#t 2006
#c 20
#% 190581
#% 464434
#% 466084
#! Most classification methods assume that the samples are drawn independently and identically from an unknown data generating distribution, yet this assumption is violated in several real life problems. In order to relax this assumption, we consider the case where batches or groups of samples may have internal correlations, whereas the samples from different batches may be considered to be uncorrelated. Two algorithms are developed to classify all the samples in a batch jointly, one based on a probabilistic analysis and another based on a mathematical programming approach. Experiments on three real-life computer aided diagnosis (CAD) problems demonstrate that the proposed algorithms are significantly more accurate than a naive SVM which ignores the correlations among the samples.

#index 1665163
#* Improving the ranking performance of decision trees
#@ Bin Wang;Harry Zhang
#t 2006
#c 20
#% 136350
#% 290482
#% 349550
#% 424997
#% 464280
#% 466078
#% 466086
#% 580510
#% 1279288
#% 1378224
#! An accurate ranking of instances based on their class probabilities, which is measured by AUC (area under the Receiver Operating Characteristics curve), is desired in many applications. In a traditional decision tree, two obstacles prevent it from yielding accurate rankings: one is that the sample size on a leaf is small, and the other is that the instances falling into the same leaf are assigned to the same class probability. In this paper, we propose two techniques to address these two issues. First, we use the statistical technique shrinkage which estimates the class probability of a test instance by using a linear interpolation of the local class probabilities on each node along the path from leaf to root. An efficient algorithm is also brought forward to learn the interpolating weights. Second, we introduce an instance-based method, the weighted probability estimation (WPE), to generate distinct local probability estimates for the test instances falling into the same leaf. The key idea is to assign different weights to training instances based on their similarities to the test instance in probability estimation. Furthermore, we combine shrinkage and WPE together to compensate for the defects of each. Our experiments show that both shrinkage and WPE improve the ranking performance of decision trees, and that their combination works even better. The experiments also indicate that various decision tree algorithms with the combination of shrinkage and WPE significantly outperform the original ones and other state-of-the-art techniques proposed to enhance the ranking performance of decision trees.

#index 1665164
#* Multiple-Instance learning via random walk
#@ Dong Wang;Jianmin Li;Bo Zhang
#t 2006
#c 20
#% 224755
#% 268079
#% 464633
#% 565537
#% 707541
#% 770827
#% 771844
#% 840452
#% 840844
#% 840892
#% 840922
#% 840965
#% 840967
#! This paper presents a decoupled two stage solution to the multiple-instance learning (MIL) problem. With a constructed affinity matrix to reflect the instance relations, a modified Random Walk on a Graph process is applied to infer the positive instances in each positive bag. This process has both a closed form solution and an efficient iterative one. Combined with the Support Vector Machine (SVM) classifier, this algorithm decouples the inferring and training stages and converts MIL into a supervised learning problem. Compared with previous algorithms on several benchmark data sets, the proposed algorithm is quite competitive in both computational efficiency and classification accuracy.

#index 1665165
#* Localized alternative cluster ensembles for collaborative structuring
#@ Michael Wurst;Katharina Morik;Ingo Mierswa
#t 2006
#c 20
#% 248790
#% 464291
#% 578670
#% 727861
#% 769935
#% 785341
#% 840862
#% 888878
#! Personal media collections are structured in very different ways by different users. Their support by standard clustering algorithms is not sufficient. First, users have their personal preferences which they hardly can express by a formal objective function. Instead, they might want to select among a set of proposed clusterings. Second, users most often do not want hand-made partial structures be overwritten by an automatic clustering. Third, given clusterings of others should not be ignored but used to enhance the own structure. In contrast to other cluster ensemble methods or distributed clustering, a global model (consensus) is not the aim. Hence, we investigate a new learning task, namely learning localized alternative cluster ensembles, where a set of given clusterings is taken into account and a set of proposed clusterings is delivered. This paper proposes an algorithm for solving the new task together with a method for evaluation.

#index 1665166
#* Distributional features for text categorization
#@ Xiao-Bing Xue;Zhi-Hua Zhou
#t 2006
#c 20
#% 169809
#% 262059
#% 266215
#% 266292
#% 280817
#% 311034
#% 344447
#% 458379
#% 722930
#! In previous research of text categorization, a word is usually described by features which express that whether the word appears in the document or how frequently the word appears. Although these features are useful, they have not fully expressed the information contained in the document. In this paper, the distributional features are used to describe a word, which express the distribution of a word in a document. In detail, the compactness of the appearances of the word and the position of the first appearance of the word are characterized as features. These features are exploited by a TFIDF style equation in this paper. Experiments show that the distributional features are useful for text categorization. In contrast to using the traditional term frequency features solely, including the distributional features requires only a little additional cost, while the categorization performance can be significantly improved.

#index 1665167
#* Subspace metric ensembles for semi-supervised clustering of high dimensional data
#@ Bojun Yan;Carlotta Domeniconi
#t 2006
#c 20
#% 464291
#% 464631
#% 565528
#% 579655
#% 633220
#% 715529
#% 722902
#% 727903
#% 769881
#% 770782
#% 770836
#% 840892
#! A critical problem in clustering research is the definition of a proper metric to measure distances between points. Semi-supervised clustering uses the information provided by the user, usually defined in terms of constraints, to guide the search of clusters. Learning effective metrics using constraints in high dimensional spaces remains an open challenge. This is because the number of parameters to be estimated is quadratic in the number of dimensions, and we seldom have enough side-information to achieve accurate estimates. In this paper, we address the high dimensionality problem by learning an ensemble of subspace metrics. This is achieved by projecting the data and the constraints in multiple subspaces, and by learning positive semi-definite similarity matrices therein. This methodology allows leveraging the given side-information while solving lower dimensional problems. We demonstrate experimentally using high dimensional data (e.g., microarray data) the superior accuracy achieved by our method with respect to competitive approaches.

#index 1665168
#* An adaptive kernel method for semi-supervised clustering
#@ Bojun Yan;Carlotta Domeniconi
#t 2006
#c 20
#% 190581
#% 425040
#% 464291
#% 592345
#% 593940
#% 769881
#% 770782
#% 840892
#% 1548501
#! Semi-supervised clustering uses the limited background knowledge to aid unsupervised clustering algorithms. Recently, a kernel method for semi-supervised clustering has been introduced, which has been shown to outperform previous semi-supervised clustering approaches. However, the setting of the kernel's parameter is left to manual tuning, and the chosen value can largely affect the quality of the results. Thus, the selection of kernel's parameters remains a critical and open problem when only limited supervision, provided in terms of pairwise constraints, is available. In this paper, we derive a new optimization criterion to automatically determine the optimal parameter of an RBF kernel, directly from the data and the given constraints. Our approach integrates the constraints into the clustering objective function, and optimizes the parameter of a Gaussian kernel iteratively during the clustering process. Our experimental comparisons and results with simulated and real data clearly demonstrate the effectiveness and advantages of the proposed algorithm.

#index 1665169
#* To select or to weigh: a comparative study of model selection and model weighing for SPODE ensembles
#@ Ying Yang;Geoff Webb;Jesús Cerquides;Kevin Korb;Janice Boughton;Kai Ming Ting
#t 2006
#c 20
#% 101217
#% 246832
#% 312728
#% 321059
#% 375636
#% 420054
#% 458168
#% 458259
#% 466252
#% 486328
#% 486797
#% 502131
#% 643688
#% 799040
#% 843342
#% 876084
#% 1674146
#% 1699581
#! An ensemble of Super-Parent-One-Dependence Estimators (SPODEs) offers a powerful yet simple alternative to naive Bayes classifiers, achieving significantly higher classification accuracy at a moderate cost in classification efficiency. Currently there exist two families of methodologies that ensemble candidate SPODEs for classification. One is to select only helpful SPODEs and uniformly average their probability estimates, a type of model selection. Another is to assign a weight to each SPODE and linearly combine their probability estimates, a methodology named model weighing. This paper presents a theoretical and empirical study comparing model selection and model weighing for ensembling SPODEs. The focus is on maximizing the ensemble's classification accuracy while minimizing its computational time. A number of representative selection and weighing schemes are studied, providing a comprehensive research on this topic and identifying effective schemes that provide alternative trades-off between speed and expected error.

#index 1665170
#* Ensembles of nearest neighbor forecasts
#@ Dragomir Yankov;Dennis DeCoste;Eamonn Keogh
#t 2006
#c 20
#% 132583
#% 146676
#% 229931
#% 534183
#% 642773
#! Nearest neighbor forecasting models are attractive with their simplicity and the ability to predict complex nonlinear behavior. They rely on the assumption that observations similar to the target one are also likely to have similar outcomes. A common practice in nearest neighbor model selection is to compute the globally optimal number of neighbors on a validation set, which is later applied for all incoming queries. For certain queries, however, this number may be suboptimal and forecasts that deviate a lot from the true realization could be produced. To address the problem we propose an alternative approach of training ensembles of nearest neighbor predictors that determine the best number of neighbors for individual queries. We demonstrate that the forecasts of the ensembles improve significantly on the globally optimal single predictors.

#index 1665171
#* Learning process models with missing data
#@ Will Bridewell;Pat Langley;Steve Racunas;Stuart Borrett
#t 2006
#c 20
#% 109848
#% 348816
#% 464625
#% 466754
#% 1269500
#% 1477204
#! In this paper, we review the task of inductive process modeling, which uses domain knowledge to compose explanatory models of continuous dynamic systems. Next we discuss approaches to learning with missing values in time series, noting that these efforts are typically applied for descriptive modeling tasks that use little background knowledge. We also point out that these methods assume that data are missing at random—a condition that may not hold in scientific domains. Using experiments with synthetic and natural data, we compare an expectation maximization approach with one that simply ignores the missing data. Results indicate that expectation maximization leads to more accurate models in most cases, even though its basic assumptions are unmet. We conclude by discussing the implications of our findings along with directions for future work.

#index 1665172
#* Case-Based label ranking
#@ Klaus Brinker;Eyke Hüllermeier
#t 2006
#c 20
#% 191910
#% 290482
#% 330769
#% 770753
#% 801673
#% 1223287
#! Label ranking studies the problem of learning a mapping from instances to rankings over a predefined set of labels. We approach this setting from a case-based perspective and propose a sophisticated k-NN framework as an alternative to previous binary decomposition techniques. It exhibits the appealing property of transparency and is based on an aggregation model which allows one to incorporate a variety of pairwise loss functions on label rankings. In addition to these conceptual advantages, we empirically show that our case-based approach is competitive to state-of-the-art model-based learners with respect to accuracy while being computationally much more efficient. Moreover, our approach suggests a natural way to associate confidence scores with predictions, a property not being shared by previous methods.

#index 1665173
#* Cascade evaluation of clustering algorithms
#@ Laurent Candillier;Isabelle Tellier;Fabien Torre;Olivier Bousquet
#t 2006
#c 20
#% 132938
#% 136350
#% 209021
#% 218961
#% 272995
#% 304305
#% 328946
#% 551723
#% 646003
#% 829043
#% 1306054
#% 1705269
#! This paper is about the evaluation of the results of clustering algorithms, and the comparison of such algorithms. We propose a new method based on the enrichment of a set of independent labeled datasets by the results of clustering, and the use of a supervised method to evaluate the interest of adding such new information to the datasets. We thus adapt the cascade generalization [1] paradigm in the case where we combine an unsupervised and a supervised learner. We also consider the case where independent supervised learnings are performed on the different groups of data objects created by the clustering [2]. We then conduct experiments using different supervised algorithms to compare various clustering algorithms. And we thus show that our proposed method exhibits a coherent behavior, pointing out, for example, that the algorithms based on the use of complex probabilistic models outperform algorithms based on the use of simpler models.

#index 1665174
#* Making good probability estimates for regression
#@ Michael Carney;Pádraig Cunningham
#t 2006
#c 20
#% 392343
#% 422324
#% 1223380
#% 1861461
#! In this paper, we show that the optimisation of density forecasting models for regression in machine learning can be formulated as a multi-objective problem. We describe the two objectives of sharpness and calibration and suggest suitable scoring metrics for both. We use the popular negative log-likelihood as a measure of sharpness and the probability integral transform as a measure of calibration.To optimise density forecasting models under multiple criteria we introduce a multi-objective evolutionary optimisation framework that can produce better density forecasts from a prediction user's perspective. Our experiments show improvements over the state-of-the-art on a risk management problem.

#index 1665175
#* Fast spectral clustering of data using sequential matrix compression
#@ Bo Chen;Bin Gao;Tie-Yan Liu;Yu-Fu Chen;Wei-Ying Ma
#t 2006
#c 20
#% 313959
#% 387427
#% 415163
#% 652071
#! Spectral clustering has attracted much research interest in recent years since it can yield impressively good clustering results. Traditional spectral clustering algorithms first solve an eigenvalue decomposition problem to get the low-dimensional embedding of the data points, and then apply some heuristic methods such as k-means to get the desired clusters. However, eigenvalue decomposition is very time-consuming, making the overall complexity of spectral clustering very high, and thus preventing spectral clustering from being widely applied in large-scale problems. To tackle this problem, different from traditional algorithms, we propose a very fast and scalable spectral clustering algorithm called the sequential matrix compression (SMC) method. In this algorithm, we scale down the computational complexity of spectral clustering by sequentially reducing the dimension of the Laplacian matrix in the iteration steps with very little loss of accuracy. Experiments showed the feasibility and efficiency of the proposed algorithm.

#index 1665176
#* An information-theoretic framework for high-order co-clustering of heterogeneous objects
#@ Antonio D. Chiaravalloti;Gianluigi Greco;Antonella Guzzo;Luigi Pontieri
#t 2006
#c 20
#% 342621
#% 342659
#% 729918
#% 769928
#% 810066
#% 823328
#% 840840
#! The high-order co-clustering problem, i.e., the problem of simultaneously clustering several heterogeneous types of domains, is usually faced by minimizing a linear combination of some optimization functions evaluated over pairs of correlated domains, where each weight expresses the reliability/relevance of the associated contingency table. Clearly enough, accurately choosing these weights is crucial to the effectiveness of the co-clustering, and techniques for their automatic tuning are particularly desirable, which are instead missing in the literature. This paper faces this issue by proposing an information-theoretic framework where the co-clustering problem does not need any explicit weighting scheme for combining pairwise objective functions, while a suitable notion of agreement among these functions is exploited. Based on this notion, an algorithm for co-clustering a “star-structured” collection of domains is defined.

#index 1665177
#* Efficient inference in large conditional random fields
#@ Trevor Cohn
#t 2006
#c 20
#% 464434
#% 815924
#% 816181
#% 816186
#% 818038
#% 823311
#% 840927
#% 840935
#% 854813
#% 858036
#% 938659
#% 939333
#% 1344869
#% 1810385
#! Conditional Random Fields (CRFs) are widely known to scale poorly, particularly for tasks with large numbers of states or with richly connected graphical structures. This is a consequence of inference having a time complexity which is at best quadratic in the number of states. This paper describes a novel parameterisation of the CRF which ties the majority of clique potentials, while allowing individual potentials for a subset of the labellings. This has two beneficial effects: the parameter space of the model (and thus the propensity to over-fit) is reduced, and the time complexity of training and decoding becomes sub-quadratic. On a standard natural language task, we reduce CRF training time four-fold, with no loss in accuracy. We also show how inference can be performed efficiently in richly connected graphs, in which current methods are intractable.

#index 1665178
#* A kernel-based approach to estimating phase shifts between irregularly sampled time series: an application to gravitational lenses
#@ Juan C. Cuevas-Tello;Peter Tiňo;Somak Raychaudhury
#t 2006
#c 20
#% 743284
#! Given two scaled, phase shifted and irregularly sampled noisy realisations of the same process, we attempt to recover the phase shift in this contribution. We suggest a kernel-based method that directly models the underlying process via a linear combination of Gaussian kernels. We apply our method to estimate the phase shift between temporal variations, in the brightness of multiple images of the same distant gravitationally lensed quasar, from irregular but simultaneous observations of all images. In a set of controlled experiments, our method outperforms other state-of-art statistical methods used in astrophysics, in particular in the presence of realistic gaps and Gaussian noise in the data. We apply the method to actual observations (at several optical frequencies) of the doubly imaged quasar Q0957+561. Our estimates at various frequencies are more consistent than those of the currently used methods.

#index 1665179
#* Cost-Sensitive decision tree learning for forensic classification
#@ Jason V. Davis;Jungwoo Ha;Christopher J. Rossbach;Hany E. Ramadan;Emmett Witchel
#t 2006
#c 20
#% 92554
#% 115608
#% 136350
#% 376266
#% 411099
#% 458361
#% 763245
#% 809099
#% 835188
#% 1272369
#% 1273393
#% 1289281
#! In some learning settings, the cost of acquiring features for classification must be paid up front, before the classifier is evaluated. In this paper, we introduce the forensic classification problem and present a new algorithm for building decision trees that maximizes classification accuracy while minimizing total feature costs. By expressing the ID3 decision tree algorithm in an information theoretic context, we derive our algorithm from a well-formulated problem objective. We evaluate our algorithm across several datasets and show that, for a given level of accuracy, our algorithm builds cheaper trees than existing methods. Finally, we apply our algorithm to a real-world system, Clarify. Clarify classifies unknown or unexpected program errors by collecting statistics during program runtime which are then used for decision tree classification after an error has occurred. We demonstrate that if the classifier used by the Clarify system is trained with our algorithm, the computational overhead (equivalently, total feature costs) can decrease by many orders of magnitude with only a slight (

#index 1665180
#* The minimum volume covering ellipsoid estimation in kernel-defined feature spaces
#@ Alexander N. Dolia;Tijl De Bie;Chris J. Harris;John Shawe-Taylor;D. M. Titterington
#t 2006
#c 20
#% 543922
#% 743284
#% 803574
#% 855602
#% 959495
#! Minimum volume covering ellipsoid estimation is important in areas such as systems identification, control, video tracking, sensor management, and novelty detection. It is well known that finding the minimum volume covering ellipsoid (MVCE) reduces to a convex optimisation problem. We propose a regularised version of the MVCE problem, and derive its dual formulation. This makes it possible to apply the MVCE problem in kernel-defined feature spaces. The solution is generally sparse, in the sense that the solution depends on a limited set of points. We argue that the MVCE is a valuable alternative to the minimum volume enclosing hypersphere for novelty detection. It is clearly a less conservative method. Besides this, we can show using statistical learning theory that the probability of a typical point being misidentified as a novelty is generally small. We illustrate our results on real data.

#index 1665181
#* Right of inference: nearest rectangle learning revisited
#@ Byron J. Gao;Martin Ester
#t 2006
#c 20
#% 92555
#% 136350
#% 169672
#% 182682
#% 252403
#% 420084
#% 428413
#! In Nearest Rectangle (NR) learning, training instances are generalized into hyperrectangles and a query is classified according to the class of its nearest rectangle. The method has not received much attention since its introduction mainly because, as a hybrid learner, it does not gain accuracy advantage while sacrificing classification time comparing to some other interpretable eager learners such as decision trees. In this paper, we seek for accuracy improvement of NR learning through controlling the generation of rectangles, so that each of them has the right of inference. Rectangles having the right of inference are compact, conservative, and good for making local decisions. Experiments on benchmark datasets validate the effectiveness of the proposed approach.

#index 1665182
#* Reinforcement learning for MDPs with constraints
#@ Peter Geibel
#t 2006
#c 20
#% 188153
#% 361730
#% 384911
#% 465915
#% 643223
#% 1272072
#% 1279370
#! In this article, I will consider Markov Decision Processes with two criteria, each defined as the expected value of an infinite horizon cumulative return. The second criterion is either itself subject to an inequality constraint, or there is maximum allowable probability that the single returns violate the constraint. I describe and discuss three new reinforcement learning approaches for solving such control problems.

#index 1665183
#* Efficient non-linear control through neuroevolution
#@ Faustino Gomez;Jürgen Schmidhuber;Risto Miikkulainen
#t 2006
#c 20
#% 124691
#% 252329
#% 305081
#% 384911
#% 446024
#% 449980
#% 679770
#% 761985
#% 846487
#% 1650314
#! Many complex control problems are not amenable to traditional controller design. Not only is it difficult to model real systems, but often it is unclear what kind of behavior is required. Reinforcement learning (RL) has made progress through direct interaction with the task environment, but it has been difficult to scale it up to large and partially observable state spaces. In recent years, neuroevolution, the artificial evolution of neural networks, has shown promise in tasks with these two properties. This paper introduces a novel neuroevolution method called CoSyNE that evolves networks at the level of weights. In the most extensive comparison of RL methods to date, it was tested in difficult versions of the pole-balancing problem that involve large state spaces and hidden state. CoSyNE was found to be significantly more efficient and powerful than the other methods on these tasks, forming a promising foundation for solving challenging real-world control tasks.

#index 1665184
#* Efficient prediction-based validation for document clustering
#@ Derek Greene;Pádraig Cunningham
#t 2006
#c 20
#% 36672
#% 118771
#% 300132
#% 393059
#% 443956
#% 763861
#% 875992
#% 995581
#! Recently, stability-based techniques have emerged as a very promising solution to the problem of cluster validation. An inherent drawback of these approaches is the computational cost of generating and assessing multiple clusterings of the data. In this paper we present an efficient prediction-based validation approach suitable for application to large, high-dimensional datasets such as text corpora. We use kernel clustering to isolate the validation procedure from the original data. Furthermore, we employ a prototype reduction strategy that allows us to work on a reduced kernel matrix, leading to significant computational savings. To ensure that this condensed representation accurately reflects the cluster structures in the data, we propose a density-biased strategy to select the reduced prototypes. This novel validation process is evaluated on real-world text datasets, where it is shown to consistently produce good estimates for the optimal number of clusters.

#index 1665185
#* On testing the missing at random assumption
#@ Manfred Jaeger
#t 2006
#c 20
#! Most approaches to learning from incomplete data are based on the assumption that unobserved values are missing at random (mar). While the mar assumption, as such, is not testable, it can become testable in the context of other distributional assumptions, e.g. the naive Bayes assumption. In this paper we investigate a method for testing the mar assumption in the presence of other distributional constraints. We present methods to (approximately) compute a test statistic consisting of the ratio of two profile likelihood functions. This requires the optimization of the likelihood under no assumptions on the missingness mechanism, for which we use our recently proposed AI & M algorithm. We present experimental results on synthetic data that show that our approximate test statistic is a good indicator for whether data is mar relative to the given distributional assumptions.

#index 1665186
#* B-Matching for spectral clustering
#@ Tony Jebara;Vlad Shchogolev
#t 2006
#c 20
#% 25998
#% 313959
#% 341148
#% 604754
#! We propose preprocessing spectral clustering with b-matching to remove spurious edges in the adjacency graph prior to clustering. B-matching is a generalization of traditional maximum weight matching and is solvable in polynomial time. Instead of a permutation matrix, it produces a binary matrix with rows and columns summing to a positive integer b. The b-matching procedure prunes graph edges such that the in-degree and out-degree of each node is b, producing a more balanced variant of k-nearest-neighbor. The combinatorial algorithm optimally solves for the maximum weight subgraph and makes subsequent spectral clustering more stable and accurate. Experiments on standard datasets, visualizations, and video data support the use of b-matching to prune graphs prior to spectral clustering.

#index 1665187
#* Multi-class ensemble-based active learning
#@ Christine Körner;Stefan Wrobel
#t 2006
#c 20
#% 116165
#% 169717
#% 236729
#% 252011
#% 290482
#% 316509
#% 466095
#% 529191
#% 565531
#% 732227
#% 770807
#% 1279286
#% 1699597
#! Ensemble-based active learning has been proven to efficiently reduce the number of training instances and thus the cost of data acquisition. To determine the utility of a candidate training instance, the disagreement about its class value among the ensemble members is used. While the disagreement for binary classification is easily determined using margins, the adaption to multi-class problems is not straightforward and little studied in the literature. In this paper we consider four approaches to measure ensemble disagreement, including margins, uncertainty sampling and entropy, and evaluate them empirically on various ensemble strategies for active learning. We show that margins outperform the other disagreement measures on three of four active learning strategies. Our experiments also show that some active learning strategies are more sensitive to the choice of disagreement measure than others.

#index 1665188
#* Active learning with irrelevant examples
#@ Dominic Mazzoni;Kiri L. Wagstaff;Michael C. Burl
#t 2006
#c 20
#% 170649
#% 191910
#% 197394
#% 714351
#% 722797
#% 1860941
#! Active learning algorithms attempt to accelerate the learning process by requesting labels for the most informative items first. In real-world problems, however, there may exist unlabeled items that are irrelevant to the user's classification goals. Queries about these points slow down learning because they provide no information about the problem of interest. We have observed that when irrelevant items are present, active learning can perform worse than random selection, requiring more time (queries) to achieve the same level of accuracy. Therefore, we propose a novel approach, Relevance Bias, in which the active learner combines its default selection heuristic with the output of a simultaneously trained relevance classifier to favor items that are likely to be both informative and relevant. In our experiments on a real-world problem and two benchmark datasets, the Relevance Bias approach significantly improves the learning rate of three different active learning approaches.

#index 1665189
#* Classification with support hyperplanes
#@ Georgi I. Nalbantov;Jan C. Bioch;Patrick J. F. Groenen
#t 2006
#c 20
#% 190581
#% 209021
#% 309208
#% 314784
#% 420077
#% 723244
#% 732385
#% 926881
#% 1558464
#! A new classification method is proposed, called Support Hyperplanes (SHs). To solve the binary classification task, SHs consider the set of all hyperplanes that do not make classification mistakes, referred to as semi-consistent hyperplanes. A test object is classified using that semi-consistent hyperplane, which is farthest away from it. In this way, a good balance between goodness-of-fit and model complexity is achieved, where model complexity is proxied by the distance between a test object and a semi-consistent hyperplane. This idea of complexity resembles the one imputed in the width of the so-called margin between two classes, which arises in the context of Support Vector Machine learning. Class overlap can be handled via the introduction of kernels and/or slack variables. The performance of SHs against standard classifiers is promising on several widely-used empirical data sets.

#index 1665190
#* (Agnostic) PAC learning concepts in higher-order logic
#@ K. S. Ng
#t 2006
#c 20
#% 163545
#% 172512
#% 175383
#% 243701
#% 278833
#% 387653
#% 398847
#% 449559
#% 643852
#% 1718462
#! This paper studies the PAC and agnostic PAC learnability of some standard function classes in the learning in higher-order logic setting introduced by Lloyd et al. In particular, it is shown that the similarity between learning in higher-order logic and traditional attribute-value learning allows many results from computational learning theory to be ‘ported' to the logical setting with ease. As a direct consequence, a number of non-trivial results in the higher-order setting can be established with straightforward proofs. Our satisfyingly simple analysis provides another case for a more in-depth study and wider uptake of the proposed higher-order logic approach to symbolic machine learning.

#index 1665191
#* Evaluating feature selection for SVMs in high dimensions
#@ Roland Nilsson;José M. Peña;Johan Björkegren;Jesper Tegnér
#t 2006
#c 20
#% 197394
#% 243728
#% 272995
#% 425048
#% 722929
#% 722937
#% 722943
#% 754400
#% 833529
#% 1378405
#% 1861020
#! We perform a systematic evaluation of feature selection (FS) methods for support vector machines (SVMs) using simulated high- dimensional data (up to 5000 dimensions). Several findings previously reported at low dimensions do not apply in high dimensions. For example, none of the FS methods investigated improved SVM accuracy, indicating that the SVM built-in regularization is sufficient. These results were also validated using microarray data. Moreover, all FS methods tend to discard many relevant features. This is a problem for applications such as microarray data analysis, where identifying all biologically important features is a major objective.

#index 1665192
#* Revisiting fisher kernels for document similarities
#@ Martin Nyffenegger;Jean-Cédric Chappelier;Éric Gaussier
#t 2006
#c 20
#% 280819
#% 304917
#% 311027
#% 387427
#% 406493
#% 430758
#% 458369
#% 769895
#! This paper presents a new metric to compute similarities between textual documents, based on the Fisher information kernel as proposed by T. Hofmann. By considering a new point-of-view on the embedding vector space and proposing a more appropriate way of handling the Fisher information matrix, we derive a new form of the kernel that yields significant improvements on an information retrieval task. We apply our approach to two different models: Naive Bayes and PLSI.

#index 1665193
#* Scaling model-based average-reward reinforcement learning for product delivery
#@ Scott Proper;Prasad Tadepalli
#t 2006
#c 20
#% 251784
#% 312787
#% 363744
#% 384911
#% 565550
#% 573671
#% 773335
#! Reinforcement learning in real-world domains suffers from three curses of dimensionality: explosions in state and action spaces, and high stochasticity. We present approaches that mitigate each of these curses. To handle the state-space explosion, we introduce “tabular linear functions” that generalize tile-coding and linear value functions. Action space complexity is reduced by replacing complete joint action space search with a form of hill climbing. To deal with high stochasticity, we introduce a new algorithm called ASH-learning, which is an afterstate version of H-Learning. Our extensions make it practical to apply reinforcement learning to a domain of product delivery – an optimization problem that combines inventory control and vehicle routing.

#index 1665194
#* Robust probabilistic calibration
#@ Stefan Rüping
#t 2006
#c 20
#% 34077
#% 577298
#% 840913
#% 961134
#! Probabilistic calibration is the task of producing reliable estimates of the conditional class probability P(class | observation) from the outputs of numerical classifiers. A recent comparative study [1] revealed that Isotonic Regression [2] and Platt Calibration [3] are most effective probabilistic calibration technique for a wide range of classifiers. This paper will demonstrate that these methods are sensitive to outliers in the data. An improved calibration method will be introduced that combines probabilistic calibration with methods from the field of robust statistics [4]. It will be shown that the integration of robustness concepts can significantly improve calibration performance.

#index 1665195
#* Missing data in kernel PCA
#@ Guido Sanguinetti;Neil D. Lawrence
#t 2006
#c 20
#% 257039
#% 380342
#% 493731
#% 916787
#! Kernel Principal Component Analysis (KPCA) is a widely used technique for visualisation and feature extraction. Despite its success and flexibility, the lack of a probabilistic interpretation means that some problems, such as handling missing or corrupted data, are very hard to deal with. In this paper we exploit the probabilistic interpretation of linear PCA together with recent results on latent variable models in Gaussian Processes in order to introduce an objective function for KPCA. This in turn allows a principled approach to the missing data problem. Furthermore, this new approach can be extended to reconstruct corrupted test data using fixed kernel feature extractors. The experimental results show strong improvements over widely used heuristics.

#index 1665196
#* Exploiting extremely rare features in text categorization
#@ Péter Schönhofen;András A. Benczúr
#t 2006
#c 20
#% 46809
#% 194289
#% 198058
#% 375017
#% 413637
#% 458379
#% 465754
#% 577285
#% 649299
#% 740407
#% 740600
#% 763708
#% 775989
#% 810771
#% 1305656
#! One of the first steps of document classification, clustering and many other information retrieval tasks is to discard words occurring only a few times in the corpus, based on the assumption that they have little contribution to the bag of words representation. However, as we will show, rare n-grams and other similar features are able to indicate surprisingly well if two documents belong to the same category, and thus can aid classification. In our experiments over four corpora, we found that while keeping the size of the training set constant, 5-25% of the test set can be classified essentially for free based on rare features without any loss of accuracy, even experiencing an improvement of 0.6-1.6%.

#index 1665197
#* Efficient large scale linear programming support vector machines
#@ Suvrit Sra
#t 2006
#c 20
#% 269218
#% 269225
#% 309208
#% 382854
#% 763708
#% 961189
#% 1558464
#! This paper presents a decomposition method for efficiently constructing ℓ1-norm Support Vector Machines (SVMs). The decomposition algorithm introduced in this paper possesses many desirable properties. For example, it is provably convergent, scales well to large datasets, is easy to implement, and can be extended to handle support vector regression and other SVM variants. We demonstrate the efficiency of our algorithm by training on (dense) synthetic datasets of sizes up to 20 million points (in ℝ32). The results show our algorithm to be several orders of magnitude faster than a previously published method for the same task. We also present experimental results on real data sets—our method is seen to be not only very fast, but also highly competitive against the leading SVM implementations.

#index 1665198
#* An efficient approximation to lookahead in relational learners
#@ Jan Struyf;Jesse Davis;David Page
#t 2006
#c 20
#% 136350
#% 392781
#% 770756
#% 840923
#% 875974
#% 1271968
#% 1290272
#% 1699582
#! Greedy machine learning algorithms suffer from shortsightedness, potentially returning suboptimal models due to limited exploration of the search space. Greedy search misses useful refinements that yield a significant gain only in conjunction with other conditions. Relational learners, such as inductive logic programming algorithms, are especially susceptible to this problem. Lookahead helps greedy search overcome myopia; unfortunately it causes an exponential increase in execution time. Furthermore, it may lead to overfitting. We propose a heuristic for greedy relational learning algorithms that can be seen as an efficient, limited form of lookahead. Our experimental evaluation shows that the proposed heuristic yields models that are as accurate as models generated using lookahead. It is also considerably faster than lookahead.

#index 1665199
#* Improvement of systems management policies using hybrid reinforcement learning
#@ Gerald Tesauro;Nicholas K. Jong;Rajarshi Das;Mohamed N. Bennani
#t 2006
#c 20
#% 307843
#% 384911
#% 820423
#% 840835
#% 1142413
#% 1269499
#% 1272005
#! Reinforcement Learning (RL) holds particular promise in an emerging application domain of performance management of computing systems. In recent work, online RL yielded effective server allocation policies in a prototype Data Center, without explicit system models or built-in domain knowledge. This paper presents a substantially improved and more practical “hybrid” approach, in which RL trains offline on data collected while a queuing-theoretic policy controls the system. This approach avoids potentially poor performance in live online training. Additionally we use nonlinear function approximators instead of tabular value functions; this greatly improves scalability, and surprisingly, eliminated the need for exploratory actions. In experiments using both open-loop and closed-loop traffic as well as large switching delays, our results show significant performance improvement over state-of-art queuing model policies.

#index 1665200
#* Diversified SVM ensembles for large data sets
#@ Ivor W. Tsang;Andras Kocsor;James T. Kwok
#t 2006
#c 20
#% 276511
#% 451221
#% 722758
#% 771838
#% 791368
#% 803575
#% 840949
#% 1699590
#! Recently, the core vector machine (CVM) has shown significant speedups on classification and regression problems with massive data sets. Its performance is also almost as accurate as other state-of-the-art SVM implementations. By incorporating the orthogonality constraints to diversify the CVM ensembles, this turns out to speed up the maximum margin discriminant analysis (MMDA) algorithm. Extensive comparisons with the MMDA ensemble along with bagging on a number of large data sets show that the proposed diversified CVM ensemble can improve classification performance, and is also faster than the original MMDA algorithm by more than an order of magnitude.

#index 1665201
#* Dynamic integration with random forests
#@ Alexey Tsymbal;Mykola Pechenizkiy;Pádraig Cunningham
#t 2006
#c 20
#% 160862
#% 290482
#% 342617
#% 400847
#% 424997
#% 478291
#% 1272304
#% 1289491
#! Random Forests (RF) are a successful ensemble prediction technique that uses majority voting or averaging as a combination function. However, it is clear that each tree in a random forest may have a different contribution in processing a certain instance. In this paper, we demonstrate that the prediction performance of RF may still be improved in some domains by replacing the combination function with dynamic integration, which is based on local performance estimates. Our experiments also demonstrate that the RF Intrinsic Similarity is better than the commonly used Heterogeneous Euclidean/Overlap Metric in finding a neighbourhood for local estimates in the context of dynamic integration of classification random forests.

#index 1665202
#* Bagging using statistical queries
#@ Anneleen Van Assche;Hendrik Blockeel
#t 2006
#c 20
#% 36280
#% 136350
#% 152934
#% 209021
#% 449588
#% 722912
#% 926881
#% 1272326
#! Bagging is an ensemble method that relies on random resampling of a data set to construct models for the ensemble. When only statistics about the data are available, but no individual examples, the straightforward resampling procedure cannot be implemented. The question is then whether bagging can somehow be simulated. In this paper we propose a method that, instead of computing certain heuristics (such as information gain) from a resampled version of the data, estimates the probability distribution of these heuristics under random resampling, and then samples from this distribution. The resulting method is not entirely equivalent to bagging because it ignores certain dependencies among statistics. Nevertheless, experiments show that this “simulated bagging” yields similar accuracy as bagging, while being as efficient and more generally applicable.

#index 1665203
#* Guiding the search in the NO region of the phase transition problem with a partial subsumption test
#@ Samuel Wieczorek;Gilles Bisson;Mirta B. Gordon
#t 2006
#c 20
#% 136362
#% 210191
#% 302388
#% 425004
#% 723257
#% 723258
#% 748639
#% 1478471
#! We introduce a test, named π-subsumption, which computes partial subsumptions between a hypothesis h and an example e, as well as a measure, the subsumption index, which quantifies the covering degree between h and e. The behavior of this measure is studied on the phase transition problem.

#index 1665204
#* Spline embedding for nonlinear dimensionality reduction
#@ Shiming Xiang;Feiping Nie;Changshui Zhang;Chunxia Zhang
#t 2006
#c 20
#% 57628
#% 593047
#% 790049
#% 1502529
#! This paper presents a new algorithm for nonlinear dimensionality reduction (NLDR). Smoothing splines are used to map the locally-coordinatized data points into a single global coordinate system of lower dimensionality. In this work setting, we can achieve two goals. First, a global embedding is obtained by minimizing the low-dimensional coordinate reconstruction error. Second, the NLDR algorithm can be naturally extended to deal with out-of-sample data points. Experimental results illustrate the validity of our method.

#index 1665205
#* Cost-Sensitive learning of SVM for ranking
#@ Jun Xu;Yunbo Cao;Hang Li;Yalou Huang
#t 2006
#c 20
#% 169777
#% 262096
#% 280437
#% 309095
#% 342611
#% 387427
#% 438557
#% 458361
#% 458623
#% 766414
#% 769875
#% 807296
#% 840846
#% 840853
#% 939451
#% 1289281
#! In this paper, we propose a new method for learning to rank. ‘Ranking SVM' is a method for performing the task. It formulizes the problem as that of binary classification on instance pairs and performs the classification by means of Support Vector Machines (SVM). In Ranking SVM, the losses for incorrect classifications of instance pairs between different rank pairs are defined as the same. We note that in many applications such as information retrieval the negative effects of making errors between higher ranks and lower ranks are larger than making errors among lower ranks. Therefore, it is natural to bring in the idea of cost-sensitive learning to learning to rank, or more precisely, to set up different losses for misclassification of instance pairs between different rank pairs. Given a cost-sensitive loss function we can construct a Ranking SVM model on the basis of the loss function. Simulation results show that our method works better than Ranking SVM in practical settings of ranking. Experimental results also indicate that our method can outperform existing methods including Ranking SVM on real information retrieval tasks such as document search and definition search.

#index 1665206
#* Variational bayesian dirichlet-multinomial allocation for exponential family mixtures
#@ Shipeng Yu;Kai Yu;Volker Tresp;Hans-Peter Kriegel
#t 2006
#c 20
#% 275111
#% 303620
#% 770861
#% 959438
#! This paper studies a Bayesian framework for density modeling with mixture of exponential family distributions. Variational Bayesian Dirichlet-Multinomial allocation (VBDMA) is introduced, which performs inference and learning efficiently using variational Bayesian methods and performs automatic model selection. The model is closely related to Dirichlet process mixture models and demonstrates similar automatic model selection in the variational Bayesian context.

#index 1699569
#* Proceedings of the 16th European conference on Machine Learning
#@ João Gama;Rui Camacho;Pavel B. Brazdil;Alípio Mário Jorge;Luís Torgo
#t 2005
#c 20

#index 1699570
#* Data analysis in the life sciences — sparking ideas —
#@ Michael R. Berthold
#t 2005
#c 20
#% 998549
#% 1346862
#! Data from various areas of Life Sciences have increasingly caught the attention of data mining and machine learning researchers. Not only is the amount of data available mind-boggling but the diverse and heterogenous nature of the information is far beyond any other data analysis problem so far. In sharp contrast to classical data analysis scenarios, the life science area poses challenges of a rather different nature for mainly two reasons. Firstly, the available data stems from heterogenous information sources of varying degrees of reliability and quality and is, without the interactive, constant interpretation of a domain expert, not useful. Furthermore, predictive models are of only marginal interest to those users – instead they hope for new insights into a complex, biological system that is only partially represented within that data anyway. In this scenario, the data serves mainly to create new insights and generate new ideas that can be tested. Secondly, the notion of feature space and the accompanying measures of similarity cannot be taken for granted. Similarity measures become context dependent and it is often the case that within one analysis task several different ways of describing the objects of interest or measuring similarity between them matter. Some more recently published work in the data analysis area has started to address some of these issues. For example, data analysis in parallel universes [1], that is, the detection of patterns of interest in various di.erent descriptor spaces at the same time, and mining of frequent, discriminative fragments in large, molecular data bases [2]. In both cases, sheer numerical performance is not the focus; it is rather the discovery of interpretable pieces of evidence that lights up new ideas in the users mind. Future work in data analysis in the life sciences needs to keep this in mind: the goal is to trigger new ideas and stimulate interesting associations.

#index 1699571
#* Machine learning for natural language processing (and vice versa?)
#@ Claire Cardie
#t 2005
#c 20
#! Over the past 10-15 years, the influence of methods from machine learning has transformed the way that research is done in the field of natural language processing. This talk will begin by covering the history of this transformation. In particular, learning methods have proved successful in producing stand-alone text-processing components to handle a number of linguistic tasks. Moreover, these components can be combined to produce systems that exhibit shallow text-understanding capabilities: they can, for example, extract key facts from unrestricted documents in limited domains or find answers to general-purpose questions from open-domain document collections. I will briefly describe the state of the art for these practical text-processing applications, focusing on the important role that machine learning methods have played in their development. The second part of the talk will explore the role that natural language processing might play in machine learning research. Here, I will explain the kinds of text-based features that are relatively easy to incorporate into machine learning data sets. In addition, I'll outline some problems from natural language processing that require, or could at least benefit from, new machine learning algorithms.

#index 1699572
#* Statistical relational learning: an inductive logic programming perspective
#@ Luc De Raedt
#t 2005
#c 20
#% 147677
#% 243701
#% 496116
#% 550743
#% 550745
#% 577225
#% 676365
#% 731606
#% 1269477
#% 1269484
#% 1272388
#% 1650280
#! In the past few years there has been a lot of work lying at the intersection of probability theory, logic programming and machine learning [14,18,13,9,6,1,11]. This work is known under the names of statistical relational learning [7,5], probabilistic logic learning [4], or probabilistic inductive logic programming. Whereas most of the existing works have started from a probabilistic learning perspective and extended probabilistic formalisms with relational aspects, I shall take a di.erent perspective, in which I shall start from inductive logic programming and study how inductive logic programming formalisms, settings and techniques can be extended to deal with probabilistic issues. This tradition has already contributed a rich variety of valuable formalisms and techniques, including probabilistic Horn abduction by David Poole, PRISMs by Sato, stochastic logic programs by Muggleton [13] and Cussens [2], Bayesian logic programs [10,8] by Kersting and De Raedt, and Logical Hidden Markov Models [11]. The main contribution of this talk is the introduction of three probabilistic inductive logic programming settings which are derived from the learning from entailment, from interpretations and from proofs settings of the field of inductive logic programming [3]. Each of these settings contributes di.erent notions of probabilistic logic representations, examples and probability distributions. The first setting, probabilistic learning from entailment, is incorporated in the wellknown PRISM system [19] and Cussens's Failure Adjusted Maximisation approach to parameter estimation in stochastic logic programs [2]. A novel system that was recently developed and that fits this paradigm is the nFOIL system [12]. It combines key principles of the well-known inductive logic programming system FOIL [15] with the naïve Bayes' appraoch. In probabilistic learning from entailment, examples are ground facts that should be probabilistically entailed by the target logic program. The second setting, probabilistic learning from interpretations, is incorporated in Bayesian logic programs [10,8], which integrate Bayesian networks with logic programs. This setting is also adopted by [6]. Examples in this setting are Herbrand interpretations that should be a probabilistic model for the target theory. The third setting, learning from proofs [17], is novel. It is motivated by the learning of stochastic context free grammars from tree banks. In this setting, examples are proof trees that should be probabilistically provable from the unknown stochastic logic programs. The sketched settings (and their instances presented) are by no means the only possible settings for probabilistic inductive logic programming, but still – I hope – provide useful insights into the state-of-the-art of this exciting field. For a full survey of statistical relational learning or probabilistic inductive logic programming, the author would like to refer to [4], and for more details on the probabilistic inductive logic programming settings to [16], where a longer and earlier version of this contribution can be found.

#index 1699573
#* Recent advances in mining time series data
#@ Eamonn Keogh
#t 2005
#c 20
#% 577221
#% 727900
#% 769896
#% 993965
#! Much of the world's supply of data is in the form of time series. Furthermore, as we shall see, many types of data can be meaningfully converted into ”time series”, including text, DNA, video, images etc. The last decade has seen an explosion of interest in mining time series data from the academic community. There has been significant work on algorithms to classify, cluster, segment, index, discover rules, visualize, and detect anomalies/novelties in time series. In this talk I will summarize the latest advances in mining time series data, including: – New representations of time series data. – New algorithms/definitions. – The migration from static problems to online problems. – New areas and applications of time series data mining. I will end the talk with a discussion of “what's left to do” in time series data mining.

#index 1699574
#* Focus the mining beacon: lessons and challenges from the world of e-commerce
#@ Ron Kohavi
#t 2005
#c 20
#! Electronic Commerce is now entering its second decade, with Amazon.com and eBay now in existence for ten years. With massive amounts of data, an actionable domain, and measurable ROI, multiple companies use data mining and knowledge discovery to understand their customers and improve interactions.We present important lessons and challenges using e-commerce examples across two dimensions: (i) business-level to technical, and (ii) the mining lifecycle from data collection, data warehouse construction, to discovery and deployment. Many of the lessons and challenges are applicable to domains outside e-commerce.

#index 1699575
#* Data streams and data synopses for massive data sets
#@ Yossi Matias
#t 2005
#c 20
#% 278835
#% 293714
#% 341100
#% 378388
#! With the proliferation of data intensive applications, it has become necessary to develop new techniques to handle massive data sets. Traditional algorithmic techniques and data structures are not always suitable to handle the amount of data that is required and the fact that the data often streams by and cannot be accessed again. A field of research established over the past decade is that of handling massive data sets using data synopses, and developing algorithmic techniques for data stream models. We will discuss some of the research work that has been done in the field, and provide a decades' perspective to data synopses and data streams.

#index 1699576
#* Clustering and metaclustering with nonnegative matrix decompositions
#@ Liviu Badea
#t 2005
#c 20
#% 374537
#% 400277
#% 424997
#% 466083
#% 469422
#! Although very widely used in unsupervised data mining, most clustering methods are affected by the instability of the resulting clusters w.r.t. the initialization of the algorithm (as e.g. in k-means). Here we show that this problem can be elegantly and efficiently tackled by meta-clustering the clusters produced in several different runs of the algorithm, especially if “soft” clustering algorithms (such as Nonnegative Matrix Factorization) are used both at the object- and the meta-level. The essential difference w.r.t. other meta-clustering approaches consists in the fact that our algorithm detects frequently occurring sub-clusters (rather than complete clusters) in the various runs, which allows it to outperform existing algorithms. Additionally, we show how to perform two-way meta-clustering, i.e. take both object and sample dimensions of clusters simultaneously into account, a feature which is essential e.g. for biclustering gene expression data, but has not been considered before.

#index 1699577
#* A SAT-based version space algorithm for acquiring constraint satisfaction problems
#@ Christian Bessiere;Remi Coletta;Frédéric Koriche;Barry O'Sullivan
#t 2005
#c 20
#% 44625
#% 388218
#% 521206
#% 534343
#% 771604
#% 787003
#% 1478476
#% 1478805
#! Constraint programming is rapidly becoming the technology of choice for modelling and solving complex combinatorial problems. However, users of this technology need significant expertise in order to model their problems appropriately. The lack of availability of such expertise is a significant bottleneck to the broader uptake of constraint technology in the real world. We present a new SAT-based version space algorithm for acquiring constraint satisfaction problems from examples of solutions and non-solutions of a target problem. An important advantage is the ease with which domain-specific knowledge can be exploited using the new algorithm. Finally, we empirically demonstrate the algorithm and the effect of exploiting domain-specific knowledge on improving the quality of the acquired constraint network.

#index 1699578
#* Estimation of mixture models using Co-EM
#@ Steffen Bickel;Tobias Scheffer
#t 2005
#c 20
#% 252011
#% 316509
#% 464466
#% 464777
#% 565531
#% 642990
#% 722927
#% 748550
#% 770772
#% 785334
#% 815908
#% 1289496
#% 1699578
#! We study estimation of mixture models for problems in which multiple views of the instances are available. Examples of this setting include clustering web pages or research papers that have intrinsic (text) and extrinsic (references) attributes. Our optimization criterion quantifies the likelihood and the consensus among models in the individual views; maximizing this consensus minimizes a bound on the risk of assigning an instance to an incorrect mixture component. We derive an algorithm that maximizes this criterion. Empirically, we observe that the resulting clustering method incurs a lower cluster entropy than regular EM for web pages, research papers, and many text collections.

#index 1699579
#* Nonrigid embeddings for dimensionality reduction
#@ Matthew Brand
#t 2005
#c 20
#% 415163
#% 770767
#! Spectral methods for embedding graphs and immersing data manifolds in low-dimensional spaces are notoriously unstable due to insufficient and/or numerically ill-conditioned constraint sets. Why show why this is endemic to spectral methods, and develop low-complexity solutions for stiffening ill-conditioned problems and regularizing ill-posed problems, with proofs of correctness. The regularization exploits sparse but complementary constraints on affine rigidity and edge lengths to obtain isometric embeddings. An implemented algorithm is fast, accurate, and industrial-strength: Experiments with problem sizes spanning four orders of magnitude show O(N) scaling. We demonstrate with speech data.

#index 1699580
#* Multi-view discriminative sequential learning
#@ Ulf Brefeld;Christoph Büscher;Tobias Scheffer
#t 2005
#c 20
#% 252011
#% 316509
#% 464434
#% 464466
#% 464777
#% 466892
#% 748550
#% 770759
#% 770763
#% 770772
#% 770855
#% 788082
#% 815908
#% 815924
#! Discriminative learning techniques for sequential data have proven to be more effective than generative models for named entity recognition, information extraction, and other tasks of discrimination. However, semi-supervised learning mechanisms that utilize inexpensive unlabeled sequences in addition to few labeled sequences – such as the Baum-Welch algorithm – are available only for generative models. The multi-view approach is based on the principle of maximizing the consensus among multiple independent hypotheses; we develop this principle into a semi-supervised hidden Markov perceptron, and a semi-supervised hidden Markov support vector learning algorithm. Experiments reveal that the resulting procedures utilize unlabeled data effectively and discriminate more accurately than their purely supervised counterparts.

#index 1699581
#* Robust bayesian linear classifier ensembles
#@ Jesús Cerquides;Ramon López de Mántaras
#t 2005
#c 20
#% 83116
#% 246832
#% 290482
#% 321059
#% 349550
#% 466583
#% 551723
#% 709446
#% 722753
#% 734911
#% 770761
#% 793238
#% 799040
#% 810947
#% 810948
#% 810949
#% 1272397
#% 1650623
#! Ensemble classifiers combine the classification results of several classifiers. Simple ensemble methods such as uniform averaging over a set of models usually provide an improvement over selecting the single best model. Usually probabilistic classifiers restrict the set of possible models that can be learnt in order to lower computational complexity costs. In these restricted spaces, where incorrect modeling assumptions are possibly made, uniform averaging sometimes performs even better than bayesian model averaging. Linear mixtures over sets of models provide an space that includes uniform averaging as a particular case. We develop two algorithms for learning maximum a posteriori weights for linear mixtures, based on expectation maximization and on constrained optimizition. We provide a nontrivial example of the utility of these two algorithms by applying them for one dependence estimators. We develop the conjugate distribution for one dependence estimators and empirically show that uniform averaging is clearly superior to Bayesian model averaging for this family of models. After that we empirically show that the maximum a posteriori linear mixture weights improve accuracy significantly over uniform aggregation.

#index 1699582
#* An integrated approach to learning bayesian networks of rules
#@ Jesse Davis;Elizabeth Burnside;Inês de Castro Dutra;David Page;Vítor Santos Costa
#t 2005
#c 20
#% 246832
#% 550390
#% 550732
#% 564956
#% 729982
#% 1269484
#% 1289459
#% 1289482
#% 1393855
#! Inductive Logic Programming (ILP) is a popular approach for learning rules for classification tasks. An important question is how to combine the individual rules to obtain a useful classifier. In some instances, converting each learned rule into a binary feature for a Bayes net learner improves the accuracy compared to the standard decision list approach [3,4,14]. This results in a two-step process, where rules are generated in the first phase, and the classifier is learned in the second phase. We propose an algorithm that interleaves the two steps, by incrementally building a Bayes net during rule learning. Each candidate rule is introduced into the network, and scored by whether it improves the performance of the classifier. We call the algorithm SAYU for Score As You Use. We evaluate two structure learning algorithms Naïve Bayes and Tree Augmented Naïve Bayes. We test SAYU on four different datasets and see a significant improvement in two out of the four applications. Furthermore, the theories that SAYU learns tend to consist of far fewer rules than the theories in the two-step approach.

#index 1699583
#* Thwarting the nigritude ultramarine: learning to identify link spam
#@ Isabel Drost;Tobias Scheffer
#t 2005
#c 20
#% 268079
#% 269217
#% 309749
#% 565488
#% 577370
#% 728115
#% 753897
#% 769885
#% 772018
#% 807297
#% 1016177
#% 1279489
#! The page rank of a commercial web site has an enormous economic impact because it directly influences the number of potential customers that find the site as a highly ranked search engine result. Link spamming – inflating the page rank of a target page by artificially creating many referring pages – has therefore become a common practice. In order to maintain the quality of their search results, search engine providers try to oppose efforts that decorrelate page rank and relevance and maintain blacklists of spamming pages while spammers, at the same time, try to camouflage their spam pages. We formulate the problem of identifying link spam and discuss a methodology for generating training data. Experiments reveal the effectiveness of classes of intrinsic and relational attributes and shed light on the robustness of classifiers against obfuscation of attributes by an adversarial spammer. We identify open research problems related to web spam.

#index 1699584
#* Rotational prior knowledge for SVMs
#@ Arkady Epshteyn;Gerald DeJong
#t 2005
#c 20
#% 190581
#% 227144
#% 260001
#% 458379
#% 466576
#% 769908
#% 770810
#% 793236
#% 1051487
#% 1705517
#% 1809459
#! Incorporation of prior knowledge into the learning process can significantly improve low-sample classification accuracy. We show how to introduce prior knowledge into linear support vector machines in form of constraints on the rotation of the normal to the separating hyperplane. Such knowledge frequently arises naturally, e.g., as inhibitory and excitatory influences of input variables. We demonstrate that the generalization ability of rotationally-constrained classifiers is improved by analyzing their VC and fat-shattering dimensions. Interestingly, the analysis shows that large-margin classification framework justifies the use of stronger prior knowledge than the traditional VC framework. Empirical experiments with text categorization and political party affiliation prediction confirm the usefulness of rotational prior knowledge.

#index 1699585
#* On the learnability of abstraction theories from observations for relational learning
#@ Stefano Ferilli;Teresa M. A. Basile;Nicola Di Mauro;Floriana Esposito
#t 2005
#c 20
#% 46803
#% 73005
#% 90029
#% 101956
#% 185232
#% 1273378
#! The most common methodology in symbolic learning consists in inducing, given a set of observations, a general concept definition. It is widely known that the choice of the proper description language for a learning problem can affect the efficacy and effectiveness of the learning task. Furthermore, most real-world domains are affected by various kinds of imperfections in data, such as inappropriateness of the description language which does not contain/facilitate an exact representation of the target concept. To deal with such kind of situations, Machine Learning approaches moved from a framework exploiting a single inference mechanism, such as induction, towards one integrating multiple inference strategies such as abstraction. The literature so far assumed that the information needed to the learning systems to apply additional inference strategies is provided by a domain expert. The goal of this work is the automatic inference of such information. The effectiveness of the proposed method was tested by providing the generated abstraction theories to the learning system INTHELEX as a background knowledge to exploit its abstraction capabilities. Various experiments were carried out on the real-world application domain of scientific paper documents, showing the validity of the approach.

#index 1699586
#* Beware the null hypothesis: critical value tables for evaluating classifiers
#@ George Forman;Ira Cohen
#t 2005
#c 20
#% 307109
#! Scientists regularly decide the statistical significance of their findings by determining whether they can, with sufficient confidence, rule out the possibility that their findings could be attributed to random variation—the ‘null hypothesis.' For this, they rely on tables with critical values pre-computed for the normal distribution, the t-distribution, etc. This paper provides such tables (and methods for generating them) for the performance metrics of binary classification: accuracy, F-measure, area under the ROC curve (AUC), and true positives in the top ten. Given a test set of a certain size, the tables provide the critical value for accepting or rejecting the null hypothesis that the score of the best classifier would be consistent with taking the best of a set of random classifiers. The tables are appropriate to consult when a researcher, practitioner or contest manager selects the best of many classifiers measured against a common test set. The risk of the null hypothesis is especially high when there is a shortage of positives or negatives in the testing set (irrespective of the training set size), as is the case for many medical and industrial classification tasks with highly skewed class distributions.

#index 1699587
#* Kernel basis pursuit
#@ Vincent Guigue;Alain Rakotomamonjy;Stéphane Canu
#t 2005
#c 20
#% 17145
#% 185955
#% 274586
#% 425062
#% 722932
#% 854484
#! Estimating a non-uniformly sampled function from a set of learning points is a classical regression problem. Kernel methods have been widely used in this context, but every problem leads to two major tasks: optimizing the kernel and setting the fitness-regularization compromise. This article presents a new method to estimate a function from noisy learning points in the context of RKHS (Reproducing Kernel Hilbert Space). We introduce the Kernel Basis Pursuit algorithm, which enables us to build a ℓ1-regularized-multiple-kernel estimator. The general idea is to decompose the function to learn on a sparse-optimal set of spanning functions. Our implementation relies on the Least Absolute Shrinkage and Selection Operator (LASSO) formulation and on the Least Angle Regression (LARS) solver. The computation of the full regularization path, through the LARS, will enable us to propose new adaptive criteria to find an optimal fitness-regularization compromise. Finally, we aim at proposing a fast parameter-free method to estimate non-uniform-sampled functions.

#index 1699588
#* Hybrid algorithms with instance-based classification
#@ Iris Hendrickx;Antal van den Bosch
#t 2005
#c 20
#% 92533
#% 140588
#% 209023
#% 229972
#% 243728
#% 490127
#% 747907
#! In this paper we aim to show that instance-based classification can replace the classifier component of a rule learner and of maximum-entropy modeling, thereby improving the generalization accuracy of both algorithms. We describe hybrid algorithms that combine rule learning models and maximum-entropy modeling with instance-based classification. Experimental results show that both hybrids are able to outperform the parent algorithm. We analyze and compare the overlap in errors and the statistical bias and variance of the hybrids, their parent algorithms, and a plain instance-based learner. We observe that the successful hybrid algorithms have a lower statistical bias component in the error than their parent algorithms; the fewer errors they make are also less systematic.

#index 1699589
#* Learning and classifying under hard budgets
#@ Aloak Kapoor;Russell Greiner
#t 2005
#c 20
#% 203337
#% 277480
#% 280406
#% 447606
#% 576214
#% 785413
#% 788079
#% 829983
#% 1272000
#% 1272369
#% 1673023
#! Since resources for data acquisition are seldom infinite, both learners and classifiers must act intelligently under hard budgets. In this paper, we consider problems in which feature values are unknown to both the learner and classifier, but can be acquired at a cost. Our goal is a learner that spends its fixed learning budget bL acquiring training data, to produce the most accurate “active classifier” that spends at most bC per instance. To produce this fixed-budget classifier, the fixed-budget learner must sequentially decide which feature values to collect to learn the relevant information about the distribution. We explore several approaches the learner can take, including the standard “round robin” policy (purchasing every feature of every instance until the bL budget is exhausted). We demonstrate empirically that round robin is problematic (especially for small bL), and provide alternate learning strategies that achieve superior performance on a variety of datasets.

#index 1699590
#* Training support vector machines with multiple equality constraints
#@ Wolf Kienzle;Bernhard Schölkopf
#t 2005
#c 20
#% 269218
#% 304942
#% 466087
#% 669214
#% 722757
#% 796238
#% 818170
#% 855583
#% 1558464
#% 1860545
#! In this paper we present a primal-dual decomposition algorithm for support vector machine training. As with existing methods that use very small working sets (such as Sequential Minimal Optimization (SMO), Successive Over-Relaxation (SOR) or the Kernel Adatron (KA)), our method scales well, is straightforward to implement, and does not require an external QP solver. Unlike SMO, SOR and KA, the method is applicable to a large number of SVM formulations regardless of the number of equality constraints involved. The effectiveness of our algorithm is demonstrated on a more difficult SVM variant in this respect, namely semi-parametric support vector regression.

#index 1699591
#* A model based method for automatic facial expression recognition
#@ Hans van Kuilenburg;Marco Wiering;Marten den Uyl
#t 2005
#c 20
#% 247890
#% 279841
#% 318793
#% 361100
#% 481741
#! Automatic facial expression recognition is a research topic with interesting applications in the field of human-computer interaction, psychology and product marketing. The classification accuracy for an automatic system which uses static images as input is however largely limited by the image quality, lighting conditions and the orientation of the depicted face. These problems can be partially overcome by using a holistic model based approach called the Active Appearance Model. A system will be described that can classify expressions from one of the emotional categories joy, anger, sadness, surprise, fear and disgust with remarkable accuracy. It is also able to detect smaller, local facial features based on minimal muscular movements described by the Facial Action Coding System (FACS). Finally, we show how the system can be used for expression analysis and synthesis.

#index 1699592
#* Margin-sparsity trade-off for the set covering machine
#@ François Laviolette;Mario Marchand;Mohak Shah
#t 2005
#c 20
#% 116149
#% 201259
#% 256118
#% 269225
#% 562940
#% 722916
#% 722932
#% 763706
#% 803574
#% 829009
#% 1810705
#! We propose a new learning algorithm for the set covering machine and a tight data-compression risk bound that the learner can use for choosing the appropriate tradeoff between the sparsity of a classifier and the magnitude of its separating margin.

#index 1699593
#* Learning from positive and unlabeled examples with different data distributions
#@ Xiao-Li Li;Bing Liu
#t 2005
#c 20
#% 169717
#% 190581
#% 252011
#% 280817
#% 311027
#% 464604
#% 464631
#% 464641
#% 466888
#% 564957
#% 577235
#% 727829
#% 727883
#% 729927
#% 770821
#% 770858
#% 770870
#% 1279298
#! We study the problem of learning from positive and unlabeled examples. Although several techniques exist for dealing with this problem, they all assume that positive examples in the positive set P and the positive examples in the unlabeled set U are generated from the same distribution. This assumption may be violated in practice. For example, one wants to collect all printer pages from the Web. One can use the printer pages from one site as the set P of positive pages and use product pages from another site as U. One wants to classify the pages in U into printer pages and non-printer pages. Although printer pages from the two sites have many similarities, they can also be quite different because different sites often present similar products in different styles and have different focuses. In such cases, existing methods perform poorly. This paper proposes a novel technique A-EM to deal with the problem. Experiment results with product page classification demonstrate the effectiveness of the proposed technique.

#index 1699594
#* Towards finite-sample convergence of direct reinforcement learning
#@ Shiau Hong Lim;Gerald DeJong
#t 2005
#c 20
#% 203608
#% 305085
#% 384911
#% 393786
#% 466075
#% 696947
#% 722895
#% 763696
#! While direct, model-free reinforcement learning often performs better than model-based approaches in practice, only the latter have yet supported theoretical guarantees for finite-sample convergence. A major difficulty in analyzing the direct approach in an online setting is the absence of a definitive exploration strategy. We extend the notion of admissibility to direct reinforcement learning and show that standard Q-learning with optimistic initial values and constant learning rate is admissible. The notion justifies the use of a greedy strategy that we believe performs very well in practice and holds theoretical significance in deriving finite-sample convergence for direct reinforcement learning. We present empirical evidence that supports our idea.

#index 1699595
#* Infinite ensemble learning with support vector machines
#@ Hsuan-Tien Lin;Ling Li
#t 2005
#c 20
#% 156186
#% 235377
#% 299255
#% 331916
#% 425033
#% 642773
#% 646003
#% 771845
#% 855583
#% 1558464
#! Ensemble learning algorithms such as boosting can achieve better performance by averaging over the predictions of base hypotheses. However, existing algorithms are limited to combining only a finite number of hypotheses, and the generated ensemble is usually sparse. It is not clear whether we should construct an ensemble classifier with a larger or even infinite number of hypotheses. In addition, constructing an infinite ensemble itself is a challenging task. In this paper, we formulate an infinite ensemble learning framework based on SVM. The framework can output an infinite and nonsparse ensemble, and can be used to construct new kernels for SVM as well as to interpret some existing ones. We demonstrate the framework with a concrete application, the stump kernel, which embodies infinitely many decision stumps. The stump kernel is simple, yet powerful. Experimental results show that SVM with the stump kernel is usually superior than boosting, even with noisy data.

#index 1699596
#* A kernel between unordered sets of data: the Gaussian mixture approach
#@ Siwei Lyu
#t 2005
#c 20
#% 304917
#% 304931
#% 464615
#% 722810
#% 724207
#% 743284
#% 771841
#! In this paper, we present a new kernel for unordered sets of data of the same type. It works by first fitting a set with a Gaussian mixture, then evaluate an efficient kernel on the two fitted Gaussian mixtures. Furthermore, we show that this kernel can be extended to sets embedded in a feature space implicitly defined by another kernel, where Gaussian mixtures are fitted with the kernelized EM algorithm [6], and the kernel for Gaussian mixtures are modified to use the outputs from the kernelized EM. All computation depends on data only through their inner products as evaluations of the base kernel. The kernel is computable in closed form, and being able to work in a feature space improves its flexibility and applicability. Its performance is evaluated in experiments on both synthesized and real data.

#index 1699597
#* Active learning for probability estimation using jensen-shannon divergence
#@ Prem Melville;Stewart M. Yang;Maytal Saar-Tsechansky;Raymond Mooney
#t 2005
#c 20
#% 115608
#% 116165
#% 170649
#% 290482
#% 464280
#% 565531
#% 577230
#% 580510
#% 735358
#% 770807
#% 788086
#% 1279286
#% 1289273
#! Active selection of good training examples is an important approach to reducing data-collection costs in machine learning; however, most existing methods focus on maximizing classification accuracy. In many applications, such as those with unequal misclassification costs, producing good class probability estimates (CPEs) is more important than optimizing classification accuracy. We introduce novel approaches to active learning based on the algorithms Bootstrap-LV and ActiveDecorate, by using Jensen-Shannon divergence (a similarity measure for probability distributions) to improve sample selection for optimizing CPEs. Comprehensive experimental results demonstrate the benefits of our approaches.

#index 1699598
#* Natural actor-critic
#@ Jan Peters;Sethu Vijayakumar;Stefan Schaal
#t 2005
#c 20
#% 258937
#% 305081
#% 384911
#% 393786
#% 466235
#% 1279357
#! This paper investigates a novel model-free reinforcement learning architecture, the Natural Actor-Critic. The actor updates are based on stochastic policy gradients employing Amari's natural gradient approach, while the critic obtains both the natural policy gradient and additional parameters of a value function simultaneously by linear regression. We show that actor improvements with natural policy gradients are particularly appealing as these are independent of coordinate frame of the chosen policy representation, and can be estimated more efficiently than regular policy gradients. The critic makes use of a special basis function parameterization motivated by the policy-gradient compatible function approximation. We show that several well-known reinforcement learning methods such as the original Actor-Critic and Bradtke's Linear Quadratic Q-Learning are in fact Natural Actor-Critic algorithms. Empirical evaluations illustrate the effectiveness of our techniques in comparison to previous methods, and also demonstrate their applicability for learning control on an anthropomorphic robot arm.

#index 1699599
#* Inducing head-driven PCFGs with latent heads: refining a tree-bank grammar for parsing
#@ Detlef Prescher
#t 2005
#c 20
#% 646980
#% 646987
#% 708948
#% 740916
#% 741115
#% 748561
#% 748722
#% 815279
#% 817431
#% 817472
#% 843647
#% 939672
#! Although state-of-the-art parsers for natural language are lexicalized, it was recently shown that an accurate unlexicalized parser for the Penn tree-bank can be simply read off a manually refined tree-bank. While lexicalized parsers often suffer from sparse data, manual mark-up is costly and largely based on individual linguistic intuition. Thus, across domains, languages, and tree-bank annotations, a fundamental question arises: Is it possible to automatically induce an accurate parser from a tree-bank without resorting to full lexicalization? In this paper, we show how to induce a probabilistic parser with latent head information from simple linguistic principles. Our parser has a performance of 85.1% (LP/LR F1), which is as good as that of early lexicalized ones. This is remarkable since the induction of probabilistic grammars is in general a hard task.

#index 1699600
#* Learning (k,l)-contextual tree languages for information extraction
#@ Stefan Raeymaekers;Maurice Bruynooghe;Jan Van den Bussche
#t 2005
#c 20
#% 71516
#% 278109
#% 289372
#% 431536
#% 450951
#% 451056
#% 458186
#% 464425
#% 478629
#% 531458
#% 1279271
#% 1279273
#! This paper introduces a novel method for learning a wrapper for extraction of text nodes from web pages based upon (k,l)-contextual tree languages. It also introduces a method to learn good values of k and l based on a few positive and negative examples. Finally, it describes how the algorithm can be integrated in a tool for information extraction.

#index 1699601
#* Neural fitted q iteration – first experiences with a data efficient neural reinforcement learning method
#@ Martin Riedmiller
#t 2005
#c 20
#% 124689
#% 124692
#% 384911
#% 734920
#% 829011
#! This paper introduces NFQ, an algorithm for efficient and effective training of a Q-value function represented by a multi-layer perceptron. Based on the principle of storing and reusing transition experiences, a model-free, neural network based Reinforcement Learning algorithm is proposed. The method is evaluated on three benchmark problems. It is shown empirically, that reasonably few interactions with the plant are needed to generate control policies of high quality.

#index 1699602
#* MCMC learning of bayesian network models by markov blanket decomposition
#@ Carsten Riggelsen
#t 2005
#c 20
#% 197387
#% 246835
#% 723247
#% 1650525
#! We propose a Bayesian method for learning Bayesian network models using Markov chain Monte Carlo (MCMC). In contrast to most existing MCMC approaches that define components in term of single edges, our approach is to decompose a Bayesian network model in larger dependence components defined by Markov blankets. The idea is based on the fact that MCMC performs significantly better when choosing the right decomposition, and that edges in the Markov blanket of the vertices form a natural dependence relationship. Using the ALARM and Insurance networks, we show that this decomposition allows MCMC to mix more rapidly, and is less prone to getting stuck in local maxima compared to the single edge approach.

#index 1699603
#* On discriminative joint density modeling
#@ Jarkko Salojärvi;Kai Puolamäki;Samuel Kaski
#t 2005
#c 20
#% 203462
#% 311027
#% 458673
#% 763708
#% 818220
#% 840929
#% 1650526
#! We study discriminative joint density models, that is, generative models for the joint density p(c,x) learned by maximizing a discriminative cost function, the conditional likelihood. We use the framework to derive generative models for generalized linear models, including logistic regression, linear discriminant analysis, and discriminative mixture of unigrams. The benefits of deriving the discriminative models from joint density models are that it is easy to extend the models and interpret the results, and missing data can be treated using justified standard methods.

#index 1699604
#* Model-based online learning of POMDPs
#@ Guy Shani;Ronen I. Brafman;Solomon E. Shimony
#t 2005
#c 20
#% 179940
#% 702594
#% 715259
#% 770768
#% 1478487
#% 1650314
#! Learning to act in an unknown partially observable domain is a difficult variant of the reinforcement learning paradigm. Research in the area has focused on model-free methods — methods that learn a policy without learning a model of the world. When sensor noise increases, model-free methods provide less accurate policies. The model-based approach — learning a POMDP model of the world, and computing an optimal policy for the learned model — may generate superior results in the presence of sensor noise, but learning and solving a model of the environment is a difficult problem. We have previously shown how such a model can be obtained from the learned policy of model-free methods, but this approach implies a distinction between a learning phase and an acting phase that is undesirable. In this paper we present a novel method for learning a POMDP model online, based on McCallums' Utile Suffix Memory (USM), in conjunction with an approximate policy obtained using an incremental POMDP solver. We show that the incrementally improving policy provides superior results to the original USM algorithm, especially in the presence of increasing sensor and action noise.

#index 1699605
#* Simple test strategies for cost-sensitive decision trees
#@ Shengli Sheng;Charles X. Ling;Qiang Yang
#t 2005
#c 20
#% 136350
#% 160852
#% 280437
#% 464639
#% 477640
#% 765519
#% 770791
#% 785338
#% 1272369
#% 1289281
#% 1673023
#! We study cost-sensitive learning of decision trees that incorporate both test costs and misclassification costs. In particular, we first propose a lazy decision tree learning that minimizes the total cost of tests and misclassifications. Then assuming test examples may contain unknown attributes whose values can be obtained at a cost (the test cost), we design several novel test strategies which attempt to minimize the total cost of tests and misclassifications for each test example. We empirically evaluate our tree-building and various test strategies, and show that they are very effective. Our results can be readily applied to real-world diagnosis tasks, such as medical diagnosis where doctors must try to determine what tests (e.g., blood tests) should be ordered for a patient to minimize the total cost of tests and misclassifications (misdiagnosis). A case study on heart disease is given throughout the paper.

#index 1699606
#* U-likelihood and U-updating algorithms: statistical inference in latent variable models
#@ Jaemo Sung;Sung-Yang Bang;Seungjin Choi;Zoubin Ghahramani
#t 2005
#c 20
#% 277483
#% 303620
#% 1650536
#! In this paper we consider latent variable models and introduce a new $\mathcal{U}$-likelihood concept for estimating the distribution over hidden variables. One can derive an estimate of parameters from this distribution. Our approach differs from the Bayesian and Maximum Likelihood (ML) approaches. It gives an alternative to Bayesian inference when we don't want to define a prior over parameters and gives an alternative to the ML method when we want a better estimate of the distribution over hidden variables. As a practical implementation, we present a $\mathcal{U}$-updating algorithm based on the mean field theory to approximate the distribution over hidden variables from the $\mathcal{U}$-likelihood. This algorithm captures some of the correlations among hidden variables by estimating reaction terms. Those reaction terms are found to penalize the likelihood. We show that the $\mathcal{U}$-updating algorithm becomes the EM algorithm as a special case in the large sample limit. The useful behavior of our method is confirmed for the case of mixture of Gaussians by comparing to the EM algorithm.

#index 1699607
#* An optimal best-first search algorithm for solving infinite horizon DEC-POMDPs
#@ Daniel Szer;François Charpillet
#t 2005
#c 20
#% 272652
#% 296585
#% 346451
#% 363744
#% 450852
#% 1250230
#% 1271975
#% 1272052
#% 1273798
#% 1279314
#% 1289555
#% 1290265
#% 1650313
#% 1650472
#! In the domain of decentralized Markov decision processes, we develop the first complete and optimal algorithm that is able to extract deterministic policy vectors based on finite state controllers for a cooperative team of agents. Our algorithm applies to the discounted infinite horizon case and extends best-first search methods to the domain of decentralized control theory. We prove the optimality of our approach and give some first experimental results for two small test problems. We believe this to be an important step forward in learning and planning in stochastic multi-agent systems.

#index 1699608
#* Ensemble learning with supervised kernels
#@ Kari Torkkola;Eugene Tuv
#t 2005
#c 20
#% 209021
#% 256615
#% 312727
#% 400847
#% 424997
#% 716271
#% 763697
#! Kernel-based methods have outstanding performance on many machine learning and pattern recognition tasks. However, they are sensitive to kernel selection, they may have low tolerance to noise, and they can not deal with mixed-type or missing data. We propose to derive a novel kernel from an ensemble of decision trees. This leads to kernel methods that naturally handle noisy and heterogeneous data with potentially non-randomly missing values. We demonstrate excellent performance of regularized least square learners based on such kernels.

#index 1699609
#* Using advice to transfer knowledge acquired in one reinforcement learning task to another
#@ Lisa Torrey;Trevor Walker;Jude Shavlik;Richard Maclin
#t 2005
#c 20
#% 124692
#% 124694
#% 126853
#% 203611
#% 384911
#% 464296
#% 464622
#% 466242
#% 793236
#% 823852
#% 1269488
#% 1269521
#% 1274747
#% 1290055
#! We present a method for transferring knowledge learned in one task to a related task. Our problem solvers employ reinforcement learning to acquire a model for one task. We then transform that learned model into advice for a new task. A human teacher provides a mapping from the old task to the new task to guide this knowledge transfer. Advice is incorporated into our problem solver using a knowledge-based support vector regression method that we previously developed. This advice-taking approach allows the problem solver to refine or even discard the transferred knowledge based on its subsequent experiences. We empirically demonstrate the effectiveness of our approach with two games from the RoboCup soccer simulator: KeepAway and BreakAway. Our results demonstrate that a problem solver learning to play BreakAway using advice extracted from KeepAway outperforms a problem solver learning without the benefit of such advice.

#index 1699610
#* A distance-based approach for action recommendation
#@ Ronan Trepos;Ansaf Salleb;Marie-Odile Cordier;Véronique Masson;Chantal Gascuel
#t 2005
#c 20
#% 136350
#% 152934
#% 342631
#% 443092
#% 458317
#% 478140
#% 543238
#% 550409
#% 565241
#% 629717
#% 727852
#% 768667
#% 1784628
#! Rule induction has attracted a great deal of attention in Machine Learning and Data Mining. However, generating rules is not an end in itself because their applicability is not so straightforward. Indeed, the user is often overwhelmed when faced with a large number of rules. In this paper, we propose an approach to lighten this burden when the user wishes to exploit such rules to decide which actions to do given an unsatisfactory situation. The method consists in comparing a situation to a set of classification rules. This is achieved using a suitable distance thus allowing to suggest action recommendations with minimal changes to improve that situation. We propose the algorithm Dakar for learning action recommendations and we present an application to an environmental protection issue. Our experiment shows the usefulness of our contribution in decision-making but also raises concerns about the impact of the redundancy of a set of rules in learning action recommendations of quality.

#index 1699611
#* Multi-armed bandit algorithms and empirical evaluation
#@ Joannès Vermorel;Mehryar Mohri
#t 2005
#c 20
#% 90041
#% 135414
#% 284108
#% 344409
#% 416988
#% 425053
#% 466090
#% 466731
#% 563266
#% 593734
#% 713108
#% 765257
#% 1080960
#! The multi-armed bandit problem for a gambler is to decide which arm of a K-slot machine to pull to maximize his total reward in a series of trials. Many real-world learning and optimization problems can be modeled in this way. Several strategies or algorithms have been proposed as a solution to this problem in the last two decades, but, to our knowledge, there has been no common evaluation of these algorithms. This paper provides a preliminary empirical evaluation of several multi-armed bandit algorithms. It also describes and analyzes a new algorithm, Poker (Price Of Knowledge and Estimated Reward) whose performance compares favorably to that of other existing algorithms in several experiments. One remarkable outcome of our experiments is that the most naive approach, the ε-greedy strategy, proves to be often hard to beat.

#index 1699612
#* Annealed discriminant analysis
#@ Gang Wang;Zhihua Zhang;Frederick H. Lochovsky
#t 2005
#c 20
#% 169378
#% 268055
#% 443948
#% 793240
#% 1762030
#! Motivated by the analogies to statistical physics, the deterministic annealing (DA) method has successfully been demonstrated in a variety of applications. In this paper, we explore a new methodology to devise the classifier under the DA method. The differential cost function is derived subject to a constraint on the randomness of the solution, which is governed by the temperature T. While gradually lowering the temperature, we can always find a good solution which can both solve the overfitting problem and avoid poor local optima. Our approach is called annealed discriminant analysis (ADA). It is a general approach, where we elaborate two classifiers, i.e., distance-based and inner product-based, in this paper. The distance-based classifier is an annealed version of linear discriminant analysis (LDA) while the inner product-based classifier is a generalization of penalized logistic regression (PLR). As such, ADA provides new insights into the workings of these two classification algorithms. The experimental results show substantial performance gains over standard learning methods.

#index 1699613
#* Network game and boosting
#@ Shijun Wang;Changshui Zhang
#t 2005
#c 20
#% 209021
#% 280496
#% 290482
#% 299255
#% 312727
#% 331916
#% 342628
#% 414609
#% 424997
#% 520224
#% 565543
#% 1273928
#% 1279286
#% 1499573
#! We propose an ensemble learning method called Network Boosting which combines weak learners together based on a random graph (network). A theoretic analysis based on the game theory shows that the algorithm can learn the target hypothesis asymptotically. The comparison results using several datasets of the UCI machine learning repository and synthetic data are promising and show that Network Boosting has much resistance to the noisy data than AdaBoost through the cooperation of classifiers in the classifier network.

#index 1699614
#* Model selection in omnivariate decision trees
#@ Olcay Taner Yıldız;Ethem Alpaydın
#t 2005
#c 20
#% 182686
#% 272995
#% 314784
#% 420084
#% 466096
#% 755460
#% 787639
#% 1011200
#% 1272358
#% 1860899
#! We propose an omnivariate decision tree architecture which contains univariate, multivariate linear or nonlinear nodes, matching the complexity of the node to the complexity of the data reaching that node. We compare the use of different model selection techniques including AIC, BIC, and CV to choose between the three types of nodes on standard datasets from the UCI repository and see that such omnivariate trees with a small percentage of multivariate nodes close to the root generalize better than pure trees with the same type of node everywhere. CV produces simpler trees than AIC and BIC without sacrificing from expected error. The only disadvantage of CV is its longer training time.

#index 1699615
#* Bayesian network learning with abstraction hierarchies and context-specific independence
#@ Marie desJardins;Priyang Rathod;Lise Getoor
#t 2005
#c 20
#% 44876
#% 129987
#% 477268
#% 785348
#% 785368
#% 1650705
#% 1650767
#% 1650783
#! Context-specific independence representations, such as tree-structured conditional probability tables (TCPTs), reduce the number of parameters in Bayesian networks by capturing local independence relationships and improve the quality of learned Bayesian networks. We previously presented Abstraction-Based Search (ABS), a technique for using attribute value hierarchies during Bayesian network learning to remove unimportant distinctions within the CPTs. In this paper, we introduce TCPT ABS (TABS), which integrates ABS with TCPT learning. Since expert-provided hierarchies may not be available, we provide a clustering technique for deriving hierarchies from data. We present empirical results for three real-world domains, finding that (1) combining TCPTs and ABS provides a significant increase in the quality of learned Bayesian networks (2) combining TCPTs and ABS provides a dramatic reduction in the number of parameters in the learned networks, and (3) data-derived hierarchies perform as well or better than expert-provided hierarchies.

#index 1699616
#* Learning to complete sentences
#@ Steffen Bickel;Peter Haider;Tobias Scheffer
#t 2005
#c 20
#% 214556
#% 403341
#% 529804
#% 716159
#% 766461
#% 1271949
#! We consider the problem of predicting how a user will continue a given initial text fragment. Intuitively, our goal is to develop a “tab-complete” function for natural language, based on a model that is learned from text data. We consider two learning mechanisms that generate predictive models from collections of application-specific document collections: we develop an N-gram based completion method and discuss the application of instance-based learning. After developing evaluation metrics for this task, we empirically compare the model-based to the instance-based method and assess the predictability of call-center emails, personal emails, and weather reports.

#index 1699617
#* The huller: a simple and efficient online SVM
#@ Antoine Bordes;Léon Bottou
#t 2005
#c 20
#% 190581
#% 197394
#% 425046
#% 466087
#% 466589
#% 722814
#% 722903
#! We propose a novel online kernel classifier algorithm that converges to the Hard Margin SVM solution. The same update rule is used to both add and remove support vectors from the current classifier. Experiments suggest that this algorithm matches the SVM accuracies after a single pass over the training examples. This algorithm is attractive when one seeks a competitive classifier with large datasets and limited computing resources.

#index 1699618
#* Inducing hidden Markov models to model long-term dependencies
#@ Jérôme Callut;Pierre Dupont
#t 2005
#c 20
#% 137711
#% 200195
#% 476398
#% 531459
#% 1272172
#! We propose in this paper a novel approach to the induction of the structure of Hidden Markov Models. The induced model is seen as a lumped process of a Markov chain. It is constructed to fit the dynamics of the target machine, that is to best approximate the stationary distribution and the mean first passage times observed in the sample. The induction relies on non-linear optimization and iterative state splitting from an initial order one Markov chain.

#index 1699619
#* A similar fragments merging approach to learn automata on proteins
#@ François Coste;Goulven Kerbellec
#t 2005
#c 20
#% 116138
#% 202295
#% 464401
#% 546119
#% 1272315
#! We propose here to learn automata for the characterization of proteins families to overcome the limitations of the position-specific characterizations classically used in Pattern Discovery. We introduce a new heuristic approach learning non-deterministic automata based on selection and ordering of significantly similar fragments to be merged and on physico-chemical properties identification. Quality of the characterization of the major intrinsic protein (MIP) family is assessed by leave-one-out cross-validation for a large range of models specificity.

#index 1699620
#* Nonnegative Lagrangian relaxation of k-means and spectral clustering
#@ Chris Ding;Xiaofeng He;Horst D. Simon
#t 2005
#c 20
#% 313959
#% 342659
#% 464291
#! We show that K-means and spectral clustering objective functions can be written as a trace of quadratic forms. Instead of relaxation by eigenvectors, we propose a novel relaxation maintaining the nonnegativity of the cluster indicators and thus give the cluster posterior probabilities, therefore resolving cluster assignment difficulty in spectral relaxation. We derive a multiplicative updating algorithm to solve the nonnegative relaxation problem. The method is briefly extended to semi-supervised classification and semi-supervised clustering.

#index 1699621
#* Severe class imbalance: why better algorithms aren't the answer
#@ Chris Drummond;Robert C. Holte
#t 2005
#c 20
#% 136350
#% 266280
#% 286972
#% 310519
#% 420064
#% 465738
#% 466086
#% 1279288
#! This paper argues that severe class imbalance is not just an interesting technical challenge that improved learning algorithms will address, it is much more serious. To be useful, a classifier must appreciably outperform a trivial solution, such as choosing the majority class. Any application that is inherently noisy limits the error rate, and cost, that is achievable. When data are normally distributed, even a Bayes optimal classifier has a vanishingly small reduction in the majority classifier's error rate, and cost, as imbalance increases. For fat tailed distributions, and when practical classifiers are used, often no reduction is achieved.

#index 1699622
#* Approximation algorithms for minimizing empirical error by axis-parallel hyperplanes
#@ Tapio Elomaa;Jussi Kujala;Juho Rousu
#t 2005
#c 20
#% 132927
#% 178511
#% 203331
#% 210347
#% 235384
#% 246832
#% 341672
#% 415068
#% 500836
#% 566098
#% 738958
#% 743284
#% 837668
#! Many learning situations involve separation of labeled training instances by hyperplanes. Consistent separation is of theoretical interest, but the real goal is rather to minimize the number of errors using a bounded number of hyperplanes. Exact minimization of empirical error in a high-dimensional grid induced into the feature space by axis-parallel hyperplanes is NP-hard. We develop two approximation schemes with performance guarantees, a greedy set covering scheme for producing a consistently labeled grid, and integer programming rounding scheme for finding the minimum error grid with bounded number of hyperplanes.

#index 1699623
#* A comparison of approaches for learning probability trees
#@ Daan Fierens;Jan Ramon;Hendrik Blockeel;Maurice Bruynooghe
#t 2005
#c 20
#% 277494
#% 342604
#% 550574
#% 580510
#% 729982
#% 1290272
#! Probability trees (or Probability Estimation Trees, PET's) are decision trees with probability distributions in the leaves. Several alternative approaches for learning probability trees have been proposed but no thorough comparison of these approaches exists. In this paper we experimentally compare the main approaches using the relational decision tree learner Tilde (both on non-relational and on relational datasets). Next to the main existing approaches, we also consider a novel variant of an existing approach based on the Bayesian Information Criterion (BIC). Our main conclusion is that overall trees built using the C4.5-approach or the C4.4-approach (C4.5 without post-pruning) have the best predictive performance. If the number of classes is low, however, BIC performs equally well. An additional advantage of BIC is that its trees are considerably smaller than trees for the C4.5- or C4.4-approach.

#index 1699624
#* Counting positives accurately despite inaccurate classification
#@ George Forman
#t 2005
#c 20
#% 290482
#% 642988
#% 722935
#% 1272000
#! Most supervised machine learning research assumes the training set is a random sample from the target population, thus the class distribution is invariant. In real world situations, however, the class distribution changes, and is known to erode the effectiveness of classifiers and calibrated probability estimators. This paper focuses on the problem of accurately estimating the number of positives in the test set—quantification—as opposed to classifying individual cases accuratel y. It compares three methods: classify & count, an adjusted variant, and a mixture model. An empirical evaluation on a text classification benchmark reveals that the simple method is consistently biased, and that the mixture model is surprisingly effective even when positives are very scarce in the training set—a common case in information retrieval.

#index 1699625
#* Optimal stopping and constraints for diffusion models of signals with discontinuities
#@ Ramūnas Girdziušas;Jorma Laaksonen
#t 2005
#c 20
#% 132676
#% 400267
#% 451827
#% 1810168
#% 1854591
#! Gaussian process regression models can be utilized in recovery of discontinuous signals. Their computational complexity is linear in the number of observations if applied with the covariance functions of nonlinear diffusion. However, such processes often result in hard-to-control jumps of the signal value. Synthetic examples presented in this work indicate that Bayesian evidence-maximizing stopping and knowledge whether signal values are discrete help to outperform the steady state solutions of nonlinear diffusion filtering.

#index 1699626
#* An evolutionary function approximation approach to compute prediction in XCSF
#@ Ali Hamzeh;Adel Rahmani
#t 2005
#c 20
#% 92237
#% 114994
#% 356892
#% 425083
#% 1022821
#% 1386605
#! XCSF is a new extension to XCS that is developed to extend XCS's reward calculation capability via computing. This new feature is called computable prediction. The first version of XCSF tries to find the most appropriate equation to compute each classifier's reward using a weight update mechanism. In this paper, we try to propose a new evolutionary mechanism to compute these equations using genetic algorithms.

#index 1699627
#* Using rewards for belief state updates in partially observable markov decision processes
#@ Masoumeh T. Izadi;Doina Precup
#t 2005
#c 20
#% 565547
#% 655325
#% 788098
#% 1271823
#% 1279358
#% 1289695
#% 1650702
#! Partially Observable Markov Decision Processes (POMDP) provide a standard framework for sequential decision making in stochastic environments. In this setting, an agent takes actions and receives observations and rewards from the environment. Many POMDP solution methods are based on computing a belief state, which is a probability distribution over possible states in which the agent could be. The action choice of the agent is then based on the belief state. The belief state is computed based on a model of the environment, and the history of actions and observations seen by the agent. However, reward information is not taken into account in updating the belief state. In this paper, we argue that rewards can carry useful information that can help disambiguate the hidden state. We present a method for updating the belief state which takes rewards into account. We present experiments with exact and approximate planning methods on several standard POMDP domains, using this belief update method, and show that it can provide advantages, both in terms of speed and in terms of the quality of the solution obtained.

#index 1699628
#* Active learning in partially observable markov decision processes
#@ Robin Jaulmes;Joelle Pineau;Doina Precup
#t 2005
#c 20
#% 702594
#% 1279358
#% 1290265
#% 1650283
#! This paper examines the problem of finding an optimal policy for a Partially Observable Markov Decision Process (POMDP) when the model is not known or is only poorly specified. We propose two approaches to this problem. The first relies on a model of the uncertainty that is added directly into the POMDP planning problem. This has theoretical guarantees, but is impractical when many of the parameters are uncertain. The second, called MEDUSA, incrementally improves the POMDP model using selected queries, while still optimizing reward. Results show good performance of the algorithm even in large problems: the most useful parameters of the model are learned quickly and the agent still accumulates high reward throughout the process.

#index 1699629
#* Machine learning of plan robustness knowledge about instances
#@ Sergio Jiménez;Fernando Fernández;Daniel Borrajo
#t 2005
#c 20
#% 124691
#% 333786
#% 421782
#% 647111
#% 1272286
#! Classical planning domain representations assume all the objects from one type are exactly the same. But when solving problems in the real world systems, the execution of a plan that theoretically solves a problem, can fail because of not properly capturing the special features of an object in the initial representation. We propose to capture this uncertainty about the world with an architecture that integrates planning, execution and learning. In this paper, we describe the PELA system (Planning-Execution-Learning Architecture). This system generates plans, executes those plans in the real world, and automatically acquires knowledge about the behaviour of the objects to strengthen the execution processes in the future.

#index 1699630
#* Two contributions of constraint programming to machine learning
#@ Arnaud Lallouet;Andreï Legtchenko
#t 2005
#c 20
#% 92148
#% 136350
#% 396021
#% 564858
#% 744223
#% 787003
#% 953070
#% 1279418
#! A constraint is a relation with an active behavior. For a given relation, we propose to learn a representation adapted to this active behavior. It yields two contributions. The first is a generic meta-technique for classifier improvement showing performances comparable to boosting. The second lies in the ability of using the learned concept in constraint-based decision or optimization problems. It opens a new way of integrating Machine Learning in Decision Support Systems.

#index 1699631
#* A clustering model based on matrix approximation with applications to cluster system log files
#@ Tao Li;Wei Peng
#t 2005
#c 20
#% 210173
#% 248790
#% 820377
#% 848846
#! In system management applications, to perform automated analysis of the historical data across multiple components when problems occur, we need to cluster the log messages with disparate formats to automatically infer the common set of semantic situations and obtain a brief description for each situation. In this paper, we propose a clustering model where the problem of clustering is formulated as matrix approximations and the clustering objective is minimizing the approximation error between the original data matrix and the reconstructed matrix based on the cluster structures. The model explicitly characterizes the data and feature memberships and thus enables the descriptions of each cluster. We present a two-side spectral relaxation optimization procedure for the clustering model. We also establish the connections between our clustering model with existing approaches. Experimental results show the effectiveness of the proposed approach.

#index 1699632
#* Detecting fraud in health insurance data: learning to model incomplete benford's law distributions
#@ Fletcher Lu;J. Efrim Boritz
#t 2005
#c 20
#! Benford's Law [1] specifies the probabilistic distribution of digits for many commonly occurring phenomena, ideally when we have complete data of the phenomena. We enhance this digital analysis technique with an unsupervised learning method to handle situations where data is incomplete. We apply this method to the detection of fraud and abuse in health insurance claims using real health insurance data. We demonstrate improved precision over the traditional Benford approach in detecting anomalous data indicative of fraud and illustrate some of the challenges to the analysis of healthcare claims fraud.

#index 1699633
#* Efficient case based feature construction
#@ Ingo Mierswa;Michael Wurst
#t 2005
#c 20
#% 190581
#% 203329
#% 243727
#% 393059
#% 429833
#% 449588
#% 799392
#% 1271814
#% 1478478
#% 1777043
#! Feature construction is essential for solving many complex learning problems. Unfortunately, the construction of features usually implies searching a very large space of possibilities and is often computationally demanding. In this work, we propose a case based approach to feature construction. Learning tasks are stored together with a corresponding set of constructed features in a case base and can be retrieved to speed up feature construction for new tasks. The essential part of our method is a new representation model for learning tasks and a corresponding distance measure. Learning tasks are compared using relevance weights on a common set of base features only. Therefore, the case base can be built and queried very efficiently. In this respect, our approach is unique and enables us to apply case based feature construction not only on a large scale, but also in distributed learning scenarios in which communication costs play an important role. We derive a distance measure for heterogeneous learning tasks by stating a set of necessary conditions. Although the conditions are quite basic, they constraint the set of applicable methods to a surprisingly small number.

#index 1699634
#* Fitting the smallest enclosing bregman ball
#@ Richard Nock;Frank Nielsen
#t 2005
#c 20
#% 302406
#% 420077
#% 770821
#% 803575
#! Finding a point which minimizes the maximal distortion with respect to a dataset is an important estimation problem that has recently received growing attentions in machine learning, with the advent of one class classification. We propose two theoretically founded generalizations to arbitrary Bregman divergences, of a recent popular smallest enclosing ball approximation algorithm for Euclidean spaces coined by Bădoiu and Clarkson in 2002.

#index 1699635
#* Similarity-based alignment and generalization
#@ Daniel Oblinger;Vittorio Castelli;Tessa Lau;Lawrence D. Bergman
#t 2005
#c 20
#% 150994
#% 466590
#% 1860077
#! We present a novel approach to learning predictive sequential models, called similarity-based alignment and generalization, which incorporates in the induction process a specific form of domain knowledge derived from a similarity function between the points in the input space. When applied to Hidden Markov Models, our framework yields a new class of learning algorithms called SimAlignGen. We discuss the application of our approach to the problem of programming by demonstration–the problem of learning a procedural model of user behavior by observing the interaction an application Graphical User Interface (GUI). We describe in detail the SimIOHMM, a specific instance of SimAlignGen that extends the known Input-Output Hidden Markov Model (IOHMM). Empirical evaluations of the SimIOHMM show the dependence of the prediction accuracy on the introduced similarity bias, and the computational gains over the IOHMM.

#index 1699636
#* Fast non-negative dimensionality reduction for protein fold recognition
#@ Oleg Okun;Helen Priisalu;Alexessander Alves
#t 2005
#c 20
#% 679330
#% 774882
#% 1395356
#! In this paper, dimensionality reduction via matrix factorization with nonnegativity constraints is studied. Because of these constraints, it stands apart from other linear dimensionality reduction methods. Here we explore nonnegative matrix factorization in combination with a classifier for protein fold recognition. Since typically matrix factorization is iteratively done, convergence can be slow. To alleviate this problem, a significantly faster (more than 11 times) algorithm is proposed.

#index 1699637
#* Mode directed path finding
#@ Irene M. Ong;Inês de Castro Dutra;David Page;Vítor Santos Costa
#t 2005
#c 20
#% 170422
#% 211658
#% 445369
#% 568786
#% 577219
#% 731609
#% 731611
#% 1478466
#! Learning from multi-relational domains has gained increasing attention over the past few years. Inductive logic programming (ILP) systems, which often rely on hill-climbing heuristics in learning first-order concepts, have been a dominating force in the area of multi-relational concept learning. However, hill-climbing heuristics are susceptible to local maxima and plateaus. In this paper, we show how we can exploit the links between objects in multi-relational data to help a first-order rule learning system direct the search by explicitly traversing these links to find paths between variables of interest. Our contributions are twofold: (i) we extend the pathfinding algorithm by Richards and Mooney [12] to make use of mode declarations, which specify the mode of call (input or output) for predicate variables, and (ii) we apply our extended path finding algorithm to saturated bottom clauses, which anchor one end of the search space, allowing us to make use of background knowledge used to build the saturated clause to further direct search. Experimental results on a medium-sized dataset show that path finding allows one to consider interesting clauses that would not easily be found by Aleph.

#index 1699638
#* Classification with maximum entropy modeling of predictive association rules
#@ Hieu X. Phan;Minh L. Nguyen;S. Horiguchi;Bao T. Ho;Y. Inoguchi
#t 2005
#c 20
#% 73441
#% 136350
#% 211044
#% 226495
#% 280409
#% 300120
#% 449566
#% 466483
#% 481290
#% 546047
#% 709765
#% 729437
#% 732388
#% 854813
#! This paper presents a new classification model in which a classifier is built upon predictive association rules (PARs) and the maximum entropy principle (maxent). In this model, PARs can be seen as confident statistical patterns discovered from training data with strong dependencies and correlations among data items. Maxent, on the other hand, is an approach to build an estimated distribution having maximum entropy while obeying a potentially large number of useful features observed in empirical data. The underlying idea of our model is that PARs have suitable characteristics to serve as features for maxent. As a result, our classifier can take advantage of both the useful correlation and confidence of PARs as well as the strong statistical modeling capability of maxent. The experimental results show that our model can achieve significantly higher accuracy in comparison with the previous methods.

#index 1699639
#* Classification of ordinal data using neural networks
#@ Joaquim Pinto da Costa;Jaime S. Cardoso
#t 2005
#c 20
#% 458623
#! Many real life problems require the classification of items in naturally ordered classes. These problems are traditionally handled by conventional methods for nominal classes, ignoring the order. This paper introduces a new training model for feedforward neural networks, for multiclass classification problems, where the classes are ordered. The proposed model has just one output unit which takes values in the interval [0,1]; this interval is then subdivided into K subintervals (one for each class), according to a specific probabilistic model. A comparison is made with conventional approaches, as well as with other architectures specific for ordinal data proposed in the literature. The new model compares favourably with the other methods under study, in the synthetic dataset used for evaluation.

#index 1699640
#* Independent subspace analysis on innovations
#@ Barnabás Póczos;Bálint Takács;András Lőrincz
#t 2005
#c 20
#% 106318
#% 176172
#% 190861
#% 272373
#% 299264
#% 339570
#% 840919
#% 856933
#% 1579112
#% 1781226
#! Independent subspace analysis (ISA) that deals with multi-dimensional independent sources, is a generalization of independent component analysis (ICA). However, all known ISA algorithms may become ineffective when the sources possess temporal structure. The innovation process instead of the original mixtures has been proposed to solve ICA problems with temporal dependencies. Here we show that this strategy can be applied to ISA as well. We demonstrate the idea on a mixture of 3D processes and also on a mixture of facial pictures used as two-dimensional deterministic sources. ISA on innovations was able to find the original subspaces, while plain ISA was not.

#index 1699641
#* On applying tabling to inductive logic programming
#@ Ricardo Rocha;Nuno Fonseca;Vítor Santos Costa
#t 2005
#c 20
#% 723258
#% 1271843
#% 1271968
#! Inductive Logic Programming (ILP) is an established sub-field of Machine Learning. Nevertheless, it is recognized that efficiency and scalability is a major obstacle to an increased usage of ILP systems in complex applications with large hypotheses spaces. In this work, we focus on improving the efficiency and scalability of ILP systems by exploring tabling mechanisms available in the underlying Logic Programming systems. Tabling is an implementation technique that improves the declarativeness and performance of Prolog systems by reusing answers to subgoals. To validate our approach, we ran the April ILP system in the YapTab Prolog tabling system using two well-known datasets. The results obtained show quite impressive gains without changing the accuracy and quality of the theories generated.

#index 1699642
#* Learning models of relational stochastic processes
#@ Sumit Sanghai;Pedro Domingos;Daniel Weld
#t 2005
#c 20
#% 226437
#% 226495
#% 265806
#% 577220
#% 577225
#% 716892
#% 748017
#% 840890
#% 850430
#% 1279354
#! Processes involving change over time, uncertainty, and rich relational structure are common in the real world, but no general algorithms exist for learning models of them. In this paper we show how Markov logic networks (MLNs), a recently developed approach to combining logic and probability, can be applied to time-changing domains. We then show how existing algorithms for parameter and structure learning in MLNs can be extended to this setting. We apply this approach in two domains: modeling the spread of research topics in scientific communities, and modeling faults in factory assembly processes. Our experiments show that it greatly outperforms purely logical (ILP) and purely probabilistic (DBN) learners.

#index 1699643
#* Error-sensitive grading for model combination
#@ Surendra K. Singhi;Huan Liu
#t 2005
#c 20
#% 132938
#% 136350
#% 209021
#% 379342
#% 464782
#% 549438
#% 738972
#% 926881
#% 1272397
#! Ensemble learning is a powerful learning approach that combines multiple classifiers to improve prediction accuracy. An important decision while using an ensemble of classifiers is to decide upon a way of combining the prediction of its base classifiers. In this paper, we introduce a novel grading-based algorithm for model combination, which uses cost-sensitive learning in building a meta-learner. This method distinguishes between the grading error of classifying an incorrect prediction as correct, and the other-way-round, and tries to assign appropriate costs to the two types of error in order to improve performance. We study issues in error-sensitive grading, and then with extensive experiments show the empirically effectiveness of this new method in comparison with representative meta-classification techniques.

#index 1699644
#* Strategy learning for reasoning agents
#@ Hendrik Skubch;Michael Thielscher
#t 2005
#c 20
#% 284647
#% 382569
#% 396021
#% 550402
#% 815506
#% 1576997
#! We present a method for knowledge-based agents to learn strategies. Using techniques of inductive logic programming, strategies are learned in two steps: A given example set is first generalized into an overly general theory, which then gets refined. We show how a learning agent can exploit background knowledge of its actions and environment in order to restrict the hypothesis space, which enables the learning of complex logic program clauses. This is a first step toward the long term goal of adaptive, reasoning agents capable of changing their behavior when appropriate.

#index 1699645
#* Combining bias and variance reduction techniques for regression trees
#@ Yuk Lai Suen;Prem Melville;Raymond J. Mooney
#t 2005
#c 20
#% 132583
#% 209021
#% 312728
#% 350335
#! Gradient Boosting and bagging applied to regressors can reduce the error due to bias and variance respectively. Alternatively, Stochastic Gradient Boosting (SGB) and Iterated Bagging (IB) attempt to simultaneously reduce the contribution of both bias and variance to error. We provide an extensive empirical analysis of these methods, along with two alternate bias-variance reduction approaches — bagging Gradient Boosting (BagGB) and bagging Stochastic Gradient Boosting (BagSGB). Experimental results demonstrate that SGB does not perform as well as IB or the alternate approaches. Furthermore, results show that, while BagGB and BagSGB perform competitively for low-bias learners, in general, Iterated Bagging is the most effective of these methods.

#index 1699646
#* Analysis of generic perceptron-like large margin classifiers
#@ Petroula Tsampouka;John Shawe-Taylor
#t 2005
#c 20
#% 190581
#% 309208
#% 464612
#% 729437
#! We analyse perceptron-like algorithms with margin considering both the standard classification condition and a modified one which demands a specific value of the margin in the augmented space. The new algorithms are shown to converge in a finite number of steps and used to approximately locate the optimal weight vector in the augmented space. As the data are embedded in the augmented space at a larger distance from the origin the maximum margin in that space approaches the maximum geometric one in the original space. Thus, our procedures exploiting the new algorithms can be regarded as approximate maximal margin classifiers.

#index 1699647
#* Multimodal function optimizing by a new hybrid nonlinear simplex search and particle swarm algorithm
#@ Fang Wang;Yuhui Qiu
#t 2005
#c 20
#% 294103
#% 329349
#% 425081
#% 546801
#% 1777201
#! A new hybrid Particle Swarm Optimization (PSO) algorithm is proposed in this paper based on the Nonlinear Simplex Search (NSS) method for multimodal function optimizing tasks. At late stage of PSO process, when the most promising regions of solutions are fixed, the algorithm isolates particles that fly very close to the extrema and applies the NSS method to them to enhance local exploitation searching. Explicit experimental results on famous benchmark functions indicate that this approach is reliable and efficient, especially on multimodal function optimizations. It yields better solution qualities and success rates compared to other three published methods.

